*GitHub Repository "NEAR-Analytics/SocialDB_Archive_Metrics"*

'''--- data_check.py ---
import pandas as pd

# Load the CSV into a DataFrame
commits_df = pd.read_csv('commit_messages_fromated.csv')

# Count unique authors
unique_authors = commits_df['NEAR_DEV'].nunique()
print(f"Number of unique authors: {unique_authors}")

# Convert the 'Time' column to datetime format using mixed format option and extract the date part
commits_df['Date'] = pd.to_datetime(commits_df['TX_TIME'], format='mixed').apply(lambda x: x.date())

# Sort the dataframe by TX_TIME
commits_df_sorted = commits_df.sort_values(by='TX_TIME')

# Group by date and count all developers (non-unique)
active_devs_per_day = commits_df.groupby('Date').size()

# Reset the index to get it into a DataFrame
active_devs_per_day = active_devs_per_day.reset_index()

# Rename columns for clarity
active_devs_per_day.columns = ['Date', 'Unique Active Developers']

active_devs_per_day.to_csv('author_activity_per_day.csv', index=False)

# Sort the dataframe by TX_TIME
commits_df_sorted = commits_df.sort_values(by='TX_TIME')

# Group by date and count unique developers
active_devs_per_day = commits_df.groupby('Date')['NEAR_DEV'].nunique()

# Reset the index to get it into a DataFrame
active_devs_per_day = active_devs_per_day.reset_index()

# Rename columns for clarity
active_devs_per_day.columns = ['Date', 'Unique Active Developers']

active_devs_per_day.to_csv('author_activity_per_day_unique.csv', index=False)

'''
'''--- extract_commits.py ---
import subprocess
import re
import csv

# Step 1: Extract Commit Messages
repo_path = ''  # Replace with the path to your repo
command = ['git', '-C', repo_path, 'log', '--pretty=format:%s']
output = subprocess.check_output(command, text=True)
commit_messages = output.splitlines()

# Step 2: Filter Messages
pattern = r'^Update .+ by .+ at .+$'
filtered_messages = [msg for msg in commit_messages if re.match(pattern, msg)]

# Step 3: Save to CSV
csv_file = 'commit_messages.csv'
with open(csv_file, 'w', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(['Commit Message'])
    for msg in filtered_messages:
        writer.writerow([msg])

input_csv = 'commit_messages.csv'  # Path to your input CSV file
output_csv = 'commit_messages_fromated.csv'  # Path to the desired output CSV file

with open(input_csv, 'r') as infile, open(output_csv, 'w', newline='') as outfile:
    reader = csv.reader(infile)
    writer = csv.writer(outfile)

    # Write headers to the output CSV
    writer.writerow(['WIDGET_NAME', 'NEAR_DEV', 'TX_TIME'])

    # Skip header in the input CSV
    next(reader)

    for row in reader:
        try:
            message = row[0]

            # Split the message into parts
            parts = message.split(' by ')
            widget_name = parts[0].split(' ')[1]
            near_dev, tx_time = parts[1].split(' at ')

            # Write the split data to the output CSV
            writer.writerow([widget_name, near_dev, tx_time])
        except Exception as e:
            print(f"Error processing row {row}: {e}")
'''
'''--- readme.md ---

# SocialDB Smart Contract Metrics (BOS Widgets):

1. git pull the archive from github: https://github.com/NEAR-Analytics/NEAR_SOCIAL_DB_WIDGETS
2. run `extract_commits.py`, to generate a time series of commits per day.
   1. update `repo_path` to where the archive is located
3. then run `data_check.py`, this will create the timeseries for unique and overall dev count overtime.
'''