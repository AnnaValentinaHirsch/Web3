*GitHub Repository "kuutamolabs/rapid-gossip-sync-server"*

'''--- .github/workflows/build.yml ---
name: Cross-platform build verification
on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - "*"

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  build:
    strategy:
      fail-fast: false
      matrix:
        toolchain:
          - stable
          - 1.56.0
          - beta
    runs-on: ubuntu-latest
    steps:
      - name: Checkout source code
        uses: actions/checkout@v3
      - name: Install Rust ${{ matrix.toolchain }} toolchain
        uses: actions-rs/toolchain@v1
        with:
          toolchain: ${{ matrix.toolchain }}
          override: true
          profile: minimal
      - name: Build on Rust ${{ matrix.toolchain }}
        run: |
          cargo build --verbose --color always
  test:
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres
        env:
          POSTGRES_PASSWORD: postgres
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    steps:
      - name: Checkout source code
        uses: actions/checkout@v3
      - name: Install Rust ${{ matrix.toolchain }} toolchain
        uses: actions-rs/toolchain@v1
        with:
          toolchain: stable
          override: true
          profile: minimal
      - name: Run tests
        run: |
          cargo test --verbose --color always -- --show-output
        env:
          RAPID_GOSSIP_TEST_DB_HOST: localhost
          RAPID_GOSSIP_TEST_DB_NAME: postgres
          RAPID_GOSSIP_TEST_DB_USER: postgres
          RAPID_GOSSIP_TEST_DB_PASSWORD: postgres
          RAPID_GOSSIP_SYNC_SERVER_LOG_LEVEL: gossip

'''
'''--- Cargo.toml ---
[package]
name = "rapid-gossip-sync-server"
version = "0.1.0"
edition = "2021"

[dependencies]
bitcoin = "0.29"
lightning = { version = "0.0.117" }
lightning-block-sync = { version = "0.0.117", features=["rest-client"] }
lightning-net-tokio = { version = "0.0.117" }
tokio = { version = "1.25", features = ["full"] }
tokio-postgres = { version = "=0.7.5" }
futures = "0.3"

[dev-dependencies]
lightning = { version = "0.0.117", features = ["_test_utils"] }
lightning-rapid-gossip-sync = { version = "0.0.117" }

[profile.dev]
panic = "abort"

[profile.release]
opt-level = 3
lto = true
panic = "abort"

'''
'''--- LICENSE-APACHE.md ---
                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright [yyyy] [name of copyright owner]

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.

'''
'''--- LICENSE-MIT.md ---
Permission is hereby granted, free of charge, to any person obtaining a copy of
this software and associated documentation files (the "Software"), to deal in
the Software without restriction, including without limitation the rights to
use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of
the Software, and to permit persons to whom the Software is furnished to do so,
subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS
FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR
COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER
IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

'''
'''--- LICENSE.md ---
This software is licensed under [Apache 2.0](LICENSE-APACHE) or
[MIT](LICENSE-MIT), at your option.

Some files retain their own copyright notice, however, for full authorship
information, see version control history.

Except as otherwise noted in individual files, all files in this repository are
licensed under the Apache License, Version 2.0 <LICENSE-APACHE or
http://www.apache.org/licenses/LICENSE-2.0> or the MIT license <LICENSE-MIT or
http://opensource.org/licenses/MIT>, at your option.

You may not use, copy, modify, merge, publish, distribute, sublicense, and/or
sell copies of this software or any files in this repository except in
accordance with one or both of these licenses.

'''
'''--- README.md ---
# rapid-gossip-sync-server

This is a server that connects to peers on the Lightning network and calculates compact rapid sync
gossip data.

These are the components it's comprised of.

## Modules

### config

A config file where the Postgres credentials and Lightning peers can be adjusted. Most adjustments
can be made by setting environment variables, whose usage is as follows:

| Name                                       | Default             | Description                                                                                                |
|:-------------------------------------------|:--------------------|:-----------------------------------------------------------------------------------------------------------|
| RAPID_GOSSIP_SYNC_SERVER_DB_HOST           | localhost           | Domain of the Postgres database                                                                            |
| RAPID_GOSSIP_SYNC_SERVER_DB_USER           | alice               | Username to access Postgres                                                                                |
| RAPID_GOSSIP_SYNC_SERVER_DB_PASSWORD       | _None_              | Password to access Postgres                                                                                |
| RAPID_GOSSIP_SYNC_SERVER_DB_NAME           | ln_graph_sync       | Name of the database to be used for gossip storage                                                         |
| RAPID_GOSSIP_SYNC_SERVER_NETWORK           | mainnet             | Network to operate in. Possible values are mainnet, testnet, signet, regtest                               |
| RAPID_GOSSIP_SYNC_SERVER_SNAPSHOT_INTERVAL | 10800               | The interval in seconds between snapshots                                                                  |
| BITCOIN_REST_DOMAIN                        | 127.0.0.1           | Domain of the [bitcoind REST server](https://github.com/bitcoin/bitcoin/blob/master/doc/REST-interface.md) |
| BITCOIN_REST_PORT                          | 8332                | HTTP port of the bitcoind REST server                                                                      |
| BITCOIN_REST_PATH                          | /rest/              | Path infix to access the bitcoind REST endpoints                                                           |
| LN_PEERS                                   | _Wallet of Satoshi_ | Comma separated list of LN peers to use for retrieving gossip                                              |

### downloader

The module responsible for initiating the scraping of the network graph from its peers.

### persistence

The module responsible for persisting all the downloaded graph data to Postgres.

### snapshot

The snapshotting module is responsible for calculating and storing snapshots. It's started up
as soon as the first full graph sync completes, and then keeps updating the snapshots at a
configurable interval with a 3-hour-default.

### lookup

The lookup module is responsible for fetching the latest data from the network graph and Postgres,
and reconciling it into an actionable delta set that the server can return in a serialized format.

It works by collecting all the channels that are currently in the network graph, and gathering
announcements as well as updates for each of them. For the updates specifically, the last update
seen prior to the given timestamp, the latest known updates, and, if necessary, all intermediate
updates are collected.

Then, any channel that has only had an announcement but never an update is dropped. Additionally,
every channel whose first update was seen after the given timestamp is collected alongside its
announcement.

Finally, all channel update transitions are evaluated and collected into either a full or an
incremental update.

## License

[Apache 2.0](LICENSE-APACHE.md) or [MIT](LICENSE-MIT.md), [at your option](LICENSE.md).

'''
'''--- docker-compose.yml ---
version: "3"

services:
  rgs_server:
    build:
      context: '$PWD'
      dockerfile: '$PWD/docker/Dockerfile.rgs'
    volumes:
      - '$PWD:/usr/src/app:cached'
    links:
      - postgres
      - bitcoin-core
    depends_on:
      - postgres
    environment:
      - RAPID_GOSSIP_SYNC_SERVER_DB_HOST=postgres
      - RAPID_GOSSIP_SYNC_SERVER_DB_USER=lightning-rgs
      - RAPID_GOSSIP_SYNC_SERVER_DB_PASSWORD=docker
      - RAPID_GOSSIP_SYNC_SERVER_DB_NAME=ln_graph_sync
      - BITCOIN_REST_DOMAIN=bitcoin-core
      - BITCOIN_REST_PORT=8332
      - BITCOIN_REST_PATH=/rest/
    command: 'cargo run'

  postgres:
    image: 'postgres:12-alpine'
    ports:
      - 5432:5432
    volumes:
      - postgres:/var/lib/postgresql/data
    environment:
      - POSTGRES_USER=lightning-rgs
      - POSTGRES_PASSWORD=docker
      - POSTGRES_DB=ln_graph_sync

  bitcoin-core:
    container_name: bitcoin-core
    image: ruimarinho/bitcoin-core:alpine
    restart: always
    ports:
      - "0.0.0.0:8332:8332"
      - "0.0.0.0:8333:8333"
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - .bitcoin:/home/bitcoin/.bitcoin
    command:
      - "-printtoconsole"
      - "-rpcallowip=0.0.0.0/0"
      - "-rpcbind=0.0.0.0"
      - "-rpcuser=vault"
      - "-rpcpassword=vault"
      - "-rpcport=8332"
      - "-rest"

  # Comment Out Below for Testnet. You'll also need to change BITCOIN_REST_PORT to 18332
  # bitcoin-core:
  #   container_name: bitcoin-core
  #   image: ruimarinho/bitcoin-core:alpine
  #   restart: always
  #   ports:
  #     - "0.0.0.0:18332:18332"
  #     - "0.0.0.0:18332:18333"
  #   volumes:
  #     - /etc/localtime:/etc/localtime:ro
  #     - .bitcoin:/home/bitcoin/.bitcoin
  #   command:
  #     - "-printtoconsole"
  #     - "-testnet"
  #     - "-rpcallowip=0.0.0.0/0"
  #     - "-rpcbind=0.0.0.0"
  #     - "-rpcuser=vault"
  #     - "-rpcpassword=vault"
  #     - "-rpcport=18332"
  #     - "-rest"

volumes:
  postgres: null

'''
'''--- src/config.rs ---
use crate::hex_utils;

use std::convert::TryInto;
use std::env;
use std::io::Cursor;
use std::net::{SocketAddr, ToSocketAddrs};
use std::time::Duration;

use bitcoin::Network;
use bitcoin::hashes::hex::FromHex;
use bitcoin::secp256k1::PublicKey;
use futures::stream::{FuturesUnordered, StreamExt};
use lightning::ln::msgs::ChannelAnnouncement;
use lightning::util::ser::Readable;
use lightning_block_sync::http::HttpEndpoint;
use tokio_postgres::Config;

pub(crate) const SCHEMA_VERSION: i32 = 13;
pub(crate) const SYMLINK_GRANULARITY_INTERVAL: u32 = 3600 * 3; // three hours
pub(crate) const MAX_SNAPSHOT_SCOPE: u32 = 3600 * 24 * 21; // three weeks
// generate symlinks based on a 3-hour-granularity
/// If the last update in either direction was more than six days ago, we send a reminder
/// That reminder may be either in the form of a channel announcement, or in the form of empty
/// updates in both directions.
pub(crate) const CHANNEL_REMINDER_AGE: Duration = Duration::from_secs(6 * 24 * 60 * 60);
/// The number of successful peer connections to await prior to continuing to gossip storage.
/// The application will still work if the number of specified peers is lower, as long as there is
/// at least one successful peer connection, but it may result in long startup times.
pub(crate) const CONNECTED_PEER_ASSERTION_LIMIT: usize = 5;
pub(crate) const DOWNLOAD_NEW_GOSSIP: bool = true;

pub(crate) fn snapshot_generation_interval() -> u32 {
	let interval = env::var("RAPID_GOSSIP_SYNC_SERVER_SNAPSHOT_INTERVAL").unwrap_or(SYMLINK_GRANULARITY_INTERVAL.to_string())
		.parse::<u32>()
		.expect("RAPID_GOSSIP_SYNC_SERVER_SNAPSHOT_INTERVAL env variable must be a u32.");
	assert!(interval > 0, "RAPID_GOSSIP_SYNC_SERVER_SNAPSHOT_INTERVAL must be positive");
	assert_eq!(interval % SYMLINK_GRANULARITY_INTERVAL, 0, "RAPID_GOSSIP_SYNC_SERVER_SNAPSHOT_INTERVAL must be a multiple of {} (seconds)", SYMLINK_GRANULARITY_INTERVAL);
	interval
}

pub(crate) fn network() -> Network {
	let network = env::var("RAPID_GOSSIP_SYNC_SERVER_NETWORK").unwrap_or("bitcoin".to_string()).to_lowercase();
	match network.as_str() {
		"mainnet" => Network::Bitcoin,
		"bitcoin" => Network::Bitcoin,
		"testnet" => Network::Testnet,
		"signet" => Network::Signet,
		"regtest" => Network::Regtest,
		_ => panic!("Invalid network"),
	}
}

pub(crate) fn log_level() -> lightning::util::logger::Level {
	let level = env::var("RAPID_GOSSIP_SYNC_SERVER_LOG_LEVEL").unwrap_or("info".to_string()).to_lowercase();
	match level.as_str() {
		"gossip" => lightning::util::logger::Level::Gossip,
		"trace" => lightning::util::logger::Level::Trace,
		"debug" => lightning::util::logger::Level::Debug,
		"info" => lightning::util::logger::Level::Info,
		"warn" => lightning::util::logger::Level::Warn,
		"error" => lightning::util::logger::Level::Error,
		_ => panic!("Invalid log level"),
	}
}

pub(crate) fn network_graph_cache_path() -> String {
	format!("{}/network_graph.bin", cache_path())
}

pub(crate) fn cache_path() -> String {
	let path = env::var("RAPID_GOSSIP_SYNC_SERVER_CACHES_PATH").unwrap_or("./res".to_string()).to_lowercase();
	path
}

pub(crate) fn db_connection_config() -> Config {
	let mut config = Config::new();
	let env_name_prefix = if cfg!(test) {
		"RAPID_GOSSIP_TEST_DB"
	} else {
		"RAPID_GOSSIP_SYNC_SERVER_DB"
	};

	let host = env::var(format!("{}{}", env_name_prefix, "_HOST")).unwrap_or("localhost".to_string());
	let user = env::var(format!("{}{}", env_name_prefix, "_USER")).unwrap_or("alice".to_string());
	let db = env::var(format!("{}{}", env_name_prefix, "_NAME")).unwrap_or("ln_graph_sync".to_string());
	config.host(&host);
	config.user(&user);
	config.dbname(&db);
	if let Ok(password) = env::var(format!("{}{}", env_name_prefix, "_PASSWORD")) {
		config.password(&password);
	}
	config
}

pub(crate) fn bitcoin_rest_endpoint() -> HttpEndpoint {
	let host = env::var("BITCOIN_REST_DOMAIN").unwrap_or("127.0.0.1".to_string());
	let port = env::var("BITCOIN_REST_PORT")
		.unwrap_or("8332".to_string())
		.parse::<u16>()
		.expect("BITCOIN_REST_PORT env variable must be a u16.");
	let path = env::var("BITCOIN_REST_PATH").unwrap_or("/rest/".to_string());
	HttpEndpoint::for_host(host).with_port(port).with_path(path)
}

pub(crate) fn db_config_table_creation_query() -> &'static str {
	"CREATE TABLE IF NOT EXISTS config (
		id SERIAL PRIMARY KEY,
		db_schema integer
	)"
}

pub(crate) fn db_announcement_table_creation_query() -> &'static str {
	"CREATE TABLE IF NOT EXISTS channel_announcements (
		id SERIAL PRIMARY KEY,
		short_channel_id bigint NOT NULL UNIQUE,
		announcement_signed BYTEA,
		seen timestamp NOT NULL DEFAULT NOW()
	)"
}

pub(crate) fn db_channel_update_table_creation_query() -> &'static str {
	"CREATE TABLE IF NOT EXISTS channel_updates (
		id SERIAL PRIMARY KEY,
		short_channel_id bigint NOT NULL,
		timestamp bigint NOT NULL,
		channel_flags smallint NOT NULL,
		direction boolean NOT NULL,
		disable boolean NOT NULL,
		cltv_expiry_delta integer NOT NULL,
		htlc_minimum_msat bigint NOT NULL,
		fee_base_msat integer NOT NULL,
		fee_proportional_millionths integer NOT NULL,
		htlc_maximum_msat bigint NOT NULL,
		blob_signed BYTEA NOT NULL,
		seen timestamp NOT NULL DEFAULT NOW()
	)"
}

pub(crate) fn db_index_creation_query() -> &'static str {
	"
	CREATE INDEX IF NOT EXISTS channel_updates_seen_scid ON channel_updates(seen, short_channel_id);
	CREATE INDEX IF NOT EXISTS channel_updates_scid_dir_seen_asc ON channel_updates(short_channel_id, direction, seen);
	CREATE INDEX IF NOT EXISTS channel_updates_scid_dir_seen_desc_with_id ON channel_updates(short_channel_id ASC, direction ASC, seen DESC) INCLUDE (id);
	CREATE UNIQUE INDEX IF NOT EXISTS channel_updates_key ON channel_updates (short_channel_id, direction, timestamp);
	CREATE INDEX IF NOT EXISTS channel_updates_seen ON channel_updates(seen);
	CREATE INDEX IF NOT EXISTS channel_updates_scid_asc_timestamp_desc ON channel_updates(short_channel_id ASC, timestamp DESC);
	"
}

pub(crate) async fn upgrade_db(schema: i32, client: &mut tokio_postgres::Client) {
	if schema == 1 {
		let tx = client.transaction().await.unwrap();
		tx.execute("ALTER TABLE channel_updates DROP COLUMN chain_hash", &[]).await.unwrap();
		tx.execute("ALTER TABLE channel_announcements DROP COLUMN chain_hash", &[]).await.unwrap();
		tx.execute("UPDATE config SET db_schema = 2 WHERE id = 1", &[]).await.unwrap();
		tx.commit().await.unwrap();
	}
	if schema == 1 || schema == 2 {
		let tx = client.transaction().await.unwrap();
		tx.execute("ALTER TABLE channel_updates DROP COLUMN short_channel_id", &[]).await.unwrap();
		tx.execute("ALTER TABLE channel_updates ADD COLUMN short_channel_id bigint DEFAULT null", &[]).await.unwrap();
		tx.execute("ALTER TABLE channel_updates DROP COLUMN direction", &[]).await.unwrap();
		tx.execute("ALTER TABLE channel_updates ADD COLUMN direction boolean DEFAULT null", &[]).await.unwrap();
		loop {
			let rows = tx.query("SELECT id, composite_index FROM channel_updates WHERE short_channel_id IS NULL LIMIT 50000", &[]).await.unwrap();
			if rows.is_empty() { break; }
			let mut updates = FuturesUnordered::new();
			for row in rows {
				let id: i32 = row.get("id");
				let index: String = row.get("composite_index");
				let tx_ref = &tx;
				updates.push(async move {
					let mut index_iter = index.split(":");
					let scid_hex = index_iter.next().unwrap();
					index_iter.next().unwrap();
					let direction_str = index_iter.next().unwrap();
					assert!(direction_str == "1" || direction_str == "0");
					let direction = direction_str == "1";
					let scid_be_bytes = hex_utils::to_vec(scid_hex).unwrap();
					let scid = i64::from_be_bytes(scid_be_bytes.try_into().unwrap());
					assert!(scid > 0); // Will roll over in some 150 years or so
					tx_ref.execute("UPDATE channel_updates SET short_channel_id = $1, direction = $2 WHERE id = $3", &[&scid, &direction, &id]).await.unwrap();
				});
			}
			while let Some(_) = updates.next().await {}
		}
		tx.execute("ALTER TABLE channel_updates ALTER short_channel_id DROP DEFAULT", &[]).await.unwrap();
		tx.execute("ALTER TABLE channel_updates ALTER short_channel_id SET NOT NULL", &[]).await.unwrap();
		tx.execute("ALTER TABLE channel_updates ALTER direction DROP DEFAULT", &[]).await.unwrap();
		tx.execute("ALTER TABLE channel_updates ALTER direction SET NOT NULL", &[]).await.unwrap();
		tx.execute("UPDATE config SET db_schema = 3 WHERE id = 1", &[]).await.unwrap();
		tx.commit().await.unwrap();
	}
	if schema >= 1 && schema <= 3 {
		let tx = client.transaction().await.unwrap();
		tx.execute("ALTER TABLE channel_announcements DROP COLUMN short_channel_id", &[]).await.unwrap();
		tx.execute("ALTER TABLE channel_announcements ADD COLUMN short_channel_id bigint DEFAULT null", &[]).await.unwrap();
		loop {
			let rows = tx.query("SELECT id, announcement_signed FROM channel_announcements WHERE short_channel_id IS NULL LIMIT 10000", &[]).await.unwrap();
			if rows.is_empty() { break; }
			let mut updates = FuturesUnordered::new();
			for row in rows {
				let id: i32 = row.get("id");
				let announcement: Vec<u8> = row.get("announcement_signed");
				let tx_ref = &tx;
				updates.push(async move {
					let scid = ChannelAnnouncement::read(&mut Cursor::new(announcement)).unwrap().contents.short_channel_id as i64;
					assert!(scid > 0); // Will roll over in some 150 years or so
					tx_ref.execute("UPDATE channel_announcements SET short_channel_id = $1 WHERE id = $2", &[&scid, &id]).await.unwrap();
				});
			}
			while let Some(_) = updates.next().await {}
		}
		tx.execute("ALTER TABLE channel_announcements ADD CONSTRAINT channel_announcements_short_channel_id_key UNIQUE (short_channel_id)", &[]).await.unwrap();
		tx.execute("ALTER TABLE channel_announcements ALTER short_channel_id DROP DEFAULT", &[]).await.unwrap();
		tx.execute("ALTER TABLE channel_announcements ALTER short_channel_id SET NOT NULL", &[]).await.unwrap();
		tx.execute("UPDATE config SET db_schema = 4 WHERE id = 1", &[]).await.unwrap();
		tx.commit().await.unwrap();
	}
	if schema >= 1 && schema <= 4 {
		let tx = client.transaction().await.unwrap();
		tx.execute("ALTER TABLE channel_updates ALTER composite_index SET DATA TYPE character(29)", &[]).await.unwrap();
		tx.execute("UPDATE config SET db_schema = 5 WHERE id = 1", &[]).await.unwrap();
		tx.commit().await.unwrap();
	}
	if schema >= 1 && schema <= 5 {
		let tx = client.transaction().await.unwrap();
		tx.execute("ALTER TABLE channel_updates ALTER channel_flags SET DATA TYPE smallint", &[]).await.unwrap();
		tx.execute("ALTER TABLE channel_announcements DROP COLUMN block_height", &[]).await.unwrap();
		tx.execute("UPDATE config SET db_schema = 6 WHERE id = 1", &[]).await.unwrap();
		tx.commit().await.unwrap();
	}
	if schema >= 1 && schema <= 6 {
		let tx = client.transaction().await.unwrap();
		tx.execute("ALTER TABLE channel_updates DROP COLUMN composite_index", &[]).await.unwrap();
		tx.execute("ALTER TABLE channel_updates ALTER timestamp SET NOT NULL", &[]).await.unwrap();
		tx.execute("ALTER TABLE channel_updates ALTER channel_flags SET NOT NULL", &[]).await.unwrap();
		tx.execute("ALTER TABLE channel_updates ALTER disable SET NOT NULL", &[]).await.unwrap();
		tx.execute("ALTER TABLE channel_updates ALTER cltv_expiry_delta SET NOT NULL", &[]).await.unwrap();
		tx.execute("ALTER TABLE channel_updates ALTER htlc_minimum_msat SET NOT NULL", &[]).await.unwrap();
		tx.execute("ALTER TABLE channel_updates ALTER fee_base_msat SET NOT NULL", &[]).await.unwrap();
		tx.execute("ALTER TABLE channel_updates ALTER fee_proportional_millionths SET NOT NULL", &[]).await.unwrap();
		tx.execute("ALTER TABLE channel_updates ALTER htlc_maximum_msat SET NOT NULL", &[]).await.unwrap();
		tx.execute("ALTER TABLE channel_updates ALTER blob_signed SET NOT NULL", &[]).await.unwrap();
		tx.execute("CREATE UNIQUE INDEX channel_updates_key ON channel_updates (short_channel_id, direction, timestamp)", &[]).await.unwrap();
		tx.execute("UPDATE config SET db_schema = 7 WHERE id = 1", &[]).await.unwrap();
		tx.commit().await.unwrap();
	}
	if schema >= 1 && schema <= 7 {
		let tx = client.transaction().await.unwrap();
		tx.execute("DROP INDEX IF EXISTS channels_seen", &[]).await.unwrap();
		tx.execute("DROP INDEX IF EXISTS channel_updates_scid", &[]).await.unwrap();
		tx.execute("DROP INDEX IF EXISTS channel_updates_direction", &[]).await.unwrap();
		tx.execute("DROP INDEX IF EXISTS channel_updates_seen", &[]).await.unwrap();
		tx.execute("DROP INDEX IF EXISTS channel_updates_scid_seen", &[]).await.unwrap();
		tx.execute("DROP INDEX IF EXISTS channel_updates_scid_dir_seen", &[]).await.unwrap();
		tx.execute("UPDATE config SET db_schema = 8 WHERE id = 1", &[]).await.unwrap();
		tx.commit().await.unwrap();
	}
	if schema >= 1 && schema <= 8 {
		let tx = client.transaction().await.unwrap();
		tx.execute("DROP INDEX IF EXISTS channel_updates_seen", &[]).await.unwrap();
		tx.execute("DROP INDEX IF EXISTS channel_updates_scid_seen", &[]).await.unwrap();
		tx.execute("UPDATE config SET db_schema = 9 WHERE id = 1", &[]).await.unwrap();
		tx.commit().await.unwrap();
	}
	if schema >= 1 && schema <= 9 {
		let tx = client.transaction().await.unwrap();
		tx.execute("DROP INDEX IF EXISTS channel_updates_scid_dir_seen", &[]).await.unwrap();
		tx.execute("UPDATE config SET db_schema = 10 WHERE id = 1", &[]).await.unwrap();
		tx.commit().await.unwrap();
	}
	if schema >= 1 && schema <= 10 {
		let tx = client.transaction().await.unwrap();
		tx.execute("DROP INDEX IF EXISTS channel_updates_id_with_scid_dir_blob", &[]).await.unwrap();
		tx.execute("UPDATE config SET db_schema = 11 WHERE id = 1", &[]).await.unwrap();
		tx.commit().await.unwrap();
	}
	if schema >= 1 && schema <= 11 {
		let tx = client.transaction().await.unwrap();
		tx.execute("DROP INDEX IF EXISTS channel_updates_seen_with_id_direction_blob", &[]).await.unwrap();
		tx.execute("UPDATE config SET db_schema = 12 WHERE id = 1", &[]).await.unwrap();
		tx.commit().await.unwrap();
	}
	if schema >= 1 && schema <= 12 {
		let tx = client.transaction().await.unwrap();
		tx.execute("DROP INDEX IF EXISTS channel_updates_timestamp_desc", &[]).await.unwrap();
		tx.execute("UPDATE config SET db_schema = 13 WHERE id = 1", &[]).await.unwrap();
		tx.commit().await.unwrap();
	}
	if schema <= 1 || schema > SCHEMA_VERSION {
		panic!("Unknown schema in db: {}, we support up to {}", schema, SCHEMA_VERSION);
	}
	// PostgreSQL (at least v13, but likely later versions as well) handles insert-only tables
	// *very* poorly. After some number of inserts, it refuses to rely on indexes, assuming them to
	// be possibly-stale, until a VACUUM happens. Thus, we set the vacuum factor really low here,
	// pushing PostgreSQL to vacuum often.
	// See https://www.cybertec-postgresql.com/en/postgresql-autovacuum-insert-only-tables/
	let _ = client.execute("ALTER TABLE channel_updates SET ( autovacuum_vacuum_insert_scale_factor = 0.005 );", &[]).await;
	let _ = client.execute("ALTER TABLE channel_announcements SET ( autovacuum_vacuum_insert_scale_factor = 0.005 );", &[]).await;
}

pub(crate) fn ln_peers() -> Vec<(PublicKey, SocketAddr)> {
	const WALLET_OF_SATOSHI: &str = "035e4ff418fc8b5554c5d9eea66396c227bd429a3251c8cbc711002ba215bfc226@170.75.163.209:9735";
	let list = env::var("LN_PEERS").unwrap_or(WALLET_OF_SATOSHI.to_string());
	let mut peers = Vec::new();
	for peer_info in list.split(',') {
		peers.push(resolve_peer_info(peer_info).expect("Invalid peer info in LN_PEERS"));
	}
	peers
}

fn resolve_peer_info(peer_info: &str) -> Result<(PublicKey, SocketAddr), &str> {
	let mut peer_info = peer_info.splitn(2, '@');

	let pubkey = peer_info.next().ok_or("Invalid peer info. Should be formatted as: `pubkey@host:port`")?;
	let pubkey = Vec::from_hex(pubkey).map_err(|_| "Invalid node pubkey")?;
	let pubkey = PublicKey::from_slice(&pubkey).map_err(|_| "Invalid node pubkey")?;

	let socket_address = peer_info.next().ok_or("Invalid peer info. Should be formatted as: `pubkey@host:port`")?;
	let socket_address = socket_address
		.to_socket_addrs()
		.map_err(|_| "Cannot resolve node address")?
		.next()
		.ok_or("Cannot resolve node address")?;

	Ok((pubkey, socket_address))
}

#[cfg(test)]
mod tests {
	use super::resolve_peer_info;
	use bitcoin::hashes::hex::ToHex;

	#[test]
	fn test_resolve_peer_info() {
		let wallet_of_satoshi = "035e4ff418fc8b5554c5d9eea66396c227bd429a3251c8cbc711002ba215bfc226@170.75.163.209:9735";
		let (pubkey, socket_address) = resolve_peer_info(wallet_of_satoshi).unwrap();
		assert_eq!(pubkey.serialize().to_hex(), "035e4ff418fc8b5554c5d9eea66396c227bd429a3251c8cbc711002ba215bfc226");
		assert_eq!(socket_address.to_string(), "170.75.163.209:9735");

		let ipv6 = "033d8656219478701227199cbd6f670335c8d408a92ae88b962c49d4dc0e83e025@[2001:db8::1]:80";
		let (pubkey, socket_address) = resolve_peer_info(ipv6).unwrap();
		assert_eq!(pubkey.serialize().to_hex(), "033d8656219478701227199cbd6f670335c8d408a92ae88b962c49d4dc0e83e025");
		assert_eq!(socket_address.to_string(), "[2001:db8::1]:80");

		let localhost = "033d8656219478701227199cbd6f670335c8d408a92ae88b962c49d4dc0e83e025@localhost:9735";
		let (pubkey, socket_address) = resolve_peer_info(localhost).unwrap();
		assert_eq!(pubkey.serialize().to_hex(), "033d8656219478701227199cbd6f670335c8d408a92ae88b962c49d4dc0e83e025");
		let socket_address = socket_address.to_string();
		assert!(socket_address == "127.0.0.1:9735" || socket_address == "[::1]:9735");
	}
}

'''
'''--- src/downloader.rs ---
use std::ops::Deref;
use std::sync::{Arc, RwLock};

use bitcoin::secp256k1::PublicKey;
use lightning::events::{MessageSendEvent, MessageSendEventsProvider};
use lightning::ln::features::{InitFeatures, NodeFeatures};
use lightning::ln::msgs::{ChannelAnnouncement, ChannelUpdate, Init, LightningError, NodeAnnouncement, QueryChannelRange, QueryShortChannelIds, ReplyChannelRange, ReplyShortChannelIdsEnd, RoutingMessageHandler};
use lightning::routing::gossip::{NetworkGraph, NodeId, P2PGossipSync};
use lightning::util::logger::Logger;
use tokio::sync::mpsc;
use tokio::sync::mpsc::error::TrySendError;

use crate::types::{GossipMessage, GossipChainAccess, GossipPeerManager};
use crate::verifier::ChainVerifier;

pub(crate) struct GossipCounter {
	pub(crate) channel_announcements: u64,
	pub(crate) channel_updates: u64,
	pub(crate) channel_updates_without_htlc_max_msats: u64,
	pub(crate) channel_announcements_with_mismatched_scripts: u64
}

impl GossipCounter {
	pub(crate) fn new() -> Self {
		Self {
			channel_announcements: 0,
			channel_updates: 0,
			channel_updates_without_htlc_max_msats: 0,
			channel_announcements_with_mismatched_scripts: 0,
		}
	}
}

pub(crate) struct GossipRouter<L: Deref + Clone + Send + Sync + 'static> where L::Target: Logger {
	native_router: P2PGossipSync<Arc<NetworkGraph<L>>, GossipChainAccess<L>, L>,
	pub(crate) counter: RwLock<GossipCounter>,
	sender: mpsc::Sender<GossipMessage>,
	verifier: Arc<ChainVerifier<L>>,
	outbound_gossiper: Arc<P2PGossipSync<Arc<NetworkGraph<L>>, GossipChainAccess<L>, L>>,
}

impl<L: Deref + Clone + Send + Sync> GossipRouter<L> where L::Target: Logger {
	pub(crate) fn new(network_graph: Arc<NetworkGraph<L>>, sender: mpsc::Sender<GossipMessage>, logger: L) -> Self {
		let outbound_gossiper = Arc::new(P2PGossipSync::new(Arc::clone(&network_graph), None, logger.clone()));
		let verifier = Arc::new(ChainVerifier::new(Arc::clone(&network_graph), Arc::clone(&outbound_gossiper), logger.clone()));
		Self {
			native_router: P2PGossipSync::new(network_graph, Some(Arc::clone(&verifier)), logger.clone()),
			outbound_gossiper,
			counter: RwLock::new(GossipCounter::new()),
			sender,
			verifier
		}
	}

	pub(crate) fn set_pm(&self, peer_handler: GossipPeerManager<L>) {
		self.verifier.set_ph(peer_handler);
	}

	fn new_channel_announcement(&self, msg: ChannelAnnouncement) {
		{
			let mut counter = self.counter.write().unwrap();
			counter.channel_announcements += 1;
		}

		let gossip_message = GossipMessage::ChannelAnnouncement(msg, None);
		if let Err(err) = self.sender.try_send(gossip_message) {
			let gossip_message = match err { TrySendError::Full(msg)|TrySendError::Closed(msg) => msg };
			tokio::task::block_in_place(move || { tokio::runtime::Handle::current().block_on(async move {
				self.sender.send(gossip_message).await.unwrap();
			})});
		}
	}

	fn new_channel_update(&self, msg: ChannelUpdate) {
		self.counter.write().unwrap().channel_updates += 1;
		let gossip_message = GossipMessage::ChannelUpdate(msg, None);

		if let Err(err) = self.sender.try_send(gossip_message) {
			let gossip_message = match err { TrySendError::Full(msg)|TrySendError::Closed(msg) => msg };
			tokio::task::block_in_place(move || { tokio::runtime::Handle::current().block_on(async move {
				self.sender.send(gossip_message).await.unwrap();
			})});
		}
	}
}

impl<L: Deref + Clone + Send + Sync> MessageSendEventsProvider for GossipRouter<L> where L::Target: Logger {
	fn get_and_clear_pending_msg_events(&self) -> Vec<MessageSendEvent> {
		let gossip_evs = self.outbound_gossiper.get_and_clear_pending_msg_events();
		for ev in gossip_evs {
			match ev {
				MessageSendEvent::BroadcastChannelAnnouncement { msg, .. } => {
					self.new_channel_announcement(msg);
				},
				MessageSendEvent::BroadcastNodeAnnouncement { .. } => {},
				MessageSendEvent::BroadcastChannelUpdate { msg } => {
					self.new_channel_update(msg);
				},
				_ => { unreachable!() },
			}
		}
		self.native_router.get_and_clear_pending_msg_events()
	}
}

impl<L: Deref + Clone + Send + Sync> RoutingMessageHandler for GossipRouter<L> where L::Target: Logger {
	fn handle_node_announcement(&self, msg: &NodeAnnouncement) -> Result<bool, LightningError> {
		self.native_router.handle_node_announcement(msg)
	}

	fn handle_channel_announcement(&self, msg: &ChannelAnnouncement) -> Result<bool, LightningError> {
		let res = self.native_router.handle_channel_announcement(msg)?;
		self.new_channel_announcement(msg.clone());
		Ok(res)
	}

	fn handle_channel_update(&self, msg: &ChannelUpdate) -> Result<bool, LightningError> {
		let res = self.native_router.handle_channel_update(msg)?;
		self.new_channel_update(msg.clone());
		Ok(res)
	}

	fn processing_queue_high(&self) -> bool {
		self.native_router.processing_queue_high()
	}

	fn get_next_channel_announcement(&self, starting_point: u64) -> Option<(ChannelAnnouncement, Option<ChannelUpdate>, Option<ChannelUpdate>)> {
		self.native_router.get_next_channel_announcement(starting_point)
	}

	fn get_next_node_announcement(&self, starting_point: Option<&NodeId>) -> Option<NodeAnnouncement> {
		self.native_router.get_next_node_announcement(starting_point)
	}

	fn peer_connected(&self, their_node_id: &PublicKey, init: &Init, inbound: bool) -> Result<(), ()> {
		self.native_router.peer_connected(their_node_id, init, inbound)
	}

	fn handle_reply_channel_range(&self, their_node_id: &PublicKey, msg: ReplyChannelRange) -> Result<(), LightningError> {
		self.native_router.handle_reply_channel_range(their_node_id, msg)
	}

	fn handle_reply_short_channel_ids_end(&self, their_node_id: &PublicKey, msg: ReplyShortChannelIdsEnd) -> Result<(), LightningError> {
		self.native_router.handle_reply_short_channel_ids_end(their_node_id, msg)
	}

	fn handle_query_channel_range(&self, their_node_id: &PublicKey, msg: QueryChannelRange) -> Result<(), LightningError> {
		self.native_router.handle_query_channel_range(their_node_id, msg)
	}

	fn handle_query_short_channel_ids(&self, their_node_id: &PublicKey, msg: QueryShortChannelIds) -> Result<(), LightningError> {
		self.native_router.handle_query_short_channel_ids(their_node_id, msg)
	}

	fn provided_init_features(&self, their_node_id: &PublicKey) -> InitFeatures {
		self.native_router.provided_init_features(their_node_id)
	}

	fn provided_node_features(&self) -> NodeFeatures {
		self.native_router.provided_node_features()
	}
}

'''
'''--- src/hex_utils.rs ---
pub fn to_vec(hex: &str) -> Option<Vec<u8>> {
    let mut out = Vec::with_capacity(hex.len() / 2);

    let mut b = 0;
    for (idx, c) in hex.as_bytes().iter().enumerate() {
        b <<= 4;
        match *c {
            b'A'..=b'F' => b |= c - b'A' + 10,
            b'a'..=b'f' => b |= c - b'a' + 10,
            b'0'..=b'9' => b |= c - b'0',
            _ => return None,
        }
        if (idx & 1) == 1 {
            out.push(b);
            b = 0;
        }
    }

    Some(out)
}

'''
'''--- src/lib.rs ---
#![deny(unsafe_code)]
#![deny(broken_intra_doc_links)]
#![deny(private_intra_doc_links)]
#![deny(non_upper_case_globals)]
#![deny(non_camel_case_types)]
#![deny(non_snake_case)]
#![deny(unused_variables)]
#![deny(unused_imports)]

extern crate core;

use std::collections::{HashMap, HashSet};
use std::fs::File;
use std::io::BufReader;
use std::ops::Deref;
use std::sync::Arc;
use lightning::log_info;

use lightning::routing::gossip::{NetworkGraph, NodeId};
use lightning::util::logger::Logger;
use lightning::util::ser::{ReadableArgs, Writeable};
use tokio::sync::mpsc;
use tokio_postgres::{Client, NoTls};
use crate::config::SYMLINK_GRANULARITY_INTERVAL;
use crate::lookup::DeltaSet;

use crate::persistence::GossipPersister;
use crate::serialization::UpdateSerialization;
use crate::snapshot::Snapshotter;
use crate::types::RGSSLogger;

mod downloader;
mod tracking;
mod lookup;
mod persistence;
mod serialization;
mod snapshot;
mod config;
mod hex_utils;
mod verifier;

pub mod types;

#[cfg(test)]
mod tests;

/// The purpose of this prefix is to identify the serialization format, should other rapid gossip
/// sync formats arise in the future.
///
/// The fourth byte is the protocol version in case our format gets updated.
const GOSSIP_PREFIX: [u8; 4] = [76, 68, 75, 1];

pub struct RapidSyncProcessor<L: Deref> where L::Target: Logger {
	network_graph: Arc<NetworkGraph<L>>,
	logger: L
}

pub struct SerializedResponse {
	pub data: Vec<u8>,
	pub message_count: u32,
	pub announcement_count: u32,
	pub update_count: u32,
	pub update_count_full: u32,
	pub update_count_incremental: u32,
}

impl<L: Deref + Clone + Send + Sync + 'static> RapidSyncProcessor<L> where L::Target: Logger {
	pub fn new(logger: L) -> Self {
		let network = config::network();
		let network_graph = if let Ok(file) = File::open(&config::network_graph_cache_path()) {
			log_info!(logger, "Initializing from cached network graph…");
			let mut buffered_reader = BufReader::new(file);
			let network_graph_result = NetworkGraph::read(&mut buffered_reader, logger.clone());
			if let Ok(network_graph) = network_graph_result {
				log_info!(logger, "Initialized from cached network graph!");
				network_graph
			} else {
				log_info!(logger, "Initialization from cached network graph failed: {}", network_graph_result.err().unwrap());
				NetworkGraph::new(network, logger.clone())
			}
		} else {
			NetworkGraph::new(network, logger.clone())
		};
		let arc_network_graph = Arc::new(network_graph);
		Self {
			network_graph: arc_network_graph,
			logger
		}
	}

	pub async fn start_sync(&self) {
		log_info!(self.logger, "Starting Rapid Gossip Sync Server");
		log_info!(self.logger, "Snapshot interval: {} seconds", config::snapshot_generation_interval());

		// means to indicate sync completion status within this module
		let (sync_completion_sender, mut sync_completion_receiver) = mpsc::channel::<()>(1);

		if config::DOWNLOAD_NEW_GOSSIP {
			let (mut persister, persistence_sender) = GossipPersister::new(self.network_graph.clone(), self.logger.clone());

			log_info!(self.logger, "Starting gossip download");
			tokio::spawn(tracking::download_gossip(persistence_sender, sync_completion_sender,
				Arc::clone(&self.network_graph), self.logger.clone()));
			log_info!(self.logger, "Starting gossip db persistence listener");
			tokio::spawn(async move { persister.persist_gossip().await; });
		} else {
			sync_completion_sender.send(()).await.unwrap();
		}

		let sync_completion = sync_completion_receiver.recv().await;
		if sync_completion.is_none() {
			panic!("Sync failed!");
		}
		log_info!(self.logger, "Initial sync complete!");

		// start the gossip snapshotting service
		Snapshotter::new(Arc::clone(&self.network_graph), self.logger.clone()).snapshot_gossip().await;
	}
}

pub(crate) async fn connect_to_db() -> Client {
	let connection_config = config::db_connection_config();
	let (client, connection) = connection_config.connect(NoTls).await.unwrap();

	tokio::spawn(async move {
		if let Err(e) = connection.await {
			panic!("connection error: {}", e);
		}
	});

	#[cfg(test)]
	{
		let schema_name = tests::db_test_schema();
		let schema_creation_command = format!("CREATE SCHEMA IF NOT EXISTS {}", schema_name);
		client.execute(&schema_creation_command, &[]).await.unwrap();
		client.execute(&format!("SET search_path TO {}", schema_name), &[]).await.unwrap();
	}

	client.execute("set time zone UTC", &[]).await.unwrap();
	client
}

/// This method generates a no-op blob that can be used as a delta where none exists.
///
/// The primary purpose of this method is the scenario of a client retrieving and processing a
/// given snapshot, and then immediately retrieving the would-be next snapshot at the timestamp
/// indicated by the one that was just processed.
/// Previously, there would not be a new snapshot to be processed for that particular timestamp yet,
/// and the server would return a 404 error.
///
/// In principle, this method could also be used to address another unfortunately all too common
/// pitfall: requesting snapshots from intermediate timestamps, i. e. those that are not multiples
/// of our granularity constant. Note that for that purpose, this method could be very dangerous,
/// because if consumed, the `timestamp` value calculated here will overwrite the timestamp that
/// the client previously had, which could result in duplicated or omitted gossip down the line.
fn serialize_empty_blob(current_timestamp: u64) -> Vec<u8> {
	let mut blob = GOSSIP_PREFIX.to_vec();

	let network = config::network();
	let genesis_block = bitcoin::blockdata::constants::genesis_block(network);
	let chain_hash = genesis_block.block_hash();
	chain_hash.write(&mut blob).unwrap();

	let blob_timestamp = Snapshotter::<Arc<RGSSLogger>>::round_down_to_nearest_multiple(current_timestamp, SYMLINK_GRANULARITY_INTERVAL as u64) as u32;
	blob_timestamp.write(&mut blob).unwrap();

	0u32.write(&mut blob).unwrap(); // node count
	0u32.write(&mut blob).unwrap(); // announcement count
	0u32.write(&mut blob).unwrap(); // update count

	blob
}

async fn serialize_delta<L: Deref + Clone>(network_graph: Arc<NetworkGraph<L>>, last_sync_timestamp: u32, logger: L) -> SerializedResponse where L::Target: Logger {
	let client = connect_to_db().await;

	network_graph.remove_stale_channels_and_tracking();

	let mut output: Vec<u8> = vec![];
	let snapshot_interval = config::snapshot_generation_interval();

	// set a flag if the chain hash is prepended
	// chain hash only necessary if either channel announcements or non-incremental updates are present
	// for announcement-free incremental-only updates, chain hash can be skipped

	let mut node_id_set: HashSet<NodeId> = HashSet::new();
	let mut node_id_indices: HashMap<NodeId, usize> = HashMap::new();
	let mut node_ids: Vec<NodeId> = Vec::new();
	let mut duplicate_node_ids: i32 = 0;

	let mut get_node_id_index = |node_id: NodeId| {
		if node_id_set.insert(node_id) {
			node_ids.push(node_id);
			let index = node_ids.len() - 1;
			node_id_indices.insert(node_id, index);
			return index;
		}
		duplicate_node_ids += 1;
		node_id_indices[&node_id]
	};

	let mut delta_set = DeltaSet::new();
	lookup::fetch_channel_announcements(&mut delta_set, network_graph, &client, last_sync_timestamp, logger.clone()).await;
	log_info!(logger, "announcement channel count: {}", delta_set.len());
	lookup::fetch_channel_updates(&mut delta_set, &client, last_sync_timestamp, logger.clone()).await;
	log_info!(logger, "update-fetched channel count: {}", delta_set.len());
	lookup::filter_delta_set(&mut delta_set, logger.clone());
	log_info!(logger, "update-filtered channel count: {}", delta_set.len());
	let serialization_details = serialization::serialize_delta_set(delta_set, last_sync_timestamp);

	// process announcements
	// write the number of channel announcements to the output
	let announcement_count = serialization_details.announcements.len() as u32;
	announcement_count.write(&mut output).unwrap();
	let mut previous_announcement_scid = 0;
	for current_announcement in serialization_details.announcements {
		let id_index_1 = get_node_id_index(current_announcement.node_id_1);
		let id_index_2 = get_node_id_index(current_announcement.node_id_2);
		let mut stripped_announcement = serialization::serialize_stripped_channel_announcement(&current_announcement, id_index_1, id_index_2, previous_announcement_scid);
		output.append(&mut stripped_announcement);

		previous_announcement_scid = current_announcement.short_channel_id;
	}

	// process updates
	let mut previous_update_scid = 0;
	let update_count = serialization_details.updates.len() as u32;
	update_count.write(&mut output).unwrap();

	let default_update_values = serialization_details.full_update_defaults;
	if update_count > 0 {
		default_update_values.cltv_expiry_delta.write(&mut output).unwrap();
		default_update_values.htlc_minimum_msat.write(&mut output).unwrap();
		default_update_values.fee_base_msat.write(&mut output).unwrap();
		default_update_values.fee_proportional_millionths.write(&mut output).unwrap();
		default_update_values.htlc_maximum_msat.write(&mut output).unwrap();
	}

	let mut update_count_full = 0;
	let mut update_count_incremental = 0;
	for current_update in serialization_details.updates {
		match &current_update {
			UpdateSerialization::Full(_) => {
				update_count_full += 1;
			}
			UpdateSerialization::Incremental(_, _) | UpdateSerialization::Reminder(_, _) => {
				update_count_incremental += 1;
			}
		};

		let mut stripped_update = serialization::serialize_stripped_channel_update(&current_update, &default_update_values, previous_update_scid);
		output.append(&mut stripped_update);

		previous_update_scid = current_update.scid();
	}

	// some stats
	let message_count = announcement_count + update_count;

	let mut prefixed_output = GOSSIP_PREFIX.to_vec();

	// always write the chain hash
	serialization_details.chain_hash.write(&mut prefixed_output).unwrap();
	// always write the latest seen timestamp
	let latest_seen_timestamp = serialization_details.latest_seen;
	let overflow_seconds = latest_seen_timestamp % snapshot_interval;
	let serialized_seen_timestamp = latest_seen_timestamp.saturating_sub(overflow_seconds);
	serialized_seen_timestamp.write(&mut prefixed_output).unwrap();

	let node_id_count = node_ids.len() as u32;
	node_id_count.write(&mut prefixed_output).unwrap();

	for current_node_id in node_ids {
		current_node_id.write(&mut prefixed_output).unwrap();
	}

	prefixed_output.append(&mut output);

	log_info!(logger, "duplicated node ids: {}", duplicate_node_ids);
	log_info!(logger, "latest seen timestamp: {:?}", serialization_details.latest_seen);

	SerializedResponse {
		data: prefixed_output,
		message_count,
		announcement_count,
		update_count,
		update_count_full,
		update_count_incremental,
	}
}

'''
'''--- src/lookup.rs ---
use std::collections::{BTreeMap, HashSet};
use std::io::Cursor;
use std::ops::Deref;
use std::sync::Arc;
use std::time::{Instant, SystemTime, UNIX_EPOCH};

use lightning::ln::msgs::{ChannelAnnouncement, ChannelUpdate, UnsignedChannelAnnouncement, UnsignedChannelUpdate};
use lightning::routing::gossip::NetworkGraph;
use lightning::util::ser::Readable;
use tokio_postgres::Client;

use futures::StreamExt;
use lightning::{log_gossip, log_info};
use lightning::util::logger::Logger;

use crate::config;
use crate::serialization::MutatedProperties;

/// The delta set needs to be a BTreeMap so the keys are sorted.
/// That way, the scids in the response automatically grow monotonically
pub(super) type DeltaSet = BTreeMap<u64, ChannelDelta>;

pub(super) struct AnnouncementDelta {
	pub(super) seen: u32,
	pub(super) announcement: UnsignedChannelAnnouncement,
}

pub(super) struct UpdateDelta {
	pub(super) seen: u32,
	pub(super) update: UnsignedChannelUpdate,
}

pub(super) struct DirectedUpdateDelta {
	pub(super) last_update_before_seen: Option<UpdateDelta>,
	pub(super) mutated_properties: MutatedProperties,
	pub(super) latest_update_after_seen: Option<UpdateDelta>,
	pub(super) serialization_update_flags: Option<u8>,
}

pub(super) struct ChannelDelta {
	pub(super) announcement: Option<AnnouncementDelta>,
	pub(super) updates: (Option<DirectedUpdateDelta>, Option<DirectedUpdateDelta>),
	pub(super) first_bidirectional_updates_seen: Option<u32>,
	/// The seen timestamp of the older of the two latest directional updates
	pub(super) requires_reminder: bool,
}

impl Default for ChannelDelta {
	fn default() -> Self {
		Self {
			announcement: None,
			updates: (None, None),
			first_bidirectional_updates_seen: None,
			requires_reminder: false,
		}
	}
}

impl Default for DirectedUpdateDelta {
	fn default() -> Self {
		Self {
			last_update_before_seen: None,
			mutated_properties: MutatedProperties::default(),
			latest_update_after_seen: None,
			serialization_update_flags: None,
		}
	}
}

/// Fetch all the channel announcements that are presently in the network graph, regardless of
/// whether they had been seen before.
/// Also include all announcements for which the first update was announced
/// after `last_sync_timestamp`
pub(super) async fn fetch_channel_announcements<L: Deref>(delta_set: &mut DeltaSet, network_graph: Arc<NetworkGraph<L>>, client: &Client, last_sync_timestamp: u32, logger: L) where L::Target: Logger {
	log_info!(logger, "Obtaining channel ids from network graph");
	let channel_ids = {
		let read_only_graph = network_graph.read_only();
		log_info!(logger, "Retrieved read-only network graph copy");
		let channel_iterator = read_only_graph.channels().unordered_iter();
		channel_iterator
			.filter(|c| c.1.announcement_message.is_some())
			.map(|c| c.1.announcement_message.as_ref().unwrap().contents.short_channel_id as i64)
			.collect::<Vec<_>>()
	};
	#[cfg(test)]
	log_info!(logger, "Channel IDs: {:?}", channel_ids);
	log_info!(logger, "Last sync timestamp: {}", last_sync_timestamp);
	let last_sync_timestamp_float = last_sync_timestamp as f64;

	log_info!(logger, "Obtaining corresponding database entries");
	// get all the channel announcements that are currently in the network graph
	let announcement_rows = client.query_raw("SELECT announcement_signed, CAST(EXTRACT('epoch' from seen) AS BIGINT) AS seen FROM channel_announcements WHERE short_channel_id = any($1) ORDER BY short_channel_id ASC", [&channel_ids]).await.unwrap();
	let mut pinned_rows = Box::pin(announcement_rows);

	let mut announcement_count = 0;
	while let Some(row_res) = pinned_rows.next().await {
		let current_announcement_row = row_res.unwrap();
		let blob: Vec<u8> = current_announcement_row.get("announcement_signed");
		let mut readable = Cursor::new(blob);
		let unsigned_announcement = ChannelAnnouncement::read(&mut readable).unwrap().contents;

		let scid = unsigned_announcement.short_channel_id;
		let current_seen_timestamp = current_announcement_row.get::<_, i64>("seen") as u32;

		let current_channel_delta = delta_set.entry(scid).or_insert(ChannelDelta::default());
		(*current_channel_delta).announcement = Some(AnnouncementDelta {
			announcement: unsigned_announcement,
			seen: current_seen_timestamp,
		});

		announcement_count += 1;
	}
	log_info!(logger, "Fetched {} announcement rows", announcement_count);

	{
		// THIS STEP IS USED TO DETERMINE IF A CHANNEL SHOULD BE OMITTED FROM THE DELTA

		log_info!(logger, "Annotating channel announcements whose oldest channel update in a given direction occurred after the last sync");
		// Steps:
		// — Obtain all updates, distinct by (scid, direction), ordered by seen DESC // to find the oldest update in a given direction
		// — From those updates, select distinct by (scid), ordered by seen DESC (to obtain the newer one per direction)
		// This will allow us to mark the first time updates in both directions were seen

		// here is where the channels whose first update in either direction occurred after
		// `last_seen_timestamp` are added to the selection
		let params: [&(dyn tokio_postgres::types::ToSql + Sync); 2] =
			[&channel_ids, &last_sync_timestamp_float];
		let newer_oldest_directional_updates = client.query_raw("
			SELECT short_channel_id, CAST(EXTRACT('epoch' from distinct_chans.seen) AS BIGINT) AS seen FROM (
				SELECT DISTINCT ON (short_channel_id) *
				FROM (
					SELECT DISTINCT ON (short_channel_id, direction) short_channel_id, seen
					FROM channel_updates
					WHERE short_channel_id = any($1)
					ORDER BY short_channel_id ASC, direction ASC, seen ASC
				) AS directional_last_seens
				ORDER BY short_channel_id ASC, seen DESC
			) AS distinct_chans
			WHERE distinct_chans.seen >= TO_TIMESTAMP($2)
			", params).await.unwrap();
		let mut pinned_updates = Box::pin(newer_oldest_directional_updates);

		let mut newer_oldest_directional_update_count = 0;
		while let Some(row_res) = pinned_updates.next().await {
			let current_row = row_res.unwrap();

			let scid: i64 = current_row.get("short_channel_id");
			let current_seen_timestamp = current_row.get::<_, i64>("seen") as u32;

			// the newer of the two oldest seen directional updates came after last sync timestamp
			let current_channel_delta = delta_set.entry(scid as u64).or_insert(ChannelDelta::default());
			// first time a channel was seen in both directions
			(*current_channel_delta).first_bidirectional_updates_seen = Some(current_seen_timestamp);

			newer_oldest_directional_update_count += 1;
		}
		log_info!(logger, "Fetched {} update rows of the first update in a new direction", newer_oldest_directional_update_count);
	}

	{
		// THIS STEP IS USED TO DETERMINE IF A REMINDER UPDATE SHOULD BE SENT

		log_info!(logger, "Annotating channel announcements whose latest channel update in a given direction occurred more than six days ago");
		// Steps:
		// — Obtain all updates, distinct by (scid, direction), ordered by seen DESC
		// — From those updates, select distinct by (scid), ordered by seen ASC (to obtain the older one per direction)
		let reminder_threshold_timestamp = SystemTime::now().checked_sub(config::CHANNEL_REMINDER_AGE).unwrap().duration_since(UNIX_EPOCH).unwrap().as_secs() as f64;

		let params: [&(dyn tokio_postgres::types::ToSql + Sync); 2] =
			[&channel_ids, &reminder_threshold_timestamp];
		let older_latest_directional_updates = client.query_raw("
			SELECT short_channel_id FROM (
				SELECT DISTINCT ON (short_channel_id) *
				FROM (
					SELECT DISTINCT ON (short_channel_id, direction) short_channel_id, seen
					FROM channel_updates
					WHERE short_channel_id = any($1)
					ORDER BY short_channel_id ASC, direction ASC, seen DESC
				) AS directional_last_seens
				ORDER BY short_channel_id ASC, seen ASC
			) AS distinct_chans
			WHERE distinct_chans.seen <= TO_TIMESTAMP($2)
			", params).await.unwrap();
		let mut pinned_updates = Box::pin(older_latest_directional_updates);

		let mut older_latest_directional_update_count = 0;
		while let Some(row_res) = pinned_updates.next().await {
			let current_row = row_res.unwrap();
			let scid: i64 = current_row.get("short_channel_id");

			// annotate this channel as requiring that reminders be sent to the client
			let current_channel_delta = delta_set.entry(scid as u64).or_insert(ChannelDelta::default());

			// way might be able to get away with not using this
			(*current_channel_delta).requires_reminder = true;

			if let Some(current_channel_info) = network_graph.read_only().channel(scid as u64) {
				if current_channel_info.one_to_two.is_none() || current_channel_info.two_to_one.is_none() {
					// we don't send reminders if we don't have bidirectional update data
					continue;
				}

				if let Some(info) = current_channel_info.one_to_two.as_ref() {
					let flags: u8 = if info.enabled { 0 } else { 2 };
					let current_update = (*current_channel_delta).updates.0.get_or_insert(DirectedUpdateDelta::default());
					current_update.serialization_update_flags = Some(flags);
				}

				if let Some(info) = current_channel_info.two_to_one.as_ref() {
					let flags: u8 = if info.enabled { 1 } else { 3 };
					let current_update = (*current_channel_delta).updates.1.get_or_insert(DirectedUpdateDelta::default());
					current_update.serialization_update_flags = Some(flags);
				}
			} else {
				// we don't send reminders if we don't have the channel
				continue;
			}
			older_latest_directional_update_count += 1;
		}
		log_info!(logger, "Fetched {} update rows of the latest update in the less recently updated direction", older_latest_directional_update_count);
	}
}

pub(super) async fn fetch_channel_updates<L: Deref>(delta_set: &mut DeltaSet, client: &Client, last_sync_timestamp: u32, logger: L) where L::Target: Logger {
	let start = Instant::now();
	let last_sync_timestamp_float = last_sync_timestamp as f64;

	// get the latest channel update in each direction prior to last_sync_timestamp, provided
	// there was an update in either direction that happened after the last sync (to avoid
	// collecting too many reference updates)
	let reference_rows = client.query_raw("
		SELECT id, direction, CAST(EXTRACT('epoch' from seen) AS BIGINT) AS seen, blob_signed FROM channel_updates
		WHERE id IN (
			SELECT DISTINCT ON (short_channel_id, direction) id
			FROM channel_updates
			WHERE seen < TO_TIMESTAMP($1) AND short_channel_id IN (
				SELECT DISTINCT ON (short_channel_id) short_channel_id
				FROM channel_updates
				WHERE seen >= TO_TIMESTAMP($1)
			)
			ORDER BY short_channel_id ASC, direction ASC, seen DESC
		)
		", [last_sync_timestamp_float]).await.unwrap();
	let mut pinned_rows = Box::pin(reference_rows);

	log_info!(logger, "Fetched reference rows in {:?}", start.elapsed());

	let mut last_seen_update_ids: Vec<i32> = Vec::new();
	let mut non_intermediate_ids: HashSet<i32> = HashSet::new();
	let mut reference_row_count = 0;

	while let Some(row_res) = pinned_rows.next().await {
		let current_reference = row_res.unwrap();
		let update_id: i32 = current_reference.get("id");
		last_seen_update_ids.push(update_id);
		non_intermediate_ids.insert(update_id);

		let direction: bool = current_reference.get("direction");
		let seen = current_reference.get::<_, i64>("seen") as u32;
		let blob: Vec<u8> = current_reference.get("blob_signed");
		let mut readable = Cursor::new(blob);
		let unsigned_channel_update = ChannelUpdate::read(&mut readable).unwrap().contents;
		let scid = unsigned_channel_update.short_channel_id;

		let current_channel_delta = delta_set.entry(scid).or_insert(ChannelDelta::default());
		let update_delta = if !direction {
			(*current_channel_delta).updates.0.get_or_insert(DirectedUpdateDelta::default())
		} else {
			(*current_channel_delta).updates.1.get_or_insert(DirectedUpdateDelta::default())
		};
		log_gossip!(logger, "Channel {} last update before seen: {}/{}/{}", scid, update_id, direction, unsigned_channel_update.timestamp);
		update_delta.last_update_before_seen = Some(UpdateDelta {
			seen,
			update: unsigned_channel_update,
		});

		reference_row_count += 1;
	}

	log_info!(logger, "Processed {} reference rows (delta size: {}) in {:?}",
		reference_row_count, delta_set.len(), start.elapsed());

	// get all the intermediate channel updates
	// (to calculate the set of mutated fields for snapshotting, where intermediate updates may
	// have been omitted)

	let intermediate_updates = client.query_raw("
		SELECT id, direction, blob_signed, CAST(EXTRACT('epoch' from seen) AS BIGINT) AS seen
		FROM channel_updates
		WHERE seen >= TO_TIMESTAMP($1)
		ORDER BY short_channel_id ASC, timestamp DESC
		", [last_sync_timestamp_float]).await.unwrap();
	let mut pinned_updates = Box::pin(intermediate_updates);
	log_info!(logger, "Fetched intermediate rows in {:?}", start.elapsed());

	let mut previous_scid = u64::MAX;
	let mut previously_seen_directions = (false, false);

	// let mut previously_seen_directions = (false, false);
	let mut intermediate_update_count = 0;
	while let Some(row_res) = pinned_updates.next().await {
		let intermediate_update = row_res.unwrap();
		let update_id: i32 = intermediate_update.get("id");
		if non_intermediate_ids.contains(&update_id) {
			continue;
		}
		intermediate_update_count += 1;

		let direction: bool = intermediate_update.get("direction");
		let current_seen_timestamp = intermediate_update.get::<_, i64>("seen") as u32;
		let blob: Vec<u8> = intermediate_update.get("blob_signed");
		let mut readable = Cursor::new(blob);
		let unsigned_channel_update = ChannelUpdate::read(&mut readable).unwrap().contents;

		let scid = unsigned_channel_update.short_channel_id;
		if scid != previous_scid {
			previous_scid = scid;
			previously_seen_directions = (false, false);
		}

		// get the write configuration for this particular channel's directional details
		let current_channel_delta = delta_set.entry(scid).or_insert(ChannelDelta::default());
		let update_delta = if !direction {
			(*current_channel_delta).updates.0.get_or_insert(DirectedUpdateDelta::default())
		} else {
			(*current_channel_delta).updates.1.get_or_insert(DirectedUpdateDelta::default())
		};

		{
			// handle the latest deltas
			if !direction && !previously_seen_directions.0 {
				previously_seen_directions.0 = true;
				update_delta.latest_update_after_seen = Some(UpdateDelta {
					seen: current_seen_timestamp,
					update: unsigned_channel_update.clone(),
				});
			} else if direction && !previously_seen_directions.1 {
				previously_seen_directions.1 = true;
				update_delta.latest_update_after_seen = Some(UpdateDelta {
					seen: current_seen_timestamp,
					update: unsigned_channel_update.clone(),
				});
			}
		}

		// determine mutations
		if let Some(last_seen_update) = update_delta.last_update_before_seen.as_ref() {
			if unsigned_channel_update.flags != last_seen_update.update.flags {
				update_delta.mutated_properties.flags = true;
			}
			if unsigned_channel_update.cltv_expiry_delta != last_seen_update.update.cltv_expiry_delta {
				update_delta.mutated_properties.cltv_expiry_delta = true;
			}
			if unsigned_channel_update.htlc_minimum_msat != last_seen_update.update.htlc_minimum_msat {
				update_delta.mutated_properties.htlc_minimum_msat = true;
			}
			if unsigned_channel_update.fee_base_msat != last_seen_update.update.fee_base_msat {
				update_delta.mutated_properties.fee_base_msat = true;
			}
			if unsigned_channel_update.fee_proportional_millionths != last_seen_update.update.fee_proportional_millionths {
				update_delta.mutated_properties.fee_proportional_millionths = true;
			}
			if unsigned_channel_update.htlc_maximum_msat != last_seen_update.update.htlc_maximum_msat {
				update_delta.mutated_properties.htlc_maximum_msat = true;
			}
		}
	}
	log_info!(logger, "Processed intermediate rows ({}) (delta size: {}): {:?}", intermediate_update_count, delta_set.len(), start.elapsed());
}

pub(super) fn filter_delta_set<L: Deref>(delta_set: &mut DeltaSet, logger: L) where L::Target: Logger {
	let original_length = delta_set.len();
	let keys: Vec<u64> = delta_set.keys().cloned().collect();
	for k in keys {
		let v = delta_set.get(&k).unwrap();
		if v.announcement.is_none() {
			// this channel is not currently in the network graph
			delta_set.remove(&k);
			continue;
		}

		let update_meets_criteria = |update: &Option<DirectedUpdateDelta>| {
			if update.is_none() {
				return false;
			};
			let update_reference = update.as_ref().unwrap();
			// update_reference.latest_update_after_seen.is_some() && !update_reference.intermediate_updates.is_empty()
			// if there has been an update after the channel was first seen

			v.requires_reminder || update_reference.latest_update_after_seen.is_some()
		};

		let direction_a_meets_criteria = update_meets_criteria(&v.updates.0);
		let direction_b_meets_criteria = update_meets_criteria(&v.updates.1);

		if !v.requires_reminder && !direction_a_meets_criteria && !direction_b_meets_criteria {
			delta_set.remove(&k);
		}
	}

	let new_length = delta_set.len();
	if original_length != new_length {
		log_info!(logger, "length modified!");
	}
}

'''
'''--- src/main.rs ---
use std::sync::Arc;
use rapid_gossip_sync_server::RapidSyncProcessor;
use rapid_gossip_sync_server::types::RGSSLogger;

#[tokio::main]
async fn main() {
	let logger = Arc::new(RGSSLogger::new());
	RapidSyncProcessor::new(logger).start_sync().await;
}

'''
'''--- src/persistence.rs ---
use std::fs::OpenOptions;
use std::io::{BufWriter, Write};
use std::ops::Deref;
use std::sync::Arc;
use std::time::{Duration, Instant};
use lightning::log_info;
use lightning::routing::gossip::NetworkGraph;
use lightning::util::logger::Logger;
use lightning::util::ser::Writeable;
use tokio::sync::mpsc;

use crate::config;
use crate::types::GossipMessage;

const POSTGRES_INSERT_TIMEOUT: Duration = Duration::from_secs(15);

pub(crate) struct GossipPersister<L: Deref> where L::Target: Logger {
	gossip_persistence_receiver: mpsc::Receiver<GossipMessage>,
	network_graph: Arc<NetworkGraph<L>>,
	logger: L
}

impl<L: Deref> GossipPersister<L> where L::Target: Logger {
	pub fn new(network_graph: Arc<NetworkGraph<L>>, logger: L) -> (Self, mpsc::Sender<GossipMessage>) {
		let (gossip_persistence_sender, gossip_persistence_receiver) =
			mpsc::channel::<GossipMessage>(100);
		(GossipPersister {
			gossip_persistence_receiver,
			network_graph,
			logger
		}, gossip_persistence_sender)
	}

	pub(crate) async fn persist_gossip(&mut self) {
		let mut client = crate::connect_to_db().await;

		{
			// initialize the database
			let initialization = client
				.execute(config::db_config_table_creation_query(), &[])
				.await;
			if let Err(initialization_error) = initialization {
				panic!("db init error: {}", initialization_error);
			}

			let cur_schema = client.query("SELECT db_schema FROM config WHERE id = $1", &[&1]).await.unwrap();
			if !cur_schema.is_empty() {
				config::upgrade_db(cur_schema[0].get(0), &mut client).await;
			}

			let preparation = client.execute("set time zone UTC", &[]).await;
			if let Err(preparation_error) = preparation {
				panic!("db preparation error: {}", preparation_error);
			}

			let initialization = client
				.execute(
					// TODO: figure out a way to fix the id value without Postgres complaining about
					// its value not being default
					"INSERT INTO config (id, db_schema) VALUES ($1, $2) ON CONFLICT (id) DO NOTHING",
					&[&1, &config::SCHEMA_VERSION]
				).await;
			if let Err(initialization_error) = initialization {
				panic!("db init error: {}", initialization_error);
			}

			let initialization = client
				.execute(config::db_announcement_table_creation_query(), &[])
				.await;
			if let Err(initialization_error) = initialization {
				panic!("db init error: {}", initialization_error);
			}

			let initialization = client
				.execute(
					config::db_channel_update_table_creation_query(),
					&[],
				)
				.await;
			if let Err(initialization_error) = initialization {
				panic!("db init error: {}", initialization_error);
			}

			let initialization = client
				.batch_execute(config::db_index_creation_query())
				.await;
			if let Err(initialization_error) = initialization {
				panic!("db init error: {}", initialization_error);
			}
		}

		// print log statement every minute
		let mut latest_persistence_log = Instant::now() - Duration::from_secs(60);
		let mut i = 0u32;
		let mut latest_graph_cache_time = Instant::now();
		// TODO: it would be nice to have some sort of timeout here so after 10 seconds of
		// inactivity, some sort of message could be broadcast signaling the activation of request
		// processing
		while let Some(gossip_message) = &self.gossip_persistence_receiver.recv().await {
			i += 1; // count the persisted gossip messages

			if latest_persistence_log.elapsed().as_secs() >= 60 {
				log_info!(self.logger, "Persisting gossip message #{}", i);
				latest_persistence_log = Instant::now();
			}

			// has it been ten minutes? Just cache it
			if latest_graph_cache_time.elapsed().as_secs() >= 600 {
				self.persist_network_graph();
				latest_graph_cache_time = Instant::now();
			}

			match &gossip_message {
				GossipMessage::ChannelAnnouncement(announcement, _) => {
					let scid = announcement.contents.short_channel_id as i64;

					// start with the type prefix, which is already known a priori
					let mut announcement_signed = Vec::new();
					announcement.write(&mut announcement_signed).unwrap();

					tokio::time::timeout(POSTGRES_INSERT_TIMEOUT, client
						.execute("INSERT INTO channel_announcements (\
							short_channel_id, \
							announcement_signed \
						) VALUES ($1, $2) ON CONFLICT (short_channel_id) DO NOTHING", &[
							&scid,
							&announcement_signed
						])).await.unwrap().unwrap();
				}
				GossipMessage::ChannelUpdate(update, seen_override) => {
					let scid = update.contents.short_channel_id as i64;

					let timestamp = update.contents.timestamp as i64;

					let direction = (update.contents.flags & 1) == 1;
					let disable = (update.contents.flags & 2) > 0;

					let cltv_expiry_delta = update.contents.cltv_expiry_delta as i32;
					let htlc_minimum_msat = update.contents.htlc_minimum_msat as i64;
					let fee_base_msat = update.contents.fee_base_msat as i32;
					let fee_proportional_millionths =
						update.contents.fee_proportional_millionths as i32;
					let htlc_maximum_msat = update.contents.htlc_maximum_msat as i64;

					// start with the type prefix, which is already known a priori
					let mut update_signed = Vec::new();
					update.write(&mut update_signed).unwrap();

					let insertion_statement = if cfg!(test) {
						"INSERT INTO channel_updates (\
							short_channel_id, \
							timestamp, \
							seen, \
							channel_flags, \
							direction, \
							disable, \
							cltv_expiry_delta, \
							htlc_minimum_msat, \
							fee_base_msat, \
							fee_proportional_millionths, \
							htlc_maximum_msat, \
							blob_signed \
						) VALUES ($1, $2, TO_TIMESTAMP($3), $4, $5, $6, $7, $8, $9, $10, $11, $12)  ON CONFLICT DO NOTHING"
					} else {
						"INSERT INTO channel_updates (\
							short_channel_id, \
							timestamp, \
							channel_flags, \
							direction, \
							disable, \
							cltv_expiry_delta, \
							htlc_minimum_msat, \
							fee_base_msat, \
							fee_proportional_millionths, \
							htlc_maximum_msat, \
							blob_signed \
						) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11)  ON CONFLICT DO NOTHING"
					};

					// this may not be used outside test cfg
					let _seen_timestamp = seen_override.unwrap_or(timestamp as u32) as f64;

					tokio::time::timeout(POSTGRES_INSERT_TIMEOUT, client
						.execute(insertion_statement, &[
							&scid,
							&timestamp,
							#[cfg(test)]
								&_seen_timestamp,
							&(update.contents.flags as i16),
							&direction,
							&disable,
							&cltv_expiry_delta,
							&htlc_minimum_msat,
							&fee_base_msat,
							&fee_proportional_millionths,
							&htlc_maximum_msat,
							&update_signed
						])).await.unwrap().unwrap();
				}
			}
		}
	}

	fn persist_network_graph(&self) {
		log_info!(self.logger, "Caching network graph…");
		let cache_path = config::network_graph_cache_path();
		let file = OpenOptions::new()
			.create(true)
			.write(true)
			.truncate(true)
			.open(&cache_path)
			.unwrap();
		self.network_graph.remove_stale_channels_and_tracking();
		let mut writer = BufWriter::new(file);
		self.network_graph.write(&mut writer).unwrap();
		writer.flush().unwrap();
		log_info!(self.logger, "Cached network graph!");
	}
}

'''
'''--- src/serialization.rs ---
use std::cmp::max;
use std::collections::HashMap;
use std::time::{SystemTime, UNIX_EPOCH};

use bitcoin::BlockHash;
use bitcoin::hashes::Hash;
use lightning::ln::msgs::{UnsignedChannelAnnouncement, UnsignedChannelUpdate};
use lightning::util::ser::{BigSize, Writeable};
use crate::config;

use crate::lookup::{DeltaSet, DirectedUpdateDelta};

pub(super) struct SerializationSet {
	pub(super) announcements: Vec<UnsignedChannelAnnouncement>,
	pub(super) updates: Vec<UpdateSerialization>,
	pub(super) full_update_defaults: DefaultUpdateValues,
	pub(super) latest_seen: u32,
	pub(super) chain_hash: BlockHash,
}

pub(super) struct DefaultUpdateValues {
	pub(super) cltv_expiry_delta: u16,
	pub(super) htlc_minimum_msat: u64,
	pub(super) fee_base_msat: u32,
	pub(super) fee_proportional_millionths: u32,
	pub(super) htlc_maximum_msat: u64,
}

impl Default for DefaultUpdateValues {
	fn default() -> Self {
		Self {
			cltv_expiry_delta: 0,
			htlc_minimum_msat: 0,
			fee_base_msat: 0,
			fee_proportional_millionths: 0,
			htlc_maximum_msat: 0,
		}
	}
}

pub(super) struct MutatedProperties {
	pub(super) flags: bool,
	pub(super) cltv_expiry_delta: bool,
	pub(super) htlc_minimum_msat: bool,
	pub(super) fee_base_msat: bool,
	pub(super) fee_proportional_millionths: bool,
	pub(super) htlc_maximum_msat: bool,
}

impl Default for MutatedProperties {
	fn default() -> Self {
		Self {
			flags: false,
			cltv_expiry_delta: false,
			htlc_minimum_msat: false,
			fee_base_msat: false,
			fee_proportional_millionths: false,
			htlc_maximum_msat: false,
		}
	}
}

impl MutatedProperties {
	/// Does not include flags because the flag byte is always sent in full
	fn len(&self) -> u8 {
		let mut mutations = 0;
		if self.cltv_expiry_delta { mutations += 1; };
		if self.htlc_minimum_msat { mutations += 1; };
		if self.fee_base_msat { mutations += 1; };
		if self.fee_proportional_millionths { mutations += 1; };
		if self.htlc_maximum_msat { mutations += 1; };
		mutations
	}
}

pub(super) enum UpdateSerialization {
	Full(UnsignedChannelUpdate),
	Incremental(UnsignedChannelUpdate, MutatedProperties),
	Reminder(u64, u8),
}
impl UpdateSerialization {
	pub(super) fn scid(&self) -> u64 {
		match self {
			UpdateSerialization::Full(latest_update)|
			UpdateSerialization::Incremental(latest_update, _) => latest_update.short_channel_id,
			UpdateSerialization::Reminder(scid, _) => *scid,
		}
	}

	fn flags(&self) -> u8 {
		match self {
			UpdateSerialization::Full(latest_update)|
			UpdateSerialization::Incremental(latest_update, _) => latest_update.flags,
			UpdateSerialization::Reminder(_, flags) => *flags,
		}
	}
}

struct FullUpdateValueHistograms {
	cltv_expiry_delta: HashMap<u16, usize>,
	htlc_minimum_msat: HashMap<u64, usize>,
	fee_base_msat: HashMap<u32, usize>,
	fee_proportional_millionths: HashMap<u32, usize>,
	htlc_maximum_msat: HashMap<u64, usize>,
}

pub(super) fn serialize_delta_set(delta_set: DeltaSet, last_sync_timestamp: u32) -> SerializationSet {
	let mut serialization_set = SerializationSet {
		announcements: vec![],
		updates: vec![],
		full_update_defaults: Default::default(),
		chain_hash: BlockHash::all_zeros(),
		latest_seen: 0,
	};

	let mut chain_hash_set = false;

	let mut full_update_histograms = FullUpdateValueHistograms {
		cltv_expiry_delta: Default::default(),
		htlc_minimum_msat: Default::default(),
		fee_base_msat: Default::default(),
		fee_proportional_millionths: Default::default(),
		htlc_maximum_msat: Default::default(),
	};

	let mut record_full_update_in_histograms = |full_update: &UnsignedChannelUpdate| {
		*full_update_histograms.cltv_expiry_delta.entry(full_update.cltv_expiry_delta).or_insert(0) += 1;
		*full_update_histograms.htlc_minimum_msat.entry(full_update.htlc_minimum_msat).or_insert(0) += 1;
		*full_update_histograms.fee_base_msat.entry(full_update.fee_base_msat).or_insert(0) += 1;
		*full_update_histograms.fee_proportional_millionths.entry(full_update.fee_proportional_millionths).or_insert(0) += 1;
		*full_update_histograms.htlc_maximum_msat.entry(full_update.htlc_maximum_msat).or_insert(0) += 1;
	};

	// if the previous seen update happened more than 6 days ago, the client may have pruned it, and an incremental update wouldn't work
	let non_incremental_previous_update_threshold_timestamp = SystemTime::now().checked_sub(config::CHANNEL_REMINDER_AGE).unwrap().duration_since(UNIX_EPOCH).unwrap().as_secs() as u32;

	for (scid, channel_delta) in delta_set.into_iter() {

		// any announcement chain hash is gonna be the same value. Just set it from the first one.
		let channel_announcement_delta = channel_delta.announcement.as_ref().unwrap();
		if !chain_hash_set {
			chain_hash_set = true;
			serialization_set.chain_hash = channel_announcement_delta.announcement.chain_hash.clone();
		}

		let current_announcement_seen = channel_announcement_delta.seen;
		let is_new_announcement = current_announcement_seen >= last_sync_timestamp;
		let is_newly_included_announcement = if let Some(first_update_seen) = channel_delta.first_bidirectional_updates_seen {
			first_update_seen >= last_sync_timestamp
		} else {
			false
		};
		let send_announcement = is_new_announcement || is_newly_included_announcement;
		if send_announcement {
			serialization_set.latest_seen = max(serialization_set.latest_seen, current_announcement_seen);
			serialization_set.announcements.push(channel_delta.announcement.unwrap().announcement);
		}

		let direction_a_updates = channel_delta.updates.0;
		let direction_b_updates = channel_delta.updates.1;

		let mut categorize_directed_update_serialization = |directed_updates: Option<DirectedUpdateDelta>| {
			if let Some(updates) = directed_updates {
				if let Some(latest_update_delta) = updates.latest_update_after_seen {
					let latest_update = latest_update_delta.update;
					assert_eq!(latest_update.short_channel_id, scid, "Update in DB had wrong SCID column");

					// the returned seen timestamp should be the latest of all the returned
					// announcements and latest updates
					serialization_set.latest_seen = max(serialization_set.latest_seen, latest_update_delta.seen);

					if let Some(update_delta) = updates.last_update_before_seen {
						let mutated_properties = updates.mutated_properties;
						if send_announcement || mutated_properties.len() == 5 || update_delta.seen <= non_incremental_previous_update_threshold_timestamp {
							// all five values have changed, it makes more sense to just
							// serialize the update as a full update instead of as a change
							// this way, the default values can be computed more efficiently
							record_full_update_in_histograms(&latest_update);
							serialization_set.updates.push(UpdateSerialization::Full(latest_update));
						} else if mutated_properties.len() > 0 || mutated_properties.flags {
							// we don't count flags as mutated properties
							serialization_set.updates.push(
								UpdateSerialization::Incremental(latest_update, mutated_properties));
						}
					} else {
						// serialize the full update
						record_full_update_in_histograms(&latest_update);
						serialization_set.updates.push(UpdateSerialization::Full(latest_update));
					}
				} else if let Some(flags) = updates.serialization_update_flags {
					serialization_set.updates.push(UpdateSerialization::Reminder(scid, flags));
				}
			}
		};

		categorize_directed_update_serialization(direction_a_updates);
		categorize_directed_update_serialization(direction_b_updates);
	}

	let default_update_values = DefaultUpdateValues {
		cltv_expiry_delta: find_most_common_histogram_entry_with_default(full_update_histograms.cltv_expiry_delta, 0),
		htlc_minimum_msat: find_most_common_histogram_entry_with_default(full_update_histograms.htlc_minimum_msat, 0),
		fee_base_msat: find_most_common_histogram_entry_with_default(full_update_histograms.fee_base_msat, 0),
		fee_proportional_millionths: find_most_common_histogram_entry_with_default(full_update_histograms.fee_proportional_millionths, 0),
		htlc_maximum_msat: find_most_common_histogram_entry_with_default(full_update_histograms.htlc_maximum_msat, 0),
	};

	serialization_set.full_update_defaults = default_update_values;
	serialization_set
}

pub fn serialize_stripped_channel_announcement(announcement: &UnsignedChannelAnnouncement, node_id_a_index: usize, node_id_b_index: usize, previous_scid: u64) -> Vec<u8> {
	let mut stripped_announcement = vec![];

	announcement.features.write(&mut stripped_announcement).unwrap();

	if previous_scid > announcement.short_channel_id {
		panic!("unsorted scids!");
	}
	let scid_delta = BigSize(announcement.short_channel_id - previous_scid);
	scid_delta.write(&mut stripped_announcement).unwrap();

	// write indices of node ids rather than the node IDs themselves
	BigSize(node_id_a_index as u64).write(&mut stripped_announcement).unwrap();
	BigSize(node_id_b_index as u64).write(&mut stripped_announcement).unwrap();

	// println!("serialized CA: {}, \n{:?}\n{:?}\n", announcement.short_channel_id, announcement.node_id_1, announcement.node_id_2);
	stripped_announcement
}

pub(super) fn serialize_stripped_channel_update(update: &UpdateSerialization, default_values: &DefaultUpdateValues, previous_scid: u64) -> Vec<u8> {
	let mut serialized_flags = update.flags();

	if previous_scid > update.scid() {
		panic!("unsorted scids!");
	}

	let mut delta_serialization = Vec::new();
	let mut prefixed_serialization = Vec::new();

	match update {
		UpdateSerialization::Full(latest_update) => {
			if latest_update.cltv_expiry_delta != default_values.cltv_expiry_delta {
				serialized_flags |= 0b_0100_0000;
				latest_update.cltv_expiry_delta.write(&mut delta_serialization).unwrap();
			}

			if latest_update.htlc_minimum_msat != default_values.htlc_minimum_msat {
				serialized_flags |= 0b_0010_0000;
				latest_update.htlc_minimum_msat.write(&mut delta_serialization).unwrap();
			}

			if latest_update.fee_base_msat != default_values.fee_base_msat {
				serialized_flags |= 0b_0001_0000;
				latest_update.fee_base_msat.write(&mut delta_serialization).unwrap();
			}

			if latest_update.fee_proportional_millionths != default_values.fee_proportional_millionths {
				serialized_flags |= 0b_0000_1000;
				latest_update.fee_proportional_millionths.write(&mut delta_serialization).unwrap();
			}

			if latest_update.htlc_maximum_msat != default_values.htlc_maximum_msat {
				serialized_flags |= 0b_0000_0100;
				latest_update.htlc_maximum_msat.write(&mut delta_serialization).unwrap();
			}
		}
		UpdateSerialization::Incremental(latest_update, mutated_properties) => {
			// indicate that this update is incremental
			serialized_flags |= 0b_1000_0000;

			if mutated_properties.cltv_expiry_delta {
				serialized_flags |= 0b_0100_0000;
				latest_update.cltv_expiry_delta.write(&mut delta_serialization).unwrap();
			}

			if mutated_properties.htlc_minimum_msat {
				serialized_flags |= 0b_0010_0000;
				latest_update.htlc_minimum_msat.write(&mut delta_serialization).unwrap();
			}

			if mutated_properties.fee_base_msat {
				serialized_flags |= 0b_0001_0000;
				latest_update.fee_base_msat.write(&mut delta_serialization).unwrap();
			}

			if mutated_properties.fee_proportional_millionths {
				serialized_flags |= 0b_0000_1000;
				latest_update.fee_proportional_millionths.write(&mut delta_serialization).unwrap();
			}

			if mutated_properties.htlc_maximum_msat {
				serialized_flags |= 0b_0000_0100;
				latest_update.htlc_maximum_msat.write(&mut delta_serialization).unwrap();
			}
		},
		UpdateSerialization::Reminder(_, _) => {
			// indicate that this update is incremental
			serialized_flags |= 0b_1000_0000;
		}
	}
	let scid_delta = BigSize(update.scid() - previous_scid);
	scid_delta.write(&mut prefixed_serialization).unwrap();

	serialized_flags.write(&mut prefixed_serialization).unwrap();
	prefixed_serialization.append(&mut delta_serialization);

	prefixed_serialization
}

pub(super) fn find_most_common_histogram_entry_with_default<T: Copy>(histogram: HashMap<T, usize>, default: T) -> T {
	let most_frequent_entry = histogram.iter().max_by(|a, b| a.1.cmp(&b.1));
	if let Some(entry_details) = most_frequent_entry {
		// .0 is the value
		// .1 is the frequency
		return entry_details.0.to_owned();
	}
	// the default should pretty much always be a 0 as T
	// though for htlc maximum msat it could be a u64::max
	default
}

'''
'''--- src/snapshot.rs ---
use std::collections::HashMap;
use std::fs;
use std::ops::Deref;
use std::os::unix::fs::symlink;
use std::sync::Arc;
use std::time::{Duration, SystemTime, UNIX_EPOCH};
use lightning::log_info;

use lightning::routing::gossip::NetworkGraph;
use lightning::util::logger::Logger;

use crate::config;
use crate::config::cache_path;

pub(crate) struct Snapshotter<L: Deref + Clone> where L::Target: Logger {
	network_graph: Arc<NetworkGraph<L>>,
	logger: L,
}

impl<L: Deref + Clone> Snapshotter<L> where L::Target: Logger {
	pub fn new(network_graph: Arc<NetworkGraph<L>>, logger: L) -> Self {
		Self { network_graph, logger }
	}

	pub(crate) async fn snapshot_gossip(&self) {
		log_info!(self.logger, "Initiating snapshotting service");

		let snapshot_interval = config::snapshot_generation_interval() as u64;
		let mut snapshot_scopes = vec![];
		{ // double the coefficient until it reaches the maximum (limited) snapshot scope
			let mut current_scope = snapshot_interval;
			loop {
				snapshot_scopes.push(current_scope);
				if current_scope >= config::MAX_SNAPSHOT_SCOPE as u64 {
					snapshot_scopes.push(u64::MAX);
					break;
				}

				// double the current factor
				current_scope <<= 1;
			}
		}

		// this is gonna be a never-ending background job
		loop {
			self.generate_snapshots(config::SYMLINK_GRANULARITY_INTERVAL as u64, snapshot_interval, &snapshot_scopes, &cache_path(), None).await;

			// constructing the snapshots may have taken a while
			let current_time = SystemTime::now().duration_since(UNIX_EPOCH).unwrap().as_secs();

			// NOTE: we're waiting until the next multiple of snapshot_interval
			// however, if the symlink granularity is lower, then during that time, no intermediate
			// symlinks will be generated. That should be ok, because any timestamps previously
			// returned would already have generated symlinks, but this does have bug potential
			let remainder = current_time % snapshot_interval;
			let time_until_next_generation = snapshot_interval - remainder;

			log_info!(self.logger, "Sleeping until next snapshot capture: {}s", time_until_next_generation);
			// add in an extra five seconds to assure the rounding down works correctly
			let sleep = tokio::time::sleep(Duration::from_secs(time_until_next_generation + 5));
			sleep.await;
		}
	}

	pub(crate) async fn generate_snapshots(&self, granularity_interval: u64, snapshot_interval: u64, snapshot_scopes: &[u64], cache_path: &str, max_symlink_count: Option<u64>) {
		let pending_snapshot_directory = format!("{}/snapshots_pending", cache_path);
		let pending_symlink_directory = format!("{}/symlinks_pending", cache_path);
		let finalized_snapshot_directory = format!("{}/snapshots", cache_path);
		let finalized_symlink_directory = format!("{}/symlinks", cache_path);
		let relative_symlink_to_snapshot_path = "../snapshots";

		// 1. get the current timestamp
		let snapshot_generation_timestamp = SystemTime::now().duration_since(UNIX_EPOCH).unwrap().as_secs();
		let reference_timestamp = Self::round_down_to_nearest_multiple(snapshot_generation_timestamp, snapshot_interval as u64);
		log_info!(self.logger, "Capturing snapshots at {} for: {}", snapshot_generation_timestamp, reference_timestamp);

		// 2. sleep until the next round interval
		// 3. refresh all snapshots

		// the stored snapshots should adhere to the following format
		// from one day ago
		// from two days ago
		// …
		// from a week ago
		// from two weeks ago
		// from three weeks ago
		// full
		// That means that at any given moment, there should only ever be
		// 6 (daily) + 3 (weekly) + 1 (total) = 10 cached snapshots
		// The snapshots, unlike dynamic updates, should account for all intermediate
		// channel updates

		// purge and recreate the pending directories
		if fs::metadata(&pending_snapshot_directory).is_ok() {
			fs::remove_dir_all(&pending_snapshot_directory).expect("Failed to remove pending snapshot directory.");
		}
		if fs::metadata(&pending_symlink_directory).is_ok() {
			fs::remove_dir_all(&pending_symlink_directory).expect("Failed to remove pending symlink directory.");
		}
		fs::create_dir_all(&pending_snapshot_directory).expect("Failed to create pending snapshot directory");
		fs::create_dir_all(&pending_symlink_directory).expect("Failed to create pending symlink directory");

		let mut snapshot_sync_timestamps: Vec<(u64, u64)> = Vec::new();
		for current_scope in snapshot_scopes {
			let timestamp = reference_timestamp.saturating_sub(current_scope.clone());
			snapshot_sync_timestamps.push((current_scope.clone(), timestamp));
		};

		let mut snapshot_filenames_by_scope: HashMap<u64, String> = HashMap::with_capacity(10);

		for (current_scope, current_last_sync_timestamp) in &snapshot_sync_timestamps {
			let network_graph_clone = self.network_graph.clone();
			{
				log_info!(self.logger, "Calculating {}-second snapshot", current_scope);
				// calculate the snapshot
				let snapshot = super::serialize_delta(network_graph_clone, current_last_sync_timestamp.clone() as u32, self.logger.clone()).await;

				// persist the snapshot and update the symlink
				let snapshot_filename = format!("snapshot__calculated-at:{}__range:{}-scope__previous-sync:{}.lngossip", reference_timestamp, current_scope, current_last_sync_timestamp);
				let snapshot_path = format!("{}/{}", pending_snapshot_directory, snapshot_filename);
				log_info!(self.logger, "Persisting {}-second snapshot: {} ({} messages, {} announcements, {} updates ({} full, {} incremental))", current_scope, snapshot_filename, snapshot.message_count, snapshot.announcement_count, snapshot.update_count, snapshot.update_count_full, snapshot.update_count_incremental);
				fs::write(&snapshot_path, snapshot.data).unwrap();
				snapshot_filenames_by_scope.insert(current_scope.clone(), snapshot_filename);
			}
		}

		{
			// create dummy symlink
			let dummy_filename = "empty_delta.lngossip";
			let dummy_snapshot = super::serialize_empty_blob(reference_timestamp);
			let dummy_snapshot_path = format!("{}/{}", pending_snapshot_directory, dummy_filename);
			fs::write(&dummy_snapshot_path, dummy_snapshot).unwrap();

			let dummy_symlink_path = format!("{}/{}.bin", pending_symlink_directory, reference_timestamp);
			let relative_dummy_snapshot_path = format!("{}/{}", relative_symlink_to_snapshot_path, dummy_filename);
			log_info!(self.logger, "Symlinking dummy: {} -> {}", dummy_symlink_path, relative_dummy_snapshot_path);
			symlink(&relative_dummy_snapshot_path, &dummy_symlink_path).unwrap();
		}

		// Number of intervals since Jan 1, 2022, a few months before RGS server was released.
		let mut symlink_count = (reference_timestamp - 1640995200) / granularity_interval;
		if let Some(max_symlink_count) = max_symlink_count {
			// this is primarily useful for testing
			symlink_count = std::cmp::min(symlink_count, max_symlink_count);
		};

		for i in 0..symlink_count {
			// let's create non-dummy-symlinks

			// first, determine which snapshot range should be referenced
			let referenced_scope = if i == 0 {
				// special-case 0 to always refer to a full/initial sync
				u64::MAX
			} else {
				/*
				We have snapshots for 6-day- and 7-day-intervals, but the next interval is
				14 days. So if somebody requests an update with a timestamp that is 10 days old,
				there is no longer a snapshot for that specific interval.

				The correct snapshot will be the next highest interval, i. e. for 14 days.

				The `snapshot_sync_day_factors` array is sorted ascendingly, so find() will
				return on the first iteration that is at least equal to the requested interval.

				Note, however, that the last value in the array is u64::max, which means that
				multiplying it with snapshot_interval will overflow. To avoid that, we use
				saturating_mul.
				 */

				// find min(x) in snapshot_scopes where i * granularity <= x (the current scope)
				snapshot_scopes.iter().find(|current_scope| {
					i * granularity_interval <= **current_scope
				}).unwrap().clone()
			};
			log_info!(self.logger, "i: {}, referenced scope: {}", i, referenced_scope);

			let snapshot_filename = snapshot_filenames_by_scope.get(&referenced_scope).unwrap();
			let relative_snapshot_path = format!("{}/{}", relative_symlink_to_snapshot_path, snapshot_filename);

			let canonical_last_sync_timestamp = if i == 0 {
				// special-case 0 to always refer to a full/initial sync
				0
			} else {
				reference_timestamp.saturating_sub(granularity_interval.saturating_mul(i))
			};
			let symlink_path = format!("{}/{}.bin", pending_symlink_directory, canonical_last_sync_timestamp);

			log_info!(self.logger, "Symlinking: {} -> {} ({} -> {}", i, referenced_scope, symlink_path, relative_snapshot_path);
			symlink(&relative_snapshot_path, &symlink_path).unwrap();
		}

		let update_time_path = format!("{}/update_time.txt", pending_symlink_directory);
		let update_time = SystemTime::now().duration_since(UNIX_EPOCH).unwrap().as_secs();
		fs::write(&update_time_path, format!("{}", update_time)).unwrap();

		if fs::metadata(&finalized_snapshot_directory).is_ok() {
			fs::remove_dir_all(&finalized_snapshot_directory).expect("Failed to remove finalized snapshot directory.");
		}
		if fs::metadata(&finalized_symlink_directory).is_ok() {
			fs::remove_dir_all(&finalized_symlink_directory).expect("Failed to remove pending symlink directory.");
		}
		fs::rename(&pending_snapshot_directory, &finalized_snapshot_directory).expect("Failed to finalize snapshot directory.");
		fs::rename(&pending_symlink_directory, &finalized_symlink_directory).expect("Failed to finalize symlink directory.");
	}

	pub(super) fn round_down_to_nearest_multiple(number: u64, multiple: u64) -> u64 {
		let round_multiple_delta = number % multiple;
		number - round_multiple_delta
	}
}

'''
'''--- src/tests/mod.rs ---
//! Multi-module tests that use database fixtures

use std::cell::RefCell;
use std::sync::Arc;
use std::{fs, thread};
use std::time::{SystemTime, UNIX_EPOCH};
use bitcoin::{BlockHash, Network};
use bitcoin::secp256k1::ecdsa::Signature;
use bitcoin::secp256k1::{Secp256k1, SecretKey};
use bitcoin::hashes::Hash;
use bitcoin::hashes::hex::ToHex;
use bitcoin::hashes::sha256d::Hash as Sha256dHash;
use lightning::ln::features::ChannelFeatures;
use lightning::ln::msgs::{ChannelAnnouncement, ChannelUpdate, UnsignedChannelAnnouncement, UnsignedChannelUpdate};
use lightning::routing::gossip::{NetworkGraph, NodeId};
use lightning::util::ser::Writeable;
use lightning_rapid_gossip_sync::RapidGossipSync;
use crate::{config, serialize_delta};
use crate::persistence::GossipPersister;
use crate::snapshot::Snapshotter;
use crate::types::{GossipMessage, tests::TestLogger};

const CLIENT_BACKDATE_INTERVAL: u32 = 3600 * 24 * 7; // client backdates RGS by a week

thread_local! {
	static DB_TEST_SCHEMA: RefCell<Option<String>> = RefCell::new(None);
	static IS_TEST_SCHEMA_CLEAN: RefCell<Option<bool>> = RefCell::new(None);
}

fn blank_signature() -> Signature {
	Signature::from_compact(&[0u8; 64]).unwrap()
}

fn genesis_hash() -> BlockHash {
	bitcoin::blockdata::constants::genesis_block(Network::Bitcoin).block_hash()
}

fn current_time() -> u32 {
	SystemTime::now().duration_since(UNIX_EPOCH).expect("Time must be > 1970").as_secs() as u32
}

pub(crate) fn db_test_schema() -> String {
	DB_TEST_SCHEMA.with(|suffix_reference| {
		let suffix_option = suffix_reference.borrow();
		suffix_option.as_ref().unwrap().clone()
	})
}

fn generate_announcement(short_channel_id: u64) -> ChannelAnnouncement {
	let secp_context = Secp256k1::new();

	let random_private_key_1 = SecretKey::from_slice(&[1; 32]).unwrap();
	let random_public_key_1 = random_private_key_1.public_key(&secp_context);
	let node_id_1 = NodeId::from_pubkey(&random_public_key_1);

	let random_private_key_2 = SecretKey::from_slice(&[2; 32]).unwrap();
	let random_public_key_2 = random_private_key_2.public_key(&secp_context);
	let node_id_2 = NodeId::from_pubkey(&random_public_key_2);

	let announcement = UnsignedChannelAnnouncement {
		features: ChannelFeatures::empty(),
		chain_hash: genesis_hash(),
		short_channel_id,
		node_id_1,
		node_id_2,
		bitcoin_key_1: node_id_1,
		bitcoin_key_2: node_id_2,
		excess_data: vec![],
	};

	let msg_hash = bitcoin::secp256k1::Message::from_slice(&Sha256dHash::hash(&announcement.encode()[..])[..]).unwrap();
	let node_signature_1 = secp_context.sign_ecdsa(&msg_hash, &random_private_key_1);
	let node_signature_2 = secp_context.sign_ecdsa(&msg_hash, &random_private_key_2);

	ChannelAnnouncement {
		node_signature_1,
		node_signature_2,
		bitcoin_signature_1: node_signature_1,
		bitcoin_signature_2: node_signature_2,
		contents: announcement,
	}
}

fn generate_update(scid: u64, direction: bool, timestamp: u32, expiry_delta: u16, min_msat: u64, max_msat: u64, base_msat: u32, fee_rate: u32) -> ChannelUpdate {
	let flag_mask = if direction { 1 } else { 0 };
	ChannelUpdate {
		signature: blank_signature(),
		contents: UnsignedChannelUpdate {
			chain_hash: genesis_hash(),
			short_channel_id: scid,
			timestamp,
			flags: 0 | flag_mask,
			cltv_expiry_delta: expiry_delta,
			htlc_minimum_msat: min_msat,
			htlc_maximum_msat: max_msat,
			fee_base_msat: base_msat,
			fee_proportional_millionths: fee_rate,
			excess_data: vec![],
		},
	}
}

struct SchemaSanitizer {}

impl SchemaSanitizer {
	fn new() -> Self {
		IS_TEST_SCHEMA_CLEAN.with(|cleanliness_reference| {
			let mut is_clean_option = cleanliness_reference.borrow_mut();
			assert!(is_clean_option.is_none());
			*is_clean_option = Some(false);
		});

		DB_TEST_SCHEMA.with(|suffix_reference| {
			let mut suffix_option = suffix_reference.borrow_mut();
			let current_time = SystemTime::now();
			let unix_time = current_time.duration_since(UNIX_EPOCH).expect("Time went backwards");
			let timestamp_seconds = unix_time.as_secs();
			let timestamp_nanos = unix_time.as_nanos();
			// sometimes Rust thinks two tests start at the same nanosecond, causing a schema conflict
			let thread_id = thread::current().id();
			let preimage = format!("{:?}-{}", thread_id, timestamp_nanos);
			println!("test schema preimage: {}", preimage);
			let suffix = Sha256dHash::hash(preimage.as_bytes()).into_inner().to_hex();
			// the schema must start with a letter
			let schema = format!("test_{}_{}", timestamp_seconds, suffix);
			*suffix_option = Some(schema);
		});

		return Self {};
	}
}

impl Drop for SchemaSanitizer {
	fn drop(&mut self) {
		IS_TEST_SCHEMA_CLEAN.with(|cleanliness_reference| {
			let is_clean_option = cleanliness_reference.borrow();
			if let Some(is_clean) = *is_clean_option {
				assert_eq!(is_clean, true);
			}
		});
	}
}

struct CacheSanitizer {}

impl CacheSanitizer {
	/// The CacheSanitizer instantiation requires that there be a schema sanitizer
	fn new(_: &SchemaSanitizer) -> Self {
		Self {}
	}

	fn cache_path(&self) -> String {
		format!("./res/{}/", db_test_schema())
	}
}

impl Drop for CacheSanitizer {
	fn drop(&mut self) {
		let cache_path = self.cache_path();
		fs::remove_dir_all(cache_path).unwrap();
	}
}

async fn clean_test_db() {
	let client = crate::connect_to_db().await;
	let schema = db_test_schema();
	client.execute(&format!("DROP SCHEMA IF EXISTS {} CASCADE", schema), &[]).await.unwrap();
	IS_TEST_SCHEMA_CLEAN.with(|cleanliness_reference| {
		let mut is_clean_option = cleanliness_reference.borrow_mut();
		*is_clean_option = Some(true);
	});
}

#[tokio::test]
async fn test_trivial_setup() {
	let _sanitizer = SchemaSanitizer::new();
	let logger = Arc::new(TestLogger::new());
	let network_graph = NetworkGraph::new(Network::Bitcoin, logger.clone());
	let network_graph_arc = Arc::new(network_graph);
	let (mut persister, receiver) = GossipPersister::new(network_graph_arc.clone(), logger.clone());

	let short_channel_id = 1;
	let timestamp = current_time() - 10;
	println!("timestamp: {}", timestamp);

	{ // seed the db
		let announcement = generate_announcement(short_channel_id);
		let update_1 = generate_update(short_channel_id, false, timestamp, 0, 0, 0, 5, 0);
		let update_2 = generate_update(short_channel_id, true, timestamp, 0, 0, 0, 10, 0);

		network_graph_arc.update_channel_from_announcement_no_lookup(&announcement).unwrap();
		network_graph_arc.update_channel_unsigned(&update_1.contents).unwrap();
		network_graph_arc.update_channel_unsigned(&update_2.contents).unwrap();

		receiver.send(GossipMessage::ChannelAnnouncement(announcement, None)).await.unwrap();
		receiver.send(GossipMessage::ChannelUpdate(update_1, None)).await.unwrap();
		receiver.send(GossipMessage::ChannelUpdate(update_2, None)).await.unwrap();
		drop(receiver);
		persister.persist_gossip().await;
	}

	let serialization = serialize_delta(network_graph_arc.clone(), 0, logger.clone()).await;
	logger.assert_log_contains("rapid_gossip_sync_server", "announcement channel count: 1", 1);
	clean_test_db().await;

	let channel_count = network_graph_arc.read_only().channels().len();

	assert_eq!(channel_count, 1);
	assert_eq!(serialization.message_count, 3);
	assert_eq!(serialization.announcement_count, 1);
	assert_eq!(serialization.update_count, 2);

	let client_graph = NetworkGraph::new(Network::Bitcoin, logger.clone());
	let client_graph_arc = Arc::new(client_graph);
	let rgs = RapidGossipSync::new(client_graph_arc.clone(), logger.clone());
	let update_result = rgs.update_network_graph(&serialization.data).unwrap();
	println!("update result: {}", update_result);
	// the update result must be a multiple of our snapshot granularity
	assert_eq!(update_result % config::snapshot_generation_interval(), 0);
	assert!(update_result < timestamp);

	let timestamp_delta = timestamp - update_result;
	println!("timestamp delta: {}", timestamp_delta);
	assert!(timestamp_delta < config::snapshot_generation_interval());

	let readonly_graph = client_graph_arc.read_only();
	let channels = readonly_graph.channels();
	let client_channel_count = channels.len();
	assert_eq!(client_channel_count, 1);

	let first_channel = channels.get(&short_channel_id).unwrap();
	assert!(&first_channel.announcement_message.is_none());
	assert_eq!(first_channel.one_to_two.as_ref().unwrap().fees.base_msat, 5);
	assert_eq!(first_channel.two_to_one.as_ref().unwrap().fees.base_msat, 10);
	let last_update_seen_a = first_channel.one_to_two.as_ref().unwrap().last_update;
	let last_update_seen_b = first_channel.two_to_one.as_ref().unwrap().last_update;
	println!("last update a: {}", last_update_seen_a);
	println!("last update b: {}", last_update_seen_b);
	assert_eq!(last_update_seen_a, update_result - CLIENT_BACKDATE_INTERVAL);
	assert_eq!(last_update_seen_b, update_result - CLIENT_BACKDATE_INTERVAL);
}

#[tokio::test]
async fn test_full_snapshot_recency() {
	let _sanitizer = SchemaSanitizer::new();
	let logger = Arc::new(TestLogger::new());
	let network_graph = NetworkGraph::new(Network::Bitcoin, logger.clone());
	let network_graph_arc = Arc::new(network_graph);

	let short_channel_id = 1;
	let timestamp = current_time();
	println!("timestamp: {}", timestamp);

	{ // seed the db
		let (mut persister, receiver) = GossipPersister::new(network_graph_arc.clone(), logger.clone());
		let announcement = generate_announcement(short_channel_id);
		network_graph_arc.update_channel_from_announcement_no_lookup(&announcement).unwrap();
		receiver.send(GossipMessage::ChannelAnnouncement(announcement, None)).await.unwrap();

		{ // direction false
			{ // first update
				let update = generate_update(short_channel_id, false, timestamp - 1, 0, 0, 0, 0, 38);
				network_graph_arc.update_channel_unsigned(&update.contents).unwrap();
				receiver.send(GossipMessage::ChannelUpdate(update, None)).await.unwrap();
			}
			{ // second update
				let update = generate_update(short_channel_id, false, timestamp, 0, 0, 0, 0, 39);
				network_graph_arc.update_channel_unsigned(&update.contents).unwrap();
				receiver.send(GossipMessage::ChannelUpdate(update, None)).await.unwrap();
			}
		}
		{ // direction true
			{ // first and only update
				let update = generate_update(short_channel_id, true, timestamp, 0, 0, 0, 0, 10);
				network_graph_arc.update_channel_unsigned(&update.contents).unwrap();
				receiver.send(GossipMessage::ChannelUpdate(update, None)).await.unwrap();
			}
		}

		drop(receiver);
		persister.persist_gossip().await;
	}

	let client_graph = NetworkGraph::new(Network::Bitcoin, logger.clone());
	let client_graph_arc = Arc::new(client_graph);

	{ // sync after initial seed
		let serialization = serialize_delta(network_graph_arc.clone(), 0, logger.clone()).await;
		logger.assert_log_contains("rapid_gossip_sync_server", "announcement channel count: 1", 1);

		let channel_count = network_graph_arc.read_only().channels().len();

		assert_eq!(channel_count, 1);
		assert_eq!(serialization.message_count, 3);
		assert_eq!(serialization.announcement_count, 1);
		assert_eq!(serialization.update_count, 2);

		let rgs = RapidGossipSync::new(client_graph_arc.clone(), logger.clone());
		let update_result = rgs.update_network_graph(&serialization.data).unwrap();
		// the update result must be a multiple of our snapshot granularity
		assert_eq!(update_result % config::snapshot_generation_interval(), 0);
		assert!(update_result < timestamp);

		let readonly_graph = client_graph_arc.read_only();
		let channels = readonly_graph.channels();
		let client_channel_count = channels.len();
		assert_eq!(client_channel_count, 1);

		let first_channel = channels.get(&short_channel_id).unwrap();
		assert!(&first_channel.announcement_message.is_none());
		// ensure the update in one direction shows the latest fee
		assert_eq!(first_channel.one_to_two.as_ref().unwrap().fees.proportional_millionths, 39);
		assert_eq!(first_channel.two_to_one.as_ref().unwrap().fees.proportional_millionths, 10);
	}

	clean_test_db().await;
}

#[tokio::test]
async fn test_full_snapshot_recency_with_wrong_seen_order() {
	let _sanitizer = SchemaSanitizer::new();
	let logger = Arc::new(TestLogger::new());
	let network_graph = NetworkGraph::new(Network::Bitcoin, logger.clone());
	let network_graph_arc = Arc::new(network_graph);

	let short_channel_id = 1;
	let timestamp = current_time();
	println!("timestamp: {}", timestamp);

	{ // seed the db
		let (mut persister, receiver) = GossipPersister::new(network_graph_arc.clone(), logger.clone());
		let announcement = generate_announcement(short_channel_id);
		network_graph_arc.update_channel_from_announcement_no_lookup(&announcement).unwrap();
		receiver.send(GossipMessage::ChannelAnnouncement(announcement, None)).await.unwrap();

		{ // direction false
			{ // first update, seen latest
				let update = generate_update(short_channel_id, false, timestamp - 1, 0, 0, 0, 0, 38);
				network_graph_arc.update_channel_unsigned(&update.contents).unwrap();
				receiver.send(GossipMessage::ChannelUpdate(update, Some(timestamp))).await.unwrap();
			}
			{ // second update, seen first
				let update = generate_update(short_channel_id, false, timestamp, 0, 0, 0, 0, 39);
				network_graph_arc.update_channel_unsigned(&update.contents).unwrap();
				receiver.send(GossipMessage::ChannelUpdate(update, Some(timestamp - 1))).await.unwrap();
			}
		}
		{ // direction true
			{ // first and only update
				let update = generate_update(short_channel_id, true, timestamp, 0, 0, 0, 0, 10);
				network_graph_arc.update_channel_unsigned(&update.contents).unwrap();
				receiver.send(GossipMessage::ChannelUpdate(update, None)).await.unwrap();
			}
		}

		drop(receiver);
		persister.persist_gossip().await;
	}

	let client_graph = NetworkGraph::new(Network::Bitcoin, logger.clone());
	let client_graph_arc = Arc::new(client_graph);

	{ // sync after initial seed
		let serialization = serialize_delta(network_graph_arc.clone(), 0, logger.clone()).await;
		logger.assert_log_contains("rapid_gossip_sync_server", "announcement channel count: 1", 1);

		let channel_count = network_graph_arc.read_only().channels().len();

		assert_eq!(channel_count, 1);
		assert_eq!(serialization.message_count, 3);
		assert_eq!(serialization.announcement_count, 1);
		assert_eq!(serialization.update_count, 2);

		let rgs = RapidGossipSync::new(client_graph_arc.clone(), logger.clone());
		let update_result = rgs.update_network_graph(&serialization.data).unwrap();
		// the update result must be a multiple of our snapshot granularity
		assert_eq!(update_result % config::snapshot_generation_interval(), 0);
		assert!(update_result < timestamp);

		let readonly_graph = client_graph_arc.read_only();
		let channels = readonly_graph.channels();
		let client_channel_count = channels.len();
		assert_eq!(client_channel_count, 1);

		let first_channel = channels.get(&short_channel_id).unwrap();
		assert!(&first_channel.announcement_message.is_none());
		// ensure the update in one direction shows the latest fee
		assert_eq!(first_channel.one_to_two.as_ref().unwrap().fees.proportional_millionths, 39);
		assert_eq!(first_channel.two_to_one.as_ref().unwrap().fees.proportional_millionths, 10);
	}

	clean_test_db().await;
}

#[tokio::test]
async fn test_full_snapshot_recency_with_wrong_propagation_order() {
	let _sanitizer = SchemaSanitizer::new();
	let logger = Arc::new(TestLogger::new());
	let network_graph = NetworkGraph::new(Network::Bitcoin, logger.clone());
	let network_graph_arc = Arc::new(network_graph);

	let short_channel_id = 1;
	let timestamp = current_time();
	println!("timestamp: {}", timestamp);

	{ // seed the db
		let (mut persister, receiver) = GossipPersister::new(network_graph_arc.clone(), logger.clone());
		let announcement = generate_announcement(short_channel_id);
		network_graph_arc.update_channel_from_announcement_no_lookup(&announcement).unwrap();
		receiver.send(GossipMessage::ChannelAnnouncement(announcement, None)).await.unwrap();

		{ // direction false
			// apply updates in their timestamp order
			let update_1 = generate_update(short_channel_id, false, timestamp - 1, 0, 0, 0, 0, 38);
			let update_2 = generate_update(short_channel_id, false, timestamp, 0, 0, 0, 0, 39);
			network_graph_arc.update_channel_unsigned(&update_1.contents).unwrap();
			network_graph_arc.update_channel_unsigned(&update_2.contents).unwrap();

			// propagate updates in their seen order
			receiver.send(GossipMessage::ChannelUpdate(update_2, Some(timestamp - 1))).await.unwrap();
			receiver.send(GossipMessage::ChannelUpdate(update_1, Some(timestamp))).await.unwrap();
		}
		{ // direction true
			{ // first and only update
				let update = generate_update(short_channel_id, true, timestamp, 0, 0, 0, 0, 10);
				network_graph_arc.update_channel_unsigned(&update.contents).unwrap();
				receiver.send(GossipMessage::ChannelUpdate(update, None)).await.unwrap();
			}
		}

		drop(receiver);
		persister.persist_gossip().await;
	}

	let client_graph = NetworkGraph::new(Network::Bitcoin, logger.clone());
	let client_graph_arc = Arc::new(client_graph);

	{ // sync after initial seed
		let serialization = serialize_delta(network_graph_arc.clone(), 0, logger.clone()).await;
		logger.assert_log_contains("rapid_gossip_sync_server", "announcement channel count: 1", 1);

		let channel_count = network_graph_arc.read_only().channels().len();

		assert_eq!(channel_count, 1);
		assert_eq!(serialization.message_count, 3);
		assert_eq!(serialization.announcement_count, 1);
		assert_eq!(serialization.update_count, 2);

		let rgs = RapidGossipSync::new(client_graph_arc.clone(), logger.clone());
		let update_result = rgs.update_network_graph(&serialization.data).unwrap();
		// the update result must be a multiple of our snapshot granularity
		assert_eq!(update_result % config::snapshot_generation_interval(), 0);
		assert!(update_result < timestamp);

		let readonly_graph = client_graph_arc.read_only();
		let channels = readonly_graph.channels();
		let client_channel_count = channels.len();
		assert_eq!(client_channel_count, 1);

		let first_channel = channels.get(&short_channel_id).unwrap();
		assert!(&first_channel.announcement_message.is_none());
		// ensure the update in one direction shows the latest fee
		assert_eq!(first_channel.one_to_two.as_ref().unwrap().fees.proportional_millionths, 39);
		assert_eq!(first_channel.two_to_one.as_ref().unwrap().fees.proportional_millionths, 10);
	}

	clean_test_db().await;
}

#[tokio::test]
async fn test_full_snapshot_mutiny_scenario() {
	let _sanitizer = SchemaSanitizer::new();
	let logger = Arc::new(TestLogger::new());
	let network_graph = NetworkGraph::new(Network::Bitcoin, logger.clone());
	let network_graph_arc = Arc::new(network_graph);

	let short_channel_id = 873706024403271681;
	let timestamp = current_time();
	// let oldest_simulation_timestamp = 1693300588;
	let latest_simulation_timestamp = 1695909301;
	let timestamp_offset = timestamp - latest_simulation_timestamp;
	println!("timestamp: {}", timestamp);

	{ // seed the db
		let (mut persister, receiver) = GossipPersister::new(network_graph_arc.clone(), logger.clone());
		let announcement = generate_announcement(short_channel_id);
		network_graph_arc.update_channel_from_announcement_no_lookup(&announcement).unwrap();
		receiver.send(GossipMessage::ChannelAnnouncement(announcement, None)).await.unwrap();

		{ // direction false
			{
				let update = generate_update(short_channel_id, false, 1693507369 + timestamp_offset, 0, 0, 0, 0, 38);
				network_graph_arc.update_channel_unsigned(&update.contents).unwrap();
				receiver.send(GossipMessage::ChannelUpdate(update, None)).await.unwrap();
			}
			{
				let update = generate_update(short_channel_id, false, 1693680390 + timestamp_offset, 0, 0, 0, 0, 38);
				network_graph_arc.update_channel_unsigned(&update.contents).unwrap();
				receiver.send(GossipMessage::ChannelUpdate(update, None)).await.unwrap();
			}
			{
				let update = generate_update(short_channel_id, false, 1693749109 + timestamp_offset, 0, 0, 0, 0, 200);
				network_graph_arc.update_channel_unsigned(&update.contents).unwrap();
				receiver.send(GossipMessage::ChannelUpdate(update, None)).await.unwrap();
			}
			{
				let update = generate_update(short_channel_id, false, 1693925190 + timestamp_offset, 0, 0, 0, 0, 200);
				network_graph_arc.update_channel_unsigned(&update.contents).unwrap();
				receiver.send(GossipMessage::ChannelUpdate(update, None)).await.unwrap();
			}
			{
				let update = generate_update(short_channel_id, false, 1694008323 + timestamp_offset, 0, 0, 0, 0, 209);
				network_graph_arc.update_channel_unsigned(&update.contents).unwrap();
				receiver.send(GossipMessage::ChannelUpdate(update, None)).await.unwrap();
			}
			{
				let update = generate_update(short_channel_id, false, 1694219924 + timestamp_offset, 0, 0, 0, 0, 209);
				network_graph_arc.update_channel_unsigned(&update.contents).unwrap();
				receiver.send(GossipMessage::ChannelUpdate(update, None)).await.unwrap();
			}
			{
				let update = generate_update(short_channel_id, false, 1694267536 + timestamp_offset, 0, 0, 0, 0, 210);
				network_graph_arc.update_channel_unsigned(&update.contents).unwrap();
				receiver.send(GossipMessage::ChannelUpdate(update, None)).await.unwrap();
			}
			{
				let update = generate_update(short_channel_id, false, 1694458808 + timestamp_offset, 0, 0, 0, 0, 210);
				network_graph_arc.update_channel_unsigned(&update.contents).unwrap();
				receiver.send(GossipMessage::ChannelUpdate(update, None)).await.unwrap();
			}
			{
				let update = generate_update(short_channel_id, false, 1694526734 + timestamp_offset, 0, 0, 0, 0, 200);
				network_graph_arc.update_channel_unsigned(&update.contents).unwrap();
				receiver.send(GossipMessage::ChannelUpdate(update, None)).await.unwrap();
			}
			{
				let update = generate_update(short_channel_id, false, 1694794765 + timestamp_offset, 0, 0, 0, 0, 200);
				network_graph_arc.update_channel_unsigned(&update.contents).unwrap();
				receiver.send(GossipMessage::ChannelUpdate(update, Some(1695909301 + 2 * config::SYMLINK_GRANULARITY_INTERVAL + timestamp_offset))).await.unwrap();
			}
			{
				let update = generate_update(short_channel_id, false, 1695909301 + timestamp_offset, 0, 0, 0, 0, 130);
				// network_graph_arc.update_channel_unsigned(&update.contents).unwrap();
				receiver.send(GossipMessage::ChannelUpdate(update, None)).await.unwrap();
			}
		}
		{ // direction true
			{
				let update = generate_update(short_channel_id, true, 1693300588 + timestamp_offset, 0, 0, 0, 0, 10);
				network_graph_arc.update_channel_unsigned(&update.contents).unwrap();
				receiver.send(GossipMessage::ChannelUpdate(update, None)).await.unwrap();
			}
			{
				let update = generate_update(short_channel_id, true, 1695003621 + timestamp_offset, 0, 0, 0, 0, 10);
				network_graph_arc.update_channel_unsigned(&update.contents).unwrap();
				receiver.send(GossipMessage::ChannelUpdate(update, None)).await.unwrap();
			}
		}

		drop(receiver);
		persister.persist_gossip().await;
	}

	let client_graph = NetworkGraph::new(Network::Bitcoin, logger.clone());
	let client_graph_arc = Arc::new(client_graph);

	{ // sync after initial seed
		let serialization = serialize_delta(network_graph_arc.clone(), 0, logger.clone()).await;
		logger.assert_log_contains("rapid_gossip_sync_server", "announcement channel count: 1", 1);

		let channel_count = network_graph_arc.read_only().channels().len();

		assert_eq!(channel_count, 1);
		assert_eq!(serialization.message_count, 3);
		assert_eq!(serialization.announcement_count, 1);
		assert_eq!(serialization.update_count, 2);

		let rgs = RapidGossipSync::new(client_graph_arc.clone(), logger.clone());
		let update_result = rgs.update_network_graph(&serialization.data).unwrap();
		println!("update result: {}", update_result);
		// the update result must be a multiple of our snapshot granularity
		assert_eq!(update_result % config::snapshot_generation_interval(), 0);
		assert!(update_result < timestamp);

		let timestamp_delta = timestamp - update_result;
		println!("timestamp delta: {}", timestamp_delta);
		assert!(timestamp_delta < config::snapshot_generation_interval());

		let readonly_graph = client_graph_arc.read_only();
		let channels = readonly_graph.channels();
		let client_channel_count = channels.len();
		assert_eq!(client_channel_count, 1);

		let first_channel = channels.get(&short_channel_id).unwrap();
		assert!(&first_channel.announcement_message.is_none());
		assert_eq!(first_channel.one_to_two.as_ref().unwrap().fees.proportional_millionths, 130);
		assert_eq!(first_channel.two_to_one.as_ref().unwrap().fees.proportional_millionths, 10);
	}

	clean_test_db().await;
}

#[tokio::test]
async fn test_full_snapshot_interlaced_channel_timestamps() {
	let _sanitizer = SchemaSanitizer::new();
	let logger = Arc::new(TestLogger::new());
	let network_graph = NetworkGraph::new(Network::Bitcoin, logger.clone());
	let network_graph_arc = Arc::new(network_graph);

	let main_channel_id = 1;
	let timestamp = current_time();
	println!("timestamp: {}", timestamp);

	{ // seed the db
		let (mut persister, receiver) = GossipPersister::new(network_graph_arc.clone(), logger.clone());
		let secondary_channel_id = main_channel_id + 1;

		{ // main channel
			let announcement = generate_announcement(main_channel_id);
			network_graph_arc.update_channel_from_announcement_no_lookup(&announcement).unwrap();
			receiver.send(GossipMessage::ChannelAnnouncement(announcement, None)).await.unwrap();
		}

		{ // secondary channel
			let announcement = generate_announcement(secondary_channel_id);
			network_graph_arc.update_channel_from_announcement_no_lookup(&announcement).unwrap();
			receiver.send(GossipMessage::ChannelAnnouncement(announcement, None)).await.unwrap();
		}

		{ // main channel
			{ // direction false
				let update = generate_update(main_channel_id, false, timestamp - 2, 0, 0, 0, 0, 10);
				network_graph_arc.update_channel_unsigned(&update.contents).unwrap();
				receiver.send(GossipMessage::ChannelUpdate(update, None)).await.unwrap();
			}
			{ // direction true
				let update = generate_update(main_channel_id, true, timestamp - 2, 0, 0, 0, 0, 5);
				network_graph_arc.update_channel_unsigned(&update.contents).unwrap();
				receiver.send(GossipMessage::ChannelUpdate(update, None)).await.unwrap();
			}
		}

		{ // in-between channel
			{ // direction false
				let update = generate_update(secondary_channel_id, false, timestamp - 1, 0, 0, 0, 0, 42);
				network_graph_arc.update_channel_unsigned(&update.contents).unwrap();
				receiver.send(GossipMessage::ChannelUpdate(update, None)).await.unwrap();
			}
			{ // direction true
				let update = generate_update(secondary_channel_id, true, timestamp - 1, 0, 0, 0, 0, 42);
				network_graph_arc.update_channel_unsigned(&update.contents).unwrap();
				receiver.send(GossipMessage::ChannelUpdate(update, None)).await.unwrap();
			}
		}

		{ // main channel
			{ // direction false
				let update = generate_update(main_channel_id, false, timestamp, 0, 0, 0, 0, 11);
				network_graph_arc.update_channel_unsigned(&update.contents).unwrap();
				receiver.send(GossipMessage::ChannelUpdate(update, None)).await.unwrap();
			}
			{ // direction true
				let update = generate_update(main_channel_id, true, timestamp, 0, 0, 0, 0, 6);
				network_graph_arc.update_channel_unsigned(&update.contents).unwrap();
				receiver.send(GossipMessage::ChannelUpdate(update, None)).await.unwrap();
			}
		}

		drop(receiver);
		persister.persist_gossip().await;
	}

	let client_graph = NetworkGraph::new(Network::Bitcoin, logger.clone());
	let client_graph_arc = Arc::new(client_graph);

	{ // sync after initial seed
		let serialization = serialize_delta(network_graph_arc.clone(), 0, logger.clone()).await;
		logger.assert_log_contains("rapid_gossip_sync_server", "announcement channel count: 2", 1);

		let channel_count = network_graph_arc.read_only().channels().len();

		assert_eq!(channel_count, 2);
		assert_eq!(serialization.message_count, 6);
		assert_eq!(serialization.announcement_count, 2);
		assert_eq!(serialization.update_count, 4);

		let rgs = RapidGossipSync::new(client_graph_arc.clone(), logger.clone());
		let update_result = rgs.update_network_graph(&serialization.data).unwrap();
		// the update result must be a multiple of our snapshot granularity
		assert_eq!(update_result % config::snapshot_generation_interval(), 0);
		assert!(update_result < timestamp);

		let readonly_graph = client_graph_arc.read_only();
		let channels = readonly_graph.channels();
		let client_channel_count = channels.len();
		assert_eq!(client_channel_count, 2);

		let first_channel = channels.get(&main_channel_id).unwrap();
		assert!(&first_channel.announcement_message.is_none());
		// ensure the update in one direction shows the latest fee
		assert_eq!(first_channel.one_to_two.as_ref().unwrap().fees.proportional_millionths, 11);
		assert_eq!(first_channel.two_to_one.as_ref().unwrap().fees.proportional_millionths, 6);
	}

	clean_test_db().await;
}

#[tokio::test]
async fn test_full_snapshot_persistence() {
	let schema_sanitizer = SchemaSanitizer::new();
	let logger = Arc::new(TestLogger::new());
	let network_graph = NetworkGraph::new(Network::Bitcoin, logger.clone());
	let network_graph_arc = Arc::new(network_graph);
	let snapshotter = Snapshotter::new(network_graph_arc.clone(), logger.clone());
	let cache_sanitizer = CacheSanitizer::new(&schema_sanitizer);

	let short_channel_id = 1;
	let timestamp = current_time();
	println!("timestamp: {}", timestamp);

	{ // seed the db
		let (mut persister, receiver) = GossipPersister::new(network_graph_arc.clone(), logger.clone());
		let announcement = generate_announcement(short_channel_id);
		network_graph_arc.update_channel_from_announcement_no_lookup(&announcement).unwrap();
		receiver.send(GossipMessage::ChannelAnnouncement(announcement, None)).await.unwrap();

		{ // direction true
			let update = generate_update(short_channel_id, true, timestamp, 0, 0, 0, 0, 10);
			network_graph_arc.update_channel_unsigned(&update.contents).unwrap();
			receiver.send(GossipMessage::ChannelUpdate(update, None)).await.unwrap();
		}

		{ // direction false
			let update = generate_update(short_channel_id, false, timestamp - 1, 0, 0, 0, 0, 38);
			network_graph_arc.update_channel_unsigned(&update.contents).unwrap();
			receiver.send(GossipMessage::ChannelUpdate(update, None)).await.unwrap();
		}

		drop(receiver);
		persister.persist_gossip().await;
	}

	let cache_path = cache_sanitizer.cache_path();
	let symlink_path = format!("{}/symlinks/0.bin", cache_path);

	// generate snapshots
	{
		snapshotter.generate_snapshots(20, 5, &[5, u64::MAX], &cache_path, Some(10)).await;

		let symlinked_data = fs::read(&symlink_path).unwrap();
		let client_graph = NetworkGraph::new(Network::Bitcoin, logger.clone());
		let client_graph_arc = Arc::new(client_graph);

		let rgs = RapidGossipSync::new(client_graph_arc.clone(), logger.clone());
		let update_result = rgs.update_network_graph(&symlinked_data).unwrap();
		// the update result must be a multiple of our snapshot granularity
		assert_eq!(update_result % config::snapshot_generation_interval(), 0);

		let readonly_graph = client_graph_arc.read_only();
		let channels = readonly_graph.channels();
		let client_channel_count = channels.len();
		assert_eq!(client_channel_count, 1);

		let first_channel = channels.get(&short_channel_id).unwrap();
		assert!(&first_channel.announcement_message.is_none());
		// ensure the update in one direction shows the latest fee
		assert_eq!(first_channel.one_to_two.as_ref().unwrap().fees.proportional_millionths, 38);
		assert_eq!(first_channel.two_to_one.as_ref().unwrap().fees.proportional_millionths, 10);
	}

	{ // update the db
		let (mut persister, receiver) = GossipPersister::new(network_graph_arc.clone(), logger.clone());

		{ // second update
			let update = generate_update(short_channel_id, false, timestamp + 30, 0, 0, 0, 0, 39);
			network_graph_arc.update_channel_unsigned(&update.contents).unwrap();
			receiver.send(GossipMessage::ChannelUpdate(update, None)).await.unwrap();
		}

		drop(receiver);
		persister.persist_gossip().await;
	}

	// regenerate snapshots
	{
		snapshotter.generate_snapshots(20, 5, &[5, u64::MAX], &cache_path, Some(10)).await;

		let symlinked_data = fs::read(&symlink_path).unwrap();
		let client_graph = NetworkGraph::new(Network::Bitcoin, logger.clone());
		let client_graph_arc = Arc::new(client_graph);

		let rgs = RapidGossipSync::new(client_graph_arc.clone(), logger.clone());
		let update_result = rgs.update_network_graph(&symlinked_data).unwrap();
		// the update result must be a multiple of our snapshot granularity
		assert_eq!(update_result % config::snapshot_generation_interval(), 0);

		let readonly_graph = client_graph_arc.read_only();
		let channels = readonly_graph.channels();
		let client_channel_count = channels.len();
		assert_eq!(client_channel_count, 1);

		let first_channel = channels.get(&short_channel_id).unwrap();
		assert!(&first_channel.announcement_message.is_none());
		// ensure the update in one direction shows the latest fee
		assert_eq!(first_channel.one_to_two.as_ref().unwrap().fees.proportional_millionths, 39);
		assert_eq!(first_channel.two_to_one.as_ref().unwrap().fees.proportional_millionths, 10);
	}

	// clean up afterwards
	clean_test_db().await;
}

'''
'''--- src/tracking.rs ---
use std::collections::hash_map::RandomState;
use std::hash::{BuildHasher, Hasher};
use std::net::SocketAddr;
use std::ops::Deref;
use std::sync::Arc;
use std::time::{Duration, Instant};

use bitcoin::hashes::hex::ToHex;
use bitcoin::secp256k1::PublicKey;
use lightning;
use lightning::ln::peer_handler::{
	ErroringMessageHandler, IgnoringMessageHandler, MessageHandler, PeerManager,
};
use lightning::{log_info, log_warn};
use lightning::routing::gossip::NetworkGraph;
use lightning::sign::KeysManager;
use lightning::util::logger::Logger;
use tokio::sync::mpsc;
use tokio::task::JoinSet;

use crate::config;
use crate::downloader::GossipRouter;
use crate::types::{GossipMessage, GossipPeerManager};

pub(crate) async fn download_gossip<L: Deref + Clone + Send + Sync + 'static>(persistence_sender: mpsc::Sender<GossipMessage>,
	completion_sender: mpsc::Sender<()>,
	network_graph: Arc<NetworkGraph<L>>,
	logger: L,
) where L::Target: Logger {
	let mut key = [42; 32];
	let mut random_data = [43; 32];
	// Get something psuedo-random from std.
	let mut key_hasher = RandomState::new().build_hasher();
	key_hasher.write_u8(1);
	key[0..8].copy_from_slice(&key_hasher.finish().to_ne_bytes());
	let mut rand_hasher = RandomState::new().build_hasher();
	rand_hasher.write_u8(2);
	random_data[0..8].copy_from_slice(&rand_hasher.finish().to_ne_bytes());

	let keys_manager = Arc::new(KeysManager::new(&key, 0xdeadbeef, 0xdeadbeef));

	let router = Arc::new(GossipRouter::new(network_graph, persistence_sender.clone(), logger.clone()));

	let message_handler = MessageHandler {
		chan_handler: ErroringMessageHandler::new(),
		route_handler: Arc::clone(&router),
		onion_message_handler: IgnoringMessageHandler {},
		custom_message_handler: IgnoringMessageHandler {},
	};
	let peer_handler = Arc::new(PeerManager::new(
		message_handler,
		0xdeadbeef,
		&random_data,
		logger.clone(),
		keys_manager,
	));
	router.set_pm(Arc::clone(&peer_handler));

	let ph_timer = Arc::clone(&peer_handler);
	tokio::spawn(async move {
		let mut intvl = tokio::time::interval(Duration::from_secs(10));
		loop {
			intvl.tick().await;
			ph_timer.timer_tick_occurred();
		}
	});

	log_info!(logger, "Connecting to Lightning peers...");
	let peers = config::ln_peers();
	let mut handles = JoinSet::new();
	let mut connected_peer_count = 0;

	if peers.len() <= config::CONNECTED_PEER_ASSERTION_LIMIT {
		log_warn!(logger, "Peer assertion threshold is {}, but only {} peers specified.", config::CONNECTED_PEER_ASSERTION_LIMIT, peers.len());
	}

	for current_peer in peers {
		let peer_handler_clone = peer_handler.clone();
		let logger_clone = logger.clone();
		handles.spawn(async move {
			connect_peer(current_peer, peer_handler_clone, logger_clone).await
		});
	}

	while let Some(connection_result) = handles.join_next().await {
		if let Ok(connection) = connection_result {
			if connection {
				connected_peer_count += 1;
				if connected_peer_count >= config::CONNECTED_PEER_ASSERTION_LIMIT {
					break;
				}
			}
		}
	}

	if connected_peer_count < 1 {
		panic!("Failed to connect to any peer.");
	}

	log_info!(logger, "Connected to {} Lightning peers!", connected_peer_count);

	let mut previous_announcement_count = 0u64;
	let mut previous_update_count = 0u64;
	let mut is_caught_up_with_gossip = false;

	let mut i = 0u32;
	let mut latest_new_gossip_time = Instant::now();
	let mut needs_to_notify_persister = false;

	loop {
		i += 1; // count the background activity
		let sleep = tokio::time::sleep(Duration::from_secs(5));
		sleep.await;

		{
			let counter = router.counter.read().unwrap();
			let total_message_count = counter.channel_announcements + counter.channel_updates;
			let new_message_count = total_message_count - previous_announcement_count - previous_update_count;

			let was_previously_caught_up_with_gossip = is_caught_up_with_gossip;
			// TODO: make new message threshold (20) adjust based on connected peer count
			is_caught_up_with_gossip = new_message_count < 20 && previous_announcement_count > 0 && previous_update_count > 0;
			if new_message_count > 0 {
				latest_new_gossip_time = Instant::now();
			}

			// if we either aren't caught up, or just stopped/started being caught up
			if !is_caught_up_with_gossip || (is_caught_up_with_gossip != was_previously_caught_up_with_gossip) {
				log_info!(
					logger,
					"gossip count (iteration {}): {} (delta: {}):\n\tannouncements: {}\n\t\tmismatched scripts: {}\n\tupdates: {}\n\t\tno HTLC max: {}\n",
					i,
					total_message_count,
					new_message_count,
					counter.channel_announcements,
					counter.channel_announcements_with_mismatched_scripts,
					counter.channel_updates,
					counter.channel_updates_without_htlc_max_msats
				);
			} else {
				log_info!(logger, "Monitoring for gossip…")
			}

			if is_caught_up_with_gossip && !was_previously_caught_up_with_gossip {
				log_info!(logger, "caught up with gossip!");
				needs_to_notify_persister = true;
			} else if !is_caught_up_with_gossip && was_previously_caught_up_with_gossip {
				log_info!(logger, "Received new messages since catching up with gossip!");
			}

			let continuous_caught_up_duration = latest_new_gossip_time.elapsed();
			if continuous_caught_up_duration.as_secs() > 600 {
				log_warn!(logger, "No new gossip messages in 10 minutes! Something's amiss!");
			}

			previous_announcement_count = counter.channel_announcements;
			previous_update_count = counter.channel_updates;
		}

		if needs_to_notify_persister {
			needs_to_notify_persister = false;
			completion_sender.send(()).await.unwrap();
		}
	}
}

async fn connect_peer<L: Deref + Clone + Send + Sync + 'static>(current_peer: (PublicKey, SocketAddr), peer_manager: GossipPeerManager<L>, logger: L) -> bool where L::Target: Logger {
	// we seek to find out if the first connection attempt was successful
	let (sender, mut receiver) = mpsc::channel::<bool>(1);
	tokio::spawn(async move {
		log_info!(logger, "Connecting to peer {}@{}...", current_peer.0.to_hex(), current_peer.1.to_string());
		let mut is_first_iteration = true;
		loop {
			if let Some(disconnection_future) = lightning_net_tokio::connect_outbound(
				Arc::clone(&peer_manager),
				current_peer.0,
				current_peer.1,
			).await {
				log_info!(logger, "Connected to peer {}@{}!", current_peer.0.to_hex(), current_peer.1.to_string());
				if is_first_iteration {
					sender.send(true).await.unwrap();
				}
				disconnection_future.await;
				log_warn!(logger, "Disconnected from peer {}@{}", current_peer.0.to_hex(), current_peer.1.to_string());
			} else {
				log_warn!(logger, "Failed to connect to peer {}@{}!", current_peer.0.to_hex(), current_peer.1.to_string());
				if is_first_iteration {
					sender.send(false).await.unwrap();
				}
			}
			is_first_iteration = false;
			tokio::time::sleep(Duration::from_secs(10)).await;
			log_warn!(logger, "Reconnecting to peer {}@{}...", current_peer.0.to_hex(), current_peer.1.to_string());
		}
	});

	let success = receiver.recv().await.unwrap();
	success
}

'''
'''--- src/types.rs ---
use std::sync::Arc;

use lightning::sign::KeysManager;
use lightning::ln::msgs::{ChannelAnnouncement, ChannelUpdate};
use lightning::ln::peer_handler::{ErroringMessageHandler, IgnoringMessageHandler, PeerManager};
use lightning::util::logger::{Logger, Record};
use crate::config;

use crate::downloader::GossipRouter;
use crate::verifier::ChainVerifier;

pub(crate) type GossipChainAccess<L> = Arc<ChainVerifier<L>>;
pub(crate) type GossipPeerManager<L> = Arc<PeerManager<lightning_net_tokio::SocketDescriptor, ErroringMessageHandler, Arc<GossipRouter<L>>, IgnoringMessageHandler, L, IgnoringMessageHandler, Arc<KeysManager>>>;

#[derive(Debug)]
pub(crate) enum GossipMessage {
	// the second element is an optional override for the seen value
	ChannelAnnouncement(ChannelAnnouncement, Option<u32>),
	ChannelUpdate(ChannelUpdate, Option<u32>),
}

#[derive(Clone, Copy)]
pub struct RGSSLogger {}

impl RGSSLogger {
	pub fn new() -> RGSSLogger {
		Self {}
	}
}

impl Logger for RGSSLogger {
	fn log(&self, record: &Record) {
		let threshold = config::log_level();
		if record.level < threshold {
			return;
		}
		println!("{:<5} [{} : {}, {}] {}", record.level.to_string(), record.module_path, record.file, record.line, record.args);
	}
}

#[cfg(test)]
pub mod tests {
	use std::collections::HashMap;
	use std::sync::{Mutex};
	use lightning::util::logger::{Level, Logger, Record};

	pub struct TestLogger {
		level: Level,
		pub(crate) id: String,
		pub lines: Mutex<HashMap<(String, String), usize>>,
	}

	impl TestLogger {
		pub fn new() -> TestLogger {
			let id = crate::tests::db_test_schema();
			Self::with_id(id)
		}
		pub fn with_id(id: String) -> TestLogger {
			TestLogger {
				level: Level::Gossip,
				id,
				lines: Mutex::new(HashMap::new()),
			}
		}
		pub fn enable(&mut self, level: Level) {
			self.level = level;
		}
		pub fn assert_log(&self, module: String, line: String, count: usize) {
			let log_entries = self.lines.lock().unwrap();
			assert_eq!(log_entries.get(&(module, line)), Some(&count));
		}

		/// Search for the number of occurrence of the logged lines which
		/// 1. belongs to the specified module and
		/// 2. contains `line` in it.
		/// And asserts if the number of occurrences is the same with the given `count`
		pub fn assert_log_contains(&self, module: &str, line: &str, count: usize) {
			let log_entries = self.lines.lock().unwrap();
			let l: usize = log_entries.iter().filter(|&(&(ref m, ref l), _c)| {
				m == module && l.contains(line)
			}).map(|(_, c)| { c }).sum();
			assert_eq!(l, count)
		}
	}

	impl Logger for TestLogger {
		fn log(&self, record: &Record) {
			*self.lines.lock().unwrap().entry((record.module_path.to_string(), format!("{}", record.args))).or_insert(0) += 1;
			println!("{:<5} {} [{} : {}, {}] {}", record.level.to_string(), self.id, record.module_path, record.file, record.line, record.args);
		}
	}
}

'''
'''--- src/verifier.rs ---
use std::convert::TryInto;
use std::ops::Deref;
use std::sync::Arc;
use std::sync::Mutex;

use bitcoin::{BlockHash, TxOut};
use bitcoin::blockdata::block::Block;
use bitcoin::hashes::Hash;
use lightning::log_error;
use lightning::routing::gossip::{NetworkGraph, P2PGossipSync};
use lightning::routing::utxo::{UtxoFuture, UtxoLookup, UtxoResult, UtxoLookupError};
use lightning::util::logger::Logger;
use lightning_block_sync::{BlockData, BlockSource};
use lightning_block_sync::http::BinaryResponse;
use lightning_block_sync::rest::RestClient;

use crate::config;
use crate::types::GossipPeerManager;

pub(crate) struct ChainVerifier<L: Deref + Clone + Send + Sync + 'static> where L::Target: Logger {
	rest_client: Arc<RestClient>,
	graph: Arc<NetworkGraph<L>>,
	outbound_gossiper: Arc<P2PGossipSync<Arc<NetworkGraph<L>>, Arc<Self>, L>>,
	peer_handler: Mutex<Option<GossipPeerManager<L>>>,
	logger: L
}

struct RestBinaryResponse(Vec<u8>);

impl<L: Deref + Clone + Send + Sync + 'static> ChainVerifier<L> where L::Target: Logger {
	pub(crate) fn new(graph: Arc<NetworkGraph<L>>, outbound_gossiper: Arc<P2PGossipSync<Arc<NetworkGraph<L>>, Arc<Self>, L>>, logger: L) -> Self {
		ChainVerifier {
			rest_client: Arc::new(RestClient::new(config::bitcoin_rest_endpoint()).unwrap()),
			outbound_gossiper,
			graph,
			peer_handler: Mutex::new(None),
			logger
		}
	}
	pub(crate) fn set_ph(&self, peer_handler: GossipPeerManager<L>) {
		*self.peer_handler.lock().unwrap() = Some(peer_handler);
	}

	async fn retrieve_utxo(client: Arc<RestClient>, short_channel_id: u64, logger: L) -> Result<TxOut, UtxoLookupError> {
		let block_height = (short_channel_id >> 5 * 8) as u32; // block height is most significant three bytes
		let transaction_index = ((short_channel_id >> 2 * 8) & 0xffffff) as u32;
		let output_index = (short_channel_id & 0xffff) as u16;

		let mut block = Self::retrieve_block(client, block_height, logger.clone()).await?;
		if transaction_index as usize >= block.txdata.len() {
			log_error!(logger, "Could't find transaction {} in block {}", transaction_index, block_height);
			return Err(UtxoLookupError::UnknownTx);
		}
		let mut transaction = block.txdata.swap_remove(transaction_index as usize);
		if output_index as usize >= transaction.output.len() {
			log_error!(logger, "Could't find output {} in transaction {}", output_index, transaction.txid());
			return Err(UtxoLookupError::UnknownTx);
		}
		Ok(transaction.output.swap_remove(output_index as usize))
	}

	async fn retrieve_block(client: Arc<RestClient>, block_height: u32, logger: L) -> Result<Block, UtxoLookupError> {
		let uri = format!("blockhashbyheight/{}.bin", block_height);
		let block_hash_result =
			client.request_resource::<BinaryResponse, RestBinaryResponse>(&uri).await;
		let block_hash: Vec<u8> = block_hash_result.map_err(|error| {
			log_error!(logger, "Could't find block hash at height {}: {}", block_height, error.to_string());
			UtxoLookupError::UnknownChain
		})?.0;
		let block_hash = BlockHash::from_slice(&block_hash).unwrap();

		let block_result = client.get_block(&block_hash).await;
		match block_result {
			Ok(BlockData::FullBlock(block)) => {
				Ok(block)
			},
			Ok(_) => unreachable!(),
			Err(error) => {
				log_error!(logger, "Couldn't retrieve block {}: {:?} ({})", block_height, error, block_hash);
				Err(UtxoLookupError::UnknownChain)
			}
		}
	}
}

impl<L: Deref + Clone + Send + Sync + 'static> UtxoLookup for ChainVerifier<L> where L::Target: Logger {
	fn get_utxo(&self, _genesis_hash: &BlockHash, short_channel_id: u64) -> UtxoResult {
		let res = UtxoFuture::new();
		let fut = res.clone();
		let graph_ref = Arc::clone(&self.graph);
		let client_ref = Arc::clone(&self.rest_client);
		let gossip_ref = Arc::clone(&self.outbound_gossiper);
		let pm_ref = self.peer_handler.lock().unwrap().clone();
		let logger_ref = self.logger.clone();
		tokio::spawn(async move {
			let res = Self::retrieve_utxo(client_ref, short_channel_id, logger_ref).await;
			fut.resolve(&*graph_ref, &*gossip_ref, res);
			if let Some(pm) = pm_ref { pm.process_events(); }
		});
		UtxoResult::Async(res)
	}
}

impl TryInto<RestBinaryResponse> for BinaryResponse {
	type Error = std::io::Error;

	fn try_into(self) -> Result<RestBinaryResponse, Self::Error> {
		Ok(RestBinaryResponse(self.0))
	}
}

'''