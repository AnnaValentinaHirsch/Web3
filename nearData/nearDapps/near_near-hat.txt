*GitHub Repository "near/near-hat"*

'''--- .github/ISSUE_TEMPLATE/BOUNTY.yml ---
name: "Simple Bounty"
description: "Use this template to create a HEROES Simple Bounty via Github bot"
title: "Bounty: "
labels: ["bounty"]
assignees: heroes-bot-test
body:
  - type: markdown
    attributes:
      value: |
        Hi! Let's set up your bounty! Please don't change the template - @heroes-bot-test won't be able to help you.

  - type: dropdown
    id: type
    attributes:
      label: What talent are you looking for?
      options:
        - Marketing
        - Development
        - Design
        - Other
        - Content
        - Research
        - Audit

  - type: textarea
    id: description
    attributes:
      label: What you need to be done?

  - type: dropdown
    id: tags
    attributes:
      label: Tags
      description: Add tags that match the topic of the work
      multiple: true
      options:
        - API
        - Blockchain
        - Community
        - CSS
        - DAO
        - dApp
        - DeFi
        - Design
        - Documentation
        - HTML
        - Javascript
        - NFT
        - React
        - Rust
        - Smart contract
        - Typescript
        - UI/UX
        - web3
        - Translation
        - Illustration
        - Branding
        - Copywriting
        - Blogging
        - Editing
        - Video Creation
        - Social Media
        - Graphic Design
        - Transcription
        - Product Design
        - Artificial Intelligence
        - Quality Assurance
        - Risk Assessment
        - Security Audit
        - Bug Bounty
        - Code Review
        - Blockchain Security
        - Smart Contract Testing
        - Penetration Testing
        - Vulnerability Assessment
        - BOS
        - News
        - Hackathon
        - NEARCON2023
        - NEARWEEK

  - type: input
    id: deadline
    attributes:
      label: Deadline
      description: "Set a deadline for your bounty. Please enter the date in format: DD.MM.YYYY"
      placeholder: "19.05.2027"

  - type: dropdown
    id: currencyType
    attributes:
      label: Currency
      description: What is the currency you want to pay?
      options:
        - USDC.e
        - USDT.e
        - DAI
        - wNEAR
        - USDt
        - XP
        - marmaj
        - NEKO
        - JUMP
        - USDC
        - NEARVIDIA
      default: 0
    validations:
      required: true

  - type: input
    id: currencyAmount
    attributes:
      label: Amount
      description: How much it will be cost?

  - type: markdown
    attributes:
      value: "## Advanced settings"

  - type: checkboxes
    id: kyc
    attributes:
      label: KYC
      description: "Use HEROES' KYC Verification, only applicants who passed HEROES' KYC can apply and work on this bounty!"
      options:
        - label: Use KYC Verification

  - type: markdown
    attributes:
      value: |
        ### This cannot be changed once the bounty is live!

'''
'''--- Cargo.toml ---
[workspace]
members = ["near-hat", "near-hat-cli"]
exclude = ["examples/"]

'''
'''--- README.md ---
# NEARHat

NEARHat is a NEAR Protocol local development environment.
It allows you to run local development of dApps and create
automated end-to-end tests from smart contracts to indexers.

![NEARHat-Logo](https://github.com/near/near-hat/assets/116191277/68326fa2-f9d9-45b4-a332-078b4733d376)

Built by the Pagoda Engineers for the NEAR Ecosystem as part of the December 2023 hackathon.

Currently supports local versions of:
* nearcore sandbox
* NEAR Lake Indexer (+ LocalStack NEAR Lake S3 bucket)
* NEAR Relayer
* Local NEAR Explorer
* Query API

Potential future support:
* Local MyNearWallet
* BOS dependency chain and near.org gateway
* FastAuth

## One line installation:
```
$ ./install.sh
```
This will install dependencies via Homebrew and setup local `.nearhat` domain.

You need to be logged into Github Container Registry (until all docker containers are published to DockerHub): https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry

![image](https://github.com/near/near-hat/assets/116191277/e20331ce-670f-43c2-b4aa-b152d490e328)

## Starting and stopping local environment
```
$ ./start.sh
```

## Forking mainnet smart contracts
NEARHat allows to fork mainnet contracts and refer to them through `http://rpc.nearhat`.
To fork the USDC contract (with account id `17208628f84f5d6ad33f0da3bbbeb27ffcb398eac501a31bd6ad2`) start NEARHat with the following command:
```bash
RUST_BACKTRACE=1 RUST_LOG=info cargo run -p near-hat-cli -- start --contracts-to-spoon 17208628f84f5d6ad33f0da3bbbeb27ffcb398eac501a31bd6ad2
```

'''
'''--- dns.py ---
"""Redirect HTTP requests to another server."""
from mitmproxy import http
import logging
import os

logger = logging.getLogger(__name__)

def proxy_address(flow: http.HTTPFlow) -> tuple[str, int]:
    if flow.request.pretty_host == "lake.nearhat":
        return ("localhost", int(os.getenv('NEARHAT_LAKE_S3_PORT')))
    elif flow.request.pretty_host == "rpc.nearhat":
        return ("localhost", int(os.getenv('NEARHAT_RPC_PORT')))
    elif flow.request.pretty_host == "relayer.nearhat":
        return ("localhost", int(os.getenv('NEARHAT_RELAYER_PORT')))
    elif flow.request.pretty_host == "explorer.nearhat":
        return ("localhost", int(os.getenv('NEARHAT_EXPLORER_UI_PORT')))
    elif flow.request.pretty_host == "playground.nearhat":
        return ("localhost", int(os.getenv('NEARHAT_GRAPHQL_PLAYGROUND_PORT')))
    else:
        return ("localhost", 3000)

def request(flow: http.HTTPFlow) -> None:
    address = proxy_address(flow)
    flow.request.host = address[0]
    flow.request.port = address[1]
'''
'''--- examples/distribute-contract/Cargo.toml ---
[package]
name = "distribute-contract"
version = "0.1.0"
edition = "2021"

[lib]
crate-type = ["cdylib"]

[dependencies]
near-sdk = "4.1.1"
borsh = { version = "1.2.1", features = ["derive"] }

[profile.release]
codegen-units = 1
# Tell `rustc` to optimize for small code size.
opt-level = "z"
lto = true
debug = false
panic = "abort"

'''
'''--- examples/distribute-contract/README.md ---
# Build Guide

1. Run `./build.sh`
2. The resulting contract WASM will be at `./target/wasm32-unknown-unknown/release/distribute_contract.wasm`

'''
'''--- examples/distribute-contract/build.sh ---
#!/bin/bash
set -e
cd "$(dirname $0)"
cargo build --target wasm32-unknown-unknown --release

'''
'''--- examples/distribute-contract/src/lib.rs ---
use near_sdk::borsh::{self, BorshDeserialize, BorshSerialize};
use near_sdk::{env, near_bindgen, require, AccountId, Promise};

#[near_bindgen]
#[derive(BorshDeserialize, BorshSerialize)]
pub struct DistributeContract {
    owner_id: AccountId,
    friends: Vec<AccountId>,
}

impl Default for DistributeContract {
    fn default() -> Self {
        Self {
            owner_id: "near".parse().unwrap(),
            friends: Vec::new(),
        }
    }
}

#[near_bindgen]
impl DistributeContract {
    #[init]
    pub fn init(owner_id: AccountId) -> Self {
        Self {
            owner_id,
            friends: Vec::new(),
        }
    }

    #[payable]
    pub fn deposit(&mut self) {
        require!(
            env::signer_account_id() == self.owner_id,
            "only owner is allowed to make deposits"
        );
    }

    pub fn add_friend(&mut self, friend_id: AccountId) {
        require!(
            env::signer_account_id() == self.owner_id,
            "only owner is allowed to add a friend"
        );
        self.friends.push(friend_id);
    }

    pub fn pay_friends(&mut self) -> Promise {
        require!(
            env::signer_account_id() == self.owner_id,
            "only owner is allowed to pay out friends"
        );
        let per_friend_amount = env::account_balance() / self.friends.len() as u128;
        let mut result_promise: Option<Promise> = None;
        for friend_id in &self.friends {
            let friend_promise = Promise::new(friend_id.clone()).transfer(per_friend_amount);
            if let Some(promise) = result_promise {
                result_promise = Some(promise.and(friend_promise));
            } else {
                result_promise = Some(friend_promise);
            }
        }
        result_promise.unwrap()
    }
}

'''
'''--- hasura/migrations/default/1691364619300_init/down.sql ---
DROP TABLE public.indexer_log_entries;
DROP TABLE public.indexer_state;

'''
'''--- hasura/migrations/default/1691364619300_init/up.sql ---
SET check_function_bodies = false;

CREATE TABLE public.indexer_log_entries (
    id uuid DEFAULT gen_random_uuid() NOT NULL,
    function_name text NOT NULL,
    block_height numeric NOT NULL,
    "timestamp" timestamp without time zone DEFAULT CURRENT_TIMESTAMP,
    message text
);

CREATE TABLE public.indexer_state (
    function_name character varying NOT NULL,
    current_block_height numeric(21,0) NOT NULL,
    status text,
    current_historical_block_height numeric(21,0)
);

ALTER TABLE ONLY public.indexer_log_entries
    ADD CONSTRAINT indexer_log_entries_pkey PRIMARY KEY (id);

ALTER TABLE ONLY public.indexer_state
    ADD CONSTRAINT indexer_state_pkey PRIMARY KEY (function_name);

CREATE INDEX idx_function_name ON indexer_log_entries(function_name);
CREATE INDEX idx_timestamp ON indexer_log_entries("timestamp");

'''
'''--- install.sh ---

#!/bin/bash

brew install --cask docker
brew install rust dnsmasq mitmproxy

mkdir -pv $(brew --prefix)/etc/
echo 'address=/.nearhat/127.0.0.1' >> $(brew --prefix)/etc/dnsmasq.conf
sudo mkdir -v /etc/resolver
sudo bash -c 'echo "nameserver 127.0.0.1" > /etc/resolver/nearhat'
sudo brew services start dnsmasq
'''
'''--- near-hat-cli/Cargo.toml ---
[package]
name = "near-hat-cli"
version = "0.1.0"
edition = "2021"

[dependencies]
anyhow = { version = "1.0", features = ["backtrace"] }
clap = { version = "4.4", features = ["derive", "env"] }
ctrlc = "3.2"
rand = "0.8"
serde = "1.0"
serde_json = "1.0"
tokio = "1.28"
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter"] }

near-hat = { path = "../near-hat" }
near-primitives = "0.17"
near-workspaces = { git = "https://github.com/near/near-workspaces-rs.git", rev = "142893b6e501c5b638b20582d81482e173d4499d" }

'''
'''--- near-hat-cli/src/main.rs ---
use std::cell::RefCell;
use std::fs::File;
use std::io::Write;
use std::rc::Rc;
use std::str::FromStr;
use std::sync::{Arc, Mutex};

use clap::Parser;
use near_hat::{DockerClient, NearHat};
use near_primitives::account::AccessKey;
use near_primitives::types::AccountId;
use near_workspaces::types::{PublicKey, KeyType};
use near_workspaces::{network::Sandbox, types::SecretKey};
use near_workspaces::{Worker, Contract};
use tokio::io::{stdin, AsyncReadExt};
use tracing_subscriber::EnvFilter;
use serde_json::{json, Value};
extern crate ctrlc;

#[derive(Parser, Debug)]
pub enum Cli {
    Start {
        /// Contracts to spoon from mainnet.
        #[arg(long, value_parser, num_args = 1.., value_delimiter = ',')]
        contracts_to_spoon: Vec<AccountId>,
    },
}

async fn patch_existing_account(worker: &Worker<Sandbox>, account_id: &AccountId, key_json_ref: Rc<RefCell<Value>>) -> anyhow::Result<()> {
    let _span = tracing::info_span!("creating account");
    let secret_key = near_workspaces::types::SecretKey::from_random(KeyType::ED25519);
    worker.patch(account_id).access_key(secret_key.public_key(), near_workspaces::AccessKey::full_access()).transact().await?;
    key_json_ref.borrow_mut()[account_id.to_string()] = json!(secret_key.to_string());
    tracing::info!(%account_id, "patched account");
    Ok(())
}

async fn spoon_contracts(worker: &Worker<Sandbox>, contracts: &[AccountId], key_json_ref: Rc<RefCell<Value>>) -> anyhow::Result<()> {
    patch_existing_account(worker, &AccountId::from_str("near").unwrap(), key_json_ref.clone()).await?;
    let _span = tracing::info_span!("spooning contracts");
    let readrpc_worker = near_workspaces::mainnet()
        .rpc_addr("https://beta.rpc.mainnet.near.org")
        .await?;
    for contract in contracts {
        worker
            .import_contract(contract, &readrpc_worker)
            .transact()
            .await?;
        tracing::info!(%contract, "imported contract");
        let state = readrpc_worker
            .view_state(contract)
            .finality(near_workspaces::types::Finality::Final)
            .prefix(b"STATE".as_slice())
            .await?
            .remove(b"STATE".as_slice())
            .unwrap();
        tracing::info!(%contract, state_size = state.len(), "pulled contract state");
        tracing::info!(%contract, "patched contract state");
        worker.patch_state(contract, b"STATE", &state).await?;
        patch_existing_account(worker, contract, key_json_ref.clone()).await?;
    }
    
    Ok(())
}

#[tokio::main]
async fn main() -> anyhow::Result<()> {
    // Install global collector configured based on RUST_LOG env var.
    let subscriber = tracing_subscriber::fmt()
        .with_thread_ids(true)
        .with_env_filter(EnvFilter::from_default_env());
    subscriber.init();

    match Cli::parse() {
        Cli::Start { contracts_to_spoon } => {
            let key_json_ref = Rc::new(RefCell::new(json!({})));
            let docker_client = DockerClient::default();
            let mut near_hat = NearHat::new(&docker_client, "nearhat", key_json_ref.clone()).await?;
            spoon_contracts(
                &near_hat.nearhat.lake_indexer_ctx.worker,
                &contracts_to_spoon,
                key_json_ref.clone()
            )
            .await?;

            let key_file_path = "tests/data/keys.json";
            let key_file = &mut File::create(key_file_path)?;
            let json_str = serde_json::to_string(&*key_json_ref.borrow())?;
            key_file.write_all(json_str.as_bytes())?;

            println!("\nNEARHat environment is ready:");
            println!(
                "  RPC: http://rpc.nearhat ({})",
                near_hat
                    .nearhat
                    .lake_indexer_ctx
                    .lake_indexer
                    .host_rpc_address_ipv4()
            );
            println!(
                "  Relayer: http://relayer.nearhat ({}), Creator Account: {}",
                near_hat
                    .nearhat
                    .relayer_ctx
                    .relayer
                    .host_http_address_ipv4(),
                near_hat.nearhat.relayer_ctx.creator_account.id()
            );
            println!(
                "  Relayer Redis: {}",
                near_hat.nearhat.relayer_ctx.redis.host_redis_connection_ipv4()
            );
            println!(
                "  QueryAPI Hasura Auth: {}",
                near_hat
                    .nearhat
                    .queryapi_ctx
                    .hasura_auth
                    .host_address_ipv4()
            );
            println!(
                "  QueryAPI Postgres: {}",
                near_hat
                    .nearhat
                    .queryapi_ctx
                    .postgres
                    .host_postgres_address_ipv4()
            );
            println!(
                "  Graphql Playground: http://playground.nearhat ({}), password: {}",
                near_hat
                    .nearhat
                    .queryapi_ctx
                    .hasura_graphql
                    .host_address_ipv4(),
                near_hat
                    .nearhat
                    .queryapi_ctx
                    .hasura_graphql
                    .hasura_password()
            );
            println!(
                "  Explorer Database: {}",
                near_hat
                    .nearhat
                    .explorer_ctx
                    .database
                    .host_postgres_connection_string()
            );
            println!(
                "  NEAR Lake S3: URL=http://lake.nearhat ({}), Region: {}, Bucket: {}",
                near_hat
                    .nearhat
                    .lake_indexer_ctx
                    .localstack
                    .host_s3_address_ipv4(),
                near_hat.nearhat.lake_indexer_ctx.localstack.s3_region,
                near_hat.nearhat.lake_indexer_ctx.localstack.s3_bucket
            );
            println!(
                "  Run `aws --endpoint-url=http://lake.nearhat s3 ls {}/000000000001/` to access block data",
                near_hat.nearhat.lake_indexer_ctx.localstack.s3_bucket
            );
            println!(
                "  Explorer Backend: {}",
                near_hat.nearhat.explorer_ctx.backend.host_address_ipv4()
            );
            println!(
                "  Explorer Frontend: http://explorer.nearhat ({})",
                near_hat.nearhat.explorer_ctx.frontend.host_address_ipv4()
            );

            println!("\nPress any button to exit and destroy all containers...");

            // Create a mutable flag to indicate if CTRL+C was received
            let running = std::sync::Arc::new(std::sync::atomic::AtomicBool::new(true));
            let r = running.clone();

            // Set up a CTRL+C handler
            ctrlc::set_handler(move || {
                r.store(false, std::sync::atomic::Ordering::SeqCst);
            }).expect("Error setting Ctrl-C handler");

            while stdin().read(&mut [0]).await? == 0 && running.load(std::sync::atomic::Ordering::SeqCst) {
                tokio::time::sleep(std::time::Duration::from_millis(25)).await;
            }
            println!("\nTerminating all Docker containers and reverse proxy...");
            let _ = near_hat.reverse_proxy_process.kill();
        }
    }

    Ok(())
}

'''
'''--- near-hat/Cargo.toml ---
[package]
name = "near-hat"
version = "0.1.0"
edition = "2021"

[dependencies]
anyhow = { version = "1.0", features = ["backtrace"] }
async-trait = "0.1"
bollard = "0.13"
futures = "0.3"
home = "0.5"
once_cell = "1.18"
rand = "0.8"
serde = "1.0"
serde_json = "1.0"
tempfile = "3.8"
testcontainers = { version = "0.15", features = ["experimental"] }
toml = "0.8.1"
tracing = "0.1"

near-crypto = "0.17"
near-lake-framework = { git = "https://github.com/near/near-lake-framework-rs.git", branch = "main" }
near-lake-primitives = { git = "https://github.com/near/near-lake-framework-rs.git", branch = "main" }
near-primitives = "0.17"
near-token = "0.2.0"
near-workspaces = { git = "https://github.com/near/near-workspaces-rs.git", rev = "142893b6e501c5b638b20582d81482e173d4499d" }

'''
'''--- near-hat/src/client.rs ---
use anyhow::anyhow;
use bollard::network::CreateNetworkOptions;
use bollard::service::Ipam;
use bollard::Docker;
use futures::lock::Mutex;
use once_cell::sync::Lazy;
use std::path::Path;
use testcontainers::clients::Cli;
use testcontainers::{Container, Image};

static NETWORK_MUTEX: Lazy<Mutex<i32>> = Lazy::new(|| Mutex::new(0));

pub struct DockerClient {
    pub(crate) docker: Docker,
    pub(crate) cli: Cli,
}

impl DockerClient {
    pub async fn get_network_ip_address<I: Image>(
        &self,
        container: &Container<'_, I>,
        network: &str,
    ) -> anyhow::Result<String> {
        let network_settings = self
            .docker
            .inspect_container(container.id(), None)
            .await?
            .network_settings
            .ok_or_else(|| anyhow!("missing NetworkSettings on container '{}'", container.id()))?;
        let ip_address = network_settings
            .networks
            .ok_or_else(|| {
                anyhow!(
                    "missing NetworkSettings.Networks on container '{}'",
                    container.id()
                )
            })?
            .get(network)
            .cloned()
            .ok_or_else(|| {
                anyhow!(
                    "container '{}' is not a part of network '{}'",
                    container.id(),
                    network
                )
            })?
            .ip_address
            .ok_or_else(|| {
                anyhow!(
                    "container '{}' belongs to network '{}', but is not assigned an IP address",
                    container.id(),
                    network
                )
            })?;

        Ok(ip_address)
    }

    pub async fn create_network(&self, network: &str) -> anyhow::Result<()> {
        let _lock = &NETWORK_MUTEX.lock().await;
        let list = self.docker.list_networks::<&str>(None).await?;
        if list.iter().any(|n| n.name == Some(network.to_string())) {
            return Ok(());
        }

        let create_network_options = CreateNetworkOptions {
            name: network,
            check_duplicate: true,
            driver: if cfg!(windows) {
                "transparent"
            } else {
                "bridge"
            },
            ipam: Ipam {
                config: None,
                ..Default::default()
            },
            ..Default::default()
        };
        let _response = &self.docker.create_network(create_network_options).await?;

        Ok(())
    }
}

impl Default for DockerClient {
    fn default() -> Self {
        let socket = std::env::var("DOCKER_HOST")
            .or(std::env::var("DOCKER_SOCK"))
            .unwrap_or_else(|_| {
                let socket = Path::new("/var/run/docker.sock");
                if socket.exists() {
                    "unix:///var/run/docker.sock".to_string()
                } else {
                    let home =
                        home::home_dir().expect("no home directory detected, please set HOME");
                    format!("unix://{}/.docker/run/docker.sock", home.display())
                }
            });
        Self {
            docker: Docker::connect_with_local(
                &socket,
                // 10 minutes timeout for all requests in case a lot of tests are being ran in parallel.
                600,
                bollard::API_DEFAULT_VERSION,
            )
            .unwrap(),
            cli: Default::default(),
        }
    }
}

'''
'''--- near-hat/src/containers/coordinator.rs ---
use crate::DockerClient;
use testcontainers::{Container, GenericImage, RunnableImage};

pub struct Coordinator<'a> {
    pub container: Container<'a, GenericImage>,
    pub metrics_address: String,
}

impl<'a> Coordinator<'a> {
    const METRICS_PORT: u16 = 9180;

    pub async fn run(
        docker_client: &'a DockerClient,
        network: &str,
        redis_address: &str,
        s3_address: &str,
        s3_bucket_name: &str,
        s3_region: &str,
        rpc_address: &str,
        registry_contract_id: &str,
    ) -> anyhow::Result<Coordinator<'a>> {
        tracing::info!(network, "starting Coordinator container");

        let image = GenericImage::new("darunrs/queryapi", "coordinator")
            .with_env_var("AWS_ACCESS_KEY_ID", "FAKE_LOCALSTACK_KEY_ID")
            .with_env_var("AWS_SECRET_ACCESS_KEY", "FAKE_LOCALSTACK_ACCESS_KEY")
            .with_env_var("AWS_REGION", s3_region)
            .with_env_var("S3_URL", s3_address)
            .with_env_var("S3_BUCKET_NAME", s3_bucket_name)
            .with_env_var("RPC_ADDRESS", rpc_address)
            .with_env_var("REDIS_CONNECTION_STRING", redis_address)
            .with_env_var("PORT", Self::METRICS_PORT.to_string())
            .with_env_var("REGISTRY_CONTRACT_ID", registry_contract_id);
            // .with_wait_for(WaitFor::message_on_stdout("Starting queryapi_coordinator..."));

        let image: RunnableImage<GenericImage> = (
            image,
            vec![
                "localnet".to_string(),
                "from-block".to_string(),
                "0".to_string(),
            ],
        ).into();
        let image = image.with_network(network);
        let container = docker_client.cli.run(image);

        let ip_address = docker_client
            .get_network_ip_address(&container, network)
            .await?;
        let metrics_address = format!("http://{}:{}", ip_address, Self::METRICS_PORT);

        tracing::info!("Coordinator container is running",);

        Ok(Coordinator {
            container,
            metrics_address,
        })
    }
}

'''
'''--- near-hat/src/containers/explorer_backend.rs ---
use crate::DockerClient;
use testcontainers::core::WaitFor;
use testcontainers::{Container, GenericImage, RunnableImage};

pub struct ExplorerBackend<'a> {
    pub container: Container<'a, GenericImage>,
    pub ip_address: String,
    pub port: u16,
}

impl<'a> ExplorerBackend<'a> {
    pub const CONTAINER_PORT: u16 = 10000;

    pub async fn run(
        docker_client: &'a DockerClient,
        network: &str,
        database_host: &str,
        database_port: u16,
        rpc_url: &str,
    ) -> anyhow::Result<ExplorerBackend<'a>> {
        tracing::info!(network, "starting NEAR Explorer Backend container");

        let image = GenericImage::new("morgsmccauley/explorer-backend", "latest")
            .with_env_var("NEAR_EXPLORER_CONFIG__ARCHIVAL_RPC_URL", rpc_url)
            .with_env_var("NEAR_EXPLORER_CONFIG__NETWORK_NAME", "localnet")
            .with_env_var(
                "NEAR_EXPLORER_CONFIG__DB__READ_ONLY_INDEXER__HOST",
                database_host,
            )
            .with_env_var(
                "NEAR_EXPLORER_CONFIG__DB__READ_ONLY_INDEXER__DATABASE",
                "postgres",
            )
            .with_env_var(
                "NEAR_EXPLORER_CONFIG__DB__READ_ONLY_INDEXER__USER",
                "postgres",
            )
            .with_env_var(
                "NEAR_EXPLORER_CONFIG__DB__READ_ONLY_INDEXER__PASSWORD",
                "postgres",
            )
            .with_env_var(
                "NEAR_EXPLORER_CONFIG__DB__READ_ONLY_INDEXER__PORT",
                database_port.to_string(),
            )
            .with_env_var(
                "NEAR_EXPLORER_CONFIG__DB__READ_ONLY_ANALYTICS__HOST",
                "34.78.19.198",
            )
            .with_env_var(
                "NEAR_EXPLORER_CONFIG__DB__READ_ONLY_ANALYTICS__DATABASE",
                "indexer_analytics_mainnet",
            )
            .with_env_var(
                "NEAR_EXPLORER_CONFIG__DB__READ_ONLY_ANALYTICS__USER",
                "public_readonly",
            )
            .with_env_var(
                "NEAR_EXPLORER_CONFIG__DB__READ_ONLY_ANALYTICS__PASSWORD",
                "nearprotocol",
            )
            .with_env_var(
                "NEAR_EXPLORER_CONFIG__DB__READ_ONLY_TELEMETRY__HOST",
                "34.78.19.198",
            )
            .with_env_var(
                "NEAR_EXPLORER_CONFIG__DB__READ_ONLY_TELEMETRY__DATABASE",
                "telemetry_mainnet",
            )
            .with_env_var(
                "NEAR_EXPLORER_CONFIG__DB__READ_ONLY_TELEMETRY__USER",
                "public_readonly",
            )
            .with_env_var(
                "NEAR_EXPLORER_CONFIG__DB__READ_ONLY_TELEMETRY__PASSWORD",
                "nearprotocol",
            )
            .with_env_var("NEAR_EXPLORER_CONFIG__DB__WRITE_ONLY_TELEMETRY__HOST", "")
            .with_env_var(
                "NEAR_EXPLORER_CONFIG__DB__WRITE_ONLY_TELEMETRY__DATABASE",
                "telemetry_mainnet",
            )
            .with_env_var("NEAR_EXPLORER_CONFIG__DB__WRITE_ONLY_TELEMETRY__USER", "")
            .with_env_var(
                "NEAR_EXPLORER_CONFIG__DB__WRITE_ONLY_TELEMETRY__PASSWORD",
                "",
            )
            .with_wait_for(WaitFor::message_on_stdout("Explorer backend started"))
            .with_exposed_port(Self::CONTAINER_PORT);

        let image: RunnableImage<GenericImage> = (image, vec![]).into();
        let image = image.with_network(network);
        let container = docker_client.cli.run(image);

        let ip_address = docker_client
            .get_network_ip_address(&container, network)
            .await?;

        tracing::info!("NEAR Explorer Indexer container is running");

        Ok(ExplorerBackend {
            container,
            ip_address,
            port: Self::CONTAINER_PORT,
        })
    }

    pub fn host_address_ipv4(&self) -> String {
        let host_port = self.container.get_host_port_ipv4(Self::CONTAINER_PORT);
        format!("http://127.0.0.1:{host_port}")
    }

    pub fn host_port_ipv4(&self) -> u16 {
        self.container.get_host_port_ipv4(Self::CONTAINER_PORT)
    }
}

'''
'''--- near-hat/src/containers/explorer_database.rs ---
use crate::DockerClient;
use testcontainers::core::WaitFor;
use testcontainers::{Container, GenericImage, RunnableImage};

pub struct ExplorerDatabase<'a> {
    pub container: Container<'a, GenericImage>,
    pub connection_string: String,
    pub host: String,
    pub port: u16,
}

impl<'a> ExplorerDatabase<'a> {
    pub const CONTAINER_PORT: u16 = 5432;

    pub async fn run(
        docker_client: &'a DockerClient,
        network: &str,
    ) -> anyhow::Result<ExplorerDatabase<'a>> {
        tracing::info!(network, "starting NEAR Explorer Database container");

        let image = GenericImage::new("morgsmccauley/explorer-database", "latest")
            .with_wait_for(WaitFor::message_on_stdout(
                "database system is ready to accept connections",
            ))
            .with_exposed_port(Self::CONTAINER_PORT);

        let image: RunnableImage<GenericImage> = (image, vec![]).into();
        let image = image.with_network(network);
        let container = docker_client.cli.run(image);

        let ip_address = docker_client
            .get_network_ip_address(&container, network)
            .await?;

        let connection_string = format!(
            "postgres://postgres:postgres@{}:{}/postgres",
            ip_address,
            Self::CONTAINER_PORT
        );

        tracing::info!("NEAR Explorer Database container is running");

        Ok(ExplorerDatabase {
            container,
            connection_string,
            host: ip_address,
            port: Self::CONTAINER_PORT,
        })
    }

    pub fn host_postgres_connection_string(&self) -> String {
        let host_port = self.container.get_host_port_ipv4(Self::CONTAINER_PORT);
        format!("postgres://postgres:postgres@localhost:{host_port}/postgres")
    }

    pub fn host_postgres_port(&self) -> u16 {
        self.container.get_host_port_ipv4(Self::CONTAINER_PORT)
    }
}

'''
'''--- near-hat/src/containers/explorer_frontend.rs ---
use crate::DockerClient;
use testcontainers::core::WaitFor;
use testcontainers::{Container, GenericImage, RunnableImage};

pub struct ExplorerFrontend<'a> {
    pub container: Container<'a, GenericImage>,
}

impl<'a> ExplorerFrontend<'a> {
    pub const CONTAINER_PORT: u16 = 3000;

    pub async fn run(
        docker_client: &'a DockerClient,
        network: &str,
        backend_host_ip: &str,
        backend_host_port: u16,
        backend_internal_ip: &str,
        backend_internal_port: u16,
    ) -> anyhow::Result<ExplorerFrontend<'a>> {
        tracing::info!(network, "starting NEAR Explorer Frontend container");

        let image = GenericImage::new("morgsmccauley/explorer-frontend", "latest")
            .with_env_var("NEAR_EXPLORER_CONFIG__SEGMENT_WRITE_KEY", "7s4Na9mAfC7092R6pxrwpfBIAEek9Dne")
            .with_env_var(
                "NEAR_EXPLORER_CONFIG__NETWORKS",
                "{ \"localnet\": { \"explorerLink\": \"http://localhost:3000\", \"nearWalletProfilePrefix\": \"https://wallet.near.org/profile\" } }",
            )
            .with_env_var("NEAR_EXPLORER_CONFIG__BACKEND__HOSTS__MAINNET", "")
            .with_env_var(
                "NEAR_EXPLORER_CONFIG__BACKEND__HOSTS__LOCALNET",
                backend_host_ip,
            )
            .with_env_var(
                "NEAR_EXPLORER_CONFIG__BACKEND_SSR__HOSTS__LOCALNET",
                backend_internal_ip,
            )
            .with_env_var(
                "NEAR_EXPLORER_CONFIG__BACKEND__PORT",
                backend_host_port.to_string(),
            )
            .with_env_var(
                "NEAR_EXPLORER_CONFIG__BACKEND_SSR__PORT",
                backend_internal_port.to_string(),
            )
            .with_env_var("NEAR_EXPLORER_CONFIG__BACKEND__SECURE", "false")
            .with_wait_for(WaitFor::message_on_stdout("ready started server"))
            .with_exposed_port(Self::CONTAINER_PORT);

        let image: RunnableImage<GenericImage> = (image, vec![]).into();
        let image = image.with_network(network);
        let container = docker_client.cli.run(image);

        tracing::info!("NEAR Explorer Frontend container is running");

        Ok(ExplorerFrontend { container })
    }

    pub fn host_frontend_port_ipv4(&self) -> u16 {
        return self.container.get_host_port_ipv4(Self::CONTAINER_PORT);
    }

    pub fn host_address_ipv4(&self) -> String {
        let host_port = self.container.get_host_port_ipv4(Self::CONTAINER_PORT);
        format!("http://127.0.0.1:{host_port}")
    }
}

'''
'''--- near-hat/src/containers/explorer_indexer.rs ---
use crate::DockerClient;
use testcontainers::core::WaitFor;
use testcontainers::{Container, GenericImage, RunnableImage};

pub struct ExplorerIndexer<'a> {
    pub container: Container<'a, GenericImage>,
}

impl<'a> ExplorerIndexer<'a> {
    pub async fn run(
        docker_client: &'a DockerClient,
        network: &str,
        s3_endpoint: &str,
        s3_bucket: &str,
        s3_region: &str,
        database_url: &str,
    ) -> anyhow::Result<ExplorerIndexer<'a>> {
        tracing::info!(
            network,
            s3_endpoint,
            s3_bucket,
            s3_region,
            database_url,
            "starting NEAR Explorer Indexer container"
        );

        let image = GenericImage::new("morgsmccauley/explorer-indexer", "latest")
            .with_env_var("AWS_ACCESS_KEY_ID", "FAKE_LOCALSTACK_KEY_ID")
            .with_env_var("AWS_SECRET_ACCESS_KEY", "FAKE_LOCALSTACK_ACCESS_KEY")
            .with_env_var("DATABASE_URL", database_url)
            .with_env_var("S3_REGION", s3_region)
            .with_env_var("AWS_REGION", s3_region)
            .with_env_var("S3_URL", s3_endpoint)
            .with_env_var("S3_BUCKET", s3_bucket)
            .with_env_var("RPC_URL", "not needed")
            .with_wait_for(WaitFor::message_on_stdout("Starting Indexer for Explorer"));

        let image: RunnableImage<GenericImage> = (
            image,
            vec![
                "localnet".to_string(),
                "from-block".to_string(),
                "0".to_string(),
            ],
        )
            .into();
        let image = image.with_network(network);
        let container = docker_client.cli.run(image);

        tracing::info!("NEAR Explorer Indexer container is running");

        Ok(ExplorerIndexer { container })
    }
}

'''
'''--- near-hat/src/containers/hasura_auth.rs ---
use crate::validator::ValidatorContainer;
use crate::DockerClient;
use testcontainers::core::WaitFor;
use testcontainers::{Container, GenericImage, RunnableImage};

pub struct HasuraAuth<'a> {
    pub container: Container<'a, GenericImage>,
    pub auth_address: String,
}

impl<'a> HasuraAuth<'a> {
    pub const CONTAINER_HASURA_AUTH_PORT: u16 = 4000;

    pub async fn run(
        docker_client: &'a DockerClient,
        network: &str
    ) -> anyhow::Result<HasuraAuth<'a>> {
        tracing::info!("starting Hasura Auth container");

        let image = GenericImage::new("darunrs/queryapi", "hasura_auth")
            .with_env_var("PORT", Self::CONTAINER_HASURA_AUTH_PORT.to_string())
            .with_env_var("DEFAULT_HASURA_ROLE", "append")
            .with_wait_for(WaitFor::message_on_stderr("starting HTTP server on port 4000"))
            .with_exposed_port(Self::CONTAINER_HASURA_AUTH_PORT);
        let image: RunnableImage<GenericImage> = image.into();
        let image = image.with_network(network);
        let container = docker_client.cli.run(image);

        let ip_address = docker_client
            .get_network_ip_address(&container, network)
            .await?;
        let auth_address = format!("http://{}:{}", ip_address, Self::CONTAINER_HASURA_AUTH_PORT);

        tracing::info!(
            auth_address,
            "Hasura Auth container is running:"
        );

        Ok(HasuraAuth {
            container,
            auth_address,
        })
    }

    pub fn host_address_ipv4(&self) -> String {
        let host_port = self.container.get_host_port_ipv4(Self::CONTAINER_HASURA_AUTH_PORT);
        format!("http://127.0.0.1:{host_port}")
    }

    pub fn host_address_ipv6(&self) -> String {
        let host_port = self.container.get_host_port_ipv6(Self::CONTAINER_HASURA_AUTH_PORT);
        format!("http://[::1]:{host_port}")
    }
}

impl<'a> ValidatorContainer<'a> for HasuraAuth<'a> {
    fn validator_container(&self) -> &Container<'a, GenericImage> {
        &self.container
    }
}

'''
'''--- near-hat/src/containers/hasura_graphql.rs ---
use std::env;

use crate::validator::ValidatorContainer;
use crate::DockerClient;
use testcontainers::{Container, GenericImage, RunnableImage};

pub struct HasuraGraphql<'a> {
    pub container: Container<'a, GenericImage>,
    pub hasura_address: String,
}

impl<'a> HasuraGraphql<'a> {
    pub const CONTAINER_HASURA_GRAPHQL_PORT: u16 = 8080;
    pub const HASURA_GRAPHQL_ADMIN_SECRET: &'static str = "nearhat";

    pub async fn run(
        docker_client: &'a DockerClient,
        network: &str,
        hasura_auth_address: &str,
        postgres_address: &str,
    ) -> anyhow::Result<HasuraGraphql<'a>> {
        tracing::info!("starting Hasura Graphql container");

        let cwd = env::current_dir()?;

        let image = GenericImage::new("hasura/graphql-engine", "latest.cli-migrations-v3")
            .with_env_var("HASURA_GRAPHQL_DATABASE_URL", postgres_address)
            .with_env_var("HASURA_GRAPHQL_ENABLE_CONSOLE", "true")
            .with_env_var("HASURA_GRAPHQL_DEV_MODE", "true")
            .with_env_var("HASURA_GRAPHQL_ENABLED_LOG_TYPES", "startup, http-log, webhook-log, websocket-log, query-log")
            .with_env_var("HASURA_GRAPHQL_ADMIN_SECRET", Self::HASURA_GRAPHQL_ADMIN_SECRET)
            .with_env_var("HASURA_GRAPHQL_AUTH_HOOK", hasura_auth_address.to_owned() + "/auth")
            .with_volume(cwd.join("hasura/migrations").to_str().unwrap(), "/hasura-migrations")
            .with_volume(cwd.join("hasura/metadata").to_str().unwrap(), "/hasura-metadata")
            .with_exposed_port(Self::CONTAINER_HASURA_GRAPHQL_PORT);
        let image: RunnableImage<GenericImage> = image.into();
        let image = image.with_network(network);
        let container = docker_client.cli.run(image);

        let ip_address = docker_client
            .get_network_ip_address(&container, network)
            .await?;
        let hasura_address = format!("http://{}:{}", ip_address, Self::CONTAINER_HASURA_GRAPHQL_PORT);

        tracing::info!(
            hasura_address,
            "Hasura Graphql container is running:"
        );

        Ok(HasuraGraphql {
            container,
            hasura_address,
        })
    }

    pub fn host_playground_port_ipv4(&self) -> u16 {
        return self.container.get_host_port_ipv4(Self::CONTAINER_HASURA_GRAPHQL_PORT);
    }

    pub fn host_address_ipv4(&self) -> String {
        let host_port = self.container.get_host_port_ipv4(Self::CONTAINER_HASURA_GRAPHQL_PORT);
        format!("http://127.0.0.1:{host_port}")
    }

    pub fn host_address_ipv6(&self) -> String {
        let host_port = self.container.get_host_port_ipv6(Self::CONTAINER_HASURA_GRAPHQL_PORT);
        format!("http://[::1]:{host_port}")
    }

    pub fn hasura_password(&self) -> String {
        Self::HASURA_GRAPHQL_ADMIN_SECRET.to_string()
    }
}

impl<'a> ValidatorContainer<'a> for HasuraGraphql<'a> {
    fn validator_container(&self) -> &Container<'a, GenericImage> {
        &self.container
    }
}

'''
'''--- near-hat/src/containers/lake_indexer.rs ---
use crate::validator::ValidatorContainer;
use crate::DockerClient;
use testcontainers::core::WaitFor;
use testcontainers::{Container, GenericImage, RunnableImage};

pub struct LakeIndexer<'a> {
    pub container: Container<'a, GenericImage>,
    pub bucket_name: String,
    pub region: String,
    pub rpc_address: String,
}

impl<'a> LakeIndexer<'a> {
    pub const CONTAINER_RPC_PORT: u16 = 3030;

    pub async fn run(
        docker_client: &'a DockerClient,
        network: &str,
        s3_address: &str,
        bucket_name: String,
        region: String,
    ) -> anyhow::Result<LakeIndexer<'a>> {
        tracing::info!(
            network,
            s3_address,
            bucket_name,
            region,
            "Starting NEAR RPC with Lake Indexer container"
        );

        let image = GenericImage::new("ghcr.io/near/near-lake-indexer", "latest-sandbox")
            .with_env_var("AWS_ACCESS_KEY_ID", "FAKE_LOCALSTACK_KEY_ID")
            .with_env_var("AWS_SECRET_ACCESS_KEY", "FAKE_LOCALSTACK_ACCESS_KEY")
            .with_wait_for(WaitFor::message_on_stderr("Starting Streamer"))
            .with_exposed_port(Self::CONTAINER_RPC_PORT);
        let image: RunnableImage<GenericImage> = (
            image,
            vec![
                "--endpoint".to_string(),
                s3_address.to_string(),
                "--bucket".to_string(),
                bucket_name.clone(),
                "--region".to_string(),
                region.clone(),
                "--stream-while-syncing".to_string(),
                "sync-from-latest".to_string(),
            ],
        )
            .into();
        let image = image.with_network(network);
        let container = docker_client.cli.run(image);
        let ip_address = docker_client
            .get_network_ip_address(&container, network)
            .await?;
        let rpc_address = format!("http://{}:{}", ip_address, Self::CONTAINER_RPC_PORT);

        tracing::info!(
            bucket_name,
            region,
            rpc_address,
            "NEAR RPC with Lake Indexer is running"
        );

        Ok(LakeIndexer {
            container,
            bucket_name,
            region,
            rpc_address,
        })
    }

    pub fn host_rpc_port_ipv4(&self) -> u16 {
        return self.container.get_host_port_ipv4(Self::CONTAINER_RPC_PORT);
    }

    pub fn host_rpc_address_ipv4(&self) -> String {
        let host_port = self.container.get_host_port_ipv4(Self::CONTAINER_RPC_PORT);
        format!("http://127.0.0.1:{host_port}")
    }

    pub fn host_rpc_address_ipv6(&self) -> String {
        let host_port = self.container.get_host_port_ipv6(Self::CONTAINER_RPC_PORT);
        format!("http://[::1]:{host_port}")
    }
}

impl<'a> ValidatorContainer<'a> for LakeIndexer<'a> {
    fn validator_container(&self) -> &Container<'a, GenericImage> {
        &self.container
    }
}

'''
'''--- near-hat/src/containers/localstack.rs ---
use crate::DockerClient;
use bollard::exec::CreateExecOptions;
use testcontainers::core::WaitFor;
use testcontainers::{Container, GenericImage, RunnableImage};

pub struct LocalStack<'a> {
    pub container: Container<'a, GenericImage>,
    pub s3_address: String,
    pub s3_bucket: String,
    pub s3_region: String,
}

impl<'a> LocalStack<'a> {
    const S3_CONTAINER_PORT: u16 = 4566;

    pub async fn run(
        docker_client: &'a DockerClient,
        network: &str,
        s3_bucket: String,
        s3_region: String,
    ) -> anyhow::Result<LocalStack<'a>> {
        tracing::info!(
            network,
            s3_bucket,
            s3_region,
            "Starting LocalStack container"
        );
        let image = GenericImage::new("localstack/localstack", "3.0.0")
            .with_wait_for(WaitFor::message_on_stdout("Running on"));
        let image: RunnableImage<GenericImage> = image.into();
        let image = image.with_network(network);
        let container = docker_client.cli.run(image);

        // Create the bucket
        let create_result = docker_client
            .docker
            .create_exec(
                container.id(),
                CreateExecOptions::<&str> {
                    attach_stdout: Some(true),
                    attach_stderr: Some(true),
                    cmd: Some(vec![
                        "awslocal",
                        "s3api",
                        "create-bucket",
                        "--bucket",
                        &s3_bucket,
                        "--region",
                        &s3_region,
                    ]),
                    ..Default::default()
                },
            )
            .await?;
        docker_client
            .docker
            .start_exec(&create_result.id, None)
            .await?;

        let ip_address = docker_client
            .get_network_ip_address(&container, network)
            .await?;
        let s3_address = format!("http://{}:{}", ip_address, Self::S3_CONTAINER_PORT);
        tracing::info!(s3_address, "LocalStack container is running");

        Ok(LocalStack {
            container,
            s3_address,
            s3_bucket,
            s3_region,
        })
    }
    
    pub fn host_port_ipv4(&self) -> u16 {
        return self.container.get_host_port_ipv4(Self::S3_CONTAINER_PORT);
    }

    pub fn host_s3_address_ipv4(&self) -> String {
        let host_port = self.container.get_host_port_ipv4(Self::S3_CONTAINER_PORT);
        format!("http://127.0.0.1:{host_port}")
    }

    pub fn host_s3_address_ipv6(&self) -> String {
        let host_port = self.container.get_host_port_ipv6(Self::S3_CONTAINER_PORT);
        format!("http://[::1]:{host_port}")
    }
}

'''
'''--- near-hat/src/containers/mod.rs ---
pub mod explorer_backend;
pub mod explorer_database;
pub mod explorer_frontend;
pub mod explorer_indexer;
pub mod lake_indexer;
pub mod localstack;
pub mod redis;
pub mod relayer;
pub mod sandbox;
pub mod hasura_auth;
pub mod hasura_graphql;
pub mod queryapi_postgres;
pub mod coordinator;
pub mod runner;

'''
'''--- near-hat/src/containers/queryapi_postgres.rs ---
use crate::DockerClient;
use testcontainers::core::WaitFor;
use testcontainers::{Container, GenericImage, RunnableImage};

pub struct QueryApiPostgres<'a> {
    pub container: Container<'a, GenericImage>,
    pub connection_string: String,
    pub postgres_host: String,
    pub postgres_port: u16,
}

impl<'a> QueryApiPostgres<'a> {
    const POSTGRES_PORT: u16 = 5432;
    const POSTGRES_USERNAME: &'static str = "postgres";
    const POSTGRES_PASSWORD: &'static str = "postgres";

    pub async fn run(
        docker_client: &'a DockerClient,
        network: &str,
    ) -> anyhow::Result<QueryApiPostgres<'a>> {
        tracing::info!(network, "starting Postgres container");

        let image = GenericImage::new("darunrs/queryapi", "postgres")
            .with_env_var("POSTGRES_USER", Self::POSTGRES_USERNAME)
            .with_env_var("POSTGRES_PASSWORD", Self::POSTGRES_PASSWORD)
            .with_exposed_port(Self::POSTGRES_PORT)
            .with_wait_for(WaitFor::message_on_stdout("ready to accept connections"));

        let image: RunnableImage<GenericImage> = image.into();
        let image = image.with_network(network);
        let container = docker_client.cli.run(image);

        let ip_address = docker_client
            .get_network_ip_address(&container, network)
            .await?;
        let connection_string = format!(
            "postgres://postgres:postgres@{}:{}/postgres",
            ip_address, Self::POSTGRES_PORT
        );

        tracing::info!(connection_string, "Postgres container is running",);

        Ok(QueryApiPostgres {
            container,
            connection_string,
            postgres_host: ip_address,
            postgres_port: Self::POSTGRES_PORT,
        })
    }

    pub fn host_postgres_address_ipv4(&self) -> String {
        let host_port = self.container.get_host_port_ipv4(Self::POSTGRES_PORT);
        format!("http://127.0.0.1:{host_port}")
    }
}

'''
'''--- near-hat/src/containers/redis.rs ---
use crate::DockerClient;
use testcontainers::core::WaitFor;
use testcontainers::{Container, GenericImage, RunnableImage};

pub struct Redis<'a> {
    pub container: Container<'a, GenericImage>,
    pub redis_address: String,
}

impl<'a> Redis<'a> {
    // Port is hardcoded in the Redis image
    const CONTAINER_REDIS_PORT: u16 = 3000;

    pub async fn run(docker_client: &'a DockerClient, network: &str) -> anyhow::Result<Redis<'a>> {
        tracing::info!(network, "starting Redis container");
        let image = GenericImage::new("redis", "latest")
            .with_exposed_port(Self::CONTAINER_REDIS_PORT)
            .with_wait_for(WaitFor::message_on_stdout("Ready to accept connections"));
        let image: RunnableImage<GenericImage> = image.into();
        let image = image.with_network(network);
        let container = docker_client.cli.run(image);

        let ip_address = docker_client
            .get_network_ip_address(&container, network)
            .await?;
        let redis_address = format!("redis://{}:{}", ip_address, 6379);
        tracing::info!(redis_address, "Redis container is running",);
        Ok(Redis {
            container,
            redis_address,
        })
    }

    pub fn host_redis_address_ipv4(&self) -> String {
        let host_port = self
            .container
            .get_host_port_ipv4(Self::CONTAINER_REDIS_PORT);
        format!("http://127.0.0.1:{host_port}")
    }

    pub fn host_redis_connection_ipv4(&self) -> String {
        let host_port = self
            .container
            .get_host_port_ipv4(6379);
        format!("http://127.0.0.1:{host_port}")
    }

    pub fn host_redis_address_ipv6(&self) -> String {
        let host_port = self
            .container
            .get_host_port_ipv6(Self::CONTAINER_REDIS_PORT);
        format!("http://[::1]:{host_port}")
    }

    pub fn host_redis_connection_ipv6(&self) -> String {
        let host_port = self
            .container
            .get_host_port_ipv6(6379);
        format!("http://[::1]:{host_port}")
    }
}

'''
'''--- near-hat/src/containers/relayer.rs ---
use crate::DockerClient;
use anyhow::Context;
use near_primitives::types::AccountId;
use near_workspaces::types::SecretKey;
use std::fs::File;
use std::io::Write;
use std::path::PathBuf;
use tempfile::{NamedTempFile, TempDir};
use testcontainers::core::WaitFor;
use testcontainers::{Container, GenericImage, RunnableImage};
use toml::Value;

struct KeyFile {
    file: NamedTempFile,
}

impl KeyFile {
    pub fn temp_in_dir(
        account_id: &AccountId,
        account_sk: &SecretKey,
        dir: &TempDir,
    ) -> anyhow::Result<Self> {
        // Field names are following the relayer's configuration format. Do not change.
        let key_file_json = serde_json::json!({
            "account_id": account_id,
            "public_key": account_sk.public_key(),
            "private_key": account_sk,
        });
        let mut file = NamedTempFile::new_in(dir).context("creating temporary key file")?;
        file.write_all(&serde_json::to_vec(&key_file_json).context("serializing key file")?)
            .context("writing key file")?;
        Ok(Self { file })
    }
}

pub struct RelayerConfig {
    pub ip_address: [u8; 4],
    pub port: u16,
    pub relayer_account_id: AccountId,
    pub keys_filenames: Vec<PathBuf>,
    pub shared_storage_account_id: AccountId,
    pub shared_storage_keys_filename: String,
    pub whitelisted_contracts: Vec<AccountId>,
    pub whitelisted_delegate_action_receiver_ids: Vec<AccountId>,
    pub redis_url: String,
    pub social_db_contract_id: AccountId,
    pub rpc_url: String,
    pub wallet_url: String,
    pub explorer_transaction_url: String,
    pub rpc_api_key: String,
}

impl RelayerConfig {
    fn to_string(self) -> String {
        let mut config_table = Value::Table(toml::value::Table::new());
        let table = config_table.as_table_mut().unwrap();

        table.insert(
            "ip_address".to_string(),
            Value::Array(
                self.ip_address
                    .into_iter()
                    .map(|ip| Value::Integer(i64::from(ip)))
                    .collect(),
            ),
        );
        table.insert("port".to_string(), Value::Integer(i64::from(self.port)));

        table.insert(
            "relayer_account_id".to_string(),
            Value::String(self.relayer_account_id.to_string()),
        );
        table.insert(
            "keys_filenames".to_string(),
            Value::Array(
                self.keys_filenames
                    .into_iter()
                    .map(|filename| Value::String(filename.to_str().unwrap().to_string()))
                    .collect(),
            ),
        );

        table.insert(
            "shared_storage_account_id".to_string(),
            Value::String(self.shared_storage_account_id.to_string()),
        );
        table.insert(
            "shared_storage_keys_filename".to_string(),
            Value::String(self.shared_storage_keys_filename),
        );

        table.insert(
            "whitelisted_contracts".to_string(),
            Value::Array(
                self.whitelisted_contracts
                    .into_iter()
                    .map(|account_id| Value::String(account_id.to_string()))
                    .collect(),
            ),
        );
        table.insert(
            "whitelisted_delegate_action_receiver_ids".to_string(),
            Value::Array(
                self.whitelisted_delegate_action_receiver_ids
                    .into_iter()
                    .map(|account_id| Value::String(account_id.to_string()))
                    .collect(),
            ),
        );

        table.insert("redis_url".to_string(), Value::String(self.redis_url));
        table.insert(
            "social_db_contract_id".to_string(),
            Value::String(self.social_db_contract_id.to_string()),
        );

        table.insert("rpc_url".to_string(), Value::String(self.rpc_url));
        table.insert("wallet_url".to_string(), Value::String(self.wallet_url)); // not used
        table.insert(
            "explorer_transaction_url".to_string(),
            Value::String(self.explorer_transaction_url),
        ); // not used
        table.insert("rpc_api_key".to_string(), Value::String(self.rpc_api_key)); // not used

        toml::to_string(&config_table).expect("failed to serialize relayer config")
    }
}

pub struct Relayer<'a> {
    pub container: Container<'a, GenericImage>,
    pub http_address: String,
    // Keep key file handles to ensure that tmp files outlive the container.
    _social_account_key_file: KeyFile,
    _relayer_keyfiles: Vec<KeyFile>,
}

impl<'a> Relayer<'a> {
    pub const CONTAINER_PORT: u16 = 3000;

    pub async fn run(
        docker_client: &'a DockerClient,
        network: &str,
        near_rpc: &str,
        redis_url: &str,
        relayer_account_id: &AccountId,
        relayer_account_sks: &[near_workspaces::types::SecretKey],
        creator_account_id: &AccountId,
        social_db_contract_id: &AccountId,
        social_account_id: &AccountId,
        social_account_sk: &near_workspaces::types::SecretKey,
    ) -> anyhow::Result<Relayer<'a>> {
        tracing::info!(
            network,
            near_rpc,
            redis_url,
            %relayer_account_id,
            %creator_account_id,
            %social_db_contract_id,
            %social_account_id,
            "running relayer container"
        );

        // Create tmp folder to store relayer configs
        let relayer_config_dir =
            tempfile::tempdir().context("creating relayer config directory")?;

        // Create dir for keys
        let key_dir =
            tempfile::tempdir_in(&relayer_config_dir).context("creating relayer keys directory")?;

        // Create JSON key files
        let social_account_key_file =
            KeyFile::temp_in_dir(social_account_id, social_account_sk, &key_dir)?;
        let mut relayer_keyfiles = Vec::with_capacity(relayer_account_sks.len());
        for relayer_sk in relayer_account_sks {
            relayer_keyfiles.push(KeyFile::temp_in_dir(
                relayer_account_id,
                relayer_sk,
                &key_dir,
            )?);
        }

        // Create relayer config file
        let relayer_config = RelayerConfig {
            ip_address: [0, 0, 0, 0],
            port: Self::CONTAINER_PORT,
            relayer_account_id: relayer_account_id.clone(),
            keys_filenames: relayer_keyfiles
                .iter()
                .map(|kf| {
                    kf.file
                        .path()
                        .strip_prefix(&relayer_config_dir)
                        .unwrap()
                        .to_path_buf()
                })
                .collect(),
            shared_storage_account_id: social_account_id.clone(),
            shared_storage_keys_filename: format!("./account_keys/{}.json", social_account_id),
            whitelisted_contracts: vec![creator_account_id.clone()],
            whitelisted_delegate_action_receiver_ids: vec![creator_account_id.clone()],
            redis_url: redis_url.to_string(),
            social_db_contract_id: social_db_contract_id.clone(),
            rpc_url: near_rpc.to_string(),
            wallet_url: "https://wallet.testnet.near.org".to_string(),
            explorer_transaction_url: "https://explorer.testnet.near.org/transactions/".to_string(),
            rpc_api_key: "".to_string(),
        };
        let relayer_config_path = relayer_config_dir.path().join("config.toml");
        let mut relayer_config_file = File::create(&relayer_config_path).unwrap_or_else(|_| {
            panic!(
                "failed to create relayer config file at {}",
                relayer_config_path.display()
            )
        });
        relayer_config_file
            .write_all(relayer_config.to_string().as_bytes())
            .unwrap_or_else(|_| {
                panic!(
                    "failed to write relayer config to {}",
                    relayer_config_path.display()
                )
            });

        let image = GenericImage::new(
            "ghcr.io/near/os-relayer",
            "12ba6e35690df3979fce0b36a41d0ca0db9c0ab4",
        )
        .with_wait_for(WaitFor::message_on_stdout("listening on"))
        .with_exposed_port(Self::CONTAINER_PORT)
        .with_volume(
            relayer_config_path.to_str().unwrap(),
            "/relayer-app/config.toml",
        )
        .with_volume(
            key_dir.path().to_str().unwrap(),
            "/relayer-app/account_keys", // FIXME: directory name is probably going to be mangled, so it wouldn't work like that
        )
        .with_env_var("RUST_LOG", "DEBUG");

        let image: RunnableImage<GenericImage> = image.into();
        let image = image.with_network(network);
        let container = docker_client.cli.run(image);

        let ip_address = docker_client
            .get_network_ip_address(&container, network)
            .await?;
        let http_address = format!("http://{}:{}", ip_address, Self::CONTAINER_PORT);
        tracing::info!(http_address, "Relayer container is running");

        Ok(Relayer {
            container,
            http_address,
            _social_account_key_file: social_account_key_file,
            _relayer_keyfiles: relayer_keyfiles,
        })
    }

    pub fn host_relayer_port_ipv4(&self) -> u16 {
        return self.container.get_host_port_ipv4(Self::CONTAINER_PORT);
    }

    pub fn host_http_address_ipv4(&self) -> String {
        let host_port = self.container.get_host_port_ipv4(Self::CONTAINER_PORT);
        format!("http://127.0.0.1:{host_port}")
    }

    pub fn host_http_address_ipv6(&self) -> String {
        let host_port = self.container.get_host_port_ipv6(Self::CONTAINER_PORT);
        format!("http://[::1]:{host_port}")
    }
}

'''
'''--- near-hat/src/containers/runner.rs ---
use crate::DockerClient;
use testcontainers::{Container, GenericImage, RunnableImage};

pub struct Runner<'a> {
    pub container: Container<'a, GenericImage>,
    pub metrics_address: String,
}

impl<'a> Runner<'a> {
    const METRICS_PORT: u16 = 9180;

    pub async fn run(
        docker_client: &'a DockerClient,
        network: &str,
        region: &str,
        hasura_address: &str,
        hasura_password: &str,
        redis_address: &str,
        postgres_host: &str,
        postgres_port: u16,
    ) -> anyhow::Result<Runner<'a>> {
        tracing::info!(network, "starting QueryAPI Runner container");

        let image = GenericImage::new("darunrs/queryapi", "runner")
            .with_env_var("AWS_ACCESS_KEY_ID", "FAKE_LOCALSTACK_KEY_ID")
            .with_env_var("AWS_SECRET_ACCESS_KEY", "FAKE_LOCALSTACK_ACCESS_KEY")
            .with_env_var("AWS_REGION", region)
            .with_env_var("REGION", region)
            .with_env_var("REDIS_CONNECTION_STRING", redis_address)
            .with_env_var("HASURA_ENDPOINT", hasura_address)
            .with_env_var("HASURA_ADMIN_SECRET", hasura_password)
            .with_env_var("PORT", Self::METRICS_PORT.to_string())
            .with_env_var("PGHOST", postgres_host)
            .with_env_var("PGPORT", postgres_port.to_string())
            .with_env_var("PGUSER", "postgres")
            .with_env_var("PGPASSWORD", "postgres")
            .with_env_var("PGDATABASE", "postgres");
            // .with_wait_for(WaitFor::message_on_stdout("server running on http://localhost:9180"));

        let image: RunnableImage<GenericImage> = image.into();
        let image = image.with_network(network);
        let container = docker_client.cli.run(image);

        let ip_address = docker_client
            .get_network_ip_address(&container, network)
            .await?;
        let metrics_address = format!("http://{}:{}", ip_address, Self::METRICS_PORT);

        tracing::info!("QueryAPI Runner container is running",);

        Ok(Runner {
            container,
            metrics_address,
        })
    }
}

'''
'''--- near-hat/src/containers/sandbox.rs ---
use crate::validator::ValidatorContainer;
use crate::DockerClient;
use testcontainers::core::{ExecCommand, WaitFor};
use testcontainers::{Container, GenericImage, RunnableImage};

pub struct Sandbox<'a> {
    pub container: Container<'a, GenericImage>,
    pub rpc_address: String,
}

impl<'a> Sandbox<'a> {
    pub const CONTAINER_RPC_PORT: u16 = 3000;
    pub const CONTAINER_NETWORK_PORT: u16 = 3001;

    pub async fn run(
        docker_client: &'a DockerClient,
        network: &str,
    ) -> anyhow::Result<Sandbox<'a>> {
        tracing::info!(network, "starting sandbox container");
        // TODO: combine macos and x86 tags under the same image
        #[cfg(all(target_os = "macos", target_arch = "aarch64"))]
        let image = GenericImage::new("ghcr.io/near/sandbox", "latest-aarch64")
            .with_wait_for(WaitFor::Nothing)
            .with_exposed_port(Self::CONTAINER_RPC_PORT);
        #[cfg(target_arch = "x86_64")]
        let image = GenericImage::new("ghcr.io/near/sandbox", "latest")
            .with_wait_for(WaitFor::Nothing)
            .with_exposed_port(Self::CONTAINER_RPC_PORT);
        let image: RunnableImage<GenericImage> = (
            image,
            vec![
                "--rpc-addr".to_string(),
                format!("0.0.0.0:{}", Self::CONTAINER_RPC_PORT),
                "--network-addr".to_string(),
                format!("0.0.0.0:{}", Self::CONTAINER_NETWORK_PORT),
            ],
        )
            .into();
        let image = image.with_network(network);
        let container = docker_client.cli.run(image);
        container.exec(ExecCommand {
            cmd: format!(
                "bash -c 'while [[ \"$(curl -H \"Content-type: application/json\" -X POST -s -o /dev/null -w ''%{{http_code}}'' -d ''{{
                \"jsonrpc\": \"2.0\",
                \"id\": \"dontcare\",
                \"method\": \"status\",
                \"params\": []
              }}'' localhost:{})\" != \"200\" ]]; do sleep 1; done; echo \"sandbox is ready to accept connections\"'",
                Self::CONTAINER_RPC_PORT
            ),
            ready_conditions: vec![WaitFor::StdErrMessage { message: "ready".to_string() }]
        });

        let ip_address = docker_client
            .get_network_ip_address(&container, network)
            .await?;
        let rpc_address = format!("http://{}:{}", ip_address, Self::CONTAINER_RPC_PORT);
        tracing::info!(rpc_address, "sandbox container is running");
        Ok(Sandbox {
            container,
            rpc_address,
        })
    }

    pub fn host_rpc_address_ipv4(&self) -> String {
        let host_port = self.container.get_host_port_ipv4(Self::CONTAINER_RPC_PORT);
        format!("http://127.0.0.1:{host_port}")
    }

    pub fn host_rpc_address_ipv6(&self) -> String {
        let host_port = self.container.get_host_port_ipv6(Self::CONTAINER_RPC_PORT);
        format!("http://[::1]:{host_port}")
    }
}

impl<'a> ValidatorContainer<'a> for Sandbox<'a> {
    fn validator_container(&self) -> &Container<'a, GenericImage> {
        &self.container
    }
}

'''
'''--- near-hat/src/ctx/explorer.rs ---
use super::lake_indexer::LakeIndexerCtx;
use crate::client::DockerClient;
use crate::containers::explorer_backend::ExplorerBackend;
use crate::containers::explorer_database::ExplorerDatabase;
use crate::containers::explorer_frontend::ExplorerFrontend;
use crate::containers::explorer_indexer::ExplorerIndexer;

pub struct ExplorerCtx<'a> {
    pub indexer: ExplorerIndexer<'a>,
    pub database: ExplorerDatabase<'a>,
    pub backend: ExplorerBackend<'a>,
    pub frontend: ExplorerFrontend<'a>,
}

impl<'a> ExplorerCtx<'a> {
    pub async fn new(
        docker_client: &'a DockerClient,
        network: &str,
        lake_indexer_ctx: &LakeIndexerCtx<'a>,
    ) -> anyhow::Result<ExplorerCtx<'a>> {
        let database = ExplorerDatabase::run(docker_client, network).await?;

        let indexer = ExplorerIndexer::run(
            docker_client,
            network,
            &lake_indexer_ctx.localstack.s3_address,
            &lake_indexer_ctx.localstack.s3_bucket,
            &lake_indexer_ctx.localstack.s3_region,
            &database.connection_string,
        )
        .await?;

        let backend = ExplorerBackend::run(
            docker_client,
            network,
            &database.host,
            database.port,
            &lake_indexer_ctx.lake_indexer.rpc_address,
        )
        .await?;

        let frontend = ExplorerFrontend::run(
            docker_client,
            network,
            "127.0.0.1",
            backend.host_port_ipv4(),
            &backend.ip_address,
            backend.port,
        )
        .await?;

        Ok(ExplorerCtx {
            indexer,
            database,
            backend,
            frontend,
        })
    }
}

'''
'''--- near-hat/src/ctx/lake_indexer.rs ---
use std::cell::RefCell;
use std::rc::Rc;

use crate::client::DockerClient;
use crate::containers::lake_indexer::LakeIndexer;
use crate::containers::localstack::LocalStack;
use crate::validator::ValidatorContainer;
use near_workspaces::network::{Sandbox, ValidatorKey};
use near_workspaces::Worker;
use serde_json::{json, Value};

pub struct LakeIndexerCtx<'a> {
    pub localstack: LocalStack<'a>,
    pub lake_indexer: LakeIndexer<'a>,
    // FIXME: Technically this network is not sandbox, but workspaces does not support plain localnet
    pub worker: Worker<Sandbox>,
}

impl<'a> LakeIndexerCtx<'a> {
    pub async fn new(
        docker_client: &'a DockerClient,
        network: &str,
        key_json_ref: Rc<RefCell<Value>>,
    ) -> anyhow::Result<LakeIndexerCtx<'a>> {
        let s3_bucket = "localnet".to_string();
        let s3_region = "us-east-1".to_string();
        let localstack =
            LocalStack::run(docker_client, network, s3_bucket.clone(), s3_region.clone()).await?;

        let lake_indexer = LakeIndexer::run(
            docker_client,
            network,
            &localstack.s3_address,
            s3_bucket,
            s3_region,
        )
        .await?;

        let validator_key = lake_indexer.fetch_keys(docker_client).await?;

        tracing::info!("initializing sandbox worker");
        let worker = near_workspaces::sandbox()
            .rpc_addr(&lake_indexer.host_rpc_address_ipv4())
            .validator_key(ValidatorKey::Known(
                validator_key.account_id.to_string().parse()?,
                validator_key.secret_key.to_string().parse()?,
            ))
            .await?;

        key_json_ref.borrow_mut()[validator_key.account_id.to_string()] = json!(validator_key.secret_key.to_string());

        Ok(LakeIndexerCtx {
            localstack,
            lake_indexer,
            worker
        })
    }
}

'''
'''--- near-hat/src/ctx/mod.rs ---
pub mod explorer;
pub mod lake_indexer;
pub mod nearcore;
pub mod relayer;
pub mod queryapi;

'''
'''--- near-hat/src/ctx/nearcore.rs ---
use near_workspaces::network::Sandbox;
use near_workspaces::types::{NearToken, SecretKey};
use near_workspaces::{AccessKey, Account, Contract, Worker};

pub struct NearcoreCtx {
    pub(crate) worker: Worker<Sandbox>,
}

impl NearcoreCtx {
    async fn initialize_social_db(worker: &Worker<Sandbox>) -> anyhow::Result<Contract> {
        let _span = tracing::info_span!("initializing socialdb contract");
        let social_db = worker
            .import_contract(&"social.near".parse()?, &near_workspaces::mainnet().await?)
            .transact()
            .await?;
        social_db
            .call("new")
            .max_gas()
            .transact()
            .await?
            .into_result()?;

        Ok(social_db)
    }

    // Linkdrop contains top-level account creation logic
    async fn initialize_linkdrop(worker: &Worker<Sandbox>) -> anyhow::Result<()> {
        let _span = tracing::info_span!("initializing linkdrop contract");
        let near_root_account = worker.root_account()?;
        worker
            .import_contract(&"near".parse()?, &near_workspaces::mainnet().await?)
            .dest_account_id(near_root_account.id())
            .transact()
            .await?;
        near_root_account
            .call(near_root_account.id(), "new")
            .max_gas()
            .transact()
            .await?
            .into_result()?;

        Ok(())
    }

    pub async fn new(worker: &Worker<Sandbox>) -> anyhow::Result<NearcoreCtx> {
        // Self::initialize_linkdrop(worker).await?;
        // TODO: move out of nearcore trait into its own ctx
        // let social_db = Self::initialize_social_db(worker).await?;

        Ok(NearcoreCtx {
            worker: worker.clone(),
        })
    }

    pub async fn create_account(
        &self,
        prefix: &str,
        initial_balance: NearToken,
    ) -> anyhow::Result<Account> {
        let _span = tracing::info_span!("creating account with random account id");
        let new_account = self
            .worker
            .root_account()?
            .create_subaccount(prefix)
            .initial_balance(initial_balance)
            .transact()
            .await?
            .into_result()?;

        tracing::info!(id = %new_account.id(), "account created");
        Ok(new_account)
    }

    pub async fn gen_rotating_keys(
        &self,
        account: &Account,
        amount: usize,
    ) -> anyhow::Result<Vec<SecretKey>> {
        let mut keys = Vec::with_capacity(amount + 1);
        keys.push(account.secret_key().clone());

        // Each batch transaction has a limit of BATCH_COUNT_LIMIT actions.
        const BATCH_COUNT_LIMIT: usize = 100;
        let num_batches = amount / BATCH_COUNT_LIMIT + 1;
        let rem_batches = amount % BATCH_COUNT_LIMIT;
        let batch_counts = (0..num_batches).map(|i| {
            if i == num_batches - 1 {
                rem_batches
            } else {
                BATCH_COUNT_LIMIT
            }
        });

        for batch_count in batch_counts {
            let mut batch_tx = account.batch(account.id());
            for _ in 0..batch_count {
                let sk = SecretKey::from_seed(
                    near_workspaces::types::KeyType::ED25519,
                    &rand::Rng::sample_iter(rand::thread_rng(), &rand::distributions::Alphanumeric)
                        .take(10)
                        .map(char::from)
                        .collect::<String>(),
                );
                batch_tx = batch_tx.add_key(sk.public_key(), AccessKey::full_access());
                keys.push(sk);
            }
            batch_tx.transact().await?.into_result()?;
        }

        Ok(keys)
    }

    /// Get the address the context is using to connect to the RPC of the network.
    pub fn rpc_address(&self) -> String {
        self.worker.rpc_addr()
    }
}

'''
'''--- near-hat/src/ctx/queryapi.rs ---
use std::cell::RefCell;
use std::fs;
use std::rc::Rc;

use near_token::NearToken;
use serde_json::{json, Value};

use crate::client::DockerClient;
use crate::containers::coordinator::Coordinator;
use crate::containers::hasura_auth::HasuraAuth;
use crate::containers::queryapi_postgres::QueryApiPostgres;
use crate::containers::hasura_graphql::HasuraGraphql;
use crate::containers::runner::Runner;

use super::nearcore::NearcoreCtx;

pub struct QueryApiCtx<'a> {
    pub hasura_auth: HasuraAuth<'a>,
    pub postgres: QueryApiPostgres<'a>,
    pub hasura_graphql: HasuraGraphql<'a>,
    pub coordinator: Coordinator<'a>,
    pub runner: Runner<'a>,
}

impl<'a> QueryApiCtx<'a> {
    pub async fn new(
        docker_client: &'a DockerClient,
        network: &str,
        redis_address: &str,
        s3_address: &str,
        s3_bucket_name: &str,
        s3_region: &str,
        nearcore: &NearcoreCtx,
        rpc_address: &str,
        key_json_ref: Rc<RefCell<Value>>,
    ) -> anyhow::Result<QueryApiCtx<'a>> {
        // Deploy registry contract and initialize it
        let wasm_bytes = fs::read("wasm/registry.wasm")?;
        let registry_holder = nearcore.create_account("dev-queryapi", NearToken::from_near(50)).await?;
        let registry_contract = registry_holder.deploy(&wasm_bytes).await?.unwrap();

        key_json_ref.borrow_mut()[registry_holder.id().to_string()] = json!(registry_holder.secret_key().to_string());

        // Set up dockers
        let hasura_auth = HasuraAuth::run(docker_client, network).await?;
        let postgres = QueryApiPostgres::run(docker_client, network).await?;
        let hasura_graphql = HasuraGraphql::run(docker_client, network, &hasura_auth.auth_address, &postgres.connection_string).await?;
        let coordinator = Coordinator::run(
            docker_client, 
            network, 
            redis_address, 
            s3_address, 
            s3_bucket_name, 
            s3_region, 
            rpc_address,
            registry_contract.id()).await?;

        let runner = Runner::run(
            docker_client, 
            network, 
            s3_region, 
            &hasura_graphql.hasura_address, 
            hasura_graphql.hasura_password().as_str(),
            redis_address, 
            &postgres.postgres_host, 
            postgres.postgres_port).await?;

        Ok(QueryApiCtx { 
            hasura_auth,
            postgres,
            hasura_graphql,
            coordinator,
            runner
        })
    }
}

'''
'''--- near-hat/src/ctx/relayer.rs ---
use super::nearcore::NearcoreCtx;
use crate::client::DockerClient;
use crate::containers::redis::Redis;
use crate::containers::relayer::Relayer;
use near_token::NearToken;
use near_workspaces::types::SecretKey;
use near_workspaces::Account;

pub struct RelayerCtx<'a> {
    pub redis: Redis<'a>,
    pub relayer: Relayer<'a>,
    pub creator_account: Account,
    pub creator_account_keys: Vec<SecretKey>,
}

impl<'a> RelayerCtx<'a> {
    pub async fn new(
        docker_client: &'a DockerClient,
        network: &str,
        nearcore_ctx: &NearcoreCtx,
    ) -> anyhow::Result<RelayerCtx<'a>> {
        let accounts_span = tracing::info_span!("initializing relayer accounts");
        let relayer_account = nearcore_ctx
            .create_account("relayer", NearToken::from_near(1000))
            .await?;
        let relayer_account_keys = nearcore_ctx.gen_rotating_keys(&relayer_account, 5).await?;

        let creator_account = nearcore_ctx
            .create_account("creator", NearToken::from_near(200))
            .await?;
        let creator_account_keys = nearcore_ctx.gen_rotating_keys(&creator_account, 5).await?;

        let social_account = nearcore_ctx
            .create_account("social", NearToken::from_near(1000))
            .await?;
        tracing::info!(
            relayer_account = %relayer_account.id(),
            creator_account = %creator_account.id(),
            social_account = %social_account.id(),
            "relayer accounts initialized",
        );
        drop(accounts_span);

        let redis = Redis::run(docker_client, network).await?;
        let relayer = Relayer::run(
            docker_client,
            network,
            &nearcore_ctx.rpc_address(),
            &redis.host_redis_address_ipv4(),
            relayer_account.id(),
            &relayer_account_keys,
            creator_account.id(),
            &"no_social_db.near".parse()?,
            social_account.id(),
            social_account.secret_key(),
        )
        .await?;

        Ok(RelayerCtx::<'a> {
            redis,
            relayer,
            creator_account,
            creator_account_keys,
        })
    }
}

'''
'''--- near-hat/src/lib.rs ---
mod client;
mod containers;
mod ctx;
mod validator;

pub use client::DockerClient;

use ctx::explorer::ExplorerCtx;
use ctx::lake_indexer::LakeIndexerCtx;
use ctx::nearcore::NearcoreCtx;
use ctx::queryapi::QueryApiCtx;
use ctx::relayer::RelayerCtx;
use serde_json::Value;
use std::{process::{Command, Child}, rc::Rc, cell::RefCell};

pub struct NearHat<'a> {
    pub queryapi_ctx: QueryApiCtx<'a>,
    pub lake_indexer_ctx: LakeIndexerCtx<'a>,
    pub nearcore_ctx: NearcoreCtx,
    pub relayer_ctx: RelayerCtx<'a>,
    pub explorer_ctx: ExplorerCtx<'a>,
}

pub struct NearHatEnvironment<'a> {
    pub nearhat: NearHat<'a>,
    pub reverse_proxy_process: Child
}

impl<'a> NearHat<'a> {
    pub async fn new(
        docker_client: &'a DockerClient,
        network: &str,
        key_json_ref: Rc<RefCell<Value>>,
    ) -> anyhow::Result<NearHatEnvironment<'a>> {
        let lake_indexer_ctx = LakeIndexerCtx::new(&docker_client, network, key_json_ref.clone()).await?;
        let nearcore_ctx = NearcoreCtx::new(&lake_indexer_ctx.worker).await?;
        let relayer_ctx = RelayerCtx::new(docker_client, network, &nearcore_ctx).await?;
        let queryapi_ctx = QueryApiCtx::new(
            docker_client,
            network,
            &relayer_ctx.redis.redis_address,
            &lake_indexer_ctx.localstack.s3_address,
            &lake_indexer_ctx.localstack.s3_bucket,
            &lake_indexer_ctx.localstack.s3_region,
            &nearcore_ctx,
            &lake_indexer_ctx.lake_indexer.rpc_address,
            key_json_ref.clone(),
        )
        .await?;
        let explorer_ctx = ExplorerCtx::new(docker_client, network, &lake_indexer_ctx).await?;

        let nearhat = NearHat {
            queryapi_ctx,
            lake_indexer_ctx,
            nearcore_ctx,
            relayer_ctx,
            explorer_ctx,
        };

        let reverse_proxy_process = Self::start_reverse_proxy(&nearhat)?;

        Ok(NearHatEnvironment{
            nearhat,
            reverse_proxy_process
        })
    }

    fn start_reverse_proxy(nearhat: &NearHat<'_>) -> std::io::Result<Child> {
        let mut command = Command::new("mitmdump");

        command.arg("--mode").arg("regular").arg("-p").arg("80").arg("-s").arg("dns.py")
            .env("NEARHAT_RPC_PORT", &nearhat.lake_indexer_ctx.lake_indexer.host_rpc_port_ipv4().to_string())
            .env("NEARHAT_LAKE_S3_PORT", &nearhat.lake_indexer_ctx.localstack.host_port_ipv4().to_string())
            .env("NEARHAT_RELAYER_PORT", &nearhat.relayer_ctx.relayer.host_relayer_port_ipv4().to_string())
            .env("NEARHAT_EXPLORER_UI_PORT", &nearhat.explorer_ctx.frontend.host_frontend_port_ipv4().to_string())
            .env("NEARHAT_GRAPHQL_PLAYGROUND_PORT", &nearhat.queryapi_ctx.hasura_graphql.host_playground_port_ipv4().to_string())
            .stdout(std::process::Stdio::null());

        return command.spawn();
    }
}

'''
'''--- near-hat/src/validator.rs ---
use crate::DockerClient;
use async_trait::async_trait;
use bollard::exec::{CreateExecOptions, StartExecResults};
use futures::StreamExt;
use near_crypto::KeyFile;
use testcontainers::{Container, GenericImage};

/// Container hosting a NEAR validator inside (e.g. Sandbox, Lake Indexer).
#[async_trait]
pub trait ValidatorContainer<'a> {
    fn validator_container(&self) -> &Container<'a, GenericImage>;

    async fn fetch(&self, docker_client: &DockerClient, path: &str) -> anyhow::Result<Vec<u8>> {
        tracing::info!(path, "fetching data from validator");
        let create_result = docker_client
            .docker
            .create_exec(
                self.validator_container().id(),
                CreateExecOptions::<&str> {
                    attach_stdout: Some(true),
                    attach_stderr: Some(true),
                    cmd: Some(vec!["cat", path]),
                    ..Default::default()
                },
            )
            .await?;

        let start_result = docker_client
            .docker
            .start_exec(&create_result.id, None)
            .await?;

        match start_result {
            StartExecResults::Attached { mut output, .. } => {
                let mut stream_contents = Vec::new();
                while let Some(chunk) = output.next().await {
                    stream_contents.extend_from_slice(&chunk?.into_bytes());
                }

                tracing::info!("data fetched");
                Ok(stream_contents)
            }
            StartExecResults::Detached => unreachable!("unexpected detached output"),
        }
    }

    async fn fetch_keys(&self, docker_client: &DockerClient) -> anyhow::Result<KeyFile> {
        let _span = tracing::info_span!("fetch_validator_keys");
        let key_data = self
            .fetch(docker_client, "/root/.near/validator_key.json")
            .await?;
        Ok(serde_json::from_slice(&key_data)?)
    }
}

'''
'''--- start.sh ---
#!/bin/bash

RUST_LOG=info cargo run -p near-hat-cli -- start --contracts-to-spoon usdt.tether-token.near

'''
'''--- tests/data/indexer_code.js ---
let transactions = [];
block.events()
  .filter((event) => (event && event.rawEvent && event.rawEvent.standard))
  .map((event) => {
    let standardEvent = JSON.parse(event.rawEvent.standard);
    console.log("event:", standardEvent);
    standardEvent.data.forEach(eventItem => {
      transactions.push({
        event: standardEvent.event, 
        amount: eventItem.amount, 
        from_account: eventItem.old_owner_id || (standardEvent.event === 'ft_burn' ? eventItem.owner_id : null), 
        to_account: eventItem.new_owner_id || (standardEvent.event === 'ft_mint' ? eventItem.owner_id : null)
      });
    })
  });
console.log(transactions);
transactions.forEach(async (tx) => {
  await context.db.UsdtTransactions.insert({amount: tx.amount, block_height: block.blockHeight, event: tx.event, from_account: tx.from_account, to_account: tx.to_account});
});

'''
'''--- tests/data/indexer_schema.sql ---
CREATE TABLE
  usdt_transactions (
    id SERIAL PRIMARY KEY,
    event VARCHAR(255) NOT NULL,
    amount VARCHAR(255) NOT NULL,
    from_account VARCHAR(255),
    to_account VARCHAR(255),
    block_height INT
  );

'''
'''--- tests/demo.test.js ---
import { connect, Contract, keyStores, utils } from 'near-api-js';
import { restoreTestAccountKeys, getOrCreateAccount, registerIndexer, fetchGraphQL } from './testUtils.js'
import assert from 'assert';

describe('Hackathon Demo', () => {
    const connectionConfig = {
        networkId: "localnet",
        nodeUrl: "http://rpc.nearhat",
        walletUrl: "NONE",
        keyStore: new keyStores.InMemoryKeyStore()
    };

    test('Transaction events queryable after successful contract executions', async () => {
        const near = await connect(connectionConfig);
        restoreTestAccountKeys(near);
        
        await registerIndexer("usdt_transactions", 'data/indexer_code.js', 'data/indexer_schema.sql', "*.near", near);

        const multisafe = await getOrCreateAccount(near, "multisafe.near", 20);
        const usdtOwner = await getOrCreateAccount(near, "tether.multisafe.near", 5);
        const alice = await getOrCreateAccount(near, "alice.near", 2);
        const bob = await getOrCreateAccount(near, "bob.near", 2);
        
        const adminUsdtContractSigner = new Contract(usdtOwner, 'usdt.tether-token.near', {
          viewMethods: ['ft_balance_of'],
          changeMethods: ['mint', 'storage_deposit'],
        });

        // Cover storage deposit
        await adminUsdtContractSigner.storage_deposit({
          args: { "account_id": alice.accountId },
          gas: "300000000000000",
          amount: "10000000000000000000000"
        });
        await adminUsdtContractSigner.storage_deposit({
          args: { "account_id": bob.accountId },
          gas: "300000000000000",
          amount: "10000000000000000000000"
        });

        // Mint 100 USDT to Alice
        const response = await adminUsdtContractSigner.mint({
            args: { account_id: alice.accountId, amount: "100000000" }
        });
        assert.strictEqual(
          await adminUsdtContractSigner.ft_balance_of(
            { account_id: alice.accountId }
          ),"100000000", "Alice should have 100 USDT to start");

        // Transfer 50 USDT from Alice to Bob
        const aliceUsdtContractSigner = new Contract(alice, 'usdt.tether-token.near', {
          viewMethods: [],
          changeMethods: ['ft_transfer', 'storage_deposit'],
        });
        await aliceUsdtContractSigner.ft_transfer({
          args: { receiver_id: bob.accountId, amount: "50000000" },
          gas: "300000000000000", amount: "1"
        });

        // Verify Alice and Bob both have 50 USDT
        assert.strictEqual(await adminUsdtContractSigner.ft_balance_of(
          { account_id: alice.accountId }
        ), "50000000", "Alice should have 50 USDT after transfer");
        assert.strictEqual(await adminUsdtContractSigner.ft_balance_of(
          { account_id: bob.accountId }
        ), "50000000", "Bob should have 50 USDT after transfer");

        // // Wait for indexer to index latest transactions
        console.log("Waiting for indexer to index latest transactions...");
        await new Promise(resolve => setTimeout(resolve, 5000));

        // assert QueryAPI has mint, transfer
        const query = `query MyQuery {
          dev_queryapi_test_near_usdt_transactions_usdt_transactions {
            event
          }
        }`;
        const result = await fetchGraphQL(query, {});
        const events = result.data.dev_queryapi_test_near_usdt_transactions_usdt_transactions;
        const hasFtMint = events.some(e => e.event === 'ft_mint');
        assert(hasFtMint, "'ft_mint' event not found");

        // Check if 'ft_transfer' is present
        const hasFtTransfer = events.some(e => e.event === 'ft_transfer');
        assert(hasFtTransfer, "'ft_transfer' event not found");
    }, 60000);
});

function doLastElementsMatch(resultArray, expectedArray) {
    // Get the last elements of the result array based on the length of the expected array
    const lastElements = resultArray.slice(-expectedArray.length);

    // Compare the last elements with the expected array
    return lastElements.length === expectedArray.length &&
           lastElements.every((element, index) => element === expectedArray[index]);
}

'''
'''--- tests/package.json ---
{
  "type": "module",
  "name": "tests",
  "version": "1.0.0",
  "description": "",
  "main": "index.js",
  "scripts": {
    "test": "node --experimental-vm-modules ./node_modules/.bin/jest"
  },
  "keywords": [],
  "author": "",
  "license": "ISC",
  "devDependencies": {
    "@types/jest": "^29.5.11",
    "@types/mocha": "^10.0.6",
    "jest": "^29.7.0",
    "mocha": "^10.2.0",
    "near-api-js": "^2.1.4"
  }
}

'''
'''--- tests/testUtils.js ---
import { KeyPair, utils } from 'near-api-js';
import { readFileSync, writeFileSync } from 'fs';
import { Contract } from 'near-api-js';

export async function registerIndexer(indexerName, codePath, schemaPath, affectedAccountId, near) {
    const queryApiAccount = await near.account("dev-queryapi.test.near");
    const indexerRegistry = new Contract(queryApiAccount, 'dev-queryapi.test.near', {
        viewMethods: [],
        changeMethods: ['register_indexer_function'],
    });
    const code = readFileSync(codePath).toString();
    const schema = readFileSync(schemaPath).toString();

    await indexerRegistry.account.functionCall({
        contractId: indexerRegistry.contractId,
        methodName: "register_indexer_function",
        args: {
            "function_name": indexerName,
            "code": code,
            "schema": schema,
            "filter_json": `{\"indexer_rule_kind\":\"Action\",\"matching_rule\":{\"rule\":\"ACTION_ANY\",\"affected_account_id\":\"${affectedAccountId}\",\"status\":\"SUCCESS\"}}`
        },
    });
    await new Promise(resolve => setTimeout(resolve, 1000));
}

export function loadTestAccountKeys() {
    const privateKeysJson = readFileSync("data/keys.json", 'utf8');
    const privateKeys = JSON.parse(privateKeysJson);
    return privateKeys;
}

export function restoreTestAccountKeys(nearConnection) {
    const privateKeys = loadTestAccountKeys();
    Object.keys(privateKeys).forEach(key => {
      nearConnection.config.keyStore.setKey("localnet", key, KeyPair.fromString(privateKeys[key]));
    });
    return privateKeys;
}

export function saveTestAccountKey(accountId, privateKey) {
    const privateKeys = loadTestAccountKeys();
    privateKeys[accountId] = privateKey;
    writeFileSync("data/keys.json", JSON.stringify(privateKeys, null, 2), 'utf8');
}

export async function getOrCreateAccount(nearConnection, accountId, nearAmount) {
    const privateKeys = loadTestAccountKeys();
    const accountPK = privateKeys[accountId];
    if (accountPK) {
        const sk = KeyPair.fromString(accountPK);
        const account = await nearConnection.account(accountId);
        nearConnection.config.keyStore.setKey("localnet", accountId, accountPK);
        return account;
    }
    const accountCreatorId = accountId.split('.').slice(1).join('.');
    const sk = KeyPair.fromString(privateKeys[accountCreatorId]);
    const accountCreator = await nearConnection.account(accountCreatorId);
    await accountCreator.createAccount(accountId, sk.getPublicKey(), utils.format.parseNearAmount(nearAmount.toString()));
    const account = await nearConnection.account(accountId);
    nearConnection.config.keyStore.setKey("localnet", accountId, sk);
    saveTestAccountKey(accountId, privateKeys[accountCreatorId]);
    return account;
}

export async function fetchGraphQL(query, variables) {
    const response = await fetch('http://playground.nearhat/v1/graphql', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'x-hasura-admin-secret': 'nearhat'
      },
      body: JSON.stringify({
        query,
        variables
      })
    });
  
    return response.json();
  }

'''