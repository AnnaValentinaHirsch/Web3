*GitHub Repository "near/near-public-lakehouse"*

'''--- README.md ---
# near-public-lakehouse

NEAR Public Lakehouse

This repository contains the source code for ingesting NEAR Protocol data stored as JSON files in AWS S3 by [near-lake-indexer](https://github.com/near/near-lake-indexer). The data is loaded in a streaming fashion using Databricks Autoloader into raw/bronze tables, and transformed with Databricks Delta Live Tables streaming jobs into cleaned/enriched/silver tables.

The silver tables are also copied into the GCP BigQuery Public Dataset.

# Intro
Blockchain data indexing in NEAR Public Lakehouse is for anyone who wants to make sense of blockchain data. This includes:
- **Users**: create queries to track NEAR assets, monitor transactions, or analyze onchain events at massive scale.
- **Researchers**: use indexed data for data science tasks including onchain activities, identifying trends, or feed AI/ML pipelines for predective analysis.
- **Startups**: can use NEAR's indexed data for deep insights on user engagement, smart contract utilization, or insights across tokens and NFT adoption.

Benefits:
- **NEAR instant insights**: Historical onchain data queried at scale.
- **Cost-effective**: eliminate the need to store and process bulk NEAR protocol data; query as little or as much data as preferred.
- **Easy to use**: no prior experience with blockchain technology required; bring a general knowledge of SQL to unlock insights.

# Architecture

![Architecture](./docs/Architecture.png "Architecture")
Note: [Databricks Medallion Architecture](https://www.databricks.com/glossary/medallion-architecture)

# What is NEAR Protocol?

NEAR is a user-friendly, carbon-neutral blockchain, built from the ground up to be performant, secure, and infinitely scalable. It's a layer one, sharded, proof-of-stake blockchain designed with usability in mind. In simple terms, NEAR is blockchain for everyone.

# Data Available

The current data that we are providing was inspired by [near-indexer-for-explorer](https://github.com/near/near-indexer-for-explorer/).
We plan to improve the data available in the NEAR Public Lakehouse making it easier to consume by denormalizing some tables.

The tables available in the NEAR Public Lakehouse are:

- **blocks**: A structure that represents an entire block in the NEAR blockchain. Block is the main entity in NEAR Protocol blockchain. Blocks are produced in NEAR Protocol every second.
- **chunks**: A structure that represents a chunk in the NEAR blockchain. Chunk of a Block is a part of a Block from a Shard. The collection of Chunks of the Block forms the NEAR Protocol Block. Chunk contains all the structures that make the Block: Transactions, Receipts, and Chunk Header.
- **transactions**: Transaction is the main way of interraction between a user and a blockchain. Transaction contains: Signer account ID, Receiver account ID, and Actions.
- **execution_outcomes**: Execution outcome is the result of execution of Transaction or Receipt. In the result of the Transaction execution will always be a Receipt.
- **receipt_details**: All cross-contract (we assume that each account lives in its own shard) communication in Near happens through Receipts. Receipts are stateful in a sense that they serve not only as messages between accounts but also can be stored in the account storage to await DataReceipts. Each receipt has a predecessor_id (who sent it) and receiver_id the current account.
- **receipt_origin**: Tracks the transaction that originated the receipt.
- **receipt_actions**: Action Receipt represents a request to apply actions on the receiver_id side. It could be derived as a result of a Transaction execution or another ACTION Receipt processing. Action kind can be: ADD_KEY, CREATE_ACCOUNT, DELEGATE_ACTION, DELETE_ACCOUNT, DELETE_KEY, DEPLOY_CONTRACT, FUNCTION_CALL, STAKE, TRANSFER.
- **receipts (view)**: It's recommended to select only the columns and partitions (block_date) needed to avoid unnecessary query costs. This view join the receipt details, the transaction that originated the receipt and the receipt execution outcome.
- **account_changes**: Each account has an associated state where it stores its metadata and all the contract-related data (contract's code + storage).

# Examples

- Queries: How many unique users do I have for my smart contract per day?

```sql
SELECT
  r.block_date collected_for_day,
  COUNT(DISTINCT r.transaction_signer_account_id)
FROM `bigquery-public-data.crypto_near_mainnet_us.receipt_actions` ra
  INNER JOIN `bigquery-public-data.crypto_near_mainnet_us.receipts` r ON r.receipt_id = ra.receipt_id
WHERE ra.action_kind = 'FUNCTION_CALL'
  AND ra.receipt_receiver_account_id = 'near.social' -- change to your contract
GROUP BY 1
ORDER BY 1 DESC;
```

# How to get started?

1. Login into your [Google Cloud Account](https://console.cloud.google.com/).
2. Open the [NEAR Protocol BigQuery Public Dataset](https://console.cloud.google.com/marketplace/product/bigquery-public-data/crypto-near-mainnet).
3. Click in the [VIEW DATASET](https://console.cloud.google.com/bigquery?p=bigquery-public-data&d=crypto_near_mainnet_us&page=dataset) button.
4. Click in the "+" to create a new tab and write your query, click in the "RUN" button, and check the "Query results" below the query.
5. Done :)

# How much it costs?

- NEAR pays for the storage and doesn't charge you to use the public dataset. To learn more about BigQuery public datasets check this [page](https://cloud.google.com/bigquery/public-data).
- Google GCP charges for the queries that you perform on the data. For example, in today's price "Sep 1st, 2023" the On-demand (per TB) query pricing is $6.25 per TB where the first 1 TB per month is free. Please check the official Google's page for detailed pricing info, options, and best practices [here](https://cloud.google.com/bigquery/pricing#analysis_pricing_models).

**Note:** You can check how much data it will query before running it in the BigQuery console UI. Again, since BigQuery uses a columnar data structure and partitions, it's recommended to select only the columns and partitions (block_date) needed to avoid unnecessary query costs.

![Query Costs](./docs/BQ_Query_Cost.png "BQ Query Costs")

# References

- https://cloud.google.com/bigquery/public-data
- https://cloud.google.com/bigquery/pricing#analysis_pricing_models

- https://docs.gcp.databricks.com/ingestion/auto-loader/index.html
- https://www.databricks.com/product/delta-live-tables

- https://docs.near.org/concepts/basics/protocol
- https://docs.near.org/concepts/data-flow/near-data-flow
- https://near-indexers.io/docs/data-flow-and-structures/structures/transaction#actionview
- https://nomicon.io/RuntimeSpec/Receipts
- https://nomicon.io/
- https://github.com/near/near-lake-indexer
- https://github.com/near/near-indexer-for-explorer/

'''
'''--- rust-extract-apis/lockups/.env ---
RPC_URL=https://archival-rpc.mainnet.near.org
'''
'''--- rust-extract-apis/lockups/Cargo.toml ---
[package]
name = "lockups"
rust-version = "1.64"
version = "0.1.0"
edition = "2021"

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[dependencies]
actix-web = "4.5.1"
actix-cors = "0.7.0"
actix = "0.13.0"
anyhow = "1.0.51"
bigdecimal = "=0.1.0"
base64 = "0.11"
chrono = "0.4.19"
dotenv = "0.15.0"
# syn version conflict, replace with crates.io version once released
near-sdk = { git = "https://github.com/near/near-sdk-rs", rev="03487c184d37b0382dd9bd41c57466acad58fc1f" }
tokio = { version = "1.1", features = ["sync", "time"] }
tracing = "0.1.13"
tracing-stackdriver = "0.7.2" # GCP logs
tracing-subscriber = "0.3.15"
uint = { version = "0.8.3", default-features = false }

near-indexer-primitives = "0.17.0"
near-jsonrpc-client = "0.6.0"
near-jsonrpc-primitives = "0.17.0"

'''
'''--- rust-extract-apis/lockups/src/account_details.rs ---
use anyhow::Context;

use near_jsonrpc_client::{methods, JsonRpcClient};
use near_jsonrpc_primitives::types::query::QueryResponseKind;

pub(crate) async fn get_contract_code_hash(
    rpc_client: &JsonRpcClient,
    account_id: &near_indexer_primitives::types::AccountId,
    block_height: &near_indexer_primitives::types::BlockHeight,
) -> anyhow::Result<near_indexer_primitives::CryptoHash> {
    get_account_view_for_block_height(rpc_client, account_id, block_height)
        .await
        .map(|account| account.code_hash)
        .with_context(|| format!("Unable to get contract code hash for {}", account_id))
}

async fn get_account_view_for_block_height(
    rpc_client: &JsonRpcClient,
    account_id: &near_indexer_primitives::types::AccountId,
    block_height: &near_indexer_primitives::types::BlockHeight,
) -> anyhow::Result<near_indexer_primitives::views::AccountView> {
    let block_reference = near_indexer_primitives::types::BlockReference::BlockId(
        near_indexer_primitives::types::BlockId::Height(*block_height),
    );
    let request = near_indexer_primitives::views::QueryRequest::ViewAccount {
        account_id: account_id.clone(),
    };
    let query = methods::query::RpcQueryRequest {
        block_reference,
        request,
    };

    let account_response = rpc_client.call(query).await.with_context(|| {
        format!(
            "Failed to deliver ViewAccount for account {}, block {}",
            account_id, block_height
        )
    })?;

    match account_response.kind {
        QueryResponseKind::ViewAccount(account) => Ok(account),
        _ => anyhow::bail!(
            "Failed to extract ViewAccount response for account {}, block {}",
            account_id,
            block_height
        ),
    }
}

'''
'''--- rust-extract-apis/lockups/src/lockup.rs ---
use std::time::Duration;

use anyhow::Context;

use near_jsonrpc_client::{methods, JsonRpcClient};
use near_jsonrpc_primitives::types::query::QueryResponseKind;
use near_sdk::borsh::BorshDeserialize;
use near_sdk::json_types::{U128, U64};
use tracing::info;

use super::lockup_types::{
    LockupContract, TransfersInformation, VestingInformation, VestingSchedule, WrappedBalance, U256,
};

// The timestamp (nanos) when transfers were enabled in the Mainnet after community voting
// Tuesday, 13 October 2020 18:38:58.293
pub(super) const TRANSFERS_ENABLED: Duration = Duration::from_nanos(1602614338293769340);

pub(super) async fn get_lockup_contract_state(
    rpc_client: &JsonRpcClient,
    account_id: &near_indexer_primitives::types::AccountId,
    block_height: &near_indexer_primitives::types::BlockHeight,
) -> anyhow::Result<LockupContract> {
    let block_reference = near_indexer_primitives::types::BlockReference::BlockId(
        near_indexer_primitives::types::BlockId::Height(*block_height),
    );
    let request = near_indexer_primitives::views::QueryRequest::ViewState {
        account_id: account_id.clone(),
        prefix: vec![].into(),
        include_proof: false,
    };
    let query = methods::query::RpcQueryRequest {
        block_reference,
        request,
    };

    let state_response = rpc_client.call(query).await.with_context(|| {
        format!(
            "Failed to deliver ViewState for lockup contract {}, block_height {}",
            account_id, block_height
        )
    })?;

    let view_state_result = match state_response.kind {
        QueryResponseKind::ViewState(state) => state,
        _ => {
            anyhow::bail!(
                "Failed to extract ViewState response for lockup contract {}, block_height {}",
                account_id,
                block_height
            )
        }
    };

    let view_state = view_state_result.values.get(0).with_context(|| {
        format!(
            "Failed to find encoded lockup contract for {}, block_height {}",
            account_id, block_height
        )
    })?;

    let mut state = LockupContract::try_from_slice(&view_state.value)
        .with_context(|| format!("Failed to construct LockupContract for {}", account_id))?;

    // If owner of the lockup account didn't call the
    // `check_transfers_vote` contract method we won't be able to
    // get proper information based on timestamp, that's why we inject
    // the `transfer_timestamp` which is phase2 timestamp
    state.lockup_information.transfers_information = TransfersInformation::TransfersEnabled {
        transfers_timestamp: U64(TRANSFERS_ENABLED.as_nanos() as u64),
    };
    Ok(state)
}

// The lockup contract implementation had a bug that affected lockup start date.
// https://github.com/near/core-contracts/pull/136
// For each contract, we should choose the logic based on the binary version of the contract
pub(super) fn is_bug_inside_contract(
    code_hash: &near_indexer_primitives::CryptoHash,
    account_id: &near_indexer_primitives::types::AccountId,
) -> bool {
    match &*code_hash.to_string() {
        // The first implementation, with the bug
        "3kVY9qcVRoW3B5498SMX6R3rtSLiCdmBzKs7zcnzDJ7Q" => true,
        // We have 6 lockups created at 6th of April 2021, assume it's buggy
        "DiC9bKCqUHqoYqUXovAnqugiuntHWnM3cAc7KrgaHTu" => true,
        // Another 5 lockups created in May/June 2021, assume they are OK
        "Cw7bnyp4B6ypwvgZuMmJtY6rHsxP2D4PC8deqeJ3HP7D" => false,
        // Most recent contracts
        "4Pfw2RU6e35dUsHQQoFYfwX8KFFvSRNwMSNLXuSFHXrC" => false,
        "3skHaUtj85RPdUZwx6M4Jp4PfC9qJHqnsyuWLtuq2xBT" => false,
        _ => {
            info!(
                target: crate::LOCKUPS,
                "Assuming contract {} for account {} is not buggy", code_hash, account_id
            );
            false
        }
    }
}

// This is almost a copy of https://github.com/near/core-contracts/blob/master/lockup/src/getters.rs#L64
impl LockupContract {
    /// Returns the amount of tokens that are locked in the account due to lockup or vesting.
    pub fn get_locked_amount(&self, timestamp: u64, has_bug: bool) -> WrappedBalance {
        let lockup_amount = self.lockup_information.lockup_amount;
        if let TransfersInformation::TransfersEnabled {
            transfers_timestamp,
        } = &self.lockup_information.transfers_information
        {
            let lockup_timestamp = std::cmp::max(
                transfers_timestamp
                    .0
                    .saturating_add(self.lockup_information.lockup_duration),
                self.lockup_information.lockup_timestamp.unwrap_or(0),
            );
            let block_timestamp = timestamp;
            if lockup_timestamp <= block_timestamp {
                let unreleased_amount =
                    if let Some(release_duration) = self.lockup_information.release_duration {
                        let start_lockup = if has_bug {
                            transfers_timestamp.0
                        } else {
                            lockup_timestamp
                        };
                        let end_timestamp = start_lockup.saturating_add(release_duration);
                        if block_timestamp >= end_timestamp {
                            // Everything is released
                            0
                        } else {
                            let time_left = U256::from(end_timestamp - block_timestamp);
                            let unreleased_amount = U256::from(lockup_amount) * time_left
                                / U256::from(release_duration);
                            // The unreleased amount can't be larger than lockup_amount because the
                            // time_left is smaller than total_time.
                            unreleased_amount.as_u128()
                        }
                    } else {
                        0
                    };

                let unvested_amount = match &self.vesting_information {
                    VestingInformation::VestingSchedule(vs) => {
                        self.get_unvested_amount(vs.clone(), block_timestamp)
                    }
                    VestingInformation::Terminating(terminating) => terminating.unvested_amount,
                    // Vesting is private, so we can assume the vesting started before lockup date.
                    _ => U128(0),
                };
                return std::cmp::max(
                    unreleased_amount
                        .saturating_sub(self.lockup_information.termination_withdrawn_tokens),
                    unvested_amount.0,
                )
                .into();
            }
        }
        // The entire balance is still locked before the lockup timestamp.
        (lockup_amount - self.lockup_information.termination_withdrawn_tokens).into()
    }

    /// Returns the amount of tokens that are locked in this account due to vesting schedule.
    /// Takes raw vesting schedule, in case the internal vesting schedule is private.
    pub fn get_unvested_amount(
        &self,
        vesting_schedule: VestingSchedule,
        block_timestamp: u64,
    ) -> WrappedBalance {
        let lockup_amount = self.lockup_information.lockup_amount;
        match &self.vesting_information {
            VestingInformation::Terminating(termination_information) => {
                termination_information.unvested_amount
            }
            VestingInformation::None => U128::from(0),
            _ => {
                if block_timestamp < vesting_schedule.cliff_timestamp.0 {
                    // Before the cliff, nothing is vested
                    lockup_amount.into()
                } else if block_timestamp >= vesting_schedule.end_timestamp.0 {
                    // After the end, everything is vested
                    0.into()
                } else {
                    // cannot overflow since block_timestamp < vesting_schedule.end_timestamp
                    let time_left = U256::from(vesting_schedule.end_timestamp.0 - block_timestamp);
                    // The total time is positive. Checked at the contract initialization.
                    let total_time = U256::from(
                        vesting_schedule.end_timestamp.0 - vesting_schedule.start_timestamp.0,
                    );
                    let unvested_amount = U256::from(lockup_amount) * time_left / total_time;
                    // The unvested amount can't be larger than lockup_amount because the
                    // time_left is smaller than total_time.
                    unvested_amount.as_u128().into()
                }
            }
        }
    }
}

'''
'''--- rust-extract-apis/lockups/src/lockup_types.rs ---
#![allow(
    clippy::assign_op_pattern,
    clippy::manual_range_contains,
    clippy::ptr_offset_with_cast
)]
// Copied from lockup contract code
// https://github.com/near/core-contracts/blob/master/lockup/src/types.rs
// https://github.com/near/core-contracts/blob/master/lockup/src/lib.rs

use uint::construct_uint;

use near_sdk::borsh::{self, BorshDeserialize, BorshSerialize};
use near_sdk::json_types::{Base64VecU8, U128, U64};
use near_sdk::serde::{Deserialize, Serialize};
use near_sdk::{AccountId, Balance, BlockHeight};

construct_uint! {
    /// 256-bit unsigned integer.
    pub struct U256(4);
}

/// Raw type for duration in nanoseconds
pub type Duration = u64;
/// Raw type for timestamp in nanoseconds
pub type Timestamp = u64;

/// Timestamp in nanosecond wrapped into a struct for JSON serialization as a string.
pub type WrappedTimestamp = U64;
/// Balance wrapped into a struct for JSON serialization as a string.
pub type WrappedBalance = U128;

#[derive(BorshDeserialize, BorshSerialize)]
pub struct LockupContract {
    /// The account ID of the owner.
    pub owner_account_id: AccountId,

    /// Information about lockup schedule and the amount.
    pub lockup_information: LockupInformation,

    /// Information about vesting including schedule or termination status.
    pub vesting_information: VestingInformation,

    /// Account ID of the staking pool whitelist contract.
    pub staking_pool_whitelist_account_id: AccountId,

    /// Information about staking and delegation.
    /// `Some` means the staking information is available and the staking pool contract is selected.
    /// `None` means there is no staking pool selected.
    pub staking_information: Option<StakingInformation>,

    /// The account ID that the NEAR Foundation, that has the ability to terminate vesting.
    pub foundation_account_id: Option<AccountId>,
}

/// Contains information about token lockups.
#[derive(BorshDeserialize, BorshSerialize)]
pub struct LockupInformation {
    /// The amount in yocto-NEAR tokens locked for this account.
    pub lockup_amount: Balance,
    /// The amount of tokens that were withdrawn by NEAR foundation due to early termination
    /// of vesting.
    /// This amount has to be accounted separately from the lockup_amount to make sure
    /// linear release is not being affected.
    pub termination_withdrawn_tokens: Balance,
    /// [deprecated] - the duration in nanoseconds of the lockup period from
    /// the moment the transfers are enabled. During this period tokens are locked and
    /// the release doesn't start. Instead of this, use `lockup_timestamp` and `release_duration`
    pub lockup_duration: Duration,
    /// If present, it is the duration when the full lockup amount will be available. The tokens
    /// are linearly released from the moment tokens are unlocked, defined by:
    /// `max(transfers_timestamp + lockup_duration, lockup_timestamp)`.
    /// If not present, the tokens are not locked (though, vesting logic could be used).
    pub release_duration: Option<Duration>,
    /// The optional absolute lockup timestamp in nanoseconds which locks the tokens until this
    /// timestamp passes. Until this moment the tokens are locked and the release doesn't start.
    /// If not present, `transfers_timestamp` will be used.
    pub lockup_timestamp: Option<Timestamp>,
    /// The information about the transfers. Either transfers are already enabled, then it contains
    /// the timestamp when they were enabled. Or the transfers are currently disabled and
    /// it contains the account ID of the transfer poll contract.
    pub transfers_information: TransfersInformation,
}

/// Contains information about the transfers. Whether transfers are enabled or disabled.
#[derive(BorshDeserialize, BorshSerialize, Deserialize, Serialize, Debug)]
#[serde(crate = "near_sdk::serde")]
pub enum TransfersInformation {
    /// The timestamp when the transfers were enabled.
    TransfersEnabled {
        transfers_timestamp: WrappedTimestamp,
    },
    /// The account ID of the transfers poll contract, to check if the transfers are enabled.
    /// The lockup period can start only after the transfer voted to be enabled.
    /// At the launch of the network transfers are disabled for all lockup contracts, once transfers
    /// are enabled, they can't be disabled and don't need to be checked again.
    TransfersDisabled { transfer_poll_account_id: AccountId },
}

/// Describes the status of transactions with the staking pool contract or terminated unvesting
/// amount withdrawal.
#[derive(BorshDeserialize, BorshSerialize, Deserialize, Serialize, PartialEq, Eq)]
#[serde(crate = "near_sdk::serde")]
pub enum TransactionStatus {
    /// There are no transactions in progress.
    Idle,
    /// There is a transaction in progress.
    Busy,
}

/// Contains information about current stake and delegation.
#[derive(BorshDeserialize, BorshSerialize)]
pub struct StakingInformation {
    /// The Account ID of the staking pool contract.
    pub staking_pool_account_id: AccountId,

    /// Contains status whether there is a transaction in progress.
    pub status: TransactionStatus,

    /// The amount of tokens that were deposited from this account to the staking pool.
    /// NOTE: The unstaked amount on the staking pool might be higher due to staking rewards.
    pub deposit_amount: WrappedBalance,
}

/// Contains information about vesting schedule.
#[derive(BorshDeserialize, BorshSerialize, Deserialize, Serialize, Clone, PartialEq, Debug)]
#[serde(crate = "near_sdk::serde")]
pub struct VestingSchedule {
    /// The timestamp in nanosecond when the vesting starts. E.g. the start date of employment.
    pub start_timestamp: WrappedTimestamp,
    /// The timestamp in nanosecond when the first part of lockup tokens becomes vested.
    /// The remaining tokens will vest continuously until they are fully vested.
    /// Example: a 1 year of employment at which moment the 1/4 of tokens become vested.
    pub cliff_timestamp: WrappedTimestamp,
    /// The timestamp in nanosecond when the vesting ends.
    pub end_timestamp: WrappedTimestamp,
}

/// Initialization argument type to define the vesting schedule
#[derive(Serialize, Deserialize, Debug)]
#[serde(crate = "near_sdk::serde")]
pub enum VestingScheduleOrHash {
    /// [deprecated] After transfers are enabled, only public schedule is used.
    /// The vesting schedule is private and this is a hash of (vesting_schedule, salt).
    /// In JSON, the hash has to be encoded with base64 to a string.
    VestingHash(Base64VecU8),
    /// The vesting schedule (public)
    VestingSchedule(VestingSchedule),
}

/// Contains information about vesting that contains vesting schedule and termination information.
#[derive(Serialize, BorshDeserialize, BorshSerialize, PartialEq, Clone, Debug)]
#[serde(crate = "near_sdk::serde")]
pub enum VestingInformation {
    None,
    /// [deprecated] After transfers are enabled, only public schedule is used.
    /// Vesting schedule is hashed for privacy and only will be revealed if the NEAR foundation
    /// has to terminate vesting.
    /// The contract assume the vesting schedule doesn't affect lockup release and duration, because
    /// the vesting started before transfers were enabled and the duration is shorter or the same.
    VestingHash(Base64VecU8),
    /// Explicit vesting schedule.
    VestingSchedule(VestingSchedule),
    /// The information about the early termination of the vesting schedule.
    /// It means the termination of the vesting is currently in progress.
    /// Once the unvested amount is transferred out, `VestingInformation` is removed.
    Terminating(TerminationInformation),
}

/// Describes the status of transactions with the staking pool contract or terminated unvesting
/// amount withdrawal.
#[derive(
    BorshDeserialize, BorshSerialize, Deserialize, Serialize, PartialEq, Eq, Copy, Clone, Debug,
)]
#[serde(crate = "near_sdk::serde")]
pub enum TerminationStatus {
    /// Initial stage of the termination in case there are deficit on the account.
    VestingTerminatedWithDeficit,
    /// A transaction to unstake everything is in progress.
    UnstakingInProgress,
    /// The transaction to unstake everything from the staking pool has completed.
    EverythingUnstaked,
    /// A transaction to withdraw everything from the staking pool is in progress.
    WithdrawingFromStakingPoolInProgress,
    /// Everything is withdrawn from the staking pool. Ready to withdraw out of the account.
    ReadyToWithdraw,
    /// A transaction to withdraw tokens from the account is in progress.
    WithdrawingFromAccountInProgress,
}

/// Contains information about early termination of the vesting schedule.
#[derive(BorshDeserialize, BorshSerialize, Deserialize, Serialize, PartialEq, Clone, Debug)]
#[serde(crate = "near_sdk::serde")]
pub struct TerminationInformation {
    /// The amount of tokens that are unvested and has to be transferred back to NEAR Foundation.
    /// These tokens are effectively locked and can't be transferred out and can't be restaked.
    pub unvested_amount: WrappedBalance,

    /// The status of the withdrawal. When the unvested amount is in progress of withdrawal the
    /// status will be marked as busy, to avoid withdrawing the funds twice.
    pub status: TerminationStatus,
}

/// Contains a vesting schedule with a salt.
#[derive(BorshSerialize, Deserialize, Serialize, Clone, Debug)]
#[serde(crate = "near_sdk::serde")]
pub struct VestingScheduleWithSalt {
    /// The vesting schedule
    pub vesting_schedule: VestingSchedule,
    /// Salt to make the hash unique
    pub salt: Base64VecU8,
}

#[derive(Deserialize, Serialize, Clone, Debug)]
#[serde(crate = "near_sdk::serde")]
pub struct RequestLockupAmount {
    pub lockup_account_id: String,
    pub block_id: BlockHeight,
    pub block_timestamp: Timestamp,
}

#[derive(Deserialize, Serialize, Clone, Debug)]
#[serde(crate = "near_sdk::serde")]
pub struct ResponseLockupAmount {
    pub lockup_amount: Balance,
}
'''
'''--- rust-extract-apis/lockups/src/main.rs ---

use actix_web::{get, post, web, App, HttpResponse, HttpServer, Responder, Result};

use anyhow::Context;
use tracing::info;

use near_jsonrpc_client::JsonRpcClient;

mod account_details;
mod lockup;
mod lockup_types;

struct AppState {
    rpc_client: JsonRpcClient,
}

const LOCKUPS: &str = "lockups";

#[get("/health")]
async fn health() -> impl Responder {
    HttpResponse::Ok().body("OK")
}

#[post("/lockup_amount")]
async fn lockup_amount(req: web::Json<lockup_types::RequestLockupAmount>, data: web::Data<AppState>) -> Result<impl Responder>  {
    let rpc_client = &data.rpc_client;

    let block_height = req.block_id;
    let block_timestamp = req.block_timestamp;
    let lockup_account_id: near_indexer_primitives::types::AccountId = req.lockup_account_id.parse().unwrap();

    let amount = compute_lockup_amount(&rpc_client, &block_height, block_timestamp, &lockup_account_id).await.unwrap();

    let response = lockup_types::ResponseLockupAmount {
        lockup_amount: amount,
    };

    info!(target: crate::LOCKUPS, "{:?} {:?} {:?} {:?}", req.block_id, req.lockup_account_id, block_timestamp, amount);

    Ok(web::Json(response))
}

#[actix_web::main]
async fn main() -> std::io::Result<()> {
    dotenv::dotenv().ok();

    HttpServer::new(|| {
        App::new()
            .app_data(web::Data::new(AppState {
                rpc_client: JsonRpcClient::connect(
                    std::env::var("RPC_URL").expect("RPC_URL must be set in either .env or environment"),
                ),
            }))
            .service(health)
            .service(lockup_amount)
    })
    .bind(("0.0.0.0", 8080))?
    .run()
    .await
}

async fn compute_lockup_amount(
    rpc_client: &JsonRpcClient,
    block_height: &u64,
    block_timestamp: u64,
    lockup_account_id: &near_indexer_primitives::types::AccountId,
) -> anyhow::Result<u128> {

    let state = lockup::get_lockup_contract_state(rpc_client, lockup_account_id, &block_height)
        .await
        .with_context(|| {
            format!(
                "Failed to get lockup contract details for {}",
                lockup_account_id
            )
        })?;
    let code_hash =
        account_details::get_contract_code_hash(rpc_client, lockup_account_id, &block_height)
            .await?;
    let is_lockup_with_bug = lockup::is_bug_inside_contract(&code_hash, lockup_account_id);
    let locked_amount = state
        .get_locked_amount(block_timestamp, is_lockup_with_bug)
        .0;

    Ok(locked_amount)
}

'''
'''--- src/lakehouse/notebooks/BQ Writer Backfill from Genesis 2020-07-21 to 2023-08-28.py ---
# Databricks notebook source
# MAGIC %md
# MAGIC # BQ Writer Backfill from Genesis 2020-07-21 to 2023-08-28

# COMMAND ----------

def bq_writer_backfill(table_name, sql):
    df = spark.sql(sql)

    df.write \
        .format("bigquery") \
        .option("temporaryGcsBucket", 'databricks-bq-buffer-near-lakehouse') \
        .option("table", f"pagoda-data-platform.crypto_near_mainnet.{table_name}") \
        .option("createDisposition", "CREATE_IF_NEEDED") \
        .option("partitionField", "block_date") \
        .option("partitionType", "DAY") \
        .option("allowFieldAddition", "true") \
        .option("allowFieldRelaxation", "true") \
        .option("writeMethod", "indirect") \
        .mode("append").save()  

# COMMAND ----------

sql = """
        SELECT 
            block_date, 
            block_height, 
            block_timestamp, 
            block_timestamp_utc, 
            block_hash, 
            prev_block_hash, 
            CAST(total_supply AS DOUBLE) AS total_supply, 
            CAST(gas_price AS DOUBLE) AS gas_price, 
            author_account_id 
        FROM hive_metastore.mainnet.silver_blocks 
        WHERE block_date >= '2020-01-01' AND  block_date <= '2023-08-28'
    """
bq_writer_backfill("blocks", sql)

# COMMAND ----------

sql = """
        SELECT 
            block_date, 
            block_height, 
            block_timestamp, 
            block_timestamp_utc, 
            block_hash, 
            chunk_hash, 
            shard_id, 
            hash AS transaction_hash, 
            index_in_chunk, 
            signer_id AS signer_account_id, 
            public_key AS signer_public_key, 
            nonce, 
            receiver_id AS receiver_account_id, 
            signature, 
            status, 
            converted_into_receipt_id, 
            receipt_conversion_gas_burnt, 
            CAST(receipt_conversion_tokens_burnt AS DOUBLE) AS receipt_conversion_tokens_burnt 
        FROM hive_metastore.mainnet.silver_transactions 
        WHERE block_date >= '2020-01-01' AND  block_date <= '2023-08-28'
    """
bq_writer_backfill("transactions", sql)

# COMMAND ----------

sql = """
        SELECT 
            block_date, 
            block_height, 
            block_timestamp, 
            block_timestamp_utc, 
            block_hash, 
            chunk_hash, 
            shard_id, 
            index_in_chunk, 
            receipt_kind, 
            receipt_id, 
            data_id, 
            predecessor_account_id, 
            receiver_account_id, 
            receipt 
        FROM hive_metastore.mainnet.silver_receipts 
        WHERE block_date >= '2020-01-01' AND  block_date <= '2023-08-28'
    """

bq_writer_backfill("receipt_details", sql)

# COMMAND ----------

sql = """
        SELECT 
            block_date, 
            block_height, 
            block_timestamp, 
            block_timestamp_utc, 
            block_hash, 
            chunk_hash, 
            shard_id, 
            index_in_action_receipt, 
            receipt_id, 
            args, 
            receipt_predecessor_account_id, 
            action_kind, 
            receipt_receiver_account_id, 
            is_delegate_action 
        FROM hive_metastore.mainnet.silver_action_receipt_actions 
        WHERE block_date >= '2020-01-01' AND  block_date <= '2023-08-28'
    """

bq_writer_backfill("receipt_actions", sql)

# COMMAND ----------

sql = """
        SELECT 
            block_date, 
            block_height, 
            block_timestamp, 
            block_timestamp_utc, 
            block_hash, 
            chunk_hash, 
            index_in_block, 
            affected_account_id, 
            caused_by_transaction_hash, 
            caused_by_receipt_id, 
            update_reason, 
            CAST(affected_account_nonstaked_balance AS DOUBLE) AS affected_account_nonstaked_balance, 
            CAST(affected_account_staked_balance AS DOUBLE) AS affected_account_staked_balance, 
            CAST(affected_account_storage_usage AS DOUBLE) AS affected_account_storage_usage 
        FROM hive_metastore.mainnet.silver_account_changes 
        WHERE block_date >= '2020-01-01' AND  block_date <= '2023-08-28'
    """

bq_writer_backfill("account_changes", sql)

# COMMAND ----------

sql = """
        SELECT 
            block_date, 
            block_height, 
            block_timestamp, 
            block_timestamp_utc, 
            block_hash, 
            chunk_hash, 
            shard_id, 
            receipt_id, 
            executed_in_block_hash, 
            outcome_receipt_ids, 
            index_in_chunk, 
            CAST(gas_burnt AS DOUBLE) AS gas_burnt, 
            CAST(tokens_burnt AS DOUBLE) AS tokens_burnt, 
            executor_account_id, 
            status ,
            logs 
        FROM hive_metastore.mainnet.silver_execution_outcomes 
        WHERE block_date >= '2020-01-01' AND  block_date <= '2023-08-28'
    """

bq_writer_backfill("execution_outcomes", sql)

# COMMAND ----------

sql = """
        SELECT 
            block_date, 
            block_height, 
            block_timestamp, 
            block_timestamp_utc, 
            block_hash, 
            chunk_hash, 
            shard_id, 
            signature, 
            CAST(gas_limit AS DOUBLE) AS gas_limit, 
            CAST(gas_used AS DOUBLE) AS gas_used, 
            author_account_id 
        FROM hive_metastore.mainnet.silver_chunks 
    """

bq_writer_backfill("chunks", f"{sql} WHERE block_date >= '2020-01-01' AND  block_date <= '2020-12-31'")
bq_writer_backfill("chunks", f"{sql} WHERE block_date >= '2021-01-01' AND  block_date <= '2021-12-31'")
bq_writer_backfill("chunks", f"{sql} WHERE block_date >= '2022-01-01' AND  block_date <= '2022-06-30'")
bq_writer_backfill("chunks", f"{sql} WHERE block_date >= '2022-07-01' AND  block_date <= '2022-12-31'")
bq_writer_backfill("chunks", f"{sql} WHERE block_date >= '2023-01-01' AND  block_date <= '2023-08-28'")

# COMMAND ----------

sql = """
        SELECT 
            block_height,
            block_timestamp_utc,
            block_date,
            signer_id,
            true_signer_id,
            predecessor_id,
            receipt_id,
            contract_id,
            method_name,
            deposit,
            gas,
            account_object,
            widget, 
            post, 
            profile, 
            graph, 
            settings,
            badge, 
            index
        FROM hive_metastore.mainnet.silver_near_social_txs_parsed 
        WHERE block_date >= '2020-01-01' AND  block_date <= '2023-10-09'
    """

bq_writer_backfill("near_social_transactions", sql)

# COMMAND ----------

'''
'''--- src/lakehouse/notebooks/BQ Writer Stream.py ---
# Databricks notebook source
# MAGIC %md
# MAGIC # BQ Writer Stream

# COMMAND ----------

def create_stream_temp_view(view_name, source_table_name, source_filter):
    return spark.readStream \
    .format("delta") \
    .option("skipChangeCommits", "true") \
    .table(source_table_name) \
    .where(source_filter) \
    .createOrReplaceTempView(view_name)

# COMMAND ----------

def bq_writer_stream(table_name, df_stream):
    return df_stream.writeStream \
        .format("bigquery") \
        .option("temporaryGcsBucket", "databricks-bq-buffer-near-lakehouse") \
        .option("table", f"pagoda-data-platform.crypto_near_mainnet.{table_name}") \
        .option("createDisposition", "CREATE_IF_NEEDED") \
        .option("partitionField", "block_date") \
        .option("partitionType", "DAY") \
        .option("allowFieldAddition", "true") \
        .option("allowFieldRelaxation", "true") \
        .option("writeMethod", "indirect") \
        .option("checkpointLocation", f"/pipelines/checkpoints/bc_{table_name}") \
        .start()

# COMMAND ----------

create_stream_temp_view("tmp_vw_blocks", "hive_metastore.mainnet.silver_blocks", "block_date >= '2023-08-29'")

df_stream_blocks = spark.sql("""
        select 
            block_date, 
            block_height, 
            block_timestamp, 
            block_timestamp_utc, 
            block_hash, 
            prev_block_hash, 
            CAST(total_supply AS DOUBLE) as total_supply, 
            CAST(gas_price AS DOUBLE) as gas_price, 
            author_account_id 
        from tmp_vw_blocks
    """)

bq_writer_stream("blocks", df_stream_blocks)

# COMMAND ----------

create_stream_temp_view("tmp_vw_transactions", "hive_metastore.mainnet.silver_transactions", "block_date >= '2023-08-29'")

df_stream_transactions = spark.sql("""
        select 
            block_date, 
            block_height, 
            block_timestamp, 
            block_timestamp_utc, 
            block_hash, 
            chunk_hash, 
            shard_id, 
            hash AS transaction_hash, 
            index_in_chunk, 
            signer_id AS signer_account_id, 
            public_key AS signer_public_key, 
            nonce, 
            receiver_id AS receiver_account_id, 
            signature, 
            status, 
            converted_into_receipt_id, 
            receipt_conversion_gas_burnt, 
            CAST(receipt_conversion_tokens_burnt AS DOUBLE) AS receipt_conversion_tokens_burnt 
        from tmp_vw_transactions
    """)

bq_writer_stream("transactions", df_stream_transactions)

# COMMAND ----------

create_stream_temp_view("tmp_vw_receipt_details", "hive_metastore.mainnet.silver_receipts", "block_date >= '2023-08-29'")

df_stream_receipt_details = spark.sql("""
        select 
            block_date, 
            block_height, 
            block_timestamp, 
            block_timestamp_utc, 
            block_hash, 
            chunk_hash, 
            shard_id, 
            index_in_chunk, 
            receipt_kind, 
            receipt_id, 
            data_id, 
            predecessor_account_id, 
            receiver_account_id, 
            receipt 
        from tmp_vw_receipt_details
    """)

bq_writer_stream("receipt_details", df_stream_receipt_details)

# COMMAND ----------

create_stream_temp_view("tmp_vw_receipt_actions", "hive_metastore.mainnet.silver_action_receipt_actions", "block_date >= '2023-08-29'")

df_stream_receipt_actions = spark.sql("""
        select 
            block_date, 
            block_height, 
            block_timestamp, 
            block_timestamp_utc, 
            block_hash, 
            chunk_hash, 
            shard_id, 
            index_in_action_receipt, 
            receipt_id, 
            args, 
            receipt_predecessor_account_id, 
            action_kind, 
            receipt_receiver_account_id, 
            is_delegate_action 
        from tmp_vw_receipt_actions
    """)

bq_writer_stream("receipt_actions", df_stream_receipt_actions)

# COMMAND ----------

create_stream_temp_view("tmp_vw_account_changes", "hive_metastore.mainnet.silver_account_changes", "block_date >= '2023-08-29'")

df_stream_account_changes = spark.sql("""
        select 
            block_date, 
            block_height, 
            block_timestamp, 
            block_timestamp_utc, 
            block_hash, 
            chunk_hash, 
            index_in_block, 
            affected_account_id, 
            caused_by_transaction_hash, 
            caused_by_receipt_id, 
            update_reason, 
            CAST(affected_account_nonstaked_balance AS DOUBLE) AS affected_account_nonstaked_balance, 
            CAST(affected_account_staked_balance AS DOUBLE) AS affected_account_staked_balance, 
            CAST(affected_account_storage_usage AS DOUBLE) AS affected_account_storage_usage 
        from tmp_vw_account_changes
    """)

bq_writer_stream("account_changes", df_stream_account_changes)

# COMMAND ----------

create_stream_temp_view("tmp_vw_execution_outcomes", "hive_metastore.mainnet.silver_execution_outcomes", "block_date >= '2023-08-29'")

df_stream_execution_outcomes = spark.sql("""
        select 
            block_date, 
            block_height, 
            block_timestamp, 
            block_timestamp_utc, 
            block_hash, 
            chunk_hash, 
            shard_id, 
            receipt_id, 
            executed_in_block_hash, 
            outcome_receipt_ids, 
            index_in_chunk, 
            CAST(gas_burnt AS DOUBLE) AS gas_burnt, 
            CAST(tokens_burnt AS DOUBLE) AS tokens_burnt, 
            executor_account_id, 
            status, 
            logs
        from tmp_vw_execution_outcomes
    """)

bq_writer_stream("execution_outcomes", df_stream_execution_outcomes)

# COMMAND ----------

create_stream_temp_view("tmp_vw_receipt_origin", "hive_metastore.mainnet.silver_receipt_originated_from_transaction", "originated_from_transaction_hash IS NOT NULL")

df_stream_receipt_origin = spark.sql("""
        select 
            block_date, 
            block_height, 
            receipt_id, 
            data_id, 
            receipt_kind, 
            originated_from_transaction_hash 
        from tmp_vw_receipt_origin
    """)

bq_writer_stream("receipt_origin", df_stream_receipt_origin)

# COMMAND ----------

create_stream_temp_view("tmp_vw_near_social_txs", "hive_metastore.mainnet.silver_near_social_txs_parsed", "block_date >= '2023-10-10'")

df_stream_near_social = spark.sql("""
        select 
            block_height,
            block_timestamp_utc,
            block_date,
            signer_id,
            true_signer_id,
            predecessor_id,
            receipt_id,
            contract_id,
            method_name,
            deposit,
            gas,
            account_object,
            widget, 
            post, 
            profile, 
            graph, 
            settings,
            badge, 
            index
        from tmp_vw_near_social_txs
    """)

bq_writer_stream("near_social_transactions", df_stream_near_social)

# COMMAND ----------

'''
'''--- src/lakehouse/notebooks/BQ Writer Views & Data Dictionary.py ---
# Databricks notebook source
# MAGIC %md
# MAGIC # BQ Writer Views & Data Dictionary

# COMMAND ----------

!pip install --upgrade google-cloud
!pip install --upgrade google-cloud-bigquery
!pip install --upgrade google-cloud-storage

# COMMAND ----------

import base64
import json
from google.oauth2 import service_account
from google.cloud import bigquery

credentials = dbutils.secrets.get(scope = "gcp_data_platform_bq", key = "credentials")
project_id = dbutils.secrets.get(scope = "gcp_data_platform_bq", key = "project_id")

decoded_string = base64.b64decode(credentials).decode("ascii")
sa_credentials = service_account.Credentials.from_service_account_info(json.loads(decoded_string))
scoped_credentials = sa_credentials.with_scopes(['https://www.googleapis.com/auth/cloud-platform'])

bq_client = bigquery.Client(project=project_id, credentials=scoped_credentials)

# COMMAND ----------

# Blocks
dataset_ref = bigquery.DatasetReference(project_id, "crypto_near_mainnet")
table_ref = dataset_ref.table('blocks')
table = bq_client.get_table(table_ref)

table.description = "A structure that represents an entire block in the NEAR blockchain. Block is the main entity in NEAR Protocol blockchain. Blocks are produced in NEAR Protocol every second."
bq_client.update_table(table, ["description"])

sql = (
        f'ALTER TABLE `{project_id}.crypto_near_mainnet.blocks` '
        'ALTER COLUMN block_date SET OPTIONS (description = \'The date of the Block. Used to partition the table\'), '
        'ALTER COLUMN block_height SET OPTIONS (description = \'The height of the Block\'),'
        'ALTER COLUMN block_timestamp SET OPTIONS (description = \'The timestamp of the Block in nanoseconds\'),'
        'ALTER COLUMN block_timestamp_utc SET OPTIONS (description = \'The timestamp of the Block in UTC\'),'
        'ALTER COLUMN block_hash SET OPTIONS (description = \'The hash of the Block\'),'
        'ALTER COLUMN prev_block_hash SET OPTIONS (description = \'The hash of the previous Block\'),'
        'ALTER COLUMN gas_price SET OPTIONS (description = \'The gas price of the Block\'),'
        'ALTER COLUMN total_supply SET OPTIONS (description = \'The total supply of the Block\'),'
        'ALTER COLUMN author_account_id SET OPTIONS (description = \'The AccountId of the author of the Block\')'
    )
query_job = bq_client.query(sql)  
query_job.result()

# COMMAND ----------

# Chunks
dataset_ref = bigquery.DatasetReference(project_id, "crypto_near_mainnet")
table_ref = dataset_ref.table('chunks')
table = bq_client.get_table(table_ref)

table.description = "A structure that represents a chunk in the NEAR blockchain. Chunk of a Block is a part of a Block from a Shard. The collection of Chunks of the Block forms the NEAR Protocol Block. Chunk contains all the structures that make the Block: Transactions, Receipts, and Chunk Header."
bq_client.update_table(table, ["description"])

sql = (
        f'ALTER TABLE `{project_id}.crypto_near_mainnet.chunks` '
        'ALTER COLUMN block_date SET OPTIONS (description = \'The date of the Block. Used to partition the table\'), '
        'ALTER COLUMN block_height SET OPTIONS (description = \'The height of the Block\'),'
        'ALTER COLUMN block_timestamp SET OPTIONS (description = \'The timestamp of the Block in nanoseconds\'),'
        'ALTER COLUMN block_timestamp_utc SET OPTIONS (description = \'The timestamp of the Block in UTC\'),'
        'ALTER COLUMN block_hash SET OPTIONS (description = \'The hash of the Block\'),'
        'ALTER COLUMN chunk_hash SET OPTIONS (description = \'The hash of the Chunk\'),'
        'ALTER COLUMN shard_id SET OPTIONS (description = \'The shard ID of the Chunk\'),'
        'ALTER COLUMN signature SET OPTIONS (description = \'The signature of the Chunk\'),'
        'ALTER COLUMN gas_limit SET OPTIONS (description = \'The gas limit of the Chunk\'),'
        'ALTER COLUMN gas_used SET OPTIONS (description = \'The amount of gas spent on computations of the Chunk\'),'
        'ALTER COLUMN author_account_id SET OPTIONS (description = \'The AccountId of the author of the Chunk\')'
    )
query_job = bq_client.query(sql)  
query_job.result()

# COMMAND ----------

# Transactions
dataset_ref = bigquery.DatasetReference(project_id, "crypto_near_mainnet")
table_ref = dataset_ref.table('transactions')
table = bq_client.get_table(table_ref)

table.description = "Transaction is the main way of interraction between a user and a blockchain. Transaction contains: Signer account ID, Receiver account ID, and Actions."
bq_client.update_table(table, ["description"])

sql = (
        f'ALTER TABLE `{project_id}.crypto_near_mainnet.transactions` '
        'ALTER COLUMN block_date SET OPTIONS (description = \'The date of the Block. Used to partition the table\'), '
        'ALTER COLUMN block_height SET OPTIONS (description = \'The height of the Block\'),'
        'ALTER COLUMN block_timestamp SET OPTIONS (description = \'The timestamp of the Block in nanoseconds\'),'
        'ALTER COLUMN block_timestamp_utc SET OPTIONS (description = \'The timestamp of the Block in UTC\'),'
        'ALTER COLUMN block_hash SET OPTIONS (description = \'The hash of the Block\'),'
        'ALTER COLUMN chunk_hash SET OPTIONS (description = \'The hash of the Chunk\'),'
        'ALTER COLUMN shard_id SET OPTIONS (description = \'The shard ID of the Chunk\'),'
        'ALTER COLUMN transaction_hash SET OPTIONS (description = \'The transaction hash\'),'
        'ALTER COLUMN index_in_chunk SET OPTIONS (description = \'The index in the Chunk\'),'
        'ALTER COLUMN signer_account_id SET OPTIONS (description = \'An account on which behalf transaction is signed\'),'
        'ALTER COLUMN signer_public_key SET OPTIONS (description = \'An access key which was used to sign a transaction\'),'
        'ALTER COLUMN nonce SET OPTIONS (description = \'Nonce is used to determine order of transaction in the pool. It increments for a combination of `signer_id` and `public_key`\'),'
        'ALTER COLUMN receiver_account_id SET OPTIONS (description = \'Receiver account for this transaction\'),'
        'ALTER COLUMN signature SET OPTIONS (description = \'A signature of a hash of the Borsh-serialized Transaction\'),'
        'ALTER COLUMN converted_into_receipt_id SET OPTIONS (description = \'Receipt ID that the transaction was converted.\'),'
        'ALTER COLUMN receipt_conversion_gas_burnt SET OPTIONS (description = \'Gas burnt in the receipt conversion\'),'
        'ALTER COLUMN receipt_conversion_tokens_burnt SET OPTIONS (description = \'Tokens burnt in the receipt conversion\'),'
        'ALTER COLUMN status SET OPTIONS (description = \'Transaction status\')'
    )
query_job = bq_client.query(sql)  
query_job.result()

# COMMAND ----------

# Execution Outcomes
dataset_ref = bigquery.DatasetReference(project_id, "crypto_near_mainnet")
table_ref = dataset_ref.table('execution_outcomes')
table = bq_client.get_table(table_ref)

table.description = "ExecutionOutcome is the result of execution of Transaction or Receipt. In the result of the Transaction execution will always be a Receipt."
bq_client.update_table(table, ["description"])

sql = (
        f'ALTER TABLE `{project_id}.crypto_near_mainnet.execution_outcomes` '
        'ALTER COLUMN block_date SET OPTIONS (description = \'The date of the Block. Used to partition the table\'), '
        'ALTER COLUMN block_height SET OPTIONS (description = \'The height of the Block\'),'
        'ALTER COLUMN block_timestamp SET OPTIONS (description = \'The timestamp of the Block in nanoseconds\'),'
        'ALTER COLUMN block_timestamp_utc SET OPTIONS (description = \'The timestamp of the Block in UTC\'),'
        'ALTER COLUMN block_hash SET OPTIONS (description = \'The hash of the Block\'),'
        'ALTER COLUMN chunk_hash SET OPTIONS (description = \'The hash of the Chunk\'),'
        'ALTER COLUMN shard_id SET OPTIONS (description = \'The shard ID of the Chunk\'),'
        'ALTER COLUMN receipt_id SET OPTIONS (description = \'The receipt ID\'),'
        'ALTER COLUMN executed_in_block_hash SET OPTIONS (description = \'The Block hash\'),'
        'ALTER COLUMN outcome_receipt_ids SET OPTIONS (description = \'Receipt IDs generated by this transaction or receipt\'),'
        'ALTER COLUMN index_in_chunk SET OPTIONS (description = \'The index in the Chunk\'),'
        'ALTER COLUMN gas_burnt SET OPTIONS (description = \'The amount of the gas burnt by the given transaction or receipt\'),'
        'ALTER COLUMN tokens_burnt SET OPTIONS (description = \'The amount of tokens burnt corresponding to the burnt gas amount. This value does not always equal to the `gas_burnt` multiplied by the gas price, because the prepaid gas price might be lower than the actual gas price and it creates a deficit\'),'
        'ALTER COLUMN executor_account_id SET OPTIONS (description = \'The id of the account on which the execution happens. For transaction this is signer_id, for receipt this is receiver_id\'),'
        'ALTER COLUMN logs SET OPTIONS (description = \'Execution outcome logs\'),'
        'ALTER COLUMN status SET OPTIONS (description = \'Execution status. Contains the result in case of successful execution\')'
    )
query_job = bq_client.query(sql)  
query_job.result()

# COMMAND ----------

# account_changes
dataset_ref = bigquery.DatasetReference(project_id, "crypto_near_mainnet")
table_ref = dataset_ref.table('account_changes')
table = bq_client.get_table(table_ref)

table.description = "Describes how account's state has changed and the reason."
bq_client.update_table(table, ["description"])

sql = (
        f'ALTER TABLE `{project_id}.crypto_near_mainnet.account_changes` '
        'ALTER COLUMN block_date SET OPTIONS (description = \'The date of the Block. Used to partition the table\'), '
        'ALTER COLUMN block_height SET OPTIONS (description = \'The height of the Block\'),'
        'ALTER COLUMN block_timestamp SET OPTIONS (description = \'The timestamp of the Block in nanoseconds\'),'
        'ALTER COLUMN block_timestamp_utc SET OPTIONS (description = \'The timestamp of the Block in UTC\'),'
        'ALTER COLUMN block_hash SET OPTIONS (description = \'The hash of the Block\'),'
        'ALTER COLUMN chunk_hash SET OPTIONS (description = \'The hash of the Chunk\'),'
        'ALTER COLUMN index_in_block SET OPTIONS (description = \'The index in the Block\'),'
        'ALTER COLUMN affected_account_id SET OPTIONS (description = \'Account ID affected by the change\'),'
        'ALTER COLUMN caused_by_transaction_hash SET OPTIONS (description = \'The transaction hash that caused the change\'),'
        'ALTER COLUMN caused_by_receipt_id SET OPTIONS (description = \'The receipt ID that caused the change\'),'
        'ALTER COLUMN update_reason SET OPTIONS (description = \'The update reason\'),'
        'ALTER COLUMN affected_account_nonstaked_balance SET OPTIONS (description = \'Non stacked balance\'),'
        'ALTER COLUMN affected_account_staked_balance SET OPTIONS (description = \'Stacked balance\'),'
        'ALTER COLUMN affected_account_storage_usage SET OPTIONS (description = \'Storage usage\')'
    )
query_job = bq_client.query(sql)  
query_job.result()

# COMMAND ----------

# receipt_details
dataset_ref = bigquery.DatasetReference(project_id, "crypto_near_mainnet")
table_ref = dataset_ref.table('receipt_details')
table = bq_client.get_table(table_ref)

table.description = "All cross-contract (we assume that each account lives in its own shard) communication in Near happens through Receipts. Receipts are stateful in a sense that they serve not only as messages between accounts but also can be stored in the account storage to await DataReceipts. Each receipt has a predecessor_id (who sent it) and receiver_id the current account. "
bq_client.update_table(table, ["description"])

sql = (
        f'ALTER TABLE `{project_id}.crypto_near_mainnet.receipt_details` '
        'ALTER COLUMN block_date SET OPTIONS (description = \'The date of the Block. Used to partition the table\'), '
        'ALTER COLUMN block_height SET OPTIONS (description = \'The height of the Block\'),'
        'ALTER COLUMN block_timestamp SET OPTIONS (description = \'The timestamp of the Block in nanoseconds\'),'
        'ALTER COLUMN block_timestamp_utc SET OPTIONS (description = \'The timestamp of the Block in UTC\'),'
        'ALTER COLUMN block_hash SET OPTIONS (description = \'The hash of the Block\'),'
        'ALTER COLUMN chunk_hash SET OPTIONS (description = \'The hash of the Chunk\'),'
        'ALTER COLUMN shard_id SET OPTIONS (description = \'The shard ID of the Chunk\'),'
        'ALTER COLUMN index_in_chunk SET OPTIONS (description = \'The index in the Chunk\'),'
        'ALTER COLUMN receipt_kind SET OPTIONS (description = \'There are 2 types of Receipt: ACTION and DATA. An ACTION receipt is a request to apply Actions, while a DATA receipt is a result of the application of these actions\'),'
        'ALTER COLUMN receipt_id SET OPTIONS (description = \'An unique id for the receipt\'),'
        'ALTER COLUMN data_id SET OPTIONS (description = \'An unique DATA receipt identifier\'),'
        'ALTER COLUMN predecessor_account_id SET OPTIONS (description = \'The account ID which issued a receipt. In case of a gas or deposit refund, the account ID is system\'),'
        'ALTER COLUMN receiver_account_id SET OPTIONS (description = \'The destination account ID\'),'
        'ALTER COLUMN receipt SET OPTIONS (description = \'Receipt details\')'
    )
query_job = bq_client.query(sql)  
query_job.result()

# COMMAND ----------

# receipt_origin
dataset_ref = bigquery.DatasetReference(project_id, "crypto_near_mainnet")
table_ref = dataset_ref.table('receipt_origin')
table = bq_client.get_table(table_ref)

table.description = "Tracks the transaction that originated the receipt"
bq_client.update_table(table, ["description"])

sql = (
        f'ALTER TABLE `{project_id}.crypto_near_mainnet.receipt_origin` '
        'ALTER COLUMN block_date SET OPTIONS (description = \'The date of the Block. Used to partition the table\'), '
        'ALTER COLUMN block_height SET OPTIONS (description = \'The height of the Block\'),'
        'ALTER COLUMN receipt_kind SET OPTIONS (description = \'There are 2 types of Receipt: ACTION and DATA. An ACTION receipt is a request to apply Actions, while a DATA receipt is a result of the application of these actions\'),'
        'ALTER COLUMN receipt_id SET OPTIONS (description = \'An unique id for the receipt\'),'
        'ALTER COLUMN data_id SET OPTIONS (description = \'An unique DATA receipt identifier\'),'
        'ALTER COLUMN originated_from_transaction_hash SET OPTIONS (description = \'The transaction hash that originated the receipt\')'
    )
query_job = bq_client.query(sql)  
query_job.result()

# COMMAND ----------

# receipt_actions
dataset_ref = bigquery.DatasetReference(project_id, "crypto_near_mainnet")
table_ref = dataset_ref.table('receipt_actions')
table = bq_client.get_table(table_ref)

table.description = "Action Receipt represents a request to apply actions on the receiver_id side. It could be derived as a result of a Transaction execution or another ACTION Receipt processing. Action kind can be: ADD_KEY, CREATE_ACCOUNT, DELEGATE_ACTION, DELETE_ACCOUNT, DELETE_KEY, DEPLOY_CONTRACT, FUNCTION_CALL, STAKE, TRANSFER"
bq_client.update_table(table, ["description"])

sql = (
        f'ALTER TABLE `{project_id}.crypto_near_mainnet.receipt_actions` '
        'ALTER COLUMN block_date SET OPTIONS (description = \'The date of the Block. Used to partition the table\'), '
        'ALTER COLUMN block_height SET OPTIONS (description = \'The height of the Block\'),'
        'ALTER COLUMN block_timestamp SET OPTIONS (description = \'The timestamp of the Block in nanoseconds\'),'
        'ALTER COLUMN block_timestamp_utc SET OPTIONS (description = \'The timestamp of the Block in UTC\'),'
        'ALTER COLUMN block_hash SET OPTIONS (description = \'The hash of the Block\'),'
        'ALTER COLUMN chunk_hash SET OPTIONS (description = \'The hash of the Chunk\'),'
        'ALTER COLUMN shard_id SET OPTIONS (description = \'The shard ID of the Chunk\'),'
        'ALTER COLUMN index_in_action_receipt SET OPTIONS (description = \'The index in the ACTION receipt\'),'
        'ALTER COLUMN receipt_id SET OPTIONS (description = \'An unique id for the receipt\'),'
        'ALTER COLUMN args SET OPTIONS (description = \'Arguments\'),'
        'ALTER COLUMN receipt_predecessor_account_id SET OPTIONS (description = \'The account ID which issued a receipt. In case of a gas or deposit refund, the account ID is system\'),'
        'ALTER COLUMN action_kind SET OPTIONS (description = \'The action kind: ADD_KEY, CREATE_ACCOUNT	, DELEGATE_ACTION, DELETE_ACCOUNT, DELETE_KEY, DEPLOY_CONTRACT, FUNCTION_CALL, STAKE, TRANSFER\'),'
        'ALTER COLUMN receipt_receiver_account_id SET OPTIONS (description = \'The destination account ID\'),'
        'ALTER COLUMN is_delegate_action SET OPTIONS (description = \'Flag for delegate action\')'
    )
query_job = bq_client.query(sql)  
query_job.result()

# COMMAND ----------

# receipts view

sql = (f"""
    CREATE OR REPLACE VIEW `{project_id}.crypto_near_mainnet.receipts`
    AS 
    SELECT 
    r.*,
    o.originated_from_transaction_hash,
    t.signer_account_id as transaction_signer_account_id,
    t.signer_public_key as transaction_signer_public_key,
    t.status as transaction_status,
    eo.executed_in_block_hash as execution_outcome_executed_in_block_hash,
    eo.outcome_receipt_ids as execution_outcome_receipt_ids,
    eo.gas_burnt as execution_outcome_gas_burnt,
    eo.tokens_burnt as execution_outcome_tokens_burnt,
    eo.executor_account_id as execution_outcome_executor_account_id,
    eo.status as execution_outcome_status
    FROM `{project_id}.crypto_near_mainnet.receipt_details` r
    LEFT JOIN `{project_id}.crypto_near_mainnet.receipt_origin` o ON r.receipt_id = o.receipt_id
    LEFT JOIN `{project_id}.crypto_near_mainnet.transactions` t ON o.originated_from_transaction_hash = t.transaction_hash
    LEFT JOIN `{project_id}.crypto_near_mainnet.execution_outcomes` eo ON eo.receipt_id = r.receipt_id;  
    """)
query_job = bq_client.query(sql)  
print(query_job.result())

dataset_ref = bigquery.DatasetReference(project_id, "crypto_near_mainnet")
table_ref = dataset_ref.table('receipts')
table = bq_client.get_table(table_ref)

table.description = "It's recomended to select only the columns and partitions (block_date) needed to avoid unecessary query costs. This view join the receipt details, the transaction that originated the receipt and the receipt execution outcome. Receipt: All cross-contract (we assume that each account lives in its own shard) communication in Near happens through Receipts. Receipts are stateful in a sense that they serve not only as messages between accounts but also can be stored in the account storage to await DataReceipts. Each receipt has a predecessor_id (who sent it) and receiver_id the current account."
bq_client.update_table(table, ["description"])

sql = (
        f'ALTER VIEW `{project_id}.crypto_near_mainnet.receipts` '
        'ALTER COLUMN block_date SET OPTIONS (description = \'The date of the Block. Used to partition the table\'), '
        'ALTER COLUMN block_height SET OPTIONS (description = \'The height of the Block\'),'
        'ALTER COLUMN block_timestamp SET OPTIONS (description = \'The timestamp of the Block in nanoseconds\'),'
        'ALTER COLUMN block_timestamp_utc SET OPTIONS (description = \'The timestamp of the Block in UTC\'),'
        'ALTER COLUMN block_hash SET OPTIONS (description = \'The hash of the Block\'),'
        'ALTER COLUMN chunk_hash SET OPTIONS (description = \'The hash of the Chunk\'),'
        'ALTER COLUMN shard_id SET OPTIONS (description = \'The shard ID of the Chunk\'),'
        'ALTER COLUMN index_in_chunk SET OPTIONS (description = \'The index in the Chunk\'),'
        'ALTER COLUMN receipt_kind SET OPTIONS (description = \'There are 2 types of Receipt: ACTION and DATA. An ACTION receipt is a request to apply Actions, while a DATA receipt is a result of the application of these actions\'),'
        'ALTER COLUMN receipt_id SET OPTIONS (description = \'An unique id for the receipt\'),'
        'ALTER COLUMN data_id SET OPTIONS (description = \'An unique DATA receipt identifier\'),'
        'ALTER COLUMN predecessor_account_id SET OPTIONS (description = \'The account ID which issued a receipt. In case of a gas or deposit refund, the account ID is system\'),'
        'ALTER COLUMN receiver_account_id SET OPTIONS (description = \'The destination account ID\'),'
        'ALTER COLUMN receipt SET OPTIONS (description = \'Receipt details\'),'
        'ALTER COLUMN originated_from_transaction_hash SET OPTIONS (description = \'The transaction hash that originated the receipt\'),'
        'ALTER COLUMN transaction_signer_account_id SET OPTIONS (description = \'An account on which behalf the origin transaction is signed\'),'
        'ALTER COLUMN transaction_signer_public_key SET OPTIONS (description = \'An access key which was used to sign the origin transaction\'),'
        'ALTER COLUMN execution_outcome_executed_in_block_hash SET OPTIONS (description = \'The execution outcome Block hash\'),'
        'ALTER COLUMN execution_outcome_receipt_ids SET OPTIONS (description = \'The execution outcome Receipt IDs generated by the transaction or receipt\'),'
        'ALTER COLUMN execution_outcome_gas_burnt SET OPTIONS (description = \'The execution outcome amount of the gas burnt by the given transaction or receipt\'),'
        'ALTER COLUMN execution_outcome_tokens_burnt SET OPTIONS (description = \'The execution outcome amount of tokens burnt corresponding to the burnt gas amount. This value does not always equal to the `gas_burnt` multiplied by the gas price, because the prepaid gas price might be lower than the actual gas price and it creates a deficit\'),'
        'ALTER COLUMN execution_outcome_executor_account_id SET OPTIONS (description = \'The execution outcome id of the account on which the execution happens. For transaction this is signer_id, for receipt this is receiver_id\'),'
        'ALTER COLUMN execution_outcome_status SET OPTIONS (description = \'The execution outcome status. Contains the result in case of successful execution\')'
    )
query_job = bq_client.query(sql)  
print(query_job.result())

# COMMAND ----------

# near_social
dataset_ref = bigquery.DatasetReference(project_id, "crypto_near_mainnet")
table_ref = dataset_ref.table('near_social_transactions')
table = bq_client.get_table(table_ref)

table.description = "NEAR Social transactions for posts, comments, likes, widgets, profiles, followers, etc."
bq_client.update_table(table, ["description"])

sql = (
        f'ALTER TABLE `{project_id}.crypto_near_mainnet.near_social_transactions` '
        'ALTER COLUMN block_date SET OPTIONS (description = \'The date of the Block. Used to partition the table\'), '
        'ALTER COLUMN block_height SET OPTIONS (description = \'The height of the Block\'),'
        'ALTER COLUMN block_timestamp_utc SET OPTIONS (description = \'The timestamp of the Block in UTC\'),'
        'ALTER COLUMN signer_id SET OPTIONS (description = \'An account on which behalf the origin transaction is signed\'),'
        'ALTER COLUMN true_signer_id SET OPTIONS (description = \'An account on which behalf the origin transaction is signed in case of action being delegated to relayer\'),'
        'ALTER COLUMN predecessor_id SET OPTIONS (description = \'The account ID which issued a receipt. In case of a gas or deposit refund, the account ID is system\'),'
        'ALTER COLUMN receipt_id SET OPTIONS (description = \'An unique id for the receipt\'),'
        'ALTER COLUMN contract_id SET OPTIONS (description = \'The contract ID\'),'
        'ALTER COLUMN method_name SET OPTIONS (description = \'The method name\'),'
        'ALTER COLUMN method_name SET OPTIONS (description = \'The deposit amount\'),'
        'ALTER COLUMN method_name SET OPTIONS (description = \'The gas fee\'),'
        'ALTER COLUMN account_object SET OPTIONS (description = \'The account object\'),'
        'ALTER COLUMN account_object SET OPTIONS (description = \'The account object edit widget\'),'
        'ALTER COLUMN account_object SET OPTIONS (description = \'The account object post or comment\'),'
        'ALTER COLUMN account_object SET OPTIONS (description = \'The account object edit profile\'),'
        'ALTER COLUMN graph SET OPTIONS (description = \'The account object graph follow or hide\'),'
        'ALTER COLUMN settings SET OPTIONS (description = \'The account object settings\'),'
        'ALTER COLUMN badge SET OPTIONS (description = \'The account object badge\'),'
        'ALTER COLUMN index SET OPTIONS (description = \'The account object index like, follow, poke, comment, post, notify\')'
    )
query_job = bq_client.query(sql)  
query_job.result()

# COMMAND ----------

'''
'''--- src/lakehouse/notebooks/Mainnet Loader.py ---
# Databricks notebook source
access_key = dbutils.secrets.get(scope = "aws", key = "access-key")
secret_key = dbutils.secrets.get(scope = "aws", key = "secret-access-key")
#sc._jsc.hadoopConfiguration().set("fs.s3a.access.key", access_key)
spark.conf.set("fs.s3a.access.key", access_key)
spark.conf.set("fs.s3a.secret.key", secret_key)
spark.conf.set("fs.s3a.requester-pays.enabled", "true")
spark.conf.set("fs.s3a.experimental.input.fadvise", "random")

# when using Auto Loader file notification mode to load files, provide the AWS Region ID.
aws_region = "eu-central-1"
spark.conf.set("fs.s3a.endpoint", "s3." + aws_region + ".amazonaws.com")

# COMMAND ----------

import dlt
from pyspark.sql.functions import *
from pyspark.sql.types import *

@dlt.table(name="blocks", comment="NEAR Lake Mainnet Raw Blocks")
def mainnet_blocks():
    return (spark.readStream.format("cloudFiles")
      .option("cloudFiles.format", "json")
      .option("cloudFiles.schemaLocation", '/nearlake/landingZone/mainnet_blocks/_checkpoint')
      .option("cloudFiles.inferColumnTypes", "true")
      .option("cloudFiles.useIncrementalListing", "true") 
      .option("pathGlobFilter", "*block.json") 
      .load("s3a://near-lake-data-mainnet/")
#      .withColumn("date_created", current_timestamp())
           )
    
@dlt.table(name="chunks", comment="NEAR Lake Mainnet Raw Chunks")
def mainnet_chunks():
    return (spark.readStream.format("cloudFiles")
      .option("cloudFiles.format", "json")
      .option("cloudFiles.schemaLocation", '/nearlake/landingZone/mainnet_chunks/_checkpoint')
      .option("cloudFiles.inferColumnTypes", "true")
      .option("cloudFiles.useIncrementalListing", "true") 
      .option("pathGlobFilter", "*shard*.json") 
      .load("s3a://near-lake-data-mainnet/")
#      .withColumn("date_created", current_timestamp())
           )

'''
'''--- src/lakehouse/notebooks/Silver Lake Mainnet.sql ---
-- Databricks notebook source
-- MAGIC %md
-- MAGIC # Silver Lake Mainnet Pipeline Prod
-- MAGIC
-- MAGIC This notebook creates the silver tables enriching each one with the most common columns to query

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ### Blocks

-- COMMAND ----------

CREATE OR REFRESH STREAMING LIVE TABLE silver_blocks
COMMENT "Stream of parsed blocks"
TBLPROPERTIES ("quality" = "silver", delta.autoOptimize.optimizeWrite = true, delta.autoOptimize.autoCompact = true)
PARTITIONED BY (block_date) AS
SELECT 
  (select max(header.height) from mainnet.blocks) as last_indexed_block,
  CAST(header.timestamp / 1000000000 AS timestamp)::date block_date,
  CAST(header.height AS BIGINT) AS block_height,
  CAST(header.timestamp AS BIGINT) as block_timestamp,
  CAST(header.timestamp / 1000000000 AS timestamp) AS block_timestamp_utc,
  header.hash AS block_hash,
  NOW() AS _dlt_synced_utc,
  BIGINT(NOW() - CAST(header.timestamp / 1000000000 AS timestamp)) as _dlt_synced_lag_seconds,
  header.prev_hash AS prev_block_hash,
  header.total_supply AS total_supply,
  header.gas_price AS gas_price,
  author AS author_account_id,
  header,
  chunks
FROM STREAM(mainnet.blocks)

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ### Chunks

-- COMMAND ----------

CREATE OR REFRESH STREAMING LIVE TABLE silver_chunks 
COMMENT "Stream of parsed chunks"
TBLPROPERTIES ("quality" = "silver", delta.autoOptimize.optimizeWrite = true, delta.autoOptimize.autoCompact = true)
PARTITIONED BY (block_date) AS
SELECT 
  b.block_date,
  b.block_height,
  b.block_timestamp,
  b.block_timestamp_utc,
  b.block_hash,
  NOW() AS _dlt_synced_utc,
  BIGINT(NOW() - b.block_timestamp_utc) as _dlt_synced_lag_seconds,
  c.chunk.header.chunk_hash as chunk_hash,
  CAST(c.chunk.header.shard_id AS BIGINT) as shard_id,
  c.chunk.header.signature as signature,
  CAST(c.chunk.header.gas_limit AS BIGINT) as gas_limit,
  CAST(c.chunk.header.gas_used AS BIGINT) as gas_used,
  c.chunk.author as author_account_id,
  c.chunk,
  c.receipt_execution_outcomes,
  c.state_changes
FROM STREAM(mainnet.chunks) c
JOIN STREAM(live.silver_blocks) b ON c.chunk.header.height_included = b.block_height

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ### Account Changes

-- COMMAND ----------

CREATE OR REFRESH STREAMING LIVE TABLE silver_account_changes
COMMENT "Stream of parsed account changes"
TBLPROPERTIES ("quality" = "silver", delta.autoOptimize.optimizeWrite = true, delta.autoOptimize.autoCompact = true)
PARTITIONED BY (block_date) AS
WITH state_changes AS (
	SELECT 
		c.block_date,
		c.block_height,
		c.block_timestamp,
		c.block_timestamp_utc,
		c.block_hash,
		c.chunk_hash,
		NOW() AS _dlt_synced_utc,
		BIGINT(NOW() - c.block_timestamp_utc) as _dlt_synced_lag_seconds,
		posexplode(c.state_changes) AS (index_in_block, state_change)
	FROM STREAM(live.silver_chunks) c
)

SELECT 
	block_date,
	block_height,
	block_timestamp,
	block_timestamp_utc,
	block_hash,
	chunk_hash,
	_dlt_synced_utc,
	_dlt_synced_lag_seconds,
	index_in_block,
	sc.state_change.change.account_id AS affected_account_id,
	IF(sc.state_change.cause.type = 'transaction_processing', sc.state_change.cause.tx_hash, NULL) AS caused_by_transaction_hash,
	IF(sc.state_change.cause.type IN ('action_receipt_processing_started', 'action_receipt_gas_reward', 'receipt_processing', 'postponed_receipt'), sc.state_change.cause.receipt_hash, NULL) AS caused_by_receipt_id,
	sc.state_change.cause.type AS update_reason,
	sc.state_change.change.amount AS affected_account_nonstaked_balance,
	sc.state_change.change.locked AS affected_account_staked_balance,
	sc.state_change.change.storage_usage AS affected_account_storage_usage
FROM state_changes sc
WHERE state_change.type = "account_update"

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ### Transactions

-- COMMAND ----------

CREATE OR REFRESH STREAMING LIVE TABLE silver_transactions
COMMENT "Stream of parsed transactions"
TBLPROPERTIES ("quality" = "silver", delta.autoOptimize.optimizeWrite = true, delta.autoOptimize.autoCompact = true)
PARTITIONED BY (block_date) AS
WITH txs AS (
  SELECT 
    c.block_date,
		c.block_height,
		c.block_timestamp,
		c.block_timestamp_utc,
		c.block_hash,
		c.chunk_hash,
    c.shard_id,
		NOW() AS _dlt_synced_utc,
		BIGINT(NOW() - c.block_timestamp_utc) as _dlt_synced_lag_seconds,
    posexplode(c.chunk.transactions) AS (index_in_chunk, tx)
  FROM STREAM(live.silver_chunks) c 
), txs_parsed AS (
  SELECT
    t.*,
    t.index_in_chunk,
    from_json(tx, 'STRUCT<outcome: STRUCT<execution_outcome: STRUCT<block_hash: STRING, id: STRING, outcome: STRUCT<executor_id: STRING, gas_burnt: BIGINT, logs: ARRAY<STRING>, metadata: STRUCT<gas_profile: STRING, version: BIGINT>, receipt_ids: ARRAY<STRING>, status: STRUCT<SuccessValue: STRING, SuccessReceiptId: STRING, Failure: STRUCT<ActionError: STRUCT<index: BIGINT, kind: STRUCT<FunctionCallError: STRUCT<HostError: STRUCT<GuestPanic: STRUCT<panic_msg: STRING>>>>>>>, tokens_burnt: STRING>, proof: ARRAY<STRING>>, receipt: STRUCT<predecessor_id: STRING, receipt: STRUCT<Action: STRUCT<actions: ARRAY<STRUCT<FunctionCall: STRUCT<args: STRING, deposit: STRING, gas: BIGINT, method_name: STRING>>>, gas_price: STRING, input_data_ids: ARRAY<STRING>, output_data_receivers: ARRAY<STRING>, signer_id: STRING, signer_public_key: STRING>>>, receipt_id: STRING, receiver_id: STRING>, transaction: STRUCT<hash: STRING, nonce: BIGINT, public_key: STRING, receiver_id: STRING, signature: STRING, signer_id: STRING, actions: ARRAY<STRING>>>') as json 
  FROM txs t
)
SELECT 
  tp.block_date,
  tp.block_height,
  tp.block_timestamp,
  tp.block_timestamp_utc,
  tp.block_hash,
  tp.chunk_hash,
  tp.shard_id,
  tp._dlt_synced_utc,
  tp._dlt_synced_lag_seconds,
  tp.json.transaction.hash,
  tp.index_in_chunk,
  tp.json.transaction.nonce,
  tp.json.transaction.signer_id,
  tp.json.transaction.public_key,
  tp.json.transaction.signature,
  tp.json.transaction.receiver_id,
  tp.json.outcome.execution_outcome.outcome.receipt_ids[0] as converted_into_receipt_id,
  CASE
    WHEN tp.json.outcome.execution_outcome.outcome.status.SuccessReceiptId IS NOT NULL THEN 'SUCCESS_RECEIPT_ID'
    WHEN tp.json.outcome.execution_outcome.outcome.status.SuccessValue IS NOT NULL THEN 'SUCCESS_VALUE'
    WHEN tp.json.outcome.execution_outcome.outcome.status.Failure IS NOT NULL THEN 'FAILURE'
    ELSE 'UNKNOWN'
  END as status,
  tp.json.outcome.execution_outcome.outcome.gas_burnt as receipt_conversion_gas_burnt,
  tp.json.outcome.execution_outcome.outcome.tokens_burnt as receipt_conversion_tokens_burnt,
  tp.json.transaction.actions
FROM txs_parsed tp

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ### Transaction Actions

-- COMMAND ----------

CREATE OR REFRESH STREAMING LIVE TABLE silver_transaction_actions
COMMENT "Stream of parsed transaction actions"
TBLPROPERTIES ("quality" = "silver", delta.autoOptimize.optimizeWrite = true, delta.autoOptimize.autoCompact = true)
PARTITIONED BY (block_date) AS
WITH ta_parsed AS (
  SELECT 
    block_date,
    block_height,
    block_timestamp,
    block_timestamp_utc,
    hash as transaction_hash,
    status as transaction_status,
    converted_into_receipt_id,
    signer_id,
    receiver_id,
    public_key,
    posexplode(t.actions) AS (index_in_transaction, action)
  FROM STREAM(live.silver_transactions) t
)
SELECT 
  block_date,
  block_height,
  block_timestamp,
  block_timestamp_utc,
  NOW() AS _dlt_synced_utc,
  BIGINT(NOW() - CAST(block_timestamp / 1000000000 AS timestamp)) as _dlt_synced_lag_seconds,
  transaction_hash,
  transaction_status,
  converted_into_receipt_id,
  signer_id,
  receiver_id,
  public_key,
  index_in_transaction,
  CASE
    WHEN contains(action, 'CreateAccount') THEN 'CREATE_ACCOUNT'
    WHEN contains(action, 'DeployContract') THEN 'DEPLOY_CONTRACT'
    WHEN contains(action, 'Transfer') THEN 'TRANSFER'
    WHEN contains(action, 'Stake') THEN 'STAKE'
    WHEN contains(action, 'AddKey') THEN 'ADD_KEY'
    WHEN contains(action, 'DeleteKey') THEN 'DELETE_KEY'
    WHEN contains(action, 'DeleteAccount') THEN 'DELETE_ACCOUNT'
    WHEN contains(action, 'FunctionCall') THEN 'FUNCTION_CALL'
    ELSE 'UNKNOWN'
  END as action_kind,
  CASE
    WHEN contains(action, 'CreateAccount') THEN '{}'
    WHEN contains(action, 'DeployContract') THEN CONCAT('{"code_sha256":"', lower(CAST(hex(unbase64(from_json(action, 'STRUCT<DeployContract: STRUCT<code:STRING>>').DeployContract.code)) AS STRING)), '"}')
    WHEN contains(action, 'Transfer') THEN CAST(from_json(action, 'STRUCT<Transfer: STRING>').Transfer AS STRING)
    WHEN contains(action, 'Stake') THEN CAST(from_json(action, 'STRUCT<Stake: STRING>').Stake AS STRING)
    WHEN contains(action, 'AddKey') THEN CAST(from_json(action, 'STRUCT<AddKey: STRING>').AddKey AS STRING)
    WHEN contains(action, 'DeleteKey') THEN CAST(from_json(action, 'STRUCT<DeleteKey: STRING>').DeleteKey AS STRING)
    WHEN contains(action, 'DeleteAccount') THEN CAST(from_json(action, 'STRUCT<DeleteAccount: STRING>').DeleteAccount AS STRING)
    WHEN contains(action, 'FunctionCall') THEN 
      CONCAT('{',
        '"gas": "', CAST(from_json(action, 'STRUCT<FunctionCall: STRUCT<gas: BIGINT>>').FunctionCall.gas AS STRING), '",',
        '"deposit": "', CAST(from_json(action, 'STRUCT<FunctionCall: STRUCT<deposit: STRING>>').FunctionCall.deposit AS STRING), '",',
        '"args_base64": "', CAST(from_json(action, 'STRUCT<FunctionCall: STRUCT<args: STRING>>').FunctionCall.args AS STRING), '",',
        '"method_name": "', CAST(from_json(action, 'STRUCT<FunctionCall: STRUCT<method_name: STRING>>').FunctionCall.method_name AS STRING), '"',
       '}')
    ELSE action
  END as args
FROM ta_parsed

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ### Transaction Actions Function Calls

-- COMMAND ----------

CREATE OR REFRESH STREAMING LIVE TABLE silver_transaction_actions_function_calls
COMMENT "Stream of parsed transaction actions function calls"
TBLPROPERTIES ("quality" = "silver", delta.autoOptimize.optimizeWrite = true, delta.autoOptimize.autoCompact = true)
PARTITIONED BY (block_date) AS
SELECT
  block_date,
  block_height,
  block_timestamp,
  block_timestamp_utc,
  NOW() AS _dlt_synced_utc,
  BIGINT(NOW() - CAST(block_timestamp / 1000000000 AS timestamp)) as _dlt_synced_lag_seconds,
  transaction_hash,
  transaction_status,
  converted_into_receipt_id,
  signer_id,
  receiver_id,
  public_key,
  CAST(from_json(args, 'STRUCT<method_name: STRING>').method_name AS STRING) as method_name, 
  CAST(from_json(args, 'STRUCT<gas: STRING>').gas AS STRING) AS gas,
  CAST(from_json(args, 'STRUCT<deposit: STRING>').deposit AS STRING) AS deposit,
  CAST(from_json(args, 'STRUCT<args_base64: STRING>').args_base64 AS STRING) as args_base64,
  CAST(unbase64(from_json(args, 'STRUCT<args_base64: STRING>').args_base64) AS STRING) as args_parsed
FROM STREAM(live.silver_transaction_actions)
WHERE action_kind = 'FUNCTION_CALL'

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ### Execution Outcomes

-- COMMAND ----------

CREATE OR REFRESH STREAMING LIVE TABLE silver_execution_outcomes
COMMENT "Stream of parsed execution outcomes"
TBLPROPERTIES ("quality" = "silver", delta.autoOptimize.optimizeWrite = true, delta.autoOptimize.autoCompact = true)
PARTITIONED BY (block_date) AS
WITH reos AS (
  SELECT 
    c.block_date,
		c.block_height,
		c.block_timestamp,
		c.block_timestamp_utc,
		c.block_hash,
		c.chunk_hash,
    c.shard_id,
		NOW() AS _dlt_synced_utc,
		BIGINT(NOW() - c.block_timestamp_utc) as _dlt_synced_lag_seconds,
    posexplode(c.receipt_execution_outcomes) AS (index_in_chunk, reo)
  FROM STREAM(live.silver_chunks) c
)
, reo_parsed AS (
  SELECT
    reos.*,
    from_json(reos.reo, 'STRUCT<execution_outcome: STRUCT<block_hash: STRING, id: STRING, outcome: STRUCT<executor_id: STRING, gas_burnt: BIGINT, logs: ARRAY<STRING>, metadata: STRUCT<gas_profile: STRING, version: BIGINT>, receipt_ids: ARRAY<STRING>, status: STRUCT<SuccessValue: STRING, SuccessReceiptId: STRING, Failure: STRUCT<ActionError: STRUCT<index: BIGINT, kind: STRUCT<FunctionCallError: STRUCT<HostError: STRUCT<GuestPanic: STRUCT<panic_msg: STRING>>>>>>>, tokens_burnt: STRING>, proof: ARRAY<STRING>>, receipt: STRUCT<predecessor_id: STRING, receipt: STRUCT<Action: STRUCT<actions: ARRAY<STRUCT<FunctionCall: STRUCT<args: STRING, deposit: STRING, gas: BIGINT, method_name: STRING>>>, gas_price: STRING, input_data_ids: ARRAY<STRING>, output_data_receivers: ARRAY<STRING>, signer_id: STRING, signer_public_key: STRING>>, receipt_id: STRING, receiver_id: STRING>, transaction: STRUCT<hash: STRING, nonce: BIGINT, public_key: STRING, receiver_id: STRING, signature: STRING, signer_id: STRING, actions: ARRAY<STRING>>>') as json
  FROM reos
)
SELECT
  reop.block_date,
	reop.block_height,
	reop.block_timestamp,
	reop.block_timestamp_utc,
	reop.block_hash,
	reop.chunk_hash,
  reop.shard_id,
	reop._dlt_synced_utc,
	reop._dlt_synced_lag_seconds,
  reop.json.execution_outcome.id as receipt_id,
  reop.json.execution_outcome.block_hash as executed_in_block_hash,
  reop.json.execution_outcome.outcome.receipt_ids as outcome_receipt_ids,
  reop.index_in_chunk,
  reop.json.execution_outcome.outcome.gas_burnt as gas_burnt,
  reop.json.execution_outcome.outcome.tokens_burnt as tokens_burnt,
  reop.json.execution_outcome.outcome.executor_id as executor_account_id,
    CASE
    WHEN reop.json.execution_outcome.outcome.status.SuccessReceiptId IS NOT NULL THEN 'SUCCESS_RECEIPT_ID'
    WHEN reop.json.execution_outcome.outcome.status.SuccessValue IS NOT NULL THEN 'SUCCESS_VALUE'
    WHEN reop.json.execution_outcome.outcome.status.Failure IS NOT NULL THEN 'FAILURE'
    ELSE 'UNKNOWN'
  END as status
FROM reo_parsed reop

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ### Execution Outcomes Receipts

-- COMMAND ----------

CREATE OR REFRESH STREAMING LIVE TABLE silver_execution_outcome_receipts
COMMENT "Stream of parsed execution outcome receipts"
TBLPROPERTIES ("quality" = "silver", delta.autoOptimize.optimizeWrite = true, delta.autoOptimize.autoCompact = true)
PARTITIONED BY (block_date) AS
WITH eo_parsed AS (
  SELECT
    *,
    posexplode(eo.outcome_receipt_ids) AS (index_in_execution_outcome, outcome_receipt_id)
  FROM STREAM(live.silver_execution_outcomes) eo
)
SELECT 
  block_date,
	block_height,
	block_timestamp,
	block_timestamp_utc,
	block_hash,
	chunk_hash,
  shard_id,
  NOW() AS _dlt_synced_utc,
  BIGINT(NOW() - block_timestamp_utc) as _dlt_synced_lag_seconds,
  index_in_chunk,
  gas_burnt,
  tokens_burnt,
  executor_account_id,
  status,
  receipt_id as executed_receipt_id,
  index_in_execution_outcome,
  outcome_receipt_id as produced_receipt_id
FROM eo_parsed

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ### Receipts

-- COMMAND ----------

CREATE OR REFRESH STREAMING LIVE TABLE silver_receipts
COMMENT "Stream of parsed receipts"
TBLPROPERTIES ("quality" = "silver", delta.autoOptimize.optimizeWrite = true, delta.autoOptimize.autoCompact = true)
PARTITIONED BY (block_date) AS
WITH rs AS (
  SELECT 
    c.block_date,
		c.block_height,
		c.block_timestamp,
		c.block_timestamp_utc,
		c.block_hash,
		c.chunk_hash,
    c.shard_id,
		NOW() AS _dlt_synced_utc,
		BIGINT(NOW() - c.block_timestamp_utc) as _dlt_synced_lag_seconds,
    posexplode(c.chunk.receipts) AS (index_in_chunk, receipt)
  FROM STREAM(live.silver_chunks) c
) 

SELECT 
  rs.block_date,
  rs.block_height,
  rs.block_timestamp,
  rs.block_timestamp_utc,
  rs.block_hash,
  rs.chunk_hash,
  rs.shard_id,
  NOW() AS _dlt_synced_utc,
  BIGINT(NOW() - rs.block_timestamp_utc) as _dlt_synced_lag_seconds,
  CASE
    WHEN rs.receipt:receipt:data:data_id IS NOT NULL THEN 'DATA'
    WHEN rs.receipt:receipt_id IS NOT NULL THEN 'ACTION'
    ELSE 'UNKNOWN'
  END as receipt_kind,
  rs.receipt:receipt_id,
  rs.receipt:receipt:data:data_id,
  rs.index_in_chunk,
  rs.receipt:predecessor_id as predecessor_account_id,
  rs.receipt:receiver_id as receiver_account_id,
  rs.receipt
FROM rs 

-- COMMAND ----------

CREATE OR REFRESH STREAMING LIVE TABLE silver_receipt_originated_from_transaction
COMMENT "Stream of parsed receipt originated from transaction"
TBLPROPERTIES ("quality" = "silver", delta.autoOptimize.optimizeWrite = true, delta.autoOptimize.autoCompact = true)
PARTITIONED BY (block_date) AS
SELECT 
  rs.block_date,
  rs.block_height,
  NOW() AS _dlt_synced_utc,
  BIGINT(NOW() - rs.block_timestamp_utc) as _dlt_synced_lag_seconds,
  rs.receipt_id,
  rs.data_id,
  rs.receipt_kind,
  '' as originated_from_transaction_hash
FROM STREAM(live.silver_receipts) rs 

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ### Action Receipts

-- COMMAND ----------

CREATE OR REFRESH STREAMING LIVE TABLE silver_action_receipts
COMMENT "Stream of parsed action receipts"
TBLPROPERTIES ("quality" = "silver", delta.autoOptimize.optimizeWrite = true, delta.autoOptimize.autoCompact = true)
PARTITIONED BY (block_date) AS
SELECT
  sr.block_date,
  sr.block_height,
  sr.block_timestamp,
  sr.block_timestamp_utc,
  sr.block_hash,
  sr.chunk_hash,
  sr.shard_id,
  NOW() AS _dlt_synced_utc,
  BIGINT(NOW() - sr.block_timestamp_utc) as _dlt_synced_lag_seconds,
  sr.receipt_id,
  sr.receipt:receipt:Action:gas_price,
  sr.receipt:receipt:Action:signer_id as signer_account_id,
  sr.receipt:receipt:Action:signer_public_key,
  receipt
FROM STREAM(live.silver_receipts) sr
WHERE sr.receipt_kind = 'ACTION'

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ### Action Receipt Actions

-- COMMAND ----------

CREATE OR REFRESH STREAMING LIVE TABLE silver_action_receipt_actions
COMMENT "Stream of parsed action receipt actions"
TBLPROPERTIES ("quality" = "silver", delta.autoOptimize.optimizeWrite = true, delta.autoOptimize.autoCompact = true)
PARTITIONED BY (block_date) AS
WITH receipt_actions AS (
  SELECT
    ar.*,
    posexplode(from_json(ar.receipt:receipt:Action, 'STRUCT<actions: ARRAY<STRING>>').actions) AS (index_in_action_receipt, action)
  FROM STREAM(live.silver_action_receipts) ar
)

SELECT 
  ra.block_date,
  ra.block_height,
  ra.block_timestamp,
  ra.block_timestamp_utc,
  ra.block_hash,
  ra.chunk_hash,
  ra.shard_id,
  NOW() AS _dlt_synced_utc,
  BIGINT(NOW() - ra.block_timestamp_utc) as _dlt_synced_lag_seconds,
  ra.index_in_action_receipt,
  ra.receipt_id,
  CASE
    WHEN contains(action, 'CreateAccount') THEN '{}'
    WHEN contains(action, 'DeployContract') THEN CONCAT('{"code_sha256":"', lower(CAST(hex(unbase64(from_json(action, 'STRUCT<DeployContract: STRUCT<code:STRING>>').DeployContract.code)) AS STRING)), '"}')
    WHEN contains(action, 'Transfer') THEN CAST(from_json(action, 'STRUCT<Transfer: STRING>').Transfer AS STRING)
    WHEN contains(action, 'Stake') THEN CAST(from_json(action, 'STRUCT<Stake: STRING>').Stake AS STRING)
    WHEN contains(action, 'AddKey') THEN CAST(from_json(action, 'STRUCT<AddKey: STRING>').AddKey AS STRING)
    WHEN contains(action, 'DeleteKey') THEN CAST(from_json(action, 'STRUCT<DeleteKey: STRING>').DeleteKey AS STRING)
    WHEN contains(action, 'DeleteAccount') THEN CAST(from_json(action, 'STRUCT<DeleteAccount: STRING>').DeleteAccount AS STRING)
    WHEN contains(action, 'Delegate') THEN action
    WHEN contains(action, 'FunctionCall') THEN 
      CONCAT('{',
        '"gas": "', CAST(from_json(action, 'STRUCT<FunctionCall: STRUCT<gas: BIGINT>>').FunctionCall.gas AS STRING), '",',
        '"deposit": "', CAST(from_json(action, 'STRUCT<FunctionCall: STRUCT<deposit: STRING>>').FunctionCall.deposit AS STRING), '",',
        '"args_base64": "', CAST(from_json(action, 'STRUCT<FunctionCall: STRUCT<args: STRING>>').FunctionCall.args AS STRING), '",',
        '"method_name": "', CAST(from_json(action, 'STRUCT<FunctionCall: STRUCT<method_name: STRING>>').FunctionCall.method_name AS STRING), '"',
       '}')    
    ELSE action
  END as args,
  ra.receipt:predecessor_id as receipt_predecessor_account_id,
  CASE
    WHEN contains(action, 'CreateAccount') THEN 'CREATE_ACCOUNT'
    WHEN contains(action, 'DeployContract') THEN 'DEPLOY_CONTRACT'
    WHEN contains(action, 'Transfer') THEN 'TRANSFER'
    WHEN contains(action, 'Stake') THEN 'STAKE'
    WHEN contains(action, 'AddKey') THEN 'ADD_KEY'
    WHEN contains(action, 'DeleteKey') THEN 'DELETE_KEY'
    WHEN contains(action, 'DeleteAccount') THEN 'DELETE_ACCOUNT'
    WHEN contains(action, 'Delegate') THEN 'DELEGATE_ACTION'
    WHEN contains(action, 'FunctionCall') THEN 'FUNCTION_CALL'
    ELSE 'UNKNOWN'
  END as action_kind,
  ra.receipt:receiver_id as receipt_receiver_account_id,
  contains(action, 'DelegateAction') as is_delegate_action
FROM receipt_actions ra

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ### Data Receipts

-- COMMAND ----------

CREATE OR REFRESH STREAMING LIVE TABLE silver_data_receipts
COMMENT "Stream of parsed data receipts"
TBLPROPERTIES ("quality" = "silver", delta.autoOptimize.optimizeWrite = true, delta.autoOptimize.autoCompact = true)
PARTITIONED BY (block_date) AS
SELECT
  sr.block_date,
  sr.block_height,
  sr.block_timestamp,
  sr.block_timestamp_utc,
  sr.block_hash,
  sr.chunk_hash,
  sr.shard_id,
  NOW() AS _dlt_synced_utc,
  BIGINT(NOW() - sr.block_timestamp_utc) as _dlt_synced_lag_seconds,
  sr.receipt_id,
  sr.data_id,
  unbase64(receipt:receipt:Data:data) as data
FROM STREAM(live.silver_receipts) sr
WHERE sr.receipt_kind = 'DATA'

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ### Action Receipt Output Data

-- COMMAND ----------

CREATE OR REFRESH STREAMING LIVE TABLE silver_action_receipt_output_data
COMMENT "Stream of parsed action receipt output data"
TBLPROPERTIES ("quality" = "silver", delta.autoOptimize.optimizeWrite = true, delta.autoOptimize.autoCompact = true)
PARTITIONED BY (block_date) AS
WITH odr_parsed AS (
  SELECT 
    block_date,
    block_height,
    block_timestamp,
    block_timestamp_utc,
    block_hash,
    chunk_hash,
    shard_id,
    receipt_id,
    explode(from_json(r.receipt:receipt:Action:output_data_receivers, 'ARRAY<STRING>')) as json_data 
    FROM STREAM(live.silver_receipts) r 
) 
SELECT
  block_date,
  block_height,
  block_timestamp,
  block_timestamp_utc,
  block_hash,
  chunk_hash,
  shard_id,
  NOW() AS _dlt_synced_utc,
  BIGINT(NOW() - block_timestamp_utc) as _dlt_synced_lag_seconds,
  json_data:data_id as output_data_id,
  receipt_id as output_from_receipt_id, 
  json_data:receiver_id as receiver_account_id 
from odr_parsed

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ### Action Receipt Input Data

-- COMMAND ----------

CREATE OR REFRESH STREAMING LIVE TABLE silver_action_receipt_input_data
COMMENT "Stream of parsed action receipt input data"
TBLPROPERTIES ("quality" = "silver", delta.autoOptimize.optimizeWrite = true, delta.autoOptimize.autoCompact = true)
PARTITIONED BY (block_date) AS
SELECT
  block_date,
  block_height,
  block_timestamp,
  block_timestamp_utc,
  block_hash,
  chunk_hash,
  shard_id,
  receipt_id as input_to_receipt_id,
  explode(from_json(r.receipt:receipt:Action:input_data_ids, 'ARRAY<STRING>')) as input_data_id
FROM STREAM(live.silver_receipts) r

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ### NEAR Social Txs

-- COMMAND ----------

CREATE OR REFRESH STREAMING LIVE TABLE silver_near_social_txs
COMMENT "Stream of receipts on mainnet for social.near."
TBLPROPERTIES ("quality" = "silver", delta.autoOptimize.optimizeWrite = true, delta.autoOptimize.autoCompact = true)
AS 
SELECT
  sara.*,
  seo.executor_account_id as contract_id,
  seo.gas_burnt, 
  seo.status,
  sr.predecessor_account_id as predecessor_id,
  sar.gas_price,
  sar.signer_account_id as signer_id,
  sar.signer_public_key
FROM STREAM(live.silver_action_receipt_actions) sara
JOIN STREAM(live.silver_receipts) sr ON sr.receipt_id = sara.receipt_id
JOIN STREAM(live.silver_action_receipts) sar ON sar.receipt_id = sara.receipt_id
JOIN STREAM(live.silver_execution_outcomes) seo ON seo.receipt_id = sara.receipt_id
WHERE sara.receipt_receiver_account_id = 'social.near'

-- COMMAND ----------

-- This table fields were update based on jo.yang@near.org query
CREATE OR REFRESH STREAMING LIVE TABLE silver_near_social_txs_parsed
COMMENT "Stream of parsed txs on mainnet for social.near."
TBLPROPERTIES ("quality" = "silver", delta.autoOptimize.optimizeWrite = true, delta.autoOptimize.autoCompact = true)
PARTITIONED BY (block_date)
(
  with 
  -- dedup as (
  --  select block_hash, block_height,block_timestamp,block_utc, shard_id, contract_id, receipt_id, gas_burnt, SuccessValue,SuccessReceiptId,gas_price, signer_id, signer_public_key, predecessor_id, action
  --  from devdb.near_social_txs_test 
  --  group by 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15
  -- )
  parsed AS (
      SELECT 
          block_height,
          -- block_utc,
          block_timestamp_utc,
          block_date,
          CAST(unbase64(args:args_base64) AS STRING) AS args,
          signer_id,
          case when signer_id = 'relayer.pagodaplatform.near' then predecessor_id else signer_id end as true_signer_id, -- in case of action being delegated to relayer
          predecessor_id,
          receipt_id,
          contract_id,
          args:method_name,
          args:deposit,
          args:gas
      FROM STREAM(live.silver_near_social_txs)
      WHERE signer_id != "social.near"
  )
  , account_object AS (
      SELECT 
        block_height,
        -- block_utc,
        block_timestamp_utc,
        block_date,
        get_json_object(args:data,concat('$["', true_signer_id ,'"]')) AS account_object,
        signer_id,
        predecessor_id,
        receipt_id,
        contract_id,
        method_name,
        deposit,
        gas
      FROM parsed
  )

      SELECT 
          block_height,
          block_timestamp_utc,
          block_date,
          signer_id,
          predecessor_id,
          receipt_id,
          contract_id,
          method_name,
          deposit,
          gas,
          account_object,
          account_object:widget, -- edit widget
          account_object:post, -- post or comment
          account_object:profile, -- edit profile
          account_object:graph, -- follow / hide 
          account_object:settings,
          account_object:badge, 
          account_object:index -- like / follow / poke / comment / post / notify
      FROM account_object
  )

'''