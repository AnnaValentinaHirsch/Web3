*GitHub Repository "kuutamolabs/ldk-node"*

'''--- .github/workflows/build.yml ---
name: Continuous Integration Checks

on: [push, pull_request]

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  build:
    strategy:
      matrix:
        platform: [
          ubuntu-latest,
          macos-latest,
          windows-latest,
          ]
        toolchain: [
          stable,
          beta,
          1.63.0, # Our MSRV
          ]
        include:
          - toolchain: stable
            check-fmt: true
            build-uniffi: true
            platform: ubuntu-latest
          - toolchain: stable
            platform: macos-latest
          - toolchain: stable
            platform: windows-latest
          - toolchain: 1.63.0
            msrv: true
    runs-on: ${{ matrix.platform }}
    steps:
      - name: Checkout source code
        uses: actions/checkout@v3
      - name: Install Rust ${{ matrix.toolchain }} toolchain
        run: |
          curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y --profile=minimal --default-toolchain ${{ matrix.toolchain }}
          rustup override set ${{ matrix.toolchain }}
      - name: Pin packages to allow for MSRV
        if: matrix.msrv
        run: |
          cargo update -p hashlink --precise "0.8.2" --verbose # hashlink 0.8.3 requires hashbrown 0.14, requiring 1.64.0
          cargo update -p proptest --precise "1.2.0" --verbose # proptest 1.3.0 requires rustc 1.64.0
          cargo update -p reqwest --precise "0.11.20" --verbose # reqwest 0.11.21 broke 1.63.0 MSRV
          cargo update -p regex --precise "1.9.6" --verbose # regex 1.10.0 requires rustc 1.65.0
          cargo update -p jobserver --precise "0.1.26" --verbose # jobserver 0.1.27 requires rustc 1.66.0
          cargo update -p zstd-sys --precise "2.0.8+zstd.1.5.5" --verbose # zstd-sys 2.0.9+zstd.1.5.5 requires rustc 1.64.0
          cargo update -p petgraph --precise "0.6.3" --verbose # petgraph v0.6.4, requires rustc 1.64 or newer
      - name: Build on Rust ${{ matrix.toolchain }}
        run: cargo build --verbose --color always
      - name: Build with UniFFI support on Rust ${{ matrix.toolchain }}
        if: matrix.build-uniffi
        run: cargo build --features uniffi --verbose --color always
      - name: Build documentation on Rust ${{ matrix.toolchain }}
        run: |
          cargo doc --release --verbose --color always
          cargo doc --document-private-items --verbose --color always
      - name: Check release build on Rust ${{ matrix.toolchain }}
        run: cargo check --release --verbose --color always
      - name: Check release build with UniFFI support on Rust ${{ matrix.toolchain }}
        if: matrix.build-uniffi
        run: cargo check --release --features uniffi --verbose --color always
      - name: Test on Rust ${{ matrix.toolchain }}
        if: "matrix.platform != 'windows-latest'"
        run: cargo test
      - name: Test with UniFFI support on Rust ${{ matrix.toolchain }}
        if: "matrix.platform != 'windows-latest' && matrix.build-uniffi"
        run: cargo test --features uniffi
      - name: Check formatting on Rust ${{ matrix.toolchain }}
        if: matrix.check-fmt
        run: rustup component add rustfmt && cargo fmt --all -- --check

'''
'''--- .github/workflows/kotlin.yml ---
name: Continuous Integration Checks - Kotlin

on: [push, pull_request]

jobs:
  check-kotlin:
    runs-on: ubuntu-latest

    env:
      LDK_NODE_JVM_DIR: bindings/kotlin/ldk-node-jvm
      LDK_NODE_ANDROID_DIR: bindings/kotlin/ldk-node-android

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up JDK
        uses: actions/setup-java@v3
        with:
          distribution: temurin
          java-version: 11

      - name: Set default Rust version to 1.73.0
        run: rustup default 1.73.0

      - name: Show default version of NDK
        run: echo $ANDROID_NDK_ROOT

      - name: Run ktlintCheck on ldk-node-jvm
        run: |
          cd $LDK_NODE_JVM_DIR
          ./gradlew ktlintCheck

      - name: Run ktlintCheck on ldk-node-android
        run: |
          cd $LDK_NODE_ANDROID_DIR
          ./gradlew ktlintCheck

      - name: Generate Kotlin JVM
        run: ./scripts/uniffi_bindgen_generate_kotlin.sh

      - name: Generate Kotlin Android
        run: ./scripts/uniffi_bindgen_generate_kotlin_android.sh

      - name: Start bitcoind and electrs
        run: docker compose up -d

      - name: Run ldk-node-jvm tests
        run: |
          cd $LDK_NODE_JVM_DIR
          ./gradlew test -Penv=ci

      - name: Run ldk-node-android tests
        run: |
          cd $LDK_NODE_ANDROID_DIR
          ./gradlew test

'''
'''--- .github/workflows/publish-android.yml ---
name: Publish ldk-node-android to Maven Central
on: [workflow_dispatch]

jobs:
  build:
    runs-on: ubuntu-20.04
    steps:
      - name: "Check out PR branch"
        uses: actions/checkout@v2

      - name: "Cache"
        uses: actions/cache@v2
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            ./target
          key: ${{ runner.os }}-${{ hashFiles('**/Cargo.toml','**/Cargo.lock') }}

      - name: "Set up JDK"
        uses: actions/setup-java@v2
        with:
          distribution: temurin
          java-version: 11

      - name: "Install Rust Android targets"
        run: rustup target add x86_64-linux-android aarch64-linux-android armv7-linux-androideabi

      - name: "Build ldk-node-android library"
        run: |
          export PATH=$PATH:$ANDROID_NDK_ROOT/toolchains/llvm/prebuilt/linux-x86_64/bin
          ./scripts/uniffi_bindgen_generate_kotlin_android.sh

      - name: "Publish to Maven Local and Maven Central"
        env:
          ORG_GRADLE_PROJECT_signingKeyId: ${{ secrets.PGP_KEY_ID }}
          ORG_GRADLE_PROJECT_signingKey: ${{ secrets.PGP_SECRET_KEY }}
          ORG_GRADLE_PROJECT_signingPassword: ${{ secrets.PGP_PASSPHRASE }}
          ORG_GRADLE_PROJECT_ossrhUsername: ${{ secrets.NEXUS_USERNAME }}
          ORG_GRADLE_PROJECT_ossrhPassword: ${{ secrets.NEXUS_PASSWORD }}
        run: |
          cd bindings/kotlin/ldk-node-android
          ./gradlew publishToSonatype closeAndReleaseSonatypeStagingRepository

'''
'''--- .github/workflows/publish-jvm.yml ---
name: Publish ldk-node-jvm to Maven Central
on: [workflow_dispatch]

jobs:
  build-jvm-macOS-M1-native-lib:
    name: "Create M1 and x86_64 JVM native binaries"
    runs-on: macos-12
    steps:
      - name: "Checkout publishing branch"
        uses: actions/checkout@v2

      - name: Cache
        uses: actions/cache@v3
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            ./target
          key: ${{ runner.os }}-${{ hashFiles('**/Cargo.toml','**/Cargo.lock') }}

      - name: Set up JDK
        uses: actions/setup-java@v2
        with:
          distribution: temurin
          java-version: 11

      - name: Install aarch64 Rust target
        run: rustup target add aarch64-apple-darwin

      - name: Build ldk-node-jvm library
        run: |
          ./scripts/uniffi_bindgen_generate_kotlin.sh

      # build aarch64 + x86_64 native libraries and upload
      - name: Upload macOS native libraries for reuse in publishing job
        uses: actions/upload-artifact@v3
        with:
          # name: no name is required because we upload the entire directory
          # the default name "artifact" will be used
          path: /Users/runner/work/ldk-node/ldk-node/bindings/kotlin/ldk-node-jvm/lib/src/main/resources/

  build-jvm-full-library:
    name: "Create full ldk-node-jvm library"
    needs: [build-jvm-macOS-M1-native-lib]
    runs-on: ubuntu-20.04
    steps:
      - name: "Check out PR branch"
        uses: actions/checkout@v2

      - name: "Cache"
        uses: actions/cache@v2
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            ./target
          key: ${{ runner.os }}-${{ hashFiles('**/Cargo.toml','**/Cargo.lock') }}

      - name: "Set up JDK"
        uses: actions/setup-java@v2
        with:
          distribution: temurin
          java-version: 11

      - name: "Build ldk-node-jvm library"
        run: |
          ./scripts/uniffi_bindgen_generate_kotlin.sh

      - name: Download macOS native libraries from previous job
        uses: actions/download-artifact@v3
        id: download
        with:
          # download the artifact created in the prior job (named "artifact")
          name: artifact
          path: ./bindings/kotlin/ldk-node-jvm/lib/src/main/resources/

      - name: "Publish to Maven Local and Maven Central"
        env:
          ORG_GRADLE_PROJECT_signingKeyId: ${{ secrets.PGP_KEY_ID }}
          ORG_GRADLE_PROJECT_signingKey: ${{ secrets.PGP_SECRET_KEY }}
          ORG_GRADLE_PROJECT_signingPassword: ${{ secrets.PGP_PASSPHRASE }}
          ORG_GRADLE_PROJECT_ossrhUsername: ${{ secrets.NEXUS_USERNAME }}
          ORG_GRADLE_PROJECT_ossrhPassword: ${{ secrets.NEXUS_PASSWORD }}
        run: |
          cd bindings/kotlin/ldk-node-jvm
          ./gradlew publishToSonatype closeAndReleaseSonatypeStagingRepository

'''
'''--- .github/workflows/python.yml ---
name: Continuous Integration Checks - Python

on: [push, pull_request]

jobs:
  check-python:
    runs-on: ubuntu-latest

    env:
      LDK_NODE_PYTHON_DIR: bindings/python

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Generate Python bindings
        run: ./scripts/uniffi_bindgen_generate_python.sh

      - name: Start bitcoind and electrs
        run: docker compose up -d

      - name: Install testing prerequisites
        run: |
          pip3 install requests

      - name: Run Python unit tests
        env:
          BITCOIN_CLI_BIN: "docker exec ldk-node-bitcoin-1 bitcoin-cli"
          BITCOIND_RPC_USER: "user"
          BITCOIND_RPC_PASSWORD: "pass"
          ESPLORA_ENDPOINT: "http://127.0.0.1:3002"
        run: |
          cd $LDK_NODE_PYTHON_DIR
          python3 -m unittest discover -s src/ldk_node

'''
'''--- CHANGELOG.md ---
# 0.1.0 - Jun 22, 2023
This is the first non-experimental release of LDK Node.

- Log files are now split based on the start date of the node (#116).
- Support for allowing inbound trusted 0conf channels has been added (#69).
- Non-permanently connected peers are now included in `Node::list_peers` (#95).
- A utility method for generating a BIP39 mnemonic is now exposed in bindings (#113).
- A `ChannelConfig` may now be specified on channel open or updated afterwards (#122).
- Logging has been improved and `Builder` now returns an error rather than panicking if encountering a build failure (#119).
- In Rust, `Builder::build` now returns a `Node` object rather than wrapping it in an `Arc` (#115).
- A number of `Config` defaults have been updated and are now exposed in bindings (#124).
- The API has been updated to be more aligned between Rust and bindings (#114).

## Compatibility Notes
- Our currently supported minimum Rust version (MSRV) is 1.60.0.
- The superfluous `SendingFailed` payment status has been removed, breaking serialization compatibility with alpha releases (#125).
- The serialization formats of `PaymentDetails` and `Event` types have been updated, ensuring users upgrading from an alpha release fail to start rather than continuing operating with bogus data. Alpha users should wipe their persisted payment metadata (`payments/*`) and event queue (`events`) after the update (#130).

In total, this release includes changes in 52 commits from 2 authors:
- Elias Rohrer
- Richard Ulrich

# 0.1-alpha.1 - Jun 6, 2023
- Generation of Swift, Kotlin (JVM and Android), and Python bindings is now supported through UniFFI (#25).
- Lists of connected peers and channels may now be retrieved in bindings (#56).
- Gossip data may now be sourced from the P2P network, or a Rapid Gossip Sync server (#70).
- Network addresses are now stored and resolved via a `NetAddress` type (#85).
- The `next_event` method has been renamed `wait_next_event` and a new non-blocking method for event queue access has been introduces as `next_event` (#91).
- Node announcements are now regularly broadcasted (#93).
- Duplicate payments are now only avoided if we actually sent them out (#96).
- The `Node` may now be used to sign and verify arbitrary messages (#99).
- A `KVStore` interface is introduced that may be used to implement custom persistence backends (#101).
- An `SqliteStore` persistence backend is added and set as the new default (#100).
- Successful fee rate updates are now mandatory on `Node` startup (#102).
- The wallet sync intervals are now configurable (#102).
- Granularity of logging can now be configured (#108).

In total, this release includes changes in 64 commits from 4 authors:
- Steve Myers
- Elias Rohrer
- Jurvis Tan
- televis

**Note:** This release is still considered experimental, should not be run in
production, and no compatibility guarantees are given until the release of 0.1.

# 0.1-alpha - Apr 27, 2023
This is the first alpha release of LDK Node. It features support for sourcing
chain data via an Esplora server, file system persistence, gossip sourcing via
the Lightning peer-to-peer network, and configurable entropy sources for the
integrated LDK and BDK-based wallets.

**Note:** This release is still considered experimental, should not be run in
production, and no compatibility guarantees are given until the release of 0.1.

'''
'''--- Cargo.toml ---
[package]
name = "ldk-node"
version = "0.1.0"
authors = ["Elias Rohrer <dev@tnull.de>"]
homepage = "https://lightningdevkit.org/"
license = "MIT OR Apache-2.0"
edition = "2021"
description = "A ready-to-go node implementation built using LDK."
repository = "https://github.com/lightningdevkit/ldk-node/"
readme = "README.md"
keywords = ["bitcoin", "lightning", "ldk", "bdk"]
categories = ["cryptography::cryptocurrencies"]

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[lib]
crate-type = ["lib", "staticlib", "cdylib"]
name = "ldk_node"

[profile.release-smaller]
inherits = "release"
opt-level = 'z'     # Optimize for size.
lto = true          # Enable Link Time Optimization
codegen-units = 1   # Reduce number of codegen units to increase optimizations.
panic = 'abort'     # Abort on panic

[features]
default = []

[dependencies]
lightning = { version = "0.0.118", features = ["std"] }
lightning-invoice = { version = "0.26.0" }
lightning-net-tokio = { version = "0.0.118" }
lightning-persister = { version = "0.0.118" }
lightning-background-processor = { version = "0.0.118", features = ["futures"] }
lightning-rapid-gossip-sync = { version = "0.0.118" }
lightning-transaction-sync = { version = "0.0.118", features = ["esplora-async-https"] }

# lightning = { git = "https://github.com/lightningdevkit/rust-lightning", branch="main", features = ["std"] }
# lightning-invoice = { git = "https://github.com/lightningdevkit/rust-lightning", branch="main" }
# lightning-net-tokio = { git = "https://github.com/lightningdevkit/rust-lightning", branch="main" }
# lightning-persister = { git = "https://github.com/lightningdevkit/rust-lightning", branch="main" }
# lightning-background-processor = { git = "https://github.com/lightningdevkit/rust-lightning", branch="main", features = ["futures"] }
# lightning-rapid-gossip-sync = { git = "https://github.com/lightningdevkit/rust-lightning", branch="main" }
# lightning-transaction-sync = { git = "https://github.com/lightningdevkit/rust-lightning", branch="main", features = ["esplora-async"] }

#lightning = { path = "../rust-lightning/lightning", features = ["std"] }
#lightning-invoice = { path = "../rust-lightning/lightning-invoice" }
#lightning-net-tokio = { path = "../rust-lightning/lightning-net-tokio" }
#lightning-persister = { path = "../rust-lightning/lightning-persister" }
#lightning-background-processor = { path = "../rust-lightning/lightning-background-processor", features = ["futures"] }
#lightning-rapid-gossip-sync = { path = "../rust-lightning/lightning-rapid-gossip-sync" }
#lightning-transaction-sync = { path = "../rust-lightning/lightning-transaction-sync", features = ["esplora-async"] }

bdk = { version = "0.28.0", default-features = false, features = ["std", "async-interface", "use-esplora-async", "sqlite-bundled", "keys-bip39"]}

reqwest = { version = "0.11", default-features = false, features = ["json", "rustls-tls"] }
rusqlite = { version = "0.28.0", features = ["bundled"] }
bitcoin = "0.29.2"
bip39 = "2.0.0"

rand = "0.8.5"
chrono = { version = "0.4", default-features = false, features = ["clock"] }
futures = "0.3"
tokio = { version = "1", default-features = false, features = [ "rt-multi-thread", "time", "sync" ] }
esplora-client = { version = "0.4", default-features = false }
libc = "0.2"
uniffi = { version = "0.25.1", features = ["build"], optional = true }

[target.'cfg(vss)'.dependencies]
vss-client = "0.1"

[target.'cfg(windows)'.dependencies]
winapi = { version = "0.3", features = ["winbase"] }

[dev-dependencies]
lightning = { version = "0.0.118", features = ["std", "_test_utils"] }
#lightning = { git = "https://github.com/lightningdevkit/rust-lightning", branch="main", features = ["std", "_test_utils"] }
electrsd = { version = "0.22.0", features = ["legacy", "esplora_a33e97e1", "bitcoind_23_0"] }
electrum-client = "0.12.0"
proptest = "1.0.0"
regex = "1.5.6"

[build-dependencies]
uniffi = { version = "0.25.1", features = ["build"], optional = true }

[profile.release]
panic = "abort"

[profile.dev]
panic = "abort"

'''
'''--- LICENSE.md ---
This software is licensed under [Apache 2.0](LICENSE-APACHE) or
[MIT](LICENSE-MIT), at your option.

Some files retain their own copyright notice, however, for full authorship
information, see version control history.

Except as otherwise noted in individual files, all files in this repository are
licensed under the Apache License, Version 2.0 <LICENSE-APACHE or
http://www.apache.org/licenses/LICENSE-2.0> or the MIT license <LICENSE-MIT or
http://opensource.org/licenses/MIT>, at your option.

You may not use, copy, modify, merge, publish, distribute, sublicense, and/or
sell copies of this software or any files in this repository except in
accordance with one or both of these licenses.

'''
'''--- Package.swift ---
// swift-tools-version:5.5
// The swift-tools-version declares the minimum version of Swift required to build this package.

import PackageDescription

let tag = "v0.1.0"
let checksum = "2ef0b46c84f7349e1940ff0b592be4dea9f2761a69d2bdad8f979dbef68fb1e8"
let url = "https://github.com/lightningdevkit/ldk-node/releases/download/\(tag)/LDKNodeFFI.xcframework.zip"

let package = Package(
    name: "ldk-node",
    platforms: [
        .iOS(.v15),
        .macOS(.v12),
    ],
    products: [
        // Products define the executables and libraries a package produces, and make them visible to other packages.
        .library(
            name: "LDKNode",
            targets: ["LDKNodeFFI", "LDKNode"]),
    ],
    targets: [
        .target(
            name: "LDKNode",
            dependencies: ["LDKNodeFFI"],
            path: "./bindings/swift/Sources",
            swiftSettings: [.unsafeFlags(["-suppress-warnings"])]
        ),
        .binaryTarget(
            name: "LDKNodeFFI",
            url: url,
            checksum: checksum
            )
    ]
)

'''
'''--- README.md ---
# LDK Node

[![Crate](https://img.shields.io/crates/v/ldk-node.svg?logo=rust)](https://crates.io/crates/ldk-node)
[![Documentation](https://img.shields.io/static/v1?logo=read-the-docs&label=docs.rs&message=ldk-node&color=informational)](https://docs.rs/ldk-node)

A ready-to-go Lightning node library built using [LDK][ldk] and [BDK][bdk].

LDK Node is a self-custodial Lightning node in library form. Its central goal is to provide a small, simple, and straightforward interface that enables users to easily set up and run a Lightning node with an integrated on-chain wallet. While minimalism is at its core, LDK Node aims to be sufficiently modular and configurable to be useful for a variety of use cases.

## Getting Started
The primary abstraction of the library is the [`Node`][api_docs_node], which can be retrieved by setting up and configuring a [`Builder`][api_docs_builder] to your liking and calling one of the `build` methods. `Node` can then be controlled via commands such as `start`, `stop`, `connect_open_channel`, `send_payment`, etc.

```rust
use ldk_node::Builder;
use ldk_node::lightning_invoice::Invoice;
use ldk_node::lightning::ln::msgs::SocketAddress;
use ldk_node::bitcoin::secp256k1::PublicKey;
use ldk_node::bitcoin::Network;
use std::str::FromStr;

fn main() {
	let mut builder = Builder::new();
	builder.set_network(Network::Testnet);
	builder.set_esplora_server("https://blockstream.info/testnet/api".to_string());
	builder.set_gossip_source_rgs("https://rapidsync.lightningdevkit.org/testnet/snapshot".to_string());

	let node = builder.build().unwrap();

	node.start().unwrap();

	let funding_address = node.new_onchain_address();

	// .. fund address ..

	let node_id = PublicKey::from_str("NODE_ID").unwrap();
	let node_addr = SocketAddress::from_str("IP_ADDR:PORT").unwrap();
	node.connect_open_channel(node_id, node_addr, 10000, None, None, false).unwrap();

	let event = node.wait_next_event();
	println!("EVENT: {:?}", event);
	node.event_handled();

	let invoice = Invoice::from_str("INVOICE_STR").unwrap();
	node.send_payment(&invoice).unwrap();

	node.stop().unwrap();
}
```

## Modularity

LDK Node currently comes with a decidedly opinionated set of design choices:

- On-chain data is handled by the integrated [BDK][bdk] wallet.
- Chain data may currently be sourced from an [Esplora][esplora] server, while support for Electrum and `bitcoind` RPC will follow soon.
- Wallet and channel state may be persisted to an [SQLite][sqlite] database, to file system, or to a custom back-end to be implemented by the user.
- Gossip data may be sourced via Lightning's peer-to-peer network or the [Rapid Gossip Sync](https://docs.rs/lightning-rapid-gossip-sync/*/lightning_rapid_gossip_sync/) protocol.
- Entropy for the Lightning and on-chain wallets may be sourced from raw bytes or a [BIP39](https://github.com/bitcoin/bips/blob/master/bip-0039.mediawiki) mnemonic. In addition, LDK Node offers the means to generate and persist the entropy bytes to disk.

## Language Support
LDK Node itself is written in [Rust][rust] and may therefore be natively added as a library dependency to any `std` Rust program. However, beyond its Rust API it also offers language bindings for [Swift][swift], [Kotlin][kotlin], and [Python][python] based on the [UniFFI](https://github.com/mozilla/uniffi-rs/). Moreover, [Flutter bindings][flutter_bindings] are also available.

## MSRV
The Minimum Supported Rust Version (MSRV) is currently 1.63.0.

[api_docs]: https://docs.rs/ldk-node/*/ldk_node/
[api_docs_node]: https://docs.rs/ldk-node/*/ldk_node/struct.Node.html
[api_docs_builder]: https://docs.rs/ldk-node/*/ldk_node/struct.Builder.html
[rust_crate]: https://crates.io/
[ldk]: https://lightningdevkit.org/
[bdk]: https://bitcoindevkit.org/
[esplora]: https://github.com/Blockstream/esplora
[sqlite]: https://sqlite.org/
[rust]: https://www.rust-lang.org/
[swift]: https://www.swift.org/
[kotlin]: https://kotlinlang.org/
[python]: https://www.python.org/
[flutter_bindings]: https://github.com/LtbLightning/ldk-node-flutter

'''
'''--- bindings/kotlin/ldk-node-android/gradlew.bat ---
@rem
@rem Copyright 2015 the original author or authors.
@rem
@rem Licensed under the Apache License, Version 2.0 (the "License");
@rem you may not use this file except in compliance with the License.
@rem You may obtain a copy of the License at
@rem
@rem      https://www.apache.org/licenses/LICENSE-2.0
@rem
@rem Unless required by applicable law or agreed to in writing, software
@rem distributed under the License is distributed on an "AS IS" BASIS,
@rem WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@rem See the License for the specific language governing permissions and
@rem limitations under the License.
@rem

@if "%DEBUG%"=="" @echo off
@rem ##########################################################################
@rem
@rem  Gradle startup script for Windows
@rem
@rem ##########################################################################

@rem Set local scope for the variables with windows NT shell
if "%OS%"=="Windows_NT" setlocal

set DIRNAME=%~dp0
if "%DIRNAME%"=="" set DIRNAME=.
@rem This is normally unused
set APP_BASE_NAME=%~n0
set APP_HOME=%DIRNAME%

@rem Resolve any "." and ".." in APP_HOME to make it shorter.
for %%i in ("%APP_HOME%") do set APP_HOME=%%~fi

@rem Add default JVM options here. You can also use JAVA_OPTS and GRADLE_OPTS to pass JVM options to this script.
set DEFAULT_JVM_OPTS="-Xmx64m" "-Xms64m"

@rem Find java.exe
if defined JAVA_HOME goto findJavaFromJavaHome

set JAVA_EXE=java.exe
%JAVA_EXE% -version >NUL 2>&1
if %ERRORLEVEL% equ 0 goto execute

echo.
echo ERROR: JAVA_HOME is not set and no 'java' command could be found in your PATH.
echo.
echo Please set the JAVA_HOME variable in your environment to match the
echo location of your Java installation.

goto fail

:findJavaFromJavaHome
set JAVA_HOME=%JAVA_HOME:"=%
set JAVA_EXE=%JAVA_HOME%/bin/java.exe

if exist "%JAVA_EXE%" goto execute

echo.
echo ERROR: JAVA_HOME is set to an invalid directory: %JAVA_HOME%
echo.
echo Please set the JAVA_HOME variable in your environment to match the
echo location of your Java installation.

goto fail

:execute
@rem Setup the command line

set CLASSPATH=%APP_HOME%\gradle\wrapper\gradle-wrapper.jar

@rem Execute Gradle
"%JAVA_EXE%" %DEFAULT_JVM_OPTS% %JAVA_OPTS% %GRADLE_OPTS% "-Dorg.gradle.appname=%APP_BASE_NAME%" -classpath "%CLASSPATH%" org.gradle.wrapper.GradleWrapperMain %*

:end
@rem End local scope for the variables with windows NT shell
if %ERRORLEVEL% equ 0 goto mainEnd

:fail
rem Set variable GRADLE_EXIT_CONSOLE if you need the _script_ return code instead of
rem the _cmd.exe /c_ return code!
set EXIT_CODE=%ERRORLEVEL%
if %EXIT_CODE% equ 0 set EXIT_CODE=1
if not ""=="%GRADLE_EXIT_CONSOLE%" exit %EXIT_CODE%
exit /b %EXIT_CODE%

:mainEnd
if "%OS%"=="Windows_NT" endlocal

:omega

'''
'''--- bindings/kotlin/ldk-node-android/lib/src/main/AndroidManifest.xml ---
<manifest xmlns:android="http://schemas.android.com/apk/res/android"
  package="org.lightningdevkit">

  <uses-permission android:name="android.permission.INTERNET" />

</manifest>

'''
'''--- bindings/kotlin/ldk-node-jvm/gradlew.bat ---
@rem
@rem Copyright 2015 the original author or authors.
@rem
@rem Licensed under the Apache License, Version 2.0 (the "License");
@rem you may not use this file except in compliance with the License.
@rem You may obtain a copy of the License at
@rem
@rem      https://www.apache.org/licenses/LICENSE-2.0
@rem
@rem Unless required by applicable law or agreed to in writing, software
@rem distributed under the License is distributed on an "AS IS" BASIS,
@rem WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@rem See the License for the specific language governing permissions and
@rem limitations under the License.
@rem

@if "%DEBUG%"=="" @echo off
@rem ##########################################################################
@rem
@rem  Gradle startup script for Windows
@rem
@rem ##########################################################################

@rem Set local scope for the variables with windows NT shell
if "%OS%"=="Windows_NT" setlocal

set DIRNAME=%~dp0
if "%DIRNAME%"=="" set DIRNAME=.
@rem This is normally unused
set APP_BASE_NAME=%~n0
set APP_HOME=%DIRNAME%

@rem Resolve any "." and ".." in APP_HOME to make it shorter.
for %%i in ("%APP_HOME%") do set APP_HOME=%%~fi

@rem Add default JVM options here. You can also use JAVA_OPTS and GRADLE_OPTS to pass JVM options to this script.
set DEFAULT_JVM_OPTS="-Xmx64m" "-Xms64m"

@rem Find java.exe
if defined JAVA_HOME goto findJavaFromJavaHome

set JAVA_EXE=java.exe
%JAVA_EXE% -version >NUL 2>&1
if %ERRORLEVEL% equ 0 goto execute

echo.
echo ERROR: JAVA_HOME is not set and no 'java' command could be found in your PATH.
echo.
echo Please set the JAVA_HOME variable in your environment to match the
echo location of your Java installation.

goto fail

:findJavaFromJavaHome
set JAVA_HOME=%JAVA_HOME:"=%
set JAVA_EXE=%JAVA_HOME%/bin/java.exe

if exist "%JAVA_EXE%" goto execute

echo.
echo ERROR: JAVA_HOME is set to an invalid directory: %JAVA_HOME%
echo.
echo Please set the JAVA_HOME variable in your environment to match the
echo location of your Java installation.

goto fail

:execute
@rem Setup the command line

set CLASSPATH=%APP_HOME%\gradle\wrapper\gradle-wrapper.jar

@rem Execute Gradle
"%JAVA_EXE%" %DEFAULT_JVM_OPTS% %JAVA_OPTS% %GRADLE_OPTS% "-Dorg.gradle.appname=%APP_BASE_NAME%" -classpath "%CLASSPATH%" org.gradle.wrapper.GradleWrapperMain %*

:end
@rem End local scope for the variables with windows NT shell
if %ERRORLEVEL% equ 0 goto mainEnd

:fail
rem Set variable GRADLE_EXIT_CONSOLE if you need the _script_ return code instead of
rem the _cmd.exe /c_ return code!
set EXIT_CODE=%ERRORLEVEL%
if %EXIT_CODE% equ 0 set EXIT_CODE=1
if not ""=="%GRADLE_EXIT_CONSOLE%" exit %EXIT_CODE%
exit /b %EXIT_CODE%

:mainEnd
if "%OS%"=="Windows_NT" endlocal

:omega

'''
'''--- bindings/python/pyproject.toml ---
[project]
name = "ldk_node"
version = "0.1-alpha.1"
authors = [
  { name="Elias Rohrer", email="dev@tnull.de" },
]
description = "A ready-to-go Lightning node library built using LDK and BDK."
readme = "README.md"
requires-python = ">=3.6"
classifiers = [
    "Topic :: Software Development :: Libraries",
    "Topic :: Security :: Cryptography",
    "License :: OSI Approved :: MIT License",
    "License :: OSI Approved :: Apache Software License",
    "Programming Language :: Python :: 3",
]

[project.urls]
"Homepage" = "https://lightningdevkit.org/"
"Github" = "https://github.com/lightningdevkit/ldk-node"
"Bug Tracker" = "https://github.com/lightningdevkit/ldk-node/issues"

'''
'''--- bindings/python/src/ldk_node/__init__.py ---
from ldk_node.ldk_node import *

'''
'''--- bindings/python/src/ldk_node/test_ldk_node.py ---
import unittest
import tempfile
import time
import subprocess
import os
import re
import requests

from ldk_node import *

DEFAULT_ESPLORA_SERVER_URL = "http://127.0.0.1:3002"
DEFAULT_TEST_NETWORK = Network.REGTEST
DEFAULT_BITCOIN_CLI_BIN = "bitcoin-cli"

def bitcoin_cli(cmd):
    args = []

    bitcoin_cli_bin = [DEFAULT_BITCOIN_CLI_BIN]
    if os.environ.get('BITCOIN_CLI_BIN'):
        bitcoin_cli_bin = os.environ['BITCOIN_CLI_BIN'].split()

    args += bitcoin_cli_bin
    args += ["-regtest"]

    if os.environ.get('BITCOIND_RPC_USER'):
        args += ["-rpcuser=" + os.environ['BITCOIND_RPC_USER']]

    if os.environ.get('BITCOIND_RPC_PASSWORD'):
        args += ["-rpcpassword=" + os.environ['BITCOIND_RPC_PASSWORD']]

    for c in cmd.split():
        args += [c]

    print("RUNNING:", args)
    res = subprocess.run(args, capture_output=True)
    return str(res.stdout.decode("utf-8"))

def mine(blocks):
    address = bitcoin_cli("getnewaddress").strip()
    mining_res = bitcoin_cli("generatetoaddress " + str(blocks) + " " + str(address))
    print("MINING_RES:", mining_res)

    m = re.search("\\n.+\n\\]$", mining_res)
    last_block = str(m.group(0))
    return str(last_block.strip().replace('"','').replace('\n]',''))

def mine_and_wait(esplora_endpoint, blocks):
    last_block = mine(blocks)
    wait_for_block(esplora_endpoint, last_block)

def wait_for_block(esplora_endpoint, block_hash):
    url = esplora_endpoint + "/block/" + block_hash + "/status"
    esplora_picked_up_block = False
    while not esplora_picked_up_block:
        res = requests.get(url)
        try:
            json = res.json()
            esplora_picked_up_block = json['in_best_chain']
        except:
            pass
        time.sleep(1)

def wait_for_tx(esplora_endpoint, txid):
    url = esplora_endpoint + "/tx/" + txid
    esplora_picked_up_tx = False
    while not esplora_picked_up_tx:
        res = requests.get(url)
        try:
            json = res.json()
            esplora_picked_up_tx = json['txid'] == txid
        except:
            pass
        time.sleep(1)

def send_to_address(address, amount_sats):
    amount_btc = amount_sats/100000000.0
    cmd = "sendtoaddress " + str(address) + " " + str(amount_btc)
    res = str(bitcoin_cli(cmd)).strip()
    print("SEND TX:", res)
    return res

def setup_node(tmp_dir, esplora_endpoint, listening_addresses):
    config = Config()
    builder = Builder.from_config(config)
    builder.set_storage_dir_path(tmp_dir)
    builder.set_esplora_server(esplora_endpoint)
    builder.set_network(DEFAULT_TEST_NETWORK)
    builder.set_listening_addresses(listening_addresses)
    return builder.build()

def get_esplora_endpoint():
    if os.environ.get('ESPLORA_ENDPOINT'):
        return str(os.environ['ESPLORA_ENDPOINT'])
    return DEFAULT_ESPLORA_SERVER_URL

class TestLdkNode(unittest.TestCase):
    def setUp(self):
        bitcoin_cli("createwallet ldk_node_test")
        mine(101)
        time.sleep(3)
        esplora_endpoint = get_esplora_endpoint()
        mine_and_wait(esplora_endpoint, 1)

    def test_channel_full_cycle(self):
        esplora_endpoint = get_esplora_endpoint()

        ## Setup Node 1
        tmp_dir_1 = tempfile.TemporaryDirectory("_ldk_node_1")
        print("TMP DIR 1:", tmp_dir_1.name)

        listening_addresses_1 = ["127.0.0.1:2323"]
        node_1 = setup_node(tmp_dir_1.name, esplora_endpoint, listening_addresses_1)
        node_1.start()
        node_id_1 = node_1.node_id()
        print("Node ID 1:", node_id_1)

        # Setup Node 2
        tmp_dir_2 = tempfile.TemporaryDirectory("_ldk_node_2")
        print("TMP DIR 2:", tmp_dir_2.name)

        listening_addresses_2 = ["127.0.0.1:2324"]
        node_2 = setup_node(tmp_dir_2.name, esplora_endpoint, listening_addresses_2)
        node_2.start()
        node_id_2 = node_2.node_id()
        print("Node ID 2:", node_id_2)

        address_1 = node_1.new_onchain_address()
        txid_1 = send_to_address(address_1, 100000)
        address_2 = node_2.new_onchain_address()
        txid_2 = send_to_address(address_2, 100000)

        wait_for_tx(esplora_endpoint, txid_1)
        wait_for_tx(esplora_endpoint, txid_2)

        mine_and_wait(esplora_endpoint, 6)

        node_1.sync_wallets()
        node_2.sync_wallets()

        spendable_balance_1 = node_1.spendable_onchain_balance_sats()
        spendable_balance_2 = node_2.spendable_onchain_balance_sats()
        total_balance_1 = node_1.total_onchain_balance_sats()
        total_balance_2 = node_2.total_onchain_balance_sats()

        print("SPENDABLE 1:", spendable_balance_1)
        self.assertEqual(spendable_balance_1, 100000)

        print("SPENDABLE 2:", spendable_balance_2)
        self.assertEqual(spendable_balance_2, 100000)

        print("TOTAL 1:", total_balance_1)
        self.assertEqual(total_balance_1, 100000)

        print("TOTAL 2:", total_balance_2)
        self.assertEqual(total_balance_2, 100000)

        node_1.connect_open_channel(node_id_2, listening_addresses_2[0], 50000, None, None, True)

        channel_pending_event_1 = node_1.wait_next_event()
        assert isinstance(channel_pending_event_1, Event.CHANNEL_PENDING)
        print("EVENT:", channel_pending_event_1)
        node_1.event_handled()

        channel_pending_event_2 = node_2.wait_next_event()
        assert isinstance(channel_pending_event_2, Event.CHANNEL_PENDING)
        print("EVENT:", channel_pending_event_2)
        node_2.event_handled()

        funding_txid = channel_pending_event_1.funding_txo.txid
        wait_for_tx(esplora_endpoint, funding_txid)
        mine_and_wait(esplora_endpoint, 6)

        node_1.sync_wallets()
        node_2.sync_wallets()

        channel_ready_event_1 = node_1.wait_next_event()
        assert isinstance(channel_ready_event_1, Event.CHANNEL_READY)
        print("EVENT:", channel_ready_event_1)
        print("funding_txo:", funding_txid)
        node_1.event_handled()

        channel_ready_event_2 = node_2.wait_next_event()
        assert isinstance(channel_ready_event_2, Event.CHANNEL_READY)
        print("EVENT:", channel_ready_event_2)
        node_2.event_handled()

        invoice = node_2.receive_payment(2500000, "asdf", 9217)
        node_1.send_payment(invoice)

        payment_successful_event_1 = node_1.wait_next_event()
        assert isinstance(payment_successful_event_1, Event.PAYMENT_SUCCESSFUL)
        print("EVENT:", payment_successful_event_1)
        node_1.event_handled()

        payment_received_event_2 = node_2.wait_next_event()
        assert isinstance(payment_received_event_2, Event.PAYMENT_RECEIVED)
        print("EVENT:", payment_received_event_2)
        node_2.event_handled()

        node_2.close_channel(channel_ready_event_2.channel_id, node_id_1)

        channel_closed_event_1 = node_1.wait_next_event()
        assert isinstance(channel_closed_event_1, Event.CHANNEL_CLOSED)
        print("EVENT:", channel_closed_event_1)
        node_1.event_handled()

        channel_closed_event_2 = node_2.wait_next_event()
        assert isinstance(channel_closed_event_2, Event.CHANNEL_CLOSED)
        print("EVENT:", channel_closed_event_2)
        node_2.event_handled()

        mine_and_wait(esplora_endpoint, 1)

        node_1.sync_wallets()
        node_2.sync_wallets()

        spendable_balance_after_close_1 = node_1.spendable_onchain_balance_sats()
        assert spendable_balance_after_close_1 > 95000
        assert spendable_balance_after_close_1 < 100000
        spendable_balance_after_close_2 = node_2.spendable_onchain_balance_sats()
        self.assertEqual(spendable_balance_after_close_2, 102500)

        # Stop nodes
        node_1.stop()
        node_2.stop()

        # Cleanup
        time.sleep(1) # Wait a sec so our logs can finish writing
        tmp_dir_1.cleanup()
        tmp_dir_2.cleanup()

if __name__ == '__main__':
    unittest.main()

'''
'''--- bindings/swift/LDKNodeFFI.xcframework/ios-arm64/LDKNodeFFI.framework/Headers/LDKNodeFFI-umbrella.h ---
// This is the "umbrella header" for our combined Rust code library.
// It needs to import all of the individual headers.

#import "LDKNodeFFI.h"

'''
'''--- bindings/swift/LDKNodeFFI.xcframework/ios-arm64_x86_64-simulator/LDKNodeFFI.framework/Headers/LDKNodeFFI-umbrella.h ---
// This is the "umbrella header" for our combined Rust code library.
// It needs to import all of the individual headers.

#import "LDKNodeFFI.h"

'''
'''--- bindings/swift/LDKNodeFFI.xcframework/macos-arm64_x86_64/LDKNodeFFI.framework/Headers/LDKNodeFFI-umbrella.h ---
// This is the "umbrella header" for our combined Rust code library.
// It needs to import all of the individual headers.

#import "LDKNodeFFI.h"

'''
'''--- bindings/swift/Package.swift ---
// swift-tools-version:5.5
// The swift-tools-version declares the minimum version of Swift required to build this package.
import PackageDescription

let package = Package(
    name: "ldk-node",
    platforms: [
        .iOS(.v15),
        .macOS(.v12),
    ],
    products: [
        // Products define the executables and libraries a package produces, and make them visible to other packages.
        .library(
            name: "LDKNode",
            targets: ["LDKNodeFFI", "LDKNode"]),
    ],
    targets: [
        .target(
            name: "LDKNode",
            dependencies: ["LDKNodeFFI"]
        ),
        .binaryTarget(
            name: "LDKNodeFFI",
            path: "./LDKNodeFFI.xcframework"
            )
    ]
)

'''
'''--- bindings/swift/Sources/LDKNode/LDKNode.swift ---
// This file was autogenerated by some hot garbage in the `uniffi` crate.
// Trust me, you don't want to mess with it!
import Foundation

// Depending on the consumer's build setup, the low-level FFI code
// might be in a separate module, or it might be compiled inline into
// this module. This is a bit of light hackery to work with both.
#if canImport(LDKNodeFFI)
    import LDKNodeFFI
#endif

private extension RustBuffer {
    // Allocate a new buffer, copying the contents of a `UInt8` array.
    init(bytes: [UInt8]) {
        let rbuf = bytes.withUnsafeBufferPointer { ptr in
            RustBuffer.from(ptr)
        }
        self.init(capacity: rbuf.capacity, len: rbuf.len, data: rbuf.data)
    }

    static func from(_ ptr: UnsafeBufferPointer<UInt8>) -> RustBuffer {
        try! rustCall { ffi_ldk_node_3490_rustbuffer_from_bytes(ForeignBytes(bufferPointer: ptr), $0) }
    }

    // Frees the buffer in place.
    // The buffer must not be used after this is called.
    func deallocate() {
        try! rustCall { ffi_ldk_node_3490_rustbuffer_free(self, $0) }
    }
}

private extension ForeignBytes {
    init(bufferPointer: UnsafeBufferPointer<UInt8>) {
        self.init(len: Int32(bufferPointer.count), data: bufferPointer.baseAddress)
    }
}

// For every type used in the interface, we provide helper methods for conveniently
// lifting and lowering that type from C-compatible data, and for reading and writing
// values of that type in a buffer.

// Helper classes/extensions that don't change.
// Someday, this will be in a library of its own.

private extension Data {
    init(rustBuffer: RustBuffer) {
        // TODO: This copies the buffer. Can we read directly from a
        // Rust buffer?
        self.init(bytes: rustBuffer.data!, count: Int(rustBuffer.len))
    }
}

// Define reader functionality.  Normally this would be defined in a class or
// struct, but we use standalone functions instead in order to make external
// types work.
//
// With external types, one swift source file needs to be able to call the read
// method on another source file's FfiConverter, but then what visibility
// should Reader have?
// - If Reader is fileprivate, then this means the read() must also
//   be fileprivate, which doesn't work with external types.
// - If Reader is internal/public, we'll get compile errors since both source
//   files will try define the same type.
//
// Instead, the read() method and these helper functions input a tuple of data

private func createReader(data: Data) -> (data: Data, offset: Data.Index) {
    (data: data, offset: 0)
}

// Reads an integer at the current offset, in big-endian order, and advances
// the offset on success. Throws if reading the integer would move the
// offset past the end of the buffer.
private func readInt<T: FixedWidthInteger>(_ reader: inout (data: Data, offset: Data.Index)) throws -> T {
    let range = reader.offset ..< reader.offset + MemoryLayout<T>.size
    guard reader.data.count >= range.upperBound else {
        throw UniffiInternalError.bufferOverflow
    }
    if T.self == UInt8.self {
        let value = reader.data[reader.offset]
        reader.offset += 1
        return value as! T
    }
    var value: T = 0
    let _ = withUnsafeMutableBytes(of: &value) { reader.data.copyBytes(to: $0, from: range) }
    reader.offset = range.upperBound
    return value.bigEndian
}

// Reads an arbitrary number of bytes, to be used to read
// raw bytes, this is useful when lifting strings
private func readBytes(_ reader: inout (data: Data, offset: Data.Index), count: Int) throws -> [UInt8] {
    let range = reader.offset ..< (reader.offset + count)
    guard reader.data.count >= range.upperBound else {
        throw UniffiInternalError.bufferOverflow
    }
    var value = [UInt8](repeating: 0, count: count)
    value.withUnsafeMutableBufferPointer { buffer in
        reader.data.copyBytes(to: buffer, from: range)
    }
    reader.offset = range.upperBound
    return value
}

// Reads a float at the current offset.
private func readFloat(_ reader: inout (data: Data, offset: Data.Index)) throws -> Float {
    return Float(bitPattern: try readInt(&reader))
}

// Reads a float at the current offset.
private func readDouble(_ reader: inout (data: Data, offset: Data.Index)) throws -> Double {
    return Double(bitPattern: try readInt(&reader))
}

// Indicates if the offset has reached the end of the buffer.
private func hasRemaining(_ reader: (data: Data, offset: Data.Index)) -> Bool {
    return reader.offset < reader.data.count
}

// Define writer functionality.  Normally this would be defined in a class or
// struct, but we use standalone functions instead in order to make external
// types work.  See the above discussion on Readers for details.

private func createWriter() -> [UInt8] {
    return []
}

private func writeBytes<S>(_ writer: inout [UInt8], _ byteArr: S) where S: Sequence, S.Element == UInt8 {
    writer.append(contentsOf: byteArr)
}

// Writes an integer in big-endian order.
//
// Warning: make sure what you are trying to write
// is in the correct type!
private func writeInt<T: FixedWidthInteger>(_ writer: inout [UInt8], _ value: T) {
    var value = value.bigEndian
    withUnsafeBytes(of: &value) { writer.append(contentsOf: $0) }
}

private func writeFloat(_ writer: inout [UInt8], _ value: Float) {
    writeInt(&writer, value.bitPattern)
}

private func writeDouble(_ writer: inout [UInt8], _ value: Double) {
    writeInt(&writer, value.bitPattern)
}

// Protocol for types that transfer other types across the FFI. This is
// analogous go the Rust trait of the same name.
private protocol FfiConverter {
    associatedtype FfiType
    associatedtype SwiftType

    static func lift(_ value: FfiType) throws -> SwiftType
    static func lower(_ value: SwiftType) -> FfiType
    static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> SwiftType
    static func write(_ value: SwiftType, into buf: inout [UInt8])
}

// Types conforming to `Primitive` pass themselves directly over the FFI.
private protocol FfiConverterPrimitive: FfiConverter where FfiType == SwiftType {}

extension FfiConverterPrimitive {
    public static func lift(_ value: FfiType) throws -> SwiftType {
        return value
    }

    public static func lower(_ value: SwiftType) -> FfiType {
        return value
    }
}

// Types conforming to `FfiConverterRustBuffer` lift and lower into a `RustBuffer`.
// Used for complex types where it's hard to write a custom lift/lower.
private protocol FfiConverterRustBuffer: FfiConverter where FfiType == RustBuffer {}

extension FfiConverterRustBuffer {
    public static func lift(_ buf: RustBuffer) throws -> SwiftType {
        var reader = createReader(data: Data(rustBuffer: buf))
        let value = try read(from: &reader)
        if hasRemaining(reader) {
            throw UniffiInternalError.incompleteData
        }
        buf.deallocate()
        return value
    }

    public static func lower(_ value: SwiftType) -> RustBuffer {
        var writer = createWriter()
        write(value, into: &writer)
        return RustBuffer(bytes: writer)
    }
}

// An error type for FFI errors. These errors occur at the UniFFI level, not
// the library level.
private enum UniffiInternalError: LocalizedError {
    case bufferOverflow
    case incompleteData
    case unexpectedOptionalTag
    case unexpectedEnumCase
    case unexpectedNullPointer
    case unexpectedRustCallStatusCode
    case unexpectedRustCallError
    case unexpectedStaleHandle
    case rustPanic(_ message: String)

    public var errorDescription: String? {
        switch self {
        case .bufferOverflow: return "Reading the requested value would read past the end of the buffer"
        case .incompleteData: return "The buffer still has data after lifting its containing value"
        case .unexpectedOptionalTag: return "Unexpected optional tag; should be 0 or 1"
        case .unexpectedEnumCase: return "Raw enum value doesn't match any cases"
        case .unexpectedNullPointer: return "Raw pointer value was null"
        case .unexpectedRustCallStatusCode: return "Unexpected RustCallStatus code"
        case .unexpectedRustCallError: return "CALL_ERROR but no errorClass specified"
        case .unexpectedStaleHandle: return "The object in the handle map has been dropped already"
        case let .rustPanic(message): return message
        }
    }
}

private let CALL_SUCCESS: Int8 = 0
private let CALL_ERROR: Int8 = 1
private let CALL_PANIC: Int8 = 2

private extension RustCallStatus {
    init() {
        self.init(
            code: CALL_SUCCESS,
            errorBuf: RustBuffer(
                capacity: 0,
                len: 0,
                data: nil
            )
        )
    }
}

private func rustCall<T>(_ callback: (UnsafeMutablePointer<RustCallStatus>) -> T) throws -> T {
    try makeRustCall(callback, errorHandler: {
        $0.deallocate()
        return UniffiInternalError.unexpectedRustCallError
    })
}

private func rustCallWithError<T, F: FfiConverter>
(_ errorFfiConverter: F.Type, _ callback: (UnsafeMutablePointer<RustCallStatus>) -> T) throws -> T
    where F.SwiftType: Error, F.FfiType == RustBuffer
{
    try makeRustCall(callback, errorHandler: { try errorFfiConverter.lift($0) })
}

private func makeRustCall<T>(_ callback: (UnsafeMutablePointer<RustCallStatus>) -> T, errorHandler: (RustBuffer) throws -> Error) throws -> T {
    var callStatus = RustCallStatus()
    let returnedVal = callback(&callStatus)
    switch callStatus.code {
    case CALL_SUCCESS:
        return returnedVal

    case CALL_ERROR:
        throw try errorHandler(callStatus.errorBuf)

    case CALL_PANIC:
        // When the rust code sees a panic, it tries to construct a RustBuffer
        // with the message.  But if that code panics, then it just sends back
        // an empty buffer.
        if callStatus.errorBuf.len > 0 {
            throw UniffiInternalError.rustPanic(try FfiConverterString.lift(callStatus.errorBuf))
        } else {
            callStatus.errorBuf.deallocate()
            throw UniffiInternalError.rustPanic("Rust panic")
        }

    default:
        throw UniffiInternalError.unexpectedRustCallStatusCode
    }
}

// Public interface members begin here.

private struct FfiConverterUInt8: FfiConverterPrimitive {
    typealias FfiType = UInt8
    typealias SwiftType = UInt8

    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> UInt8 {
        return try lift(readInt(&buf))
    }

    public static func write(_ value: UInt8, into buf: inout [UInt8]) {
        writeInt(&buf, lower(value))
    }
}

private struct FfiConverterUInt16: FfiConverterPrimitive {
    typealias FfiType = UInt16
    typealias SwiftType = UInt16

    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> UInt16 {
        return try lift(readInt(&buf))
    }

    public static func write(_ value: SwiftType, into buf: inout [UInt8]) {
        writeInt(&buf, lower(value))
    }
}

private struct FfiConverterUInt32: FfiConverterPrimitive {
    typealias FfiType = UInt32
    typealias SwiftType = UInt32

    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> UInt32 {
        return try lift(readInt(&buf))
    }

    public static func write(_ value: SwiftType, into buf: inout [UInt8]) {
        writeInt(&buf, lower(value))
    }
}

private struct FfiConverterUInt64: FfiConverterPrimitive {
    typealias FfiType = UInt64
    typealias SwiftType = UInt64

    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> UInt64 {
        return try lift(readInt(&buf))
    }

    public static func write(_ value: SwiftType, into buf: inout [UInt8]) {
        writeInt(&buf, lower(value))
    }
}

private struct FfiConverterBool: FfiConverter {
    typealias FfiType = Int8
    typealias SwiftType = Bool

    public static func lift(_ value: Int8) throws -> Bool {
        return value != 0
    }

    public static func lower(_ value: Bool) -> Int8 {
        return value ? 1 : 0
    }

    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> Bool {
        return try lift(readInt(&buf))
    }

    public static func write(_ value: Bool, into buf: inout [UInt8]) {
        writeInt(&buf, lower(value))
    }
}

private struct FfiConverterString: FfiConverter {
    typealias SwiftType = String
    typealias FfiType = RustBuffer

    public static func lift(_ value: RustBuffer) throws -> String {
        defer {
            value.deallocate()
        }
        if value.data == nil {
            return String()
        }
        let bytes = UnsafeBufferPointer<UInt8>(start: value.data!, count: Int(value.len))
        return String(bytes: bytes, encoding: String.Encoding.utf8)!
    }

    public static func lower(_ value: String) -> RustBuffer {
        return value.utf8CString.withUnsafeBufferPointer { ptr in
            // The swift string gives us int8_t, we want uint8_t.
            ptr.withMemoryRebound(to: UInt8.self) { ptr in
                // The swift string gives us a trailing null byte, we don't want it.
                let buf = UnsafeBufferPointer(rebasing: ptr.prefix(upTo: ptr.count - 1))
                return RustBuffer.from(buf)
            }
        }
    }

    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> String {
        let len: Int32 = try readInt(&buf)
        return String(bytes: try readBytes(&buf, count: Int(len)), encoding: String.Encoding.utf8)!
    }

    public static func write(_ value: String, into buf: inout [UInt8]) {
        let len = Int32(value.utf8.count)
        writeInt(&buf, len)
        writeBytes(&buf, value.utf8)
    }
}

public protocol BuilderProtocol {
    func setEntropySeedPath(seedPath: String)
    func setEntropySeedBytes(seedBytes: [UInt8]) throws
    func setEntropyBip39Mnemonic(mnemonic: Mnemonic, passphrase: String?)
    func setEsploraServer(esploraServerUrl: String)
    func setGossipSourceP2p()
    func setGossipSourceRgs(rgsServerUrl: String)
    func setStorageDirPath(storageDirPath: String)
    func setNetwork(network: Network)
    func setListeningAddress(listeningAddress: NetAddress)
    func build() throws -> LdkNode
}

public class Builder: BuilderProtocol {
    fileprivate let pointer: UnsafeMutableRawPointer

    // TODO: We'd like this to be `private` but for Swifty reasons,
    // we can't implement `FfiConverter` without making this `required` and we can't
    // make it `required` without making it `public`.
    required init(unsafeFromRawPointer pointer: UnsafeMutableRawPointer) {
        self.pointer = pointer
    }

    public convenience init() {
        self.init(unsafeFromRawPointer: try!

            rustCall {
                ldk_node_3490_Builder_new($0)
            })
    }

    deinit {
        try! rustCall { ffi_ldk_node_3490_Builder_object_free(pointer, $0) }
    }

    public static func fromConfig(config: Config) -> Builder {
        return Builder(unsafeFromRawPointer: try!

            rustCall {
                ldk_node_3490_Builder_from_config(
                    FfiConverterTypeConfig.lower(config), $0
                )
            })
    }

    public func setEntropySeedPath(seedPath: String) {
        try!
            rustCall {
                ldk_node_3490_Builder_set_entropy_seed_path(self.pointer,
                                                            FfiConverterString.lower(seedPath), $0)
            }
    }

    public func setEntropySeedBytes(seedBytes: [UInt8]) throws {
        try
            rustCallWithError(FfiConverterTypeBuildError.self) {
                ldk_node_3490_Builder_set_entropy_seed_bytes(self.pointer,
                                                             FfiConverterSequenceUInt8.lower(seedBytes), $0)
            }
    }

    public func setEntropyBip39Mnemonic(mnemonic: Mnemonic, passphrase: String?) {
        try!
            rustCall {
                ldk_node_3490_Builder_set_entropy_bip39_mnemonic(self.pointer,
                                                                 FfiConverterTypeMnemonic.lower(mnemonic),
                                                                 FfiConverterOptionString.lower(passphrase), $0)
            }
    }

    public func setEsploraServer(esploraServerUrl: String) {
        try!
            rustCall {
                ldk_node_3490_Builder_set_esplora_server(self.pointer,
                                                         FfiConverterString.lower(esploraServerUrl), $0)
            }
    }

    public func setGossipSourceP2p() {
        try!
            rustCall {
                ldk_node_3490_Builder_set_gossip_source_p2p(self.pointer, $0)
            }
    }

    public func setGossipSourceRgs(rgsServerUrl: String) {
        try!
            rustCall {
                ldk_node_3490_Builder_set_gossip_source_rgs(self.pointer,
                                                            FfiConverterString.lower(rgsServerUrl), $0)
            }
    }

    public func setStorageDirPath(storageDirPath: String) {
        try!
            rustCall {
                ldk_node_3490_Builder_set_storage_dir_path(self.pointer,
                                                           FfiConverterString.lower(storageDirPath), $0)
            }
    }

    public func setNetwork(network: Network) {
        try!
            rustCall {
                ldk_node_3490_Builder_set_network(self.pointer,
                                                  FfiConverterTypeNetwork.lower(network), $0)
            }
    }

    public func setListeningAddress(listeningAddress: NetAddress) {
        try!
            rustCall {
                ldk_node_3490_Builder_set_listening_address(self.pointer,
                                                            FfiConverterTypeNetAddress.lower(listeningAddress), $0)
            }
    }

    public func build() throws -> LdkNode {
        return try FfiConverterTypeLdkNode.lift(
            try
                rustCallWithError(FfiConverterTypeBuildError.self) {
                    ldk_node_3490_Builder_build(self.pointer, $0)
                }
        )
    }
}

public struct FfiConverterTypeBuilder: FfiConverter {
    typealias FfiType = UnsafeMutableRawPointer
    typealias SwiftType = Builder

    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> Builder {
        let v: UInt64 = try readInt(&buf)
        // The Rust code won't compile if a pointer won't fit in a UInt64.
        // We have to go via `UInt` because that's the thing that's the size of a pointer.
        let ptr = UnsafeMutableRawPointer(bitPattern: UInt(truncatingIfNeeded: v))
        if ptr == nil {
            throw UniffiInternalError.unexpectedNullPointer
        }
        return try lift(ptr!)
    }

    public static func write(_ value: Builder, into buf: inout [UInt8]) {
        // This fiddling is because `Int` is the thing that's the same size as a pointer.
        // The Rust code won't compile if a pointer won't fit in a `UInt64`.
        writeInt(&buf, UInt64(bitPattern: Int64(Int(bitPattern: lower(value)))))
    }

    public static func lift(_ pointer: UnsafeMutableRawPointer) throws -> Builder {
        return Builder(unsafeFromRawPointer: pointer)
    }

    public static func lower(_ value: Builder) -> UnsafeMutableRawPointer {
        return value.pointer
    }
}

public protocol LDKNodeProtocol {
    func start() throws
    func stop() throws
    func nextEvent() -> Event?
    func waitNextEvent() -> Event
    func eventHandled()
    func nodeId() -> PublicKey
    func listeningAddress() -> NetAddress?
    func newOnchainAddress() throws -> Address
    func sendToOnchainAddress(address: Address, amountMsat: UInt64) throws -> Txid
    func sendAllToOnchainAddress(address: Address) throws -> Txid
    func spendableOnchainBalanceSats() throws -> UInt64
    func totalOnchainBalanceSats() throws -> UInt64
    func connect(nodeId: PublicKey, address: NetAddress, persist: Bool) throws
    func disconnect(nodeId: PublicKey) throws
    func connectOpenChannel(nodeId: PublicKey, address: NetAddress, channelAmountSats: UInt64, pushToCounterpartyMsat: UInt64?, channelConfig: ChannelConfig?, announceChannel: Bool) throws
    func closeChannel(channelId: ChannelId, counterpartyNodeId: PublicKey) throws
    func updateChannelConfig(channelId: ChannelId, counterpartyNodeId: PublicKey, channelConfig: ChannelConfig) throws
    func syncWallets() throws
    func sendPayment(invoice: Invoice) throws -> PaymentHash
    func sendPaymentUsingAmount(invoice: Invoice, amountMsat: UInt64) throws -> PaymentHash
    func sendSpontaneousPayment(amountMsat: UInt64, nodeId: PublicKey) throws -> PaymentHash
    func receivePayment(amountMsat: UInt64, description: String, expirySecs: UInt32) throws -> Invoice
    func receiveVariableAmountPayment(description: String, expirySecs: UInt32) throws -> Invoice
    func payment(paymentHash: PaymentHash) -> PaymentDetails?
    func removePayment(paymentHash: PaymentHash) throws -> Bool
    func listPayments() -> [PaymentDetails]
    func listPeers() -> [PeerDetails]
    func listChannels() -> [ChannelDetails]
    func signMessage(msg: [UInt8]) throws -> String
    func verifySignature(msg: [UInt8], sig: String, pkey: PublicKey) -> Bool
}

public class LdkNode: LDKNodeProtocol {
    fileprivate let pointer: UnsafeMutableRawPointer

    // TODO: We'd like this to be `private` but for Swifty reasons,
    // we can't implement `FfiConverter` without making this `required` and we can't
    // make it `required` without making it `public`.
    required init(unsafeFromRawPointer pointer: UnsafeMutableRawPointer) {
        self.pointer = pointer
    }

    deinit {
        try! rustCall { ffi_ldk_node_3490_LDKNode_object_free(pointer, $0) }
    }

    public func start() throws {
        try
            rustCallWithError(FfiConverterTypeNodeError.self) {
                ldk_node_3490_LDKNode_start(self.pointer, $0)
            }
    }

    public func stop() throws {
        try
            rustCallWithError(FfiConverterTypeNodeError.self) {
                ldk_node_3490_LDKNode_stop(self.pointer, $0)
            }
    }

    public func nextEvent() -> Event? {
        return try! FfiConverterOptionTypeEvent.lift(
            try!
                rustCall {
                    ldk_node_3490_LDKNode_next_event(self.pointer, $0)
                }
        )
    }

    public func waitNextEvent() -> Event {
        return try! FfiConverterTypeEvent.lift(
            try!
                rustCall {
                    ldk_node_3490_LDKNode_wait_next_event(self.pointer, $0)
                }
        )
    }

    public func eventHandled() {
        try!
            rustCall {
                ldk_node_3490_LDKNode_event_handled(self.pointer, $0)
            }
    }

    public func nodeId() -> PublicKey {
        return try! FfiConverterTypePublicKey.lift(
            try!
                rustCall {
                    ldk_node_3490_LDKNode_node_id(self.pointer, $0)
                }
        )
    }

    public func listeningAddress() -> NetAddress? {
        return try! FfiConverterOptionTypeNetAddress.lift(
            try!
                rustCall {
                    ldk_node_3490_LDKNode_listening_address(self.pointer, $0)
                }
        )
    }

    public func newOnchainAddress() throws -> Address {
        return try FfiConverterTypeAddress.lift(
            try
                rustCallWithError(FfiConverterTypeNodeError.self) {
                    ldk_node_3490_LDKNode_new_onchain_address(self.pointer, $0)
                }
        )
    }

    public func sendToOnchainAddress(address: Address, amountMsat: UInt64) throws -> Txid {
        return try FfiConverterTypeTxid.lift(
            try
                rustCallWithError(FfiConverterTypeNodeError.self) {
                    ldk_node_3490_LDKNode_send_to_onchain_address(self.pointer,
                                                                  FfiConverterTypeAddress.lower(address),
                                                                  FfiConverterUInt64.lower(amountMsat), $0)
                }
        )
    }

    public func sendAllToOnchainAddress(address: Address) throws -> Txid {
        return try FfiConverterTypeTxid.lift(
            try
                rustCallWithError(FfiConverterTypeNodeError.self) {
                    ldk_node_3490_LDKNode_send_all_to_onchain_address(self.pointer,
                                                                      FfiConverterTypeAddress.lower(address), $0)
                }
        )
    }

    public func spendableOnchainBalanceSats() throws -> UInt64 {
        return try FfiConverterUInt64.lift(
            try
                rustCallWithError(FfiConverterTypeNodeError.self) {
                    ldk_node_3490_LDKNode_spendable_onchain_balance_sats(self.pointer, $0)
                }
        )
    }

    public func totalOnchainBalanceSats() throws -> UInt64 {
        return try FfiConverterUInt64.lift(
            try
                rustCallWithError(FfiConverterTypeNodeError.self) {
                    ldk_node_3490_LDKNode_total_onchain_balance_sats(self.pointer, $0)
                }
        )
    }

    public func connect(nodeId: PublicKey, address: NetAddress, persist: Bool) throws {
        try
            rustCallWithError(FfiConverterTypeNodeError.self) {
                ldk_node_3490_LDKNode_connect(self.pointer,
                                              FfiConverterTypePublicKey.lower(nodeId),
                                              FfiConverterTypeNetAddress.lower(address),
                                              FfiConverterBool.lower(persist), $0)
            }
    }

    public func disconnect(nodeId: PublicKey) throws {
        try
            rustCallWithError(FfiConverterTypeNodeError.self) {
                ldk_node_3490_LDKNode_disconnect(self.pointer,
                                                 FfiConverterTypePublicKey.lower(nodeId), $0)
            }
    }

    public func connectOpenChannel(nodeId: PublicKey, address: NetAddress, channelAmountSats: UInt64, pushToCounterpartyMsat: UInt64?, channelConfig: ChannelConfig?, announceChannel: Bool) throws {
        try
            rustCallWithError(FfiConverterTypeNodeError.self) {
                ldk_node_3490_LDKNode_connect_open_channel(self.pointer,
                                                           FfiConverterTypePublicKey.lower(nodeId),
                                                           FfiConverterTypeNetAddress.lower(address),
                                                           FfiConverterUInt64.lower(channelAmountSats),
                                                           FfiConverterOptionUInt64.lower(pushToCounterpartyMsat),
                                                           FfiConverterOptionTypeChannelConfig.lower(channelConfig),
                                                           FfiConverterBool.lower(announceChannel), $0)
            }
    }

    public func closeChannel(channelId: ChannelId, counterpartyNodeId: PublicKey) throws {
        try
            rustCallWithError(FfiConverterTypeNodeError.self) {
                ldk_node_3490_LDKNode_close_channel(self.pointer,
                                                    FfiConverterTypeChannelId.lower(channelId),
                                                    FfiConverterTypePublicKey.lower(counterpartyNodeId), $0)
            }
    }

    public func updateChannelConfig(channelId: ChannelId, counterpartyNodeId: PublicKey, channelConfig: ChannelConfig) throws {
        try
            rustCallWithError(FfiConverterTypeNodeError.self) {
                ldk_node_3490_LDKNode_update_channel_config(self.pointer,
                                                            FfiConverterTypeChannelId.lower(channelId),
                                                            FfiConverterTypePublicKey.lower(counterpartyNodeId),
                                                            FfiConverterTypeChannelConfig.lower(channelConfig), $0)
            }
    }

    public func syncWallets() throws {
        try
            rustCallWithError(FfiConverterTypeNodeError.self) {
                ldk_node_3490_LDKNode_sync_wallets(self.pointer, $0)
            }
    }

    public func sendPayment(invoice: Invoice) throws -> PaymentHash {
        return try FfiConverterTypePaymentHash.lift(
            try
                rustCallWithError(FfiConverterTypeNodeError.self) {
                    ldk_node_3490_LDKNode_send_payment(self.pointer,
                                                       FfiConverterTypeInvoice.lower(invoice), $0)
                }
        )
    }

    public func sendPaymentUsingAmount(invoice: Invoice, amountMsat: UInt64) throws -> PaymentHash {
        return try FfiConverterTypePaymentHash.lift(
            try
                rustCallWithError(FfiConverterTypeNodeError.self) {
                    ldk_node_3490_LDKNode_send_payment_using_amount(self.pointer,
                                                                    FfiConverterTypeInvoice.lower(invoice),
                                                                    FfiConverterUInt64.lower(amountMsat), $0)
                }
        )
    }

    public func sendSpontaneousPayment(amountMsat: UInt64, nodeId: PublicKey) throws -> PaymentHash {
        return try FfiConverterTypePaymentHash.lift(
            try
                rustCallWithError(FfiConverterTypeNodeError.self) {
                    ldk_node_3490_LDKNode_send_spontaneous_payment(self.pointer,
                                                                   FfiConverterUInt64.lower(amountMsat),
                                                                   FfiConverterTypePublicKey.lower(nodeId), $0)
                }
        )
    }

    public func receivePayment(amountMsat: UInt64, description: String, expirySecs: UInt32) throws -> Invoice {
        return try FfiConverterTypeInvoice.lift(
            try
                rustCallWithError(FfiConverterTypeNodeError.self) {
                    ldk_node_3490_LDKNode_receive_payment(self.pointer,
                                                          FfiConverterUInt64.lower(amountMsat),
                                                          FfiConverterString.lower(description),
                                                          FfiConverterUInt32.lower(expirySecs), $0)
                }
        )
    }

    public func receiveVariableAmountPayment(description: String, expirySecs: UInt32) throws -> Invoice {
        return try FfiConverterTypeInvoice.lift(
            try
                rustCallWithError(FfiConverterTypeNodeError.self) {
                    ldk_node_3490_LDKNode_receive_variable_amount_payment(self.pointer,
                                                                          FfiConverterString.lower(description),
                                                                          FfiConverterUInt32.lower(expirySecs), $0)
                }
        )
    }

    public func payment(paymentHash: PaymentHash) -> PaymentDetails? {
        return try! FfiConverterOptionTypePaymentDetails.lift(
            try!
                rustCall {
                    ldk_node_3490_LDKNode_payment(self.pointer,
                                                  FfiConverterTypePaymentHash.lower(paymentHash), $0)
                }
        )
    }

    public func removePayment(paymentHash: PaymentHash) throws -> Bool {
        return try FfiConverterBool.lift(
            try
                rustCallWithError(FfiConverterTypeNodeError.self) {
                    ldk_node_3490_LDKNode_remove_payment(self.pointer,
                                                         FfiConverterTypePaymentHash.lower(paymentHash), $0)
                }
        )
    }

    public func listPayments() -> [PaymentDetails] {
        return try! FfiConverterSequenceTypePaymentDetails.lift(
            try!
                rustCall {
                    ldk_node_3490_LDKNode_list_payments(self.pointer, $0)
                }
        )
    }

    public func listPeers() -> [PeerDetails] {
        return try! FfiConverterSequenceTypePeerDetails.lift(
            try!
                rustCall {
                    ldk_node_3490_LDKNode_list_peers(self.pointer, $0)
                }
        )
    }

    public func listChannels() -> [ChannelDetails] {
        return try! FfiConverterSequenceTypeChannelDetails.lift(
            try!
                rustCall {
                    ldk_node_3490_LDKNode_list_channels(self.pointer, $0)
                }
        )
    }

    public func signMessage(msg: [UInt8]) throws -> String {
        return try FfiConverterString.lift(
            try
                rustCallWithError(FfiConverterTypeNodeError.self) {
                    ldk_node_3490_LDKNode_sign_message(self.pointer,
                                                       FfiConverterSequenceUInt8.lower(msg), $0)
                }
        )
    }

    public func verifySignature(msg: [UInt8], sig: String, pkey: PublicKey) -> Bool {
        return try! FfiConverterBool.lift(
            try!
                rustCall {
                    ldk_node_3490_LDKNode_verify_signature(self.pointer,
                                                           FfiConverterSequenceUInt8.lower(msg),
                                                           FfiConverterString.lower(sig),
                                                           FfiConverterTypePublicKey.lower(pkey), $0)
                }
        )
    }
}

public struct FfiConverterTypeLdkNode: FfiConverter {
    typealias FfiType = UnsafeMutableRawPointer
    typealias SwiftType = LdkNode

    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> LdkNode {
        let v: UInt64 = try readInt(&buf)
        // The Rust code won't compile if a pointer won't fit in a UInt64.
        // We have to go via `UInt` because that's the thing that's the size of a pointer.
        let ptr = UnsafeMutableRawPointer(bitPattern: UInt(truncatingIfNeeded: v))
        if ptr == nil {
            throw UniffiInternalError.unexpectedNullPointer
        }
        return try lift(ptr!)
    }

    public static func write(_ value: LdkNode, into buf: inout [UInt8]) {
        // This fiddling is because `Int` is the thing that's the same size as a pointer.
        // The Rust code won't compile if a pointer won't fit in a `UInt64`.
        writeInt(&buf, UInt64(bitPattern: Int64(Int(bitPattern: lower(value)))))
    }

    public static func lift(_ pointer: UnsafeMutableRawPointer) throws -> LdkNode {
        return LdkNode(unsafeFromRawPointer: pointer)
    }

    public static func lower(_ value: LdkNode) -> UnsafeMutableRawPointer {
        return value.pointer
    }
}

public struct ChannelConfig {
    public var forwardingFeeProportionalMillionths: UInt32
    public var forwardingFeeBaseMsat: UInt32
    public var cltvExpiryDelta: UInt16
    public var maxDustHtlcExposureMsat: UInt64
    public var forceCloseAvoidanceMaxFeeSatoshis: UInt64

    // Default memberwise initializers are never public by default, so we
    // declare one manually.
    public init(forwardingFeeProportionalMillionths: UInt32, forwardingFeeBaseMsat: UInt32, cltvExpiryDelta: UInt16, maxDustHtlcExposureMsat: UInt64, forceCloseAvoidanceMaxFeeSatoshis: UInt64) {
        self.forwardingFeeProportionalMillionths = forwardingFeeProportionalMillionths
        self.forwardingFeeBaseMsat = forwardingFeeBaseMsat
        self.cltvExpiryDelta = cltvExpiryDelta
        self.maxDustHtlcExposureMsat = maxDustHtlcExposureMsat
        self.forceCloseAvoidanceMaxFeeSatoshis = forceCloseAvoidanceMaxFeeSatoshis
    }
}

extension ChannelConfig: Equatable, Hashable {
    public static func == (lhs: ChannelConfig, rhs: ChannelConfig) -> Bool {
        if lhs.forwardingFeeProportionalMillionths != rhs.forwardingFeeProportionalMillionths {
            return false
        }
        if lhs.forwardingFeeBaseMsat != rhs.forwardingFeeBaseMsat {
            return false
        }
        if lhs.cltvExpiryDelta != rhs.cltvExpiryDelta {
            return false
        }
        if lhs.maxDustHtlcExposureMsat != rhs.maxDustHtlcExposureMsat {
            return false
        }
        if lhs.forceCloseAvoidanceMaxFeeSatoshis != rhs.forceCloseAvoidanceMaxFeeSatoshis {
            return false
        }
        return true
    }

    public func hash(into hasher: inout Hasher) {
        hasher.combine(forwardingFeeProportionalMillionths)
        hasher.combine(forwardingFeeBaseMsat)
        hasher.combine(cltvExpiryDelta)
        hasher.combine(maxDustHtlcExposureMsat)
        hasher.combine(forceCloseAvoidanceMaxFeeSatoshis)
    }
}

public struct FfiConverterTypeChannelConfig: FfiConverterRustBuffer {
    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> ChannelConfig {
        return try ChannelConfig(
            forwardingFeeProportionalMillionths: FfiConverterUInt32.read(from: &buf),
            forwardingFeeBaseMsat: FfiConverterUInt32.read(from: &buf),
            cltvExpiryDelta: FfiConverterUInt16.read(from: &buf),
            maxDustHtlcExposureMsat: FfiConverterUInt64.read(from: &buf),
            forceCloseAvoidanceMaxFeeSatoshis: FfiConverterUInt64.read(from: &buf)
        )
    }

    public static func write(_ value: ChannelConfig, into buf: inout [UInt8]) {
        FfiConverterUInt32.write(value.forwardingFeeProportionalMillionths, into: &buf)
        FfiConverterUInt32.write(value.forwardingFeeBaseMsat, into: &buf)
        FfiConverterUInt16.write(value.cltvExpiryDelta, into: &buf)
        FfiConverterUInt64.write(value.maxDustHtlcExposureMsat, into: &buf)
        FfiConverterUInt64.write(value.forceCloseAvoidanceMaxFeeSatoshis, into: &buf)
    }
}

public func FfiConverterTypeChannelConfig_lift(_ buf: RustBuffer) throws -> ChannelConfig {
    return try FfiConverterTypeChannelConfig.lift(buf)
}

public func FfiConverterTypeChannelConfig_lower(_ value: ChannelConfig) -> RustBuffer {
    return FfiConverterTypeChannelConfig.lower(value)
}

public struct ChannelDetails {
    public var channelId: ChannelId
    public var counterpartyNodeId: PublicKey
    public var fundingTxo: OutPoint?
    public var channelValueSats: UInt64
    public var unspendablePunishmentReserve: UInt64?
    public var userChannelId: UserChannelId
    public var feerateSatPer1000Weight: UInt32
    public var balanceMsat: UInt64
    public var outboundCapacityMsat: UInt64
    public var inboundCapacityMsat: UInt64
    public var confirmationsRequired: UInt32?
    public var confirmations: UInt32?
    public var isOutbound: Bool
    public var isChannelReady: Bool
    public var isUsable: Bool
    public var isPublic: Bool
    public var cltvExpiryDelta: UInt16?

    // Default memberwise initializers are never public by default, so we
    // declare one manually.
    public init(channelId: ChannelId, counterpartyNodeId: PublicKey, fundingTxo: OutPoint?, channelValueSats: UInt64, unspendablePunishmentReserve: UInt64?, userChannelId: UserChannelId, feerateSatPer1000Weight: UInt32, balanceMsat: UInt64, outboundCapacityMsat: UInt64, inboundCapacityMsat: UInt64, confirmationsRequired: UInt32?, confirmations: UInt32?, isOutbound: Bool, isChannelReady: Bool, isUsable: Bool, isPublic: Bool, cltvExpiryDelta: UInt16?) {
        self.channelId = channelId
        self.counterpartyNodeId = counterpartyNodeId
        self.fundingTxo = fundingTxo
        self.channelValueSats = channelValueSats
        self.unspendablePunishmentReserve = unspendablePunishmentReserve
        self.userChannelId = userChannelId
        self.feerateSatPer1000Weight = feerateSatPer1000Weight
        self.balanceMsat = balanceMsat
        self.outboundCapacityMsat = outboundCapacityMsat
        self.inboundCapacityMsat = inboundCapacityMsat
        self.confirmationsRequired = confirmationsRequired
        self.confirmations = confirmations
        self.isOutbound = isOutbound
        self.isChannelReady = isChannelReady
        self.isUsable = isUsable
        self.isPublic = isPublic
        self.cltvExpiryDelta = cltvExpiryDelta
    }
}

extension ChannelDetails: Equatable, Hashable {
    public static func == (lhs: ChannelDetails, rhs: ChannelDetails) -> Bool {
        if lhs.channelId != rhs.channelId {
            return false
        }
        if lhs.counterpartyNodeId != rhs.counterpartyNodeId {
            return false
        }
        if lhs.fundingTxo != rhs.fundingTxo {
            return false
        }
        if lhs.channelValueSats != rhs.channelValueSats {
            return false
        }
        if lhs.unspendablePunishmentReserve != rhs.unspendablePunishmentReserve {
            return false
        }
        if lhs.userChannelId != rhs.userChannelId {
            return false
        }
        if lhs.feerateSatPer1000Weight != rhs.feerateSatPer1000Weight {
            return false
        }
        if lhs.balanceMsat != rhs.balanceMsat {
            return false
        }
        if lhs.outboundCapacityMsat != rhs.outboundCapacityMsat {
            return false
        }
        if lhs.inboundCapacityMsat != rhs.inboundCapacityMsat {
            return false
        }
        if lhs.confirmationsRequired != rhs.confirmationsRequired {
            return false
        }
        if lhs.confirmations != rhs.confirmations {
            return false
        }
        if lhs.isOutbound != rhs.isOutbound {
            return false
        }
        if lhs.isChannelReady != rhs.isChannelReady {
            return false
        }
        if lhs.isUsable != rhs.isUsable {
            return false
        }
        if lhs.isPublic != rhs.isPublic {
            return false
        }
        if lhs.cltvExpiryDelta != rhs.cltvExpiryDelta {
            return false
        }
        return true
    }

    public func hash(into hasher: inout Hasher) {
        hasher.combine(channelId)
        hasher.combine(counterpartyNodeId)
        hasher.combine(fundingTxo)
        hasher.combine(channelValueSats)
        hasher.combine(unspendablePunishmentReserve)
        hasher.combine(userChannelId)
        hasher.combine(feerateSatPer1000Weight)
        hasher.combine(balanceMsat)
        hasher.combine(outboundCapacityMsat)
        hasher.combine(inboundCapacityMsat)
        hasher.combine(confirmationsRequired)
        hasher.combine(confirmations)
        hasher.combine(isOutbound)
        hasher.combine(isChannelReady)
        hasher.combine(isUsable)
        hasher.combine(isPublic)
        hasher.combine(cltvExpiryDelta)
    }
}

public struct FfiConverterTypeChannelDetails: FfiConverterRustBuffer {
    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> ChannelDetails {
        return try ChannelDetails(
            channelId: FfiConverterTypeChannelId.read(from: &buf),
            counterpartyNodeId: FfiConverterTypePublicKey.read(from: &buf),
            fundingTxo: FfiConverterOptionTypeOutPoint.read(from: &buf),
            channelValueSats: FfiConverterUInt64.read(from: &buf),
            unspendablePunishmentReserve: FfiConverterOptionUInt64.read(from: &buf),
            userChannelId: FfiConverterTypeUserChannelId.read(from: &buf),
            feerateSatPer1000Weight: FfiConverterUInt32.read(from: &buf),
            balanceMsat: FfiConverterUInt64.read(from: &buf),
            outboundCapacityMsat: FfiConverterUInt64.read(from: &buf),
            inboundCapacityMsat: FfiConverterUInt64.read(from: &buf),
            confirmationsRequired: FfiConverterOptionUInt32.read(from: &buf),
            confirmations: FfiConverterOptionUInt32.read(from: &buf),
            isOutbound: FfiConverterBool.read(from: &buf),
            isChannelReady: FfiConverterBool.read(from: &buf),
            isUsable: FfiConverterBool.read(from: &buf),
            isPublic: FfiConverterBool.read(from: &buf),
            cltvExpiryDelta: FfiConverterOptionUInt16.read(from: &buf)
        )
    }

    public static func write(_ value: ChannelDetails, into buf: inout [UInt8]) {
        FfiConverterTypeChannelId.write(value.channelId, into: &buf)
        FfiConverterTypePublicKey.write(value.counterpartyNodeId, into: &buf)
        FfiConverterOptionTypeOutPoint.write(value.fundingTxo, into: &buf)
        FfiConverterUInt64.write(value.channelValueSats, into: &buf)
        FfiConverterOptionUInt64.write(value.unspendablePunishmentReserve, into: &buf)
        FfiConverterTypeUserChannelId.write(value.userChannelId, into: &buf)
        FfiConverterUInt32.write(value.feerateSatPer1000Weight, into: &buf)
        FfiConverterUInt64.write(value.balanceMsat, into: &buf)
        FfiConverterUInt64.write(value.outboundCapacityMsat, into: &buf)
        FfiConverterUInt64.write(value.inboundCapacityMsat, into: &buf)
        FfiConverterOptionUInt32.write(value.confirmationsRequired, into: &buf)
        FfiConverterOptionUInt32.write(value.confirmations, into: &buf)
        FfiConverterBool.write(value.isOutbound, into: &buf)
        FfiConverterBool.write(value.isChannelReady, into: &buf)
        FfiConverterBool.write(value.isUsable, into: &buf)
        FfiConverterBool.write(value.isPublic, into: &buf)
        FfiConverterOptionUInt16.write(value.cltvExpiryDelta, into: &buf)
    }
}

public func FfiConverterTypeChannelDetails_lift(_ buf: RustBuffer) throws -> ChannelDetails {
    return try FfiConverterTypeChannelDetails.lift(buf)
}

public func FfiConverterTypeChannelDetails_lower(_ value: ChannelDetails) -> RustBuffer {
    return FfiConverterTypeChannelDetails.lower(value)
}

public struct Config {
    public var storageDirPath: String
    public var network: Network
    public var listeningAddress: NetAddress?
    public var defaultCltvExpiryDelta: UInt32
    public var onchainWalletSyncIntervalSecs: UInt64
    public var walletSyncIntervalSecs: UInt64
    public var feeRateCacheUpdateIntervalSecs: UInt64
    public var logLevel: LogLevel
    public var trustedPeers0conf: [PublicKey]

    // Default memberwise initializers are never public by default, so we
    // declare one manually.
    public init(storageDirPath: String = "/tmp/ldk_node/", network: Network = .bitcoin, listeningAddress: NetAddress? = nil, defaultCltvExpiryDelta: UInt32 = UInt32(144), onchainWalletSyncIntervalSecs: UInt64 = UInt64(80), walletSyncIntervalSecs: UInt64 = UInt64(30), feeRateCacheUpdateIntervalSecs: UInt64 = UInt64(600), logLevel: LogLevel = .debug, trustedPeers0conf: [PublicKey] = []) {
        self.storageDirPath = storageDirPath
        self.network = network
        self.listeningAddress = listeningAddress
        self.defaultCltvExpiryDelta = defaultCltvExpiryDelta
        self.onchainWalletSyncIntervalSecs = onchainWalletSyncIntervalSecs
        self.walletSyncIntervalSecs = walletSyncIntervalSecs
        self.feeRateCacheUpdateIntervalSecs = feeRateCacheUpdateIntervalSecs
        self.logLevel = logLevel
        self.trustedPeers0conf = trustedPeers0conf
    }
}

extension Config: Equatable, Hashable {
    public static func == (lhs: Config, rhs: Config) -> Bool {
        if lhs.storageDirPath != rhs.storageDirPath {
            return false
        }
        if lhs.network != rhs.network {
            return false
        }
        if lhs.listeningAddress != rhs.listeningAddress {
            return false
        }
        if lhs.defaultCltvExpiryDelta != rhs.defaultCltvExpiryDelta {
            return false
        }
        if lhs.onchainWalletSyncIntervalSecs != rhs.onchainWalletSyncIntervalSecs {
            return false
        }
        if lhs.walletSyncIntervalSecs != rhs.walletSyncIntervalSecs {
            return false
        }
        if lhs.feeRateCacheUpdateIntervalSecs != rhs.feeRateCacheUpdateIntervalSecs {
            return false
        }
        if lhs.logLevel != rhs.logLevel {
            return false
        }
        if lhs.trustedPeers0conf != rhs.trustedPeers0conf {
            return false
        }
        return true
    }

    public func hash(into hasher: inout Hasher) {
        hasher.combine(storageDirPath)
        hasher.combine(network)
        hasher.combine(listeningAddress)
        hasher.combine(defaultCltvExpiryDelta)
        hasher.combine(onchainWalletSyncIntervalSecs)
        hasher.combine(walletSyncIntervalSecs)
        hasher.combine(feeRateCacheUpdateIntervalSecs)
        hasher.combine(logLevel)
        hasher.combine(trustedPeers0conf)
    }
}

public struct FfiConverterTypeConfig: FfiConverterRustBuffer {
    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> Config {
        return try Config(
            storageDirPath: FfiConverterString.read(from: &buf),
            network: FfiConverterTypeNetwork.read(from: &buf),
            listeningAddress: FfiConverterOptionTypeNetAddress.read(from: &buf),
            defaultCltvExpiryDelta: FfiConverterUInt32.read(from: &buf),
            onchainWalletSyncIntervalSecs: FfiConverterUInt64.read(from: &buf),
            walletSyncIntervalSecs: FfiConverterUInt64.read(from: &buf),
            feeRateCacheUpdateIntervalSecs: FfiConverterUInt64.read(from: &buf),
            logLevel: FfiConverterTypeLogLevel.read(from: &buf),
            trustedPeers0conf: FfiConverterSequenceTypePublicKey.read(from: &buf)
        )
    }

    public static func write(_ value: Config, into buf: inout [UInt8]) {
        FfiConverterString.write(value.storageDirPath, into: &buf)
        FfiConverterTypeNetwork.write(value.network, into: &buf)
        FfiConverterOptionTypeNetAddress.write(value.listeningAddress, into: &buf)
        FfiConverterUInt32.write(value.defaultCltvExpiryDelta, into: &buf)
        FfiConverterUInt64.write(value.onchainWalletSyncIntervalSecs, into: &buf)
        FfiConverterUInt64.write(value.walletSyncIntervalSecs, into: &buf)
        FfiConverterUInt64.write(value.feeRateCacheUpdateIntervalSecs, into: &buf)
        FfiConverterTypeLogLevel.write(value.logLevel, into: &buf)
        FfiConverterSequenceTypePublicKey.write(value.trustedPeers0conf, into: &buf)
    }
}

public func FfiConverterTypeConfig_lift(_ buf: RustBuffer) throws -> Config {
    return try FfiConverterTypeConfig.lift(buf)
}

public func FfiConverterTypeConfig_lower(_ value: Config) -> RustBuffer {
    return FfiConverterTypeConfig.lower(value)
}

public struct OutPoint {
    public var txid: Txid
    public var vout: UInt32

    // Default memberwise initializers are never public by default, so we
    // declare one manually.
    public init(txid: Txid, vout: UInt32) {
        self.txid = txid
        self.vout = vout
    }
}

extension OutPoint: Equatable, Hashable {
    public static func == (lhs: OutPoint, rhs: OutPoint) -> Bool {
        if lhs.txid != rhs.txid {
            return false
        }
        if lhs.vout != rhs.vout {
            return false
        }
        return true
    }

    public func hash(into hasher: inout Hasher) {
        hasher.combine(txid)
        hasher.combine(vout)
    }
}

public struct FfiConverterTypeOutPoint: FfiConverterRustBuffer {
    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> OutPoint {
        return try OutPoint(
            txid: FfiConverterTypeTxid.read(from: &buf),
            vout: FfiConverterUInt32.read(from: &buf)
        )
    }

    public static func write(_ value: OutPoint, into buf: inout [UInt8]) {
        FfiConverterTypeTxid.write(value.txid, into: &buf)
        FfiConverterUInt32.write(value.vout, into: &buf)
    }
}

public func FfiConverterTypeOutPoint_lift(_ buf: RustBuffer) throws -> OutPoint {
    return try FfiConverterTypeOutPoint.lift(buf)
}

public func FfiConverterTypeOutPoint_lower(_ value: OutPoint) -> RustBuffer {
    return FfiConverterTypeOutPoint.lower(value)
}

public struct PaymentDetails {
    public var hash: PaymentHash
    public var preimage: PaymentPreimage?
    public var secret: PaymentSecret?
    public var amountMsat: UInt64?
    public var direction: PaymentDirection
    public var status: PaymentStatus

    // Default memberwise initializers are never public by default, so we
    // declare one manually.
    public init(hash: PaymentHash, preimage: PaymentPreimage?, secret: PaymentSecret?, amountMsat: UInt64?, direction: PaymentDirection, status: PaymentStatus) {
        self.hash = hash
        self.preimage = preimage
        self.secret = secret
        self.amountMsat = amountMsat
        self.direction = direction
        self.status = status
    }
}

extension PaymentDetails: Equatable, Hashable {
    public static func == (lhs: PaymentDetails, rhs: PaymentDetails) -> Bool {
        if lhs.hash != rhs.hash {
            return false
        }
        if lhs.preimage != rhs.preimage {
            return false
        }
        if lhs.secret != rhs.secret {
            return false
        }
        if lhs.amountMsat != rhs.amountMsat {
            return false
        }
        if lhs.direction != rhs.direction {
            return false
        }
        if lhs.status != rhs.status {
            return false
        }
        return true
    }

    public func hash(into hasher: inout Hasher) {
        hasher.combine(hash)
        hasher.combine(preimage)
        hasher.combine(secret)
        hasher.combine(amountMsat)
        hasher.combine(direction)
        hasher.combine(status)
    }
}

public struct FfiConverterTypePaymentDetails: FfiConverterRustBuffer {
    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> PaymentDetails {
        return try PaymentDetails(
            hash: FfiConverterTypePaymentHash.read(from: &buf),
            preimage: FfiConverterOptionTypePaymentPreimage.read(from: &buf),
            secret: FfiConverterOptionTypePaymentSecret.read(from: &buf),
            amountMsat: FfiConverterOptionUInt64.read(from: &buf),
            direction: FfiConverterTypePaymentDirection.read(from: &buf),
            status: FfiConverterTypePaymentStatus.read(from: &buf)
        )
    }

    public static func write(_ value: PaymentDetails, into buf: inout [UInt8]) {
        FfiConverterTypePaymentHash.write(value.hash, into: &buf)
        FfiConverterOptionTypePaymentPreimage.write(value.preimage, into: &buf)
        FfiConverterOptionTypePaymentSecret.write(value.secret, into: &buf)
        FfiConverterOptionUInt64.write(value.amountMsat, into: &buf)
        FfiConverterTypePaymentDirection.write(value.direction, into: &buf)
        FfiConverterTypePaymentStatus.write(value.status, into: &buf)
    }
}

public func FfiConverterTypePaymentDetails_lift(_ buf: RustBuffer) throws -> PaymentDetails {
    return try FfiConverterTypePaymentDetails.lift(buf)
}

public func FfiConverterTypePaymentDetails_lower(_ value: PaymentDetails) -> RustBuffer {
    return FfiConverterTypePaymentDetails.lower(value)
}

public struct PeerDetails {
    public var nodeId: PublicKey
    public var address: NetAddress
    public var isPersisted: Bool
    public var isConnected: Bool

    // Default memberwise initializers are never public by default, so we
    // declare one manually.
    public init(nodeId: PublicKey, address: NetAddress, isPersisted: Bool, isConnected: Bool) {
        self.nodeId = nodeId
        self.address = address
        self.isPersisted = isPersisted
        self.isConnected = isConnected
    }
}

extension PeerDetails: Equatable, Hashable {
    public static func == (lhs: PeerDetails, rhs: PeerDetails) -> Bool {
        if lhs.nodeId != rhs.nodeId {
            return false
        }
        if lhs.address != rhs.address {
            return false
        }
        if lhs.isPersisted != rhs.isPersisted {
            return false
        }
        if lhs.isConnected != rhs.isConnected {
            return false
        }
        return true
    }

    public func hash(into hasher: inout Hasher) {
        hasher.combine(nodeId)
        hasher.combine(address)
        hasher.combine(isPersisted)
        hasher.combine(isConnected)
    }
}

public struct FfiConverterTypePeerDetails: FfiConverterRustBuffer {
    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> PeerDetails {
        return try PeerDetails(
            nodeId: FfiConverterTypePublicKey.read(from: &buf),
            address: FfiConverterTypeNetAddress.read(from: &buf),
            isPersisted: FfiConverterBool.read(from: &buf),
            isConnected: FfiConverterBool.read(from: &buf)
        )
    }

    public static func write(_ value: PeerDetails, into buf: inout [UInt8]) {
        FfiConverterTypePublicKey.write(value.nodeId, into: &buf)
        FfiConverterTypeNetAddress.write(value.address, into: &buf)
        FfiConverterBool.write(value.isPersisted, into: &buf)
        FfiConverterBool.write(value.isConnected, into: &buf)
    }
}

public func FfiConverterTypePeerDetails_lift(_ buf: RustBuffer) throws -> PeerDetails {
    return try FfiConverterTypePeerDetails.lift(buf)
}

public func FfiConverterTypePeerDetails_lower(_ value: PeerDetails) -> RustBuffer {
    return FfiConverterTypePeerDetails.lower(value)
}

// Note that we don't yet support `indirect` for enums.
// See https://github.com/mozilla/uniffi-rs/issues/396 for further discussion.
public enum Event {
    case paymentSuccessful(paymentHash: PaymentHash)
    case paymentFailed(paymentHash: PaymentHash)
    case paymentReceived(paymentHash: PaymentHash, amountMsat: UInt64)
    case channelPending(channelId: ChannelId, userChannelId: UserChannelId, formerTemporaryChannelId: ChannelId, counterpartyNodeId: PublicKey, fundingTxo: OutPoint)
    case channelReady(channelId: ChannelId, userChannelId: UserChannelId)
    case channelClosed(channelId: ChannelId, userChannelId: UserChannelId)
}

public struct FfiConverterTypeEvent: FfiConverterRustBuffer {
    typealias SwiftType = Event

    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> Event {
        let variant: Int32 = try readInt(&buf)
        switch variant {
        case 1: return .paymentSuccessful(
                paymentHash: try FfiConverterTypePaymentHash.read(from: &buf)
            )

        case 2: return .paymentFailed(
                paymentHash: try FfiConverterTypePaymentHash.read(from: &buf)
            )

        case 3: return .paymentReceived(
                paymentHash: try FfiConverterTypePaymentHash.read(from: &buf),
                amountMsat: try FfiConverterUInt64.read(from: &buf)
            )

        case 4: return .channelPending(
                channelId: try FfiConverterTypeChannelId.read(from: &buf),
                userChannelId: try FfiConverterTypeUserChannelId.read(from: &buf),
                formerTemporaryChannelId: try FfiConverterTypeChannelId.read(from: &buf),
                counterpartyNodeId: try FfiConverterTypePublicKey.read(from: &buf),
                fundingTxo: try FfiConverterTypeOutPoint.read(from: &buf)
            )

        case 5: return .channelReady(
                channelId: try FfiConverterTypeChannelId.read(from: &buf),
                userChannelId: try FfiConverterTypeUserChannelId.read(from: &buf)
            )

        case 6: return .channelClosed(
                channelId: try FfiConverterTypeChannelId.read(from: &buf),
                userChannelId: try FfiConverterTypeUserChannelId.read(from: &buf)
            )

        default: throw UniffiInternalError.unexpectedEnumCase
        }
    }

    public static func write(_ value: Event, into buf: inout [UInt8]) {
        switch value {
        case let .paymentSuccessful(paymentHash):
            writeInt(&buf, Int32(1))
            FfiConverterTypePaymentHash.write(paymentHash, into: &buf)

        case let .paymentFailed(paymentHash):
            writeInt(&buf, Int32(2))
            FfiConverterTypePaymentHash.write(paymentHash, into: &buf)

        case let .paymentReceived(paymentHash, amountMsat):
            writeInt(&buf, Int32(3))
            FfiConverterTypePaymentHash.write(paymentHash, into: &buf)
            FfiConverterUInt64.write(amountMsat, into: &buf)

        case let .channelPending(channelId, userChannelId, formerTemporaryChannelId, counterpartyNodeId, fundingTxo):
            writeInt(&buf, Int32(4))
            FfiConverterTypeChannelId.write(channelId, into: &buf)
            FfiConverterTypeUserChannelId.write(userChannelId, into: &buf)
            FfiConverterTypeChannelId.write(formerTemporaryChannelId, into: &buf)
            FfiConverterTypePublicKey.write(counterpartyNodeId, into: &buf)
            FfiConverterTypeOutPoint.write(fundingTxo, into: &buf)

        case let .channelReady(channelId, userChannelId):
            writeInt(&buf, Int32(5))
            FfiConverterTypeChannelId.write(channelId, into: &buf)
            FfiConverterTypeUserChannelId.write(userChannelId, into: &buf)

        case let .channelClosed(channelId, userChannelId):
            writeInt(&buf, Int32(6))
            FfiConverterTypeChannelId.write(channelId, into: &buf)
            FfiConverterTypeUserChannelId.write(userChannelId, into: &buf)
        }
    }
}

public func FfiConverterTypeEvent_lift(_ buf: RustBuffer) throws -> Event {
    return try FfiConverterTypeEvent.lift(buf)
}

public func FfiConverterTypeEvent_lower(_ value: Event) -> RustBuffer {
    return FfiConverterTypeEvent.lower(value)
}

extension Event: Equatable, Hashable {}

// Note that we don't yet support `indirect` for enums.
// See https://github.com/mozilla/uniffi-rs/issues/396 for further discussion.
public enum LogLevel {
    case gossip
    case trace
    case debug
    case info
    case warn
    case error
}

public struct FfiConverterTypeLogLevel: FfiConverterRustBuffer {
    typealias SwiftType = LogLevel

    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> LogLevel {
        let variant: Int32 = try readInt(&buf)
        switch variant {
        case 1: return .gossip

        case 2: return .trace

        case 3: return .debug

        case 4: return .info

        case 5: return .warn

        case 6: return .error

        default: throw UniffiInternalError.unexpectedEnumCase
        }
    }

    public static func write(_ value: LogLevel, into buf: inout [UInt8]) {
        switch value {
        case .gossip:
            writeInt(&buf, Int32(1))

        case .trace:
            writeInt(&buf, Int32(2))

        case .debug:
            writeInt(&buf, Int32(3))

        case .info:
            writeInt(&buf, Int32(4))

        case .warn:
            writeInt(&buf, Int32(5))

        case .error:
            writeInt(&buf, Int32(6))
        }
    }
}

public func FfiConverterTypeLogLevel_lift(_ buf: RustBuffer) throws -> LogLevel {
    return try FfiConverterTypeLogLevel.lift(buf)
}

public func FfiConverterTypeLogLevel_lower(_ value: LogLevel) -> RustBuffer {
    return FfiConverterTypeLogLevel.lower(value)
}

extension LogLevel: Equatable, Hashable {}

// Note that we don't yet support `indirect` for enums.
// See https://github.com/mozilla/uniffi-rs/issues/396 for further discussion.
public enum Network {
    case bitcoin
    case testnet
    case signet
    case regtest
}

public struct FfiConverterTypeNetwork: FfiConverterRustBuffer {
    typealias SwiftType = Network

    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> Network {
        let variant: Int32 = try readInt(&buf)
        switch variant {
        case 1: return .bitcoin

        case 2: return .testnet

        case 3: return .signet

        case 4: return .regtest

        default: throw UniffiInternalError.unexpectedEnumCase
        }
    }

    public static func write(_ value: Network, into buf: inout [UInt8]) {
        switch value {
        case .bitcoin:
            writeInt(&buf, Int32(1))

        case .testnet:
            writeInt(&buf, Int32(2))

        case .signet:
            writeInt(&buf, Int32(3))

        case .regtest:
            writeInt(&buf, Int32(4))
        }
    }
}

public func FfiConverterTypeNetwork_lift(_ buf: RustBuffer) throws -> Network {
    return try FfiConverterTypeNetwork.lift(buf)
}

public func FfiConverterTypeNetwork_lower(_ value: Network) -> RustBuffer {
    return FfiConverterTypeNetwork.lower(value)
}

extension Network: Equatable, Hashable {}

// Note that we don't yet support `indirect` for enums.
// See https://github.com/mozilla/uniffi-rs/issues/396 for further discussion.
public enum PaymentDirection {
    case inbound
    case outbound
}

public struct FfiConverterTypePaymentDirection: FfiConverterRustBuffer {
    typealias SwiftType = PaymentDirection

    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> PaymentDirection {
        let variant: Int32 = try readInt(&buf)
        switch variant {
        case 1: return .inbound

        case 2: return .outbound

        default: throw UniffiInternalError.unexpectedEnumCase
        }
    }

    public static func write(_ value: PaymentDirection, into buf: inout [UInt8]) {
        switch value {
        case .inbound:
            writeInt(&buf, Int32(1))

        case .outbound:
            writeInt(&buf, Int32(2))
        }
    }
}

public func FfiConverterTypePaymentDirection_lift(_ buf: RustBuffer) throws -> PaymentDirection {
    return try FfiConverterTypePaymentDirection.lift(buf)
}

public func FfiConverterTypePaymentDirection_lower(_ value: PaymentDirection) -> RustBuffer {
    return FfiConverterTypePaymentDirection.lower(value)
}

extension PaymentDirection: Equatable, Hashable {}

// Note that we don't yet support `indirect` for enums.
// See https://github.com/mozilla/uniffi-rs/issues/396 for further discussion.
public enum PaymentStatus {
    case pending
    case succeeded
    case failed
}

public struct FfiConverterTypePaymentStatus: FfiConverterRustBuffer {
    typealias SwiftType = PaymentStatus

    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> PaymentStatus {
        let variant: Int32 = try readInt(&buf)
        switch variant {
        case 1: return .pending

        case 2: return .succeeded

        case 3: return .failed

        default: throw UniffiInternalError.unexpectedEnumCase
        }
    }

    public static func write(_ value: PaymentStatus, into buf: inout [UInt8]) {
        switch value {
        case .pending:
            writeInt(&buf, Int32(1))

        case .succeeded:
            writeInt(&buf, Int32(2))

        case .failed:
            writeInt(&buf, Int32(3))
        }
    }
}

public func FfiConverterTypePaymentStatus_lift(_ buf: RustBuffer) throws -> PaymentStatus {
    return try FfiConverterTypePaymentStatus.lift(buf)
}

public func FfiConverterTypePaymentStatus_lower(_ value: PaymentStatus) -> RustBuffer {
    return FfiConverterTypePaymentStatus.lower(value)
}

extension PaymentStatus: Equatable, Hashable {}

public enum BuildError {
    // Simple error enums only carry a message
    case InvalidSeedBytes(message: String)

    // Simple error enums only carry a message
    case InvalidSeedFile(message: String)

    // Simple error enums only carry a message
    case InvalidSystemTime(message: String)

    // Simple error enums only carry a message
    case ReadFailed(message: String)

    // Simple error enums only carry a message
    case WriteFailed(message: String)

    // Simple error enums only carry a message
    case StoragePathAccessFailed(message: String)

    // Simple error enums only carry a message
    case WalletSetupFailed(message: String)

    // Simple error enums only carry a message
    case LoggerSetupFailed(message: String)
}

public struct FfiConverterTypeBuildError: FfiConverterRustBuffer {
    typealias SwiftType = BuildError

    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> BuildError {
        let variant: Int32 = try readInt(&buf)
        switch variant {
        case 1: return .InvalidSeedBytes(
                message: try FfiConverterString.read(from: &buf)
            )

        case 2: return .InvalidSeedFile(
                message: try FfiConverterString.read(from: &buf)
            )

        case 3: return .InvalidSystemTime(
                message: try FfiConverterString.read(from: &buf)
            )

        case 4: return .ReadFailed(
                message: try FfiConverterString.read(from: &buf)
            )

        case 5: return .WriteFailed(
                message: try FfiConverterString.read(from: &buf)
            )

        case 6: return .StoragePathAccessFailed(
                message: try FfiConverterString.read(from: &buf)
            )

        case 7: return .WalletSetupFailed(
                message: try FfiConverterString.read(from: &buf)
            )

        case 8: return .LoggerSetupFailed(
                message: try FfiConverterString.read(from: &buf)
            )

        default: throw UniffiInternalError.unexpectedEnumCase
        }
    }

    public static func write(_ value: BuildError, into buf: inout [UInt8]) {
        switch value {
        case let .InvalidSeedBytes(message):
            writeInt(&buf, Int32(1))
            FfiConverterString.write(message, into: &buf)
        case let .InvalidSeedFile(message):
            writeInt(&buf, Int32(2))
            FfiConverterString.write(message, into: &buf)
        case let .InvalidSystemTime(message):
            writeInt(&buf, Int32(3))
            FfiConverterString.write(message, into: &buf)
        case let .ReadFailed(message):
            writeInt(&buf, Int32(4))
            FfiConverterString.write(message, into: &buf)
        case let .WriteFailed(message):
            writeInt(&buf, Int32(5))
            FfiConverterString.write(message, into: &buf)
        case let .StoragePathAccessFailed(message):
            writeInt(&buf, Int32(6))
            FfiConverterString.write(message, into: &buf)
        case let .WalletSetupFailed(message):
            writeInt(&buf, Int32(7))
            FfiConverterString.write(message, into: &buf)
        case let .LoggerSetupFailed(message):
            writeInt(&buf, Int32(8))
            FfiConverterString.write(message, into: &buf)
        }
    }
}

extension BuildError: Equatable, Hashable {}

extension BuildError: Error {}

public enum NodeError {
    // Simple error enums only carry a message
    case AlreadyRunning(message: String)

    // Simple error enums only carry a message
    case NotRunning(message: String)

    // Simple error enums only carry a message
    case OnchainTxCreationFailed(message: String)

    // Simple error enums only carry a message
    case ConnectionFailed(message: String)

    // Simple error enums only carry a message
    case InvoiceCreationFailed(message: String)

    // Simple error enums only carry a message
    case PaymentSendingFailed(message: String)

    // Simple error enums only carry a message
    case ChannelCreationFailed(message: String)

    // Simple error enums only carry a message
    case ChannelClosingFailed(message: String)

    // Simple error enums only carry a message
    case ChannelConfigUpdateFailed(message: String)

    // Simple error enums only carry a message
    case PersistenceFailed(message: String)

    // Simple error enums only carry a message
    case WalletOperationFailed(message: String)

    // Simple error enums only carry a message
    case OnchainTxSigningFailed(message: String)

    // Simple error enums only carry a message
    case MessageSigningFailed(message: String)

    // Simple error enums only carry a message
    case TxSyncFailed(message: String)

    // Simple error enums only carry a message
    case GossipUpdateFailed(message: String)

    // Simple error enums only carry a message
    case InvalidAddress(message: String)

    // Simple error enums only carry a message
    case InvalidNetAddress(message: String)

    // Simple error enums only carry a message
    case InvalidPublicKey(message: String)

    // Simple error enums only carry a message
    case InvalidSecretKey(message: String)

    // Simple error enums only carry a message
    case InvalidPaymentHash(message: String)

    // Simple error enums only carry a message
    case InvalidPaymentPreimage(message: String)

    // Simple error enums only carry a message
    case InvalidPaymentSecret(message: String)

    // Simple error enums only carry a message
    case InvalidAmount(message: String)

    // Simple error enums only carry a message
    case InvalidInvoice(message: String)

    // Simple error enums only carry a message
    case InvalidChannelId(message: String)

    // Simple error enums only carry a message
    case InvalidNetwork(message: String)

    // Simple error enums only carry a message
    case DuplicatePayment(message: String)

    // Simple error enums only carry a message
    case InsufficientFunds(message: String)
}

public struct FfiConverterTypeNodeError: FfiConverterRustBuffer {
    typealias SwiftType = NodeError

    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> NodeError {
        let variant: Int32 = try readInt(&buf)
        switch variant {
        case 1: return .AlreadyRunning(
                message: try FfiConverterString.read(from: &buf)
            )

        case 2: return .NotRunning(
                message: try FfiConverterString.read(from: &buf)
            )

        case 3: return .OnchainTxCreationFailed(
                message: try FfiConverterString.read(from: &buf)
            )

        case 4: return .ConnectionFailed(
                message: try FfiConverterString.read(from: &buf)
            )

        case 5: return .InvoiceCreationFailed(
                message: try FfiConverterString.read(from: &buf)
            )

        case 6: return .PaymentSendingFailed(
                message: try FfiConverterString.read(from: &buf)
            )

        case 7: return .ChannelCreationFailed(
                message: try FfiConverterString.read(from: &buf)
            )

        case 8: return .ChannelClosingFailed(
                message: try FfiConverterString.read(from: &buf)
            )

        case 9: return .ChannelConfigUpdateFailed(
                message: try FfiConverterString.read(from: &buf)
            )

        case 10: return .PersistenceFailed(
                message: try FfiConverterString.read(from: &buf)
            )

        case 11: return .WalletOperationFailed(
                message: try FfiConverterString.read(from: &buf)
            )

        case 12: return .OnchainTxSigningFailed(
                message: try FfiConverterString.read(from: &buf)
            )

        case 13: return .MessageSigningFailed(
                message: try FfiConverterString.read(from: &buf)
            )

        case 14: return .TxSyncFailed(
                message: try FfiConverterString.read(from: &buf)
            )

        case 15: return .GossipUpdateFailed(
                message: try FfiConverterString.read(from: &buf)
            )

        case 16: return .InvalidAddress(
                message: try FfiConverterString.read(from: &buf)
            )

        case 17: return .InvalidNetAddress(
                message: try FfiConverterString.read(from: &buf)
            )

        case 18: return .InvalidPublicKey(
                message: try FfiConverterString.read(from: &buf)
            )

        case 19: return .InvalidSecretKey(
                message: try FfiConverterString.read(from: &buf)
            )

        case 20: return .InvalidPaymentHash(
                message: try FfiConverterString.read(from: &buf)
            )

        case 21: return .InvalidPaymentPreimage(
                message: try FfiConverterString.read(from: &buf)
            )

        case 22: return .InvalidPaymentSecret(
                message: try FfiConverterString.read(from: &buf)
            )

        case 23: return .InvalidAmount(
                message: try FfiConverterString.read(from: &buf)
            )

        case 24: return .InvalidInvoice(
                message: try FfiConverterString.read(from: &buf)
            )

        case 25: return .InvalidChannelId(
                message: try FfiConverterString.read(from: &buf)
            )

        case 26: return .InvalidNetwork(
                message: try FfiConverterString.read(from: &buf)
            )

        case 27: return .DuplicatePayment(
                message: try FfiConverterString.read(from: &buf)
            )

        case 28: return .InsufficientFunds(
                message: try FfiConverterString.read(from: &buf)
            )

        default: throw UniffiInternalError.unexpectedEnumCase
        }
    }

    public static func write(_ value: NodeError, into buf: inout [UInt8]) {
        switch value {
        case let .AlreadyRunning(message):
            writeInt(&buf, Int32(1))
            FfiConverterString.write(message, into: &buf)
        case let .NotRunning(message):
            writeInt(&buf, Int32(2))
            FfiConverterString.write(message, into: &buf)
        case let .OnchainTxCreationFailed(message):
            writeInt(&buf, Int32(3))
            FfiConverterString.write(message, into: &buf)
        case let .ConnectionFailed(message):
            writeInt(&buf, Int32(4))
            FfiConverterString.write(message, into: &buf)
        case let .InvoiceCreationFailed(message):
            writeInt(&buf, Int32(5))
            FfiConverterString.write(message, into: &buf)
        case let .PaymentSendingFailed(message):
            writeInt(&buf, Int32(6))
            FfiConverterString.write(message, into: &buf)
        case let .ChannelCreationFailed(message):
            writeInt(&buf, Int32(7))
            FfiConverterString.write(message, into: &buf)
        case let .ChannelClosingFailed(message):
            writeInt(&buf, Int32(8))
            FfiConverterString.write(message, into: &buf)
        case let .ChannelConfigUpdateFailed(message):
            writeInt(&buf, Int32(9))
            FfiConverterString.write(message, into: &buf)
        case let .PersistenceFailed(message):
            writeInt(&buf, Int32(10))
            FfiConverterString.write(message, into: &buf)
        case let .WalletOperationFailed(message):
            writeInt(&buf, Int32(11))
            FfiConverterString.write(message, into: &buf)
        case let .OnchainTxSigningFailed(message):
            writeInt(&buf, Int32(12))
            FfiConverterString.write(message, into: &buf)
        case let .MessageSigningFailed(message):
            writeInt(&buf, Int32(13))
            FfiConverterString.write(message, into: &buf)
        case let .TxSyncFailed(message):
            writeInt(&buf, Int32(14))
            FfiConverterString.write(message, into: &buf)
        case let .GossipUpdateFailed(message):
            writeInt(&buf, Int32(15))
            FfiConverterString.write(message, into: &buf)
        case let .InvalidAddress(message):
            writeInt(&buf, Int32(16))
            FfiConverterString.write(message, into: &buf)
        case let .InvalidNetAddress(message):
            writeInt(&buf, Int32(17))
            FfiConverterString.write(message, into: &buf)
        case let .InvalidPublicKey(message):
            writeInt(&buf, Int32(18))
            FfiConverterString.write(message, into: &buf)
        case let .InvalidSecretKey(message):
            writeInt(&buf, Int32(19))
            FfiConverterString.write(message, into: &buf)
        case let .InvalidPaymentHash(message):
            writeInt(&buf, Int32(20))
            FfiConverterString.write(message, into: &buf)
        case let .InvalidPaymentPreimage(message):
            writeInt(&buf, Int32(21))
            FfiConverterString.write(message, into: &buf)
        case let .InvalidPaymentSecret(message):
            writeInt(&buf, Int32(22))
            FfiConverterString.write(message, into: &buf)
        case let .InvalidAmount(message):
            writeInt(&buf, Int32(23))
            FfiConverterString.write(message, into: &buf)
        case let .InvalidInvoice(message):
            writeInt(&buf, Int32(24))
            FfiConverterString.write(message, into: &buf)
        case let .InvalidChannelId(message):
            writeInt(&buf, Int32(25))
            FfiConverterString.write(message, into: &buf)
        case let .InvalidNetwork(message):
            writeInt(&buf, Int32(26))
            FfiConverterString.write(message, into: &buf)
        case let .DuplicatePayment(message):
            writeInt(&buf, Int32(27))
            FfiConverterString.write(message, into: &buf)
        case let .InsufficientFunds(message):
            writeInt(&buf, Int32(28))
            FfiConverterString.write(message, into: &buf)
        }
    }
}

extension NodeError: Equatable, Hashable {}

extension NodeError: Error {}

private struct FfiConverterOptionUInt16: FfiConverterRustBuffer {
    typealias SwiftType = UInt16?

    public static func write(_ value: SwiftType, into buf: inout [UInt8]) {
        guard let value = value else {
            writeInt(&buf, Int8(0))
            return
        }
        writeInt(&buf, Int8(1))
        FfiConverterUInt16.write(value, into: &buf)
    }

    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> SwiftType {
        switch try readInt(&buf) as Int8 {
        case 0: return nil
        case 1: return try FfiConverterUInt16.read(from: &buf)
        default: throw UniffiInternalError.unexpectedOptionalTag
        }
    }
}

private struct FfiConverterOptionUInt32: FfiConverterRustBuffer {
    typealias SwiftType = UInt32?

    public static func write(_ value: SwiftType, into buf: inout [UInt8]) {
        guard let value = value else {
            writeInt(&buf, Int8(0))
            return
        }
        writeInt(&buf, Int8(1))
        FfiConverterUInt32.write(value, into: &buf)
    }

    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> SwiftType {
        switch try readInt(&buf) as Int8 {
        case 0: return nil
        case 1: return try FfiConverterUInt32.read(from: &buf)
        default: throw UniffiInternalError.unexpectedOptionalTag
        }
    }
}

private struct FfiConverterOptionUInt64: FfiConverterRustBuffer {
    typealias SwiftType = UInt64?

    public static func write(_ value: SwiftType, into buf: inout [UInt8]) {
        guard let value = value else {
            writeInt(&buf, Int8(0))
            return
        }
        writeInt(&buf, Int8(1))
        FfiConverterUInt64.write(value, into: &buf)
    }

    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> SwiftType {
        switch try readInt(&buf) as Int8 {
        case 0: return nil
        case 1: return try FfiConverterUInt64.read(from: &buf)
        default: throw UniffiInternalError.unexpectedOptionalTag
        }
    }
}

private struct FfiConverterOptionString: FfiConverterRustBuffer {
    typealias SwiftType = String?

    public static func write(_ value: SwiftType, into buf: inout [UInt8]) {
        guard let value = value else {
            writeInt(&buf, Int8(0))
            return
        }
        writeInt(&buf, Int8(1))
        FfiConverterString.write(value, into: &buf)
    }

    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> SwiftType {
        switch try readInt(&buf) as Int8 {
        case 0: return nil
        case 1: return try FfiConverterString.read(from: &buf)
        default: throw UniffiInternalError.unexpectedOptionalTag
        }
    }
}

private struct FfiConverterOptionTypeChannelConfig: FfiConverterRustBuffer {
    typealias SwiftType = ChannelConfig?

    public static func write(_ value: SwiftType, into buf: inout [UInt8]) {
        guard let value = value else {
            writeInt(&buf, Int8(0))
            return
        }
        writeInt(&buf, Int8(1))
        FfiConverterTypeChannelConfig.write(value, into: &buf)
    }

    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> SwiftType {
        switch try readInt(&buf) as Int8 {
        case 0: return nil
        case 1: return try FfiConverterTypeChannelConfig.read(from: &buf)
        default: throw UniffiInternalError.unexpectedOptionalTag
        }
    }
}

private struct FfiConverterOptionTypeOutPoint: FfiConverterRustBuffer {
    typealias SwiftType = OutPoint?

    public static func write(_ value: SwiftType, into buf: inout [UInt8]) {
        guard let value = value else {
            writeInt(&buf, Int8(0))
            return
        }
        writeInt(&buf, Int8(1))
        FfiConverterTypeOutPoint.write(value, into: &buf)
    }

    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> SwiftType {
        switch try readInt(&buf) as Int8 {
        case 0: return nil
        case 1: return try FfiConverterTypeOutPoint.read(from: &buf)
        default: throw UniffiInternalError.unexpectedOptionalTag
        }
    }
}

private struct FfiConverterOptionTypePaymentDetails: FfiConverterRustBuffer {
    typealias SwiftType = PaymentDetails?

    public static func write(_ value: SwiftType, into buf: inout [UInt8]) {
        guard let value = value else {
            writeInt(&buf, Int8(0))
            return
        }
        writeInt(&buf, Int8(1))
        FfiConverterTypePaymentDetails.write(value, into: &buf)
    }

    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> SwiftType {
        switch try readInt(&buf) as Int8 {
        case 0: return nil
        case 1: return try FfiConverterTypePaymentDetails.read(from: &buf)
        default: throw UniffiInternalError.unexpectedOptionalTag
        }
    }
}

private struct FfiConverterOptionTypeEvent: FfiConverterRustBuffer {
    typealias SwiftType = Event?

    public static func write(_ value: SwiftType, into buf: inout [UInt8]) {
        guard let value = value else {
            writeInt(&buf, Int8(0))
            return
        }
        writeInt(&buf, Int8(1))
        FfiConverterTypeEvent.write(value, into: &buf)
    }

    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> SwiftType {
        switch try readInt(&buf) as Int8 {
        case 0: return nil
        case 1: return try FfiConverterTypeEvent.read(from: &buf)
        default: throw UniffiInternalError.unexpectedOptionalTag
        }
    }
}

private struct FfiConverterOptionTypeNetAddress: FfiConverterRustBuffer {
    typealias SwiftType = NetAddress?

    public static func write(_ value: SwiftType, into buf: inout [UInt8]) {
        guard let value = value else {
            writeInt(&buf, Int8(0))
            return
        }
        writeInt(&buf, Int8(1))
        FfiConverterTypeNetAddress.write(value, into: &buf)
    }

    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> SwiftType {
        switch try readInt(&buf) as Int8 {
        case 0: return nil
        case 1: return try FfiConverterTypeNetAddress.read(from: &buf)
        default: throw UniffiInternalError.unexpectedOptionalTag
        }
    }
}

private struct FfiConverterOptionTypePaymentPreimage: FfiConverterRustBuffer {
    typealias SwiftType = PaymentPreimage?

    public static func write(_ value: SwiftType, into buf: inout [UInt8]) {
        guard let value = value else {
            writeInt(&buf, Int8(0))
            return
        }
        writeInt(&buf, Int8(1))
        FfiConverterTypePaymentPreimage.write(value, into: &buf)
    }

    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> SwiftType {
        switch try readInt(&buf) as Int8 {
        case 0: return nil
        case 1: return try FfiConverterTypePaymentPreimage.read(from: &buf)
        default: throw UniffiInternalError.unexpectedOptionalTag
        }
    }
}

private struct FfiConverterOptionTypePaymentSecret: FfiConverterRustBuffer {
    typealias SwiftType = PaymentSecret?

    public static func write(_ value: SwiftType, into buf: inout [UInt8]) {
        guard let value = value else {
            writeInt(&buf, Int8(0))
            return
        }
        writeInt(&buf, Int8(1))
        FfiConverterTypePaymentSecret.write(value, into: &buf)
    }

    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> SwiftType {
        switch try readInt(&buf) as Int8 {
        case 0: return nil
        case 1: return try FfiConverterTypePaymentSecret.read(from: &buf)
        default: throw UniffiInternalError.unexpectedOptionalTag
        }
    }
}

private struct FfiConverterSequenceUInt8: FfiConverterRustBuffer {
    typealias SwiftType = [UInt8]

    public static func write(_ value: [UInt8], into buf: inout [UInt8]) {
        let len = Int32(value.count)
        writeInt(&buf, len)
        for item in value {
            FfiConverterUInt8.write(item, into: &buf)
        }
    }

    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> [UInt8] {
        let len: Int32 = try readInt(&buf)
        var seq = [UInt8]()
        seq.reserveCapacity(Int(len))
        for _ in 0 ..< len {
            seq.append(try FfiConverterUInt8.read(from: &buf))
        }
        return seq
    }
}

private struct FfiConverterSequenceTypeChannelDetails: FfiConverterRustBuffer {
    typealias SwiftType = [ChannelDetails]

    public static func write(_ value: [ChannelDetails], into buf: inout [UInt8]) {
        let len = Int32(value.count)
        writeInt(&buf, len)
        for item in value {
            FfiConverterTypeChannelDetails.write(item, into: &buf)
        }
    }

    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> [ChannelDetails] {
        let len: Int32 = try readInt(&buf)
        var seq = [ChannelDetails]()
        seq.reserveCapacity(Int(len))
        for _ in 0 ..< len {
            seq.append(try FfiConverterTypeChannelDetails.read(from: &buf))
        }
        return seq
    }
}

private struct FfiConverterSequenceTypePaymentDetails: FfiConverterRustBuffer {
    typealias SwiftType = [PaymentDetails]

    public static func write(_ value: [PaymentDetails], into buf: inout [UInt8]) {
        let len = Int32(value.count)
        writeInt(&buf, len)
        for item in value {
            FfiConverterTypePaymentDetails.write(item, into: &buf)
        }
    }

    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> [PaymentDetails] {
        let len: Int32 = try readInt(&buf)
        var seq = [PaymentDetails]()
        seq.reserveCapacity(Int(len))
        for _ in 0 ..< len {
            seq.append(try FfiConverterTypePaymentDetails.read(from: &buf))
        }
        return seq
    }
}

private struct FfiConverterSequenceTypePeerDetails: FfiConverterRustBuffer {
    typealias SwiftType = [PeerDetails]

    public static func write(_ value: [PeerDetails], into buf: inout [UInt8]) {
        let len = Int32(value.count)
        writeInt(&buf, len)
        for item in value {
            FfiConverterTypePeerDetails.write(item, into: &buf)
        }
    }

    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> [PeerDetails] {
        let len: Int32 = try readInt(&buf)
        var seq = [PeerDetails]()
        seq.reserveCapacity(Int(len))
        for _ in 0 ..< len {
            seq.append(try FfiConverterTypePeerDetails.read(from: &buf))
        }
        return seq
    }
}

private struct FfiConverterSequenceTypePublicKey: FfiConverterRustBuffer {
    typealias SwiftType = [PublicKey]

    public static func write(_ value: [PublicKey], into buf: inout [UInt8]) {
        let len = Int32(value.count)
        writeInt(&buf, len)
        for item in value {
            FfiConverterTypePublicKey.write(item, into: &buf)
        }
    }

    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> [PublicKey] {
        let len: Int32 = try readInt(&buf)
        var seq = [PublicKey]()
        seq.reserveCapacity(Int(len))
        for _ in 0 ..< len {
            seq.append(try FfiConverterTypePublicKey.read(from: &buf))
        }
        return seq
    }
}

/**
 * Typealias from the type name used in the UDL file to the builtin type.  This
 * is needed because the UDL type name is used in function/method signatures.
 */
public typealias Address = String
public struct FfiConverterTypeAddress: FfiConverter {
    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> Address {
        return try FfiConverterString.read(from: &buf)
    }

    public static func write(_ value: Address, into buf: inout [UInt8]) {
        return FfiConverterString.write(value, into: &buf)
    }

    public static func lift(_ value: RustBuffer) throws -> Address {
        return try FfiConverterString.lift(value)
    }

    public static func lower(_ value: Address) -> RustBuffer {
        return FfiConverterString.lower(value)
    }
}

/**
 * Typealias from the type name used in the UDL file to the builtin type.  This
 * is needed because the UDL type name is used in function/method signatures.
 */
public typealias ChannelId = String
public struct FfiConverterTypeChannelId: FfiConverter {
    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> ChannelId {
        return try FfiConverterString.read(from: &buf)
    }

    public static func write(_ value: ChannelId, into buf: inout [UInt8]) {
        return FfiConverterString.write(value, into: &buf)
    }

    public static func lift(_ value: RustBuffer) throws -> ChannelId {
        return try FfiConverterString.lift(value)
    }

    public static func lower(_ value: ChannelId) -> RustBuffer {
        return FfiConverterString.lower(value)
    }
}

/**
 * Typealias from the type name used in the UDL file to the builtin type.  This
 * is needed because the UDL type name is used in function/method signatures.
 */
public typealias Invoice = String
public struct FfiConverterTypeInvoice: FfiConverter {
    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> Invoice {
        return try FfiConverterString.read(from: &buf)
    }

    public static func write(_ value: Invoice, into buf: inout [UInt8]) {
        return FfiConverterString.write(value, into: &buf)
    }

    public static func lift(_ value: RustBuffer) throws -> Invoice {
        return try FfiConverterString.lift(value)
    }

    public static func lower(_ value: Invoice) -> RustBuffer {
        return FfiConverterString.lower(value)
    }
}

/**
 * Typealias from the type name used in the UDL file to the builtin type.  This
 * is needed because the UDL type name is used in function/method signatures.
 */
public typealias Mnemonic = String
public struct FfiConverterTypeMnemonic: FfiConverter {
    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> Mnemonic {
        return try FfiConverterString.read(from: &buf)
    }

    public static func write(_ value: Mnemonic, into buf: inout [UInt8]) {
        return FfiConverterString.write(value, into: &buf)
    }

    public static func lift(_ value: RustBuffer) throws -> Mnemonic {
        return try FfiConverterString.lift(value)
    }

    public static func lower(_ value: Mnemonic) -> RustBuffer {
        return FfiConverterString.lower(value)
    }
}

/**
 * Typealias from the type name used in the UDL file to the builtin type.  This
 * is needed because the UDL type name is used in function/method signatures.
 */
public typealias NetAddress = String
public struct FfiConverterTypeNetAddress: FfiConverter {
    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> NetAddress {
        return try FfiConverterString.read(from: &buf)
    }

    public static func write(_ value: NetAddress, into buf: inout [UInt8]) {
        return FfiConverterString.write(value, into: &buf)
    }

    public static func lift(_ value: RustBuffer) throws -> NetAddress {
        return try FfiConverterString.lift(value)
    }

    public static func lower(_ value: NetAddress) -> RustBuffer {
        return FfiConverterString.lower(value)
    }
}

/**
 * Typealias from the type name used in the UDL file to the builtin type.  This
 * is needed because the UDL type name is used in function/method signatures.
 */
public typealias PaymentHash = String
public struct FfiConverterTypePaymentHash: FfiConverter {
    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> PaymentHash {
        return try FfiConverterString.read(from: &buf)
    }

    public static func write(_ value: PaymentHash, into buf: inout [UInt8]) {
        return FfiConverterString.write(value, into: &buf)
    }

    public static func lift(_ value: RustBuffer) throws -> PaymentHash {
        return try FfiConverterString.lift(value)
    }

    public static func lower(_ value: PaymentHash) -> RustBuffer {
        return FfiConverterString.lower(value)
    }
}

/**
 * Typealias from the type name used in the UDL file to the builtin type.  This
 * is needed because the UDL type name is used in function/method signatures.
 */
public typealias PaymentPreimage = String
public struct FfiConverterTypePaymentPreimage: FfiConverter {
    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> PaymentPreimage {
        return try FfiConverterString.read(from: &buf)
    }

    public static func write(_ value: PaymentPreimage, into buf: inout [UInt8]) {
        return FfiConverterString.write(value, into: &buf)
    }

    public static func lift(_ value: RustBuffer) throws -> PaymentPreimage {
        return try FfiConverterString.lift(value)
    }

    public static func lower(_ value: PaymentPreimage) -> RustBuffer {
        return FfiConverterString.lower(value)
    }
}

/**
 * Typealias from the type name used in the UDL file to the builtin type.  This
 * is needed because the UDL type name is used in function/method signatures.
 */
public typealias PaymentSecret = String
public struct FfiConverterTypePaymentSecret: FfiConverter {
    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> PaymentSecret {
        return try FfiConverterString.read(from: &buf)
    }

    public static func write(_ value: PaymentSecret, into buf: inout [UInt8]) {
        return FfiConverterString.write(value, into: &buf)
    }

    public static func lift(_ value: RustBuffer) throws -> PaymentSecret {
        return try FfiConverterString.lift(value)
    }

    public static func lower(_ value: PaymentSecret) -> RustBuffer {
        return FfiConverterString.lower(value)
    }
}

/**
 * Typealias from the type name used in the UDL file to the builtin type.  This
 * is needed because the UDL type name is used in function/method signatures.
 */
public typealias PublicKey = String
public struct FfiConverterTypePublicKey: FfiConverter {
    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> PublicKey {
        return try FfiConverterString.read(from: &buf)
    }

    public static func write(_ value: PublicKey, into buf: inout [UInt8]) {
        return FfiConverterString.write(value, into: &buf)
    }

    public static func lift(_ value: RustBuffer) throws -> PublicKey {
        return try FfiConverterString.lift(value)
    }

    public static func lower(_ value: PublicKey) -> RustBuffer {
        return FfiConverterString.lower(value)
    }
}

/**
 * Typealias from the type name used in the UDL file to the builtin type.  This
 * is needed because the UDL type name is used in function/method signatures.
 */
public typealias Txid = String
public struct FfiConverterTypeTxid: FfiConverter {
    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> Txid {
        return try FfiConverterString.read(from: &buf)
    }

    public static func write(_ value: Txid, into buf: inout [UInt8]) {
        return FfiConverterString.write(value, into: &buf)
    }

    public static func lift(_ value: RustBuffer) throws -> Txid {
        return try FfiConverterString.lift(value)
    }

    public static func lower(_ value: Txid) -> RustBuffer {
        return FfiConverterString.lower(value)
    }
}

/**
 * Typealias from the type name used in the UDL file to the builtin type.  This
 * is needed because the UDL type name is used in function/method signatures.
 */
public typealias UserChannelId = String
public struct FfiConverterTypeUserChannelId: FfiConverter {
    public static func read(from buf: inout (data: Data, offset: Data.Index)) throws -> UserChannelId {
        return try FfiConverterString.read(from: &buf)
    }

    public static func write(_ value: UserChannelId, into buf: inout [UInt8]) {
        return FfiConverterString.write(value, into: &buf)
    }

    public static func lift(_ value: RustBuffer) throws -> UserChannelId {
        return try FfiConverterString.lift(value)
    }

    public static func lower(_ value: UserChannelId) -> RustBuffer {
        return FfiConverterString.lower(value)
    }
}

public func generateEntropyMnemonic() -> Mnemonic {
    return try! FfiConverterTypeMnemonic.lift(
        try!

            rustCall {
                ldk_node_3490_generate_entropy_mnemonic($0)
            }
    )
}

/**
 * Top level initializers and tear down methods.
 *
 * This is generated by uniffi.
 */
public enum LdkNodeLifecycle {
    /**
     * Initialize the FFI and Rust library. This should be only called once per application.
     */
    func initialize() {}
}

'''
'''--- bindings/uniffi-bindgen/Cargo.toml ---
[package]
name = "uniffi-bindgen"
version = "0.1.0"
edition = "2021"

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[dependencies]
uniffi = { version = "0.25.1", features = ["cli"] }

'''
'''--- bindings/uniffi-bindgen/src/main.rs ---
fn main() {
	uniffi::uniffi_bindgen_main()
}

'''
'''--- build.rs ---
fn main() {
	#[cfg(feature = "uniffi")]
	uniffi::generate_scaffolding("bindings/ldk_node.udl").unwrap();
}

'''
'''--- docker-compose.yml ---
version: '3'

services:
  bitcoin:
    image: blockstream/bitcoind:24.1
    platform: linux/amd64
    command:
      [
        "bitcoind",
        "-printtoconsole",
        "-regtest=1",
        "-rpcallowip=0.0.0.0/0",
        "-rpcbind=0.0.0.0",
        "-rpcuser=user",
        "-rpcpassword=pass",
        "-fallbackfee=0.00001"
      ]
    ports:
      - "18443:18443"  # Regtest RPC port
      - "18444:18444"  # Regtest P2P port
    networks:
      - bitcoin-electrs
    healthcheck:
      test: ["CMD", "bitcoin-cli", "-regtest", "-rpcuser=user", "-rpcpassword=pass", "getblockchaininfo"]
      interval: 5s
      timeout: 10s
      retries: 5

  electrs:
    image: blockstream/esplora:electrs-cd9f90c115751eb9d2bca9a4da89d10d048ae931
    platform: linux/amd64
    depends_on:
      bitcoin:
        condition: service_healthy
    command:
      [
        "/app/electrs_bitcoin/bin/electrs",
        "-vvvv",
        "--timestamp",
        "--jsonrpc-import",
        "--cookie=user:pass",
        "--network=regtest",
        "--daemon-rpc-addr=bitcoin:18443",
        "--http-addr=0.0.0.0:3002"
      ]
    ports:
      - "3002:3002"
    networks:
      - bitcoin-electrs

networks:
  bitcoin-electrs:
    driver: bridge

'''
'''--- rustfmt.toml ---
hard_tabs = true # use tab characters for indentation, spaces for alignment
use_field_init_shorthand = true
max_width = 100
use_small_heuristics = "Max"
fn_args_layout = "Compressed"

'''
'''--- scripts/format_kotlin.sh ---
#!/bin/bash
LDK_NODE_ANDROID_DIR="bindings/kotlin/ldk-node-android"
LDK_NODE_JVM_DIR="bindings/kotlin/ldk-node-jvm"

# Run ktlintFormat in ldk-node-android
(
  cd $LDK_NODE_ANDROID_DIR || exit 1
  ./gradlew ktlintFormat
)

# Run ktlintFormat in ldk-node-jvm
(
  cd $LDK_NODE_JVM_DIR || exit 1
  ./gradlew ktlintFormat
)

'''
'''--- scripts/python_create_package.sh ---
#!/bin/bash
cd bindings/python || exit 1
python3 -m build

'''
'''--- scripts/swift_create_xcframework_archive.sh ---
ditto -c -k --sequesterRsrc --keepParent ./bindings/swift/LDKNodeFFI.xcframework ./bindings/swift/LDKNodeFFI.xcframework.zip || exit 1
CHECKSUM=`swift package compute-checksum ./bindings/swift/LDKNodeFFI.xcframework.zip` || exit 1
echo "New checksum: $CHECKSUM" || exit 1
python3 ./scripts/swift_update_package_checksum.py --checksum "${CHECKSUM}" || exit 1

'''
'''--- scripts/swift_update_package_checksum.py ---
import argparse
import json
import os
import re
import sys

def run(new_checksum: str = None, new_tag: str = None):
	if new_checksum is None and new_tag is None:
		print('At least one of --checksum or --tag arguments must be provided.', file=sys.stderr)
		sys.exit(1)

	if new_checksum is not None:
		if not new_checksum.isalnum():
			print('Checksum must be alphanumeric.', file=sys.stderr)
			sys.exit(1)

		if not new_checksum.islower():
			print('Checksum must be lowercase.', file=sys.stderr)
			sys.exit(1)

		try:
			int(new_checksum, 16)
		except:
			print('Checksum must be hexadecimal.', file=sys.stderr)
			sys.exit(1)

	if new_tag is not None:
		if new_tag.strip() != new_tag:
			print('Tag must not contain any whitespace.', file=sys.stderr)

		tag_regex = re.compile("^\d+[.]\d+[.]\d+$")
		tag_match = tag_regex.match(new_tag)
		if tag_match is None:
			print('Tag must adhere to x.x.x major/minor/patch format.', file=sys.stderr)

	settings = [
		{'variable_name': 'checksum', 'value': new_checksum},
		{'variable_name': 'tag', 'value': new_tag},
	]

	package_file_path = os.path.realpath(os.path.join(os.path.dirname(__file__), '../Package.swift'))
	print(package_file_path)

	original_package_file = None
	try:
		with open(package_file_path, 'r') as package_file_handle:
			original_package_file = package_file_handle.read()
	except:
		print('Failed to read Package.swift file.', file=sys.stderr)
		sys.exit(1)

	package_file = original_package_file
	for current_setting in settings:
		current_variable_name = current_setting['variable_name']
		new_value = current_setting['value']
		if new_value is None:
			continue

		print(f'setting {current_variable_name} (JSON-serialization):')
		print(json.dumps(new_value))

		regex = re.compile(f'(let[\s]+{current_variable_name}[\s]*=[\s]*)(.*)')

		previous_value = regex.search(package_file).group(2)
		package_file = package_file.replace(previous_value, f'"{new_value}"')

	with open(package_file_path, "w") as f:
		f.write(package_file)

if __name__ == '__main__':
	parser = argparse.ArgumentParser(description='Process some integers.')
	parser.add_argument('--checksum', type=str, help='new checksum of LDKNode.xcframework.zip', required=False, default=None)
	parser.add_argument('--tag', type=str, help='new release tag', required=False, default=None)
	args = parser.parse_args()
	run(new_checksum=args.checksum, new_tag=args.tag)

'''
'''--- scripts/uniffi_bindgen_generate.sh ---
#!/bin/bash
source ./scripts/uniffi_bindgen_generate_kotlin.sh || exit 1
source ./scripts/uniffi_bindgen_generate_python.sh || exit 1
source ./scripts/uniffi_bindgen_generate_swift.sh || exit 1

'''
'''--- scripts/uniffi_bindgen_generate_kotlin.sh ---
#!/bin/bash
BINDINGS_DIR="bindings/kotlin"
TARGET_DIR="target/bindings/kotlin"
PROJECT_DIR="ldk-node-jvm"
PACKAGE_DIR="org/lightningdevkit/ldknode"
UNIFFI_BINDGEN_BIN="cargo run --manifest-path bindings/uniffi-bindgen/Cargo.toml"

if [[ "$OSTYPE" == "linux-gnu"* ]]; then
	rustup target add x86_64-unknown-linux-gnu || exit 1
	cargo build --release --target x86_64-unknown-linux-gnu --features uniffi || exit 1
	DYNAMIC_LIB_PATH="target/x86_64-unknown-linux-gnu/release/libldk_node.so"
	RES_DIR="$BINDINGS_DIR/$PROJECT_DIR/lib/src/main/resources/linux-x86-64/"
	mkdir -p $RES_DIR || exit 1
	cp $DYNAMIC_LIB_PATH $RES_DIR || exit 1
else
	rustup target add x86_64-apple-darwin || exit 1
	cargo build --release --target x86_64-apple-darwin --features uniffi || exit 1
	DYNAMIC_LIB_PATH="target/x86_64-apple-darwin/release/libldk_node.dylib"
	RES_DIR="$BINDINGS_DIR/$PROJECT_DIR/lib/src/main/resources/darwin-x86-64/"
	mkdir -p $RES_DIR || exit 1
	cp $DYNAMIC_LIB_PATH $RES_DIR || exit 1

	rustup target add aarch64-apple-darwin || exit 1
	cargo build --release --target aarch64-apple-darwin --features uniffi || exit 1
	DYNAMIC_LIB_PATH="target/aarch64-apple-darwin/release/libldk_node.dylib"
	RES_DIR="$BINDINGS_DIR/$PROJECT_DIR/lib/src/main/resources/darwin-aarch64/"
	mkdir -p $RES_DIR || exit 1
	cp $DYNAMIC_LIB_PATH $RES_DIR || exit 1
fi

mkdir -p "$BINDINGS_DIR"/"$PROJECT_DIR"/lib/src/main/kotlin/"$PACKAGE_DIR" || exit 1
$UNIFFI_BINDGEN_BIN generate bindings/ldk_node.udl --language kotlin -o "$TARGET_DIR" || exit 1

cp "$TARGET_DIR"/"$PACKAGE_DIR"/ldk_node.kt "$BINDINGS_DIR"/"$PROJECT_DIR"/lib/src/main/kotlin/"$PACKAGE_DIR"/ || exit 1

'''
'''--- scripts/uniffi_bindgen_generate_kotlin_android.sh ---
#!/bin/bash

BINDINGS_DIR="bindings/kotlin"
TARGET_DIR="target"
PROJECT_DIR="ldk-node-android"
UNIFFI_BINDGEN_BIN="cargo run --manifest-path bindings/uniffi-bindgen/Cargo.toml"

export_variable_if_not_present() {
  local name="$1"
  local value="$2"

  # Check if the variable is already set
  if [ -z "${!name}" ]; then
    export "$name=$value"
    echo "Exported $name=$value"
  else
    echo "$name is already set to ${!name}, not exporting."
  fi
}

case "$OSTYPE" in
    linux-gnu)
      export_variable_if_not_present "ANDROID_NDK_ROOT" "/opt/android-ndk"
      export_variable_if_not_present "LLVM_ARCH_PATH" "linux-x86_64"
      ;;
    darwin*)
      export_variable_if_not_present "ANDROID_NDK_ROOT" "/opt/homebrew/share/android-ndk"
      export_variable_if_not_present "LLVM_ARCH_PATH" "darwin-x86_64"
      ;;
    *)
      echo "Unknown operating system: $OSTYPE"
      ;;
    esac

PATH="$ANDROID_NDK_ROOT/toolchains/llvm/prebuilt/$LLVM_ARCH_PATH/bin:$PATH"

rustup target add x86_64-linux-android aarch64-linux-android armv7-linux-androideabi
CFLAGS="-D__ANDROID_MIN_SDK_VERSION__=21" AR=llvm-ar CARGO_TARGET_X86_64_LINUX_ANDROID_LINKER="x86_64-linux-android21-clang" CC="x86_64-linux-android21-clang" cargo build --profile release-smaller --features uniffi --target x86_64-linux-android || exit 1
CFLAGS="-D__ANDROID_MIN_SDK_VERSION__=21" AR=llvm-ar CARGO_TARGET_ARMV7_LINUX_ANDROIDEABI_LINKER="armv7a-linux-androideabi21-clang" CC="armv7a-linux-androideabi21-clang" cargo build --profile release-smaller --features uniffi --target armv7-linux-androideabi || exit 1
CFLAGS="-D__ANDROID_MIN_SDK_VERSION__=21" AR=llvm-ar CARGO_TARGET_AARCH64_LINUX_ANDROID_LINKER="aarch64-linux-android21-clang" CC="aarch64-linux-android21-clang" cargo build --profile release-smaller --features uniffi --target aarch64-linux-android || exit 1
$UNIFFI_BINDGEN_BIN generate bindings/ldk_node.udl --language kotlin -o "$BINDINGS_DIR"/"$PROJECT_DIR"/lib/src/main/kotlin || exit 1

JNI_LIB_DIR="$BINDINGS_DIR"/"$PROJECT_DIR"/lib/src/main/jniLibs/ 
mkdir -p $JNI_LIB_DIR/x86_64 || exit 1
mkdir -p $JNI_LIB_DIR/armeabi-v7a || exit 1
mkdir -p $JNI_LIB_DIR/arm64-v8a || exit 1
cp $TARGET_DIR/x86_64-linux-android/release-smaller/libldk_node.so $JNI_LIB_DIR/x86_64/ || exit 1
cp $TARGET_DIR/armv7-linux-androideabi/release-smaller/libldk_node.so $JNI_LIB_DIR/armeabi-v7a/ || exit 1
cp $TARGET_DIR/aarch64-linux-android/release-smaller/libldk_node.so $JNI_LIB_DIR/arm64-v8a/ || exit 1

'''
'''--- scripts/uniffi_bindgen_generate_python.sh ---
#!/bin/bash
BINDINGS_DIR="./bindings/python/src/ldk_node"
UNIFFI_BINDGEN_BIN="cargo run --manifest-path bindings/uniffi-bindgen/Cargo.toml"

if [[ "$OSTYPE" == "linux-gnu"* ]]; then
	DYNAMIC_LIB_PATH="./target/release-smaller/libldk_node.so"
else
	DYNAMIC_LIB_PATH="./target/release-smaller/libldk_node.dylib"
fi

cargo build --profile release-smaller --features uniffi || exit 1
$UNIFFI_BINDGEN_BIN generate bindings/ldk_node.udl --language python -o "$BINDINGS_DIR" || exit 1

mkdir -p $BINDINGS_DIR
cp "$DYNAMIC_LIB_PATH" "$BINDINGS_DIR" || exit 1

'''
'''--- scripts/uniffi_bindgen_generate_swift.sh ---
#!/bin/bash
BINDINGS_DIR="./bindings/swift"
UNIFFI_BINDGEN_BIN="cargo run --manifest-path bindings/uniffi-bindgen/Cargo.toml"

cargo build --release || exit 1
$UNIFFI_BINDGEN_BIN generate bindings/ldk_node.udl --language swift -o "$BINDINGS_DIR" || exit 1

mkdir -p $BINDINGS_DIR

# Install rust target toolchains
rustup install 1.73.0
rustup component add rust-src --toolchain 1.73.0
rustup target add aarch64-apple-ios x86_64-apple-ios --toolchain 1.73.0
rustup target add aarch64-apple-ios-sim --toolchain 1.73.0
rustup target add aarch64-apple-darwin x86_64-apple-darwin --toolchain 1.73.0

# Build rust target libs
cargo build --profile release-smaller --features uniffi || exit 1
cargo build --profile release-smaller --features uniffi --target x86_64-apple-darwin || exit 1
cargo build --profile release-smaller --features uniffi --target aarch64-apple-darwin || exit 1
cargo build --profile release-smaller --features uniffi --target x86_64-apple-ios || exit 1
cargo build --profile release-smaller --features uniffi --target aarch64-apple-ios || exit 1
cargo +1.73.0 build --release --features uniffi --target aarch64-apple-ios-sim || exit 1

# Combine ios-sim and apple-darwin (macos) libs for x86_64 and aarch64 (m1)
mkdir -p target/lipo-ios-sim/release-smaller || exit 1
lipo target/aarch64-apple-ios-sim/release/libldk_node.a target/x86_64-apple-ios/release-smaller/libldk_node.a -create -output target/lipo-ios-sim/release-smaller/libldk_node.a || exit 1
mkdir -p target/lipo-macos/release-smaller || exit 1
lipo target/aarch64-apple-darwin/release-smaller/libldk_node.a target/x86_64-apple-darwin/release-smaller/libldk_node.a -create -output target/lipo-macos/release-smaller/libldk_node.a || exit 1

$UNIFFI_BINDGEN_BIN generate bindings/ldk_node.udl --language swift -o "$BINDINGS_DIR" || exit 1

swiftc -module-name LDKNode -emit-library -o "$BINDINGS_DIR"/libldk_node.dylib -emit-module -emit-module-path "$BINDINGS_DIR" -parse-as-library -L ./target/release-smaller -lldk_node -Xcc -fmodule-map-file="$BINDINGS_DIR"/LDKNodeFFI.modulemap "$BINDINGS_DIR"/LDKNode.swift -v || exit 1

# Create xcframework from bindings Swift file and libs
mkdir -p "$BINDINGS_DIR"/Sources/LDKNode || exit 1
mv "$BINDINGS_DIR"/LDKNode.swift "$BINDINGS_DIR"/Sources/LDKNode/LDKNode.swift || exit 1
cp "$BINDINGS_DIR"/LDKNodeFFI.h "$BINDINGS_DIR"/LDKNodeFFI.xcframework/ios-arm64/LDKNodeFFI.framework/Headers || exit 1
cp "$BINDINGS_DIR"/LDKNodeFFI.h "$BINDINGS_DIR"/LDKNodeFFI.xcframework/ios-arm64_x86_64-simulator/LDKNodeFFI.framework/Headers || exit 1
cp "$BINDINGS_DIR"/LDKNodeFFI.h "$BINDINGS_DIR"/LDKNodeFFI.xcframework/macos-arm64_x86_64/LDKNodeFFI.framework/Headers || exit 1
cp target/aarch64-apple-ios/release-smaller/libldk_node.a "$BINDINGS_DIR"/LDKNodeFFI.xcframework/ios-arm64/LDKNodeFFI.framework/LDKNodeFFI || exit 1
cp target/lipo-ios-sim/release-smaller/libldk_node.a "$BINDINGS_DIR"/LDKNodeFFI.xcframework/ios-arm64_x86_64-simulator/LDKNodeFFI.framework/LDKNodeFFI || exit 1
cp target/lipo-macos/release-smaller/libldk_node.a "$BINDINGS_DIR"/LDKNodeFFI.xcframework/macos-arm64_x86_64/LDKNodeFFI.framework/LDKNodeFFI || exit 1
# rm "$BINDINGS_DIR"/LDKNodeFFI.h || exit 1
# rm "$BINDINGS_DIR"/LDKNodeFFI.modulemap || exit 1
echo finished successfully!

'''
'''--- src/builder.rs ---
use crate::event::EventQueue;
use crate::gossip::GossipSource;
use crate::io;
use crate::io::sqlite_store::SqliteStore;
use crate::logger::{log_error, FilesystemLogger, Logger};
use crate::payment_store::PaymentStore;
use crate::peer_store::PeerStore;
use crate::types::{
	ChainMonitor, ChannelManager, FakeMessageRouter, GossipSync, KeysManager, NetworkGraph,
	OnionMessenger, PeerManager,
};
use crate::wallet::Wallet;
use crate::LogLevel;
use crate::{
	Config, Node, BDK_CLIENT_CONCURRENCY, BDK_CLIENT_STOP_GAP, DEFAULT_ESPLORA_SERVER_URL,
	WALLET_KEYS_SEED_LEN,
};

use lightning::chain::{chainmonitor, BestBlock, Watch};
use lightning::ln::channelmanager::{self, ChainParameters, ChannelManagerReadArgs};
use lightning::ln::msgs::{RoutingMessageHandler, SocketAddress};
use lightning::ln::peer_handler::{IgnoringMessageHandler, MessageHandler};
use lightning::routing::router::DefaultRouter;
use lightning::routing::scoring::{
	ProbabilisticScorer, ProbabilisticScoringDecayParameters, ProbabilisticScoringFeeParameters,
};
use lightning::sign::EntropySource;

use lightning::util::config::UserConfig;
use lightning::util::persist::{
	read_channel_monitors, KVStore, CHANNEL_MANAGER_PERSISTENCE_KEY,
	CHANNEL_MANAGER_PERSISTENCE_PRIMARY_NAMESPACE, CHANNEL_MANAGER_PERSISTENCE_SECONDARY_NAMESPACE,
};
use lightning::util::ser::ReadableArgs;

use lightning_persister::fs_store::FilesystemStore;

use lightning_transaction_sync::EsploraSyncClient;

#[cfg(any(vss, vss_test))]
use crate::io::vss_store::VssStore;
use bdk::bitcoin::secp256k1::Secp256k1;
use bdk::blockchain::esplora::EsploraBlockchain;
use bdk::database::SqliteDatabase;
use bdk::template::Bip84;

use bitcoin::Network;

use bip39::Mnemonic;

use bitcoin::BlockHash;

use std::convert::TryInto;
use std::default::Default;
use std::fmt;
use std::fs;
use std::io::Cursor;
use std::path::PathBuf;
use std::sync::{Arc, Mutex, RwLock};
use std::time::SystemTime;

#[derive(Debug, Clone)]
enum ChainDataSourceConfig {
	Esplora(String),
}

#[derive(Debug, Clone)]
enum EntropySourceConfig {
	SeedFile(String),
	SeedBytes([u8; WALLET_KEYS_SEED_LEN]),
	Bip39Mnemonic { mnemonic: Mnemonic, passphrase: Option<String> },
}

#[derive(Debug, Clone)]
enum GossipSourceConfig {
	P2PNetwork,
	RapidGossipSync(String),
}

/// An error encountered during building a [`Node`].
///
/// [`Node`]: crate::Node
#[derive(Debug, Clone)]
pub enum BuildError {
	/// The given seed bytes are invalid, e.g., have invalid length.
	InvalidSeedBytes,
	/// The given seed file is invalid, e.g., has invalid length, or could not be read.
	InvalidSeedFile,
	/// The current system time is invalid, clocks might have gone backwards.
	InvalidSystemTime,
	/// The a read channel monitor is invalid.
	InvalidChannelMonitor,
	/// The given listening addresses are invalid, e.g. too many were passed.
	InvalidListeningAddresses,
	/// We failed to read data from the [`KVStore`].
	ReadFailed,
	/// We failed to write data to the [`KVStore`].
	WriteFailed,
	/// We failed to access the given `storage_dir_path`.
	StoragePathAccessFailed,
	/// We failed to setup our [`KVStore`].
	KVStoreSetupFailed,
	/// We failed to setup the onchain wallet.
	WalletSetupFailed,
	/// We failed to setup the logger.
	LoggerSetupFailed,
}

impl fmt::Display for BuildError {
	fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
		match *self {
			Self::InvalidSeedBytes => write!(f, "Given seed bytes are invalid."),
			Self::InvalidSeedFile => write!(f, "Given seed file is invalid or could not be read."),
			Self::InvalidSystemTime => {
				write!(f, "System time is invalid. Clocks might have gone back in time.")
			}
			Self::InvalidChannelMonitor => {
				write!(f, "Failed to watch a deserialized ChannelMonitor")
			}
			Self::InvalidListeningAddresses => write!(f, "Given listening addresses are invalid."),
			Self::ReadFailed => write!(f, "Failed to read from store."),
			Self::WriteFailed => write!(f, "Failed to write to store."),
			Self::StoragePathAccessFailed => write!(f, "Failed to access the given storage path."),
			Self::KVStoreSetupFailed => write!(f, "Failed to setup KVStore."),
			Self::WalletSetupFailed => write!(f, "Failed to setup onchain wallet."),
			Self::LoggerSetupFailed => write!(f, "Failed to setup the logger."),
		}
	}
}

impl std::error::Error for BuildError {}

/// A builder for an [`Node`] instance, allowing to set some configuration and module choices from
/// the getgo.
///
/// ### Defaults
/// - Wallet entropy is sourced from a `keys_seed` file located under [`Config::storage_dir_path`]
/// - Chain data is sourced from the Esplora endpoint `https://blockstream.info/api`
/// - Gossip data is sourced via the peer-to-peer network
#[derive(Debug)]
pub struct NodeBuilder {
	config: Config,
	entropy_source_config: Option<EntropySourceConfig>,
	chain_data_source_config: Option<ChainDataSourceConfig>,
	gossip_source_config: Option<GossipSourceConfig>,
}

impl NodeBuilder {
	/// Creates a new builder instance with the default configuration.
	pub fn new() -> Self {
		let config = Config::default();
		let entropy_source_config = None;
		let chain_data_source_config = None;
		let gossip_source_config = None;
		Self { config, entropy_source_config, chain_data_source_config, gossip_source_config }
	}

	/// Creates a new builder instance from an [`Config`].
	pub fn from_config(config: Config) -> Self {
		let config = config;
		let entropy_source_config = None;
		let chain_data_source_config = None;
		let gossip_source_config = None;
		Self { config, entropy_source_config, chain_data_source_config, gossip_source_config }
	}

	/// Configures the [`Node`] instance to source its wallet entropy from a seed file on disk.
	///
	/// If the given file does not exist a new random seed file will be generated and
	/// stored at the given location.
	pub fn set_entropy_seed_path(&mut self, seed_path: String) -> &mut Self {
		self.entropy_source_config = Some(EntropySourceConfig::SeedFile(seed_path));
		self
	}

	/// Configures the [`Node`] instance to source its wallet entropy from the given 64 seed bytes.
	pub fn set_entropy_seed_bytes(&mut self, seed_bytes: Vec<u8>) -> Result<&mut Self, BuildError> {
		if seed_bytes.len() != WALLET_KEYS_SEED_LEN {
			return Err(BuildError::InvalidSeedBytes);
		}
		let mut bytes = [0u8; WALLET_KEYS_SEED_LEN];
		bytes.copy_from_slice(&seed_bytes);
		self.entropy_source_config = Some(EntropySourceConfig::SeedBytes(bytes));
		Ok(self)
	}

	/// Configures the [`Node`] instance to source its wallet entropy from a [BIP 39] mnemonic.
	///
	/// [BIP 39]: https://github.com/bitcoin/bips/blob/master/bip-0039.mediawiki
	pub fn set_entropy_bip39_mnemonic(
		&mut self, mnemonic: Mnemonic, passphrase: Option<String>,
	) -> &mut Self {
		self.entropy_source_config =
			Some(EntropySourceConfig::Bip39Mnemonic { mnemonic, passphrase });
		self
	}

	/// Configures the [`Node`] instance to source its chain data from the given Esplora server.
	pub fn set_esplora_server(&mut self, esplora_server_url: String) -> &mut Self {
		self.chain_data_source_config = Some(ChainDataSourceConfig::Esplora(esplora_server_url));
		self
	}

	/// Configures the [`Node`] instance to source its gossip data from the Lightning peer-to-peer
	/// network.
	pub fn set_gossip_source_p2p(&mut self) -> &mut Self {
		self.gossip_source_config = Some(GossipSourceConfig::P2PNetwork);
		self
	}

	/// Configures the [`Node`] instance to source its gossip data from the given RapidGossipSync
	/// server.
	pub fn set_gossip_source_rgs(&mut self, rgs_server_url: String) -> &mut Self {
		self.gossip_source_config = Some(GossipSourceConfig::RapidGossipSync(rgs_server_url));
		self
	}

	/// Sets the used storage directory path.
	pub fn set_storage_dir_path(&mut self, storage_dir_path: String) -> &mut Self {
		self.config.storage_dir_path = storage_dir_path;
		self
	}

	/// Sets the log dir path if logs need to live separate from the storage directory path.
	pub fn set_log_dir_path(&mut self, log_dir_path: String) -> &mut Self {
		self.config.log_dir_path = Some(log_dir_path);
		self
	}

	/// Sets the Bitcoin network used.
	pub fn set_network(&mut self, network: Network) -> &mut Self {
		self.config.network = network;
		self
	}

	/// Sets the IP address and TCP port on which [`Node`] will listen for incoming network connections.
	pub fn set_listening_addresses(
		&mut self, listening_addresses: Vec<SocketAddress>,
	) -> Result<&mut Self, BuildError> {
		if listening_addresses.len() > 100 {
			return Err(BuildError::InvalidListeningAddresses);
		}

		self.config.listening_addresses = Some(listening_addresses);
		Ok(self)
	}

	/// Sets the level at which [`Node`] will log messages.
	pub fn set_log_level(&mut self, level: LogLevel) -> &mut Self {
		self.config.log_level = level;
		self
	}

	/// Builds a [`Node`] instance with a [`SqliteStore`] backend and according to the options
	/// previously configured.
	pub fn build(&self) -> Result<Node<SqliteStore>, BuildError> {
		let storage_dir_path = self.config.storage_dir_path.clone();
		fs::create_dir_all(storage_dir_path.clone())
			.map_err(|_| BuildError::StoragePathAccessFailed)?;
		let kv_store = Arc::new(
			SqliteStore::new(
				storage_dir_path.into(),
				Some(io::sqlite_store::SQLITE_DB_FILE_NAME.to_string()),
				Some(io::sqlite_store::KV_TABLE_NAME.to_string()),
			)
			.map_err(|_| BuildError::KVStoreSetupFailed)?,
		);
		self.build_with_store(kv_store)
	}

	/// Builds a [`Node`] instance with a [`FilesystemStore`] backend and according to the options
	/// previously configured.
	pub fn build_with_fs_store(&self) -> Result<Node<FilesystemStore>, BuildError> {
		let mut storage_dir_path: PathBuf = self.config.storage_dir_path.clone().into();
		storage_dir_path.push("fs_store");

		fs::create_dir_all(storage_dir_path.clone())
			.map_err(|_| BuildError::StoragePathAccessFailed)?;
		let kv_store = Arc::new(FilesystemStore::new(storage_dir_path));
		self.build_with_store(kv_store)
	}

	/// Builds a [`Node`] instance with a [`VssStore`] backend and according to the options
	/// previously configured.
	#[cfg(any(vss, vss_test))]
	pub fn build_with_vss_store(
		&self, url: &str, store_id: String,
	) -> Result<Node<VssStore>, BuildError> {
		let vss = Arc::new(VssStore::new(url, store_id));
		self.build_with_store(vss)
	}

	/// Builds a [`Node`] instance according to the options previously configured.
	pub fn build_with_store<K: KVStore + Sync + Send + 'static>(
		&self, kv_store: Arc<K>,
	) -> Result<Node<K>, BuildError> {
		let logger = setup_logger(&self.config)?;
		let seed_bytes = seed_bytes_from_config(
			&self.config,
			self.entropy_source_config.as_ref(),
			Arc::clone(&logger),
		)?;
		let config = Arc::new(self.config.clone());

		build_with_store_internal(
			config,
			self.chain_data_source_config.as_ref(),
			self.gossip_source_config.as_ref(),
			seed_bytes,
			logger,
			kv_store,
		)
	}
}

/// A builder for an [`Node`] instance, allowing to set some configuration and module choices from
/// the getgo.
///
/// ### Defaults
/// - Wallet entropy is sourced from a `keys_seed` file located under [`Config::storage_dir_path`]
/// - Chain data is sourced from the Esplora endpoint `https://blockstream.info/api`
/// - Gossip data is sourced via the peer-to-peer network
#[derive(Debug)]
#[cfg(feature = "uniffi")]
pub struct ArcedNodeBuilder {
	inner: RwLock<NodeBuilder>,
}

#[cfg(feature = "uniffi")]
impl ArcedNodeBuilder {
	/// Creates a new builder instance with the default configuration.
	pub fn new() -> Self {
		let inner = RwLock::new(NodeBuilder::new());
		Self { inner }
	}

	/// Creates a new builder instance from an [`Config`].
	pub fn from_config(config: Config) -> Self {
		let inner = RwLock::new(NodeBuilder::from_config(config));
		Self { inner }
	}

	/// Configures the [`Node`] instance to source its wallet entropy from a seed file on disk.
	///
	/// If the given file does not exist a new random seed file will be generated and
	/// stored at the given location.
	pub fn set_entropy_seed_path(&self, seed_path: String) {
		self.inner.write().unwrap().set_entropy_seed_path(seed_path);
	}

	/// Configures the [`Node`] instance to source its wallet entropy from the given 64 seed bytes.
	///
	/// **Note:** Panics if the length of the given `seed_bytes` differs from 64.
	pub fn set_entropy_seed_bytes(&self, seed_bytes: Vec<u8>) -> Result<(), BuildError> {
		self.inner.write().unwrap().set_entropy_seed_bytes(seed_bytes).map(|_| ())
	}

	/// Configures the [`Node`] instance to source its wallet entropy from a [BIP 39] mnemonic.
	///
	/// [BIP 39]: https://github.com/bitcoin/bips/blob/master/bip-0039.mediawiki
	pub fn set_entropy_bip39_mnemonic(&self, mnemonic: Mnemonic, passphrase: Option<String>) {
		self.inner.write().unwrap().set_entropy_bip39_mnemonic(mnemonic, passphrase);
	}

	/// Configures the [`Node`] instance to source its chain data from the given Esplora server.
	pub fn set_esplora_server(&self, esplora_server_url: String) {
		self.inner.write().unwrap().set_esplora_server(esplora_server_url);
	}

	/// Configures the [`Node`] instance to source its gossip data from the Lightning peer-to-peer
	/// network.
	pub fn set_gossip_source_p2p(&self) {
		self.inner.write().unwrap().set_gossip_source_p2p();
	}

	/// Configures the [`Node`] instance to source its gossip data from the given RapidGossipSync
	/// server.
	pub fn set_gossip_source_rgs(&self, rgs_server_url: String) {
		self.inner.write().unwrap().set_gossip_source_rgs(rgs_server_url);
	}

	/// Sets the used storage directory path.
	pub fn set_storage_dir_path(&self, storage_dir_path: String) {
		self.inner.write().unwrap().set_storage_dir_path(storage_dir_path);
	}

	/// Sets the log dir path if logs need to live separate from the storage directory path.
	pub fn set_log_dir_path(&self, log_dir_path: String) {
		self.inner.write().unwrap().set_log_dir_path(log_dir_path);
	}

	/// Sets the Bitcoin network used.
	pub fn set_network(&self, network: Network) {
		self.inner.write().unwrap().set_network(network);
	}

	/// Sets the IP address and TCP port on which [`Node`] will listen for incoming network connections.
	pub fn set_listening_addresses(
		&self, listening_addresses: Vec<SocketAddress>,
	) -> Result<(), BuildError> {
		self.inner.write().unwrap().set_listening_addresses(listening_addresses).map(|_| ())
	}

	/// Sets the level at which [`Node`] will log messages.
	pub fn set_log_level(&self, level: LogLevel) {
		self.inner.write().unwrap().set_log_level(level);
	}

	/// Builds a [`Node`] instance with a [`SqliteStore`] backend and according to the options
	/// previously configured.
	pub fn build(&self) -> Result<Arc<Node<SqliteStore>>, BuildError> {
		self.inner.read().unwrap().build().map(Arc::new)
	}

	/// Builds a [`Node`] instance with a [`FilesystemStore`] backend and according to the options
	/// previously configured.
	pub fn build_with_fs_store(&self) -> Result<Arc<Node<FilesystemStore>>, BuildError> {
		self.inner.read().unwrap().build_with_fs_store().map(Arc::new)
	}

	/// Builds a [`Node`] instance according to the options previously configured.
	pub fn build_with_store<K: KVStore + Sync + Send + 'static>(
		&self, kv_store: Arc<K>,
	) -> Result<Arc<Node<K>>, BuildError> {
		self.inner.read().unwrap().build_with_store(kv_store).map(Arc::new)
	}
}

/// Builds a [`Node`] instance according to the options previously configured.
fn build_with_store_internal<K: KVStore + Sync + Send + 'static>(
	config: Arc<Config>, chain_data_source_config: Option<&ChainDataSourceConfig>,
	gossip_source_config: Option<&GossipSourceConfig>, seed_bytes: [u8; 64],
	logger: Arc<FilesystemLogger>, kv_store: Arc<K>,
) -> Result<Node<K>, BuildError> {
	// Initialize the on-chain wallet and chain access
	let xprv = bitcoin::util::bip32::ExtendedPrivKey::new_master(config.network, &seed_bytes)
		.map_err(|e| {
			log_error!(logger, "Failed to derive master secret: {}", e);
			BuildError::InvalidSeedBytes
		})?;

	let wallet_name = bdk::wallet::wallet_name_from_descriptor(
		Bip84(xprv, bdk::KeychainKind::External),
		Some(Bip84(xprv, bdk::KeychainKind::Internal)),
		config.network,
		&Secp256k1::new(),
	)
	.map_err(|e| {
		log_error!(logger, "Failed to derive wallet name: {}", e);
		BuildError::WalletSetupFailed
	})?;

	let database_path = format!("{}/bdk_wallet_{}.sqlite", config.storage_dir_path, wallet_name);
	let database = SqliteDatabase::new(database_path);

	let bdk_wallet = bdk::Wallet::new(
		Bip84(xprv, bdk::KeychainKind::External),
		Some(Bip84(xprv, bdk::KeychainKind::Internal)),
		config.network,
		database,
	)
	.map_err(|e| {
		log_error!(logger, "Failed to set up wallet: {}", e);
		BuildError::WalletSetupFailed
	})?;

	let (blockchain, tx_sync) = match chain_data_source_config {
		Some(ChainDataSourceConfig::Esplora(server_url)) => {
			let tx_sync = Arc::new(EsploraSyncClient::new(server_url.clone(), Arc::clone(&logger)));
			let blockchain =
				EsploraBlockchain::from_client(tx_sync.client().clone(), BDK_CLIENT_STOP_GAP)
					.with_concurrency(BDK_CLIENT_CONCURRENCY);
			(blockchain, tx_sync)
		}
		None => {
			// Default to Esplora client.
			let server_url = DEFAULT_ESPLORA_SERVER_URL.to_string();
			let tx_sync = Arc::new(EsploraSyncClient::new(server_url, Arc::clone(&logger)));
			let blockchain =
				EsploraBlockchain::from_client(tx_sync.client().clone(), BDK_CLIENT_STOP_GAP)
					.with_concurrency(BDK_CLIENT_CONCURRENCY);
			(blockchain, tx_sync)
		}
	};

	let runtime = Arc::new(RwLock::new(None));
	let wallet =
		Arc::new(Wallet::new(blockchain, bdk_wallet, Arc::clone(&runtime), Arc::clone(&logger)));

	// Initialize the ChainMonitor
	let chain_monitor: Arc<ChainMonitor<K>> = Arc::new(chainmonitor::ChainMonitor::new(
		Some(Arc::clone(&tx_sync)),
		Arc::clone(&wallet),
		Arc::clone(&logger),
		Arc::clone(&wallet),
		Arc::clone(&kv_store),
	));

	// Initialize the KeysManager
	let cur_time = SystemTime::now().duration_since(SystemTime::UNIX_EPOCH).map_err(|e| {
		log_error!(logger, "Failed to get current time: {}", e);
		BuildError::InvalidSystemTime
	})?;

	let ldk_seed_bytes: [u8; 32] = xprv.private_key.secret_bytes();
	let keys_manager = Arc::new(KeysManager::new(
		&ldk_seed_bytes,
		cur_time.as_secs(),
		cur_time.subsec_nanos(),
		Arc::clone(&wallet),
		Arc::clone(&logger),
	));

	// Initialize the network graph, scorer, and router
	let network_graph =
		match io::utils::read_network_graph(Arc::clone(&kv_store), Arc::clone(&logger)) {
			Ok(graph) => Arc::new(graph),
			Err(e) => {
				if e.kind() == std::io::ErrorKind::NotFound {
					Arc::new(NetworkGraph::new(config.network, Arc::clone(&logger)))
				} else {
					return Err(BuildError::ReadFailed);
				}
			}
		};

	let scorer = match io::utils::read_scorer(
		Arc::clone(&kv_store),
		Arc::clone(&network_graph),
		Arc::clone(&logger),
	) {
		Ok(scorer) => Arc::new(Mutex::new(scorer)),
		Err(e) => {
			if e.kind() == std::io::ErrorKind::NotFound {
				let params = ProbabilisticScoringDecayParameters::default();
				Arc::new(Mutex::new(ProbabilisticScorer::new(
					params,
					Arc::clone(&network_graph),
					Arc::clone(&logger),
				)))
			} else {
				return Err(BuildError::ReadFailed);
			}
		}
	};

	let scoring_fee_params = ProbabilisticScoringFeeParameters::default();
	let router = Arc::new(DefaultRouter::new(
		Arc::clone(&network_graph),
		Arc::clone(&logger),
		keys_manager.get_secure_random_bytes(),
		Arc::clone(&scorer),
		scoring_fee_params,
	));

	// Read ChannelMonitor state from store
	let mut channel_monitors = match read_channel_monitors(
		Arc::clone(&kv_store),
		Arc::clone(&keys_manager),
		Arc::clone(&keys_manager),
	) {
		Ok(monitors) => monitors,
		Err(e) => {
			if e.kind() == std::io::ErrorKind::NotFound {
				Vec::new()
			} else {
				log_error!(logger, "Failed to read channel monitors: {}", e.to_string());
				return Err(BuildError::ReadFailed);
			}
		}
	};

	// Initialize the ChannelManager
	let mut user_config = UserConfig::default();
	user_config.channel_handshake_limits.force_announced_channel_preference = false;

	if !config.trusted_peers_0conf.is_empty() {
		// Manually accept inbound channels if we expect 0conf channel requests, avoid
		// generating the events otherwise.
		user_config.manually_accept_inbound_channels = true;
	}
	let channel_manager = {
		if let Ok(res) = kv_store.read(
			CHANNEL_MANAGER_PERSISTENCE_PRIMARY_NAMESPACE,
			CHANNEL_MANAGER_PERSISTENCE_SECONDARY_NAMESPACE,
			CHANNEL_MANAGER_PERSISTENCE_KEY,
		) {
			let mut reader = Cursor::new(res);
			let channel_monitor_references =
				channel_monitors.iter_mut().map(|(_, chanmon)| chanmon).collect();
			let read_args = ChannelManagerReadArgs::new(
				Arc::clone(&keys_manager),
				Arc::clone(&keys_manager),
				Arc::clone(&keys_manager),
				Arc::clone(&wallet),
				Arc::clone(&chain_monitor),
				Arc::clone(&wallet),
				Arc::clone(&router),
				Arc::clone(&logger),
				user_config,
				channel_monitor_references,
			);
			let (_hash, channel_manager) =
				<(BlockHash, ChannelManager<K>)>::read(&mut reader, read_args).map_err(|e| {
					log_error!(logger, "Failed to read channel manager from KVStore: {}", e);
					BuildError::ReadFailed
				})?;
			channel_manager
		} else {
			// We're starting a fresh node.
			let genesis_block_hash =
				bitcoin::blockdata::constants::genesis_block(config.network).block_hash();

			let chain_params = ChainParameters {
				network: config.network,
				best_block: BestBlock::new(genesis_block_hash, 0),
			};
			channelmanager::ChannelManager::new(
				Arc::clone(&wallet),
				Arc::clone(&chain_monitor),
				Arc::clone(&wallet),
				Arc::clone(&router),
				Arc::clone(&logger),
				Arc::clone(&keys_manager),
				Arc::clone(&keys_manager),
				Arc::clone(&keys_manager),
				user_config,
				chain_params,
				cur_time.as_secs() as u32,
			)
		}
	};

	let channel_manager = Arc::new(channel_manager);

	// Give ChannelMonitors to ChainMonitor
	for (_blockhash, channel_monitor) in channel_monitors.into_iter() {
		let funding_outpoint = channel_monitor.get_funding_txo().0;
		chain_monitor.watch_channel(funding_outpoint, channel_monitor).map_err(|e| {
			log_error!(logger, "Failed to watch channel monitor: {:?}", e);
			BuildError::InvalidChannelMonitor
		})?;
	}

	// Initialize the PeerManager
	let onion_messenger: Arc<OnionMessenger> = Arc::new(OnionMessenger::new(
		Arc::clone(&keys_manager),
		Arc::clone(&keys_manager),
		Arc::clone(&logger),
		Arc::new(FakeMessageRouter {}),
		IgnoringMessageHandler {},
		IgnoringMessageHandler {},
	));
	let ephemeral_bytes: [u8; 32] = keys_manager.get_secure_random_bytes();

	// Initialize the GossipSource
	// Use the configured gossip source, if the user set one, otherwise default to P2PNetwork.
	let gossip_source_config = gossip_source_config.unwrap_or(&GossipSourceConfig::P2PNetwork);

	let gossip_source = match gossip_source_config {
		GossipSourceConfig::P2PNetwork => {
			let p2p_source =
				Arc::new(GossipSource::new_p2p(Arc::clone(&network_graph), Arc::clone(&logger)));

			// Reset the RGS sync timestamp in case we somehow switch gossip sources
			io::utils::write_latest_rgs_sync_timestamp(
				0,
				Arc::clone(&kv_store),
				Arc::clone(&logger),
			)
			.map_err(|e| {
				log_error!(logger, "Failed writing to store: {}", e);
				BuildError::WriteFailed
			})?;
			p2p_source
		}
		GossipSourceConfig::RapidGossipSync(rgs_server) => {
			let latest_sync_timestamp = io::utils::read_latest_rgs_sync_timestamp(
				Arc::clone(&kv_store),
				Arc::clone(&logger),
			)
			.unwrap_or(0);
			Arc::new(GossipSource::new_rgs(
				rgs_server.clone(),
				latest_sync_timestamp,
				Arc::clone(&network_graph),
				Arc::clone(&logger),
			))
		}
	};

	let msg_handler = match gossip_source.as_gossip_sync() {
		GossipSync::P2P(p2p_gossip_sync) => MessageHandler {
			chan_handler: Arc::clone(&channel_manager),
			route_handler: Arc::clone(&p2p_gossip_sync)
				as Arc<dyn RoutingMessageHandler + Sync + Send>,
			onion_message_handler: onion_messenger,
			custom_message_handler: IgnoringMessageHandler {},
		},
		GossipSync::Rapid(_) => MessageHandler {
			chan_handler: Arc::clone(&channel_manager),
			route_handler: Arc::new(IgnoringMessageHandler {})
				as Arc<dyn RoutingMessageHandler + Sync + Send>,
			onion_message_handler: onion_messenger,
			custom_message_handler: IgnoringMessageHandler {},
		},
		GossipSync::None => {
			unreachable!("We must always have a gossip sync!");
		}
	};

	let cur_time = SystemTime::now().duration_since(SystemTime::UNIX_EPOCH).map_err(|e| {
		log_error!(logger, "Failed to get current time: {}", e);
		BuildError::InvalidSystemTime
	})?;

	let peer_manager = Arc::new(PeerManager::new(
		msg_handler,
		cur_time.as_secs().try_into().map_err(|e| {
			log_error!(logger, "Failed to get current time: {}", e);
			BuildError::InvalidSystemTime
		})?,
		&ephemeral_bytes,
		Arc::clone(&logger),
		Arc::clone(&keys_manager),
	));

	// Init payment info storage
	let payment_store = match io::utils::read_payments(Arc::clone(&kv_store), Arc::clone(&logger)) {
		Ok(payments) => {
			Arc::new(PaymentStore::new(payments, Arc::clone(&kv_store), Arc::clone(&logger)))
		}
		Err(_) => {
			return Err(BuildError::ReadFailed);
		}
	};

	let event_queue = match io::utils::read_event_queue(Arc::clone(&kv_store), Arc::clone(&logger))
	{
		Ok(event_queue) => Arc::new(event_queue),
		Err(e) => {
			if e.kind() == std::io::ErrorKind::NotFound {
				Arc::new(EventQueue::new(Arc::clone(&kv_store), Arc::clone(&logger)))
			} else {
				return Err(BuildError::ReadFailed);
			}
		}
	};

	let peer_store = match io::utils::read_peer_info(Arc::clone(&kv_store), Arc::clone(&logger)) {
		Ok(peer_store) => Arc::new(peer_store),
		Err(e) => {
			if e.kind() == std::io::ErrorKind::NotFound {
				Arc::new(PeerStore::new(Arc::clone(&kv_store), Arc::clone(&logger)))
			} else {
				return Err(BuildError::ReadFailed);
			}
		}
	};

	let (stop_sender, stop_receiver) = tokio::sync::watch::channel(());

	Ok(Node {
		runtime,
		stop_sender,
		stop_receiver,
		config,
		wallet,
		tx_sync,
		event_queue,
		channel_manager,
		chain_monitor,
		peer_manager,
		keys_manager,
		network_graph,
		gossip_source,
		kv_store,
		logger,
		_router: router,
		scorer,
		peer_store,
		payment_store,
	})
}

fn setup_logger(config: &Config) -> Result<Arc<FilesystemLogger>, BuildError> {
	let log_dir = match &config.log_dir_path {
		Some(log_dir) => String::from(log_dir),
		None => config.storage_dir_path.clone() + "/logs",
	};

	Ok(Arc::new(
		FilesystemLogger::new(log_dir, config.log_level)
			.map_err(|_| BuildError::LoggerSetupFailed)?,
	))
}

fn seed_bytes_from_config(
	config: &Config, entropy_source_config: Option<&EntropySourceConfig>,
	logger: Arc<FilesystemLogger>,
) -> Result<[u8; 64], BuildError> {
	match entropy_source_config {
		Some(EntropySourceConfig::SeedBytes(bytes)) => Ok(bytes.clone()),
		Some(EntropySourceConfig::SeedFile(seed_path)) => {
			Ok(io::utils::read_or_generate_seed_file(&seed_path, Arc::clone(&logger))
				.map_err(|_| BuildError::InvalidSeedFile)?)
		}
		Some(EntropySourceConfig::Bip39Mnemonic { mnemonic, passphrase }) => match passphrase {
			Some(passphrase) => Ok(mnemonic.to_seed(passphrase)),
			None => Ok(mnemonic.to_seed("")),
		},
		None => {
			// Default to read or generate from the default location generate a seed file.
			let seed_path = format!("{}/keys_seed", config.storage_dir_path);
			Ok(io::utils::read_or_generate_seed_file(&seed_path, Arc::clone(&logger))
				.map_err(|_| BuildError::InvalidSeedFile)?)
		}
	}
}

'''
'''--- src/error.rs ---
use std::fmt;

#[derive(Debug, PartialEq, Eq)]
/// An error that possibly needs to be handled by the user.
pub enum Error {
	/// Returned when trying to start [`crate::Node`] while it is already running.
	AlreadyRunning,
	/// Returned when trying to stop [`crate::Node`] while it is not running.
	NotRunning,
	/// An on-chain transaction could not be created.
	OnchainTxCreationFailed,
	/// A network connection has been closed.
	ConnectionFailed,
	/// Invoice creation failed.
	InvoiceCreationFailed,
	/// Sending a payment has failed.
	PaymentSendingFailed,
	/// Sending a payment probe has failed.
	ProbeSendingFailed,
	/// A channel could not be opened.
	ChannelCreationFailed,
	/// A channel could not be closed.
	ChannelClosingFailed,
	/// A channel config could not be updated.
	ChannelConfigUpdateFailed,
	/// Persistence failed.
	PersistenceFailed,
	/// A wallet operation failed.
	WalletOperationFailed,
	/// A signing operation for transaction failed.
	OnchainTxSigningFailed,
	/// A signing operation for message failed.
	MessageSigningFailed,
	/// A transaction sync operation failed.
	TxSyncFailed,
	/// A gossip updating operation failed.
	GossipUpdateFailed,
	/// The given address is invalid.
	InvalidAddress,
	/// The given network address is invalid.
	InvalidSocketAddress,
	/// The given public key is invalid.
	InvalidPublicKey,
	/// The given secret key is invalid.
	InvalidSecretKey,
	/// The given payment hash is invalid.
	InvalidPaymentHash,
	/// The given payment preimage is invalid.
	InvalidPaymentPreimage,
	/// The given payment secret is invalid.
	InvalidPaymentSecret,
	/// The given amount is invalid.
	InvalidAmount,
	/// The given invoice is invalid.
	InvalidInvoice,
	/// The given channel ID is invalid.
	InvalidChannelId,
	/// The given network is invalid.
	InvalidNetwork,
	/// A payment with the given hash has already been intiated.
	DuplicatePayment,
	/// There are insufficient funds to complete the given operation.
	InsufficientFunds,
}

impl fmt::Display for Error {
	fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
		match *self {
			Self::AlreadyRunning => write!(f, "Node is already running."),
			Self::NotRunning => write!(f, "Node is not running."),
			Self::OnchainTxCreationFailed => {
				write!(f, "On-chain transaction could not be created.")
			}
			Self::ConnectionFailed => write!(f, "Network connection closed."),
			Self::InvoiceCreationFailed => write!(f, "Failed to create invoice."),
			Self::PaymentSendingFailed => write!(f, "Failed to send the given payment."),
			Self::ProbeSendingFailed => write!(f, "Failed to send the given payment probe."),
			Self::ChannelCreationFailed => write!(f, "Failed to create channel."),
			Self::ChannelClosingFailed => write!(f, "Failed to close channel."),
			Self::ChannelConfigUpdateFailed => write!(f, "Failed to update channel config."),
			Self::PersistenceFailed => write!(f, "Failed to persist data."),
			Self::WalletOperationFailed => write!(f, "Failed to conduct wallet operation."),
			Self::OnchainTxSigningFailed => write!(f, "Failed to sign given transaction."),
			Self::MessageSigningFailed => write!(f, "Failed to sign given message."),
			Self::TxSyncFailed => write!(f, "Failed to sync transactions."),
			Self::GossipUpdateFailed => write!(f, "Failed to update gossip data."),
			Self::InvalidAddress => write!(f, "The given address is invalid."),
			Self::InvalidSocketAddress => write!(f, "The given network address is invalid."),
			Self::InvalidPublicKey => write!(f, "The given public key is invalid."),
			Self::InvalidSecretKey => write!(f, "The given secret key is invalid."),
			Self::InvalidPaymentHash => write!(f, "The given payment hash is invalid."),
			Self::InvalidPaymentPreimage => write!(f, "The given payment preimage is invalid."),
			Self::InvalidPaymentSecret => write!(f, "The given payment secret is invalid."),
			Self::InvalidAmount => write!(f, "The given amount is invalid."),
			Self::InvalidInvoice => write!(f, "The given invoice is invalid."),
			Self::InvalidChannelId => write!(f, "The given channel ID is invalid."),
			Self::InvalidNetwork => write!(f, "The given network is invalid."),
			Self::DuplicatePayment => {
				write!(f, "A payment with the given hash has already been initiated.")
			}
			Self::InsufficientFunds => {
				write!(f, "There are insufficient funds to complete the given operation.")
			}
		}
	}
}

impl std::error::Error for Error {}

impl From<bdk::Error> for Error {
	fn from(e: bdk::Error) -> Self {
		match e {
			bdk::Error::Signer(_) => Self::OnchainTxSigningFailed,
			_ => Self::WalletOperationFailed,
		}
	}
}

impl From<lightning_transaction_sync::TxSyncError> for Error {
	fn from(_e: lightning_transaction_sync::TxSyncError) -> Self {
		Self::TxSyncFailed
	}
}

'''
'''--- src/event.rs ---
use crate::{
	hex_utils, ChannelManager, Config, Error, KeysManager, NetworkGraph, UserChannelId, Wallet,
};

use crate::payment_store::{
	PaymentDetails, PaymentDetailsUpdate, PaymentDirection, PaymentStatus, PaymentStore,
};

use crate::io::{
	EVENT_QUEUE_PERSISTENCE_KEY, EVENT_QUEUE_PERSISTENCE_PRIMARY_NAMESPACE,
	EVENT_QUEUE_PERSISTENCE_SECONDARY_NAMESPACE,
};
use crate::logger::{log_debug, log_error, log_info, Logger};

use lightning::chain::chaininterface::{BroadcasterInterface, ConfirmationTarget, FeeEstimator};
use lightning::events::Event as LdkEvent;
use lightning::events::PaymentPurpose;
use lightning::impl_writeable_tlv_based_enum;
use lightning::ln::{ChannelId, PaymentHash};
use lightning::routing::gossip::NodeId;
use lightning::util::errors::APIError;
use lightning::util::persist::KVStore;
use lightning::util::ser::{Readable, ReadableArgs, Writeable, Writer};

use bitcoin::secp256k1::{PublicKey, Secp256k1};
use bitcoin::{LockTime, OutPoint, PackedLockTime};
use rand::{thread_rng, Rng};
use std::collections::VecDeque;
use std::ops::Deref;
use std::sync::{Arc, Condvar, Mutex, RwLock};
use std::time::Duration;

/// An event emitted by [`Node`], which should be handled by the user.
///
/// [`Node`]: [`crate::Node`]
#[derive(Debug, Clone, PartialEq, Eq)]
pub enum Event {
	/// A sent payment was successful.
	PaymentSuccessful {
		/// The hash of the payment.
		payment_hash: PaymentHash,
	},
	/// A sent payment has failed.
	PaymentFailed {
		/// The hash of the payment.
		payment_hash: PaymentHash,
	},
	/// A payment has been received.
	PaymentReceived {
		/// The hash of the payment.
		payment_hash: PaymentHash,
		/// The value, in thousandths of a satoshi, that has been received.
		amount_msat: u64,
	},
	/// A channel has been created and is pending confirmation on-chain.
	ChannelPending {
		/// The `channel_id` of the channel.
		channel_id: ChannelId,
		/// The `user_channel_id` of the channel.
		user_channel_id: UserChannelId,
		/// The `temporary_channel_id` this channel used to be known by during channel establishment.
		former_temporary_channel_id: ChannelId,
		/// The `node_id` of the channel counterparty.
		counterparty_node_id: PublicKey,
		/// The outpoint of the channel's funding transaction.
		funding_txo: OutPoint,
	},
	/// A channel is ready to be used.
	ChannelReady {
		/// The `channel_id` of the channel.
		channel_id: ChannelId,
		/// The `user_channel_id` of the channel.
		user_channel_id: UserChannelId,
		/// The `node_id` of the channel counterparty.
		///
		/// This will be `None` for events serialized by LDK Node XXX TODO and prior.
		counterparty_node_id: Option<PublicKey>,
	},
	/// A channel has been closed.
	ChannelClosed {
		/// The `channel_id` of the channel.
		channel_id: ChannelId,
		/// The `user_channel_id` of the channel.
		user_channel_id: UserChannelId,
		/// The `node_id` of the channel counterparty.
		///
		/// This will be `None` for events serialized by LDK Node XXX TODO and prior.
		counterparty_node_id: Option<PublicKey>,
	},
}

impl_writeable_tlv_based_enum!(Event,
	(0, PaymentSuccessful) => {
		(0, payment_hash, required),
	},
	(1, PaymentFailed) => {
		(0, payment_hash, required),
	},
	(2, PaymentReceived) => {
		(0, payment_hash, required),
		(2, amount_msat, required),
	},
	(3, ChannelReady) => {
		(0, channel_id, required),
		(1, counterparty_node_id, option),
		(2, user_channel_id, required),
	},
	(4, ChannelPending) => {
		(0, channel_id, required),
		(2, user_channel_id, required),
		(4, former_temporary_channel_id, required),
		(6, counterparty_node_id, required),
		(8, funding_txo, required),
	},
	(5, ChannelClosed) => {
		(0, channel_id, required),
		(1, counterparty_node_id, option),
		(2, user_channel_id, required),
	};
);

pub struct EventQueue<K: KVStore + Sync + Send, L: Deref>
where
	L::Target: Logger,
{
	queue: Mutex<VecDeque<Event>>,
	notifier: Condvar,
	kv_store: Arc<K>,
	logger: L,
}

impl<K: KVStore + Sync + Send, L: Deref> EventQueue<K, L>
where
	L::Target: Logger,
{
	pub(crate) fn new(kv_store: Arc<K>, logger: L) -> Self {
		let queue: Mutex<VecDeque<Event>> = Mutex::new(VecDeque::new());
		let notifier = Condvar::new();
		Self { queue, notifier, kv_store, logger }
	}

	pub(crate) fn add_event(&self, event: Event) -> Result<(), Error> {
		{
			let mut locked_queue = self.queue.lock().unwrap();
			locked_queue.push_back(event);
			self.persist_queue(&locked_queue)?;
		}

		self.notifier.notify_one();
		Ok(())
	}

	pub(crate) fn next_event(&self) -> Option<Event> {
		let locked_queue = self.queue.lock().unwrap();
		locked_queue.front().map(|e| e.clone())
	}

	pub(crate) fn wait_next_event(&self) -> Event {
		let locked_queue =
			self.notifier.wait_while(self.queue.lock().unwrap(), |queue| queue.is_empty()).unwrap();
		locked_queue.front().unwrap().clone()
	}

	pub(crate) fn event_handled(&self) -> Result<(), Error> {
		{
			let mut locked_queue = self.queue.lock().unwrap();
			locked_queue.pop_front();
			self.persist_queue(&locked_queue)?;
		}
		self.notifier.notify_one();
		Ok(())
	}

	fn persist_queue(&self, locked_queue: &VecDeque<Event>) -> Result<(), Error> {
		let data = EventQueueSerWrapper(locked_queue).encode();
		self.kv_store
			.write(
				EVENT_QUEUE_PERSISTENCE_PRIMARY_NAMESPACE,
				EVENT_QUEUE_PERSISTENCE_SECONDARY_NAMESPACE,
				EVENT_QUEUE_PERSISTENCE_KEY,
				&data,
			)
			.map_err(|e| {
				log_error!(
					self.logger,
					"Write for key {}/{}/{} failed due to: {}",
					EVENT_QUEUE_PERSISTENCE_PRIMARY_NAMESPACE,
					EVENT_QUEUE_PERSISTENCE_SECONDARY_NAMESPACE,
					EVENT_QUEUE_PERSISTENCE_KEY,
					e
				);
				Error::PersistenceFailed
			})?;
		Ok(())
	}
}

impl<K: KVStore + Sync + Send, L: Deref> ReadableArgs<(Arc<K>, L)> for EventQueue<K, L>
where
	L::Target: Logger,
{
	#[inline]
	fn read<R: lightning::io::Read>(
		reader: &mut R, args: (Arc<K>, L),
	) -> Result<Self, lightning::ln::msgs::DecodeError> {
		let (kv_store, logger) = args;
		let read_queue: EventQueueDeserWrapper = Readable::read(reader)?;
		let queue: Mutex<VecDeque<Event>> = Mutex::new(read_queue.0);
		let notifier = Condvar::new();
		Ok(Self { queue, notifier, kv_store, logger })
	}
}

struct EventQueueDeserWrapper(VecDeque<Event>);

impl Readable for EventQueueDeserWrapper {
	fn read<R: lightning::io::Read>(
		reader: &mut R,
	) -> Result<Self, lightning::ln::msgs::DecodeError> {
		let len: u16 = Readable::read(reader)?;
		let mut queue = VecDeque::with_capacity(len as usize);
		for _ in 0..len {
			queue.push_back(Readable::read(reader)?);
		}
		Ok(Self(queue))
	}
}

struct EventQueueSerWrapper<'a>(&'a VecDeque<Event>);

impl Writeable for EventQueueSerWrapper<'_> {
	fn write<W: Writer>(&self, writer: &mut W) -> Result<(), lightning::io::Error> {
		(self.0.len() as u16).write(writer)?;
		for e in self.0.iter() {
			e.write(writer)?;
		}
		Ok(())
	}
}

pub(crate) struct EventHandler<K: KVStore + Sync + Send, L: Deref>
where
	L::Target: Logger,
{
	wallet: Arc<Wallet<bdk::database::SqliteDatabase, L>>,
	event_queue: Arc<EventQueue<K, L>>,
	channel_manager: Arc<ChannelManager<K>>,
	network_graph: Arc<NetworkGraph>,
	keys_manager: Arc<KeysManager>,
	payment_store: Arc<PaymentStore<K, L>>,
	runtime: Arc<RwLock<Option<tokio::runtime::Runtime>>>,
	logger: L,
	config: Arc<Config>,
}

impl<K: KVStore + Sync + Send + 'static, L: Deref> EventHandler<K, L>
where
	L::Target: Logger,
{
	pub fn new(
		wallet: Arc<Wallet<bdk::database::SqliteDatabase, L>>, event_queue: Arc<EventQueue<K, L>>,
		channel_manager: Arc<ChannelManager<K>>, network_graph: Arc<NetworkGraph>,
		keys_manager: Arc<KeysManager>, payment_store: Arc<PaymentStore<K, L>>,
		runtime: Arc<RwLock<Option<tokio::runtime::Runtime>>>, logger: L, config: Arc<Config>,
	) -> Self {
		Self {
			event_queue,
			wallet,
			channel_manager,
			network_graph,
			keys_manager,
			payment_store,
			logger,
			runtime,
			config,
		}
	}

	pub async fn handle_event(&self, event: LdkEvent) {
		match event {
			LdkEvent::FundingGenerationReady {
				temporary_channel_id,
				counterparty_node_id,
				channel_value_satoshis,
				output_script,
				..
			} => {
				// Construct the raw transaction with the output that is paid the amount of the
				// channel.
				let confirmation_target = ConfirmationTarget::NonAnchorChannelFee;

				// We set nLockTime to the current height to discourage fee sniping.
				let cur_height = self.channel_manager.current_best_block().height();
				let locktime = LockTime::from_height(cur_height).unwrap_or(LockTime::ZERO);

				// Sign the final funding transaction and broadcast it.
				match self.wallet.create_funding_transaction(
					output_script,
					channel_value_satoshis,
					confirmation_target,
					locktime,
				) {
					Ok(final_tx) => {
						// Give the funding transaction back to LDK for opening the channel.
						match self.channel_manager.funding_transaction_generated(
							&temporary_channel_id,
							&counterparty_node_id,
							final_tx,
						) {
							Ok(()) => {}
							Err(APIError::APIMisuseError { err }) => {
								log_error!(self.logger, "Panicking due to APIMisuseError: {}", err);
								panic!("APIMisuseError: {}", err);
							}
							Err(APIError::ChannelUnavailable { err }) => {
								log_error!(
									self.logger,
									"Failed to process funding transaction as channel went away before we could fund it: {}",
									err
								)
							}
							Err(err) => {
								log_error!(
									self.logger,
									"Failed to process funding transaction: {:?}",
									err
								)
							}
						}
					}
					Err(err) => {
						log_error!(self.logger, "Failed to create funding transaction: {}", err);
						self.channel_manager
							.force_close_without_broadcasting_txn(
								&temporary_channel_id,
								&counterparty_node_id,
							)
							.unwrap_or_else(|e| {
								log_error!(self.logger, "Failed to force close channel after funding generation failed: {:?}", e);
								panic!(
									"Failed to force close channel after funding generation failed"
								);
							});
					}
				}
			}
			LdkEvent::PaymentClaimable {
				payment_hash,
				purpose,
				amount_msat,
				receiver_node_id: _,
				via_channel_id: _,
				via_user_channel_id: _,
				claim_deadline: _,
				onion_fields: _,
				counterparty_skimmed_fee_msat: _,
			} => {
				if let Some(info) = self.payment_store.get(&payment_hash) {
					if info.status == PaymentStatus::Succeeded {
						log_info!(
							self.logger,
							"Refused duplicate inbound payment from payment hash {} of {}msat",
							hex_utils::to_string(&payment_hash.0),
							amount_msat,
						);
						self.channel_manager.fail_htlc_backwards(&payment_hash);

						let update = PaymentDetailsUpdate {
							status: Some(PaymentStatus::Failed),
							..PaymentDetailsUpdate::new(payment_hash)
						};
						self.payment_store.update(&update).unwrap_or_else(|e| {
							log_error!(self.logger, "Failed to access payment store: {}", e);
							panic!("Failed to access payment store");
						});
						return;
					}
				}

				log_info!(
					self.logger,
					"Received payment from payment hash {} of {}msat",
					hex_utils::to_string(&payment_hash.0),
					amount_msat,
				);
				let payment_preimage = match purpose {
					PaymentPurpose::InvoicePayment { payment_preimage, payment_secret } => {
						if payment_preimage.is_some() {
							payment_preimage
						} else {
							self.channel_manager
								.get_payment_preimage(payment_hash, payment_secret)
								.ok()
						}
					}
					PaymentPurpose::SpontaneousPayment(preimage) => Some(preimage),
				};

				if let Some(preimage) = payment_preimage {
					self.channel_manager.claim_funds(preimage);
				} else {
					log_error!(
						self.logger,
						"Failed to claim payment with hash {}: preimage unknown.",
						hex_utils::to_string(&payment_hash.0),
					);
					self.channel_manager.fail_htlc_backwards(&payment_hash);

					let update = PaymentDetailsUpdate {
						status: Some(PaymentStatus::Failed),
						..PaymentDetailsUpdate::new(payment_hash)
					};
					self.payment_store.update(&update).unwrap_or_else(|e| {
						log_error!(self.logger, "Failed to access payment store: {}", e);
						panic!("Failed to access payment store");
					});
				}
			}
			LdkEvent::PaymentClaimed {
				payment_hash,
				purpose,
				amount_msat,
				receiver_node_id: _,
				htlcs: _,
				sender_intended_total_msat: _,
			} => {
				log_info!(
					self.logger,
					"Claimed payment from payment hash {} of {}msat.",
					hex_utils::to_string(&payment_hash.0),
					amount_msat,
				);
				match purpose {
					PaymentPurpose::InvoicePayment { payment_preimage, payment_secret, .. } => {
						let update = PaymentDetailsUpdate {
							preimage: Some(payment_preimage),
							secret: Some(Some(payment_secret)),
							amount_msat: Some(Some(amount_msat)),
							status: Some(PaymentStatus::Succeeded),
							..PaymentDetailsUpdate::new(payment_hash)
						};
						match self.payment_store.update(&update) {
							Ok(true) => (),
							Ok(false) => {
								log_error!(
									self.logger,
									"Payment with hash {} couldn't be found in store",
									hex_utils::to_string(&payment_hash.0)
								);
								debug_assert!(false);
							}
							Err(e) => {
								log_error!(
									self.logger,
									"Failed to update payment with hash {}: {}",
									hex_utils::to_string(&payment_hash.0),
									e
								);
								debug_assert!(false);
							}
						}
					}
					PaymentPurpose::SpontaneousPayment(preimage) => {
						let payment = PaymentDetails {
							preimage: Some(preimage),
							hash: payment_hash,
							secret: None,
							amount_msat: Some(amount_msat),
							direction: PaymentDirection::Inbound,
							status: PaymentStatus::Succeeded,
						};

						match self.payment_store.insert(payment) {
							Ok(false) => (),
							Ok(true) => {
								log_error!(
									self.logger,
									"Spontaneous payment with hash {} was previosly known",
									hex_utils::to_string(&payment_hash.0)
								);
								debug_assert!(false);
							}
							Err(e) => {
								log_error!(
									self.logger,
									"Failed to insert payment with hash {}: {}",
									hex_utils::to_string(&payment_hash.0),
									e
								);
								debug_assert!(false);
							}
						}
					}
				};

				self.event_queue
					.add_event(Event::PaymentReceived { payment_hash, amount_msat })
					.unwrap_or_else(|e| {
						log_error!(self.logger, "Failed to push to event queue: {}", e);
						panic!("Failed to push to event queue");
					});
			}
			LdkEvent::PaymentSent { payment_preimage, payment_hash, fee_paid_msat, .. } => {
				if let Some(mut payment) = self.payment_store.get(&payment_hash) {
					payment.preimage = Some(payment_preimage);
					payment.status = PaymentStatus::Succeeded;
					self.payment_store.insert(payment.clone()).unwrap_or_else(|e| {
						log_error!(self.logger, "Failed to access payment store: {}", e);
						panic!("Failed to access payment store");
					});
					log_info!(
						self.logger,
						"Successfully sent payment of {}msat{} from \
						payment hash {:?} with preimage {:?}",
						payment.amount_msat.unwrap(),
						if let Some(fee) = fee_paid_msat {
							format!(" (fee {} msat)", fee)
						} else {
							"".to_string()
						},
						hex_utils::to_string(&payment_hash.0),
						hex_utils::to_string(&payment_preimage.0)
					);
				}
				self.event_queue
					.add_event(Event::PaymentSuccessful { payment_hash })
					.unwrap_or_else(|e| {
						log_error!(self.logger, "Failed to push to event queue: {}", e);
						panic!("Failed to push to event queue");
					});
			}
			LdkEvent::PaymentFailed { payment_hash, .. } => {
				log_info!(
					self.logger,
					"Failed to send payment to payment hash {:?}.",
					hex_utils::to_string(&payment_hash.0)
				);

				let update = PaymentDetailsUpdate {
					status: Some(PaymentStatus::Failed),
					..PaymentDetailsUpdate::new(payment_hash)
				};
				self.payment_store.update(&update).unwrap_or_else(|e| {
					log_error!(self.logger, "Failed to access payment store: {}", e);
					panic!("Failed to access payment store");
				});
				self.event_queue.add_event(Event::PaymentFailed { payment_hash }).unwrap_or_else(
					|e| {
						log_error!(self.logger, "Failed to push to event queue: {}", e);
						panic!("Failed to push to event queue");
					},
				);
			}

			LdkEvent::PaymentPathSuccessful { .. } => {}
			LdkEvent::PaymentPathFailed { .. } => {}
			LdkEvent::ProbeSuccessful { .. } => {}
			LdkEvent::ProbeFailed { .. } => {}
			LdkEvent::HTLCHandlingFailed { .. } => {}
			LdkEvent::PendingHTLCsForwardable { time_forwardable } => {
				let forwarding_channel_manager = self.channel_manager.clone();
				let min = time_forwardable.as_millis() as u64;

				let runtime_lock = self.runtime.read().unwrap();
				debug_assert!(runtime_lock.is_some());

				if let Some(runtime) = runtime_lock.as_ref() {
					runtime.spawn(async move {
						let millis_to_sleep = thread_rng().gen_range(min..min * 5) as u64;
						tokio::time::sleep(Duration::from_millis(millis_to_sleep)).await;

						forwarding_channel_manager.process_pending_htlc_forwards();
					});
				}
			}
			LdkEvent::SpendableOutputs { outputs, channel_id: _ } => {
				// TODO: We should eventually remember the outputs and supply them to the wallet's coin selection, once BDK allows us to do so.
				let destination_address = self.wallet.get_new_address().unwrap_or_else(|e| {
					log_error!(self.logger, "Failed to get destination address: {}", e);
					panic!("Failed to get destination address");
				});

				let output_descriptors = &outputs.iter().collect::<Vec<_>>();
				let tx_feerate = self
					.wallet
					.get_est_sat_per_1000_weight(ConfirmationTarget::NonAnchorChannelFee);

				// We set nLockTime to the current height to discourage fee sniping.
				let cur_height = self.channel_manager.current_best_block().height();
				let locktime: PackedLockTime =
					LockTime::from_height(cur_height).map_or(PackedLockTime::ZERO, |l| l.into());
				let res = self.keys_manager.spend_spendable_outputs(
					output_descriptors,
					Vec::new(),
					destination_address.script_pubkey(),
					tx_feerate,
					Some(locktime),
					&Secp256k1::new(),
				);

				match res {
					Ok(Some(spending_tx)) => self.wallet.broadcast_transactions(&[&spending_tx]),
					Ok(None) => {
						log_debug!(self.logger, "Omitted spending static outputs: {:?}", outputs);
					}
					Err(err) => {
						log_error!(self.logger, "Error spending outputs: {:?}", err);
					}
				}
			}
			LdkEvent::OpenChannelRequest {
				temporary_channel_id,
				counterparty_node_id,
				funding_satoshis,
				channel_type: _,
				push_msat: _,
			} => {
				let user_channel_id: u128 = rand::thread_rng().gen::<u128>();
				let allow_0conf = self.config.trusted_peers_0conf.contains(&counterparty_node_id);
				let res = if allow_0conf {
					self.channel_manager.accept_inbound_channel_from_trusted_peer_0conf(
						&temporary_channel_id,
						&counterparty_node_id,
						user_channel_id,
					)
				} else {
					self.channel_manager.accept_inbound_channel(
						&temporary_channel_id,
						&counterparty_node_id,
						user_channel_id,
					)
				};

				match res {
					Ok(()) => {
						log_info!(
							self.logger,
							"Accepting inbound{} channel of {}sats from{} peer {}",
							if allow_0conf { " 0conf" } else { "" },
							funding_satoshis,
							if allow_0conf { " trusted" } else { "" },
							counterparty_node_id,
						);
					}
					Err(e) => {
						log_error!(
							self.logger,
							"Error while accepting inbound{} channel from{} peer {}: {:?}",
							if allow_0conf { " 0conf" } else { "" },
							counterparty_node_id,
							if allow_0conf { " trusted" } else { "" },
							e,
						);
					}
				}
			}
			LdkEvent::PaymentForwarded {
				prev_channel_id,
				next_channel_id,
				fee_earned_msat,
				claim_from_onchain_tx,
				outbound_amount_forwarded_msat,
			} => {
				let read_only_network_graph = self.network_graph.read_only();
				let nodes = read_only_network_graph.nodes();
				let channels = self.channel_manager.list_channels();

				let node_str = |channel_id: &Option<ChannelId>| {
					channel_id
						.and_then(|channel_id| channels.iter().find(|c| c.channel_id == channel_id))
						.and_then(|channel| {
							nodes.get(&NodeId::from_pubkey(&channel.counterparty.node_id))
						})
						.map_or("private_node".to_string(), |node| {
							node.announcement_info
								.as_ref()
								.map_or("unnamed node".to_string(), |ann| {
									format!("node {}", ann.alias)
								})
						})
				};
				let channel_str = |channel_id: &Option<ChannelId>| {
					channel_id
						.map(|channel_id| format!(" with channel {}", channel_id))
						.unwrap_or_default()
				};
				let from_prev_str = format!(
					" from {}{}",
					node_str(&prev_channel_id),
					channel_str(&prev_channel_id)
				);
				let to_next_str =
					format!(" to {}{}", node_str(&next_channel_id), channel_str(&next_channel_id));

				let fee_earned = fee_earned_msat.unwrap_or(0);
				let outbound_amount_forwarded_msat = outbound_amount_forwarded_msat.unwrap_or(0);
				if claim_from_onchain_tx {
					log_info!(
						self.logger,
						"Forwarded payment{}{} of {}msat, earning {}msat in fees from claiming onchain.",
						from_prev_str,
						to_next_str,
						outbound_amount_forwarded_msat,
						fee_earned,
					);
				} else {
					log_info!(
						self.logger,
						"Forwarded payment{}{} of {}msat, earning {}msat in fees.",
						from_prev_str,
						to_next_str,
						outbound_amount_forwarded_msat,
						fee_earned,
					);
				}
			}
			LdkEvent::ChannelPending {
				channel_id,
				user_channel_id,
				former_temporary_channel_id,
				counterparty_node_id,
				funding_txo,
			} => {
				log_info!(
					self.logger,
					"New channel {} with counterparty {} has been created and is pending confirmation on chain.",
					channel_id,
					counterparty_node_id,
				);
				self.event_queue
					.add_event(Event::ChannelPending {
						channel_id,
						user_channel_id: UserChannelId(user_channel_id),
						former_temporary_channel_id: former_temporary_channel_id.unwrap(),
						counterparty_node_id,
						funding_txo,
					})
					.unwrap_or_else(|e| {
						log_error!(self.logger, "Failed to push to event queue: {}", e);
						panic!("Failed to push to event queue");
					});
			}
			LdkEvent::ChannelReady {
				channel_id, user_channel_id, counterparty_node_id, ..
			} => {
				log_info!(
					self.logger,
					"Channel {} with counterparty {} ready to be used.",
					channel_id,
					counterparty_node_id,
				);
				self.event_queue
					.add_event(Event::ChannelReady {
						channel_id,
						user_channel_id: UserChannelId(user_channel_id),
						counterparty_node_id: Some(counterparty_node_id),
					})
					.unwrap_or_else(|e| {
						log_error!(self.logger, "Failed to push to event queue: {}", e);
						panic!("Failed to push to event queue");
					});
			}
			LdkEvent::ChannelClosed {
				channel_id,
				reason,
				user_channel_id,
				counterparty_node_id,
				..
			} => {
				log_info!(self.logger, "Channel {} closed due to: {:?}", channel_id, reason);
				self.event_queue
					.add_event(Event::ChannelClosed {
						channel_id,
						user_channel_id: UserChannelId(user_channel_id),
						counterparty_node_id,
					})
					.unwrap_or_else(|e| {
						log_error!(self.logger, "Failed to push to event queue: {}", e);
						panic!("Failed to push to event queue");
					});
			}
			LdkEvent::DiscardFunding { .. } => {}
			LdkEvent::HTLCIntercepted { .. } => {}
			LdkEvent::BumpTransaction(_) => {}
			LdkEvent::InvoiceRequestFailed { .. } => {}
		}
	}
}

#[cfg(test)]
mod tests {
	use super::*;
	use crate::test::utils::TestLogger;
	use lightning::util::test_utils::TestStore;

	#[test]
	fn event_queue_persistence() {
		let store = Arc::new(TestStore::new(false));
		let logger = Arc::new(TestLogger::new());
		let event_queue = EventQueue::new(Arc::clone(&store), Arc::clone(&logger));
		assert_eq!(event_queue.next_event(), None);

		let expected_event = Event::ChannelReady {
			channel_id: ChannelId([23u8; 32]),
			user_channel_id: UserChannelId(2323),
			counterparty_node_id: None,
		};
		event_queue.add_event(expected_event.clone()).unwrap();

		// Check we get the expected event and that it is returned until we mark it handled.
		for _ in 0..5 {
			assert_eq!(event_queue.wait_next_event(), expected_event);
			assert_eq!(event_queue.next_event(), Some(expected_event.clone()));
		}

		// Check we can read back what we persisted.
		let persisted_bytes = store
			.read(
				EVENT_QUEUE_PERSISTENCE_PRIMARY_NAMESPACE,
				EVENT_QUEUE_PERSISTENCE_SECONDARY_NAMESPACE,
				EVENT_QUEUE_PERSISTENCE_KEY,
			)
			.unwrap();
		let deser_event_queue =
			EventQueue::read(&mut &persisted_bytes[..], (Arc::clone(&store), logger)).unwrap();
		assert_eq!(deser_event_queue.wait_next_event(), expected_event);

		event_queue.event_handled().unwrap();
		assert_eq!(event_queue.next_event(), None);
	}
}

'''
'''--- src/gossip.rs ---
use crate::logger::{log_trace, FilesystemLogger, Logger};
use crate::types::{GossipSync, NetworkGraph, P2PGossipSync, RapidGossipSync};
use crate::Error;

use lightning::routing::utxo::UtxoLookup;

use std::sync::atomic::{AtomicU32, Ordering};
use std::sync::Arc;

pub(crate) enum GossipSource {
	P2PNetwork {
		gossip_sync: Arc<P2PGossipSync>,
	},
	RapidGossipSync {
		gossip_sync: Arc<RapidGossipSync>,
		server_url: String,
		latest_sync_timestamp: AtomicU32,
		logger: Arc<FilesystemLogger>,
	},
}

impl GossipSource {
	pub fn new_p2p(network_graph: Arc<NetworkGraph>, logger: Arc<FilesystemLogger>) -> Self {
		let gossip_sync = Arc::new(P2PGossipSync::new(
			network_graph,
			None::<Arc<dyn UtxoLookup + Send + Sync>>,
			logger,
		));
		Self::P2PNetwork { gossip_sync }
	}

	pub fn new_rgs(
		server_url: String, latest_sync_timestamp: u32, network_graph: Arc<NetworkGraph>,
		logger: Arc<FilesystemLogger>,
	) -> Self {
		let gossip_sync = Arc::new(RapidGossipSync::new(network_graph, Arc::clone(&logger)));
		let latest_sync_timestamp = AtomicU32::new(latest_sync_timestamp);
		Self::RapidGossipSync { gossip_sync, server_url, latest_sync_timestamp, logger }
	}

	pub fn is_rgs(&self) -> bool {
		if let Self::RapidGossipSync { .. } = self {
			true
		} else {
			false
		}
	}

	pub fn as_gossip_sync(&self) -> GossipSync {
		match self {
			Self::RapidGossipSync { gossip_sync, .. } => {
				GossipSync::Rapid(Arc::clone(&gossip_sync))
			}
			Self::P2PNetwork { gossip_sync, .. } => GossipSync::P2P(Arc::clone(&gossip_sync)),
		}
	}

	pub async fn update_rgs_snapshot(&self) -> Result<u32, Error> {
		match self {
			Self::P2PNetwork { gossip_sync: _ } => Ok(0),
			Self::RapidGossipSync { gossip_sync, server_url, latest_sync_timestamp, logger } => {
				let query_timestamp = latest_sync_timestamp.load(Ordering::Acquire);
				let query_url = format!("{}/{}", server_url, query_timestamp);
				let response = reqwest::get(query_url).await.map_err(|e| {
					log_trace!(logger, "Failed to retrieve RGS gossip update: {}", e);
					Error::GossipUpdateFailed
				})?;

				match response.error_for_status() {
					Ok(res) => {
						let update_data = res.bytes().await.map_err(|e| {
							log_trace!(logger, "Failed to retrieve RGS gossip update: {}", e);
							Error::GossipUpdateFailed
						})?;

						let new_latest_sync_timestamp = gossip_sync
							.update_network_graph(&update_data)
							.map_err(|_| Error::GossipUpdateFailed)?;
						latest_sync_timestamp.store(new_latest_sync_timestamp, Ordering::Release);
						Ok(new_latest_sync_timestamp)
					}
					Err(e) => {
						log_trace!(logger, "Failed to retrieve RGS gossip update: {}", e);
						Err(Error::GossipUpdateFailed)
					}
				}
			}
		}
	}
}

'''
'''--- src/hex_utils.rs ---
use std::fmt::Write;

#[cfg(feature = "uniffi")]
pub fn to_vec(hex: &str) -> Option<Vec<u8>> {
	let mut out = Vec::with_capacity(hex.len() / 2);

	let mut b = 0;
	for (idx, c) in hex.as_bytes().iter().enumerate() {
		b <<= 4;
		match *c {
			b'A'..=b'F' => b |= c - b'A' + 10,
			b'a'..=b'f' => b |= c - b'a' + 10,
			b'0'..=b'9' => b |= c - b'0',
			_ => return None,
		}
		if (idx & 1) == 1 {
			out.push(b);
			b = 0;
		}
	}

	Some(out)
}

#[inline]
pub fn to_string(value: &[u8]) -> String {
	let mut res = String::with_capacity(2 * value.len());
	for v in value {
		write!(&mut res, "{:02x}", v).expect("Unable to write");
	}
	res
}

'''
'''--- src/io/mod.rs ---
//! Objects and traits for data persistence.

pub mod sqlite_store;
#[cfg(test)]
pub(crate) mod test_utils;
pub(crate) mod utils;
#[cfg(any(vss, vss_test))]
pub(crate) mod vss_store;

/// The event queue will be persisted under this key.
pub(crate) const EVENT_QUEUE_PERSISTENCE_PRIMARY_NAMESPACE: &str = "";
pub(crate) const EVENT_QUEUE_PERSISTENCE_SECONDARY_NAMESPACE: &str = "";
pub(crate) const EVENT_QUEUE_PERSISTENCE_KEY: &str = "events";

/// The peer information will be persisted under this key.
pub(crate) const PEER_INFO_PERSISTENCE_PRIMARY_NAMESPACE: &str = "";
pub(crate) const PEER_INFO_PERSISTENCE_SECONDARY_NAMESPACE: &str = "";
pub(crate) const PEER_INFO_PERSISTENCE_KEY: &str = "peers";

/// The payment information will be persisted under this prefix.
pub(crate) const PAYMENT_INFO_PERSISTENCE_PRIMARY_NAMESPACE: &str = "payments";
pub(crate) const PAYMENT_INFO_PERSISTENCE_SECONDARY_NAMESPACE: &str = "";

/// RapidGossipSync's `latest_sync_timestamp` will be persisted under this key.
pub(crate) const LATEST_RGS_SYNC_TIMESTAMP_PRIMARY_NAMESPACE: &str = "";
pub(crate) const LATEST_RGS_SYNC_TIMESTAMP_SECONDARY_NAMESPACE: &str = "";
pub(crate) const LATEST_RGS_SYNC_TIMESTAMP_KEY: &str = "latest_rgs_sync_timestamp";

/// The last time we broadcast a node announcement will be persisted under this key.
pub(crate) const LATEST_NODE_ANN_BCAST_TIMESTAMP_PRIMARY_NAMESPACE: &str = "";
pub(crate) const LATEST_NODE_ANN_BCAST_TIMESTAMP_SECONDARY_NAMESPACE: &str = "";
pub(crate) const LATEST_NODE_ANN_BCAST_TIMESTAMP_KEY: &str = "latest_node_ann_bcast_timestamp";

'''
'''--- src/io/sqlite_store/migrations.rs ---
use rusqlite::Connection;

use lightning::io;

pub(super) fn migrate_schema(
	connection: &mut Connection, kv_table_name: &str, from_version: u16, to_version: u16,
) -> io::Result<()> {
	assert!(from_version < to_version);
	if from_version == 1 && to_version == 2 {
		let tx = connection.transaction().map_err(|e| {
			let msg = format!(
				"Failed to migrate table {} from user_version {} to {}: {}",
				kv_table_name, from_version, to_version, e
			);
			io::Error::new(io::ErrorKind::Other, msg)
		})?;

		// Rename 'namespace' column to 'primary_namespace'
		let sql = format!(
			"ALTER TABLE {}
				RENAME COLUMN namespace TO primary_namespace;",
			kv_table_name
		);

		tx.execute(&sql, []).map_err(|e| {
			let msg = format!(
				"Failed to migrate table {} from user_version {} to {}: {}",
				kv_table_name, from_version, to_version, e
			);
			io::Error::new(io::ErrorKind::Other, msg)
		})?;

		// Add new 'secondary_namespace' column
		let sql = format!(
			"ALTER TABLE {}
				ADD secondary_namespace TEXT DEFAULT \"\" NOT NULL;",
			kv_table_name
		);

		tx.execute(&sql, []).map_err(|e| {
			let msg = format!(
				"Failed to migrate table {} from user_version {} to {}: {}",
				kv_table_name, from_version, to_version, e
			);
			io::Error::new(io::ErrorKind::Other, msg)
		})?;

		// Update user_version
		tx.pragma(Some(rusqlite::DatabaseName::Main), "user_version", to_version, |_| Ok(()))
			.map_err(|e| {
				let msg = format!(
					"Failed to upgrade user_version from {} to {}: {}",
					from_version, to_version, e
				);
				io::Error::new(io::ErrorKind::Other, msg)
			})?;

		tx.commit().map_err(|e| {
			let msg = format!(
				"Failed to migrate table {} from user_version {} to {}: {}",
				kv_table_name, from_version, to_version, e
			);
			io::Error::new(io::ErrorKind::Other, msg)
		})?;
	}
	Ok(())
}

#[cfg(test)]
mod tests {
	use crate::io::sqlite_store::SqliteStore;
	use crate::io::test_utils::do_read_write_remove_list_persist;
	use crate::test::utils::random_storage_path;

	use lightning::util::persist::KVStore;

	use rusqlite::{named_params, Connection};

	use std::fs;

	#[test]
	fn rwrl_post_schema_1_migration() {
		let old_schema_version = 1;

		let mut temp_path = random_storage_path();
		temp_path.push("rwrl_post_schema_1_migration");

		let db_file_name = "test_db".to_string();
		let kv_table_name = "test_table".to_string();

		let test_namespace = "testspace".to_string();
		let test_key = "testkey".to_string();
		let test_data = [42u8; 32];

		{
			// We create a database with a SCHEMA_VERSION 1 table
			fs::create_dir_all(temp_path.clone()).unwrap();
			let mut db_file_path = temp_path.clone();
			db_file_path.push(db_file_name.clone());

			let connection = Connection::open(db_file_path.clone()).unwrap();

			connection
				.pragma(
					Some(rusqlite::DatabaseName::Main),
					"user_version",
					old_schema_version,
					|_| Ok(()),
				)
				.unwrap();

			let sql = format!(
				"CREATE TABLE IF NOT EXISTS {} (
					namespace TEXT NOT NULL,
					key TEXT NOT NULL CHECK (key <> ''),
					value BLOB, PRIMARY KEY ( namespace, key )
					);",
				kv_table_name
			);

			connection.execute(&sql, []).unwrap();

			// We write some data to to the table
			let sql = format!(
				"INSERT OR REPLACE INTO {} (namespace, key, value) VALUES (:namespace, :key, :value);",
				kv_table_name
				);
			let mut stmt = connection.prepare_cached(&sql).unwrap();

			stmt.execute(named_params! {
				":namespace": test_namespace,
				":key": test_key,
				":value": test_data,
			})
			.unwrap();

			// We read the just written data back to assert it happened.
			let sql = format!(
				"SELECT value FROM {} WHERE namespace=:namespace AND key=:key;",
				kv_table_name
			);
			let mut stmt = connection.prepare_cached(&sql).unwrap();

			let res: Vec<u8> = stmt
				.query_row(
					named_params! {
						":namespace": test_namespace,
						":key": test_key,
					},
					|row| row.get(0),
				)
				.unwrap();

			assert_eq!(res, test_data);
		}

		// Check we migrate the db just fine without losing our written data.
		let store = SqliteStore::new(temp_path, Some(db_file_name), Some(kv_table_name)).unwrap();
		let res = store.read(&test_namespace, "", &test_key).unwrap();
		assert_eq!(res, test_data);

		// Check we can continue to use the store just fine.
		do_read_write_remove_list_persist(&store);
	}
}

'''
'''--- src/io/sqlite_store/mod.rs ---
//! Objects related to [`SqliteStore`] live here.
use crate::io::utils::check_namespace_key_validity;

use lightning::io;
use lightning::util::persist::KVStore;
use lightning::util::string::PrintableString;

use rusqlite::{named_params, Connection};

use std::fs;
use std::path::PathBuf;
use std::sync::{Arc, Mutex};

mod migrations;

/// LDK Node's database file name.
pub const SQLITE_DB_FILE_NAME: &str = "ldk_node_data.sqlite";
/// LDK Node's table in which we store all data.
pub const KV_TABLE_NAME: &str = "ldk_node_data";

/// The default database file name.
pub const DEFAULT_SQLITE_DB_FILE_NAME: &str = "ldk_data.sqlite";

/// The default table in which we store all data.
pub const DEFAULT_KV_TABLE_NAME: &str = "ldk_data";

// The current SQLite `user_version`, which we can use if we'd ever need to do a schema migration.
const SCHEMA_USER_VERSION: u16 = 2;

/// A [`KVStore`] implementation that writes to and reads from an [SQLite] database.
///
/// [SQLite]: https://sqlite.org
pub struct SqliteStore {
	connection: Arc<Mutex<Connection>>,
	data_dir: PathBuf,
	kv_table_name: String,
}

impl SqliteStore {
	/// Constructs a new [`SqliteStore`].
	///
	/// If not already existing, a new SQLite database will be created in the given `data_dir` under the
	/// given `db_file_name` (or the default to [`DEFAULT_SQLITE_DB_FILE_NAME`] if set to `None`).
	///
	/// Similarly, the given `kv_table_name` will be used or default to [`DEFAULT_KV_TABLE_NAME`].
	pub fn new(
		data_dir: PathBuf, db_file_name: Option<String>, kv_table_name: Option<String>,
	) -> io::Result<Self> {
		let db_file_name = db_file_name.unwrap_or(DEFAULT_SQLITE_DB_FILE_NAME.to_string());
		let kv_table_name = kv_table_name.unwrap_or(DEFAULT_KV_TABLE_NAME.to_string());

		fs::create_dir_all(data_dir.clone()).map_err(|e| {
			let msg = format!(
				"Failed to create database destination directory {}: {}",
				data_dir.display(),
				e
			);
			io::Error::new(io::ErrorKind::Other, msg)
		})?;
		let mut db_file_path = data_dir.clone();
		db_file_path.push(db_file_name);

		let mut connection = Connection::open(db_file_path.clone()).map_err(|e| {
			let msg =
				format!("Failed to open/create database file {}: {}", db_file_path.display(), e);
			io::Error::new(io::ErrorKind::Other, msg)
		})?;

		let sql = format!("SELECT user_version FROM pragma_user_version");
		let version_res: u16 = connection.query_row(&sql, [], |row| row.get(0)).unwrap();

		if version_res == 0 {
			// New database, set our SCHEMA_USER_VERSION and continue
			connection
				.pragma(
					Some(rusqlite::DatabaseName::Main),
					"user_version",
					SCHEMA_USER_VERSION,
					|_| Ok(()),
				)
				.map_err(|e| {
					let msg = format!("Failed to set PRAGMA user_version: {}", e);
					io::Error::new(io::ErrorKind::Other, msg)
				})?;
		} else if version_res < SCHEMA_USER_VERSION {
			migrations::migrate_schema(
				&mut connection,
				&kv_table_name,
				version_res,
				SCHEMA_USER_VERSION,
			)?;
		} else if version_res > SCHEMA_USER_VERSION {
			let msg = format!(
				"Failed to open database: incompatible schema version {}. Expected: {}",
				version_res, SCHEMA_USER_VERSION
			);
			return Err(io::Error::new(io::ErrorKind::Other, msg));
		}

		let sql = format!(
			"CREATE TABLE IF NOT EXISTS {} (
			primary_namespace TEXT NOT NULL,
			secondary_namespace TEXT DEFAULT \"\" NOT NULL,
			key TEXT NOT NULL CHECK (key <> ''),
			value BLOB, PRIMARY KEY ( primary_namespace, secondary_namespace, key )
			);",
			kv_table_name
		);

		connection.execute(&sql, []).map_err(|e| {
			let msg = format!("Failed to create table {}: {}", kv_table_name, e);
			io::Error::new(io::ErrorKind::Other, msg)
		})?;

		let connection = Arc::new(Mutex::new(connection));
		Ok(Self { connection, data_dir, kv_table_name })
	}

	/// Returns the data directory.
	pub fn get_data_dir(&self) -> PathBuf {
		self.data_dir.clone()
	}
}

impl KVStore for SqliteStore {
	fn read(
		&self, primary_namespace: &str, secondary_namespace: &str, key: &str,
	) -> std::io::Result<Vec<u8>> {
		check_namespace_key_validity(primary_namespace, secondary_namespace, Some(key), "read")?;

		let locked_conn = self.connection.lock().unwrap();
		let sql =
			format!("SELECT value FROM {} WHERE primary_namespace=:primary_namespace AND secondary_namespace=:secondary_namespace AND key=:key;",
			self.kv_table_name);

		let mut stmt = locked_conn.prepare_cached(&sql).map_err(|e| {
			let msg = format!("Failed to prepare statement: {}", e);
			std::io::Error::new(std::io::ErrorKind::Other, msg)
		})?;

		let res = stmt
			.query_row(
				named_params! {
					":primary_namespace": primary_namespace,
					":secondary_namespace": secondary_namespace,
					":key": key,
				},
				|row| row.get(0),
			)
			.map_err(|e| match e {
				rusqlite::Error::QueryReturnedNoRows => {
					let msg = format!(
						"Failed to read as key could not be found: {}/{}/{}",
						PrintableString(primary_namespace),
						PrintableString(secondary_namespace),
						PrintableString(key)
					);
					std::io::Error::new(std::io::ErrorKind::NotFound, msg)
				}
				e => {
					let msg = format!(
						"Failed to read from key {}/{}/{}: {}",
						PrintableString(primary_namespace),
						PrintableString(secondary_namespace),
						PrintableString(key),
						e
					);
					std::io::Error::new(std::io::ErrorKind::Other, msg)
				}
			})?;
		Ok(res)
	}

	fn write(
		&self, primary_namespace: &str, secondary_namespace: &str, key: &str, buf: &[u8],
	) -> std::io::Result<()> {
		check_namespace_key_validity(primary_namespace, secondary_namespace, Some(key), "write")?;

		let locked_conn = self.connection.lock().unwrap();

		let sql = format!(
			"INSERT OR REPLACE INTO {} (primary_namespace, secondary_namespace, key, value) VALUES (:primary_namespace, :secondary_namespace, :key, :value);",
			self.kv_table_name
		);

		let mut stmt = locked_conn.prepare_cached(&sql).map_err(|e| {
			let msg = format!("Failed to prepare statement: {}", e);
			std::io::Error::new(std::io::ErrorKind::Other, msg)
		})?;

		stmt.execute(named_params! {
			":primary_namespace": primary_namespace,
			":secondary_namespace": secondary_namespace,
			":key": key,
			":value": buf,
		})
		.map(|_| ())
		.map_err(|e| {
			let msg = format!(
				"Failed to write to key {}/{}/{}: {}",
				PrintableString(primary_namespace),
				PrintableString(secondary_namespace),
				PrintableString(key),
				e
			);
			std::io::Error::new(std::io::ErrorKind::Other, msg)
		})
	}

	fn remove(
		&self, primary_namespace: &str, secondary_namespace: &str, key: &str, _lazy: bool,
	) -> std::io::Result<()> {
		check_namespace_key_validity(primary_namespace, secondary_namespace, Some(key), "remove")?;

		let locked_conn = self.connection.lock().unwrap();

		let sql = format!("DELETE FROM {} WHERE primary_namespace=:primary_namespace AND secondary_namespace=:secondary_namespace AND key=:key;", self.kv_table_name);

		let mut stmt = locked_conn.prepare_cached(&sql).map_err(|e| {
			let msg = format!("Failed to prepare statement: {}", e);
			std::io::Error::new(std::io::ErrorKind::Other, msg)
		})?;

		stmt.execute(named_params! {
			":primary_namespace": primary_namespace,
			":secondary_namespace": secondary_namespace,
			":key": key,
		})
		.map_err(|e| {
			let msg = format!(
				"Failed to delete key {}/{}/{}: {}",
				PrintableString(primary_namespace),
				PrintableString(secondary_namespace),
				PrintableString(key),
				e
			);
			std::io::Error::new(std::io::ErrorKind::Other, msg)
		})?;
		Ok(())
	}

	fn list(
		&self, primary_namespace: &str, secondary_namespace: &str,
	) -> std::io::Result<Vec<String>> {
		check_namespace_key_validity(primary_namespace, secondary_namespace, None, "list")?;

		let locked_conn = self.connection.lock().unwrap();

		let sql = format!(
			"SELECT key FROM {} WHERE primary_namespace=:primary_namespace AND secondary_namespace=:secondary_namespace",
			self.kv_table_name
		);
		let mut stmt = locked_conn.prepare_cached(&sql).map_err(|e| {
			let msg = format!("Failed to prepare statement: {}", e);
			std::io::Error::new(std::io::ErrorKind::Other, msg)
		})?;

		let mut keys = Vec::new();

		let rows_iter = stmt
			.query_map(
				named_params! {
						":primary_namespace": primary_namespace,
						":secondary_namespace": secondary_namespace,
				},
				|row| row.get(0),
			)
			.map_err(|e| {
				let msg = format!("Failed to retrieve queried rows: {}", e);
				std::io::Error::new(std::io::ErrorKind::Other, msg)
			})?;

		for k in rows_iter {
			keys.push(k.map_err(|e| {
				let msg = format!("Failed to retrieve queried rows: {}", e);
				std::io::Error::new(std::io::ErrorKind::Other, msg)
			})?);
		}

		Ok(keys)
	}
}

#[cfg(test)]
mod tests {
	use super::*;
	use crate::io::test_utils::{do_read_write_remove_list_persist, do_test_store};
	use crate::test::utils::random_storage_path;

	impl Drop for SqliteStore {
		fn drop(&mut self) {
			match fs::remove_dir_all(&self.data_dir) {
				Err(e) => println!("Failed to remove test store directory: {}", e),
				_ => {}
			}
		}
	}

	#[test]
	fn read_write_remove_list_persist() {
		let mut temp_path = random_storage_path();
		temp_path.push("read_write_remove_list_persist");
		let store = SqliteStore::new(
			temp_path,
			Some("test_db".to_string()),
			Some("test_table".to_string()),
		)
		.unwrap();
		do_read_write_remove_list_persist(&store);
	}

	#[test]
	fn test_sqlite_store() {
		let mut temp_path = random_storage_path();
		temp_path.push("test_sqlite_store");
		let store_0 = SqliteStore::new(
			temp_path.clone(),
			Some("test_db_0".to_string()),
			Some("test_table".to_string()),
		)
		.unwrap();
		let store_1 = SqliteStore::new(
			temp_path,
			Some("test_db_1".to_string()),
			Some("test_table".to_string()),
		)
		.unwrap();
		do_test_store(&store_0, &store_1)
	}
}

#[cfg(ldk_bench)]
/// Benches
pub mod bench {
	use criterion::Criterion;

	/// Bench!
	pub fn bench_sends(bench: &mut Criterion) {
		let store_a = super::SqliteStore::new("bench_sqlite_store_a".into(), None, None).unwrap();
		let store_b = super::SqliteStore::new("bench_sqlite_store_b".into(), None, None).unwrap();
		lightning::ln::channelmanager::bench::bench_two_sends(
			bench,
			"bench_sqlite_persisted_sends",
			store_a,
			store_b,
		);
	}
}

'''
'''--- src/io/test_utils.rs ---
use crate::io::sqlite_store::SqliteStore;
use lightning_persister::fs_store::FilesystemStore;

use lightning::ln::functional_test_utils::{
	connect_block, create_announced_chan_between_nodes, create_chanmon_cfgs, create_dummy_block,
	create_network, create_node_cfgs, create_node_chanmgrs, send_payment,
};
use lightning::util::persist::{read_channel_monitors, KVStore, KVSTORE_NAMESPACE_KEY_MAX_LEN};

use lightning::chain::channelmonitor::CLOSED_CHANNEL_UPDATE_ID;
use lightning::events::ClosureReason;
use lightning::util::test_utils::{self, TestStore};
use lightning::{check_added_monitors, check_closed_broadcast, check_closed_event};

use std::panic::RefUnwindSafe;
use std::path::PathBuf;
use std::sync::RwLock;

pub(crate) fn do_read_write_remove_list_persist<K: KVStore + RefUnwindSafe>(kv_store: &K) {
	let data = [42u8; 32];

	let primary_namespace = "testspace";
	let secondary_namespace = "testsubspace";
	let key = "testkey";

	// Test the basic KVStore operations.
	kv_store.write(primary_namespace, secondary_namespace, key, &data).unwrap();

	// Test empty primary/secondary namespaces are allowed, but not empty primary namespace and non-empty
	// secondary primary_namespace, and not empty key.
	kv_store.write("", "", key, &data).unwrap();
	let res = std::panic::catch_unwind(|| kv_store.write("", secondary_namespace, key, &data));
	assert!(res.is_err());
	let res = std::panic::catch_unwind(|| {
		kv_store.write(primary_namespace, secondary_namespace, "", &data)
	});
	assert!(res.is_err());

	let listed_keys = kv_store.list(primary_namespace, secondary_namespace).unwrap();
	assert_eq!(listed_keys.len(), 1);
	assert_eq!(listed_keys[0], key);

	let read_data = kv_store.read(primary_namespace, secondary_namespace, key).unwrap();
	assert_eq!(data, &*read_data);

	kv_store.remove(primary_namespace, secondary_namespace, key, false).unwrap();

	let listed_keys = kv_store.list(primary_namespace, secondary_namespace).unwrap();
	assert_eq!(listed_keys.len(), 0);

	// Ensure we have no issue operating with primary_namespace/secondary_namespace/key being KVSTORE_NAMESPACE_KEY_MAX_LEN
	let max_chars: String = std::iter::repeat('A').take(KVSTORE_NAMESPACE_KEY_MAX_LEN).collect();
	kv_store.write(&max_chars, &max_chars, &max_chars, &data).unwrap();

	let listed_keys = kv_store.list(&max_chars, &max_chars).unwrap();
	assert_eq!(listed_keys.len(), 1);
	assert_eq!(listed_keys[0], max_chars);

	let read_data = kv_store.read(&max_chars, &max_chars, &max_chars).unwrap();
	assert_eq!(data, &*read_data);

	kv_store.remove(&max_chars, &max_chars, &max_chars, false).unwrap();

	let listed_keys = kv_store.list(&max_chars, &max_chars).unwrap();
	assert_eq!(listed_keys.len(), 0);
}

// Integration-test the given KVStore implementation. Test relaying a few payments and check that
// the persisted data is updated the appropriate number of times.
pub(crate) fn do_test_store<K: KVStore>(store_0: &K, store_1: &K) {
	let chanmon_cfgs = create_chanmon_cfgs(2);
	let mut node_cfgs = create_node_cfgs(2, &chanmon_cfgs);
	let chain_mon_0 = test_utils::TestChainMonitor::new(
		Some(&chanmon_cfgs[0].chain_source),
		&chanmon_cfgs[0].tx_broadcaster,
		&chanmon_cfgs[0].logger,
		&chanmon_cfgs[0].fee_estimator,
		store_0,
		node_cfgs[0].keys_manager,
	);
	let chain_mon_1 = test_utils::TestChainMonitor::new(
		Some(&chanmon_cfgs[1].chain_source),
		&chanmon_cfgs[1].tx_broadcaster,
		&chanmon_cfgs[1].logger,
		&chanmon_cfgs[1].fee_estimator,
		store_1,
		node_cfgs[1].keys_manager,
	);
	node_cfgs[0].chain_monitor = chain_mon_0;
	node_cfgs[1].chain_monitor = chain_mon_1;
	let node_chanmgrs = create_node_chanmgrs(2, &node_cfgs, &[None, None]);
	let nodes = create_network(2, &node_cfgs, &node_chanmgrs);

	// Check that the persisted channel data is empty before any channels are
	// open.
	let mut persisted_chan_data_0 =
		read_channel_monitors(store_0, nodes[0].keys_manager, nodes[0].keys_manager).unwrap();
	assert_eq!(persisted_chan_data_0.len(), 0);
	let mut persisted_chan_data_1 =
		read_channel_monitors(store_1, nodes[1].keys_manager, nodes[1].keys_manager).unwrap();
	assert_eq!(persisted_chan_data_1.len(), 0);

	// Helper to make sure the channel is on the expected update ID.
	macro_rules! check_persisted_data {
		($expected_update_id: expr) => {
			persisted_chan_data_0 =
				read_channel_monitors(store_0, nodes[0].keys_manager, nodes[0].keys_manager)
					.unwrap();
			assert_eq!(persisted_chan_data_0.len(), 1);
			for (_, mon) in persisted_chan_data_0.iter() {
				assert_eq!(mon.get_latest_update_id(), $expected_update_id);
			}
			persisted_chan_data_1 =
				read_channel_monitors(store_1, nodes[1].keys_manager, nodes[1].keys_manager)
					.unwrap();
			assert_eq!(persisted_chan_data_1.len(), 1);
			for (_, mon) in persisted_chan_data_1.iter() {
				assert_eq!(mon.get_latest_update_id(), $expected_update_id);
			}
		};
	}

	// Create some initial channel and check that a channel was persisted.
	let _ = create_announced_chan_between_nodes(&nodes, 0, 1);
	check_persisted_data!(0);

	// Send a few payments and make sure the monitors are updated to the latest.
	send_payment(&nodes[0], &vec![&nodes[1]][..], 8000000);
	check_persisted_data!(5);
	send_payment(&nodes[1], &vec![&nodes[0]][..], 4000000);
	check_persisted_data!(10);

	// Force close because cooperative close doesn't result in any persisted
	// updates.
	nodes[0]
		.node
		.force_close_broadcasting_latest_txn(
			&nodes[0].node.list_channels()[0].channel_id,
			&nodes[1].node.get_our_node_id(),
		)
		.unwrap();
	check_closed_event!(
		nodes[0],
		1,
		ClosureReason::HolderForceClosed,
		[nodes[1].node.get_our_node_id()],
		100000
	);
	check_closed_broadcast!(nodes[0], true);
	check_added_monitors!(nodes[0], 1);

	let node_txn = nodes[0].tx_broadcaster.txn_broadcasted.lock().unwrap();
	assert_eq!(node_txn.len(), 1);

	connect_block(
		&nodes[1],
		&create_dummy_block(
			nodes[0].best_block_hash(),
			42,
			vec![node_txn[0].clone(), node_txn[0].clone()],
		),
	);
	check_closed_broadcast!(nodes[1], true);
	check_closed_event!(
		nodes[1],
		1,
		ClosureReason::CommitmentTxConfirmed,
		[nodes[0].node.get_our_node_id()],
		100000
	);
	check_added_monitors!(nodes[1], 1);

	// Make sure everything is persisted as expected after close.
	check_persisted_data!(CLOSED_CHANNEL_UPDATE_ID);
}

// A `KVStore` impl for testing purposes that wraps all our `KVStore`s and asserts their synchronicity.
pub(crate) struct TestSyncStore {
	serializer: RwLock<()>,
	test_store: TestStore,
	fs_store: FilesystemStore,
	sqlite_store: SqliteStore,
}

impl TestSyncStore {
	pub(crate) fn new(dest_dir: PathBuf) -> Self {
		let serializer = RwLock::new(());
		let mut fs_dir = dest_dir.clone();
		fs_dir.push("fs_store");
		let fs_store = FilesystemStore::new(fs_dir);
		let mut sql_dir = dest_dir.clone();
		sql_dir.push("sqlite_store");
		let sqlite_store = SqliteStore::new(
			sql_dir,
			Some("test_sync_db".to_string()),
			Some("test_sync_table".to_string()),
		)
		.unwrap();
		let test_store = TestStore::new(false);
		Self { serializer, fs_store, sqlite_store, test_store }
	}

	fn do_list(
		&self, primary_namespace: &str, secondary_namespace: &str,
	) -> std::io::Result<Vec<String>> {
		let fs_res = self.fs_store.list(primary_namespace, secondary_namespace);
		let sqlite_res = self.sqlite_store.list(primary_namespace, secondary_namespace);
		let test_res = self.test_store.list(primary_namespace, secondary_namespace);

		match fs_res {
			Ok(mut list) => {
				list.sort();

				let mut sqlite_list = sqlite_res.unwrap();
				sqlite_list.sort();
				assert_eq!(list, sqlite_list);

				let mut test_list = test_res.unwrap();
				test_list.sort();
				assert_eq!(list, test_list);

				Ok(list)
			}
			Err(e) => {
				assert!(sqlite_res.is_err());
				assert!(test_res.is_err());
				Err(e)
			}
		}
	}
}

impl KVStore for TestSyncStore {
	fn read(
		&self, primary_namespace: &str, secondary_namespace: &str, key: &str,
	) -> std::io::Result<Vec<u8>> {
		let _guard = self.serializer.read().unwrap();

		let fs_res = self.fs_store.read(primary_namespace, secondary_namespace, key);
		let sqlite_res = self.sqlite_store.read(primary_namespace, secondary_namespace, key);
		let test_res = self.test_store.read(primary_namespace, secondary_namespace, key);

		match fs_res {
			Ok(read) => {
				assert_eq!(read, sqlite_res.unwrap());
				assert_eq!(read, test_res.unwrap());
				Ok(read)
			}
			Err(e) => {
				assert!(sqlite_res.is_err());
				assert_eq!(e.kind(), unsafe { sqlite_res.unwrap_err_unchecked().kind() });
				assert!(test_res.is_err());
				assert_eq!(e.kind(), unsafe { test_res.unwrap_err_unchecked().kind() });
				Err(e)
			}
		}
	}

	fn write(
		&self, primary_namespace: &str, secondary_namespace: &str, key: &str, buf: &[u8],
	) -> std::io::Result<()> {
		let _guard = self.serializer.write().unwrap();
		let fs_res = self.fs_store.write(primary_namespace, secondary_namespace, key, buf);
		let sqlite_res = self.sqlite_store.write(primary_namespace, secondary_namespace, key, buf);
		let test_res = self.test_store.write(primary_namespace, secondary_namespace, key, buf);

		assert!(self
			.do_list(primary_namespace, secondary_namespace)
			.unwrap()
			.contains(&key.to_string()));

		match fs_res {
			Ok(()) => {
				assert!(sqlite_res.is_ok());
				assert!(test_res.is_ok());
				Ok(())
			}
			Err(e) => {
				assert!(sqlite_res.is_err());
				assert!(test_res.is_err());
				Err(e)
			}
		}
	}

	fn remove(
		&self, primary_namespace: &str, secondary_namespace: &str, key: &str, lazy: bool,
	) -> std::io::Result<()> {
		let _guard = self.serializer.write().unwrap();
		let fs_res = self.fs_store.remove(primary_namespace, secondary_namespace, key, lazy);
		let sqlite_res =
			self.sqlite_store.remove(primary_namespace, secondary_namespace, key, lazy);
		let test_res = self.test_store.remove(primary_namespace, secondary_namespace, key, lazy);

		assert!(!self
			.do_list(primary_namespace, secondary_namespace)
			.unwrap()
			.contains(&key.to_string()));

		match fs_res {
			Ok(()) => {
				assert!(sqlite_res.is_ok());
				assert!(test_res.is_ok());
				Ok(())
			}
			Err(e) => {
				assert!(sqlite_res.is_err());
				assert!(test_res.is_err());
				Err(e)
			}
		}
	}

	fn list(
		&self, primary_namespace: &str, secondary_namespace: &str,
	) -> std::io::Result<Vec<String>> {
		let _guard = self.serializer.read().unwrap();
		self.do_list(primary_namespace, secondary_namespace)
	}
}

'''
'''--- src/io/utils.rs ---
use super::*;
use crate::WALLET_KEYS_SEED_LEN;

use crate::logger::log_error;
use crate::peer_store::PeerStore;
use crate::{Error, EventQueue, PaymentDetails};

use lightning::routing::gossip::NetworkGraph;
use lightning::routing::scoring::{ProbabilisticScorer, ProbabilisticScoringDecayParameters};
use lightning::util::logger::Logger;
use lightning::util::persist::{
	KVStore, KVSTORE_NAMESPACE_KEY_ALPHABET, KVSTORE_NAMESPACE_KEY_MAX_LEN,
	NETWORK_GRAPH_PERSISTENCE_KEY, NETWORK_GRAPH_PERSISTENCE_PRIMARY_NAMESPACE,
	NETWORK_GRAPH_PERSISTENCE_SECONDARY_NAMESPACE, SCORER_PERSISTENCE_KEY,
	SCORER_PERSISTENCE_PRIMARY_NAMESPACE, SCORER_PERSISTENCE_SECONDARY_NAMESPACE,
};
use lightning::util::ser::{Readable, ReadableArgs, Writeable};
use lightning::util::string::PrintableString;

use bip39::Mnemonic;
use rand::{thread_rng, RngCore};

use std::fs;
use std::io::{Cursor, Write};
use std::ops::Deref;
use std::path::Path;
use std::sync::Arc;

/// Generates a random [BIP 39] mnemonic.
///
/// The result may be used to initialize the [`Node`] entropy, i.e., can be given to
/// [`Builder::set_entropy_bip39_mnemonic`].
///
/// [BIP 39]: https://github.com/bitcoin/bips/blob/master/bip-0039.mediawiki
/// [`Node`]: crate::Node
/// [`Builder::set_entropy_bip39_mnemonic`]: crate::Builder::set_entropy_bip39_mnemonic
pub fn generate_entropy_mnemonic() -> Mnemonic {
	// bip39::Mnemonic supports 256 bit entropy max
	let mut entropy = [0; 32];
	thread_rng().fill_bytes(&mut entropy);
	Mnemonic::from_entropy(&entropy).unwrap()
}

pub(crate) fn read_or_generate_seed_file<L: Deref>(
	keys_seed_path: &str, logger: L,
) -> std::io::Result<[u8; WALLET_KEYS_SEED_LEN]>
where
	L::Target: Logger,
{
	if Path::new(&keys_seed_path).exists() {
		let seed = fs::read(keys_seed_path).map_err(|e| {
			log_error!(logger, "Failed to read keys seed file: {}", keys_seed_path);
			e
		})?;

		if seed.len() != WALLET_KEYS_SEED_LEN {
			log_error!(
				logger,
				"Failed to read keys seed file due to invalid length: {}",
				keys_seed_path
			);
			return Err(std::io::Error::new(
				std::io::ErrorKind::InvalidData,
				"Failed to read keys seed file due to invalid length",
			));
		}

		let mut key = [0; WALLET_KEYS_SEED_LEN];
		key.copy_from_slice(&seed);
		Ok(key)
	} else {
		let mut key = [0; WALLET_KEYS_SEED_LEN];
		thread_rng().fill_bytes(&mut key);

		let mut f = fs::File::create(keys_seed_path).map_err(|e| {
			log_error!(logger, "Failed to create keys seed file: {}", keys_seed_path);
			e
		})?;

		f.write_all(&key).map_err(|e| {
			log_error!(logger, "Failed to write node keys seed to disk: {}", keys_seed_path);
			e
		})?;

		f.sync_all().map_err(|e| {
			log_error!(logger, "Failed to sync node keys seed to disk: {}", keys_seed_path);
			e
		})?;

		Ok(key)
	}
}

/// Read a previously persisted [`NetworkGraph`] from the store.
pub(crate) fn read_network_graph<K: KVStore + Sync + Send, L: Deref + Clone>(
	kv_store: Arc<K>, logger: L,
) -> Result<NetworkGraph<L>, std::io::Error>
where
	L::Target: Logger,
{
	let mut reader = Cursor::new(kv_store.read(
		NETWORK_GRAPH_PERSISTENCE_PRIMARY_NAMESPACE,
		NETWORK_GRAPH_PERSISTENCE_SECONDARY_NAMESPACE,
		NETWORK_GRAPH_PERSISTENCE_KEY,
	)?);
	NetworkGraph::read(&mut reader, logger.clone()).map_err(|e| {
		log_error!(logger, "Failed to deserialize NetworkGraph: {}", e);
		std::io::Error::new(std::io::ErrorKind::InvalidData, "Failed to deserialize NetworkGraph")
	})
}

/// Read a previously persisted [`ProbabilisticScorer`] from the store.
pub(crate) fn read_scorer<
	K: KVStore + Send + Sync,
	G: Deref<Target = NetworkGraph<L>>,
	L: Deref + Clone,
>(
	kv_store: Arc<K>, network_graph: G, logger: L,
) -> Result<ProbabilisticScorer<G, L>, std::io::Error>
where
	L::Target: Logger,
{
	let params = ProbabilisticScoringDecayParameters::default();
	let mut reader = Cursor::new(kv_store.read(
		SCORER_PERSISTENCE_PRIMARY_NAMESPACE,
		SCORER_PERSISTENCE_SECONDARY_NAMESPACE,
		SCORER_PERSISTENCE_KEY,
	)?);
	let args = (params, network_graph, logger.clone());
	ProbabilisticScorer::read(&mut reader, args).map_err(|e| {
		log_error!(logger, "Failed to deserialize scorer: {}", e);
		std::io::Error::new(std::io::ErrorKind::InvalidData, "Failed to deserialize Scorer")
	})
}

/// Read previously persisted events from the store.
pub(crate) fn read_event_queue<K: KVStore + Sync + Send, L: Deref + Clone>(
	kv_store: Arc<K>, logger: L,
) -> Result<EventQueue<K, L>, std::io::Error>
where
	L::Target: Logger,
{
	let mut reader = Cursor::new(kv_store.read(
		EVENT_QUEUE_PERSISTENCE_PRIMARY_NAMESPACE,
		EVENT_QUEUE_PERSISTENCE_SECONDARY_NAMESPACE,
		EVENT_QUEUE_PERSISTENCE_KEY,
	)?);
	EventQueue::read(&mut reader, (kv_store, logger.clone())).map_err(|e| {
		log_error!(logger, "Failed to deserialize event queue: {}", e);
		std::io::Error::new(std::io::ErrorKind::InvalidData, "Failed to deserialize EventQueue")
	})
}

/// Read previously persisted peer info from the store.
pub(crate) fn read_peer_info<K: KVStore + Sync + Send, L: Deref + Clone>(
	kv_store: Arc<K>, logger: L,
) -> Result<PeerStore<K, L>, std::io::Error>
where
	L::Target: Logger,
{
	let mut reader = Cursor::new(kv_store.read(
		PEER_INFO_PERSISTENCE_PRIMARY_NAMESPACE,
		PEER_INFO_PERSISTENCE_SECONDARY_NAMESPACE,
		PEER_INFO_PERSISTENCE_KEY,
	)?);
	PeerStore::read(&mut reader, (kv_store, logger.clone())).map_err(|e| {
		log_error!(logger, "Failed to deserialize peer store: {}", e);
		std::io::Error::new(std::io::ErrorKind::InvalidData, "Failed to deserialize PeerStore")
	})
}

/// Read previously persisted payments information from the store.
pub(crate) fn read_payments<K: KVStore + Sync + Send, L: Deref>(
	kv_store: Arc<K>, logger: L,
) -> Result<Vec<PaymentDetails>, std::io::Error>
where
	L::Target: Logger,
{
	let mut res = Vec::new();

	for stored_key in kv_store.list(
		PAYMENT_INFO_PERSISTENCE_PRIMARY_NAMESPACE,
		PAYMENT_INFO_PERSISTENCE_SECONDARY_NAMESPACE,
	)? {
		let mut reader = Cursor::new(kv_store.read(
			PAYMENT_INFO_PERSISTENCE_PRIMARY_NAMESPACE,
			PAYMENT_INFO_PERSISTENCE_SECONDARY_NAMESPACE,
			&stored_key,
		)?);
		let payment = PaymentDetails::read(&mut reader).map_err(|e| {
			log_error!(logger, "Failed to deserialize PaymentDetails: {}", e);
			std::io::Error::new(
				std::io::ErrorKind::InvalidData,
				"Failed to deserialize PaymentDetails",
			)
		})?;
		res.push(payment);
	}
	Ok(res)
}

pub(crate) fn read_latest_rgs_sync_timestamp<K: KVStore + Sync + Send, L: Deref>(
	kv_store: Arc<K>, logger: L,
) -> Result<u32, std::io::Error>
where
	L::Target: Logger,
{
	let mut reader = Cursor::new(kv_store.read(
		LATEST_RGS_SYNC_TIMESTAMP_PRIMARY_NAMESPACE,
		LATEST_RGS_SYNC_TIMESTAMP_SECONDARY_NAMESPACE,
		LATEST_RGS_SYNC_TIMESTAMP_KEY,
	)?);
	u32::read(&mut reader).map_err(|e| {
		log_error!(logger, "Failed to deserialize latest RGS sync timestamp: {}", e);
		std::io::Error::new(
			std::io::ErrorKind::InvalidData,
			"Failed to deserialize latest RGS sync timestamp",
		)
	})
}

pub(crate) fn write_latest_rgs_sync_timestamp<K: KVStore + Sync + Send, L: Deref>(
	updated_timestamp: u32, kv_store: Arc<K>, logger: L,
) -> Result<(), Error>
where
	L::Target: Logger,
{
	let data = updated_timestamp.encode();
	kv_store
		.write(
			LATEST_RGS_SYNC_TIMESTAMP_PRIMARY_NAMESPACE,
			LATEST_RGS_SYNC_TIMESTAMP_SECONDARY_NAMESPACE,
			LATEST_RGS_SYNC_TIMESTAMP_KEY,
			&data,
		)
		.map_err(|e| {
			log_error!(
				logger,
				"Writing data to key {}/{}/{} failed due to: {}",
				LATEST_RGS_SYNC_TIMESTAMP_PRIMARY_NAMESPACE,
				LATEST_RGS_SYNC_TIMESTAMP_SECONDARY_NAMESPACE,
				LATEST_RGS_SYNC_TIMESTAMP_KEY,
				e
			);
			Error::PersistenceFailed
		})
}

pub(crate) fn read_latest_node_ann_bcast_timestamp<K: KVStore + Sync + Send, L: Deref>(
	kv_store: Arc<K>, logger: L,
) -> Result<u64, std::io::Error>
where
	L::Target: Logger,
{
	let mut reader = Cursor::new(kv_store.read(
		LATEST_NODE_ANN_BCAST_TIMESTAMP_PRIMARY_NAMESPACE,
		LATEST_NODE_ANN_BCAST_TIMESTAMP_SECONDARY_NAMESPACE,
		LATEST_NODE_ANN_BCAST_TIMESTAMP_KEY,
	)?);
	u64::read(&mut reader).map_err(|e| {
		log_error!(
			logger,
			"Failed to deserialize latest node announcement broadcast timestamp: {}",
			e
		);
		std::io::Error::new(
			std::io::ErrorKind::InvalidData,
			"Failed to deserialize latest node announcement broadcast timestamp",
		)
	})
}

pub(crate) fn write_latest_node_ann_bcast_timestamp<K: KVStore + Sync + Send, L: Deref>(
	updated_timestamp: u64, kv_store: Arc<K>, logger: L,
) -> Result<(), Error>
where
	L::Target: Logger,
{
	let data = updated_timestamp.encode();
	kv_store
		.write(
			LATEST_NODE_ANN_BCAST_TIMESTAMP_PRIMARY_NAMESPACE,
			LATEST_NODE_ANN_BCAST_TIMESTAMP_SECONDARY_NAMESPACE,
			LATEST_NODE_ANN_BCAST_TIMESTAMP_KEY,
			&data,
		)
		.map_err(|e| {
			log_error!(
				logger,
				"Writing data to key {}/{}/{} failed due to: {}",
				LATEST_NODE_ANN_BCAST_TIMESTAMP_PRIMARY_NAMESPACE,
				LATEST_NODE_ANN_BCAST_TIMESTAMP_SECONDARY_NAMESPACE,
				LATEST_NODE_ANN_BCAST_TIMESTAMP_KEY,
				e
			);
			Error::PersistenceFailed
		})
}

pub(crate) fn is_valid_kvstore_str(key: &str) -> bool {
	key.len() <= KVSTORE_NAMESPACE_KEY_MAX_LEN
		&& key.chars().all(|c| KVSTORE_NAMESPACE_KEY_ALPHABET.contains(c))
}

pub(crate) fn check_namespace_key_validity(
	primary_namespace: &str, secondary_namespace: &str, key: Option<&str>, operation: &str,
) -> Result<(), std::io::Error> {
	if let Some(key) = key {
		if key.is_empty() {
			debug_assert!(
				false,
				"Failed to {} {}/{}/{}: key may not be empty.",
				operation,
				PrintableString(primary_namespace),
				PrintableString(secondary_namespace),
				PrintableString(key)
			);
			let msg = format!(
				"Failed to {} {}/{}/{}: key may not be empty.",
				operation,
				PrintableString(primary_namespace),
				PrintableString(secondary_namespace),
				PrintableString(key)
			);
			return Err(std::io::Error::new(std::io::ErrorKind::Other, msg));
		}

		if primary_namespace.is_empty() && !secondary_namespace.is_empty() {
			debug_assert!(false,
				"Failed to {} {}/{}/{}: primary namespace may not be empty if a non-empty secondary namespace is given.",
				operation,
				PrintableString(primary_namespace), PrintableString(secondary_namespace), PrintableString(key));
			let msg = format!(
				"Failed to {} {}/{}/{}: primary namespace may not be empty if a non-empty secondary namespace is given.", operation,
				PrintableString(primary_namespace), PrintableString(secondary_namespace), PrintableString(key));
			return Err(std::io::Error::new(std::io::ErrorKind::Other, msg));
		}

		if !is_valid_kvstore_str(primary_namespace)
			|| !is_valid_kvstore_str(secondary_namespace)
			|| !is_valid_kvstore_str(key)
		{
			debug_assert!(
				false,
				"Failed to {} {}/{}/{}: primary namespace, secondary namespace, and key must be valid.",
				operation,
				PrintableString(primary_namespace),
				PrintableString(secondary_namespace),
				PrintableString(key)
			);
			let msg = format!(
				"Failed to {} {}/{}/{}: primary namespace, secondary namespace, and key must be valid.",
				operation,
				PrintableString(primary_namespace),
				PrintableString(secondary_namespace),
				PrintableString(key)
			);
			return Err(std::io::Error::new(std::io::ErrorKind::Other, msg));
		}
	} else {
		if primary_namespace.is_empty() && !secondary_namespace.is_empty() {
			debug_assert!(false,
				"Failed to {} {}/{}: primary namespace may not be empty if a non-empty secondary namespace is given.",
				operation, PrintableString(primary_namespace), PrintableString(secondary_namespace));
			let msg = format!(
				"Failed to {} {}/{}: primary namespace may not be empty if a non-empty secondary namespace is given.",
				operation, PrintableString(primary_namespace), PrintableString(secondary_namespace));
			return Err(std::io::Error::new(std::io::ErrorKind::Other, msg));
		}
		if !is_valid_kvstore_str(primary_namespace) || !is_valid_kvstore_str(secondary_namespace) {
			debug_assert!(
				false,
				"Failed to {} {}/{}: primary namespace and secondary namespace must be valid.",
				operation,
				PrintableString(primary_namespace),
				PrintableString(secondary_namespace)
			);
			let msg = format!(
				"Failed to {} {}/{}: primary namespace and secondary namespace must be valid.",
				operation,
				PrintableString(primary_namespace),
				PrintableString(secondary_namespace)
			);
			return Err(std::io::Error::new(std::io::ErrorKind::Other, msg));
		}
	}

	Ok(())
}

#[cfg(test)]
mod tests {
	use super::*;

	#[test]
	fn mnemonic_to_entropy_to_mnemonic() {
		let mnemonic = generate_entropy_mnemonic();

		let entropy = mnemonic.to_entropy();
		assert_eq!(mnemonic, Mnemonic::from_entropy(&entropy).unwrap());
	}
}

'''
'''--- src/io/vss_store.rs ---
use io::Error;
use std::io;
use std::io::ErrorKind;
#[cfg(test)]
use std::panic::RefUnwindSafe;

use crate::io::utils::check_namespace_key_validity;
use lightning::util::persist::KVStore;
use tokio::runtime::Runtime;
use vss_client::client::VssClient;
use vss_client::error::VssError;
use vss_client::types::{
	DeleteObjectRequest, GetObjectRequest, KeyValue, ListKeyVersionsRequest, PutObjectRequest,
};

/// A [`KVStore`] implementation that writes to and reads from a [VSS](https://github.com/lightningdevkit/vss-server/blob/main/README.md) backend.
pub struct VssStore {
	client: VssClient,
	store_id: String,
	runtime: Runtime,
}

impl VssStore {
	pub(crate) fn new(base_url: &str, store_id: String) -> Self {
		let client = VssClient::new(base_url);
		let runtime = tokio::runtime::Builder::new_multi_thread().enable_all().build().unwrap();
		Self { client, store_id, runtime }
	}

	fn build_key(
		&self, primary_namespace: &str, secondary_namespace: &str, key: &str,
	) -> io::Result<String> {
		if primary_namespace.is_empty() {
			Ok(key.to_string())
		} else {
			Ok(format!("{}#{}#{}", primary_namespace, secondary_namespace, key))
		}
	}

	fn extract_key(&self, unified_key: &str) -> io::Result<String> {
		let mut parts = unified_key.splitn(3, '#');
		let (_primary_namespace, _secondary_namespace) = (parts.next(), parts.next());
		match parts.next() {
			Some(actual_key) => Ok(actual_key.to_string()),
			None => Err(Error::new(ErrorKind::InvalidData, "Invalid key format")),
		}
	}

	async fn list_all_keys(
		&self, primary_namespace: &str, secondary_namespace: &str,
	) -> io::Result<Vec<String>> {
		let mut page_token = None;
		let mut keys = vec![];
		let key_prefix = format!("{}#{}", primary_namespace, secondary_namespace);
		while page_token != Some("".to_string()) {
			let request = ListKeyVersionsRequest {
				store_id: self.store_id.clone(),
				key_prefix: Some(key_prefix.clone()),
				page_token,
				page_size: None,
			};

			let response = self.client.list_key_versions(&request).await.map_err(|e| {
				let msg = format!(
					"Failed to list keys in {}/{}: {}",
					primary_namespace, secondary_namespace, e
				);
				Error::new(ErrorKind::Other, msg)
			})?;

			for kv in response.key_versions {
				keys.push(self.extract_key(&kv.key)?);
			}
			page_token = response.next_page_token;
		}
		Ok(keys)
	}
}

impl KVStore for VssStore {
	fn read(
		&self, primary_namespace: &str, secondary_namespace: &str, key: &str,
	) -> io::Result<Vec<u8>> {
		check_namespace_key_validity(primary_namespace, secondary_namespace, Some(key), "read")?;
		let request = GetObjectRequest {
			store_id: self.store_id.clone(),
			key: self.build_key(primary_namespace, secondary_namespace, key)?,
		};

		let resp =
			tokio::task::block_in_place(|| self.runtime.block_on(self.client.get_object(&request)))
				.map_err(|e| {
					let msg = format!(
						"Failed to read from key {}/{}/{}: {}",
						primary_namespace, secondary_namespace, key, e
					);
					match e {
						VssError::NoSuchKeyError(..) => Error::new(ErrorKind::NotFound, msg),
						_ => Error::new(ErrorKind::Other, msg),
					}
				})?;
		Ok(resp.value.unwrap().value)
	}

	fn write(
		&self, primary_namespace: &str, secondary_namespace: &str, key: &str, buf: &[u8],
	) -> io::Result<()> {
		check_namespace_key_validity(primary_namespace, secondary_namespace, Some(key), "write")?;
		let request = PutObjectRequest {
			store_id: self.store_id.clone(),
			global_version: None,
			transaction_items: vec![KeyValue {
				key: self.build_key(primary_namespace, secondary_namespace, key)?,
				version: -1,
				value: buf.to_vec(),
			}],
			delete_items: vec![],
		};

		tokio::task::block_in_place(|| self.runtime.block_on(self.client.put_object(&request)))
			.map_err(|e| {
				let msg = format!(
					"Failed to write to key {}/{}/{}: {}",
					primary_namespace, secondary_namespace, key, e
				);
				Error::new(ErrorKind::Other, msg)
			})?;

		Ok(())
	}

	fn remove(
		&self, primary_namespace: &str, secondary_namespace: &str, key: &str, _lazy: bool,
	) -> io::Result<()> {
		check_namespace_key_validity(primary_namespace, secondary_namespace, Some(key), "remove")?;
		let request = DeleteObjectRequest {
			store_id: self.store_id.clone(),
			key_value: Some(KeyValue {
				key: self.build_key(primary_namespace, secondary_namespace, key)?,
				version: -1,
				value: vec![],
			}),
		};

		tokio::task::block_in_place(|| self.runtime.block_on(self.client.delete_object(&request)))
			.map_err(|e| {
				let msg = format!(
					"Failed to delete key {}/{}/{}: {}",
					primary_namespace, secondary_namespace, key, e
				);
				Error::new(ErrorKind::Other, msg)
			})?;
		Ok(())
	}

	fn list(&self, primary_namespace: &str, secondary_namespace: &str) -> io::Result<Vec<String>> {
		check_namespace_key_validity(primary_namespace, secondary_namespace, None, "list")?;

		let keys = tokio::task::block_in_place(|| {
			self.runtime.block_on(self.list_all_keys(primary_namespace, secondary_namespace))
		})
		.map_err(|e| {
			let msg = format!(
				"Failed to retrieve keys in namespace: {}/{} : {}",
				primary_namespace, secondary_namespace, e
			);
			Error::new(ErrorKind::Other, msg)
		})?;

		Ok(keys)
	}
}

#[cfg(test)]
impl RefUnwindSafe for VssStore {}

#[cfg(test)]
#[cfg(vss_test)]
mod tests {
	use super::*;
	use crate::io::test_utils::do_read_write_remove_list_persist;
	use rand::distributions::Alphanumeric;
	use rand::{thread_rng, Rng};

	#[test]
	fn read_write_remove_list_persist() {
		let vss_base_url = std::env::var("TEST_VSS_BASE_URL").unwrap();
		let mut rng = thread_rng();
		let rand_store_id: String = (0..7).map(|_| rng.sample(Alphanumeric) as char).collect();
		let vss_store = VssStore::new(&vss_base_url, rand_store_id);

		do_read_write_remove_list_persist(&vss_store);
	}
}

'''
'''--- src/lib.rs ---
// This file is Copyright its original authors, visible in version contror
// history.
//
// This file is licensed under the Apache License, Version 2.0 <LICENSE-APACHE
// or http://www.apache.org/licenses/LICENSE-2.0> or the MIT license
// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your option.
// You may not use this file except in accordance with one or both of these
// licenses.

#![crate_name = "ldk_node"]

//! # LDK Node
//! A ready-to-go Lightning node library built using [LDK](https://lightningdevkit.org/) and
//! [BDK](https://bitcoindevkit.org/).
//!
//! LDK Node is a non-custodial Lightning node in library form. Its central goal is to provide a
//! small, simple, and straightforward interface that enables users to easily set up and run a
//! Lightning node with an integrated on-chain wallet. While minimalism is at its core, LDK Node
//! aims to be sufficiently modular and configurable to be useful for a variety of use cases.
//!
//! ## Getting Started
//!
//! The primary abstraction of the library is the [`Node`], which can be retrieved by setting up
//! and configuring a [`Builder`] to your liking and calling [`build`]. `Node` can then be
//! controlled via commands such as [`start`], [`stop`], [`connect_open_channel`],
//! [`send_payment`], etc.:
//!
//! ```no_run
//! use ldk_node::Builder;
//! use ldk_node::lightning_invoice::Bolt11Invoice;
//! use ldk_node::lightning::ln::msgs::SocketAddress;
//! use ldk_node::bitcoin::secp256k1::PublicKey;
//! use ldk_node::bitcoin::Network;
//! use std::str::FromStr;
//!
//! fn main() {
//! 	let mut builder = Builder::new();
//! 	builder.set_network(Network::Testnet);
//! 	builder.set_esplora_server("https://blockstream.info/testnet/api".to_string());
//! 	builder.set_gossip_source_rgs("https://rapidsync.lightningdevkit.org/testnet/snapshot".to_string());
//!
//! 	let node = builder.build().unwrap();
//!
//! 	node.start().unwrap();
//!
//! 	let funding_address = node.new_onchain_address();
//!
//! 	// .. fund address ..
//!
//! 	let node_id = PublicKey::from_str("NODE_ID").unwrap();
//! 	let node_addr = SocketAddress::from_str("IP_ADDR:PORT").unwrap();
//! 	node.connect_open_channel(node_id, node_addr, 10000, None, None, false).unwrap();
//!
//! 	let event = node.wait_next_event();
//! 	println!("EVENT: {:?}", event);
//! 	node.event_handled();
//!
//! 	let invoice = Bolt11Invoice::from_str("INVOICE_STR").unwrap();
//! 	node.send_payment(&invoice).unwrap();
//!
//! 	node.stop().unwrap();
//! }
//! ```
//!
//! [`build`]: Builder::build
//! [`start`]: Node::start
//! [`stop`]: Node::stop
//! [`connect_open_channel`]: Node::connect_open_channel
//! [`send_payment`]: Node::send_payment
//!
#![cfg_attr(not(feature = "uniffi"), deny(missing_docs))]
#![deny(rustdoc::broken_intra_doc_links)]
#![deny(rustdoc::private_intra_doc_links)]
#![allow(bare_trait_objects)]
#![allow(ellipsis_inclusive_range_patterns)]
#![cfg_attr(docsrs, feature(doc_auto_cfg))]

mod builder;
mod error;
mod event;
mod gossip;
mod hex_utils;
pub mod io;
mod logger;
mod payment_store;
mod peer_store;
#[cfg(test)]
mod test;
mod types;
#[cfg(feature = "uniffi")]
mod uniffi_types;
mod wallet;

pub use bip39;
pub use bitcoin;
pub use lightning;
pub use lightning_invoice;

pub use error::Error as NodeError;
use error::Error;

pub use event::Event;
pub use types::ChannelConfig;

pub use io::utils::generate_entropy_mnemonic;

#[cfg(feature = "uniffi")]
use {bip39::Mnemonic, bitcoin::OutPoint, lightning::ln::PaymentSecret, uniffi_types::*};

#[cfg(feature = "uniffi")]
pub use builder::ArcedNodeBuilder as Builder;
pub use builder::BuildError;
#[cfg(not(feature = "uniffi"))]
pub use builder::NodeBuilder as Builder;

use event::{EventHandler, EventQueue};
use gossip::GossipSource;
use payment_store::PaymentStore;
pub use payment_store::{PaymentDetails, PaymentDirection, PaymentStatus};
use peer_store::{PeerInfo, PeerStore};
use types::{ChainMonitor, ChannelManager, KeysManager, NetworkGraph, PeerManager, Router, Scorer};
pub use types::{ChannelDetails, PeerDetails, UserChannelId};
use wallet::Wallet;

use logger::{log_error, log_info, log_trace, FilesystemLogger, Logger};

use lightning::chain::Confirm;
use lightning::ln::channelmanager::{self, PaymentId, RecipientOnionFields, Retry};
use lightning::ln::msgs::SocketAddress;
use lightning::ln::{ChannelId, PaymentHash, PaymentPreimage};
use lightning::sign::EntropySource;

use lightning::util::persist::KVStore;

use lightning::util::config::{ChannelHandshakeConfig, UserConfig};
pub use lightning::util::logger::Level as LogLevel;

use lightning_background_processor::process_events_async;

use lightning_transaction_sync::EsploraSyncClient;

use lightning::routing::router::{PaymentParameters, RouteParameters};
use lightning_invoice::{payment, Bolt11Invoice, Currency};

use bitcoin::hashes::sha256::Hash as Sha256;
use bitcoin::hashes::Hash;
use bitcoin::secp256k1::PublicKey;
use bitcoin::Network;

use bitcoin::{Address, Txid};

use rand::Rng;

use std::default::Default;
use std::net::ToSocketAddrs;
use std::sync::{Arc, Mutex, RwLock};
use std::time::{Duration, Instant, SystemTime};

#[cfg(feature = "uniffi")]
uniffi::include_scaffolding!("ldk_node");

// Config defaults
const DEFAULT_STORAGE_DIR_PATH: &str = "/tmp/ldk_node/";
const DEFAULT_NETWORK: Network = Network::Bitcoin;
const DEFAULT_CLTV_EXPIRY_DELTA: u32 = 144;
const DEFAULT_BDK_WALLET_SYNC_INTERVAL_SECS: u64 = 80;
const DEFAULT_LDK_WALLET_SYNC_INTERVAL_SECS: u64 = 30;
const DEFAULT_FEE_RATE_CACHE_UPDATE_INTERVAL_SECS: u64 = 60 * 10;
const DEFAULT_PROBING_LIQUIDITY_LIMIT_MULTIPLIER: u64 = 3;
const DEFAULT_LOG_LEVEL: LogLevel = LogLevel::Debug;

// The 'stop gap' parameter used by BDK's wallet sync. This seems to configure the threshold
// number of derivation indexes after which BDK stops looking for new scripts belonging to the wallet.
const BDK_CLIENT_STOP_GAP: usize = 20;

// The number of concurrent requests made against the API provider.
const BDK_CLIENT_CONCURRENCY: u8 = 4;

// The default Esplora server we're using.
const DEFAULT_ESPLORA_SERVER_URL: &str = "https://blockstream.info/api";

// The timeout after which we abandon retrying failed payments.
const LDK_PAYMENT_RETRY_TIMEOUT: Duration = Duration::from_secs(10);

// The time in-between peer reconnection attempts.
const PEER_RECONNECTION_INTERVAL: Duration = Duration::from_secs(10);

// The time in-between RGS sync attempts.
const RGS_SYNC_INTERVAL: Duration = Duration::from_secs(60 * 60);

// The time in-between node announcement broadcast attempts.
const NODE_ANN_BCAST_INTERVAL: Duration = Duration::from_secs(60 * 60);

// The lower limit which we apply to any configured wallet sync intervals.
const WALLET_SYNC_INTERVAL_MINIMUM_SECS: u64 = 10;

// The length in bytes of our wallets' keys seed.
const WALLET_KEYS_SEED_LEN: usize = 64;

#[derive(Debug, Clone)]
/// Represents the configuration of an [`Node`] instance.
///
/// ### Defaults
///
/// | Parameter                              | Value              |
/// |----------------------------------------|--------------------|
/// | `storage_dir_path`                     | /tmp/ldk_node/     |
/// | `log_dir_path`                         | None               |
/// | `network`                              | Bitcoin            |
/// | `listening_addresses`                  | None               |
/// | `default_cltv_expiry_delta`            | 144                |
/// | `onchain_wallet_sync_interval_secs`    | 80                 |
/// | `wallet_sync_interval_secs`            | 30                 |
/// | `fee_rate_cache_update_interval_secs`  | 600                |
/// | `trusted_peers_0conf`                  | []                 |
/// | `probing_liquidity_limit_multiplier`   | 3                  |
/// | `log_level`                            | Debug              |
///
pub struct Config {
	/// The path where the underlying LDK and BDK persist their data.
	pub storage_dir_path: String,
	/// The path where logs are stored.
	///
	/// If set to `None`, logs can be found in the `logs` subdirectory in [`Config::storage_dir_path`].
	pub log_dir_path: Option<String>,
	/// The used Bitcoin network.
	pub network: Network,
	/// The addresses on which the node will listen for incoming connections.
	pub listening_addresses: Option<Vec<SocketAddress>>,
	/// The default CLTV expiry delta to be used for payments.
	pub default_cltv_expiry_delta: u32,
	/// The time in-between background sync attempts of the onchain wallet, in seconds.
	///
	/// **Note:** A minimum of 10 seconds is always enforced.
	pub onchain_wallet_sync_interval_secs: u64,
	/// The time in-between background sync attempts of the LDK wallet, in seconds.
	///
	/// **Note:** A minimum of 10 seconds is always enforced.
	pub wallet_sync_interval_secs: u64,
	/// The time in-between background update attempts to our fee rate cache, in seconds.
	///
	/// **Note:** A minimum of 10 seconds is always enforced.
	pub fee_rate_cache_update_interval_secs: u64,
	/// A list of peers that we allow to establish zero confirmation channels to us.
	///
	/// **Note:** Allowing payments via zero-confirmation channels is potentially insecure if the
	/// funding transaction ends up never being confirmed on-chain. Zero-confirmation channels
	/// should therefore only be accepted from trusted peers.
	pub trusted_peers_0conf: Vec<PublicKey>,
	/// The liquidity factor by which we filter the outgoing channels used for sending probes.
	///
	/// Channels with available liquidity less than the required amount times this value won't be
	/// used to send pre-flight probes.
	pub probing_liquidity_limit_multiplier: u64,
	/// The level at which we log messages.
	///
	/// Any messages below this level will be excluded from the logs.
	pub log_level: LogLevel,
}

impl Default for Config {
	fn default() -> Self {
		Self {
			storage_dir_path: DEFAULT_STORAGE_DIR_PATH.to_string(),
			log_dir_path: None,
			network: DEFAULT_NETWORK,
			listening_addresses: None,
			default_cltv_expiry_delta: DEFAULT_CLTV_EXPIRY_DELTA,
			onchain_wallet_sync_interval_secs: DEFAULT_BDK_WALLET_SYNC_INTERVAL_SECS,
			wallet_sync_interval_secs: DEFAULT_LDK_WALLET_SYNC_INTERVAL_SECS,
			fee_rate_cache_update_interval_secs: DEFAULT_FEE_RATE_CACHE_UPDATE_INTERVAL_SECS,
			trusted_peers_0conf: Vec::new(),
			probing_liquidity_limit_multiplier: DEFAULT_PROBING_LIQUIDITY_LIMIT_MULTIPLIER,
			log_level: DEFAULT_LOG_LEVEL,
		}
	}
}

/// The main interface object of LDK Node, wrapping the necessary LDK and BDK functionalities.
///
/// Needs to be initialized and instantiated through [`Builder::build`].
pub struct Node<K: KVStore + Sync + Send + 'static> {
	runtime: Arc<RwLock<Option<tokio::runtime::Runtime>>>,
	stop_sender: tokio::sync::watch::Sender<()>,
	stop_receiver: tokio::sync::watch::Receiver<()>,
	config: Arc<Config>,
	wallet: Arc<Wallet<bdk::database::SqliteDatabase, Arc<FilesystemLogger>>>,
	tx_sync: Arc<EsploraSyncClient<Arc<FilesystemLogger>>>,
	event_queue: Arc<EventQueue<K, Arc<FilesystemLogger>>>,
	channel_manager: Arc<ChannelManager<K>>,
	chain_monitor: Arc<ChainMonitor<K>>,
	peer_manager: Arc<PeerManager<K>>,
	keys_manager: Arc<KeysManager>,
	network_graph: Arc<NetworkGraph>,
	gossip_source: Arc<GossipSource>,
	kv_store: Arc<K>,
	logger: Arc<FilesystemLogger>,
	_router: Arc<Router>,
	scorer: Arc<Mutex<Scorer>>,
	peer_store: Arc<PeerStore<K, Arc<FilesystemLogger>>>,
	payment_store: Arc<PaymentStore<K, Arc<FilesystemLogger>>>,
}

impl<K: KVStore + Sync + Send + 'static> Node<K> {
	/// Starts the necessary background tasks, such as handling events coming from user input,
	/// LDK/BDK, and the peer-to-peer network.
	///
	/// After this returns, the [`Node`] instance can be controlled via the provided API methods in
	/// a thread-safe manner.
	pub fn start(&self) -> Result<(), Error> {
		// Acquire a run lock and hold it until we're setup.
		let mut runtime_lock = self.runtime.write().unwrap();
		if runtime_lock.is_some() {
			// We're already running.
			return Err(Error::AlreadyRunning);
		}

		log_info!(self.logger, "Starting up LDK Node on network: {}", self.config.network);

		let runtime = tokio::runtime::Builder::new_multi_thread().enable_all().build().unwrap();

		// Block to ensure we update our fee rate cache once on startup
		let wallet = Arc::clone(&self.wallet);
		let sync_logger = Arc::clone(&self.logger);
		let runtime_ref = &runtime;
		tokio::task::block_in_place(move || {
			runtime_ref.block_on(async move {
				let now = Instant::now();
				match wallet.update_fee_estimates().await {
					Ok(()) => {
						log_info!(
							sync_logger,
							"Initial fee rate cache update finished in {}ms.",
							now.elapsed().as_millis()
						);
						Ok(())
					}
					Err(e) => {
						log_error!(sync_logger, "Initial fee rate cache update failed: {}", e,);
						Err(e)
					}
				}
			})
		})?;

		// Setup wallet sync
		let wallet = Arc::clone(&self.wallet);
		let sync_logger = Arc::clone(&self.logger);
		let mut stop_sync = self.stop_receiver.clone();
		let onchain_wallet_sync_interval_secs =
			self.config.onchain_wallet_sync_interval_secs.max(WALLET_SYNC_INTERVAL_MINIMUM_SECS);
		let fee_rate_cache_update_interval_secs =
			self.config.fee_rate_cache_update_interval_secs.max(WALLET_SYNC_INTERVAL_MINIMUM_SECS);
		std::thread::spawn(move || {
			tokio::runtime::Builder::new_current_thread().enable_all().build().unwrap().block_on(
				async move {
					let mut onchain_wallet_sync_interval = tokio::time::interval(
						Duration::from_secs(onchain_wallet_sync_interval_secs),
					);
					onchain_wallet_sync_interval
						.set_missed_tick_behavior(tokio::time::MissedTickBehavior::Skip);
					let mut fee_rate_update_interval = tokio::time::interval(Duration::from_secs(
						fee_rate_cache_update_interval_secs,
					));
					// We just blocked on updating, so skip the first tick.
					fee_rate_update_interval.reset();
					loop {
						tokio::select! {
							_ = stop_sync.changed() => {
								return;
							}
							_ = onchain_wallet_sync_interval.tick() => {
								let now = Instant::now();
								match wallet.sync().await {
									Ok(()) => log_trace!(
										sync_logger,
										"Background sync of on-chain wallet finished in {}ms.",
										now.elapsed().as_millis()
										),
									Err(err) => {
										log_error!(
											sync_logger,
											"Background sync of on-chain wallet failed: {}",
											err
											)
									}
								}
							}
							_ = fee_rate_update_interval.tick() => {
								let now = Instant::now();
								match wallet.update_fee_estimates().await {
									Ok(()) => log_trace!(
										sync_logger,
										"Background update of fee rate cache finished in {}ms.",
										now.elapsed().as_millis()
										),
									Err(err) => {
										log_error!(
											sync_logger,
											"Background update of fee rate cache failed: {}",
											err
											)
									}
								}
							}
						}
					}
				},
			);
		});

		let tx_sync = Arc::clone(&self.tx_sync);
		let sync_cman = Arc::clone(&self.channel_manager);
		let sync_cmon = Arc::clone(&self.chain_monitor);
		let sync_logger = Arc::clone(&self.logger);
		let mut stop_sync = self.stop_receiver.clone();
		let wallet_sync_interval_secs =
			self.config.wallet_sync_interval_secs.max(WALLET_SYNC_INTERVAL_MINIMUM_SECS);
		runtime.spawn(async move {
			let mut wallet_sync_interval =
				tokio::time::interval(Duration::from_secs(wallet_sync_interval_secs));
			wallet_sync_interval.set_missed_tick_behavior(tokio::time::MissedTickBehavior::Skip);
			loop {
				tokio::select! {
					_ = stop_sync.changed() => {
						return;
					}
					_ = wallet_sync_interval.tick() => {
						let confirmables = vec![
							&*sync_cman as &(dyn Confirm + Sync + Send),
							&*sync_cmon as &(dyn Confirm + Sync + Send),
						];
						let now = Instant::now();
						match tx_sync.sync(confirmables).await {
							Ok(()) => log_trace!(
								sync_logger,
								"Background sync of Lightning wallet finished in {}ms.",
								now.elapsed().as_millis()
								),
							Err(e) => {
								log_error!(sync_logger, "Background sync of Lightning wallet failed: {}", e)
							}
						}
					}
				}
			}
		});

		if self.gossip_source.is_rgs() {
			let gossip_source = Arc::clone(&self.gossip_source);
			let gossip_sync_store = Arc::clone(&self.kv_store);
			let gossip_sync_logger = Arc::clone(&self.logger);
			let mut stop_gossip_sync = self.stop_receiver.clone();
			runtime.spawn(async move {
				let mut interval = tokio::time::interval(RGS_SYNC_INTERVAL);
				loop {
					tokio::select! {
						_ = stop_gossip_sync.changed() => {
							return;
						}
						_ = interval.tick() => {
							let gossip_sync_logger = Arc::clone(&gossip_sync_logger);
							let now = Instant::now();
							match gossip_source.update_rgs_snapshot().await {
								Ok(updated_timestamp) => {
									log_trace!(
										gossip_sync_logger,
										"Background sync of RGS gossip data finished in {}ms.",
										now.elapsed().as_millis()
										);
									io::utils::write_latest_rgs_sync_timestamp(
										updated_timestamp,
										Arc::clone(&gossip_sync_store),
										Arc::clone(&gossip_sync_logger),
										)
										.unwrap_or_else(|e| {
											log_error!(gossip_sync_logger, "Persistence failed: {}", e);
											panic!("Persistence failed");
										});
								}
								Err(e) => log_error!(
									gossip_sync_logger,
									"Background sync of RGS gossip data failed: {}",
									e
									),
							}
						}
					}
				}
			});
		}

		if let Some(listening_addresses) = &self.config.listening_addresses {
			// Setup networking
			let peer_manager_connection_handler = Arc::clone(&self.peer_manager);
			let mut stop_listen = self.stop_receiver.clone();
			let listening_logger = Arc::clone(&self.logger);

			let mut bind_addrs = Vec::with_capacity(listening_addresses.len());

			for listening_addr in listening_addresses {
				let resolved_address = listening_addr.to_socket_addrs().map_err(|e| {
					log_error!(
						self.logger,
						"Unable to resolve listening address: {:?}. Error details: {}",
						listening_addr,
						e,
					);
					Error::InvalidSocketAddress
				})?;

				bind_addrs.extend(resolved_address);
			}

			runtime.spawn(async move {
				let listener =
					tokio::net::TcpListener::bind(&*bind_addrs).await
										.unwrap_or_else(|e| {
											log_error!(listening_logger, "Failed to bind to listen addresses/ports - is something else already listening on it?: {}", e);
											panic!(
												"Failed to bind to listen address/port - is something else already listening on it?",
												);
										});

				loop {
					let peer_mgr = Arc::clone(&peer_manager_connection_handler);
					tokio::select! {
						_ = stop_listen.changed() => {
							return;
						}
						res = listener.accept() => {
							let tcp_stream = res.unwrap().0;
							tokio::spawn(async move {
								lightning_net_tokio::setup_inbound(
									Arc::clone(&peer_mgr),
									tcp_stream.into_std().unwrap(),
									)
									.await;
							});
						}
					}
				}
			});
		}

		// Regularly reconnect to channel peers.
		let connect_cm = Arc::clone(&self.channel_manager);
		let connect_pm = Arc::clone(&self.peer_manager);
		let connect_logger = Arc::clone(&self.logger);
		let connect_peer_store = Arc::clone(&self.peer_store);
		let mut stop_connect = self.stop_receiver.clone();
		runtime.spawn(async move {
			let mut interval = tokio::time::interval(PEER_RECONNECTION_INTERVAL);
			interval.set_missed_tick_behavior(tokio::time::MissedTickBehavior::Skip);
			loop {
				tokio::select! {
						_ = stop_connect.changed() => {
							return;
						}
						_ = interval.tick() => {
							let pm_peers = connect_pm
								.get_peer_node_ids()
								.iter()
								.map(|(peer, _addr)| *peer)
								.collect::<Vec<_>>();
							for node_id in connect_cm
								.list_channels()
									.iter()
									.map(|chan| chan.counterparty.node_id)
									.filter(|id| !pm_peers.contains(id))
									{
										if let Some(peer_info) = connect_peer_store.get_peer(&node_id) {
											let _ = do_connect_peer(
												peer_info.node_id,
												peer_info.address,
												Arc::clone(&connect_pm),
												Arc::clone(&connect_logger),
												)
												.await;
										}
									}
						}
				}
			}
		});

		// Regularly broadcast node announcements.
		let bcast_cm = Arc::clone(&self.channel_manager);
		let bcast_pm = Arc::clone(&self.peer_manager);
		let bcast_config = Arc::clone(&self.config);
		let bcast_store = Arc::clone(&self.kv_store);
		let bcast_logger = Arc::clone(&self.logger);
		let mut stop_bcast = self.stop_receiver.clone();
		runtime.spawn(async move {
			// We check every 30 secs whether our last broadcast is NODE_ANN_BCAST_INTERVAL away.
			let mut interval = tokio::time::interval(Duration::from_secs(30));
			loop {
				tokio::select! {
						_ = stop_bcast.changed() => {
							return;
						}
						_ = interval.tick() => {
							let skip_broadcast = match io::utils::read_latest_node_ann_bcast_timestamp(Arc::clone(&bcast_store), Arc::clone(&bcast_logger)) {
								Ok(latest_bcast_time_secs) => {
									// Skip if the time hasn't elapsed yet.
									let next_bcast_unix_time = SystemTime::UNIX_EPOCH + Duration::from_secs(latest_bcast_time_secs) + NODE_ANN_BCAST_INTERVAL;
									next_bcast_unix_time.elapsed().is_err()
								}
								Err(_) => {
									// Don't skip if we haven't broadcasted before.
									false
								}
							};

							if skip_broadcast {
								continue;
							}

							if bcast_cm.list_channels().iter().any(|chan| chan.is_public) {
								// Skip if we don't have any public channels.
								continue;
							}

							if bcast_pm.get_peer_node_ids().is_empty() {
								// Skip if we don't have any connected peers to gossip to.
								continue;
							}

							let addresses = bcast_config.listening_addresses.clone().unwrap_or(Vec::new());

							if addresses.is_empty() {
								// Skip if we are not listening on any addresses.
								continue;
							}

							bcast_pm.broadcast_node_announcement([0; 3], [0; 32], addresses);

							let unix_time_secs = SystemTime::now().duration_since(SystemTime::UNIX_EPOCH).unwrap().as_secs();
							io::utils::write_latest_node_ann_bcast_timestamp(unix_time_secs, Arc::clone(&bcast_store), Arc::clone(&bcast_logger))
								.unwrap_or_else(|e| {
									log_error!(bcast_logger, "Persistence failed: {}", e);
									panic!("Persistence failed");
								});
						}
				}
			}
		});

		let event_handler = Arc::new(EventHandler::new(
			Arc::clone(&self.wallet),
			Arc::clone(&self.event_queue),
			Arc::clone(&self.channel_manager),
			Arc::clone(&self.network_graph),
			Arc::clone(&self.keys_manager),
			Arc::clone(&self.payment_store),
			Arc::clone(&self.runtime),
			Arc::clone(&self.logger),
			Arc::clone(&self.config),
		));

		// Setup background processing
		let background_persister = Arc::clone(&self.kv_store);
		let background_event_handler = Arc::clone(&event_handler);
		let background_chain_mon = Arc::clone(&self.chain_monitor);
		let background_chan_man = Arc::clone(&self.channel_manager);
		let background_gossip_sync = self.gossip_source.as_gossip_sync();
		let background_peer_man = Arc::clone(&self.peer_manager);
		let background_logger = Arc::clone(&self.logger);
		let background_error_logger = Arc::clone(&self.logger);
		let background_scorer = Arc::clone(&self.scorer);
		let stop_bp = self.stop_receiver.clone();
		let sleeper = move |d| {
			let mut stop = stop_bp.clone();
			Box::pin(async move {
				tokio::select! {
					_ = stop.changed() => {
						true
					}
					_ = tokio::time::sleep(d) => {
						false
					}
				}
			})
		};

		runtime.spawn(async move {
			process_events_async(
				background_persister,
				|e| background_event_handler.handle_event(e),
				background_chain_mon,
				background_chan_man,
				background_gossip_sync,
				background_peer_man,
				background_logger,
				Some(background_scorer),
				sleeper,
				true,
			)
			.await
			.unwrap_or_else(|e| {
				log_error!(background_error_logger, "Failed to process events: {}", e);
				panic!("Failed to process events");
			});
		});

		*runtime_lock = Some(runtime);

		log_info!(self.logger, "Startup complete.");
		Ok(())
	}

	/// Returns whether the [`Node`] is running.
	pub fn is_running(&self) -> bool {
		self.runtime.read().unwrap().is_some()
	}

	/// Disconnects all peers, stops all running background tasks, and shuts down [`Node`].
	///
	/// After this returns most API methods will return [`Error::NotRunning`].
	pub fn stop(&self) -> Result<(), Error> {
		let runtime = self.runtime.write().unwrap().take().ok_or(Error::NotRunning)?;

		log_info!(self.logger, "Shutting down LDK Node...");

		// Stop the runtime.
		match self.stop_sender.send(()) {
			Ok(_) => (),
			Err(e) => {
				log_error!(
					self.logger,
					"Failed to send shutdown signal. This should never happen: {}",
					e
				);
				debug_assert!(false);
			}
		}

		// Stop disconnect peers.
		self.peer_manager.disconnect_all_peers();

		runtime.shutdown_timeout(Duration::from_secs(10));

		log_info!(self.logger, "Shutdown complete.");
		Ok(())
	}

	/// Returns the next event in the event queue, if currently available.
	///
	/// Will return `Some(..)` if an event is available and `None` otherwise.
	///
	/// **Note:** this will always return the same event until handling is confirmed via [`Node::event_handled`].
	pub fn next_event(&self) -> Option<Event> {
		self.event_queue.next_event()
	}

	/// Returns the next event in the event queue.
	///
	/// Will block the current thread until the next event is available.
	///
	/// **Note:** this will always return the same event until handling is confirmed via [`Node::event_handled`].
	pub fn wait_next_event(&self) -> Event {
		self.event_queue.wait_next_event()
	}

	/// Confirm the last retrieved event handled.
	///
	/// **Note:** This **MUST** be called after each event has been handled.
	pub fn event_handled(&self) {
		self.event_queue.event_handled().unwrap_or_else(|e| {
			log_error!(
				self.logger,
				"Couldn't mark event handled due to persistence failure: {}",
				e
			);
			panic!("Couldn't mark event handled due to persistence failure");
		});
	}

	/// Returns our own node id
	pub fn node_id(&self) -> PublicKey {
		self.channel_manager.get_our_node_id()
	}

	/// Returns our own listening addresses.
	pub fn listening_addresses(&self) -> Option<Vec<SocketAddress>> {
		self.config.listening_addresses.clone()
	}

	/// Retrieve a new on-chain/funding address.
	pub fn new_onchain_address(&self) -> Result<Address, Error> {
		let funding_address = self.wallet.get_new_address()?;
		log_info!(self.logger, "Generated new funding address: {}", funding_address);
		Ok(funding_address)
	}

	/// Retrieve the currently spendable on-chain balance in satoshis.
	pub fn spendable_onchain_balance_sats(&self) -> Result<u64, Error> {
		Ok(self.wallet.get_balance().map(|bal| bal.get_spendable())?)
	}

	/// Retrieve the current total on-chain balance in satoshis.
	pub fn total_onchain_balance_sats(&self) -> Result<u64, Error> {
		Ok(self.wallet.get_balance().map(|bal| bal.get_total())?)
	}

	/// Send an on-chain payment to the given address.
	pub fn send_to_onchain_address(
		&self, address: &bitcoin::Address, amount_sats: u64,
	) -> Result<Txid, Error> {
		let rt_lock = self.runtime.read().unwrap();
		if rt_lock.is_none() {
			return Err(Error::NotRunning);
		}

		let cur_balance = self.wallet.get_balance()?;
		if cur_balance.get_spendable() < amount_sats {
			log_error!(self.logger, "Unable to send payment due to insufficient funds.");
			return Err(Error::InsufficientFunds);
		}
		self.wallet.send_to_address(address, Some(amount_sats))
	}

	/// Send an on-chain payment to the given address, draining all the available funds.
	pub fn send_all_to_onchain_address(&self, address: &bitcoin::Address) -> Result<Txid, Error> {
		let rt_lock = self.runtime.read().unwrap();
		if rt_lock.is_none() {
			return Err(Error::NotRunning);
		}

		self.wallet.send_to_address(address, None)
	}

	/// Retrieve a list of known channels.
	pub fn list_channels(&self) -> Vec<ChannelDetails> {
		self.channel_manager.list_channels().into_iter().map(|c| c.into()).collect()
	}

	/// Connect to a node on the peer-to-peer network.
	///
	/// If `persist` is set to `true`, we'll remember the peer and reconnect to it on restart.
	pub fn connect(
		&self, node_id: PublicKey, address: SocketAddress, persist: bool,
	) -> Result<(), Error> {
		let rt_lock = self.runtime.read().unwrap();
		if rt_lock.is_none() {
			return Err(Error::NotRunning);
		}
		let runtime = rt_lock.as_ref().unwrap();

		let peer_info = PeerInfo { node_id, address };

		let con_node_id = peer_info.node_id;
		let con_addr = peer_info.address.clone();
		let con_logger = Arc::clone(&self.logger);
		let con_pm = Arc::clone(&self.peer_manager);

		// We need to use our main runtime here as a local runtime might not be around to poll
		// connection futures going forward.
		tokio::task::block_in_place(move || {
			runtime.block_on(async move {
				connect_peer_if_necessary(con_node_id, con_addr, con_pm, con_logger).await
			})
		})?;

		log_info!(self.logger, "Connected to peer {}@{}. ", peer_info.node_id, peer_info.address);

		if persist {
			self.peer_store.add_peer(peer_info)?;
		}

		Ok(())
	}

	/// Disconnects the peer with the given node id.
	///
	/// Will also remove the peer from the peer store, i.e., after this has been called we won't
	/// try to reconnect on restart.
	pub fn disconnect(&self, counterparty_node_id: PublicKey) -> Result<(), Error> {
		let rt_lock = self.runtime.read().unwrap();
		if rt_lock.is_none() {
			return Err(Error::NotRunning);
		}

		log_info!(self.logger, "Disconnecting peer {}..", counterparty_node_id);

		match self.peer_store.remove_peer(&counterparty_node_id) {
			Ok(()) => {}
			Err(e) => {
				log_error!(self.logger, "Failed to remove peer {}: {}", counterparty_node_id, e)
			}
		}

		self.peer_manager.disconnect_by_node_id(counterparty_node_id);
		Ok(())
	}

	/// Connect to a node and open a new channel. Disconnects and re-connects are handled automatically
	///
	/// Disconnects and reconnects are handled automatically.
	///
	/// If `push_to_counterparty_msat` is set, the given value will be pushed (read: sent) to the
	/// channel counterparty on channel open. This can be useful to start out with the balance not
	/// entirely shifted to one side, therefore allowing to receive payments from the getgo.
	///
	/// Returns a temporary channel id.
	pub fn connect_open_channel(
		&self, node_id: PublicKey, address: SocketAddress, channel_amount_sats: u64,
		push_to_counterparty_msat: Option<u64>, channel_config: Option<Arc<ChannelConfig>>,
		announce_channel: bool,
	) -> Result<(), Error> {
		let rt_lock = self.runtime.read().unwrap();
		if rt_lock.is_none() {
			return Err(Error::NotRunning);
		}
		let runtime = rt_lock.as_ref().unwrap();

		let cur_balance = self.wallet.get_balance()?;
		if cur_balance.get_spendable() < channel_amount_sats {
			log_error!(self.logger, "Unable to create channel due to insufficient funds.");
			return Err(Error::InsufficientFunds);
		}

		let peer_info = PeerInfo { node_id, address };

		let con_node_id = peer_info.node_id;
		let con_addr = peer_info.address.clone();
		let con_logger = Arc::clone(&self.logger);
		let con_pm = Arc::clone(&self.peer_manager);

		// We need to use our main runtime here as a local runtime might not be around to poll
		// connection futures going forward.
		tokio::task::block_in_place(move || {
			runtime.block_on(async move {
				connect_peer_if_necessary(con_node_id, con_addr, con_pm, con_logger).await
			})
		})?;

		let channel_config = (*(channel_config.unwrap_or_default())).clone().into();
		let user_config = UserConfig {
			channel_handshake_limits: Default::default(),
			channel_handshake_config: ChannelHandshakeConfig {
				announced_channel: announce_channel,
				..Default::default()
			},
			channel_config,
			..Default::default()
		};

		let push_msat = push_to_counterparty_msat.unwrap_or(0);
		let user_channel_id: u128 = rand::thread_rng().gen::<u128>();

		match self.channel_manager.create_channel(
			peer_info.node_id,
			channel_amount_sats,
			push_msat,
			user_channel_id,
			Some(user_config),
		) {
			Ok(_) => {
				log_info!(
					self.logger,
					"Initiated channel creation with peer {}. ",
					peer_info.node_id
				);
				self.peer_store.add_peer(peer_info)?;
				Ok(())
			}
			Err(e) => {
				log_error!(self.logger, "Failed to initiate channel creation: {:?}", e);
				Err(Error::ChannelCreationFailed)
			}
		}
	}

	/// Manually sync the LDK and BDK wallets with the current chain state.
	///
	/// **Note:** The wallets are regularly synced in the background, which is configurable via
	/// [`Config::onchain_wallet_sync_interval_secs`] and [`Config::wallet_sync_interval_secs`].
	/// Therefore, using this blocking sync method is almost always redudant and should be avoided
	/// where possible.
	pub fn sync_wallets(&self) -> Result<(), Error> {
		let rt_lock = self.runtime.read().unwrap();
		if rt_lock.is_none() {
			return Err(Error::NotRunning);
		}

		let wallet = Arc::clone(&self.wallet);
		let tx_sync = Arc::clone(&self.tx_sync);
		let sync_cman = Arc::clone(&self.channel_manager);
		let sync_cmon = Arc::clone(&self.chain_monitor);
		let sync_logger = Arc::clone(&self.logger);
		let confirmables = vec![
			&*sync_cman as &(dyn Confirm + Sync + Send),
			&*sync_cmon as &(dyn Confirm + Sync + Send),
		];

		tokio::task::block_in_place(move || {
			tokio::runtime::Builder::new_current_thread().enable_all().build().unwrap().block_on(
				async move {
					let now = Instant::now();
					match wallet.sync().await {
						Ok(()) => {
							log_info!(
								sync_logger,
								"Sync of on-chain wallet finished in {}ms.",
								now.elapsed().as_millis()
							);
						}
						Err(e) => {
							log_error!(sync_logger, "Sync of on-chain wallet failed: {}", e);
							return Err(e);
						}
					};

					let now = Instant::now();
					match tx_sync.sync(confirmables).await {
						Ok(()) => {
							log_info!(
								sync_logger,
								"Sync of Lightning wallet finished in {}ms.",
								now.elapsed().as_millis()
							);
							Ok(())
						}
						Err(e) => {
							log_error!(sync_logger, "Sync of Lightning wallet failed: {}", e);
							Err(e.into())
						}
					}
				},
			)
		})
	}

	/// Close a previously opened channel.
	pub fn close_channel(
		&self, channel_id: &ChannelId, counterparty_node_id: PublicKey,
	) -> Result<(), Error> {
		self.peer_store.remove_peer(&counterparty_node_id)?;
		match self.channel_manager.close_channel(&channel_id, &counterparty_node_id) {
			Ok(_) => Ok(()),
			Err(_) => Err(Error::ChannelClosingFailed),
		}
	}

	/// Update the config for a previously opened channel.
	pub fn update_channel_config(
		&self, channel_id: &ChannelId, counterparty_node_id: PublicKey,
		channel_config: Arc<ChannelConfig>,
	) -> Result<(), Error> {
		self.channel_manager
			.update_channel_config(
				&counterparty_node_id,
				&[*channel_id],
				&(*channel_config).clone().into(),
			)
			.map_err(|_| Error::ChannelConfigUpdateFailed)
	}

	/// Send a payment given an invoice.
	pub fn send_payment(&self, invoice: &Bolt11Invoice) -> Result<PaymentHash, Error> {
		let rt_lock = self.runtime.read().unwrap();
		if rt_lock.is_none() {
			return Err(Error::NotRunning);
		}

		let payment_hash = PaymentHash((*invoice.payment_hash()).into_inner());

		if let Some(payment) = self.payment_store.get(&payment_hash) {
			if payment.status == PaymentStatus::Pending
				|| payment.status == PaymentStatus::Succeeded
			{
				log_error!(self.logger, "Payment error: an invoice must not be paid twice.");
				return Err(Error::DuplicatePayment);
			}
		}

		let payment_secret = Some(*invoice.payment_secret());

		match lightning_invoice::payment::pay_invoice(
			&invoice,
			Retry::Timeout(LDK_PAYMENT_RETRY_TIMEOUT),
			self.channel_manager.as_ref(),
		) {
			Ok(_payment_id) => {
				let payee_pubkey = invoice.recover_payee_pub_key();
				let amt_msat = invoice.amount_milli_satoshis().unwrap();
				log_info!(self.logger, "Initiated sending {}msat to {}", amt_msat, payee_pubkey);

				let payment = PaymentDetails {
					preimage: None,
					hash: payment_hash,
					secret: payment_secret,
					amount_msat: invoice.amount_milli_satoshis(),
					direction: PaymentDirection::Outbound,
					status: PaymentStatus::Pending,
				};
				self.payment_store.insert(payment)?;

				Ok(payment_hash)
			}
			Err(payment::PaymentError::Invoice(e)) => {
				log_error!(self.logger, "Failed to send payment due to invalid invoice: {}", e);
				Err(Error::InvalidInvoice)
			}
			Err(payment::PaymentError::Sending(e)) => {
				log_error!(self.logger, "Failed to send payment: {:?}", e);
				match e {
					channelmanager::RetryableSendFailure::DuplicatePayment => {
						Err(Error::DuplicatePayment)
					}
					_ => {
						let payment = PaymentDetails {
							preimage: None,
							hash: payment_hash,
							secret: payment_secret,
							amount_msat: invoice.amount_milli_satoshis(),
							direction: PaymentDirection::Outbound,
							status: PaymentStatus::Failed,
						};

						self.payment_store.insert(payment)?;
						Err(Error::PaymentSendingFailed)
					}
				}
			}
		}
	}

	/// Send a payment given an invoice and an amount in millisatoshi.
	///
	/// This will fail if the amount given is less than the value required by the given invoice.
	///
	/// This can be used to pay a so-called "zero-amount" invoice, i.e., an invoice that leaves the
	/// amount paid to be determined by the user.
	pub fn send_payment_using_amount(
		&self, invoice: &Bolt11Invoice, amount_msat: u64,
	) -> Result<PaymentHash, Error> {
		let rt_lock = self.runtime.read().unwrap();
		if rt_lock.is_none() {
			return Err(Error::NotRunning);
		}

		if let Some(invoice_amount_msat) = invoice.amount_milli_satoshis() {
			if amount_msat < invoice_amount_msat {
				log_error!(
					self.logger,
					"Failed to pay as the given amount needs to be at least the invoice amount: required {}msat, gave {}msat.", invoice_amount_msat, amount_msat);
				return Err(Error::InvalidAmount);
			}
		}

		let payment_hash = PaymentHash((*invoice.payment_hash()).into_inner());
		if let Some(payment) = self.payment_store.get(&payment_hash) {
			if payment.status == PaymentStatus::Pending
				|| payment.status == PaymentStatus::Succeeded
			{
				log_error!(self.logger, "Payment error: an invoice must not be paid twice.");
				return Err(Error::DuplicatePayment);
			}
		}

		let payment_id = PaymentId(invoice.payment_hash().into_inner());
		let payment_secret = invoice.payment_secret();
		let expiry_time = invoice.duration_since_epoch().saturating_add(invoice.expiry_time());
		let mut payment_params = PaymentParameters::from_node_id(
			invoice.recover_payee_pub_key(),
			invoice.min_final_cltv_expiry_delta() as u32,
		)
		.with_expiry_time(expiry_time.as_secs())
		.with_route_hints(invoice.route_hints())
		.map_err(|_| Error::InvalidInvoice)?;
		if let Some(features) = invoice.features() {
			payment_params = payment_params
				.with_bolt11_features(features.clone())
				.map_err(|_| Error::InvalidInvoice)?;
		}
		let route_params =
			RouteParameters::from_payment_params_and_value(payment_params, amount_msat);

		let retry_strategy = Retry::Timeout(LDK_PAYMENT_RETRY_TIMEOUT);
		let recipient_fields = RecipientOnionFields::secret_only(*payment_secret);

		match self
			.channel_manager
			.send_payment(payment_hash, recipient_fields, payment_id, route_params, retry_strategy)
			.map_err(payment::PaymentError::Sending)
		{
			Ok(_payment_id) => {
				let payee_pubkey = invoice.recover_payee_pub_key();
				log_info!(
					self.logger,
					"Initiated sending {} msat to {}",
					amount_msat,
					payee_pubkey
				);

				let payment = PaymentDetails {
					hash: payment_hash,
					preimage: None,
					secret: Some(*payment_secret),
					amount_msat: Some(amount_msat),
					direction: PaymentDirection::Outbound,
					status: PaymentStatus::Pending,
				};
				self.payment_store.insert(payment)?;

				Ok(payment_hash)
			}
			Err(payment::PaymentError::Invoice(e)) => {
				log_error!(self.logger, "Failed to send payment due to invalid invoice: {}", e);
				Err(Error::InvalidInvoice)
			}
			Err(payment::PaymentError::Sending(e)) => {
				log_error!(self.logger, "Failed to send payment: {:?}", e);

				match e {
					channelmanager::RetryableSendFailure::DuplicatePayment => {
						Err(Error::DuplicatePayment)
					}
					_ => {
						let payment = PaymentDetails {
							hash: payment_hash,
							preimage: None,
							secret: Some(*payment_secret),
							amount_msat: Some(amount_msat),
							direction: PaymentDirection::Outbound,
							status: PaymentStatus::Failed,
						};
						self.payment_store.insert(payment)?;

						Err(Error::PaymentSendingFailed)
					}
				}
			}
		}
	}

	/// Send a spontaneous, aka. "keysend", payment
	pub fn send_spontaneous_payment(
		&self, amount_msat: u64, node_id: PublicKey,
	) -> Result<PaymentHash, Error> {
		let rt_lock = self.runtime.read().unwrap();
		if rt_lock.is_none() {
			return Err(Error::NotRunning);
		}

		let payment_preimage = PaymentPreimage(self.keys_manager.get_secure_random_bytes());
		let payment_hash = PaymentHash(Sha256::hash(&payment_preimage.0).into_inner());

		if let Some(payment) = self.payment_store.get(&payment_hash) {
			if payment.status == PaymentStatus::Pending
				|| payment.status == PaymentStatus::Succeeded
			{
				log_error!(self.logger, "Payment error: must not send duplicate payments.");
				return Err(Error::DuplicatePayment);
			}
		}

		let route_params = RouteParameters::from_payment_params_and_value(
			PaymentParameters::from_node_id(node_id, self.config.default_cltv_expiry_delta),
			amount_msat,
		);
		let recipient_fields = RecipientOnionFields::spontaneous_empty();

		match self.channel_manager.send_spontaneous_payment_with_retry(
			Some(payment_preimage),
			recipient_fields,
			PaymentId(payment_hash.0),
			route_params,
			Retry::Timeout(LDK_PAYMENT_RETRY_TIMEOUT),
		) {
			Ok(_payment_id) => {
				log_info!(self.logger, "Initiated sending {}msat to {}.", amount_msat, node_id);

				let payment = PaymentDetails {
					hash: payment_hash,
					preimage: Some(payment_preimage),
					secret: None,
					status: PaymentStatus::Pending,
					direction: PaymentDirection::Outbound,
					amount_msat: Some(amount_msat),
				};
				self.payment_store.insert(payment)?;

				Ok(payment_hash)
			}
			Err(e) => {
				log_error!(self.logger, "Failed to send payment: {:?}", e);

				match e {
					channelmanager::RetryableSendFailure::DuplicatePayment => {
						Err(Error::DuplicatePayment)
					}
					_ => {
						let payment = PaymentDetails {
							hash: payment_hash,
							preimage: Some(payment_preimage),
							secret: None,
							status: PaymentStatus::Failed,
							direction: PaymentDirection::Outbound,
							amount_msat: Some(amount_msat),
						};

						self.payment_store.insert(payment)?;
						Err(Error::PaymentSendingFailed)
					}
				}
			}
		}
	}

	/// Sends payment probes over all paths of a route that would be used to pay the given invoice.
	///
	/// This may be used to send "pre-flight" probes, i.e., to train our scorer before conducting
	/// the actual payment. Note this is only useful if there likely is sufficient time for the
	/// probe to settle before sending out the actual payment, e.g., when waiting for user
	/// confirmation in a wallet UI.
	///
	/// Otherwise, there is a chance the probe could take up some liquidity needed to complete the
	/// actual payment. Users should therefore be cautious and might avoid sending probes if
	/// liquidity is scarce and/or they don't expect the probe to return before they send the
	/// payment. To mitigate this issue, channels with available liquidity less than the required
	/// amount times [`Config::probing_liquidity_limit_multiplier`] won't be used to send
	/// pre-flight probes.
	pub fn send_payment_probes(&self, invoice: &Bolt11Invoice) -> Result<(), Error> {
		let rt_lock = self.runtime.read().unwrap();
		if rt_lock.is_none() {
			return Err(Error::NotRunning);
		}

		let liquidity_limit_multiplier = Some(self.config.probing_liquidity_limit_multiplier);

		payment::preflight_probe_invoice(
			invoice,
			&*self.channel_manager,
			liquidity_limit_multiplier,
		)
		.map_err(|e| {
			log_error!(self.logger, "Failed to send payment probes: {:?}", e);
			Error::ProbeSendingFailed
		})?;

		Ok(())
	}

	/// Sends payment probes over all paths of a route that would be used to pay the given
	/// amount to the given `node_id`.
	///
	/// See [`Self::send_payment_probes`] for more information.
	pub fn send_spontaneous_payment_probes(
		&self, amount_msat: u64, node_id: PublicKey,
	) -> Result<(), Error> {
		let rt_lock = self.runtime.read().unwrap();
		if rt_lock.is_none() {
			return Err(Error::NotRunning);
		}

		let liquidity_limit_multiplier = Some(self.config.probing_liquidity_limit_multiplier);
		let cltv_expiry_delta = self.config.default_cltv_expiry_delta;

		self.channel_manager
			.send_spontaneous_preflight_probes(
				node_id,
				amount_msat,
				cltv_expiry_delta,
				liquidity_limit_multiplier,
			)
			.map_err(|e| {
				log_error!(self.logger, "Failed to send payment probes: {:?}", e);
				Error::ProbeSendingFailed
			})?;

		Ok(())
	}

	/// Sends payment probes over all paths of a route that would be used to pay the given
	/// zero-value invoice using the given amount.
	///
	/// This can be used to send pre-flight probes for a so-called "zero-amount" invoice, i.e., an
	/// invoice that leaves the amount paid to be determined by the user.
	///
	/// See [`Self::send_payment_probes`] for more information.
	pub fn send_payment_probes_using_amount(
		&self, invoice: &Bolt11Invoice, amount_msat: u64,
	) -> Result<(), Error> {
		let rt_lock = self.runtime.read().unwrap();
		if rt_lock.is_none() {
			return Err(Error::NotRunning);
		}

		if let Some(invoice_amount_msat) = invoice.amount_milli_satoshis() {
			if amount_msat < invoice_amount_msat {
				log_error!(
					self.logger,
					"Failed to send probes as the given amount needs to be at least the invoice amount: required {}msat, gave {}msat.", invoice_amount_msat, amount_msat);
				return Err(Error::InvalidAmount);
			}
		}

		let liquidity_limit_multiplier = Some(self.config.probing_liquidity_limit_multiplier);

		payment::preflight_probe_zero_value_invoice(
			invoice,
			amount_msat,
			&*self.channel_manager,
			liquidity_limit_multiplier,
		)
		.map_err(|e| {
			log_error!(self.logger, "Failed to send payment probes: {:?}", e);
			Error::ProbeSendingFailed
		})?;

		Ok(())
	}

	/// Returns a payable invoice that can be used to request and receive a payment of the amount
	/// given.
	pub fn receive_payment(
		&self, amount_msat: u64, description: &str, expiry_secs: u32,
	) -> Result<Bolt11Invoice, Error> {
		self.receive_payment_inner(Some(amount_msat), description, expiry_secs)
	}

	/// Returns a payable invoice that can be used to request and receive a payment for which the
	/// amount is to be determined by the user, also known as a "zero-amount" invoice.
	pub fn receive_variable_amount_payment(
		&self, description: &str, expiry_secs: u32,
	) -> Result<Bolt11Invoice, Error> {
		self.receive_payment_inner(None, description, expiry_secs)
	}

	fn receive_payment_inner(
		&self, amount_msat: Option<u64>, description: &str, expiry_secs: u32,
	) -> Result<Bolt11Invoice, Error> {
		let currency = Currency::from(self.config.network);
		let keys_manager = Arc::clone(&self.keys_manager);
		let invoice = match lightning_invoice::utils::create_invoice_from_channelmanager(
			&self.channel_manager,
			keys_manager,
			Arc::clone(&self.logger),
			currency,
			amount_msat,
			description.to_string(),
			expiry_secs,
			None,
		) {
			Ok(inv) => {
				log_info!(self.logger, "Invoice created: {}", inv);
				inv
			}
			Err(e) => {
				log_error!(self.logger, "Failed to create invoice: {}", e);
				return Err(Error::InvoiceCreationFailed);
			}
		};

		let payment_hash = PaymentHash((*invoice.payment_hash()).into_inner());
		let payment = PaymentDetails {
			hash: payment_hash,
			preimage: None,
			secret: Some(invoice.payment_secret().clone()),
			amount_msat,
			direction: PaymentDirection::Inbound,
			status: PaymentStatus::Pending,
		};

		self.payment_store.insert(payment)?;

		Ok(invoice)
	}

	/// Retrieve the details of a specific payment with the given hash.
	///
	/// Returns `Some` if the payment was known and `None` otherwise.
	pub fn payment(&self, payment_hash: &PaymentHash) -> Option<PaymentDetails> {
		self.payment_store.get(payment_hash)
	}

	/// Remove the payment with the given hash from the store.
	pub fn remove_payment(&self, payment_hash: &PaymentHash) -> Result<(), Error> {
		self.payment_store.remove(&payment_hash)
	}

	/// Retrieves all payments that match the given predicate.
	///
	/// For example, you could retrieve all stored outbound payments as follows:
	/// ```
	/// # use ldk_node::{Builder, Config, PaymentDirection};
	/// # use ldk_node::bitcoin::Network;
	/// # let mut config = Config::default();
	/// # config.network = Network::Regtest;
	/// # config.storage_dir_path = "/tmp/ldk_node_test/".to_string();
	/// # let builder = Builder::from_config(config);
	/// # let node = builder.build().unwrap();
	/// node.list_payments_with_filter(|p| p.direction == PaymentDirection::Outbound);
	/// ```
	pub fn list_payments_with_filter<F: FnMut(&&PaymentDetails) -> bool>(
		&self, f: F,
	) -> Vec<PaymentDetails> {
		self.payment_store.list_filter(f)
	}

	/// Retrieves all payments.
	pub fn list_payments(&self) -> Vec<PaymentDetails> {
		self.payment_store.list_filter(|_| true)
	}

	/// Retrieves a list of known peers.
	pub fn list_peers(&self) -> Vec<PeerDetails> {
		let mut peers = Vec::new();

		// First add all connected peers, preferring to list the connected address if available.
		let connected_peers = self.peer_manager.get_peer_node_ids();
		let connected_peers_len = connected_peers.len();
		for (node_id, con_addr_opt) in connected_peers {
			let stored_peer = self.peer_store.get_peer(&node_id);
			let stored_addr_opt = stored_peer.as_ref().map(|p| p.address.clone());
			let address = match (con_addr_opt, stored_addr_opt) {
				(Some(con_addr), _) => con_addr,
				(None, Some(stored_addr)) => stored_addr,
				(None, None) => continue,
			};

			let is_persisted = stored_peer.is_some();
			let is_connected = true;
			let details = PeerDetails { node_id, address, is_persisted, is_connected };
			peers.push(details);
		}

		// Now add all known-but-offline peers, too.
		for p in self.peer_store.list_peers() {
			if peers.iter().take(connected_peers_len).find(|d| d.node_id == p.node_id).is_some() {
				continue;
			}

			let details = PeerDetails {
				node_id: p.node_id,
				address: p.address,
				is_persisted: true,
				is_connected: false,
			};

			peers.push(details);
		}

		peers
	}

	/// Creates a digital ECDSA signature of a message with the node's secret key.
	///
	/// A receiver knowing the corresponding `PublicKey` (e.g. the node’s id) and the message
	/// can be sure that the signature was generated by the caller.
	/// Signatures are EC recoverable, meaning that given the message and the
	/// signature the `PublicKey` of the signer can be extracted.
	pub fn sign_message(&self, msg: &[u8]) -> Result<String, Error> {
		self.keys_manager.sign_message(msg)
	}

	/// Verifies that the given ECDSA signature was created for the given message with the
	/// secret key corresponding to the given public key.
	pub fn verify_signature(&self, msg: &[u8], sig: &str, pkey: &PublicKey) -> bool {
		self.keys_manager.verify_signature(msg, sig, pkey)
	}
}

impl<K: KVStore + Sync + Send + 'static> Drop for Node<K> {
	fn drop(&mut self) {
		let _ = self.stop();
	}
}

async fn connect_peer_if_necessary<K: KVStore + Sync + Send + 'static>(
	node_id: PublicKey, addr: SocketAddress, peer_manager: Arc<PeerManager<K>>,
	logger: Arc<FilesystemLogger>,
) -> Result<(), Error> {
	for (pman_node_id, _pman_addr) in peer_manager.get_peer_node_ids() {
		if node_id == pman_node_id {
			return Ok(());
		}
	}

	do_connect_peer(node_id, addr, peer_manager, logger).await
}

async fn do_connect_peer<K: KVStore + Sync + Send + 'static>(
	node_id: PublicKey, addr: SocketAddress, peer_manager: Arc<PeerManager<K>>,
	logger: Arc<FilesystemLogger>,
) -> Result<(), Error> {
	log_info!(logger, "Connecting to peer: {}@{}", node_id, addr);

	let socket_addr = addr
		.to_socket_addrs()
		.map_err(|e| {
			log_error!(logger, "Failed to resolve network address: {}", e);
			Error::InvalidSocketAddress
		})?
		.next()
		.ok_or(Error::ConnectionFailed)?;

	match lightning_net_tokio::connect_outbound(Arc::clone(&peer_manager), node_id, socket_addr)
		.await
	{
		Some(connection_closed_future) => {
			let mut connection_closed_future = Box::pin(connection_closed_future);
			loop {
				match futures::poll!(&mut connection_closed_future) {
					std::task::Poll::Ready(_) => {
						log_info!(logger, "Peer connection closed: {}@{}", node_id, addr);
						return Err(Error::ConnectionFailed);
					}
					std::task::Poll::Pending => {}
				}
				// Avoid blocking the tokio context by sleeping a bit
				match peer_manager.get_peer_node_ids().iter().find(|(id, _addr)| *id == node_id) {
					Some(_) => return Ok(()),
					None => tokio::time::sleep(Duration::from_millis(10)).await,
				}
			}
		}
		None => {
			log_error!(logger, "Failed to connect to peer: {}@{}", node_id, addr);
			Err(Error::ConnectionFailed)
		}
	}
}

'''
'''--- src/logger.rs ---
pub(crate) use lightning::util::logger::Logger;
pub(crate) use lightning::{log_debug, log_error, log_info, log_trace};

use lightning::util::logger::{Level, Record};
use lightning::util::ser::Writer;

use chrono::Utc;

use std::fs;
#[cfg(not(target_os = "windows"))]
use std::os::unix::fs::symlink;
use std::path::Path;

pub(crate) struct FilesystemLogger {
	file_path: String,
	level: Level,
}

impl FilesystemLogger {
	pub(crate) fn new(log_dir: String, level: Level) -> Result<Self, ()> {
		let log_file_name =
			format!("ldk_node_{}.log", chrono::offset::Local::now().format("%Y_%m_%d"));
		let log_file_path = format!("{}/{}", log_dir, log_file_name);

		if let Some(parent_dir) = Path::new(&log_file_path).parent() {
			fs::create_dir_all(parent_dir).expect("Failed to create log parent directory");

			// make sure the file exists, so that the symlink has something to point to.
			fs::OpenOptions::new()
				.create(true)
				.append(true)
				.open(log_file_path.clone())
				.map_err(|e| eprintln!("ERROR: Failed to open log file: {}", e))?;

			#[cfg(not(target_os = "windows"))]
			{
				// Create a symlink to the current log file, with prior cleanup
				let log_file_symlink = parent_dir.join("ldk_node_latest.log");
				if log_file_symlink.as_path().is_symlink() {
					fs::remove_file(&log_file_symlink).map_err(|e| {
						eprintln!("ERROR: Failed to remove log file symlink: {}", e)
					})?;
				}
				symlink(&log_file_name, &log_file_symlink)
					.map_err(|e| eprintln!("ERROR: Failed to create log file symlink: {}", e))?;
			}
		}

		Ok(Self { file_path: log_file_path, level })
	}
}
impl Logger for FilesystemLogger {
	fn log(&self, record: &Record) {
		if record.level < self.level {
			return;
		}
		let raw_log = record.args.to_string();
		let log = format!(
			"{} {:<5} [{}:{}] {}\n",
			Utc::now().format("%Y-%m-%d %H:%M:%S"),
			record.level.to_string(),
			record.module_path,
			record.line,
			raw_log
		);
		fs::OpenOptions::new()
			.create(true)
			.append(true)
			.open(self.file_path.clone())
			.expect("Failed to open log file")
			.write_all(log.as_bytes())
			.expect("Failed to write to log file")
	}
}

'''
'''--- src/payment_store.rs ---
use crate::hex_utils;
use crate::io::{
	PAYMENT_INFO_PERSISTENCE_PRIMARY_NAMESPACE, PAYMENT_INFO_PERSISTENCE_SECONDARY_NAMESPACE,
};
use crate::logger::{log_error, Logger};
use crate::Error;

use lightning::ln::{PaymentHash, PaymentPreimage, PaymentSecret};
use lightning::util::persist::KVStore;
use lightning::util::ser::Writeable;
use lightning::{impl_writeable_tlv_based, impl_writeable_tlv_based_enum};

use std::collections::HashMap;
use std::iter::FromIterator;
use std::ops::Deref;
use std::sync::{Arc, Mutex};

/// Represents a payment.
#[derive(Clone, Debug, PartialEq, Eq)]
pub struct PaymentDetails {
	/// The payment hash, i.e., the hash of the `preimage`.
	pub hash: PaymentHash,
	/// The pre-image used by the payment.
	pub preimage: Option<PaymentPreimage>,
	/// The secret used by the payment.
	pub secret: Option<PaymentSecret>,
	/// The amount transferred.
	pub amount_msat: Option<u64>,
	/// The direction of the payment.
	pub direction: PaymentDirection,
	/// The status of the payment.
	pub status: PaymentStatus,
}

impl_writeable_tlv_based!(PaymentDetails, {
	(0, hash, required),
	(2, preimage, required),
	(4, secret, required),
	(6, amount_msat, required),
	(8, direction, required),
	(10, status, required)
});

/// Represents the direction of a payment.
#[derive(Copy, Clone, Debug, PartialEq, Eq)]
pub enum PaymentDirection {
	/// The payment is inbound.
	Inbound,
	/// The payment is outbound.
	Outbound,
}

impl_writeable_tlv_based_enum!(PaymentDirection,
	(0, Inbound) => {},
	(1, Outbound) => {};
);

/// Represents the current status of a payment.
#[derive(Copy, Clone, Debug, PartialEq, Eq)]
pub enum PaymentStatus {
	/// The payment is still pending.
	Pending,
	/// The payment suceeded.
	Succeeded,
	/// The payment failed.
	Failed,
}

impl_writeable_tlv_based_enum!(PaymentStatus,
	(0, Pending) => {},
	(2, Succeeded) => {},
	(4, Failed) => {};
);

#[derive(Clone, Debug, PartialEq, Eq)]
pub(crate) struct PaymentDetailsUpdate {
	pub hash: PaymentHash,
	pub preimage: Option<Option<PaymentPreimage>>,
	pub secret: Option<Option<PaymentSecret>>,
	pub amount_msat: Option<Option<u64>>,
	pub direction: Option<PaymentDirection>,
	pub status: Option<PaymentStatus>,
}

impl PaymentDetailsUpdate {
	pub fn new(hash: PaymentHash) -> Self {
		Self {
			hash,
			preimage: None,
			secret: None,
			amount_msat: None,
			direction: None,
			status: None,
		}
	}
}

pub(crate) struct PaymentStore<K: KVStore + Sync + Send, L: Deref>
where
	L::Target: Logger,
{
	payments: Mutex<HashMap<PaymentHash, PaymentDetails>>,
	kv_store: Arc<K>,
	logger: L,
}

impl<K: KVStore + Sync + Send, L: Deref> PaymentStore<K, L>
where
	L::Target: Logger,
{
	pub(crate) fn new(payments: Vec<PaymentDetails>, kv_store: Arc<K>, logger: L) -> Self {
		let payments = Mutex::new(HashMap::from_iter(
			payments.into_iter().map(|payment| (payment.hash, payment)),
		));
		Self { payments, kv_store, logger }
	}

	pub(crate) fn insert(&self, payment: PaymentDetails) -> Result<bool, Error> {
		let mut locked_payments = self.payments.lock().unwrap();

		let hash = payment.hash.clone();
		let updated = locked_payments.insert(hash.clone(), payment.clone()).is_some();
		self.persist_info(&hash, &payment)?;
		Ok(updated)
	}

	pub(crate) fn remove(&self, hash: &PaymentHash) -> Result<(), Error> {
		let store_key = hex_utils::to_string(&hash.0);
		self.kv_store
			.remove(
				PAYMENT_INFO_PERSISTENCE_PRIMARY_NAMESPACE,
				PAYMENT_INFO_PERSISTENCE_SECONDARY_NAMESPACE,
				&store_key,
				false,
			)
			.map_err(|e| {
				log_error!(
					self.logger,
					"Removing payment data for key {}/{}/{} failed due to: {}",
					PAYMENT_INFO_PERSISTENCE_PRIMARY_NAMESPACE,
					PAYMENT_INFO_PERSISTENCE_SECONDARY_NAMESPACE,
					store_key,
					e
				);
				Error::PersistenceFailed
			})
	}

	pub(crate) fn get(&self, hash: &PaymentHash) -> Option<PaymentDetails> {
		self.payments.lock().unwrap().get(hash).cloned()
	}

	pub(crate) fn update(&self, update: &PaymentDetailsUpdate) -> Result<bool, Error> {
		let mut updated = false;
		let mut locked_payments = self.payments.lock().unwrap();

		if let Some(payment) = locked_payments.get_mut(&update.hash) {
			if let Some(preimage_opt) = update.preimage {
				payment.preimage = preimage_opt;
			}

			if let Some(secret_opt) = update.secret {
				payment.secret = secret_opt;
			}

			if let Some(amount_opt) = update.amount_msat {
				payment.amount_msat = amount_opt;
			}

			if let Some(status) = update.status {
				payment.status = status;
			}

			self.persist_info(&update.hash, payment)?;
			updated = true;
		}

		Ok(updated)
	}

	pub(crate) fn list_filter<F: FnMut(&&PaymentDetails) -> bool>(
		&self, f: F,
	) -> Vec<PaymentDetails> {
		self.payments
			.lock()
			.unwrap()
			.iter()
			.map(|(_, p)| p)
			.filter(f)
			.cloned()
			.collect::<Vec<PaymentDetails>>()
	}

	fn persist_info(&self, hash: &PaymentHash, payment: &PaymentDetails) -> Result<(), Error> {
		let store_key = hex_utils::to_string(&hash.0);
		let data = payment.encode();
		self.kv_store
			.write(
				PAYMENT_INFO_PERSISTENCE_PRIMARY_NAMESPACE,
				PAYMENT_INFO_PERSISTENCE_SECONDARY_NAMESPACE,
				&store_key,
				&data,
			)
			.map_err(|e| {
				log_error!(
					self.logger,
					"Write for key {}/{}/{} failed due to: {}",
					PAYMENT_INFO_PERSISTENCE_PRIMARY_NAMESPACE,
					PAYMENT_INFO_PERSISTENCE_SECONDARY_NAMESPACE,
					store_key,
					e
				);
				Error::PersistenceFailed
			})?;
		Ok(())
	}
}

#[cfg(test)]
mod tests {
	use super::*;
	use crate::test::utils::TestLogger;
	use lightning::util::test_utils::TestStore;
	use std::sync::Arc;

	#[test]
	fn payment_info_is_persisted() {
		let store = Arc::new(TestStore::new(false));
		let logger = Arc::new(TestLogger::new());
		let payment_store = PaymentStore::new(Vec::new(), Arc::clone(&store), logger);

		let hash = PaymentHash([42u8; 32]);
		assert!(!payment_store.get(&hash).is_some());

		let store_key = hex_utils::to_string(&hash.0);
		assert!(store
			.read(
				PAYMENT_INFO_PERSISTENCE_PRIMARY_NAMESPACE,
				PAYMENT_INFO_PERSISTENCE_SECONDARY_NAMESPACE,
				&store_key
			)
			.is_err());

		let payment = PaymentDetails {
			hash,
			preimage: None,
			secret: None,
			amount_msat: None,
			direction: PaymentDirection::Inbound,
			status: PaymentStatus::Pending,
		};

		assert_eq!(Ok(false), payment_store.insert(payment.clone()));
		assert!(payment_store.get(&hash).is_some());
		assert!(store
			.read(
				PAYMENT_INFO_PERSISTENCE_PRIMARY_NAMESPACE,
				PAYMENT_INFO_PERSISTENCE_SECONDARY_NAMESPACE,
				&store_key
			)
			.is_ok());

		assert_eq!(Ok(true), payment_store.insert(payment));
		assert!(payment_store.get(&hash).is_some());

		let mut update = PaymentDetailsUpdate::new(hash);
		update.status = Some(PaymentStatus::Succeeded);
		assert_eq!(Ok(true), payment_store.update(&update));
		assert!(payment_store.get(&hash).is_some());

		assert_eq!(PaymentStatus::Succeeded, payment_store.get(&hash).unwrap().status);
	}
}

'''
'''--- src/peer_store.rs ---
use crate::io::{
	PEER_INFO_PERSISTENCE_KEY, PEER_INFO_PERSISTENCE_PRIMARY_NAMESPACE,
	PEER_INFO_PERSISTENCE_SECONDARY_NAMESPACE,
};
use crate::logger::{log_error, Logger};
use crate::{Error, SocketAddress};

use lightning::impl_writeable_tlv_based;
use lightning::util::persist::KVStore;
use lightning::util::ser::{Readable, ReadableArgs, Writeable, Writer};

use bitcoin::secp256k1::PublicKey;

use std::collections::HashMap;
use std::ops::Deref;
use std::sync::{Arc, RwLock};

pub struct PeerStore<K: KVStore + Sync + Send, L: Deref>
where
	L::Target: Logger,
{
	peers: RwLock<HashMap<PublicKey, PeerInfo>>,
	kv_store: Arc<K>,
	logger: L,
}

impl<K: KVStore + Sync + Send, L: Deref> PeerStore<K, L>
where
	L::Target: Logger,
{
	pub(crate) fn new(kv_store: Arc<K>, logger: L) -> Self {
		let peers = RwLock::new(HashMap::new());
		Self { peers, kv_store, logger }
	}

	pub(crate) fn add_peer(&self, peer_info: PeerInfo) -> Result<(), Error> {
		let mut locked_peers = self.peers.write().unwrap();

		if locked_peers.contains_key(&peer_info.node_id) {
			return Ok(());
		}

		locked_peers.insert(peer_info.node_id, peer_info);
		self.persist_peers(&*locked_peers)
	}

	pub(crate) fn remove_peer(&self, node_id: &PublicKey) -> Result<(), Error> {
		let mut locked_peers = self.peers.write().unwrap();

		locked_peers.remove(node_id);
		self.persist_peers(&*locked_peers)
	}

	pub(crate) fn list_peers(&self) -> Vec<PeerInfo> {
		self.peers.read().unwrap().values().cloned().collect()
	}

	pub(crate) fn get_peer(&self, node_id: &PublicKey) -> Option<PeerInfo> {
		self.peers.read().unwrap().get(node_id).cloned()
	}

	fn persist_peers(&self, locked_peers: &HashMap<PublicKey, PeerInfo>) -> Result<(), Error> {
		let data = PeerStoreSerWrapper(&*locked_peers).encode();
		self.kv_store
			.write(
				PEER_INFO_PERSISTENCE_PRIMARY_NAMESPACE,
				PEER_INFO_PERSISTENCE_SECONDARY_NAMESPACE,
				PEER_INFO_PERSISTENCE_KEY,
				&data,
			)
			.map_err(|e| {
				log_error!(
					self.logger,
					"Write for key {}/{}/{} failed due to: {}",
					PEER_INFO_PERSISTENCE_PRIMARY_NAMESPACE,
					PEER_INFO_PERSISTENCE_SECONDARY_NAMESPACE,
					PEER_INFO_PERSISTENCE_KEY,
					e
				);
				Error::PersistenceFailed
			})?;
		Ok(())
	}
}

impl<K: KVStore + Sync + Send, L: Deref> ReadableArgs<(Arc<K>, L)> for PeerStore<K, L>
where
	L::Target: Logger,
{
	#[inline]
	fn read<R: lightning::io::Read>(
		reader: &mut R, args: (Arc<K>, L),
	) -> Result<Self, lightning::ln::msgs::DecodeError> {
		let (kv_store, logger) = args;
		let read_peers: PeerStoreDeserWrapper = Readable::read(reader)?;
		let peers: RwLock<HashMap<PublicKey, PeerInfo>> = RwLock::new(read_peers.0);
		Ok(Self { peers, kv_store, logger })
	}
}

#[derive(Clone, Debug, PartialEq, Eq)]
pub(crate) struct PeerStoreDeserWrapper(HashMap<PublicKey, PeerInfo>);

impl Readable for PeerStoreDeserWrapper {
	fn read<R: lightning::io::Read>(
		reader: &mut R,
	) -> Result<Self, lightning::ln::msgs::DecodeError> {
		let len: u16 = Readable::read(reader)?;
		let mut peers = HashMap::with_capacity(len as usize);
		for _ in 0..len {
			let k: PublicKey = Readable::read(reader)?;
			let v: PeerInfo = Readable::read(reader)?;
			peers.insert(k, v);
		}
		Ok(Self(peers))
	}
}

pub(crate) struct PeerStoreSerWrapper<'a>(&'a HashMap<PublicKey, PeerInfo>);

impl Writeable for PeerStoreSerWrapper<'_> {
	fn write<W: Writer>(&self, writer: &mut W) -> Result<(), lightning::io::Error> {
		(self.0.len() as u16).write(writer)?;
		for (k, v) in self.0.iter() {
			k.write(writer)?;
			v.write(writer)?;
		}
		Ok(())
	}
}

#[derive(Clone, Debug, PartialEq, Eq)]
pub(crate) struct PeerInfo {
	pub node_id: PublicKey,
	pub address: SocketAddress,
}

impl_writeable_tlv_based!(PeerInfo, {
	(0, node_id, required),
	(2, address, required),
});

#[cfg(test)]
mod tests {
	use super::*;
	use crate::test::utils::TestLogger;

	use lightning::util::test_utils::TestStore;

	use std::str::FromStr;
	use std::sync::Arc;

	#[test]
	fn peer_info_persistence() {
		let store = Arc::new(TestStore::new(false));
		let logger = Arc::new(TestLogger::new());
		let peer_store = PeerStore::new(Arc::clone(&store), Arc::clone(&logger));

		let node_id = PublicKey::from_str(
			"0276607124ebe6a6c9338517b6f485825b27c2dcc0b9fc2aa6a4c0df91194e5993",
		)
		.unwrap();
		let address = SocketAddress::from_str("127.0.0.1:9738").unwrap();
		let expected_peer_info = PeerInfo { node_id, address };
		assert!(store
			.read(
				PEER_INFO_PERSISTENCE_PRIMARY_NAMESPACE,
				PEER_INFO_PERSISTENCE_SECONDARY_NAMESPACE,
				PEER_INFO_PERSISTENCE_KEY,
			)
			.is_err());
		peer_store.add_peer(expected_peer_info.clone()).unwrap();

		// Check we can read back what we persisted.
		let persisted_bytes = store
			.read(
				PEER_INFO_PERSISTENCE_PRIMARY_NAMESPACE,
				PEER_INFO_PERSISTENCE_SECONDARY_NAMESPACE,
				PEER_INFO_PERSISTENCE_KEY,
			)
			.unwrap();
		let deser_peer_store =
			PeerStore::read(&mut &persisted_bytes[..], (Arc::clone(&store), logger)).unwrap();

		let peers = deser_peer_store.list_peers();
		assert_eq!(peers.len(), 1);
		assert_eq!(peers[0], expected_peer_info);
		assert_eq!(deser_peer_store.get_peer(&node_id), Some(expected_peer_info));
	}
}

'''
'''--- src/test/functional_tests.rs ---
use crate::builder::NodeBuilder;
use crate::io::test_utils::TestSyncStore;
use crate::test::utils::*;
use crate::test::utils::{expect_channel_pending_event, expect_event, open_channel, random_config};
use crate::{Error, Event, Node, PaymentDirection, PaymentStatus};

use bitcoin::Amount;
use electrsd::bitcoind::BitcoinD;
use electrsd::ElectrsD;
use lightning::util::persist::KVStore;

use std::sync::Arc;

#[test]
fn channel_full_cycle() {
	let (bitcoind, electrsd) = setup_bitcoind_and_electrsd();
	let (node_a, node_b) = setup_two_nodes(&electrsd, false);
	do_channel_full_cycle(node_a, node_b, &bitcoind, &electrsd, false);
}

#[test]
#[cfg(vss_test)]
fn channel_full_cycle_with_vss_store() {
	let (bitcoind, electrsd) = setup_bitcoind_and_electrsd();
	println!("== Node A ==");
	let esplora_url = format!("http://{}", electrsd.esplora_url.as_ref().unwrap());
	let config_a = random_config();
	let mut builder_a = NodeBuilder::from_config(config_a);
	builder_a.set_esplora_server(esplora_url.clone());
	let vss_base_url = std::env::var("TEST_VSS_BASE_URL").unwrap();
	let node_a = builder_a.build_with_vss_store(&vss_base_url, "node_1_store".to_string()).unwrap();
	node_a.start().unwrap();

	println!("\n== Node B ==");
	let config_b = random_config();
	let mut builder_b = NodeBuilder::from_config(config_b);
	builder_b.set_esplora_server(esplora_url);
	let node_b = builder_b.build_with_vss_store(&vss_base_url, "node_2_store".to_string()).unwrap();
	node_b.start().unwrap();

	do_channel_full_cycle(node_a, node_b, &bitcoind, &electrsd, false);
}

#[test]
fn channel_full_cycle_0conf() {
	let (bitcoind, electrsd) = setup_bitcoind_and_electrsd();
	let (node_a, node_b) = setup_two_nodes(&electrsd, true);
	do_channel_full_cycle(node_a, node_b, &bitcoind, &electrsd, true)
}

fn do_channel_full_cycle<K: KVStore + Sync + Send>(
	node_a: Node<K>, node_b: Node<K>, bitcoind: &BitcoinD, electrsd: &ElectrsD, allow_0conf: bool,
) {
	let addr_a = node_a.new_onchain_address().unwrap();
	let addr_b = node_b.new_onchain_address().unwrap();

	let premine_amount_sat = 100_000;

	premine_and_distribute_funds(
		&bitcoind,
		&electrsd,
		vec![addr_a, addr_b],
		Amount::from_sat(premine_amount_sat),
	);
	node_a.sync_wallets().unwrap();
	node_b.sync_wallets().unwrap();
	assert_eq!(node_a.spendable_onchain_balance_sats().unwrap(), premine_amount_sat);
	assert_eq!(node_b.spendable_onchain_balance_sats().unwrap(), premine_amount_sat);

	// Check we haven't got any events yet
	assert_eq!(node_a.next_event(), None);
	assert_eq!(node_b.next_event(), None);

	println!("\nA -- connect_open_channel -> B");
	let funding_amount_sat = 80_000;
	let push_msat = (funding_amount_sat / 2) * 1000; // balance the channel
	node_a
		.connect_open_channel(
			node_b.node_id(),
			node_b.listening_addresses().unwrap().first().unwrap().clone(),
			funding_amount_sat,
			Some(push_msat),
			None,
			true,
		)
		.unwrap();

	assert_eq!(node_a.list_peers().first().unwrap().node_id, node_b.node_id());
	let funding_txo_a = expect_channel_pending_event!(node_a, node_b.node_id());
	let funding_txo_b = expect_channel_pending_event!(node_b, node_a.node_id());
	assert_eq!(funding_txo_a, funding_txo_b);

	wait_for_tx(&electrsd, funding_txo_a.txid);

	if !allow_0conf {
		generate_blocks_and_wait(&bitcoind, &electrsd, 6);
	}

	node_a.sync_wallets().unwrap();
	node_b.sync_wallets().unwrap();

	let onchain_fee_buffer_sat = 1500;
	let node_a_upper_bound_sat = premine_amount_sat - funding_amount_sat;
	let node_a_lower_bound_sat = premine_amount_sat - funding_amount_sat - onchain_fee_buffer_sat;
	assert!(node_a.spendable_onchain_balance_sats().unwrap() < node_a_upper_bound_sat);
	assert!(node_a.spendable_onchain_balance_sats().unwrap() > node_a_lower_bound_sat);
	assert_eq!(node_b.spendable_onchain_balance_sats().unwrap(), premine_amount_sat);

	expect_event!(node_a, ChannelReady);

	let ev = node_b.wait_next_event();
	let channel_id = match ev {
		ref e @ Event::ChannelReady { ref channel_id, .. } => {
			println!("{} got event {:?}", std::stringify!(node_b), e);
			node_b.event_handled();
			channel_id
		}
		ref e => {
			panic!("{} got unexpected event!: {:?}", std::stringify!(node_b), e);
		}
	};

	println!("\nB receive_payment");
	let invoice_amount_1_msat = 2500_000;
	let invoice = node_b.receive_payment(invoice_amount_1_msat, &"asdf", 9217).unwrap();

	println!("\nA send_payment");
	let payment_hash = node_a.send_payment(&invoice).unwrap();
	assert_eq!(node_a.send_payment(&invoice), Err(Error::DuplicatePayment));

	assert_eq!(node_a.list_payments().first().unwrap().hash, payment_hash);

	let outbound_payments_a =
		node_a.list_payments_with_filter(|p| p.direction == PaymentDirection::Outbound);
	assert_eq!(outbound_payments_a.len(), 1);

	let inbound_payments_a =
		node_a.list_payments_with_filter(|p| p.direction == PaymentDirection::Inbound);
	assert_eq!(inbound_payments_a.len(), 0);

	let outbound_payments_b =
		node_b.list_payments_with_filter(|p| p.direction == PaymentDirection::Outbound);
	assert_eq!(outbound_payments_b.len(), 0);

	let inbound_payments_b =
		node_b.list_payments_with_filter(|p| p.direction == PaymentDirection::Inbound);
	assert_eq!(inbound_payments_b.len(), 1);

	expect_event!(node_a, PaymentSuccessful);
	expect_event!(node_b, PaymentReceived);
	assert_eq!(node_a.payment(&payment_hash).unwrap().status, PaymentStatus::Succeeded);
	assert_eq!(node_a.payment(&payment_hash).unwrap().direction, PaymentDirection::Outbound);
	assert_eq!(node_a.payment(&payment_hash).unwrap().amount_msat, Some(invoice_amount_1_msat));
	assert_eq!(node_b.payment(&payment_hash).unwrap().status, PaymentStatus::Succeeded);
	assert_eq!(node_b.payment(&payment_hash).unwrap().direction, PaymentDirection::Inbound);
	assert_eq!(node_b.payment(&payment_hash).unwrap().amount_msat, Some(invoice_amount_1_msat));

	// Assert we fail duplicate outbound payments and check the status hasn't changed.
	assert_eq!(Err(Error::DuplicatePayment), node_a.send_payment(&invoice));
	assert_eq!(node_a.payment(&payment_hash).unwrap().status, PaymentStatus::Succeeded);
	assert_eq!(node_a.payment(&payment_hash).unwrap().direction, PaymentDirection::Outbound);
	assert_eq!(node_a.payment(&payment_hash).unwrap().amount_msat, Some(invoice_amount_1_msat));
	assert_eq!(node_b.payment(&payment_hash).unwrap().status, PaymentStatus::Succeeded);
	assert_eq!(node_b.payment(&payment_hash).unwrap().direction, PaymentDirection::Inbound);
	assert_eq!(node_b.payment(&payment_hash).unwrap().amount_msat, Some(invoice_amount_1_msat));

	// Test under-/overpayment
	let invoice_amount_2_msat = 2500_000;
	let invoice = node_b.receive_payment(invoice_amount_2_msat, &"asdf", 9217).unwrap();

	let underpaid_amount = invoice_amount_2_msat - 1;
	assert_eq!(
		Err(Error::InvalidAmount),
		node_a.send_payment_using_amount(&invoice, underpaid_amount)
	);

	println!("\nB overpaid receive_payment");
	let invoice = node_b.receive_payment(invoice_amount_2_msat, &"asdf", 9217).unwrap();
	let overpaid_amount_msat = invoice_amount_2_msat + 100;

	println!("\nA overpaid send_payment");
	let payment_hash = node_a.send_payment_using_amount(&invoice, overpaid_amount_msat).unwrap();
	expect_event!(node_a, PaymentSuccessful);
	let received_amount = match node_b.wait_next_event() {
		ref e @ Event::PaymentReceived { amount_msat, .. } => {
			println!("{} got event {:?}", std::stringify!(node_b), e);
			node_b.event_handled();
			amount_msat
		}
		ref e => {
			panic!("{} got unexpected event!: {:?}", std::stringify!(node_b), e);
		}
	};
	assert_eq!(received_amount, overpaid_amount_msat);
	assert_eq!(node_a.payment(&payment_hash).unwrap().status, PaymentStatus::Succeeded);
	assert_eq!(node_a.payment(&payment_hash).unwrap().direction, PaymentDirection::Outbound);
	assert_eq!(node_a.payment(&payment_hash).unwrap().amount_msat, Some(overpaid_amount_msat));
	assert_eq!(node_b.payment(&payment_hash).unwrap().status, PaymentStatus::Succeeded);
	assert_eq!(node_b.payment(&payment_hash).unwrap().direction, PaymentDirection::Inbound);
	assert_eq!(node_b.payment(&payment_hash).unwrap().amount_msat, Some(overpaid_amount_msat));

	// Test "zero-amount" invoice payment
	println!("\nB receive_variable_amount_payment");
	let variable_amount_invoice = node_b.receive_variable_amount_payment(&"asdf", 9217).unwrap();
	let determined_amount_msat = 2345_678;
	assert_eq!(Err(Error::InvalidInvoice), node_a.send_payment(&variable_amount_invoice));
	println!("\nA send_payment_using_amount");
	let payment_hash =
		node_a.send_payment_using_amount(&variable_amount_invoice, determined_amount_msat).unwrap();

	expect_event!(node_a, PaymentSuccessful);
	let received_amount = match node_b.wait_next_event() {
		ref e @ Event::PaymentReceived { amount_msat, .. } => {
			println!("{} got event {:?}", std::stringify!(node_b), e);
			node_b.event_handled();
			amount_msat
		}
		ref e => {
			panic!("{} got unexpected event!: {:?}", std::stringify!(node_b), e);
		}
	};
	assert_eq!(received_amount, determined_amount_msat);
	assert_eq!(node_a.payment(&payment_hash).unwrap().status, PaymentStatus::Succeeded);
	assert_eq!(node_a.payment(&payment_hash).unwrap().direction, PaymentDirection::Outbound);
	assert_eq!(node_a.payment(&payment_hash).unwrap().amount_msat, Some(determined_amount_msat));
	assert_eq!(node_b.payment(&payment_hash).unwrap().status, PaymentStatus::Succeeded);
	assert_eq!(node_b.payment(&payment_hash).unwrap().direction, PaymentDirection::Inbound);
	assert_eq!(node_b.payment(&payment_hash).unwrap().amount_msat, Some(determined_amount_msat));

	// Test spontaneous/keysend payments
	println!("\nA send_spontaneous_payment");
	let keysend_amount_msat = 2500_000;
	let keysend_payment_hash =
		node_a.send_spontaneous_payment(keysend_amount_msat, node_b.node_id()).unwrap();
	expect_event!(node_a, PaymentSuccessful);
	let received_keysend_amount = match node_b.wait_next_event() {
		ref e @ Event::PaymentReceived { amount_msat, .. } => {
			println!("{} got event {:?}", std::stringify!(node_b), e);
			node_b.event_handled();
			amount_msat
		}
		ref e => {
			panic!("{} got unexpected event!: {:?}", std::stringify!(node_b), e);
		}
	};
	assert_eq!(received_keysend_amount, keysend_amount_msat);
	assert_eq!(node_a.payment(&keysend_payment_hash).unwrap().status, PaymentStatus::Succeeded);
	assert_eq!(
		node_a.payment(&keysend_payment_hash).unwrap().direction,
		PaymentDirection::Outbound
	);
	assert_eq!(
		node_a.payment(&keysend_payment_hash).unwrap().amount_msat,
		Some(keysend_amount_msat)
	);
	assert_eq!(node_b.payment(&keysend_payment_hash).unwrap().status, PaymentStatus::Succeeded);
	assert_eq!(node_b.payment(&keysend_payment_hash).unwrap().direction, PaymentDirection::Inbound);
	assert_eq!(
		node_b.payment(&keysend_payment_hash).unwrap().amount_msat,
		Some(keysend_amount_msat)
	);

	println!("\nB close_channel");
	node_b.close_channel(&channel_id, node_a.node_id()).unwrap();
	expect_event!(node_a, ChannelClosed);
	expect_event!(node_b, ChannelClosed);

	wait_for_outpoint_spend(&electrsd, funding_txo_b);

	generate_blocks_and_wait(&bitcoind, &electrsd, 1);
	node_a.sync_wallets().unwrap();
	node_b.sync_wallets().unwrap();

	let sum_of_all_payments_sat = (push_msat
		+ invoice_amount_1_msat
		+ overpaid_amount_msat
		+ determined_amount_msat
		+ keysend_amount_msat)
		/ 1000;
	let node_a_upper_bound_sat =
		(premine_amount_sat - funding_amount_sat) + (funding_amount_sat - sum_of_all_payments_sat);
	let node_a_lower_bound_sat = node_a_upper_bound_sat - onchain_fee_buffer_sat;
	assert!(node_a.spendable_onchain_balance_sats().unwrap() > node_a_lower_bound_sat);
	assert!(node_a.spendable_onchain_balance_sats().unwrap() < node_a_upper_bound_sat);
	let expected_final_amount_node_b_sat = premine_amount_sat + sum_of_all_payments_sat;
	assert_eq!(node_b.spendable_onchain_balance_sats().unwrap(), expected_final_amount_node_b_sat);

	// Check we handled all events
	assert_eq!(node_a.next_event(), None);
	assert_eq!(node_b.next_event(), None);

	node_a.stop().unwrap();
	println!("\nA stopped");
	node_b.stop().unwrap();
	println!("\nB stopped");
}

#[test]
fn channel_open_fails_when_funds_insufficient() {
	let (bitcoind, electrsd) = setup_bitcoind_and_electrsd();
	let (node_a, node_b) = setup_two_nodes(&electrsd, false);

	let addr_a = node_a.new_onchain_address().unwrap();
	let addr_b = node_b.new_onchain_address().unwrap();

	let premine_amount_sat = 100_000;

	premine_and_distribute_funds(
		&bitcoind,
		&electrsd,
		vec![addr_a, addr_b],
		Amount::from_sat(premine_amount_sat),
	);
	node_a.sync_wallets().unwrap();
	node_b.sync_wallets().unwrap();
	assert_eq!(node_a.spendable_onchain_balance_sats().unwrap(), premine_amount_sat);
	assert_eq!(node_b.spendable_onchain_balance_sats().unwrap(), premine_amount_sat);

	println!("\nA -- connect_open_channel -> B");
	assert_eq!(
		Err(Error::InsufficientFunds),
		node_a.connect_open_channel(
			node_b.node_id(),
			node_b.listening_addresses().unwrap().first().unwrap().clone(),
			120000,
			None,
			None,
			true
		)
	);
}

#[test]
fn multi_hop_sending() {
	let (bitcoind, electrsd) = setup_bitcoind_and_electrsd();
	let esplora_url = format!("http://{}", electrsd.esplora_url.as_ref().unwrap());

	// Setup and fund 5 nodes
	let mut nodes = Vec::new();
	for _ in 0..5 {
		let config = random_config();
		let mut builder = NodeBuilder::from_config(config);
		builder.set_esplora_server(esplora_url.clone());
		let node = builder.build().unwrap();
		node.start().unwrap();
		nodes.push(node);
	}

	let addresses = nodes.iter().map(|n| n.new_onchain_address().unwrap()).collect();
	let premine_amount_sat = 5_000_000;
	premine_and_distribute_funds(
		&bitcoind,
		&electrsd,
		addresses,
		Amount::from_sat(premine_amount_sat),
	);

	for n in &nodes {
		n.sync_wallets().unwrap();
		assert_eq!(n.spendable_onchain_balance_sats().unwrap(), premine_amount_sat);
		assert_eq!(n.next_event(), None);
	}

	// Setup channel topology:
	//                    (1M:0)- N2 -(1M:0)
	//                   /                  \
	//  N0 -(100k:0)-> N1                    N4
	//                   \                  /
	//                    (1M:0)- N3 -(1M:0)

	open_channel(&nodes[0], &nodes[1], 100_000, true, &electrsd);
	open_channel(&nodes[1], &nodes[2], 1_000_000, true, &electrsd);
	// We need to sync wallets in-between back-to-back channel opens from the same node so BDK
	// wallet picks up on the broadcast funding tx and doesn't double-spend itself.
	//
	// TODO: Remove once fixed in BDK.
	nodes[1].sync_wallets().unwrap();
	open_channel(&nodes[1], &nodes[3], 1_000_000, true, &electrsd);
	open_channel(&nodes[2], &nodes[4], 1_000_000, true, &electrsd);
	open_channel(&nodes[3], &nodes[4], 1_000_000, true, &electrsd);

	generate_blocks_and_wait(&bitcoind, &electrsd, 6);

	for n in &nodes {
		n.sync_wallets().unwrap();
	}

	expect_event!(nodes[0], ChannelReady);
	expect_event!(nodes[1], ChannelReady);
	expect_event!(nodes[1], ChannelReady);
	expect_event!(nodes[1], ChannelReady);
	expect_event!(nodes[2], ChannelReady);
	expect_event!(nodes[2], ChannelReady);
	expect_event!(nodes[3], ChannelReady);
	expect_event!(nodes[3], ChannelReady);
	expect_event!(nodes[4], ChannelReady);
	expect_event!(nodes[4], ChannelReady);

	// Sleep a bit for gossip to propagate.
	std::thread::sleep(std::time::Duration::from_secs(1));

	let invoice = nodes[4].receive_payment(2_500_000, &"asdf", 9217).unwrap();
	nodes[0].send_payment(&invoice).unwrap();

	expect_event!(nodes[4], PaymentReceived);
	expect_event!(nodes[0], PaymentSuccessful);
}

#[test]
fn connect_to_public_testnet_esplora() {
	let mut config = random_config();
	config.network = bitcoin::Network::Testnet;
	let mut builder = NodeBuilder::from_config(config);
	builder.set_esplora_server("https://blockstream.info/testnet/api".to_string());
	let node = builder.build().unwrap();
	node.start().unwrap();
	node.stop().unwrap();
}

#[test]
fn start_stop_reinit() {
	let (bitcoind, electrsd) = setup_bitcoind_and_electrsd();
	let config = random_config();

	let esplora_url = format!("http://{}", electrsd.esplora_url.as_ref().unwrap());

	let test_sync_store = Arc::new(TestSyncStore::new(config.storage_dir_path.clone().into()));

	let mut builder = NodeBuilder::from_config(config.clone());
	builder.set_esplora_server(esplora_url.clone());

	let node = builder.build_with_store(Arc::clone(&test_sync_store)).unwrap();
	node.start().unwrap();

	let expected_node_id = node.node_id();
	assert_eq!(node.start(), Err(Error::AlreadyRunning));

	let funding_address = node.new_onchain_address().unwrap();

	assert_eq!(node.total_onchain_balance_sats().unwrap(), 0);

	let expected_amount = Amount::from_sat(100000);
	premine_and_distribute_funds(&bitcoind, &electrsd, vec![funding_address], expected_amount);

	node.sync_wallets().unwrap();
	assert_eq!(node.spendable_onchain_balance_sats().unwrap(), expected_amount.to_sat());

	let log_file_symlink = format!("{}/logs/ldk_node_latest.log", config.clone().storage_dir_path);
	assert!(std::path::Path::new(&log_file_symlink).is_symlink());

	node.stop().unwrap();
	assert_eq!(node.stop(), Err(Error::NotRunning));

	node.start().unwrap();
	assert_eq!(node.start(), Err(Error::AlreadyRunning));

	node.stop().unwrap();
	assert_eq!(node.stop(), Err(Error::NotRunning));
	drop(node);

	let mut builder = NodeBuilder::from_config(config.clone());
	builder.set_esplora_server(esplora_url.clone());

	let reinitialized_node = builder.build_with_store(Arc::clone(&test_sync_store)).unwrap();
	reinitialized_node.start().unwrap();
	assert_eq!(reinitialized_node.node_id(), expected_node_id);

	assert_eq!(
		reinitialized_node.spendable_onchain_balance_sats().unwrap(),
		expected_amount.to_sat()
	);

	reinitialized_node.sync_wallets().unwrap();
	assert_eq!(
		reinitialized_node.spendable_onchain_balance_sats().unwrap(),
		expected_amount.to_sat()
	);

	reinitialized_node.stop().unwrap();
}

#[test]
fn onchain_spend_receive() {
	let (bitcoind, electrsd) = setup_bitcoind_and_electrsd();
	let (node_a, node_b) = setup_two_nodes(&electrsd, false);

	let addr_a = node_a.new_onchain_address().unwrap();
	let addr_b = node_b.new_onchain_address().unwrap();

	premine_and_distribute_funds(
		&bitcoind,
		&electrsd,
		vec![addr_b.clone()],
		Amount::from_sat(100000),
	);

	node_a.sync_wallets().unwrap();
	node_b.sync_wallets().unwrap();
	assert_eq!(node_b.spendable_onchain_balance_sats().unwrap(), 100000);

	assert_eq!(Err(Error::InsufficientFunds), node_a.send_to_onchain_address(&addr_b, 1000));

	let txid = node_b.send_to_onchain_address(&addr_a, 1000).unwrap();
	generate_blocks_and_wait(&bitcoind, &electrsd, 6);
	wait_for_tx(&electrsd, txid);

	node_a.sync_wallets().unwrap();
	node_b.sync_wallets().unwrap();

	assert_eq!(node_a.spendable_onchain_balance_sats().unwrap(), 1000);
	assert!(node_b.spendable_onchain_balance_sats().unwrap() > 98000);
	assert!(node_b.spendable_onchain_balance_sats().unwrap() < 100000);

	let addr_b = node_b.new_onchain_address().unwrap();
	let txid = node_a.send_all_to_onchain_address(&addr_b).unwrap();
	generate_blocks_and_wait(&bitcoind, &electrsd, 6);
	wait_for_tx(&electrsd, txid);

	node_a.sync_wallets().unwrap();
	node_b.sync_wallets().unwrap();

	assert_eq!(node_a.total_onchain_balance_sats().unwrap(), 0);
	assert!(node_b.spendable_onchain_balance_sats().unwrap() > 99000);
	assert!(node_b.spendable_onchain_balance_sats().unwrap() < 100000);
}

#[test]
fn sign_verify_msg() {
	let (_bitcoind, electrsd) = setup_bitcoind_and_electrsd();
	let config = random_config();
	let node = setup_node(&electrsd, config);

	// Tests arbitrary message signing and later verification
	let msg = "OK computer".as_bytes();
	let sig = node.sign_message(msg).unwrap();
	let pkey = node.node_id();
	assert!(node.verify_signature(msg, sig.as_str(), &pkey));
}

'''
'''--- src/test/mod.rs ---
pub mod functional_tests;
pub mod utils;

'''
'''--- src/test/utils.rs ---
use crate::builder::NodeBuilder;
use crate::io::test_utils::TestSyncStore;
use crate::{Config, Event, Node};
use lightning::ln::msgs::SocketAddress;
use lightning::util::logger::{Level, Logger, Record};
use lightning::util::persist::KVStore;

use bitcoin::{Address, Amount, Network, OutPoint, Txid};

use bitcoind::bitcoincore_rpc::RpcApi;
use electrsd::bitcoind::bitcoincore_rpc::bitcoincore_rpc_json::AddressType;
use electrsd::{bitcoind, bitcoind::BitcoinD, ElectrsD};
use electrum_client::ElectrumApi;

use regex;

use rand::distributions::Alphanumeric;
use rand::{thread_rng, Rng};
use std::collections::HashMap;
use std::env;
use std::path::PathBuf;
use std::sync::Arc;
use std::sync::Mutex;
use std::time::Duration;

macro_rules! expect_event {
	($node: expr, $event_type: ident) => {{
		match $node.wait_next_event() {
			ref e @ Event::$event_type { .. } => {
				println!("{} got event {:?}", $node.node_id(), e);
				$node.event_handled();
			}
			ref e => {
				panic!("{} got unexpected event!: {:?}", std::stringify!($node), e);
			}
		}
	}};
}

pub(crate) use expect_event;

macro_rules! expect_channel_pending_event {
	($node: expr, $counterparty_node_id: expr) => {{
		match $node.wait_next_event() {
			ref e @ Event::ChannelPending { funding_txo, counterparty_node_id, .. } => {
				println!("{} got event {:?}", $node.node_id(), e);
				assert_eq!(counterparty_node_id, $counterparty_node_id);
				$node.event_handled();
				funding_txo
			}
			ref e => {
				panic!("{} got unexpected event!: {:?}", std::stringify!($node), e);
			}
		}
	}};
}

pub(crate) use expect_channel_pending_event;

// Copied over from upstream LDK
#[allow(dead_code)]
pub struct TestLogger {
	level: Level,
	pub(crate) id: String,
	pub lines: Mutex<HashMap<(String, String), usize>>,
}

impl TestLogger {
	#[allow(dead_code)]
	pub fn new() -> TestLogger {
		Self::with_id("".to_owned())
	}

	#[allow(dead_code)]
	pub fn with_id(id: String) -> TestLogger {
		TestLogger { level: Level::Trace, id, lines: Mutex::new(HashMap::new()) }
	}

	#[allow(dead_code)]
	pub fn enable(&mut self, level: Level) {
		self.level = level;
	}

	#[allow(dead_code)]
	pub fn assert_log(&self, module: String, line: String, count: usize) {
		let log_entries = self.lines.lock().unwrap();
		assert_eq!(log_entries.get(&(module, line)), Some(&count));
	}

	/// Search for the number of occurrence of the logged lines which
	/// 1. belongs to the specified module and
	/// 2. contains `line` in it.
	/// And asserts if the number of occurrences is the same with the given `count`
	#[allow(dead_code)]
	pub fn assert_log_contains(&self, module: &str, line: &str, count: usize) {
		let log_entries = self.lines.lock().unwrap();
		let l: usize = log_entries
			.iter()
			.filter(|&(&(ref m, ref l), _c)| m == module && l.contains(line))
			.map(|(_, c)| c)
			.sum();
		assert_eq!(l, count)
	}

	/// Search for the number of occurrences of logged lines which
	/// 1. belong to the specified module and
	/// 2. match the given regex pattern.
	/// Assert that the number of occurrences equals the given `count`
	#[allow(dead_code)]
	pub fn assert_log_regex(&self, module: &str, pattern: regex::Regex, count: usize) {
		let log_entries = self.lines.lock().unwrap();
		let l: usize = log_entries
			.iter()
			.filter(|&(&(ref m, ref l), _c)| m == module && pattern.is_match(&l))
			.map(|(_, c)| c)
			.sum();
		assert_eq!(l, count)
	}
}

impl Logger for TestLogger {
	fn log(&self, record: &Record) {
		*self
			.lines
			.lock()
			.unwrap()
			.entry((record.module_path.to_string(), format!("{}", record.args)))
			.or_insert(0) += 1;
		if record.level >= self.level {
			#[cfg(feature = "std")]
			println!(
				"{:<5} {} [{} : {}, {}] {}",
				record.level.to_string(),
				self.id,
				record.module_path,
				record.file,
				record.line,
				record.args
			);
		}
	}
}

pub fn random_storage_path() -> PathBuf {
	let mut temp_path = std::env::temp_dir();
	let mut rng = thread_rng();
	let rand_dir: String = (0..7).map(|_| rng.sample(Alphanumeric) as char).collect();
	temp_path.push(rand_dir);
	temp_path
}

pub fn random_port() -> u16 {
	let mut rng = thread_rng();
	rng.gen_range(5000..65535)
}

pub fn random_listening_addresses() -> Vec<SocketAddress> {
	let num_addresses = 2;
	let mut listening_addresses = Vec::with_capacity(num_addresses);

	for _ in 0..num_addresses {
		let rand_port = random_port();
		let address: SocketAddress = format!("127.0.0.1:{}", rand_port).parse().unwrap();
		listening_addresses.push(address);
	}

	listening_addresses
}

pub fn random_config() -> Config {
	let mut config = Config::default();

	config.network = Network::Regtest;
	println!("Setting network: {}", config.network);

	let rand_dir = random_storage_path();
	println!("Setting random LDK storage dir: {}", rand_dir.display());
	config.storage_dir_path = rand_dir.to_str().unwrap().to_owned();

	let rand_listening_addresses = random_listening_addresses();
	println!("Setting random LDK listening addresses: {:?}", rand_listening_addresses);
	config.listening_addresses = Some(rand_listening_addresses);

	config.log_level = Level::Gossip;

	config
}

pub fn setup_bitcoind_and_electrsd() -> (BitcoinD, ElectrsD) {
	let bitcoind_exe =
		env::var("BITCOIND_EXE").ok().or_else(|| bitcoind::downloaded_exe_path().ok()).expect(
			"you need to provide an env var BITCOIND_EXE or specify a bitcoind version feature",
		);
	let mut bitcoind_conf = bitcoind::Conf::default();
	bitcoind_conf.network = "regtest";
	let bitcoind = BitcoinD::with_conf(bitcoind_exe, &bitcoind_conf).unwrap();

	let electrs_exe = env::var("ELECTRS_EXE")
		.ok()
		.or_else(electrsd::downloaded_exe_path)
		.expect("you need to provide env var ELECTRS_EXE or specify an electrsd version feature");
	let mut electrsd_conf = electrsd::Conf::default();
	electrsd_conf.http_enabled = true;
	electrsd_conf.network = "regtest";
	let electrsd = ElectrsD::with_conf(electrs_exe, &bitcoind, &electrsd_conf).unwrap();
	(bitcoind, electrsd)
}

pub(crate) fn setup_two_nodes(
	electrsd: &ElectrsD, allow_0conf: bool,
) -> (Node<TestSyncStore>, Node<TestSyncStore>) {
	println!("== Node A ==");
	let config_a = random_config();
	let node_a = setup_node(electrsd, config_a);

	println!("\n== Node B ==");
	let mut config_b = random_config();
	if allow_0conf {
		config_b.trusted_peers_0conf.push(node_a.node_id());
	}
	let node_b = setup_node(electrsd, config_b);
	(node_a, node_b)
}

pub(crate) fn setup_node(electrsd: &ElectrsD, config: Config) -> Node<TestSyncStore> {
	let esplora_url = format!("http://{}", electrsd.esplora_url.as_ref().unwrap());
	let mut builder = NodeBuilder::from_config(config.clone());
	builder.set_esplora_server(esplora_url.clone());
	let test_sync_store = Arc::new(TestSyncStore::new(config.storage_dir_path.into()));
	let node = builder.build_with_store(test_sync_store).unwrap();
	node.start().unwrap();
	node
}

pub fn generate_blocks_and_wait(bitcoind: &BitcoinD, electrsd: &ElectrsD, num: usize) {
	print!("Generating {} blocks...", num);
	let cur_height = bitcoind.client.get_block_count().expect("failed to get current block height");
	let address = bitcoind
		.client
		.get_new_address(Some("test"), Some(AddressType::Legacy))
		.expect("failed to get new address");
	// TODO: expect this Result once the WouldBlock issue is resolved upstream.
	let _block_hashes_res = bitcoind.client.generate_to_address(num as u64, &address);
	wait_for_block(electrsd, cur_height as usize + num);
	print!(" Done!");
	println!("\n");
}

pub fn wait_for_block(electrsd: &ElectrsD, min_height: usize) {
	let mut header = match electrsd.client.block_headers_subscribe() {
		Ok(header) => header,
		Err(_) => {
			// While subscribing should succeed the first time around, we ran into some cases where
			// it didn't. Since we can't proceed without subscribing, we try again after a delay
			// and panic if it still fails.
			std::thread::sleep(Duration::from_secs(1));
			electrsd.client.block_headers_subscribe().expect("failed to subscribe to block headers")
		}
	};
	loop {
		if header.height >= min_height {
			break;
		}
		header = exponential_backoff_poll(|| {
			electrsd.trigger().expect("failed to trigger electrsd");
			electrsd.client.ping().expect("failed to ping electrsd");
			electrsd.client.block_headers_pop().expect("failed to pop block header")
		});
	}
}

pub fn wait_for_tx(electrsd: &ElectrsD, txid: Txid) {
	let mut tx_res = electrsd.client.transaction_get(&txid);
	loop {
		if tx_res.is_ok() {
			break;
		}
		tx_res = exponential_backoff_poll(|| {
			electrsd.trigger().unwrap();
			electrsd.client.ping().unwrap();
			Some(electrsd.client.transaction_get(&txid))
		});
	}
}

pub fn wait_for_outpoint_spend(electrsd: &ElectrsD, outpoint: OutPoint) {
	let tx = electrsd.client.transaction_get(&outpoint.txid).unwrap();
	let txout_script = tx.output.get(outpoint.vout as usize).unwrap().clone().script_pubkey;
	let mut is_spent = !electrsd.client.script_get_history(&txout_script).unwrap().is_empty();
	loop {
		if is_spent {
			break;
		}

		is_spent = exponential_backoff_poll(|| {
			electrsd.trigger().unwrap();
			electrsd.client.ping().unwrap();
			Some(!electrsd.client.script_get_history(&txout_script).unwrap().is_empty())
		});
	}
}

pub fn exponential_backoff_poll<T, F>(mut poll: F) -> T
where
	F: FnMut() -> Option<T>,
{
	let mut delay = Duration::from_millis(64);
	let mut tries = 0;
	loop {
		match poll() {
			Some(data) => break data,
			None if delay.as_millis() < 512 => {
				delay = delay.mul_f32(2.0);
			}

			None => {}
		}
		assert!(tries < 20, "Reached max tries.");
		tries += 1;
		std::thread::sleep(delay);
	}
}

pub fn premine_and_distribute_funds(
	bitcoind: &BitcoinD, electrsd: &ElectrsD, addrs: Vec<Address>, amount: Amount,
) {
	generate_blocks_and_wait(bitcoind, electrsd, 101);

	for addr in addrs {
		let txid = bitcoind
			.client
			.send_to_address(&addr, amount, None, None, None, None, None, None)
			.unwrap();
		wait_for_tx(electrsd, txid);
	}

	generate_blocks_and_wait(bitcoind, electrsd, 1);
}

pub fn open_channel<K: KVStore + Sync + Send>(
	node_a: &Node<K>, node_b: &Node<K>, funding_amount_sat: u64, announce: bool,
	electrsd: &ElectrsD,
) {
	node_a
		.connect_open_channel(
			node_b.node_id(),
			node_b.listening_addresses().unwrap().first().unwrap().clone(),
			funding_amount_sat,
			None,
			None,
			announce,
		)
		.unwrap();
	assert!(node_a.list_peers().iter().find(|c| { c.node_id == node_b.node_id() }).is_some());

	let funding_txo_a = expect_channel_pending_event!(node_a, node_b.node_id());
	let funding_txo_b = expect_channel_pending_event!(node_b, node_a.node_id());
	assert_eq!(funding_txo_a, funding_txo_b);
	wait_for_tx(&electrsd, funding_txo_a.txid);
}

'''
'''--- src/types.rs ---
use crate::logger::FilesystemLogger;
use crate::wallet::{Wallet, WalletKeysManager};

use lightning::chain::chainmonitor;
use lightning::ln::channelmanager::ChannelDetails as LdkChannelDetails;
use lightning::ln::msgs::RoutingMessageHandler;
use lightning::ln::msgs::SocketAddress;
use lightning::ln::peer_handler::IgnoringMessageHandler;
use lightning::ln::ChannelId;
use lightning::routing::gossip;
use lightning::routing::router::DefaultRouter;
use lightning::routing::scoring::{ProbabilisticScorer, ProbabilisticScoringFeeParameters};
use lightning::sign::InMemorySigner;
use lightning::util::config::ChannelConfig as LdkChannelConfig;
use lightning::util::config::MaxDustHTLCExposure as LdkMaxDustHTLCExposure;
use lightning::util::ser::{Readable, Writeable, Writer};
use lightning_net_tokio::SocketDescriptor;
use lightning_transaction_sync::EsploraSyncClient;

use bitcoin::secp256k1::PublicKey;
use bitcoin::OutPoint;

use std::sync::{Arc, Mutex, RwLock};

pub(crate) type ChainMonitor<K> = chainmonitor::ChainMonitor<
	InMemorySigner,
	Arc<EsploraSyncClient<Arc<FilesystemLogger>>>,
	Arc<Wallet<bdk::database::SqliteDatabase, Arc<FilesystemLogger>>>,
	Arc<Wallet<bdk::database::SqliteDatabase, Arc<FilesystemLogger>>>,
	Arc<FilesystemLogger>,
	Arc<K>,
>;

pub(crate) type PeerManager<K> = lightning::ln::peer_handler::PeerManager<
	SocketDescriptor,
	Arc<ChannelManager<K>>,
	Arc<dyn RoutingMessageHandler + Send + Sync>,
	Arc<OnionMessenger>,
	Arc<FilesystemLogger>,
	IgnoringMessageHandler,
	Arc<WalletKeysManager<bdk::database::SqliteDatabase, Arc<FilesystemLogger>>>,
>;

pub(crate) type ChannelManager<K> = lightning::ln::channelmanager::ChannelManager<
	Arc<ChainMonitor<K>>,
	Arc<Wallet<bdk::database::SqliteDatabase, Arc<FilesystemLogger>>>,
	Arc<WalletKeysManager<bdk::database::SqliteDatabase, Arc<FilesystemLogger>>>,
	Arc<WalletKeysManager<bdk::database::SqliteDatabase, Arc<FilesystemLogger>>>,
	Arc<WalletKeysManager<bdk::database::SqliteDatabase, Arc<FilesystemLogger>>>,
	Arc<Wallet<bdk::database::SqliteDatabase, Arc<FilesystemLogger>>>,
	Arc<Router>,
	Arc<FilesystemLogger>,
>;

pub(crate) type KeysManager =
	WalletKeysManager<bdk::database::SqliteDatabase, Arc<FilesystemLogger>>;

pub(crate) type Router = DefaultRouter<
	Arc<NetworkGraph>,
	Arc<FilesystemLogger>,
	Arc<Mutex<Scorer>>,
	ProbabilisticScoringFeeParameters,
	Scorer,
>;
pub(crate) type Scorer = ProbabilisticScorer<Arc<NetworkGraph>, Arc<FilesystemLogger>>;

pub(crate) type NetworkGraph = gossip::NetworkGraph<Arc<FilesystemLogger>>;

pub(crate) type UtxoLookup = dyn lightning::routing::utxo::UtxoLookup + Send + Sync;

pub(crate) type P2PGossipSync = lightning::routing::gossip::P2PGossipSync<
	Arc<NetworkGraph>,
	Arc<UtxoLookup>,
	Arc<FilesystemLogger>,
>;
pub(crate) type RapidGossipSync =
	lightning_rapid_gossip_sync::RapidGossipSync<Arc<NetworkGraph>, Arc<FilesystemLogger>>;

pub(crate) type GossipSync = lightning_background_processor::GossipSync<
	Arc<P2PGossipSync>,
	Arc<RapidGossipSync>,
	Arc<NetworkGraph>,
	Arc<UtxoLookup>,
	Arc<FilesystemLogger>,
>;

pub(crate) type OnionMessenger = lightning::onion_message::OnionMessenger<
	Arc<WalletKeysManager<bdk::database::SqliteDatabase, Arc<FilesystemLogger>>>,
	Arc<WalletKeysManager<bdk::database::SqliteDatabase, Arc<FilesystemLogger>>>,
	Arc<FilesystemLogger>,
	Arc<FakeMessageRouter>,
	IgnoringMessageHandler,
	IgnoringMessageHandler,
>;

pub(crate) struct FakeMessageRouter {}

impl lightning::onion_message::MessageRouter for FakeMessageRouter {
	fn find_path(
		&self, _sender: PublicKey, _peers: Vec<PublicKey>,
		_destination: lightning::onion_message::Destination,
	) -> Result<lightning::onion_message::OnionMessagePath, ()> {
		unimplemented!()
	}
}

/// A local, potentially user-provided, identifier of a channel.
///
/// By default, this will be randomly generated for the user to ensure local uniqueness.
#[derive(Debug, Copy, Clone, PartialEq, Eq)]
pub struct UserChannelId(pub u128);

impl Writeable for UserChannelId {
	fn write<W: Writer>(&self, writer: &mut W) -> Result<(), lightning::io::Error> {
		Ok(self.0.write(writer)?)
	}
}

impl Readable for UserChannelId {
	fn read<R: lightning::io::Read>(
		reader: &mut R,
	) -> Result<Self, lightning::ln::msgs::DecodeError> {
		Ok(Self(Readable::read(reader)?))
	}
}

/// Details of a channel as returned by [`Node::list_channels`].
///
/// [`Node::list_channels`]: crate::Node::list_channels
#[derive(Debug, Clone)]
pub struct ChannelDetails {
	/// The channel ID (prior to funding transaction generation, this is a random 32-byte
	/// identifier, afterwards this is the transaction ID of the funding transaction XOR the
	/// funding transaction output).
	///
	/// Note that this means this value is *not* persistent - it can change once during the
	/// lifetime of the channel.
	pub channel_id: ChannelId,
	/// The node ID of our the channel's counterparty.
	pub counterparty_node_id: PublicKey,
	/// The channel's funding transaction output, if we've negotiated the funding transaction with
	/// our counterparty already.
	pub funding_txo: Option<OutPoint>,
	/// The value, in satoshis, of this channel as it appears in the funding output.
	pub channel_value_sats: u64,
	/// The value, in satoshis, that must always be held as a reserve in the channel for us. This
	/// value ensures that if we broadcast a revoked state, our counterparty can punish us by
	/// claiming at least this value on chain.
	///
	/// This value is not included in [`outbound_capacity_msat`] as it can never be spent.
	///
	/// This value will be `None` for outbound channels until the counterparty accepts the channel.
	///
	/// [`outbound_capacity_msat`]: Self::outbound_capacity_msat
	pub unspendable_punishment_reserve: Option<u64>,
	/// The local `user_channel_id` of this channel.
	pub user_channel_id: UserChannelId,
	/// The currently negotiated fee rate denominated in satoshi per 1000 weight units,
	/// which is applied to commitment and HTLC transactions.
	pub feerate_sat_per_1000_weight: u32,
	/// The total balance of the channel. This is the amount that will be returned to
	/// the user if the channel is closed.
	///
	/// The value is not exact, due to potential in-flight and fee-rate changes. Therefore, exactly
	/// this amount is likely irrecoverable on close.
	pub balance_msat: u64,
	/// The available outbound capacity for sending HTLCs to the remote peer.
	///
	/// The amount does not include any pending HTLCs which are not yet resolved (and, thus, whose
	/// balance is not available for inclusion in new outbound HTLCs). This further does not include
	/// any pending outgoing HTLCs which are awaiting some other resolution to be sent.
	pub outbound_capacity_msat: u64,
	/// The available outbound capacity for sending HTLCs to the remote peer.
	///
	/// The amount does not include any pending HTLCs which are not yet resolved
	/// (and, thus, whose balance is not available for inclusion in new inbound HTLCs). This further
	/// does not include any pending outgoing HTLCs which are awaiting some other resolution to be
	/// sent.
	pub inbound_capacity_msat: u64,
	/// The number of required confirmations on the funding transactions before the funding is
	/// considered "locked". The amount is selected by the channel fundee.
	///
	/// The value will be `None` for outbound channels until the counterparty accepts the channel.
	pub confirmations_required: Option<u32>,
	/// The current number of confirmations on the funding transaction.
	pub confirmations: Option<u32>,
	/// Returns `true` if the channel was initiated (and therefore funded) by us.
	pub is_outbound: bool,
	/// Returns `true` if both parties have exchanged `channel_ready` messages, and the channel is
	/// not currently being shut down. Both parties exchange `channel_ready` messages upon
	/// independently verifying that the required confirmations count provided by
	/// `confirmations_required` has been reached.
	pub is_channel_ready: bool,
	/// Returns `true` if the channel (a) `channel_ready` messages have been exchanged, (b) the
	/// peer is connected, and (c) the channel is not currently negotiating shutdown.
	///
	/// This is a strict superset of `is_channel_ready`.
	pub is_usable: bool,
	/// Returns `true` if this channel is (or will be) publicly-announced
	pub is_public: bool,
	/// The difference in the CLTV value between incoming HTLCs and an outbound HTLC forwarded over
	/// the channel.
	pub cltv_expiry_delta: Option<u16>,
	/// The value, in satoshis, that must always be held in the channel for our counterparty. This
	/// value ensures that if our counterparty broadcasts a revoked state, we can punish them by
	/// claiming at least this value on chain.
	///
	/// This value is not included in [`inbound_capacity_msat`] as it can never be spent.
	///
	/// [`inbound_capacity_msat`]: ChannelDetails::inbound_capacity_msat
	pub counterparty_unspendable_punishment_reserve: u64,
	/// The smallest value HTLC (in msat) the remote peer will accept, for this channel.
	///
	/// This field is only `None` before we have received either the `OpenChannel` or
	/// `AcceptChannel` message from the remote peer.
	pub counterparty_outbound_htlc_minimum_msat: Option<u64>,
	/// The largest value HTLC (in msat) the remote peer currently will accept, for this channel.
	pub counterparty_outbound_htlc_maximum_msat: Option<u64>,
	/// Base routing fee in millisatoshis.
	pub counterparty_forwarding_info_fee_base_msat: Option<u32>,
	/// Proportional fee, in millionths of a satoshi the channel will charge per transferred satoshi.
	pub counterparty_forwarding_info_fee_proportional_millionths: Option<u32>,
	/// The minimum difference in CLTV expiry between an ingoing HTLC and its outgoing counterpart,
	/// such that the outgoing HTLC is forwardable to this counterparty.
	pub counterparty_forwarding_info_cltv_expiry_delta: Option<u16>,
	/// The available outbound capacity for sending a single HTLC to the remote peer. This is
	/// similar to [`ChannelDetails::outbound_capacity_msat`] but it may be further restricted by
	/// the current state and per-HTLC limit(s). This is intended for use when routing, allowing us
	/// to use a limit as close as possible to the HTLC limit we can currently send.
	///
	/// See also [`ChannelDetails::next_outbound_htlc_minimum_msat`],
	/// [`ChannelDetails::balance_msat`], and [`ChannelDetails::outbound_capacity_msat`].
	pub next_outbound_htlc_limit_msat: u64,
	/// The minimum value for sending a single HTLC to the remote peer. This is the equivalent of
	/// [`ChannelDetails::next_outbound_htlc_limit_msat`] but represents a lower-bound, rather than
	/// an upper-bound. This is intended for use when routing, allowing us to ensure we pick a
	/// route which is valid.
	pub next_outbound_htlc_minimum_msat: u64,
	/// The number of blocks (after our commitment transaction confirms) that we will need to wait
	/// until we can claim our funds after we force-close the channel. During this time our
	/// counterparty is allowed to punish us if we broadcasted a stale state. If our counterparty
	/// force-closes the channel and broadcasts a commitment transaction we do not have to wait any
	/// time to claim our non-HTLC-encumbered funds.
	///
	/// This value will be `None` for outbound channels until the counterparty accepts the channel.
	pub force_close_spend_delay: Option<u16>,
	/// The smallest value HTLC (in msat) we will accept, for this channel.
	pub inbound_htlc_minimum_msat: u64,
	/// The largest value HTLC (in msat) we currently will accept, for this channel.
	pub inbound_htlc_maximum_msat: Option<u64>,
	/// Set of configurable parameters that affect channel operation.
	pub config: Arc<ChannelConfig>,
}

impl From<LdkChannelDetails> for ChannelDetails {
	fn from(value: LdkChannelDetails) -> Self {
		ChannelDetails {
			channel_id: value.channel_id,
			counterparty_node_id: value.counterparty.node_id,
			funding_txo: value.funding_txo.and_then(|o| Some(o.into_bitcoin_outpoint())),
			channel_value_sats: value.channel_value_satoshis,
			unspendable_punishment_reserve: value.unspendable_punishment_reserve,
			user_channel_id: UserChannelId(value.user_channel_id),
			// unwrap safety: This value will be `None` for objects serialized with LDK versions
			// prior to 0.0.115.
			feerate_sat_per_1000_weight: value.feerate_sat_per_1000_weight.unwrap(),
			balance_msat: value.balance_msat,
			outbound_capacity_msat: value.outbound_capacity_msat,
			inbound_capacity_msat: value.inbound_capacity_msat,
			confirmations_required: value.confirmations_required,
			confirmations: value.confirmations,
			is_outbound: value.is_outbound,
			is_channel_ready: value.is_channel_ready,
			is_usable: value.is_usable,
			is_public: value.is_public,
			cltv_expiry_delta: value.config.map(|c| c.cltv_expiry_delta),
			counterparty_unspendable_punishment_reserve: value
				.counterparty
				.unspendable_punishment_reserve,
			counterparty_outbound_htlc_minimum_msat: value.counterparty.outbound_htlc_minimum_msat,
			counterparty_outbound_htlc_maximum_msat: value.counterparty.outbound_htlc_maximum_msat,
			counterparty_forwarding_info_fee_base_msat: value
				.counterparty
				.forwarding_info
				.as_ref()
				.map(|f| f.fee_base_msat),
			counterparty_forwarding_info_fee_proportional_millionths: value
				.counterparty
				.forwarding_info
				.as_ref()
				.map(|f| f.fee_proportional_millionths),
			counterparty_forwarding_info_cltv_expiry_delta: value
				.counterparty
				.forwarding_info
				.as_ref()
				.map(|f| f.cltv_expiry_delta),
			next_outbound_htlc_limit_msat: value.next_outbound_htlc_limit_msat,
			next_outbound_htlc_minimum_msat: value.next_outbound_htlc_minimum_msat,
			force_close_spend_delay: value.force_close_spend_delay,
			// unwrap safety: This field is only `None` for objects serialized prior to LDK 0.0.107
			inbound_htlc_minimum_msat: value.inbound_htlc_minimum_msat.unwrap_or(0),
			inbound_htlc_maximum_msat: value.inbound_htlc_maximum_msat,
			// unwrap safety: `config` is only `None` for LDK objects serialized prior to 0.0.109.
			config: value.config.map(|c| Arc::new(c.into())).unwrap(),
		}
	}
}

/// Details of a known Lightning peer as returned by [`Node::list_peers`].
///
/// [`Node::list_peers`]: crate::Node::list_peers
#[derive(Debug, Clone, PartialEq, Eq)]
pub struct PeerDetails {
	/// The node ID of the peer.
	pub node_id: PublicKey,
	/// The network address of the peer.
	pub address: SocketAddress,
	/// Indicates whether we'll try to reconnect to this peer after restarts.
	pub is_persisted: bool,
	/// Indicates whether we currently have an active connection with the peer.
	pub is_connected: bool,
}

/// Options which apply on a per-channel basis.
///
/// See documentation of [`LdkChannelConfig`] for details.
#[derive(Debug)]
pub struct ChannelConfig {
	inner: RwLock<LdkChannelConfig>,
}

impl Clone for ChannelConfig {
	fn clone(&self) -> Self {
		self.inner.read().unwrap().clone().into()
	}
}

impl ChannelConfig {
	/// Constructs a new `ChannelConfig`.
	pub fn new() -> Self {
		Self::default()
	}

	/// Returns the set `forwarding_fee_proportional_millionths`.
	pub fn forwarding_fee_proportional_millionths(&self) -> u32 {
		self.inner.read().unwrap().forwarding_fee_proportional_millionths
	}

	/// Sets the `forwarding_fee_proportional_millionths`.
	pub fn set_forwarding_fee_proportional_millionths(&self, value: u32) {
		self.inner.write().unwrap().forwarding_fee_proportional_millionths = value;
	}

	/// Returns the set `forwarding_fee_base_msat`.
	pub fn forwarding_fee_base_msat(&self) -> u32 {
		self.inner.read().unwrap().forwarding_fee_base_msat
	}

	/// Sets the `forwarding_fee_base_msat`.
	pub fn set_forwarding_fee_base_msat(&self, fee_msat: u32) {
		self.inner.write().unwrap().forwarding_fee_base_msat = fee_msat;
	}

	/// Returns the set `cltv_expiry_delta`.
	pub fn cltv_expiry_delta(&self) -> u16 {
		self.inner.read().unwrap().cltv_expiry_delta
	}

	/// Sets the `cltv_expiry_delta`.
	pub fn set_cltv_expiry_delta(&self, value: u16) {
		self.inner.write().unwrap().cltv_expiry_delta = value;
	}

	/// Returns the set `force_close_avoidance_max_fee_satoshis`.
	pub fn force_close_avoidance_max_fee_satoshis(&self) -> u64 {
		self.inner.read().unwrap().force_close_avoidance_max_fee_satoshis
	}

	/// Sets the `force_close_avoidance_max_fee_satoshis`.
	pub fn set_force_close_avoidance_max_fee_satoshis(&self, value_sat: u64) {
		self.inner.write().unwrap().force_close_avoidance_max_fee_satoshis = value_sat;
	}

	/// Returns the set `accept_underpaying_htlcs`.
	pub fn accept_underpaying_htlcs(&self) -> bool {
		self.inner.read().unwrap().accept_underpaying_htlcs
	}

	/// Sets the `accept_underpaying_htlcs`.
	pub fn set_accept_underpaying_htlcs(&self, value: bool) {
		self.inner.write().unwrap().accept_underpaying_htlcs = value;
	}

	/// Sets the `max_dust_htlc_exposure` from a fixed limit.
	pub fn set_max_dust_htlc_exposure_from_fixed_limit(&self, limit_msat: u64) {
		self.inner.write().unwrap().max_dust_htlc_exposure =
			LdkMaxDustHTLCExposure::FixedLimitMsat(limit_msat);
	}

	/// Sets the `max_dust_htlc_exposure` from a fee rate multiplier.
	pub fn set_max_dust_htlc_exposure_from_fee_rate_multiplier(&self, multiplier: u64) {
		self.inner.write().unwrap().max_dust_htlc_exposure =
			LdkMaxDustHTLCExposure::FeeRateMultiplier(multiplier);
	}
}

impl From<LdkChannelConfig> for ChannelConfig {
	fn from(value: LdkChannelConfig) -> Self {
		Self { inner: RwLock::new(value) }
	}
}

impl From<ChannelConfig> for LdkChannelConfig {
	fn from(value: ChannelConfig) -> Self {
		*value.inner.read().unwrap()
	}
}

impl Default for ChannelConfig {
	fn default() -> Self {
		LdkChannelConfig::default().into()
	}
}

'''
'''--- src/uniffi_types.rs ---
use crate::UniffiCustomTypeConverter;

use crate::error::Error;
use crate::hex_utils;
use crate::io::sqlite_store::SqliteStore;
use crate::{Node, SocketAddress, UserChannelId};

use bitcoin::hashes::sha256::Hash as Sha256;
use bitcoin::hashes::Hash;
use bitcoin::secp256k1::PublicKey;
use bitcoin::{Address, Txid};
use lightning::ln::{ChannelId, PaymentHash, PaymentPreimage, PaymentSecret};
use lightning_invoice::{Bolt11Invoice, SignedRawBolt11Invoice};

use bip39::Mnemonic;

use std::convert::TryInto;
use std::str::FromStr;

/// This type alias is required as Uniffi doesn't support generics, i.e., we can only expose the
/// concretized types via this aliasing hack.
pub type LDKNode = Node<SqliteStore>;

impl UniffiCustomTypeConverter for PublicKey {
	type Builtin = String;

	fn into_custom(val: Self::Builtin) -> uniffi::Result<Self> {
		if let Ok(key) = PublicKey::from_str(&val) {
			return Ok(key);
		}

		Err(Error::InvalidPublicKey.into())
	}

	fn from_custom(obj: Self) -> Self::Builtin {
		obj.to_string()
	}
}

impl UniffiCustomTypeConverter for Address {
	type Builtin = String;

	fn into_custom(val: Self::Builtin) -> uniffi::Result<Self> {
		if let Ok(addr) = Address::from_str(&val) {
			return Ok(addr);
		}

		Err(Error::InvalidAddress.into())
	}

	fn from_custom(obj: Self) -> Self::Builtin {
		obj.to_string()
	}
}

impl UniffiCustomTypeConverter for Bolt11Invoice {
	type Builtin = String;

	fn into_custom(val: Self::Builtin) -> uniffi::Result<Self> {
		if let Ok(signed) = val.parse::<SignedRawBolt11Invoice>() {
			if let Ok(invoice) = Bolt11Invoice::from_signed(signed) {
				return Ok(invoice);
			}
		}

		Err(Error::InvalidInvoice.into())
	}

	fn from_custom(obj: Self) -> Self::Builtin {
		obj.to_string()
	}
}

impl UniffiCustomTypeConverter for PaymentHash {
	type Builtin = String;

	fn into_custom(val: Self::Builtin) -> uniffi::Result<Self> {
		if let Ok(hash) = Sha256::from_str(&val) {
			Ok(PaymentHash(hash.into_inner()))
		} else {
			Err(Error::InvalidPaymentHash.into())
		}
	}

	fn from_custom(obj: Self) -> Self::Builtin {
		Sha256::from_slice(&obj.0).unwrap().to_string()
	}
}

impl UniffiCustomTypeConverter for PaymentPreimage {
	type Builtin = String;

	fn into_custom(val: Self::Builtin) -> uniffi::Result<Self> {
		if let Some(bytes_vec) = hex_utils::to_vec(&val) {
			let bytes_res = bytes_vec.try_into();
			if let Ok(bytes) = bytes_res {
				return Ok(PaymentPreimage(bytes));
			}
		}
		Err(Error::InvalidPaymentPreimage.into())
	}

	fn from_custom(obj: Self) -> Self::Builtin {
		hex_utils::to_string(&obj.0)
	}
}

impl UniffiCustomTypeConverter for PaymentSecret {
	type Builtin = String;

	fn into_custom(val: Self::Builtin) -> uniffi::Result<Self> {
		if let Some(bytes_vec) = hex_utils::to_vec(&val) {
			let bytes_res = bytes_vec.try_into();
			if let Ok(bytes) = bytes_res {
				return Ok(PaymentSecret(bytes));
			}
		}
		Err(Error::InvalidPaymentSecret.into())
	}

	fn from_custom(obj: Self) -> Self::Builtin {
		hex_utils::to_string(&obj.0)
	}
}

impl UniffiCustomTypeConverter for ChannelId {
	type Builtin = String;

	fn into_custom(val: Self::Builtin) -> uniffi::Result<Self> {
		if let Some(hex_vec) = hex_utils::to_vec(&val) {
			if hex_vec.len() == 32 {
				let mut channel_id = [0u8; 32];
				channel_id.copy_from_slice(&hex_vec[..]);
				return Ok(Self(channel_id));
			}
		}
		Err(Error::InvalidChannelId.into())
	}

	fn from_custom(obj: Self) -> Self::Builtin {
		hex_utils::to_string(&obj.0)
	}
}

impl UniffiCustomTypeConverter for UserChannelId {
	type Builtin = String;

	fn into_custom(val: Self::Builtin) -> uniffi::Result<Self> {
		Ok(UserChannelId(u128::from_str(&val).map_err(|_| Error::InvalidChannelId)?))
	}

	fn from_custom(obj: Self) -> Self::Builtin {
		obj.0.to_string()
	}
}

impl UniffiCustomTypeConverter for Txid {
	type Builtin = String;
	fn into_custom(val: Self::Builtin) -> uniffi::Result<Self> {
		Ok(Txid::from_str(&val)?)
	}

	fn from_custom(obj: Self) -> Self::Builtin {
		obj.to_string()
	}
}

impl UniffiCustomTypeConverter for Mnemonic {
	type Builtin = String;
	fn into_custom(val: Self::Builtin) -> uniffi::Result<Self> {
		Ok(Mnemonic::from_str(&val).map_err(|_| Error::InvalidSecretKey)?)
	}

	fn from_custom(obj: Self) -> Self::Builtin {
		obj.to_string()
	}
}

impl UniffiCustomTypeConverter for SocketAddress {
	type Builtin = String;
	fn into_custom(val: Self::Builtin) -> uniffi::Result<Self> {
		Ok(SocketAddress::from_str(&val).map_err(|_| Error::InvalidSocketAddress)?)
	}

	fn from_custom(obj: Self) -> Self::Builtin {
		obj.to_string()
	}
}

'''
'''--- src/wallet.rs ---
use crate::logger::{log_error, log_info, log_trace, Logger};

use crate::Error;

use lightning::chain::chaininterface::{
	BroadcasterInterface, ConfirmationTarget, FeeEstimator, FEERATE_FLOOR_SATS_PER_KW,
};

use lightning::ln::msgs::{DecodeError, UnsignedGossipMessage};
use lightning::ln::script::ShutdownScript;
use lightning::sign::{
	EntropySource, InMemorySigner, KeyMaterial, KeysManager, NodeSigner, Recipient, SignerProvider,
	SpendableOutputDescriptor,
};

use lightning::util::message_signing;

use bdk::blockchain::{Blockchain, EsploraBlockchain};
use bdk::database::BatchDatabase;
use bdk::wallet::AddressIndex;
use bdk::{FeeRate, SignOptions, SyncOptions};

use bitcoin::bech32::u5;
use bitcoin::secp256k1::ecdh::SharedSecret;
use bitcoin::secp256k1::ecdsa::{RecoverableSignature, Signature};
use bitcoin::secp256k1::{PublicKey, Scalar, Secp256k1, Signing};
use bitcoin::{LockTime, PackedLockTime, Script, Transaction, TxOut, Txid};

use std::collections::HashMap;
use std::ops::Deref;
use std::sync::{Arc, Condvar, Mutex, RwLock};
use std::time::Duration;

pub struct Wallet<D, L: Deref>
where
	D: BatchDatabase,
	L::Target: Logger,
{
	// A BDK blockchain used for wallet sync.
	blockchain: EsploraBlockchain,
	// A BDK on-chain wallet.
	inner: Mutex<bdk::Wallet<D>>,
	// A cache storing the most recently retrieved fee rate estimations.
	fee_rate_cache: RwLock<HashMap<ConfirmationTarget, FeeRate>>,
	runtime: Arc<RwLock<Option<tokio::runtime::Runtime>>>,
	sync_lock: (Mutex<()>, Condvar),
	logger: L,
}

impl<D, L: Deref> Wallet<D, L>
where
	D: BatchDatabase,
	L::Target: Logger,
{
	pub(crate) fn new(
		blockchain: EsploraBlockchain, wallet: bdk::Wallet<D>,
		runtime: Arc<RwLock<Option<tokio::runtime::Runtime>>>, logger: L,
	) -> Self {
		let inner = Mutex::new(wallet);
		let fee_rate_cache = RwLock::new(HashMap::new());
		let sync_lock = (Mutex::new(()), Condvar::new());
		Self { blockchain, inner, fee_rate_cache, runtime, sync_lock, logger }
	}

	pub(crate) async fn sync(&self) -> Result<(), Error> {
		let (lock, cvar) = &self.sync_lock;

		let guard = match lock.try_lock() {
			Ok(guard) => guard,
			Err(_) => {
				log_info!(self.logger, "Sync in progress, skipping.");
				let guard = cvar.wait(lock.lock().unwrap());
				drop(guard);
				cvar.notify_all();
				return Ok(());
			}
		};

		let sync_options = SyncOptions { progress: None };
		let wallet_lock = self.inner.lock().unwrap();
		let res = match wallet_lock.sync(&self.blockchain, sync_options).await {
			Ok(()) => Ok(()),
			Err(e) => match e {
				bdk::Error::Esplora(ref be) => match **be {
					bdk::blockchain::esplora::EsploraError::Reqwest(_) => {
						tokio::time::sleep(Duration::from_secs(1)).await;
						log_error!(
							self.logger,
							"Sync failed due to HTTP connection error, retrying: {}",
							e
						);
						let sync_options = SyncOptions { progress: None };
						wallet_lock
							.sync(&self.blockchain, sync_options)
							.await
							.map_err(|e| From::from(e))
					}
					_ => {
						log_error!(self.logger, "Sync failed due to Esplora error: {}", e);
						Err(From::from(e))
					}
				},
				_ => {
					log_error!(self.logger, "Wallet sync error: {}", e);
					Err(From::from(e))
				}
			},
		};

		drop(guard);
		cvar.notify_all();
		res
	}

	pub(crate) async fn update_fee_estimates(&self) -> Result<(), Error> {
		let mut locked_fee_rate_cache = self.fee_rate_cache.write().unwrap();

		let confirmation_targets = vec![
			ConfirmationTarget::OnChainSweep,
			ConfirmationTarget::MaxAllowedNonAnchorChannelRemoteFee,
			ConfirmationTarget::MinAllowedAnchorChannelRemoteFee,
			ConfirmationTarget::MinAllowedNonAnchorChannelRemoteFee,
			ConfirmationTarget::AnchorChannelFee,
			ConfirmationTarget::NonAnchorChannelFee,
			ConfirmationTarget::ChannelCloseMinimum,
		];
		for target in confirmation_targets {
			let num_blocks = match target {
				ConfirmationTarget::OnChainSweep => 6,
				ConfirmationTarget::MaxAllowedNonAnchorChannelRemoteFee => 1,
				ConfirmationTarget::MinAllowedAnchorChannelRemoteFee => 1008,
				ConfirmationTarget::MinAllowedNonAnchorChannelRemoteFee => 144,
				ConfirmationTarget::AnchorChannelFee => 1008,
				ConfirmationTarget::NonAnchorChannelFee => 12,
				ConfirmationTarget::ChannelCloseMinimum => 144,
			};

			let est_fee_rate = self.blockchain.estimate_fee(num_blocks).await;

			match est_fee_rate {
				Ok(rate) => {
					// LDK 0.0.118 introduced changes to the `ConfirmationTarget` semantics that
					// require some post-estimation adjustments to the fee rates, which we do here.
					let adjusted_fee_rate = match target {
						ConfirmationTarget::MaxAllowedNonAnchorChannelRemoteFee => {
							let really_high_prio = rate.as_sat_per_vb() * 10.0;
							FeeRate::from_sat_per_vb(really_high_prio)
						}
						ConfirmationTarget::MinAllowedNonAnchorChannelRemoteFee => {
							let slightly_less_than_background = rate.fee_wu(1000) - 250;
							FeeRate::from_sat_per_kwu(slightly_less_than_background as f32)
						}
						_ => rate,
					};
					locked_fee_rate_cache.insert(target, adjusted_fee_rate);
					log_trace!(
						self.logger,
						"Fee rate estimation updated for {:?}: {} sats/kwu",
						target,
						adjusted_fee_rate.fee_wu(1000)
					);
				}
				Err(e) => {
					log_error!(
						self.logger,
						"Failed to update fee rate estimation for {:?}: {}",
						target,
						e
					);
				}
			}
		}
		Ok(())
	}

	pub(crate) fn create_funding_transaction(
		&self, output_script: Script, value_sats: u64, confirmation_target: ConfirmationTarget,
		locktime: LockTime,
	) -> Result<Transaction, Error> {
		let fee_rate = self.estimate_fee_rate(confirmation_target);

		let locked_wallet = self.inner.lock().unwrap();
		let mut tx_builder = locked_wallet.build_tx();

		tx_builder
			.add_recipient(output_script, value_sats)
			.fee_rate(fee_rate)
			.nlocktime(locktime)
			.enable_rbf();

		let mut psbt = match tx_builder.finish() {
			Ok((psbt, _)) => {
				log_trace!(self.logger, "Created funding PSBT: {:?}", psbt);
				psbt
			}
			Err(err) => {
				log_error!(self.logger, "Failed to create funding transaction: {}", err);
				return Err(err.into());
			}
		};

		match locked_wallet.sign(&mut psbt, SignOptions::default()) {
			Ok(finalized) => {
				if !finalized {
					return Err(Error::OnchainTxCreationFailed);
				}
			}
			Err(err) => {
				log_error!(self.logger, "Failed to create funding transaction: {}", err);
				return Err(err.into());
			}
		}

		Ok(psbt.extract_tx())
	}

	pub(crate) fn get_new_address(&self) -> Result<bitcoin::Address, Error> {
		let address_info = self.inner.lock().unwrap().get_address(AddressIndex::New)?;
		Ok(address_info.address)
	}

	pub(crate) fn get_balance(&self) -> Result<bdk::Balance, Error> {
		Ok(self.inner.lock().unwrap().get_balance()?)
	}

	/// Send funds to the given address.
	///
	/// If `amount_msat_or_drain` is `None` the wallet will be drained, i.e., all available funds will be
	/// spent.
	pub(crate) fn send_to_address(
		&self, address: &bitcoin::Address, amount_msat_or_drain: Option<u64>,
	) -> Result<Txid, Error> {
		let confirmation_target = ConfirmationTarget::NonAnchorChannelFee;
		let fee_rate = self.estimate_fee_rate(confirmation_target);

		let tx = {
			let locked_wallet = self.inner.lock().unwrap();
			let mut tx_builder = locked_wallet.build_tx();

			if let Some(amount_sats) = amount_msat_or_drain {
				tx_builder
					.add_recipient(address.script_pubkey(), amount_sats)
					.fee_rate(fee_rate)
					.enable_rbf();
			} else {
				tx_builder
					.drain_wallet()
					.drain_to(address.script_pubkey())
					.fee_rate(fee_rate)
					.enable_rbf();
			}

			let mut psbt = match tx_builder.finish() {
				Ok((psbt, _)) => {
					log_trace!(self.logger, "Created PSBT: {:?}", psbt);
					psbt
				}
				Err(err) => {
					log_error!(self.logger, "Failed to create transaction: {}", err);
					return Err(err.into());
				}
			};

			match locked_wallet.sign(&mut psbt, SignOptions::default()) {
				Ok(finalized) => {
					if !finalized {
						return Err(Error::OnchainTxCreationFailed);
					}
				}
				Err(err) => {
					log_error!(self.logger, "Failed to create transaction: {}", err);
					return Err(err.into());
				}
			}
			psbt.extract_tx()
		};

		self.broadcast_transactions(&[&tx]);

		let txid = tx.txid();

		if let Some(amount_sats) = amount_msat_or_drain {
			log_info!(
				self.logger,
				"Created new transaction {} sending {}sats on-chain to address {}",
				txid,
				amount_sats,
				address
			);
		} else {
			log_info!(
				self.logger,
				"Created new transaction {} sending all available on-chain funds to address {}",
				txid,
				address
			);
		}

		Ok(txid)
	}

	fn estimate_fee_rate(&self, confirmation_target: ConfirmationTarget) -> FeeRate {
		let locked_fee_rate_cache = self.fee_rate_cache.read().unwrap();

		let fallback_sats_kwu = match confirmation_target {
			ConfirmationTarget::OnChainSweep => 5000,
			ConfirmationTarget::MaxAllowedNonAnchorChannelRemoteFee => 25 * 250,
			ConfirmationTarget::MinAllowedAnchorChannelRemoteFee => FEERATE_FLOOR_SATS_PER_KW,
			ConfirmationTarget::MinAllowedNonAnchorChannelRemoteFee => FEERATE_FLOOR_SATS_PER_KW,
			ConfirmationTarget::AnchorChannelFee => 500,
			ConfirmationTarget::NonAnchorChannelFee => 1000,
			ConfirmationTarget::ChannelCloseMinimum => 500,
		};

		// We'll fall back on this, if we really don't have any other information.
		let fallback_rate = FeeRate::from_sat_per_kwu(fallback_sats_kwu as f32);

		*locked_fee_rate_cache.get(&confirmation_target).unwrap_or(&fallback_rate)
	}
}

impl<D, L: Deref> FeeEstimator for Wallet<D, L>
where
	D: BatchDatabase,
	L::Target: Logger,
{
	fn get_est_sat_per_1000_weight(&self, confirmation_target: ConfirmationTarget) -> u32 {
		(self.estimate_fee_rate(confirmation_target).fee_wu(1000) as u32)
			.max(FEERATE_FLOOR_SATS_PER_KW)
	}
}

impl<D, L: Deref> BroadcasterInterface for Wallet<D, L>
where
	D: BatchDatabase,
	L::Target: Logger,
{
	fn broadcast_transactions(&self, txs: &[&Transaction]) {
		let locked_runtime = self.runtime.read().unwrap();
		if locked_runtime.as_ref().is_none() {
			log_error!(self.logger, "Failed to broadcast transaction: No runtime.");
			return;
		}

		let errors = tokio::task::block_in_place(move || {
			locked_runtime.as_ref().unwrap().block_on(async move {
				let mut handles = Vec::new();
				let mut errors = Vec::new();

				for tx in txs {
					handles.push((tx.txid(), self.blockchain.broadcast(tx)));
				}

				for handle in handles {
					match handle.1.await {
						Ok(_) => {}
						Err(e) => {
							errors.push((e, handle.0));
						}
					}
				}
				errors
			})
		});

		for (e, txid) in errors {
			log_error!(self.logger, "Failed to broadcast transaction {}: {}", txid, e);
		}
	}
}

/// Similar to [`KeysManager`], but overrides the destination and shutdown scripts so they are
/// directly spendable by the BDK wallet.
pub struct WalletKeysManager<D, L: Deref>
where
	D: BatchDatabase,
	L::Target: Logger,
{
	inner: KeysManager,
	wallet: Arc<Wallet<D, L>>,
	logger: L,
}

impl<D, L: Deref> WalletKeysManager<D, L>
where
	D: BatchDatabase,
	L::Target: Logger,
{
	/// Constructs a `WalletKeysManager` that overrides the destination and shutdown scripts.
	///
	/// See [`KeysManager::new`] for more information on `seed`, `starting_time_secs`, and
	/// `starting_time_nanos`.
	pub fn new(
		seed: &[u8; 32], starting_time_secs: u64, starting_time_nanos: u32,
		wallet: Arc<Wallet<D, L>>, logger: L,
	) -> Self {
		let inner = KeysManager::new(seed, starting_time_secs, starting_time_nanos);
		Self { inner, wallet, logger }
	}

	/// See [`KeysManager::spend_spendable_outputs`] for documentation on this method.
	pub fn spend_spendable_outputs<C: Signing>(
		&self, descriptors: &[&SpendableOutputDescriptor], outputs: Vec<TxOut>,
		change_destination_script: Script, feerate_sat_per_1000_weight: u32,
		locktime: Option<PackedLockTime>, secp_ctx: &Secp256k1<C>,
	) -> Result<Option<Transaction>, ()> {
		let only_non_static = &descriptors
			.iter()
			.filter(|desc| !matches!(desc, SpendableOutputDescriptor::StaticOutput { .. }))
			.copied()
			.collect::<Vec<_>>();
		if only_non_static.is_empty() {
			return Ok(None);
		}
		self.inner
			.spend_spendable_outputs(
				only_non_static,
				outputs,
				change_destination_script,
				feerate_sat_per_1000_weight,
				locktime,
				secp_ctx,
			)
			.map(Some)
	}

	pub fn sign_message(&self, msg: &[u8]) -> Result<String, Error> {
		message_signing::sign(msg, &self.inner.get_node_secret_key())
			.or(Err(Error::MessageSigningFailed))
	}

	pub fn verify_signature(&self, msg: &[u8], sig: &str, pkey: &PublicKey) -> bool {
		message_signing::verify(msg, sig, pkey)
	}
}

impl<D, L: Deref> NodeSigner for WalletKeysManager<D, L>
where
	D: BatchDatabase,
	L::Target: Logger,
{
	fn get_node_id(&self, recipient: Recipient) -> Result<PublicKey, ()> {
		self.inner.get_node_id(recipient)
	}

	fn ecdh(
		&self, recipient: Recipient, other_key: &PublicKey, tweak: Option<&Scalar>,
	) -> Result<SharedSecret, ()> {
		self.inner.ecdh(recipient, other_key, tweak)
	}

	fn get_inbound_payment_key_material(&self) -> KeyMaterial {
		self.inner.get_inbound_payment_key_material()
	}

	fn sign_invoice(
		&self, hrp_bytes: &[u8], invoice_data: &[u5], recipient: Recipient,
	) -> Result<RecoverableSignature, ()> {
		self.inner.sign_invoice(hrp_bytes, invoice_data, recipient)
	}

	fn sign_gossip_message(&self, msg: UnsignedGossipMessage<'_>) -> Result<Signature, ()> {
		self.inner.sign_gossip_message(msg)
	}

	fn sign_bolt12_invoice(
		&self, invoice: &lightning::offers::invoice::UnsignedBolt12Invoice,
	) -> Result<bitcoin::secp256k1::schnorr::Signature, ()> {
		self.inner.sign_bolt12_invoice(invoice)
	}

	fn sign_bolt12_invoice_request(
		&self, invoice_request: &lightning::offers::invoice_request::UnsignedInvoiceRequest,
	) -> Result<bitcoin::secp256k1::schnorr::Signature, ()> {
		self.inner.sign_bolt12_invoice_request(invoice_request)
	}
}

impl<D, L: Deref> EntropySource for WalletKeysManager<D, L>
where
	D: BatchDatabase,
	L::Target: Logger,
{
	fn get_secure_random_bytes(&self) -> [u8; 32] {
		self.inner.get_secure_random_bytes()
	}
}

impl<D, L: Deref> SignerProvider for WalletKeysManager<D, L>
where
	D: BatchDatabase,
	L::Target: Logger,
{
	type Signer = InMemorySigner;

	fn generate_channel_keys_id(
		&self, inbound: bool, channel_value_satoshis: u64, user_channel_id: u128,
	) -> [u8; 32] {
		self.inner.generate_channel_keys_id(inbound, channel_value_satoshis, user_channel_id)
	}

	fn derive_channel_signer(
		&self, channel_value_satoshis: u64, channel_keys_id: [u8; 32],
	) -> Self::Signer {
		self.inner.derive_channel_signer(channel_value_satoshis, channel_keys_id)
	}

	fn read_chan_signer(&self, reader: &[u8]) -> Result<Self::Signer, DecodeError> {
		self.inner.read_chan_signer(reader)
	}

	fn get_destination_script(&self) -> Result<Script, ()> {
		let address = self.wallet.get_new_address().map_err(|e| {
			log_error!(self.logger, "Failed to retrieve new address from wallet: {}", e);
		})?;
		Ok(address.script_pubkey())
	}

	fn get_shutdown_scriptpubkey(&self) -> Result<ShutdownScript, ()> {
		let address = self.wallet.get_new_address().map_err(|e| {
			log_error!(self.logger, "Failed to retrieve new address from wallet: {}", e);
		})?;

		match address.payload {
			bitcoin::util::address::Payload::WitnessProgram { version, program } => {
				ShutdownScript::new_witness_program(version, &program).map_err(|e| {
					log_error!(self.logger, "Invalid shutdown script: {:?}", e);
				})
			}
			_ => {
				log_error!(
					self.logger,
					"Tried to use a non-witness address. This must never happen."
				);
				panic!("Tried to use a non-witness address. This must never happen.");
			}
		}
	}
}

'''
'''--- uniffi.toml ---
[bindings.kotlin]
package_name = "org.lightningdevkit.ldknode"
cdylib_name = "ldk_node"

[bindings.python]
cdylib_name = "ldk_node"

[bindings.swift]
module_name = "LDKNode"
ffi_module_name = "LDKNodeFFI"
ffi_module_filename ="LDKNodeFFI"
cdylib_name = "ldk_node"

'''