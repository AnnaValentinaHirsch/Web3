*GitHub Repository "near/nearcore"*

'''--- chain/jsonrpc/res/chain_n_chunk_info.html ---
<!DOCTYPE html>
<head>
    <link rel="stylesheet" href="chain_n_chunk_info.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script>
        function printTimeInMs(time) {
            if (time == null) {
                return "N/A"
            } else {
                return time + "ms"
            }
        }

        function prettyTime(dtString) {
            let time = new Date(Date.parse(dtString));
            return time.getUTCHours() + ":" + String(time.getUTCMinutes()).padStart(2, "0") + ":" +
                String(time.getUTCSeconds()).padStart(2, "0") + "." + String(time.getUTCMilliseconds()).padStart(3, '0')
        }

        function printStatus(blockStatus) {
            if (typeof blockStatus === "string") return blockStatus
            return JSON.stringify(blockStatus)
        }

        function printDuration(start, end) {
            let duration = Date.parse(end) - Date.parse(start);
            if (duration > 0) {
                return "+" + duration + "ms"
            } else {
                return duration + "ms"
            }
        }

        function generateBlocksTableHeader(num_shards) {
            let row = $('<tr>');
            row.append("<th>Height</th>");
            row.append("<th>Hash</th>");
            row.append("<th>Received</th>");
            row.append("<th>Status</th>");
            row.append("<th>In Progress for</th>");
            row.append("<th>In Orphan for</th>");
            row.append("<th>Missing Chunks for</th>");
            for (i = 0; i < num_shards; ++i) {
                row.append("<th>Shard " + i + "</th>");
            }
            $('.js-blocks-thead').append(row);
        }

        function getChunkStatusSymbol(chunk_status) {
            switch (chunk_status) {
                case "Completed":
                    return "✔";
                case "Requested":
                    return "⬇";
                case "NeedToRequest":
                    return ".";
                default:
                    break;
            }
        }

        function printChunksInfo(chunks_info, block_received_time, row) {
            chunks_info.forEach(chunk => {
                let cell = $('<td>');
                if (chunk == null) {
                    cell.append("<strong>No Chunk</strong>")
                } else {
                    cell.append("<strong>" + chunk.status + " " + getChunkStatusSymbol(chunk.status) + "</strong>");
                    if (chunk.completed_timestamp != null) {
                        cell.append("<br>Completed @ BR" + printDuration(block_received_time, chunk.completed_timestamp));
                    }
                    if (chunk.requested_timestamp != null) {
                        cell.append("<br>Requested @ BR" + printDuration(block_received_time, chunk.requested_timestamp));
                    }
                    if (chunk.request_duration != null) {
                        cell.append("<br>Duration " + chunk.request_duration + "ms");
                    }
                }
                row.append(cell);
            })
        }

        function onChainInfoFetched(status_data, data) {
            let head = status_data.detailed_debug_status.current_head_status;
            let header_head = status_data.detailed_debug_status.current_header_head_status;
            $('.js-chain-info-summary-head').append("Current head: " + head.hash + "@" + head.height + "\n");
            $('.js-chain-info-summary-header-head').append("Current header head: " + header_head.hash + "@" + header_head.height + "\n");

            let chain_info = data.status_response.ChainProcessingStatus;
            $('.js-chain-info-summary-orphans').append("Num blocks in orphan pool: " + chain_info.num_orphans + "\n");
            $('.js-chain-info-summary-missing-chunks').append("Num blocks in missing chunks pool: " + chain_info.num_blocks_missing_chunks + "\n");
            $('.js-chain-info-summary-processing').append("Num blocks in processing: " + chain_info.num_blocks_in_processing + "\n");

            let num_shards = 0;
            chain_info.blocks_info.forEach(block => {
                if (block.hash == head.hash) {
                    $('.js-blocks-tbody').append($("<tr><th colspan=10>HEAD</th></tr>"));
                }
                num_shards = block.chunks_info.length;
                let row = $('<tr>');
                row.append($('<td>').append(block.height));
                row.append($('<td>').append(block.hash));
                row.append($('<td>').append(prettyTime(block.received_timestamp)));
                row.append($('<td>').append(printStatus(block.block_status)));
                row.append($('<td>').append(printTimeInMs(block.in_progress_ms)));
                row.append($('<td>').append(printTimeInMs(block.orphaned_ms)));
                row.append($('<td>').append(printTimeInMs(block.missing_chunks_ms)));
                printChunksInfo(block.chunks_info, block.received_timestamp, row);
                $('.js-blocks-tbody').append(row);
            })

            chain_info.floating_chunks_info.forEach(chunk => {
                let row = $('<tr>');
                row.append($('<td>').append(chunk.height_created));
                row.append($('<td>').append(chunk.shard_id));
                row.append($('<td>').append(chunk.chunk_hash));
                row.append($('<td>').append(chunk.created_by));
                row.append($('<td>').append(chunk.status));
                $('.js-floating-chunks-tbody').append(row);
            })
            generateBlocksTableHeader(num_shards);
        }

        function fetchChainInfo(status_data) {
            $.ajax({
                type: "GET",
                url: "../api/chain_processing_status",
                success: data => {
                    onChainInfoFetched(status_data, data);
                },
                dataType: "json",
                error: function (errMsg, textStatus, errorThrown) {
                    alert("Failed: " + textStatus + " :" + errorThrown);
                },
                contentType: "application/json; charset=utf-8",
            });
        }

        function fetchStatus() {
            $.ajax({
                type: "GET",
                url: "../api/status",
                success: data => {
                    fetchChainInfo(data);
                },
                dataType: "json",
                error: function (errMsg, textStatus, errorThrown) {
                    alert("Failed: " + textStatus + " :" + errorThrown);
                },
                contentType: "application/json; charset=utf-8",
            })
        }

        $(document).ready(() => {
            $('span').text("Loading...");
            fetchStatus();
        });
    </script>
</head>

<body>
    <h1>
        Welcome to the Chain & Chunk Status page!
    </h1>
    <h2> Chain Info Summary </h2>
    <h3 class="js-chain-info-summary-head"></h3>
    <h3 class="js-chain-info-summary-header-head"></h3>
    <h3 class="js-chain-info-summary-orphans"></h3>
    <h3 class="js-chain-info-summary-missing-chunks"></h3>
    <h3 class="js-chain-info-summary-processing"></h3>

    <h3>Floating chunks</h3>
    <div>Floating chunks are the chunks for which we don't know the block they belong to yet.</div>
    <table>
        <thead>
            <tr>
                <th>Height</th>
                <th>ShardId</th>
                <th>Hash</th>
                <th>Created by</th>
                <th>Status</th>
            </tr>
        </thead>
        <tbody class="js-floating-chunks-tbody">
        </tbody>
    </table>

    <h3>Blocks</h3>
    <table>
        <thead class="js-blocks-thead">
        </thead>
        <tbody class="js-blocks-tbody">
        </tbody>
    </table>
</body>

'''
'''--- chain/jsonrpc/res/debug.html ---
<html>

<head>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script>
        function convertTime(total_seconds) {
            let seconds = total_seconds % 60;
            let total_minutes = (total_seconds - seconds) / 60;
            let minutes = total_minutes % 60;
            let total_hours = (total_minutes - minutes) / 60;
            let hours = total_hours % 24;
            let days = (total_hours - hours) / 24;
            return `${days}d ${addZeros(hours)}:${addZeros(minutes)}:${addZeros(seconds)}`;
        }
        function addZeros(x) {
            if (x >= 10) {
                return x;
            } else {
                return "0" + x;
            }
        }
        $(document).ready(() => {
            $.ajax({
                type: "GET",
                url: "status",
                success: data => {
                    let binaryText = $('<span>');
                    let version = data.version;
                    let githubLink = $('<a>')
                        .text(version.build)
                        .attr('href', `https://github.com/near/nearcore/tree/${data.version.build}`);
                    if (version.version == 'trunk') {
                        binaryText.append('Nightly build ').append(githubLink);
                    } else if (version.version == version.build) {
                        binaryText.append('Release ').append(githubLink);
                    } else {
                        binaryText.append(`Release ${version.version} (build `).append(githubLink).append(')');
                    }
                    binaryText.append(` compiled with rustc ${version.rustc_version}`);
                    $('.js-chain').text(data.chain_id);
                    $('.js-protocol').text(data.protocol_version);
                    $('.js-binary').children().remove();
                    $('.js-binary').append(binaryText);
                    $('.js-uptime').text(convertTime(data.uptime_sec));
                },
                dataType: "json",
                contentType: "application/json; charset=utf-8",
            });
        });
    </script>
</head>

<body>
    <h3>
        <p>Chain: <span class="js-chain"></span></p>
        <p>Protocol: <span class="js-protocol"></span></p>
        <p>Binary: <span class="js-binary"></span></p>
        <p>Uptime: <span class="js-uptime"></span></p>

    </h3>

    <h1><a href="debug/pages/last_blocks">Last blocks</a></h1>
    <h1><a href="debug/pages/network_info">Network info</a></h1>
    <h1><a href="debug/pages/tier1_network_info">TIER1 Network info</a></h1>
    <h1><a href="debug/pages/epoch_info">Epoch info</a></h1>
    <h1><a href="debug/pages/chain_n_chunk_info">Chain & Chunk info</a></h1>
    <h1><a href="debug/pages/sync">Sync info</a></h1>
    <h1><a href="debug/pages/validator">Validator info</a></h1>
    <h1><a href="debug/client_config">Client Config</a></h1>
    <h1><a href="debug/pages/split_store">Split Store</a></h1>
</body>

</html>

'''
'''--- chain/jsonrpc/res/epoch_info.html ---
<html>

<head>
   <link rel="stylesheet" href="epoch_info.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script>
        function convertTime(millis) {
            let total_seconds = Math.floor(millis / 1000);
            let hours = Math.floor(total_seconds / 3600)
            let minutes = Math.floor((total_seconds - (hours * 3600)) / 60)
            let seconds = total_seconds - (hours * 3600) - (minutes * 60)
            if (hours > 0) {
                if (minutes > 0) {
                    return `${hours}h ${minutes}m ${seconds}s`
                } else {
                    return `${hours}h ${seconds}s`
                }
            }
            if (minutes > 0) {
                return `${minutes}m ${seconds}s`
            }
            return `${seconds}s`
        }
        function humanFileSize(bytes, si = false, dp = 1) {
            const thresh = si ? 1000 : 1024;

            if (Math.abs(bytes) < thresh) {
                return bytes + ' B';
            }

            const units = si
                ? ['kB', 'MB', 'GB', 'TB', 'PB', 'EB', 'ZB', 'YB']
                : ['KiB', 'MiB', 'GiB', 'TiB', 'PiB', 'EiB', 'ZiB', 'YiB'];
            let u = -1;
            const r = 10 ** dp;

            do {
                bytes /= thresh;
                ++u;
            } while (Math.round(Math.abs(bytes) * r) / r >= thresh && u < units.length - 1);

            return bytes.toFixed(dp) + ' ' + units[u];
        }

        function humanStake(stake) {
            let stake_in_near = parseInt(stake / 10**24);
            return stake_in_near.toLocaleString('en-US')
        }

        function toggle_tbody(element) {
            let x = document.getElementsByClassName(element)[0];
            if (x.style.display === "none") {
                x.style.display = "table-row-group";
            } else {
                x.style.display = "none";
            }
        }

        function process_validator_info(validator_info) {
            let total_stake = 0;
            validator_info.current_validators.forEach(validator => {
                total_stake += parseInt(validator.stake);
            })
            $('.js-tbody-curepoch').append("Total stake " + humanStake(total_stake))
            validator_info.current_validators.forEach(validator => {
                let row = $('<tr>');
                row.append($('<td>').append(validator.account_id));
                row.append($('<td>').append(humanStake(validator.stake) + "/" + (100 * validator.stake / total_stake).toFixed(2) + "%"));
                row.append($('<td>').append(validator.shards.join(",")));
                row.append($('<td>').append(validator.num_expected_blocks - validator.num_produced_blocks));
                row.append($('<td>').append(validator.num_expected_blocks));
                let missed_blocks_perc = 100 * (validator.num_expected_blocks - validator.num_produced_blocks) / validator.num_expected_blocks;
                if (missed_blocks_perc >= 20) {
                    row.append($('<td>').append("<strong>" + missed_blocks_perc.toFixed(2) + "%</strong>"));
                } else {
                    row.append($('<td>').append(missed_blocks_perc.toFixed(2) + "%"));
                }
                row.append($('<td>').append(validator.num_expected_chunks - validator.num_produced_chunks));
                row.append($('<td>').append(validator.num_expected_chunks));
                let missed_chunks_perc = 100 * (validator.num_expected_chunks - validator.num_produced_chunks) / validator.num_expected_chunks;
                if (missed_chunks_perc >= 20) {
                    row.append($('<td>').append("<strong>" + missed_chunks_perc.toFixed(2) + "%</strong>"));
                } else {
                    row.append($('<td>').append(missed_chunks_perc.toFixed(2) + "%"));
                }
                if (missed_blocks_perc >= 20 || missed_chunks_perc >= 20) {
                    row.css('background-color', 'red');
                }
                $('.js-tbody-curepoch-curvalidators').append(row)
            })

            validator_info.next_validators.forEach(validator => {
                let row = $('<tr>');
                row.append($('<td>').append(validator.account_id));
                row.append($('<td>').append(humanStake(validator.stake) + "/" + (100 * validator.stake / total_stake).toFixed(2) + "%"));
                row.append($('<td>').append(validator.shards.join(",")));
                $('.js-tbody-curepoch-nextvalidators').append(row)
            })

            validator_info.current_proposals.forEach(proposal=> {
                let row = $('<tr>');
                row.append($('<td>').append(proposal.account_id));
                row.append($('<td>').append(humanStake(proposal.stake) + "/" + (100*proposal.stake/total_stake).toFixed(2) + "%"));
                $('.js-tbody-curepoch-proposals').append(row);
            })

            validator_info.prev_epoch_kickout.forEach(kickout=> {
                let row = $('<tr>');
                row.append($('<td>').append(kickout.account_id));
                row.append($('<td>').append(JSON.stringify(kickout.reason)));
                $('.js-tbody-curepoch-kickouts').append(row);
            })
            return total_stake
        }

        function process_responses(data, epoch_data) {
            let validatorMap = new Map();
            let maxShards = 0;
            epoch_data.status_response.EpochInfo.forEach((epoch, index) => {
                {
                    let row = $('<tr>');
                    row.append($('<td>').append(epoch.epoch_id));
                    row.append($('<td>').append(epoch.height));
                    row.append($('<td>').append(epoch.protocol_version));
                    if (epoch.first_block === null) {
                        if (index == 0) {
                            let blocks_remaining = (epoch.height - data.sync_info.latest_block_height);
                            let milliseconds_remaining = blocks_remaining * data.detailed_debug_status.block_production_delay_millis;
                            row.append($('<td>').append("Next epoch - in " + blocks_remaining + " blocks "));
                            $('.js-next-epoch').append("Next epoch in " + convertTime(milliseconds_remaining));
                        } else {
                            row.append($('<td>'));
                        }
                        row.append($('<td>'));
                    } else {
                        row.append($('<td>').append(epoch.first_block[0]));
                        row.append($('<td>').append(convertTime(Date.now() - Date.parse(epoch.first_block[1])) + " ago"));
                    }
                    row.append($('<td>').append(epoch.block_producers.length));
                    row.append($('<td>').append(epoch.chunk_only_producers.length));
                    $('.js-tbody-epochs').append(row);
                    if (index == 1) {
                        $('.js-thead-curepoch').append("Current Epoch " + epoch.epoch_id);
                        process_validator_info(epoch.validator_info);
                    }
                }

                $('.js-head-validators').append($('<th>').append(epoch.epoch_id.substr(0, 10)));
                // Use 2 bits to encode the validator's role in a given epoch.
                // 00 -- not participated
                // 01 -- block producer
                // 10 -- chunk only producer
                epoch.block_producers.forEach(validator => {
                    let account_id = validator.account_id;
                    let value = 2 ** (index * 2);
                    if (validatorMap.has(account_id)) {
                        validatorMap.set(account_id, validatorMap.get(account_id) + value);
                    } else {
                        validatorMap.set(account_id, value);
                    }
                });
                epoch.chunk_only_producers.forEach(account_id => {
                    let value = 2 ** (index * 2 + 1);
                    if (validatorMap.has(account_id)) {
                        validatorMap.set(account_id, validatorMap.get(account_id) + value);
                    } else {
                        validatorMap.set(account_id, value);
                    }
                });

                {
                    let row = $('<tr>');
                    row.append($('<td>').append(epoch.epoch_id));

                    epoch.shards_size_and_parts.forEach(element => {
                        let cell = $('<td>').append(humanFileSize(element[0]));
                        if (element[2] == true) {
                            cell.css('background-color', 'orange');
                        }
                        row.append(cell);
                        row.append($('<td>').append(element[1]));
                    });
                    $('.js-tbody-shard-sizes').append(row);
                }
                maxShards = Math.max(epoch.shards_size_and_parts.length, maxShards);
            });

            validatorMap.forEach((value, key) => {
                let row = $('<tr>').append($('<td>').append(key));
                for (i = 0; i < epoch_data.status_response.EpochInfo.length; i += 1) {
                    let cell = $('<td>');
                    if (value & 2 ** (i * 2)) {
                        cell.append('B');
                    } else if (value & 2 ** (i * 2 + 1)) {
                        cell.append('C');
                    } else {
                        cell.append('0');
                    }
                    row.append(cell);
                }
                $('.js-tbody-validators').append(row);
            });
            for (const x of Array(maxShards).keys()) {
                $('.js-head-shard-sizes').append($('<th colspan="2">').append("Shard " + x));
                $('.js-head-shard-sizes-line2').append($('<th>').append("Size"));
                $('.js-head-shard-sizes-line2').append($('<th>').append("Parts"));

            }
        }

        function request_epoch_info(status_data) {
            $.ajax({
                type: "GET",
                url: "../api/epoch_info",
                success: epoch_data => {
                    process_responses(status_data, epoch_data);
                },
                dataType: "json",
                error: function (errMsg, textStatus, errorThrown) {
                    alert("Failed: " + textStatus + " :" + errorThrown);
                },
                contentType: "application/json; charset=utf-8",
            })
        }

        $(document).ready(() => {
            $.ajax({
                type: "GET",
                url: "../api/status",
                success: data => {
                    request_epoch_info(data);
                },
                dataType: "json",
                error: function (errMsg, textStatus, errorThrown) {
                    alert("Failed: " + textStatus + " :" + errorThrown);
                },
                contentType: "application/json; charset=utf-8",
            })
        });
    </script>
</head>

<body>

    <h2>Epochs </h2>
    <h3><span class="js-next-epoch"></span></h3>

    <table>
        <thead>
            <tr>
                <th>Epoch id</th>
                <th>Start height</th>
                <th>Protocol version</th>
                <th>First block</th>
                <th>Epoch start</th>
                <th>Block producers</th>
                <th>Chunk only producers</th>
            </tr>
        </thead>
        <tbody class="js-tbody-epochs">
        </tbody>
    </table>

    <div> Click table headers to toggle.</div>

    <h2> <span class="js-thead-curepoch"></span></h2>
    <h3> <span class="js-tbody-curepoch"></span></h3>
    <h3>Validators</h3>
    <div> Validators that missed more than 20% blocks or chunks are marked red.</div>
    <table>
        <thead onclick="toggle_tbody('js-tbody-curepoch-curvalidators')">
        <tr>
            <th>Account id</th>
            <th>Stake / Stake %</th>
            <th>Shards</th>
            <th># Missed blocks</th>
            <th># Expected blocks</th>
            <th>Missed blocks %</th>
            <th># Missed chunks</th>
            <th># Expected chunks</th>
            <th>Missed chunks %</th>
        </tr>
        </thead>
        <tbody class ="js-tbody-curepoch-curvalidators">
        </tbody>
    </table>

    <h3>Validators for next epoch</h3>
    <table>
        <thead onclick="toggle_tbody('js-tbody-curepoch-nextvalidators')">
        <tr>
            <th>AccountId</th>
            <th>Stake/Stake Percentage</th>
            <th>Shards</th>
        </tr>
        </thead>
        <tbody class = "js-tbody-curepoch-nextvalidators">
        </tbody>
    </table>

    <h3>Proposals</h3>
    <table>
        <thead onclick="toggle_tbody('js-tbody-curepoch-proposals')">
        <tr>
            <th>AccountId</th>
            <th>Stake/Stake Percentage</th>
        </tr>
        </thead>
        <tbody class = "js-tbody-curepoch-proposals">
        </tbody>
    </table>

    <h3>Previous Epoch Kickouts</h3>
    <table>
        <thead onclick="toggle_tbody('js-tbody-curepoch-kickouts')">
        <tr>
            <th>AccountId</th>
            <th>Kickout Reason</th>
        </tr>
        </thead>
        <tbody class = "js-tbody-curepoch-kickouts">
        </tbody>
    </table>

    <h2> Validators </h2>

    <table>
        <thead onclick="toggle_tbody('js-tbody-validators')">
            <tr class="js-head-validators">
                <th>Account ID</th>
            </tr>
        </thead>
        <tbody class="js-tbody-validators">
        </tbody>
    </table>

    <h2> Shard sizes </h2>

    <table>
        <thead onclick="toggle_tbody('js-tbody-shard-sizes')">
            <tr class="js-head-shard-sizes">
                <th rowspan="2">Epoch Id</th>
            </tr>
            <tr class="js-head-shard-sizes-line2">

            </tr>
        </thead>
        <tbody class="js-tbody-shard-sizes">
        </tbody>
    </table>
    Orange color means, that a given shard was requested by other peer for syncing.
</body>

</html>

'''
'''--- chain/jsonrpc/res/last_blocks.html ---
<html>

<head>
    <link rel="stylesheet" href="last_blocks.css">
    <script src="https://unpkg.com/@babel/standalone/babel.min.js"></script>
    <script src="https://unpkg.com/react@18.2.0/umd/react.development.js"></script>
    <script src="https://unpkg.com/react-dom@18.2.0/umd/react-dom.development.js"></script>
    <script src="https://unpkg.com/lodash@4.17.21/lodash.min.js"></script>
    <script src="https://unpkg.com/prop-types@15.8.1/prop-types.min.js"></script>
    <script>
        // react-xarrows expects dependencies to be named differently... so let's do that.
        window.react = React;
        window['prop-types'] = PropTypes;
        window.lodash = _;
    </script>
    <script src="https://unpkg.com/react-xarrows@2.0.2/lib/index.js"></script>
</head>

<body>
    <div id="react-container"></div>

    <script type="text/jsx" src="last_blocks.js"></script>
</body>

</html>
'''
'''--- chain/jsonrpc/res/last_blocks.js ---
function ellipsify(str, maxLen) {
    if (str.length > maxLen) {
        return str.substring(0, maxLen - 3) + '...';
    }
    return str;
}

// Makes an element that when clicked, expands or ellipsifies the hash and creator.
function HashElement({ hashValue, creator, expandAll, knownProducers }) {
    let [expanded, setExpanded] = React.useState(false);
    let updateXarrow = reactXarrow.useXarrow();
    return <span
        className={`hash-element ${knownProducers.has(creator) ? '' : 'validator-unavailable'}`}
        onClick={() => {
            setExpanded((value) => !value);
            // xarrows need to be updated whenever graph dot positions may change.
            updateXarrow();
        }}>
        {expanded || expandAll
            ? `${hashValue} ${creator}`
            : `${ellipsify(hashValue, 8)} ${ellipsify(creator, 13)}`}
    </span>;
}

// Sorts the API response into easily displayable rows, and computes the graph layout.
//
// Inputs:
//   blocks: array of DebugBlockStatus
//   missedHeights: array of MissedHeightInfo
//   head: block hash of the chain's head
//   headerHead: block hash of the chain's header head
// Output: array of elements where each element is either {
//   block: DebugBlockStatus,
//   parentIndex: number?,  // the index of the parent block, or null if parent not included in the data
//   graphColumn: number,  // the column to display the graph node in
//   blockDelay: number?,  // number of seconds since parent's block timestamp, or null if parent not included in the data
//   chunkSkipped: boolean[],  // for each chunk, whether the chunk is the same as that chunk of parent block
//   isHead: boolean,
//   isHeaderHead: boolean,
// } or { missedHeight: MissedHeightInfo }
function sortBlocksAndDetermineBlockGraphLayout(blocks, missedHeights, head, headerHead) {
    const rows = [];
    for (let block of blocks) {
        rows.push({
            block,
            parentIndex: null,
            graphColumn: -1,
            blockDelay: null,
            chunkSkipped: block.chunks.map(() => false),
            isHead: head == block.block_hash,
            isHeaderHead: headerHead == block.block_hash,
        });
    }
    for (let missedHeight of missedHeights) {
        rows.push({ missedHeight });
    }

    function sortingKey(row) {
        if ('block' in row) {
            // some lousy tie-breaking for same-height rows.
            return row.block.block_height + (row.block.block_timestamp / 1e12 % 1);
        } else {
            return row.missedHeight.block_height;
        }
    }

    rows.sort((a, b) => sortingKey(b) - sortingKey(a));

    const rowIndexByHash = new Map();
    rows.forEach((row, rowIndex) => {
        if ('block' in row) {
            rowIndexByHash.set(row.block.block_hash, rowIndex);
        }
    });

    let highestNodeOnFirstColumn = rows.length;
    for (let i = rows.length - 1; i >= 0; i--) {
        let row = rows[i];
        if ('missedHeight' in row) {
            continue;
        }
        const block = row.block;

        // Look up parent index, and also compute things that depend on the parent block.
        if (rowIndexByHash.has(block.prev_block_hash)) {
            row.parentIndex = rowIndexByHash.get(block.prev_block_hash);
            const parent = rows[row.parentIndex];
            row.blockDelay = (block.block_timestamp - parent.block.block_timestamp) / 1e9;
            for (let j = 0;
                j < Math.min(block.chunks.length, parent.block.chunks.length);
                j++) {
                row.chunkSkipped[j] =
                    block.chunks[j].chunk_hash == parent.block.chunks[j].chunk_hash;
            }
        }
        // We'll use a two-column layout for the block graph. We traverse from bottom
        // up (oldest block to latest), and for each row we pick the first column unless
        // that would make us draw a line (from the parent to this node) through another
        // node; in which case we would pick the second column. To do that we just need
        // to keep track of the highest node we've seen so far for the first column.
        //
        // Not the best layout for a graph, but it's sufficient since we rarely have forks.
        let column = 0;
        if (row.parentIndex != null &&
            rows[row.parentIndex].graphColumn == 0 &&
            row.parentIndex > highestNodeOnFirstColumn) {
            column = 1;
        } else {
            highestNodeOnFirstColumn = i;
        }
        row.graphColumn = column;
    }
    return rows;
}

function BlocksTable({ rows, knownProducers, expandAll, hideMissingHeights }) {
    let numGraphColumns = 1;  // either 1 or 2; determines the width of leftmost td
    let numShards = 0;
    for (let row of rows) {
        if ('block' in row) {
            numGraphColumns = Math.max(numGraphColumns, row.graphColumn + 1);
            for (let chunk of row.block.chunks) {
                numShards = Math.max(numShards, chunk.shard_id + 1);
            }
        }
    }
    const header = <tr>
        <th>Chain</th>
        <th>Height</th>
        <th>{'Hash & creator'}</th>
        <th>Processing Time (ms)</th>
        <th>Block Delay (s)</th>
        <th>Gas price ratio</th>
        {[...Array(numShards).keys()].map(i =>
            <th key={i} colSpan="3">Shard {i} (hash/gas(Tgas)/time(ms))</th>)}
    </tr>;

    // One xarrow element per arrow (from block to block).
    const graphArrows = [];

    // One 'tr' element per row.
    const tableRows = [];
    for (let i = 0; i < rows.length; i++) {
        const row = rows[i];
        if ('missedHeight' in row) {
            if (!hideMissingHeights) {
                tableRows.push(<tr key={row.missedHeight.block_height} className="missed-height">
                    <td className="graph-node-cell" />
                    <td>{row.missedHeight.block_height}</td>
                    <td colSpan={4 + numShards * 3}>{row.missedHeight.block_producer} missed block</td>
                </tr>);
            }
            continue;
        }
        let block = row.block;

        const chunkCells = [];
        block.chunks.forEach((chunk, shardId) => {
            chunkCells.push(<React.Fragment key={shardId}>
                <td className={row.chunkSkipped[shardId] ? 'skipped-chunk' : ''}>
                    <HashElement
                        hashValue={chunk.chunk_hash}
                        creator={chunk.chunk_producer}
                        expandAll={expandAll}
                        knownProducers={knownProducers} />
                </td>
                <td>{(chunk.gas_used / (1024 * 1024 * 1024 * 1024)).toFixed(1)}</td>
                <td>{chunk.processing_time_ms}</td>
            </React.Fragment>);
        });

        tableRows.push(
            <tr key={block.block_hash}
                className={`block-row ${row.block.is_on_canonical_chain ? '' : 'not-on-canonical-chain'}`}>
                <td className="graph-node-cell">
                    <div id={`graph-node-${i}`}
                        className={`graph-dot graph-dot-col-${row.graphColumn} graph-dot-total-${numGraphColumns}`}>
                    </div>
                </td>
                <td>
                    <span>{block.block_height}</span>
                    {row.isHead && <div className="head-label">HEAD</div>}
                    {row.isHeaderHead && <div className="header-head-label">HEADER HEAD</div>}
                </td>
                <td>
                    <HashElement
                        hashValue={block.block_hash}
                        creator={block.block_producer}
                        expandAll={expandAll}
                        knownProducers={knownProducers} />
                </td>
                <td>{block.processing_time_ms}</td>
                <td>{row.blockDelay ?? ''}</td>
                <td>{block.gas_price_ratio}</td>
                {block.full_block_missing && <td colSpan={numShards * 3}>header only</td>}
                {chunkCells}
            </tr>);
        if (row.parentIndex != null) {
            graphArrows.push(<reactXarrow.default
                key={i}
                start={`graph-node-${i}`}
                end={`graph-node-${row.parentIndex}`}
                color={row.block.is_on_canonical_chain ? 'black' : 'darkgray'}
                strokeWidth={row.block.is_on_canonical_chain ? 3 : 1}
                headSize="0"
                path="straight" />);
        }
    }
    return <div>
        {graphArrows}
        <table>
            <tbody>
                {header}
                {tableRows}
            </tbody>
        </table>
    </div>
}

function Page() {
    const [rows, setRows] = React.useState([]);
    const [error, setError] = React.useState(null);
    const [knownProducers, setKnownProducers] = React.useState(new Set());
    const [expandAll, setExpandAll] = React.useState(false);
    const [hideMissingHeights, setHideMissingHeights] = React.useState(false);
    const updateXarrow = reactXarrow.useXarrow();
    let blockStatusApiPath = '../api/block_status';
    const url = new URL(window.location.toString());
    let title = 'Most Recent Blocks';
    if (url.searchParams.has('height')) {
        blockStatusApiPath += '/' + url.searchParams.get('height');
        title = 'Blocks from ' + url.searchParams.get('height');
    }
    // useEffect with empty dependency list means to run this once at beginning.
    React.useEffect(() => {
        (async () => {
            try {
                let resp = await fetch('../api/status');
                if (resp.status == 405) {
                    throw new Error('Debug not allowed - did you set enable_debug_rpc: true in your config?');
                } else if (!resp.ok) {
                    throw new Error('Debug API call failed: ' + resp.statusText);
                }
                const { detailed_debug_status: { network_info: { known_producers } } } = await resp.json();
                const knownProducerSet = new Set();
                for (const producer of known_producers) {
                    knownProducerSet.add(producer.account_id);
                }
                setKnownProducers(knownProducerSet);

                resp = await fetch(blockStatusApiPath);
                if (!resp.ok) {
                    throw new Error('Could not fetch block debug status: ' + resp.statusText);
                }
                const { status_response: { BlockStatus: data } } = await resp.json();
                setRows(sortBlocksAndDetermineBlockGraphLayout(
                    data.blocks,
                    data.missed_heights,
                    data.head,
                    data.header_head));
            } catch (error) {
                setError(error);
            }
        })();
    }, []);

    // Compute missing blocks and chunks statistics (whenever rows changes).
    const { numCanonicalBlocks, canonicalHeightCount, numChunksSkipped } = React.useMemo(() => {
        let firstCanonicalHeight = 0;
        let lastCanonicalHeight = 0;
        let numCanonicalBlocks = 0;
        const numChunksSkipped = [];
        for (const row of rows) {
            if (!('block' in row)) {
                continue;
            }
            const block = row.block;
            if (!block.is_on_canonical_chain) {
                continue;
            }
            if (firstCanonicalHeight == 0) {
                firstCanonicalHeight = block.block_height;
            }
            lastCanonicalHeight = block.block_height;
            numCanonicalBlocks++;
            for (let i = 0; i < row.chunkSkipped.length; i++) {
                while (numChunksSkipped.length < i + 1) {
                    numChunksSkipped.push(0);
                }
                if (row.chunkSkipped[i]) {
                    numChunksSkipped[i]++;
                }
            }
        }
        return {
            numCanonicalBlocks,
            canonicalHeightCount: firstCanonicalHeight - lastCanonicalHeight + 1,
            numChunksSkipped,
        };
    }, [rows]);

    return <reactXarrow.Xwrapper>
        <h1>{title}</h1>
        <div className="explanation">Skipped chunks have grey background.</div>
        <div className="explanation">
            Red text means that we don't know this producer
            (it's not present in our announce account list).
        </div>
        {error && <div className="error">{error.stack}</div>}
        <div className="missed-blocks">
            Missing blocks: {canonicalHeightCount - numCanonicalBlocks} { }
            Produced: {numCanonicalBlocks} { }
            Missing Rate: {((canonicalHeightCount - numCanonicalBlocks) / canonicalHeightCount * 100).toFixed(2)}%
        </div>
        <div className="missed-chunks">
            {numChunksSkipped.map((numSkipped, shardId) =>
                <div key={shardId}>
                    Shard {shardId}: Missing chunks: {numSkipped} { }
                    Produced: {numCanonicalBlocks - numSkipped} { }
                    Missing Rate: {(numSkipped / numCanonicalBlocks * 100).toFixed(2)}%
                </div>)}
        </div>
        <button onClick={() => {
            setExpandAll(value => !value);
            updateXarrow();
        }}>
            {expandAll ? "Don't expand all" : 'Expand all'}
        </button>
        <button onClick={() => {
            setHideMissingHeights(value => !value);
            updateXarrow();
        }}>
            {hideMissingHeights ? 'Show missing heights' : 'Hide missing heights'}
        </button>
        <BlocksTable
            rows={rows}
            knownProducers={knownProducers}
            expandAll={expandAll}
            hideMissingHeights={hideMissingHeights} />

    </reactXarrow.Xwrapper>;
}

ReactDOM
    .createRoot(document.getElementById('react-container'))
    .render(<Page />);

'''
'''--- chain/jsonrpc/res/network_info.html ---
<html>

<head>
    <link rel="stylesheet" href="network_info.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="network_info.js"></script>
    <script>
        function fetchProducers(epoch_id, producers_callback) {
            $.ajax({
                type: "GET",
                url: "../api/epoch_info",
                success: data => {
                    let epoch_found = false;
                    data.status_response.EpochInfo.forEach(element => {
                        if (element.epoch_id == epoch_id) {
                            epoch_found = true;
                            producers_callback(element.block_producers, element.chunk_only_producers);
                        }
                    });
                    // This can happen if we're in sync mode - in such case, still print the list of peers,
                    // but don't show producers.
                    if (epoch_found == false) {
                        producers_callback([], []);
                    }
                },
                dataType: "json",
                error: function (errMsg, textStatus, errorThrown) {
                    alert("Failed: " + textStatus + " :" + errorThrown);
                },
                contentType: "application/json; charset=utf-8",
            })
        }

        $(document).ready(() => {
            $('.detailed-peer-storage-div').hide();
            $('.recent-outbound-connections-div').hide();
            $('span').text("Loading...");
            $.ajax({
                type: "GET",
                url: "../api/status",
                success: data => {

                    fetchProducers(data.sync_info.epoch_id, (block_producers, chunk_producers) => {
                        let block_producer_set = new Set();
                        block_producers.forEach(element => {
                            block_producer_set.add(element.account_id);
                        });
                        let chunk_producer_set = new Set();
                        chunk_producers.forEach(element => {
                            chunk_producer_set.add(element);
                        });

                        let known_set = new Set();
                        data.detailed_debug_status.network_info.known_producers.forEach(element => {
                            known_set.add(element.account_id);
                        });

                        let reachable_set = new Set();
                        data.detailed_debug_status.network_info.known_producers.forEach(element => {
                            if (element.next_hops != null && element.next_hops.length > 0) {
                                reachable_set.add(element.account_id);
                            }
                        });

                        $('.js-num-block-producers').text(block_producers.length);
                        $('.js-num-known-block-producers').text(getIntersection(block_producer_set, known_set).size);
                        $('.js-num-reachable-block-producers').text(getIntersection(block_producer_set, reachable_set).size);
                        let uknown_set = getDifference(block_producer_set, known_set);
                        let known_but_unreachable = getIntersection(block_producer_set, getDifference(known_set, reachable_set));
                        $('.js-unknown-block-producers').text(Array.from(uknown_set).join(","));
                        $('.js-unreachable-block-producers').text(Array.from(known_but_unreachable).join(","));

                        $('.js-num-chunk-producers').text(chunk_producers.length);
                        $('.js-num-known-chunk-producers').text(getIntersection(chunk_producer_set, known_set).size);
                        $('.js-num-reachable-chunk-producers').text(getIntersection(chunk_producer_set, reachable_set).size);
                        let chunk_uknown_set = getDifference(chunk_producer_set, known_set);
                        let chunk_known_but_unreachable = getIntersection(chunk_producer_set, getDifference(known_set, reachable_set));
                        $('.js-unknown-chunk-producers').text(Array.from(chunk_uknown_set).join(","));
                        $('.js-unreachable-chunk-producers').text(Array.from(chunk_known_but_unreachable).join(","));

                        let node_public_key = data.node_public_key;
                        let sync_status = data.detailed_debug_status.sync_status;
                        let network_info = data.detailed_debug_status.network_info;
                        $('.js-node-public-key').text(node_public_key);
                        $('.js-sync-status').text(sync_status);
                        $('.js-max-peers').text(network_info.peer_max_count);
                        $('.js-num-peers').text(network_info.num_connected_peers);
                        let current_height = data.sync_info.latest_block_height;
                        let peer_status_map = new Map();
                        network_info.connected_peers.forEach(function (peer, index) {
                            let peer_id = peer.peer_id;
                            let validator = new Array();
                            let routedValidator = new Array();
                            data.detailed_debug_status.network_info.known_producers.forEach(element => {
                                if (block_producer_set.has(element.account_id) || chunk_producer_set.has(element.account_id)) {
                                    if (element.peer_id == peer_id) {
                                        // This means that the peer that we're connected to is a validator.
                                        validator.push(element.account_id);
                                    } else {
                                        if (element.next_hops != null) {
                                            if (element.next_hops.includes(peer_id)) {
                                                // This means that the peer that we're connected to is on the shortest path
                                                // to this validator.
                                                routedValidator.push(element.account_id);
                                            }
                                        }
                                    }
                                }
                            });
                            let peer_class = peerClass(current_height, peer.height)
                            if (peer_status_map.has(peer_class)) {
                                peer_status_map.set(peer_class, peer_status_map.get(peer_class) + 1);
                            } else {
                                peer_status_map.set(peer_class, 1);
                            }

                            let last_ping_class = ""
                            if (peer.last_time_received_message_millis > 60 * 1000) {
                                last_ping_class = "peer_far_behind";
                            }

                            let row = $('.js-tbody-peers').append($('<tr>')
                                .append($('<td>').append(add_debug_port_link(peer.addr)))
                                .append($('<td>').append(validator.join(",")))
                                .append($('<td>').append(peer.peer_id.substr(8, 5) + "..."))
                                .append($('<td>').append(convertTime(peer.last_time_received_message_millis)).addClass(last_ping_class))
                                .append($('<td>').append(JSON.stringify(peer.height)).addClass(peer_class))
                                .append($('<td>').append(displayHash(peer)))
                                .append($('<td>').append(JSON.stringify(peer.tracked_shards)))
                                .append($('<td>').append(JSON.stringify(peer.archival)))
                                .append($('<td>').append(((peer.is_outbound_peer) ? 'OUT' : 'IN')))
                                // If this is a new style nonce - show the approx time since it was created.
                                .append($('<td>').append(peer.nonce + " <br> " + ((peer.nonce > 1660000000) ? convertTime(Date.now() - peer.nonce * 1000) : "old style nonce")))
                                .append($('<td>').append(convertTime(peer.connection_established_time_millis)))
                                .append($('<td>').append(computeTraffic(peer.received_bytes_per_sec, peer.sent_bytes_per_sec)))
                                .append($('<td>').append(routedValidator.join(",")))
                            )
                        });
                        let legend = [["peer_ahead_alot", "Peer ahead a lot"],
                        ["peer_ahead", "Peer ahead"],
                        ["peer_in_sync", "Peer in sync"],
                        ["peer_behind_a_little", "Peer behind a little"],
                        ["peer_behind", "Peer behind"],
                        ["peer_far_behind", "Peer far behind"]]
                        legend.forEach(function (elem) {
                            $('.legend').append($('<td>').addClass(elem[0]).text(elem[1] + " " + (peer_status_map.get(elem[0]) || 0)));
                        });
                    });
                },
                dataType: "json",
                error: function (errMsg, textStatus, errorThrown) {
                    alert("Failed: " + textStatus + " :" + errorThrown);
                },
                contentType: "application/json; charset=utf-8",
            })

        });

        function show_peer_storage() {
            $(".detailed-peer-storage-button").text("Loading...");
            $(".tbody-detailed-peer-storage").html("");
            $.ajax({
                type: "GET",
                url: "../api/peer_store",
                success: data => {
                    $(".detailed-peer-storage-size").text(data.status_response.PeerStore.peer_states.length);
                    data.status_response.PeerStore.peer_states.forEach(element => {
                        let row = $("<tr>");
                        row.append($("<td>").append(element['peer_id']));
                        row.append($("<td>").append(element['addr']));

                        let first_seen =
                            row.append($("<td>").append(to_human_time(Math.floor(Date.now() / 1000) - element['first_seen'])));
                        row.append($("<td>").append(to_human_time(Math.floor(Date.now() / 1000) - element['last_seen'])));
                        if (element['last_attempt'] != null) {
                            row.append($("<td>").append(to_human_time(Math.floor(Date.now() / 1000) - element['last_attempt'][0])));
                            row.append($("<td>").append(element['status'] + " Last attempt: " + element['last_attempt'][1]));
                        } else {
                            row.append($("<td>"));
                            row.append($("<td>").append(element['status']));
                        }

                        $(".tbody-detailed-peer-storage").append(row);
                    });

                    $(".detailed-peer-storage-div").show();
                    $(".detailed-peer-storage-button").text("Refresh peer storage");
                }
            });
        }

        function show_recent_outbound_connections() {
            $(".recent-outbound-connections-button").text("Loading...");
            $(".tbody-recent-outbound-connections").html("");
            $.ajax({
                type: "GET",
                url: "../api/recent_outbound_connections",
                success: data => {
                    $(".recent-outbound-connections-size").text(data.status_response.RecentOutboundConnections.recent_outbound_connections.length);
                    data.status_response.RecentOutboundConnections.recent_outbound_connections.forEach(element => {
                        let row = $("<tr>");
                        row.append($("<td>").append(element['peer_id']));
                        row.append($("<td>").append(element['addr']));
                        row.append($("<td>").append(to_human_time(Math.floor(Date.now() / 1000) - element['time_established'])));
                        row.append($("<td>").append(to_human_time(Math.floor(Date.now() / 1000) - element['time_connected_until'])));

                        $(".tbody-recent-outbound-connections").append(row);
                    });

                    $(".recent-outbound-connections-div").show();
                    $(".recent-outbound-connections-button").text("Refresh connection storage");
                }
            });
        }
    </script>
</head>

<body>
    <h1>
        Welcome to the Network Info page!
    </h1>
    <h2>
        <p>
            PeerId:
            <span class="js-node-public-key"></span>
        </p>
        <p>
            Current Sync Status:
            <span class="js-sync-status"></span>
        </p>

        <p>
            Number of peers: <span class="js-num-peers"></span>/<span class="js-max-peers"></span>
        </p>
        <p>
            Block Producers: <span class="js-num-block-producers"></span> Known: <span
                class="js-num-known-block-producers"></span>
            Reachable: <span class="js-num-reachable-block-producers"></span>
        </p>
    </h2>

    <pre>
Unknown: <span class="js-unknown-block-producers"></span>
Unreachable: <span class="js-unreachable-block-producers"></span>
    </pre>
    <h2>
        <p>
            Chunk Producers: <span class="js-num-chunk-producers"></span> Known: <span
                class="js-num-known-chunk-producers"></span>
            Reachable: <span class="js-num-reachable-chunk-producers"></span>
        </p>
    </h2>
    <pre>
Unknown: <span class="js-unknown-chunk-producers"></span>
Unreachable: <span class="js-unreachable-chunk-producers"></span>
    </pre>

    <b>Unknown</b> means that we didn't receive 'announce' information about this validator (so we don't know on which
    peer it
    is). This usually means that the validator didn't connect to the network
    during current epoch.

    <br>

    <b>Unreachable</b> means, that we know the peer_id of this validator, but we cannot find it in our routing table.
    This
    usually means that validator did connect
    to the network in the past, but now it is gone for at least 1 hour.
    <br>
    <br>

    <table class="legend">
    </table>

    <table>
        <thead>
            <tr>
                <th>Address</th>
                <th>Validator?</th>
                <th>PeerId</th>
                <th>Last ping</th>
                <th>Height</th>
                <th>Last Block Hash</th>
                <th>Tracked Shards</th>
                <th>Archival</th>
                <th>Connection type</th>
                <th>Nonce</th>
                <th>First connection</th>
                <th>Traffic (last minute)</th>
                <th>Route to validators</th>
            </tr>
        </thead>
        <tbody class="js-tbody-peers">
        </tbody>
    </table>
    <br>
    <button onclick="show_peer_storage()" class="detailed-peer-storage-button">
        Show detailed peer storage
    </button>
    <div class="detailed-peer-storage-div">
        <h2>Peers in storage: <span class="detailed-peer-storage-size"></span></h2>
        <table class="detailed-peer-storage">
            <thead>
                <th>Peer id</th>
                <th>Peer address</th>
                <th>First seen</th>
                <th>Last seen</th>
                <th>Last connection attempt</th>
                <th>Status</th>
            </thead>
            <tbody class="tbody-detailed-peer-storage">

            </tbody>
        </table>
        <br>
    </div>
    <button onclick="show_recent_outbound_connections()" class="recent-outbound-connections-button">
        Show outbound connections in storage
    </button>
    <div class="recent-outbound-connections-div">
        <h2>Outbound connections in storage: <span class="recent-outbound-connections-size"></span></h2>
        <table class="recent-outbound-connections">
            <thead>
                <th>Peer id</th>
                <th>Peer address</th>
                <th>First connected</th>
                <th>Last connected</th>
            </thead>
            <tbody class="tbody-recent-outbound-connections">

            </tbody>
        </table>
    </div>
</body>

</html>

'''
'''--- chain/jsonrpc/res/network_info.js ---
function convertTime(millis) {
    if (millis == null) {
        return '(null)';
    }
    let total_seconds = Math.floor(millis / 1000);
    let hours = Math.floor(total_seconds / 3600)
    let minutes = Math.floor((total_seconds - (hours * 3600)) / 60)
    let seconds = total_seconds - (hours * 3600) - (minutes * 60)
    if (hours > 0) {
        if (minutes > 0) {
            return `${hours}h ${minutes}m ${seconds}s`
        } else {
            return `${hours}h ${seconds}s`
        }
    }
    if (minutes > 0) {
        return `${minutes}m ${seconds}s`
    }
    return `${seconds}s`
}

function convertBps(bytes_per_second) {
    if (bytes_per_second == null) {
        return '-'
    }
    if (bytes_per_second < 3000) {
        return `${bytes_per_second} bps`
    } []
    let kilobytes_per_second = bytes_per_second / 1024;
    if (kilobytes_per_second < 3000) {
        return `${kilobytes_per_second.toFixed(1)} Kbps`
    }
    let megabytes_per_second = kilobytes_per_second / 1024;
    return `${megabytes_per_second.toFixed(1)} Mbps`
}

function computeTraffic(bytes_received, bytes_sent) {
    return "⬇ " + convertBps(bytes_received) + "<br>⬆ " + convertBps(bytes_sent);
}

function add_debug_port_link(peer_network_addr) {
    // Each node running in a machine is assigned ports 24567 + peer_num and 3030 + peer_num, whereby peer_num is a whole number
    // peer_rpc_address is not shared between peer nodes. Hence, it cannot be programmatically fetched.
    // https://github.com/near/nearcore/blob/700ec29270f72f2e78a17029b4799a8228926c07/chain/network/src/network_protocol/peer.rs#L13-L19
    DEFAULT_RPC_PORT = 3030
    DEFAULT_NETWORK_PORT = 24567
    peer_network_addr_array = peer_network_addr.split(":")
    peer_network_port = peer_network_addr_array.pop()
    peer_network_ip = peer_network_addr_array.pop()
    peer_num = 0;
    if (peer_network_ip.includes("127.0.0.1")) {
        peer_num = peer_network_port - DEFAULT_NETWORK_PORT;
    }
    peer_rpc_port = DEFAULT_RPC_PORT + peer_num;
    peer_rpc_address = "http://" + peer_network_addr.replace(/:.*/, ":") + peer_rpc_port + "/debug"
    return $('<a>', {
        href: peer_rpc_address,
        text: peer_network_addr
    });
}

function displayHash(peer) {
    if (peer.is_highest_block_invalid) {
        return peer.block_hash + " (INVALID)"
    } else {
        return peer.block_hash + " (Valid)"
    }
}

function peerClass(current_height, peer_height) {
    if (peer_height > current_height + 5) {
        return 'peer_ahead_alot';
    }
    if (peer_height > current_height + 2) {
        return 'peer_ahead';
    }

    if (peer_height < current_height - 100) {
        return 'peer_far_behind';
    }
    if (peer_height < current_height - 10) {
        return 'peer_behind';
    }
    if (peer_height < current_height - 3) {
        return 'peer_behind_a_little';
    }
    return 'peer_in_sync';
}

function getIntersection(setA, setB) {
    const intersection = new Set(
        [...setA].filter(element => setB.has(element))
    );

    return intersection;
}

function getDifference(setA, setB) {
    return new Set(
        [...setA].filter(element => !setB.has(element))
    );
}

function to_human_time(seconds) {
    let result = "";
    if (seconds >= 60) {
        let minutes = Math.floor(seconds / 60);
        seconds = seconds % 60;
        if (minutes > 60) {
            let hours = Math.floor(minutes / 60);
            minutes = minutes % 60;
            if (hours > 24) {
                let days = Math.floor(hours / 24);
                hours = hours % 24;
                result += days + " days ";
            }
            result += hours + " h ";
        }
        result += minutes + " m ";
    }
    result += seconds + " s"
    return result;
}

'''
'''--- chain/jsonrpc/res/split_store.html ---
<html>

<head>
    <title> Split Store </title>
</head>

<body>
    <h1>
        Split Store
    </h1>

    <ul>
        <li> Head height: <span id="head-height"></span></li>
        <li> Cold head height: <span id="cold-head-height"></span></li>
        <li> Final head height: <span id="final-head-height"></span></li>
        <li> Hot db kind: <span id="hot-db-kind"></span></li>
    </ul>

    <script>
        document.body.onload = async () => {
            response = await fetch("../api/split_store_info")
            response_json = await response.json()
            info = response_json['status_response']['SplitStoreStatus']

            document.getElementById("head-height").textContent = String(info["head_height"])
            document.getElementById("cold-head-height").textContent = String(info["cold_head_height"])
            document.getElementById("final-head-height").textContent = String(info["final_head_height"])
            document.getElementById("hot-db-kind").textContent = String(info["hot_db_kind"])
        }
    </script>
</body>

</html>

'''
'''--- chain/jsonrpc/res/sync.html ---
<html>

<head>
    <link rel="stylesheet" href="sync.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script>
        function drawPixel(context, x, y, color) {
            // Math.round() used to decrease smoothing when numbers have decimal parts.
            var roundedX = Math.round(x);
            var roundedY = Math.round(y);
            context.fillStyle = color || '#000';
            context.fillRect(roundedX, roundedY, 300, 150);
        }

        function drawSlot(context, slot_id, slot_size, color) {
            let start_position = slot_id * slot_size;
            if (start_position > 300 * 150) {
                return;
            }
            let pos_y = Math.floor(start_position / 300);
            let pos_x = start_position % 300;

            let left = slot_size;
            context.fillStyle = color;
            while (left > 0) {
                let draw = Math.min(300 - pos_x, left);
                context.fillRect(pos_x, pos_y, draw, 1);
                pos_x = (pos_x + draw) % 300;
                pos_y += 1;
                left -= draw;
            }
        }

        function process_sync_status(data) {
            let sync_status = data.status_response.SyncStatus;
            $('.js-header-sync').text("Header sync - not started.")
            $('.js-state-sync').text("State sync - not started.")
            $('.js-block-sync').text("Block sync - not started.")

            if (typeof (sync_status) === 'string') {
                if (sync_status == 'NoSync') {
                    $('.js-header-sync').text("Header sync - ✅.");
                    $('.js-state-sync').text("State sync - ✅.");
                    $('.js-block-sync').text("Block sync - ✅.");
                }
            } else {
                if ('StateSync' in sync_status) {
                    $('.div-progress').show();
                    $('.js-header-sync').text("Header sync - ✅.")
                    let state_sync = sync_status.StateSync;
                    let sync_hash = state_sync[0];
                    $('.js-state-sync').text("State sync @" + sync_hash);
                    for (const [shard_id, shard_info] of Object.entries(state_sync[1])) {
                        let canvas = $('<canvas>');
                        // 45k
                        let canvas_slots = 300 * 150;
                        let slot_size = Math.max(1, Math.floor(canvas_slots / shard_info.downloads.length))

                        console.log(slot_size);

                        let parts_done = 0;

                        shard_info.downloads.forEach((element, index) => {
                            if (element.done == true) {
                                parts_done += 1;
                                drawSlot(canvas[0].getContext('2d'), index, slot_size, 'green');
                            } else if (element.error == true) {
                                drawSlot(canvas[0].getContext('2d'), index, slot_size, 'red');
                            } else {
                                drawSlot(canvas[0].getContext('2d'), index, slot_size, 'grey');
                            }
                        });

                        let progress_percent = 0;
                        if (shard_info.downloads.length > 0) {
                            progress_percent = parts_done / shard_info.downloads.length * 100;

                        }

                        $('.js-tbody-progress').append($('<tr>')
                            .append($('<td>').append(shard_id))
                            .append($('<td>').append(progress_percent.toFixed(1) + "% " + parts_done + " / " + shard_info.downloads.length))
                            .append($('<td>').append(shard_info.status))
                            .append($('<td>').append(canvas))
                        );
                    }
                }
                if ('HeaderSync' in sync_status) {
                    let from = sync_status.HeaderSync.current_height;
                    let to = sync_status.HeaderSync.highest_height;
                    $('.js-header-sync').text("Header sync - " + from + " -> " + to + ":    " + (to - from) + " remaining");
                }
                if ('BodySync' in sync_status) {
                    $('.js-header-sync').text("Header sync - ✅.");
                    $('.js-state-sync').text("State sync - ✅.");
                    let from = sync_status.BodySync.current_height;
                    let to = sync_status.BodySync.highest_height;
                    $('.js-block-sync').text("Block sync - " + from + " -> " + to + ":    " + (to - from) + " remaining");
                }
            }
        }

        function process_tracked_shards(data) {
            let tracked_shards = data.status_response.TrackedShards;
            let max_shards = Math.max(tracked_shards.shards_tracked_this_epoch.length, tracked_shards.shards_tracked_next_epoch.length);
            for (let shard = 0; shard < max_shards; shard += 1) {
                console.log(shard);
                $('.js-thead-tracked').append($('<th>').append("Shard " + shard));
            }

            let row = $('<tr>').append('<td>Current epoch</td>');
            for (let shard = 0; shard < tracked_shards.shards_tracked_this_epoch.length; shard += 1) {
                row.append($('<td>').append(tracked_shards.shards_tracked_this_epoch[shard]));
            }
            $('.js-tbody-tracked').append(row);

            row = $('<tr>').append('<td>Next epoch</td>');
            for (let shard = 0; shard < tracked_shards.shards_tracked_next_epoch.length; shard += 1) {
                row.append($('<td>').append(tracked_shards.shards_tracked_next_epoch[shard]));
            }
            $('.js-tbody-tracked').append(row);

        }

        function process_catchup_status(data) {
            let catchup_status = data.status_response.CatchupStatus;
            $('.catchup-body').text("");
            catchup_status.forEach((catchup) => {
                $('.catchup-body').append(catchup);
                console.log(catchup);
                $('.catchup-body').append("Epoch starting at block " + catchup.sync_block_hash + " " + catchup.sync_block_height + "<br>");
                Object.entries(catchup.shard_sync_status).forEach(([shard_id, shard_status]) => {
                    $('.catchup-body').append("Shard " + shard_id + " status: " + shard_status + "<br>");
                });
                $('.catchup-body').append("Blocks to catchup: <br>");
                catchup.blocks_to_catchup.forEach(block => {
                    $('.catchup-body').append("Block " + block.hash + " " + block.height + "<br>");
                });
            })
        }

        $(document).ready(() => {
            $('.div-progress').hide();
            $('span').text("Loading...");
            $.ajax({
                type: "GET",
                url: "../api/sync_status",
                success: data => {
                    process_sync_status(data);
                },
                dataType: "json",
                error: function (errMsg, textStatus, errorThrown) {
                    alert("Failed: " + textStatus + " :" + errorThrown);
                },
                contentType: "application/json; charset=utf-8",
            });
            $.ajax({
                type: "GET",
                url: "../api/tracked_shards",
                success: data => {
                    process_tracked_shards(data);
                },
                dataType: "json",
                error: function (errMsg, textStatus, errorThrown) {
                    alert("Failed: " + textStatus + " :" + errorThrown);
                },
                contentType: "application/json; charset=utf-8",
            });
            $.ajax({
                type: "GET",
                url: "../api/catchup_status",
                success: data => {
                    process_catchup_status(data);
                },
                dataType: "json",
                error: function (errMsg, textStatus, errorThrown) {
                    alert("Failed: " + textStatus + " :" + errorThrown);
                },
                contentType: "application/json; charset=utf-8",
            });
        });
    </script>
</head>

<body>
    <h1>
        Sync page
    </h1>
    <div class="div-tracked">
        <h2>
            <p>Tracked shards</p>
        </h2>
        <table>
            <thead>
                <tr class="js-thead-tracked">
                    <th>Epoch</th>
                </tr>
            </thead>
            <tbody class="js-tbody-tracked">
            </tbody>
        </table>
    </div>
    <h2>
        <p>
            <span class="js-header-sync"></span>
        </p>
        <p>
            <span class="js-state-sync"></span>
        </p>
        <p>
            <span class="js-block-sync"></span>
        </p>
    </h2>
    <div class="div-progress">
        <h2>
            <p>Progress</p>
        </h2>
        <table>
            <thead>
                <tr>
                    <th>Shard</th>
                    <th>Progress</th>
                    <th>Status</th>
                </tr>
            </thead>
            <tbody class="js-tbody-progress">
            </tbody>
        </table>
    </div>
    <h2>
        <p>Catchup</p>
    </h2>
    <h3>
        <span class="catchup-body"></span>
    </h3>

    Header sync is a fast process, where we fetch 512 'headers' at a time from the network (basically header consists of
    a few hashes).
    <br>
    State sync is downloading the current storage state - it can take couple GB for each shard that you're tracking.
    <br>
    Block sync is executing all the transactions in the a block (which might take up to 1 second/block) - can be faster
    if blocks are not full.

</body>

</html>

'''
'''--- chain/jsonrpc/res/tier1_network_info.html ---
<html>

<head>
    <link rel="stylesheet" href="network_info.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="network_info.js"></script>
    <script>
        $(document).ready(() => {
            $('.detailed-peer-storage-div').hide();
            $('span').text("Loading...");
            $.ajax({
                type: "GET",
                url: "../api/status",
                success: data => {
                    let rendered = new Set();
                    let network_info = data.detailed_debug_status.network_info;

                    // tier1_connections contains TIER1 nodes we are currently connected to
                    network_info.tier1_connections.forEach(function (peer) {
                        let account_key = "";
                        let proxies = new Array();

                        network_info.tier1_accounts_data.forEach(account => {
                            if (account.peer_id == peer.peer_id) {
                                account_key = account.account_key;
                                account.proxies.forEach(proxy => {
                                    proxies.push(proxy.peer_id.substr(8, 5) + "...@" + proxy.addr);
                                });
                            }
                        });

                        rendered.add(account_key);

                        let account_id = "";
                        network_info.known_producers.forEach(producer => {
                            if (producer.peer_id == peer.peer_id) {
                                account_id = producer.account_id;
                            }
                        });

                        let last_ping = convertTime(peer.last_time_received_message_millis)
                        let last_ping_class = ""
                        if (data.node_public_key == account_key) {
                            last_ping = "n/a"
                        } else if (peer.last_time_received_message_millis > 60 * 1000) {
                            last_ping_class = "peer_far_behind";
                        }

                        let row = $('.js-tbody-peers').append($('<tr>')
                            .append($('<td>').append(add_debug_port_link(peer.addr)))
                            .append($('<td>').append(account_key.substr(8, 5) + "..."))
                            .append($('<td>').append(account_id))
                            .append($('<td>').append("[" + proxies.join(",") + "]"))
                            .append($('<td>').append(peer.peer_id.substr(8, 5) + "..."))
                            .append($('<td>').append(last_ping).addClass(last_ping_class))
                            .append($('<td>').append(JSON.stringify(peer.tracked_shards)))
                            .append($('<td>').append(JSON.stringify(peer.archival)))
                            .append($('<td>').append(((peer.is_outbound_peer) ? 'OUT' : 'IN')))
                            .append($('<td>').append(convertTime(peer.connection_established_time_millis)))
                            .append($('<td>').append(computeTraffic(peer.received_bytes_per_sec, peer.sent_bytes_per_sec)))
                        )
                    });

                    // tier1_accounts_data contains data about TIER1 nodes we would like to connect to
                    network_info.tier1_accounts_data.forEach(account => {
                        let account_key = account.account_key;

                        if (rendered.has(account_key)) {
                            return;
                        }
                        rendered.add(account_key);

                        let account_id = "";
                        network_info.known_producers.forEach(producer => {
                            if (producer.peer_id == account.peer_id) {
                                account_id = producer.account_id;
                            }
                        });

                        let proxies = new Array();
                        account.proxies.forEach(proxy => {
                            proxies.push(proxy.peer_id.substr(8, 5) + "...@" + proxy.addr);
                        });

                        let row = $('.js-tbody-peers').append($('<tr>')
                            .append($('<td>'))
                            .append($('<td>').append(account_key.substr(8, 5) + "..."))
                            .append($('<td>').append(account_id))
                            .append($('<td>').append("[" + proxies.join(",") + "]"))
                            .append($('<td>').append(account.peer_id.substr(8, 5) + "..."))
                            .append($('<td>'))
                            .append($('<td>'))
                            .append($('<td>'))
                            .append($('<td>'))
                            .append($('<td>'))
                            .append($('<td>'))
                        )
                    });

                    // tier1_accounts_keys contains accounts whose data we would like to collect
                    network_info.tier1_accounts_keys.forEach(account_key => {
                        if (rendered.has(account_key)) {
                            return;
                        }
                        rendered.add(account_key);

                        let row = $('.js-tbody-peers').append($('<tr>')
                            .append($('<td>'))
                            .append($('<td>').append(account_key.substr(8, 5) + "..."))
                            .append($('<td>'))
                            .append($('<td>'))
                            .append($('<td>'))
                            .append($('<td>'))
                            .append($('<td>'))
                            .append($('<td>'))
                            .append($('<td>'))
                            .append($('<td>'))
                            .append($('<td>'))
                        )
                    });
                },

                dataType: "json",
                error: function (errMsg, textStatus, errorThrown) {
                    alert("Failed: " + textStatus + " :" + errorThrown);
                },
                contentType: "application/json; charset=utf-8",
            })

        });
    </script>
</head>

<body>
    <h1>
        Welcome to the TIER1 Network Info page!
    </h1>

    <table>
        <thead>
            <tr>
                <th>Address</th>
                <th>AccountKey</th>
                <th>AccountId</th>
                <th>Proxies</th>
                <th>PeerId</th>
                <th>Last ping</th>
                <th>Tracked Shards</th>
                <th>Archival</th>
                <th>Connection type</th>
                <th>First connection</th>
                <th>Traffic (last minute)</th>
            </tr>
        </thead>
        <tbody class="js-tbody-peers">
        </tbody>
    </table>
</body>

</html>

'''
'''--- chain/jsonrpc/res/validator.html ---
<html>

<head>
    <link rel="stylesheet" href="validator.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>

    <script>
        // draw the colorful bar with the approvals.
        // approval_map should contain the mapping from 'skips' (int) to number of votes, skip == 0 means that this is an endorsement.
        function drawApprovals(approvalMap, missingCount, context) {
            // We use 5 different colors (and they should roughly map the validator-skip-XX css above).
            var colors = {
                0: { "bg": 'rgba(117, 240, 158, 0.2)', "border": 'rgba(117, 240, 158, 1.0)' },
                1: { "bg": 'rgba(255, 206, 86, 0.2)', "border": 'rgba(255, 206, 86, 1.0)' },
                2: { "bg": 'rgba(54, 162, 235, 0.2)', "border": 'rgba(54, 162, 235, 1.0)' },
                3: { "bg": 'rgba(75, 192, 192, 0.2)', "border": 'rgba(75, 192, 192, 1.0)' },
                4: { "bg": 'rgba(255, 100, 30, 0.2)', "border": 'rgba(255, 100, 30, 1.0)' },
            };
            let datasets = [];
            Array.from(approvalMap.keys()).sort().forEach(element => {
                if (element == 0) {
                    // This is endorsement
                    label = "Endorsement";
                } else {
                    label = "Skip " + element;
                }
                datasets.push({
                    label: label,
                    data: [approvalMap.get(element)],
                    backgroundColor: [colors[element % 5]["bg"]],
                    borderColor: [colors[element % 5]["border"]],
                    borderWidth: 1,
                })
            });
            datasets.push({
                label: 'Missing',
                data: [missingCount],
                backgroundColor: ['rgba(128, 128, 128, 0.2)'],
                borderColor: ['rgba(128, 128, 128, 1.0)'],
                borderWidth: 1,
            });

            var myChart = new Chart(context, {
                type: 'bar',
                data: {
                    labels: ['Votes'],
                    datasets: datasets,
                },
                options: {
                    indexAxis: 'y',
                    scales: {
                        y: {
                            stacked: true,
                            beginAtZero: true
                        },
                        x: {
                            stacked: true,
                        }
                    },
                    plugins: {
                        legend: {
                            display: false,
                        }
                    }

                }
            });
        }

        function processApprovals(height, all_validators, block_production) {
            let approvalCells = [];
            let fastestValidator = null;

            if (block_production != null && !jQuery.isEmptyObject(block_production.approvals.approvals)) {
                let approvals = block_production.approvals.approvals;
                let approvalsMap = new Map();
                let missingCount = 0;

                all_validators.forEach(entry => {
                    let validator = entry[0];
                    let stake = entry[1];
                    if (approvals[validator] == null) {
                        missingCount += stake;
                    } else {
                        if (approvals[validator][0]["Endorsement"] != null) {
                            approvalsMap.set(0, (approvalsMap.get(0) ?? 0) + stake);
                        } else {
                            let skip_delta = height - approvals[validator][0]["Skip"];
                            approvalsMap.set(skip_delta, (approvalsMap.get(skip_delta) ?? 0) + stake);
                        }
                        let ts = Date.parse(approvals[validator][1]);
                        if (fastestValidator == null || fastestValidator > ts) {
                            fastestValidator = ts;
                        }
                    }
                });

                let canvas = $('<canvas>').height(100);
                drawApprovals(approvalsMap, missingCount, canvas[0].getContext('2d'));
                approvalCells.push($('<td>').append($('<div>').width("200px").append(canvas)));

                all_validators.forEach(entry => {
                    let validator = entry[0];
                    let cell = $('<td>');
                    if (approvals[validator] == null) {
                        cell.addClass('validator-missing');
                    } else {
                        let cellText = "";
                        if (approvals[validator][0]["Endorsement"] != null) {
                            cell.addClass('validator-endorse');
                        } else {
                            let skip_delta = height - approvals[validator][0]["Skip"];

                            cell.addClass('validator-skip-' + (skip_delta % 5));
                            cellText += "Skip " + skip_delta + "<br>"
                        }
                        let ts = Date.parse(approvals[validator][1]);
                        cellText += "F + " + (ts - fastestValidator) + "ms";
                        cell.append(cellText);
                    }
                    approvalCells.push(cell);
                });
            } else {
                approvalCells.push($('<td>'));
                all_validators.forEach(_validator => {
                    approvalCells.push($('<td>'));
                });
            }

            return [approvalCells, fastestValidator]
        }

        function prettyTime(dtString) {
            let time = new Date(Date.parse(dtString));
            return time.getUTCHours() + ":" + String(time.getUTCMinutes()).padStart(2, "0") + ":" +
                String(time.getUTCSeconds()).padStart(2, "0") + "." + String(time.getUTCMilliseconds()).padStart(3, '0')
        }

        function processChunkProduction(chunk_production, cell) {
            if (chunk_production != null) {
                if (chunk_production.chunk_production_time != null) {
                    prettyTime(chunk_production.chunk_production_time)
                    cell.append("Produced<br>@" + prettyTime(chunk_production.chunk_production_time));
                    cell.append("<br>Duration: " + chunk_production.chunk_production_duration_millis + "ms");
                } else {
                    cell.append("<b>MISSED CHUNK PRODUCTION</b>");
                }
            }
        }

        function process_validator_status(data) {
            let validator_name = data.status_response.ValidatorStatus.validator_name;
            if (validator_name != null) {
                $(".is-validator").text("Validator Id: " + validator_name);
            } else {
                $(".is-validator").text("NOT A VALIDATOR");
            }

            let head = data.status_response.ValidatorStatus.head_height;
            let approvalHistory = data.status_response.ValidatorStatus.approval_history;

            // Updates the bottom table with the history of approvals created.
            approvalHistory.reverse().forEach((element, index) => {
                let row =
                    $('<tr>')
                        .append($('<td>').append(element.approval_creation_time))
                        .append($('<td>').append(element.parent_height))
                        .append($('<td>').append(element.target_height))
                        .append($('<td>').append(element.expected_delay_millis));

                let client_actor_delay = element.timer_started_ago_millis - element.expected_delay_millis;
                let client_actor_delay_cell = $('<td>').append(client_actor_delay);
                if (client_actor_delay > 150) {
                    client_actor_delay_cell.addClass('client-actor-delayed');
                }

                row.append(client_actor_delay_cell);

                if (index < approvalHistory.length - 1 && approvalHistory[index + 1].target_height > element.target_height) {
                    row.addClass('approval-target-rollback');
                } else {
                    // This means that this is a skip
                    if (element.target_height != element.parent_height + 1) {
                        row.addClass('approval-skip');
                    } else {
                        row.addClass('approval-ok');

                    }
                }
                $('.js-tbody-approvals-sent').append(row);
            });

            // Updates the top table with production information.
            let all_validators = data.status_response.ValidatorStatus.validators;
            let validators_size = all_validators.length;

            let numShards = data.status_response.ValidatorStatus.shards;
            $('.js-thead-production').append($("<th>").text("Block production"));
            for (let i = 0; i < numShards; i += 1) {
                $('.js-thead-production').append($("<th>").text("Shard " + i));
            }
            $('.js-thead-production').append($("<th>").text("Approvals"));
            for (let i = 0; i < validators_size; i += 1) {
                $('.js-thead-production').append($("<th>").text(all_validators[i][0].substring(0, 7)));
            }
            let head_printed = false;
            data.status_response.ValidatorStatus.production.forEach(item => {
                let [height, entry] = item;
                if (height <= head && !head_printed) {
                    head_printed = true;
                    $('.js-tbody-production').append($("<tr><td colspan=10><b>HEAD</b></td></tr>"));
                }
                let row = $('<tr>')
                    .append($('<td>').append(height));
                let thresholdApprovalTime = null;

                // In case the current validator has received any approvals for this block.
                let [approvalCells, fastestValidator] = processApprovals(height, all_validators, entry.block_production);

                // In case the current validator is responsible for producting this block.
                if (entry.block_production != null) {
                    let block_production = entry.block_production;

                    let cell = $('<td>');
                    let content = "";
                    if (block_production.block_production_time != null) {
                        content += "Approval Threshold T = <b>F + " + (Date.parse(block_production.approvals.ready_at) - fastestValidator) + " ms</b> <br>";
                        thresholdApprovalTime = Date.parse(block_production.approvals.ready_at);
                        content += "Block produced: <br>@" + prettyTime(block_production.block_production_time);
                        content += "<br> <b>F+" + (Date.parse(block_production.block_production_time) - fastestValidator) + "ms</b>";
                        content += "<br> <b>T+" + (Date.parse(block_production.block_production_time) - thresholdApprovalTime) + "ms</b>";
                    } else {
                        content += "No block produced"
                    }
                    if (!block_production.block_included) {
                        cell.addClass("block-missing");
                    }
                    cell.append(content);
                    row.append(cell);

                    for (let i = 0; i < numShards; i += 1) {
                        let chunk_cell = $('<td>');

                        if (block_production.chunks_collection_time[i] != null) {
                            let chunk_producer = block_production.chunks_collection_time[i].chunk_producer;
                            if (chunk_producer == validator_name) {
                                chunk_cell.append("<strong>ME</strong><br>");
                            } else {
                                chunk_cell.append(chunk_producer + "<br>");
                            }
                            let chunk_collection_time = block_production.chunks_collection_time[i].received_time;
                            if (chunk_collection_time != null) {
                                let time_delta = Date.parse(chunk_collection_time) - fastestValidator;
                                chunk_cell.append("Collected<br>@" + prettyTime(chunk_collection_time) + "<br>")
                                chunk_cell.append("<b>F + " + time_delta + "ms</b><br>")
                                if (thresholdApprovalTime != null) {
                                    let time_since_threshold = Date.parse(chunk_collection_time) - thresholdApprovalTime;
                                    let text = $("<span><b>T + " + time_since_threshold + "ms</b><br></span>");
                                    if (time_since_threshold > 300) {
                                        text.addClass('chunk-delay-red')
                                    } else if (time_since_threshold > 150) {
                                        text.addClass('chunk-delay-orange')
                                    }
                                    chunk_cell.append(text);
                                }
                            } else {
                                chunk_cell.append("No chunk collected<br>")
                            }
                            if (thresholdApprovalTime != null) {
                                time_since_threshold = Date.parse(chunk_collection_time) - thresholdApprovalTime;
                                if (!block_production.chunks_collection_time[i].chunk_included) {
                                    chunk_cell.addClass('chunk-missing')
                                }
                            }
                        }

                        processChunkProduction(entry.chunk_production[i], chunk_cell);
                        row.append(chunk_cell);
                    }
                } else {
                    row.append($('<td>'));
                    for (let i = 0; i < numShards; i += 1) {
                        let cell = $('<td>');
                        processChunkProduction(entry.chunk_production[i], cell);
                        row.append(cell);
                    }
                }

                approvalCells.forEach(cell => {
                    row.append(cell);
                });

                $('.js-tbody-production').append(row);

            })

            if (!head_printed) {
                $('.js-tbody-production').append($("<tr><td colspan=10><b>HEAD</b></td></tr>"));
            }

            for (let [epoch_id, chunk_producers] of data.status_response.ValidatorStatus.banned_chunk_producers) {
                $('#banned-chunk-producers').append(
                    $('<p>')
                        .text('Banned chunk producers for epoch ' + epoch_id + ': ' + chunk_producers.join(', '))
                );
            }
        };

        $(document).ready(() => {
            $('.div-progress').hide();
            $('span').text("Loading...");
            $.ajax({
                type: "GET",
                url: "../api/validator_status",
                success: data => {
                    process_validator_status(data);
                },
                dataType: "json",
                error: function (errMsg, textStatus, errorThrown) {
                    alert("Failed: " + textStatus + " :" + errorThrown);
                },
                contentType: "application/json; charset=utf-8",
            });
        });
    </script>
</head>

<body>
    <h1>
        Validator page -
        <span class="is-validator"></span>
    </h1>

    <div class=" div-production">
        <h2>
            <p>Production</p>
        </h2>
        <p>
            <b>F</b> is the time when the first approval arrives for a given height.<br>
            <b>T</b> is the time when a given blocks gets a threshold approval (66%).<br>
            Shards can either be produced by this validator (marked as 'ME') or received from other
            validators.<br>
            Shards have missing chunks are marked as grey. <br>
            We also mark shards arrival time in color. Shards that are delayed by more than 150ms after T are marked as
            orange,
            and ones delayed more than 300 marked as red.<br>
            <b>Approvals</b><br>
            Green field means that validators endorses the PREVIOUS block.<br>
            Grey means that we didn't get any info from the validator.<br>
            Other colors means different amount of skips.

        </p>
        <div id="banned-chunk-producers"></div>
        <table>
            <thead>
                <tr class="js-thead-production">
                    <th>Height</th>
                </tr>
            </thead>
            <tbody class="js-tbody-production">
            </tbody>
        </table>
    </div>

    <div class="div-approvals-sent">
        <h2>
            <p>Approval history</p>
        </h2>
        <table>
            <thead>
                <tr class="js-thead-approvals-sent">
                    <th>Time</th>
                    <th>Approval from</th>
                    <th>Approval to</th>
                    <th>Delay</th>
                    <th>Client-actor delay</th>

                </tr>
            </thead>
            <tbody class="js-tbody-approvals-sent">
            </tbody>
        </table>
    </div>

</body>

</html>
'''
'''--- debug_scripts/__init__.py ---

'''
'''--- debug_scripts/request_chain_info.py ---
#!/usr/bin/env python3

import requests
import json
import argparse

if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description='This is a script to request for blockchain info')
    parser.add_argument('--chain',
                        choices=['mainnet', 'testnet', 'betanet'],
                        required=True)
    parser.add_argument('--archive',
                        action='store_true',
                        help='whether to request from archival nodes')
    parser.add_argument('--method',
                        choices=['block', 'chunk'],
                        required=True,
                        help='type of request')
    parser.add_argument('--block_id',
                        type=str,
                        help='block id, can be either block height or hash')
    parser.add_argument('--shard_id', type=int, help='shard id for the chunk')
    parser.add_argument('--chunk_id', type=str, help='chunk hash')
    parser.add_argument('--result_key',
                        type=str,
                        nargs='*',
                        help='filter results by these keys')
    args = parser.parse_args()

    url = 'https://{}.{}.near.org'.format(
        'archival-rpc' if args.archive else 'rpc', args.chain)

    def get_block_id(block_id):
        if block_id.isnumeric():
            return int(block_id)
        return block_id

    if args.method == 'block':
        if args.block_id is not None:
            params = {'block_id': get_block_id(args.block_id)}
        else:
            params = {'finality': 'final'}
    elif args.method == 'chunk':
        if args.shard_id is not None:
            assert args.block_id is not None
            params = {
                'shard_id': args.shard_id,
                'block_id': get_block_id(args.block_id)
            }
        elif args.chunk_id is not None:
            params = {'chunk_id': args.chunk_id}
    else:
        assert False

    payload = {
        'jsonrpc': '2.0',
        'id': 'dontcare',
        'method': args.method,
        'params': params
    }

    response = requests.post(url, json=payload)
    result = response.json()['result']
    if args.result_key is not None:
        for key in args.result_key:
            result = result[key]
    print(json.dumps(result, indent=4))

'''
'''--- debug_scripts/send_validator_logs.py ---
#!/bin/python3
import argparse
import boto3
import datetime
import gzip
import io
import sys
import urllib.parse

def filter_log_file(log_file: str, start_time: datetime.datetime,
                    end_time: datetime.datetime) -> list:
    """
    Filter log file for a time range.
    start_time: datetime.datetime
                    start time for logs
    end_time: datetime.datetime
                    end time for logs
    return: list
                list of log lines
    """
    print(f"Log time range: {start_time} \t {end_time}")

    filtered_logs = []

    # filter logs for time range
    with open(log_file) as f:
        for line in f:
            # [0m and [2m are ANSI shell color codes. Removing them to parse dates.
            split_lines = line.split("[0m", 1)[0].replace("\x1b[2m", "")
            dt = datetime.datetime.strptime(
                split_lines[:-5],
                "%b %d %H:%M:%S").replace(year=datetime.datetime.now().year)
            if start_time <= dt <= end_time:
                filtered_logs.append(line)
    return filtered_logs

def upload_to_s3(file_lines: list, account: str) -> str:
    """
    Upload File like object to S3 bucket near-protocol-validator-logs-public.
    file_obj: io.BytesIO
    account: str
    return string with S3 file path
    """
    BUCKET = "near-protocol-validator-logs-public"
    current_time = datetime.datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ")
    s3_destination = f"{account}/{current_time}.log.gzip"
    file_string = io.StringIO()
    for line in file_lines:
        file_string.write(line)

    file_obj = io.BytesIO(file_string.getvalue().encode())
    gzipped_content = gzip.compress(file_obj.read())
    print(
        f"uploading compressed file. File size is: {sys.getsizeof(gzipped_content)} Bytes"
    )

    s3 = boto3.resource('s3')
    s3.Bucket(BUCKET).upload_fileobj(io.BytesIO(gzipped_content),
                                     f"logs/{s3_destination}")
    s3_link = f"https://{BUCKET}.s3.amazonaws.com/logs/{urllib.parse.quote(s3_destination)}"
    print(f"Log File was uploaded to S3: {s3_link}")
    file_obj.close()
    return s3_link

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Send logs to near.')
    parser.add_argument('--log_file',
                        type=str,
                        help='Absolute path to log file.',
                        required=True)
    parser.add_argument('--account',
                        type=str,
                        help='Near account id.',
                        required=True)
    parser.add_argument('--last_seconds',
                        type=int,
                        help='Filter logs for last x seconds.',
                        required=True)
    args = parser.parse_args()

    log_file_path = args.log_file
    end_timestamp = datetime.datetime.utcnow()
    start_timestamp = end_timestamp - datetime.timedelta(
        seconds=args.last_seconds)

    filtered_log_lines = filter_log_file(log_file=args.log_file,
                                         start_time=start_timestamp,
                                         end_time=end_timestamp)
    upload_to_s3(file_lines=filtered_log_lines, account=args.account)

'''
'''--- debug_scripts/tests/__init__.py ---

'''
'''--- debug_scripts/tests/send_validator_logs_test.py ---
import unittest
import datetime
import io
import sys
from send_validator_logs import filter_log_file

class test_validator_log_filtering(unittest.TestCase):

    def test_time_filtering(self):
        start_time = datetime.datetime(2022, 4, 4, 23, 42, 0, 0)
        end_time = datetime.datetime(2022, 4, 4, 23, 49, 0, 0)
        output_file_obj = filter_log_file("./tests/data/node0.logs",
                                          start_time=start_time,
                                          end_time=end_time)
        self.assertIsInstance(
            output_file_obj, list,
            "Parsed file object should be of type io.BytesIO")
        self.assertEqual(len(output_file_obj), 48,
                         "Filtered log should have 48 lines")

if __name__ == '__main__':
    unittest.main()

'''
'''--- nightly/expensive.txt ---
# catchup tests
expensive --timeout=1800 near-client near_client tests::catching_up::test_catchup_receipts_sync_third_epoch
expensive --timeout=1800 near-client near_client tests::catching_up::test_catchup_receipts_sync_third_epoch --features nightly
expensive --timeout=1800 near-client near_client tests::catching_up::test_catchup_receipts_sync_last_block
expensive --timeout=1800 near-client near_client tests::catching_up::test_catchup_receipts_sync_last_block --features nightly
expensive --timeout=1800 near-client near_client tests::catching_up::test_catchup_receipts_sync_distant_epoch
expensive --timeout=1800 near-client near_client tests::catching_up::test_catchup_receipts_sync_distant_epoch --features nightly
expensive --timeout=1800 near-client near_client tests::catching_up::test_catchup_random_single_part_sync
expensive --timeout=1800 near-client near_client tests::catching_up::test_catchup_random_single_part_sync --features nightly
expensive --timeout=1800 near-client near_client tests::catching_up::test_catchup_random_single_part_sync_skip_15
expensive --timeout=1800 near-client near_client tests::catching_up::test_catchup_random_single_part_sync_skip_15 --features nightly
expensive --timeout=1800 near-client near_client tests::catching_up::test_catchup_random_single_part_sync_send_15
expensive --timeout=1800 near-client near_client tests::catching_up::test_catchup_random_single_part_sync_send_15 --features nightly
expensive --timeout=1800 near-client near_client tests::catching_up::test_catchup_random_single_part_sync_non_zero_amounts
expensive --timeout=1800 near-client near_client tests::catching_up::test_catchup_random_single_part_sync_non_zero_amounts --features nightly
expensive --timeout=1800 near-client near_client tests::catching_up::test_catchup_random_single_part_sync_height_6
expensive --timeout=1800 near-client near_client tests::catching_up::test_catchup_random_single_part_sync_height_6 --features nightly
expensive --timeout=1800 near-client near_client tests::catching_up::test_catchup_sanity_blocks_produced
expensive --timeout=1800 near-client near_client tests::catching_up::test_catchup_sanity_blocks_produced --features nightly
expensive --timeout=3600 near-client near_client tests::catching_up::test_all_chunks_accepted_1000
expensive --timeout=3600 near-client near_client tests::catching_up::test_all_chunks_accepted_1000 --features nightly
expensive --timeout=7200 near-client near_client tests::catching_up::test_all_chunks_accepted_1000_slow
expensive --timeout=7200 near-client near_client tests::catching_up::test_all_chunks_accepted_1000_slow --features nightly
expensive --timeout=1800 near-client near_client tests::catching_up::test_all_chunks_accepted_1000_rare_epoch_changing
expensive --timeout=1800 near-client near_client tests::catching_up::test_all_chunks_accepted_1000_rare_epoch_changing --features nightly
expensive --timeout=1800 near-client near_client tests::catching_up::test_catchup_receipts_sync_hold
expensive --timeout=1800 near-client near_client tests::catching_up::test_catchup_receipts_sync_hold --features nightly

expensive integration-tests integration_tests tests::test_catchup::test_catchup
expensive integration-tests integration_tests tests::test_catchup::test_catchup --features nightly

# cross-shard transactions tests
# TODO(#4618): Those tests are currently broken.  Comment out while we’re
# working on a fix / deciding whether to remove them.
# expensive --timeout=3000 near-client near_client tests::cross_shard_tx::test_cross_shard_tx
# expensive --timeout=3000 near-client near_client tests::cross_shard_tx::test_cross_shard_tx --features nightly
expensive --timeout=3000 near-client near_client tests::cross_shard_tx::test_cross_shard_tx_doomslug
expensive --timeout=3000 near-client near_client tests::cross_shard_tx::test_cross_shard_tx_doomslug --features nightly
expensive --timeout=3000 near-client near_client tests::cross_shard_tx::test_cross_shard_tx_drop_chunks
expensive --timeout=3000 near-client near_client tests::cross_shard_tx::test_cross_shard_tx_drop_chunks --features nightly
# TODO(#4618): Those tests are currently broken.  Comment out while we’re
# working on a fix / deciding whether to remove them.
# expensive --timeout=5400 near-client near_client tests::cross_shard_tx::test_cross_shard_tx_with_validator_rotation_1
# expensive --timeout=5400 near-client near_client tests::cross_shard_tx::test_cross_shard_tx_with_validator_rotation_1 --features nightly
# expensive --timeout=5400 near-client near_client tests::cross_shard_tx::test_cross_shard_tx_with_validator_rotation_2
# expensive --timeout=5400 near-client near_client tests::cross_shard_tx::test_cross_shard_tx_with_validator_rotation_2 --features nightly
# expensive --timeout=4800 near-client near_client tests::cross_shard_tx::test_cross_shard_tx_8_iterations
# expensive --timeout=4800 near-client near_client tests::cross_shard_tx::test_cross_shard_tx_8_iterations_drop_chunks

# consensus tests
expensive --timeout=3000 near-chain near_chain tests::doomslug::test_fuzzy_doomslug_liveness_and_safety
expensive --timeout=3000 near-chain near_chain tests::doomslug::test_fuzzy_doomslug_liveness_and_safety --features nightly
expensive --timeout=500 near-client near_client tests::consensus::test_consensus_with_epoch_switches
expensive --timeout=500 near-client near_client tests::consensus::test_consensus_with_epoch_switches --features nightly

# testnet rpc
expensive integration-tests integration_tests tests::test_tps_regression::test_highload
expensive integration-tests integration_tests tests::test_tps_regression::test_highload --features nightly

expensive integration-tests integration_tests tests::standard_cases::rpc::test_access_key_smart_contract_testnet
expensive integration-tests integration_tests tests::standard_cases::rpc::test_access_key_smart_contract_testnet --features nightly
expensive integration-tests integration_tests tests::standard_cases::rpc::test_add_access_key_function_call_testnet
expensive integration-tests integration_tests tests::standard_cases::rpc::test_add_access_key_function_call_testnet --features nightly
expensive integration-tests integration_tests tests::standard_cases::rpc::test_add_access_key_with_allowance_testnet
expensive integration-tests integration_tests tests::standard_cases::rpc::test_add_access_key_with_allowance_testnet --features nightly
expensive integration-tests integration_tests tests::standard_cases::rpc::test_add_existing_key_testnet
expensive integration-tests integration_tests tests::standard_cases::rpc::test_add_existing_key_testnet --features nightly
expensive integration-tests integration_tests tests::standard_cases::rpc::test_add_key_testnet
expensive integration-tests integration_tests tests::standard_cases::rpc::test_add_key_testnet --features nightly
expensive integration-tests integration_tests tests::standard_cases::rpc::test_create_account_again_testnet
expensive integration-tests integration_tests tests::standard_cases::rpc::test_create_account_again_testnet --features nightly
expensive integration-tests integration_tests tests::standard_cases::rpc::test_create_account_failure_already_exists_testnet
expensive integration-tests integration_tests tests::standard_cases::rpc::test_create_account_failure_already_exists_testnet --features nightly
expensive integration-tests integration_tests tests::standard_cases::rpc::test_create_account_testnet
expensive integration-tests integration_tests tests::standard_cases::rpc::test_create_account_testnet --features nightly
expensive integration-tests integration_tests tests::standard_cases::rpc::test_delete_access_key_testnet
expensive integration-tests integration_tests tests::standard_cases::rpc::test_delete_access_key_testnet --features nightly
expensive integration-tests integration_tests tests::standard_cases::rpc::test_delete_access_key_with_allowance_testnet
expensive integration-tests integration_tests tests::standard_cases::rpc::test_delete_access_key_with_allowance_testnet --features nightly
expensive integration-tests integration_tests tests::standard_cases::rpc::test_delete_key_last_testnet
expensive integration-tests integration_tests tests::standard_cases::rpc::test_delete_key_last_testnet --features nightly
expensive integration-tests integration_tests tests::standard_cases::rpc::test_delete_key_not_owned_testnet
expensive integration-tests integration_tests tests::standard_cases::rpc::test_delete_key_not_owned_testnet --features nightly
expensive integration-tests integration_tests tests::standard_cases::rpc::test_delete_key_testnet
expensive integration-tests integration_tests tests::standard_cases::rpc::test_delete_key_testnet --features nightly
expensive integration-tests integration_tests tests::standard_cases::rpc::test_nonce_update_when_deploying_contract_testnet
expensive integration-tests integration_tests tests::standard_cases::rpc::test_nonce_update_when_deploying_contract_testnet --features nightly
expensive integration-tests integration_tests tests::standard_cases::rpc::test_nonce_updated_when_tx_failed_testnet
expensive integration-tests integration_tests tests::standard_cases::rpc::test_nonce_updated_when_tx_failed_testnet --features nightly
expensive integration-tests integration_tests tests::standard_cases::rpc::test_redeploy_contract_testnet
expensive integration-tests integration_tests tests::standard_cases::rpc::test_redeploy_contract_testnet --features nightly
expensive integration-tests integration_tests tests::standard_cases::rpc::test_refund_on_send_money_to_non_existent_account_testnet
expensive integration-tests integration_tests tests::standard_cases::rpc::test_refund_on_send_money_to_non_existent_account_testnet --features nightly
expensive integration-tests integration_tests tests::standard_cases::rpc::test_send_money_over_balance_testnet
expensive integration-tests integration_tests tests::standard_cases::rpc::test_send_money_over_balance_testnet --features nightly
expensive integration-tests integration_tests tests::standard_cases::rpc::test_send_money_testnet
expensive integration-tests integration_tests tests::standard_cases::rpc::test_send_money_testnet --features nightly
expensive integration-tests integration_tests tests::standard_cases::rpc::test_smart_contract_bad_method_name_testnet
expensive integration-tests integration_tests tests::standard_cases::rpc::test_smart_contract_bad_method_name_testnet --features nightly
expensive integration-tests integration_tests tests::standard_cases::rpc::test_smart_contract_empty_method_name_with_no_tokens_testnet
expensive integration-tests integration_tests tests::standard_cases::rpc::test_smart_contract_empty_method_name_with_no_tokens_testnet --features nightly
expensive integration-tests integration_tests tests::standard_cases::rpc::test_smart_contract_empty_method_name_with_tokens_testnet
expensive integration-tests integration_tests tests::standard_cases::rpc::test_smart_contract_empty_method_name_with_tokens_testnet --features nightly
expensive integration-tests integration_tests tests::standard_cases::rpc::test_smart_contract_self_call_testnet
expensive integration-tests integration_tests tests::standard_cases::rpc::test_smart_contract_self_call_testnet --features nightly
expensive integration-tests integration_tests tests::standard_cases::rpc::test_smart_contract_simple_testnet
expensive integration-tests integration_tests tests::standard_cases::rpc::test_smart_contract_simple_testnet --features nightly
expensive integration-tests integration_tests tests::standard_cases::rpc::test_smart_contract_with_args_testnet
expensive integration-tests integration_tests tests::standard_cases::rpc::test_smart_contract_with_args_testnet --features nightly
expensive integration-tests integration_tests tests::standard_cases::rpc::test_swap_key_testnet
expensive integration-tests integration_tests tests::standard_cases::rpc::test_swap_key_testnet --features nightly
expensive integration-tests integration_tests tests::standard_cases::rpc::test_upload_contract_testnet
expensive integration-tests integration_tests tests::standard_cases::rpc::test_upload_contract_testnet --features nightly

# GC tests
expensive --timeout=900 near-chain near_chain tests::garbage_collection::test_gc_remove_fork_large
expensive --timeout=900 near-chain near_chain tests::garbage_collection::test_gc_remove_fork_large --features nightly
expensive --timeout=1200 near-chain near_chain tests::garbage_collection::test_gc_not_remove_fork_large
expensive --timeout=1200 near-chain near_chain tests::garbage_collection::test_gc_not_remove_fork_large --features nightly
expensive --timeout=1200 near-chain near_chain tests::garbage_collection::test_gc_boundaries_large
expensive --timeout=1200 near-chain near_chain tests::garbage_collection::test_gc_boundaries_large --features nightly
expensive --timeout=900 near-chain near_chain tests::garbage_collection::test_gc_random_large
expensive --timeout=900 near-chain near_chain tests::garbage_collection::test_gc_random_large --features nightly
expensive --timeout=600 near-chain near_chain tests::garbage_collection::test_gc_pine
expensive --timeout=600 near-chain near_chain tests::garbage_collection::test_gc_pine --features nightly
expensive --timeout=700 near-chain near_chain tests::garbage_collection::test_gc_star_large
expensive --timeout=700 near-chain near_chain tests::garbage_collection::test_gc_star_large --features nightly

expensive --timeout=1200 integration-tests integration_tests tests::client::block_corruption::check_process_flipped_block_fails
expensive --timeout=1200 integration-tests integration_tests tests::client::block_corruption::check_process_flipped_block_fails --features nightly
expensive integration-tests integration_tests tests::client::chunks_management::chunks_recovered_from_full
expensive integration-tests integration_tests tests::client::chunks_management::chunks_recovered_from_full --features nightly
expensive integration-tests integration_tests tests::client::chunks_management::chunks_recovered_from_full_cop
expensive integration-tests integration_tests tests::client::chunks_management::chunks_recovered_from_full_cop --features nightly
expensive integration-tests integration_tests tests::client::chunks_management::chunks_recovered_from_full_timeout_too_short
expensive integration-tests integration_tests tests::client::chunks_management::chunks_recovered_from_full_timeout_too_short --features nightly
expensive integration-tests integration_tests tests::client::chunks_management::chunks_recovered_from_full_timeout_too_short_cop
expensive integration-tests integration_tests tests::client::chunks_management::chunks_recovered_from_full_timeout_too_short_cop --features nightly
expensive integration-tests integration_tests tests::client::chunks_management::chunks_recovered_from_others
expensive integration-tests integration_tests tests::client::chunks_management::chunks_recovered_from_others --features nightly
expensive integration-tests integration_tests tests::client::chunks_management::chunks_recovered_from_others_cop
expensive integration-tests integration_tests tests::client::chunks_management::chunks_recovered_from_others_cop --features nightly
expensive integration-tests integration_tests tests::client::process_blocks::test_gc_after_state_sync
expensive integration-tests integration_tests tests::client::process_blocks::test_gc_after_state_sync --features nightly
expensive integration-tests integration_tests tests::client::process_blocks::test_process_block_after_state_sync
expensive integration-tests integration_tests tests::client::process_blocks::test_process_block_after_state_sync --features nightly
expensive integration-tests integration_tests tests::client::sync_state_nodes::sync_empty_state
expensive integration-tests integration_tests tests::client::sync_state_nodes::sync_empty_state --features nightly
expensive integration-tests integration_tests tests::client::sync_state_nodes::sync_state_dump
expensive integration-tests integration_tests tests::client::sync_state_nodes::sync_state_dump --features nightly
expensive integration-tests integration_tests tests::client::sync_state_nodes::sync_state_nodes
expensive integration-tests integration_tests tests::client::sync_state_nodes::sync_state_nodes --features nightly
expensive integration-tests integration_tests tests::client::sync_state_nodes::sync_state_nodes_multishard
expensive integration-tests integration_tests tests::client::sync_state_nodes::sync_state_nodes_multishard --features nightly

# other tests
expensive --timeout=300 near-chain near_chain tests::garbage_collection::test_clear_old_data_too_many_heights

expensive integration-tests integration_tests tests::test_simple::test_2_10_multiple_nodes
expensive integration-tests integration_tests tests::test_simple::test_2_10_multiple_nodes --features nightly
expensive integration-tests integration_tests tests::test_simple::test_4_10_multiple_nodes
expensive integration-tests integration_tests tests::test_simple::test_4_10_multiple_nodes --features nightly
expensive integration-tests integration_tests tests::test_simple::test_7_10_multiple_nodes
expensive integration-tests integration_tests tests::test_simple::test_7_10_multiple_nodes --features nightly

expensive integration-tests integration_tests tests::nearcore::sync_nodes::sync_after_sync_nodes
expensive integration-tests integration_tests tests::nearcore::sync_nodes::sync_after_sync_nodes --features nightly
expensive integration-tests integration_tests tests::nearcore::sync_nodes::sync_nodes
expensive integration-tests integration_tests tests::nearcore::sync_nodes::sync_nodes --features nightly
expensive integration-tests integration_tests tests::nearcore::sync_nodes::sync_state_stake_change
expensive integration-tests integration_tests tests::nearcore::sync_nodes::sync_state_stake_change --features nightly

expensive integration-tests integration_tests tests::nearcore::rpc_error_structs::test_block_unknown_block_error
expensive integration-tests integration_tests tests::nearcore::rpc_error_structs::test_block_unknown_block_error --features nightly
expensive integration-tests integration_tests tests::nearcore::rpc_error_structs::test_chunk_unknown_chunk_error
expensive integration-tests integration_tests tests::nearcore::rpc_error_structs::test_chunk_unknown_chunk_error --features nightly
expensive integration-tests integration_tests tests::nearcore::rpc_error_structs::test_gas_price_unknown_block_error
expensive integration-tests integration_tests tests::nearcore::rpc_error_structs::test_gas_price_unknown_block_error --features nightly
expensive integration-tests integration_tests tests::nearcore::rpc_error_structs::test_protocol_config_unknown_block_error
expensive integration-tests integration_tests tests::nearcore::rpc_error_structs::test_protocol_config_unknown_block_error --features nightly
expensive integration-tests integration_tests tests::nearcore::rpc_error_structs::test_query_rpc_account_view_unknown_block_must_return_error
expensive integration-tests integration_tests tests::nearcore::rpc_error_structs::test_query_rpc_account_view_unknown_block_must_return_error --features nightly
expensive integration-tests integration_tests tests::nearcore::rpc_error_structs::test_receipt_id_unknown_receipt_error
expensive integration-tests integration_tests tests::nearcore::rpc_error_structs::test_receipt_id_unknown_receipt_error --features nightly
expensive integration-tests integration_tests tests::nearcore::rpc_error_structs::test_tx_invalid_tx_error
expensive integration-tests integration_tests tests::nearcore::rpc_error_structs::test_tx_invalid_tx_error --features nightly

expensive integration-tests integration_tests tests::nearcore::rpc_nodes::test_tx_status_on_lightclient_must_return_does_not_track_shard
expensive integration-tests integration_tests tests::nearcore::rpc_nodes::test_tx_status_on_lightclient_must_return_does_not_track_shard --features nightly
expensive integration-tests integration_tests tests::nearcore::rpc_nodes::test_check_unknown_tx_must_return_error
expensive integration-tests integration_tests tests::nearcore::rpc_nodes::test_check_unknown_tx_must_return_error --features nightly
expensive integration-tests integration_tests tests::nearcore::rpc_nodes::test_get_execution_outcome_tx_failure
expensive integration-tests integration_tests tests::nearcore::rpc_nodes::test_get_execution_outcome_tx_failure --features nightly
expensive integration-tests integration_tests tests::nearcore::rpc_nodes::test_get_execution_outcome_tx_success
expensive integration-tests integration_tests tests::nearcore::rpc_nodes::test_get_execution_outcome_tx_success --features nightly
expensive integration-tests integration_tests tests::nearcore::rpc_nodes::test_get_validator_info_rpc
expensive integration-tests integration_tests tests::nearcore::rpc_nodes::test_get_validator_info_rpc --features nightly
expensive integration-tests integration_tests tests::nearcore::rpc_nodes::test_protocol_config_rpc
expensive integration-tests integration_tests tests::nearcore::rpc_nodes::test_protocol_config_rpc --features nightly
expensive integration-tests integration_tests tests::nearcore::rpc_nodes::test_query_rpc_account_view_account_doesnt_exist_must_return_error
expensive integration-tests integration_tests tests::nearcore::rpc_nodes::test_query_rpc_account_view_account_doesnt_exist_must_return_error --features nightly
expensive integration-tests integration_tests tests::nearcore::rpc_nodes::test_query_rpc_account_view_must_succeed
expensive integration-tests integration_tests tests::nearcore::rpc_nodes::test_query_rpc_account_view_must_succeed --features nightly
expensive integration-tests integration_tests tests::nearcore::rpc_nodes::test_tx_not_enough_balance_must_return_error
expensive integration-tests integration_tests tests::nearcore::rpc_nodes::test_tx_not_enough_balance_must_return_error --features nightly
expensive integration-tests integration_tests tests::nearcore::rpc_nodes::test_validators_by_epoch_id_current_epoch_not_fails
expensive integration-tests integration_tests tests::nearcore::rpc_nodes::test_validators_by_epoch_id_current_epoch_not_fails --features nightly
expensive integration-tests integration_tests tests::nearcore::run_nodes::run_nodes_1_2_1
expensive integration-tests integration_tests tests::nearcore::run_nodes::run_nodes_1_2_1 --features nightly
expensive integration-tests integration_tests tests::nearcore::run_nodes::run_nodes_1_2_2
expensive integration-tests integration_tests tests::nearcore::run_nodes::run_nodes_1_2_2 --features nightly
expensive integration-tests integration_tests tests::nearcore::run_nodes::run_nodes_1_4_4
expensive integration-tests integration_tests tests::nearcore::run_nodes::run_nodes_1_4_4 --features nightly
expensive integration-tests integration_tests tests::nearcore::run_nodes::run_nodes_4_4_2
expensive integration-tests integration_tests tests::nearcore::run_nodes::run_nodes_4_4_2 --features nightly

expensive integration-tests integration_tests tests::nearcore::stake_nodes::test_inflation
expensive integration-tests integration_tests tests::nearcore::stake_nodes::test_inflation --features nightly
expensive integration-tests integration_tests tests::nearcore::stake_nodes::test_stake_nodes
expensive integration-tests integration_tests tests::nearcore::stake_nodes::test_stake_nodes --features nightly
expensive integration-tests integration_tests tests::nearcore::stake_nodes::test_validator_join
expensive integration-tests integration_tests tests::nearcore::stake_nodes::test_validator_join --features nightly
expensive integration-tests integration_tests tests::nearcore::stake_nodes::test_validator_kickout
expensive integration-tests integration_tests tests::nearcore::stake_nodes::test_validator_kickout --features nightly

expensive integration-tests integration_tests tests::nearcore::track_shards::track_shards
expensive integration-tests integration_tests tests::nearcore::track_shards::track_shards --features nightly

'''
'''--- nightly/nayduck.py ---
#!/usr/bin/env python3
"""Runs integration tests in the cloud on NayDuck.

To request a new run, use the following command:

   python3 scripts/nayduck.py      \
       --branch    <your_branch>   \
       --test-file <test_file>.txt

Scheduled runs can be seen at <https://nayduck.nearone.org/>.

See README.md in nightly directory for documentation of the test suite file
format.  Note that you must be a member of the Near or Near Protocol
organisation on GitHub to authenticate (<https://github.com/orgs/near/people>).

The source code for NayDuck itself is at <https://github.com/Near-One/nayduck>.
"""

import getpass
import json
import os
import pathlib
import shlex
import subprocess
import sys
import typing

REPO_DIR = pathlib.Path(__file__).resolve().parents[1]

DEFAULT_TEST_FILE = 'nightly/nightly.txt'
NAYDUCK_BASE_HREF = 'https://nayduck.nearone.org'

def _parse_args():
    import argparse

    default_test_path = (pathlib.Path(__file__).parent.parent /
                         DEFAULT_TEST_FILE)

    parser = argparse.ArgumentParser(description='Run tests.')
    parser.add_argument(
        '--cancel',
        '-c',
        help='Cancel scheduled run. In progress tests cannot be stopped.')
    parser.add_argument('--branch',
                        '-b',
                        help='Branch to test. By default gets current one.')
    parser.add_argument(
        '--sha',
        '-s',
        help=
        'Commit sha to test. By default gets current one. This is ignored if branch name is provided'
    )
    group = parser.add_mutually_exclusive_group()
    group.add_argument('--test-file',
                       '-t',
                       default=default_test_path,
                       help=f'Test set file; {DEFAULT_TEST_FILE} by default.')
    group.add_argument('--stdin',
                       '-i',
                       action='store_true',
                       help='Read test set from standard input.')
    parser.add_argument('--run-locally',
                        '-l',
                        action='store_true',
                        help='Run tests locally.')
    parser.add_argument(
        '--dry-run',
        '-n',
        action='store_true',
        help='Prints list of tests to execute, without doing anything')
    args = parser.parse_args()

    return args

def get_sha(branch: str):
    try:
        sha = subprocess.check_output(['git', 'rev-parse', branch], text=True)

    except subprocess.CalledProcessError as e:
        remote_branch = 'remotes/origin/' + branch
        print(
            f"Couldn't find a local branch \'{branch}\'. Trying remote: {remote_branch}"
        )
        sha = subprocess.check_output(
            ['git', 'rev-parse', remote_branch],
            text=True,
        )

    return sha

def get_branch(sha: str = ""):
    if sha:
        return subprocess.check_output(['git', 'name-rev', sha],
                                       text=True).strip().split(' ')[-1]
    else:
        return subprocess.check_output(
            ['git', 'rev-parse', '--abbrev-ref', "HEAD"], text=True).strip()

FileReader = typing.Callable[[pathlib.Path], str]

def read_tests_from_file(
    path: pathlib.Path,
    *,
    include_comments: bool = False,
    reader: FileReader = lambda path: path.read_text()
) -> typing.Iterable[str]:
    """Reads lines from file handling `./<path>` includes.

    Returns an iterable over lines in given file but also handles `./<path>`
    syntax for including other files in the output and optionally filters
    commented out lines.

    A `./<path>` syntax acts like C's #include directive, Rust's `include!`
    macro or shell's `source` command.  All lines are read from file at <path>
    as if they were directly in the source file.  The `./<path>` directives are
    handled recursively up to three levels deep.

    If include_comments is True, `#./<path>` lines are handled as well with all
    included line commented out.  This is useful to comment out a include a with
    TODO comment included and have check_nightly.py and check_pytest.py scripts
    still recognise those includes.  Note that the line must start with `#./`,
    i.e. there must be no space between hash and the dot.

    Args:
        path: Path to the file to read.
        include_comments: By default empty lines and lines whose first non-space
            character is hash are ignored and not included in the output.  With
            this set to `True` such lines are included as well.
        reader: A callback which reads text content of a given file.  This is
            used by NayDuck.
    Returns:
        An iterable over lines in the given file.  All lines are stripped of
        leading and trailing white space.
    """
    return __read_tests(reader(path).splitlines(),
                        filename=path,
                        dirpath=path.parent,
                        include_comments=include_comments,
                        reader=reader)

def read_tests_from_stdin(
    *,
    include_comments: bool = False,
    reader: FileReader = lambda path: path.read_text()
) -> typing.Iterable[str]:
    """Reads lines from standard input handling `./<path>` includes.

    Behaves like `read_tests_from_file` but rather than reading contents of
    given file reads lines from standard input.  `./<path>` includes are
    resolved relative to current working directory.

    Returns:
        An iterable over lines in the given file.  All lines are stripped of
        leading and trailing white space.
    """
    return __read_tests(sys.stdin,
                        filename='<stdin>',
                        dirpath=pathlib.Path.cwd(),
                        include_comments=include_comments,
                        reader=reader)

def __read_tests(
    lines: typing.Iterable[str],
    *,
    filename: typing.Union[str, pathlib.Path],
    dirpath: pathlib.Path,
    include_comments: bool = False,
    reader: FileReader = lambda path: path.read_text()
) -> typing.Iterable[str]:

    def impl(lines: typing.Iterable[str],
             filename: typing.Union[str, pathlib.Path],
             dirpath: pathlib.Path,
             depth: int = 1,
             comment: bool = False) -> typing.Iterable[str]:
        for lineno, line in enumerate(lines, start=1):
            line = line.rstrip()
            if line.startswith('./') or (include_comments and
                                         line.startswith('#./')):
                if depth == 3:
                    print(f'{filename}:{lineno}: ignoring {line}; '
                          f'would exceed depth limit of {depth}')
                else:
                    incpath = dirpath / line.lstrip('#')
                    yield from impl(
                        reader(incpath).splitlines(), incpath, incpath.parent,
                        depth + 1, comment or line.startswith('#'))
            elif include_comments or (line and line[0] != '#'):
                if comment and not line.startswith('#'):
                    line = '#' + line
                yield line

    return impl(lines, filename, dirpath)

def github_auth(code_path: pathlib.Path):
    print('Go to the following link in your browser:\n\n{}/login/cli\n'.format(
        NAYDUCK_BASE_HREF))
    code = getpass.getpass('Enter authorisation code: ')
    code_path.parent.mkdir(parents=True, exist_ok=True)
    code_path.write_text(code)
    return code

def _parse_timeout(timeout: typing.Optional[str]) -> typing.Optional[int]:
    """Parses timeout interval and converts it into number of seconds.

    Args:
        timeout: An integer with an optional ‘h’, ‘m’ or ‘s’ suffix which
            multiply the integer by 3600, 60 and 1 respectively.
    Returns:
        Interval in seconds.
    """
    if not timeout:
        return None
    mul_ary = {'h': 3600, 'm': 60, 's': 1}
    mul = mul_ary.get(timeout[-1])
    if mul:
        timeout = timeout[:-1]
    else:
        mul = 1
    return int(timeout) * mul

def run_locally(args, tests):
    for test in tests:
        # See nayduck specs at https://github.com/Near-One/nayduck/blob/master/lib/testspec.py
        fields = test.split()

        timeout = None
        index = 1
        ignored = []
        while len(fields) > index and fields[index].startswith('--'):
            if fields[index].startswith("--timeout="):
                timeout = fields[index][10:]
            elif fields[index] != '--skip-build':
                ignored.append(fields[index])
            index += 1

        del fields[1:index]
        message = f'Running ‘{"".join(fields)}’'
        if ignored:
            message = f'{message} (ignoring flags ‘{" ".join(ignored)}`)'
        if not args.dry_run:
            print(message)

        if fields[0] == 'expensive':
            # TODO --test doesn't work
            cmd = [
                'cargo',
                'test',
                '-p',
                fields[1],  # '--test', fields[2],
                '--features',
                'expensive_tests',
                '--',
                '--exact',
                fields[3]
            ]
            cwd = REPO_DIR
        elif fields[0] in ('pytest', 'mocknet'):
            fields[0] = sys.executable
            fields[1] = os.path.join('tests', fields[1])
            cmd = fields
            cwd = REPO_DIR / 'pytest'
        else:
            print(f'Unrecognised test category ‘{fields[0]}’', file=sys.stderr)
            continue
        if args.dry_run:
            print('( cd {} && {} )'.format(
                shlex.quote(str(cwd)),
                ' '.join(shlex.quote(str(arg)) for arg in cmd)))
            continue
        print(f"RUNNING COMMAND cwd = {cwd} cmd = {cmd}")
        subprocess.check_call(cmd, cwd=cwd, timeout=_parse_timeout(timeout))

def run_command(args, tests):
    import requests

    try:
        import colorama
        styles = (colorama.Fore.RED, colorama.Fore.GREEN,
                  colorama.Style.RESET_ALL)
    except ImportError:
        styles = ('', '', '')

    code_path = pathlib.Path(
        os.environ.get('XDG_CONFIG_HOME') or
        pathlib.Path.home() / '.config') / 'nayduck-code'
    if code_path.is_file():
        code = code_path.read_text().strip()
    else:
        code = github_auth(code_path)

    if args.dry_run:
        for test in tests:
            print(test)
        return

    if args.branch:
        test_branch = args.branch
        test_sha = get_sha(test_branch).strip()

    else:
        test_sha = args.sha or get_sha("HEAD").strip()
        test_branch = get_branch(test_sha)

    post = {'branch': test_branch, 'sha': test_sha, 'tests': list(tests)}

    print("Scheduling tests for: \n"
          f"branch - {post['branch']} \n"
          f"commit hash - {post['sha']}")

    while True:
        print('Sending request ...')
        if args.cancel:
            res = requests.post(NAYDUCK_BASE_HREF +
                                f'/api/run/{args.cancel}/cancel',
                                cookies={'nay-code': code})
        else:
            res = requests.post(NAYDUCK_BASE_HREF + '/api/run/new',
                                json=post,
                                cookies={'nay-code': code})
        if res.status_code != 401:
            break
        print(f'{styles[0]}Unauthorised.{styles[2]}\n')
        code = github_auth(code_path)

    if res.status_code == 200:
        json_res = json.loads(res.text)
        if args.cancel:
            print(styles[1] + 'Cancelled ' + str(json_res) + ' test(s)' +
                  styles[2])
        else:
            print(styles[json_res['code'] == 0] + json_res['response'] +
                  styles[2])
    else:
        print(f'{styles[0]}Got status code {res.status_code}:{styles[2]}\n')
        print(res.text)
    code = res.cookies.get('nay-code')
    if code:
        code_path.write_text(code)

def main():
    args = _parse_args()

    if args.stdin:
        tests = list(read_tests_from_stdin())
    else:
        tests = list(read_tests_from_file(pathlib.Path(args.test_file)))

    if args.run_locally:
        run_locally(args, tests)
    else:
        run_command(args, tests)

if __name__ == "__main__":
    main()

'''
'''--- nightly/nightly.txt ---
./sandbox.txt
./pytest.txt
./expensive.txt

'''
'''--- nightly/pytest-adversarial.txt ---
pytest --timeout=600 adversarial/fork_sync.py
pytest --timeout=600 adversarial/fork_sync.py --features nightly
pytest adversarial/malicious_chain.py
pytest adversarial/malicious_chain.py --features nightly
pytest adversarial/malicious_chain.py valid_blocks_only
pytest adversarial/malicious_chain.py valid_blocks_only --features nightly
pytest adversarial/start_from_genesis.py
pytest adversarial/start_from_genesis.py --features nightly
pytest adversarial/start_from_genesis.py overtake
pytest adversarial/start_from_genesis.py overtake --features nightly
pytest adversarial/start_from_genesis.py doomslug_off
pytest adversarial/start_from_genesis.py doomslug_off --features nightly
# TODO(#6421): Currently broken.
#pytest adversarial/start_from_genesis.py overtake doomslug_off
#pytest adversarial/start_from_genesis.py overtake doomslug_off --features nightly

# TODO(#4618): Those tests are currently broken.  Comment out while we’re
# working on a fix.
#pytest adversarial/gc_rollback.py
#pytest adversarial/gc_rollback.py --features nightly

'''
'''--- nightly/pytest-contracts.txt ---
# python tests for smart contract deployment and invocation
pytest contracts/deploy_call_smart_contract.py
pytest contracts/deploy_call_smart_contract.py --features nightly
pytest --timeout=10m contracts/gibberish.py
pytest --timeout=10m contracts/gibberish.py --features nightly
pytest --timeout=400 contracts/infinite_loops.py
pytest --timeout=400 contracts/infinite_loops.py --features nightly

'''
'''--- nightly/pytest-sanity.txt ---
# python sanity tests
pytest sanity/simple.py
pytest sanity/simple.py --features nightly
pytest sanity/block_production.py
pytest sanity/block_production.py --features nightly
pytest sanity/transactions.py
pytest sanity/transactions.py --features nightly
pytest sanity/staking1.py
pytest sanity/staking1.py --features nightly
pytest --timeout=800 sanity/staking2.py
pytest --timeout=800 sanity/staking2.py --features nightly
pytest --timeout=800 sanity/staking_repro1.py
pytest --timeout=800 sanity/staking_repro1.py --features nightly
pytest --timeout=800 sanity/staking_repro2.py
pytest --timeout=800 sanity/staking_repro2.py --features nightly
pytest sanity/epoch_switches.py
pytest sanity/epoch_switches.py --features nightly
pytest sanity/state_sync.py manytx 30
pytest sanity/state_sync.py manytx 30 --features nightly
pytest --timeout=600 sanity/state_sync.py manytx 265
pytest --timeout=600 sanity/state_sync.py manytx 265 --features nightly
pytest sanity/state_sync.py onetx 30
pytest sanity/state_sync.py onetx 30 --features nightly
pytest --timeout=600 sanity/state_sync.py onetx 265
pytest --timeout=600 sanity/state_sync.py onetx 265 --features nightly
pytest --timeout=240 sanity/state_sync1.py
pytest --timeout=240 sanity/state_sync1.py --features nightly
# TODO(#4618): Those tests are currently broken.  Comment out while we’re
# working on a fix / deciding whether to remove them.
#pytest --timeout=900 sanity/state_sync2.py
#pytest --timeout=900 sanity/state_sync2.py nightly --features nightly
pytest --timeout=1200 sanity/state_sync3.py
pytest --timeout=1200 sanity/state_sync3.py --features nightly
pytest --timeout=240 sanity/state_sync4.py
pytest --timeout=240 sanity/state_sync4.py --features nightly
pytest --timeout=240 sanity/state_sync5.py
pytest --timeout=240 sanity/state_sync5.py --features nightly
pytest --timeout=600 sanity/state_sync_routed.py manytx 115
pytest --timeout=600 sanity/state_sync_routed.py manytx 115 --features nightly
# TODO(#4618): Those tests are currently broken.  Comment out while we’re
# working on a fix / deciding whether to remove them.
#pytest --timeout=300 sanity/state_sync_late.py notx
#pytest --timeout=300 sanity/state_sync_late.py notx --features nightly

pytest --timeout=3600 sanity/state_sync_massive.py
pytest --timeout=3600 sanity/state_sync_massive_validator.py
pytest --timeout=3600 sanity/state_sync_massive.py --features nightly
pytest --timeout=3600 sanity/state_sync_massive_validator.py --features nightly

pytest sanity/sync_chunks_from_archival.py
pytest sanity/sync_chunks_from_archival.py --features nightly
pytest sanity/rpc_tx_forwarding.py
pytest sanity/rpc_tx_forwarding.py --features nightly
pytest --timeout=480 sanity/skip_epoch.py
pytest --timeout=480 sanity/skip_epoch.py --features nightly
pytest --timeout=240 sanity/one_val.py
pytest --timeout=240 sanity/one_val.py nightly --features nightly
pytest --timeout=240 sanity/lightclnt.py
pytest --timeout=240 sanity/lightclnt.py --features nightly
pytest sanity/rpc_light_client_execution_outcome_proof.py
pytest sanity/rpc_light_client_execution_outcome_proof.py --features nightly
pytest --timeout=240 sanity/block_sync.py
pytest --timeout=240 sanity/block_sync.py --features nightly
pytest --timeout=10m sanity/block_sync_archival.py
pytest --timeout=10m sanity/block_sync_archival.py --features nightly
pytest --timeout=120 sanity/block_sync_flat_storage.py
pytest --timeout=120 sanity/block_sync_flat_storage.py --features nightly
pytest --timeout=240 sanity/state_sync_epoch_boundary.py
pytest --timeout=240 sanity/state_sync_epoch_boundary.py --features nightly
pytest --timeout=240 sanity/state_sync_then_catchup.py
pytest --timeout=240 sanity/state_sync_then_catchup.py --features nightly
pytest --timeout=240 sanity/state_parts_dump_check.py
pytest --timeout=240 sanity/state_parts_dump_check.py --features nightly
pytest --timeout=120 sanity/catchup_flat_storage_deletions.py
pytest --timeout=120 sanity/catchup_flat_storage_deletions.py --features nightly
pytest --timeout=240 sanity/validator_switch.py
pytest --timeout=240 sanity/validator_switch.py --features nightly
pytest --timeout=240 sanity/rpc_state_changes.py
pytest --timeout=240 sanity/rpc_state_changes.py --features nightly
pytest sanity/rpc_max_gas_burnt.py
pytest sanity/rpc_max_gas_burnt.py --features nightly
pytest sanity/rpc_tx_status.py
pytest sanity/rpc_tx_status.py --features nightly
pytest --timeout=120 sanity/garbage_collection.py
pytest --timeout=120 sanity/garbage_collection.py --features nightly
pytest --timeout=120 sanity/garbage_collection1.py
pytest --timeout=120 sanity/garbage_collection1.py --features nightly
pytest --timeout=120 sanity/garbage_collection_intense.py
pytest --timeout=120 sanity/garbage_collection_intense.py --features nightly
pytest --timeout=120 sanity/garbage_collection_archival.py
pytest --timeout=120 sanity/garbage_collection_archival.py --features nightly
pytest --timeout=300 sanity/gc_after_sync.py
pytest --timeout=300 sanity/gc_after_sync.py --features nightly
pytest --timeout=300 sanity/gc_after_sync1.py
pytest --timeout=300 sanity/gc_after_sync1.py --features nightly
pytest --timeout=300 sanity/gc_sync_after_sync.py
pytest --timeout=300 sanity/gc_sync_after_sync.py --features nightly
pytest --timeout=300 sanity/gc_sync_after_sync.py swap_nodes
pytest --timeout=300 sanity/gc_sync_after_sync.py swap_nodes --features nightly
pytest --timeout=300 sanity/large_messages.py
pytest --timeout=300 sanity/large_messages.py --features nightly
pytest --timeout=120 sanity/handshake_tie_resolution.py
pytest --timeout=120 sanity/handshake_tie_resolution.py --features nightly
pytest sanity/repro_2916.py
pytest sanity/repro_2916.py --features nightly
pytest --timeout=240 sanity/switch_node_key.py
pytest --timeout=240 sanity/switch_node_key.py --features nightly
pytest --timeout=240 sanity/validator_switch_key.py
pytest --timeout=240 sanity/validator_switch_key.py --features nightly
pytest sanity/proxy_simple.py
pytest sanity/proxy_simple.py --features nightly
pytest sanity/proxy_restart.py
pytest sanity/proxy_restart.py --features nightly
pytest sanity/network_drop_package.py
pytest sanity/network_drop_package.py --features nightly
pytest --timeout=900 sanity/sync_ban.py true
pytest --timeout=900 sanity/sync_ban.py true --features nightly
pytest --timeout=900 sanity/sync_ban.py false
pytest --timeout=900 sanity/sync_ban.py false --features nightly
pytest sanity/block_chunk_signature.py
pytest sanity/block_chunk_signature.py --features nightly
pytest sanity/concurrent_function_calls.py
pytest sanity/concurrent_function_calls.py --features nightly
pytest sanity/proxy_example.py
pytest sanity/proxy_example.py --features nightly
pytest sanity/rpc_tx_submission.py
pytest sanity/rpc_tx_submission.py --features nightly
pytest sanity/state_sync_fail.py
pytest sanity/state_sync_fail.py --features nightly

pytest sanity/restart.py
pytest sanity/restart.py --features nightly
pytest sanity/rpc_finality.py
pytest sanity/rpc_finality.py --features nightly

pytest sanity/rpc_hash.py
pytest sanity/rpc_hash.py --features nightly

# Rosetta RPC tests
pytest sanity/rosetta.py
pytest sanity/rosetta.py --features nightly

# Make sure Docker image can be build and run
pytest --skip-build --timeout=1h sanity/docker.py

pytest sanity/recompress_storage.py
pytest sanity/recompress_storage.py --features nightly

# This is the test for meta transactions.
pytest sanity/meta_tx.py --features nightly

# Tests for split storage and split storage migration
pytest --timeout=600 sanity/split_storage.py
pytest --timeout=600 sanity/split_storage.py --features nightly

# Tests for resharding
pytest --timeout=120 sanity/resharding.py
pytest --timeout=120 sanity/resharding_rpc_tx.py
pytest --timeout=120 sanity/resharding_restart.py
pytest --timeout=120 sanity/resharding_error_handling.py

# Tests for resharding in nightly are disabled because resharding is not
# compatible with stateless validation. 
# pytest --timeout=120 sanity/resharding.py --features nightly
# pytest --timeout=120 sanity/resharding_rpc_tx.py --features nightly
# pytest --timeout=120 sanity/resharding_restart.py --features nightly
# pytest --timeout=120 sanity/resharding_error_handling.py --features nightly

'''
'''--- nightly/pytest-spec.txt ---
# python spec tests
pytest spec/network/peers_request.py
pytest spec/network/peers_request.py --features nightly

'''
'''--- nightly/pytest.txt ---
./pytest-adversarial.txt
./pytest-contracts.txt
./pytest-sanity.txt
./pytest-spec.txt

'''
'''--- nightly/sandbox.txt ---
# python sandbox node tests
pytest sandbox/patch_state.py --features sandbox
pytest sandbox/fast_forward.py --features sandbox
pytest sandbox/fast_forward_epoch_boundary.py --features sandbox

'''
'''--- pytest/__init__.py ---

'''
'''--- pytest/endtoend/__init__.py ---

'''
'''--- pytest/endtoend/endtoend.py ---
#!/usr/bin/env python3
"""
End-to-end test for canary nodes.
Assuming each canary node has an account with positive balance.
Periodically (1 minute) each account sends itself 0 tokens. Some tokens get burned in the process.
Account balances get exported to prometheus and can be used to detect when transactions stop affecting the world, aka the chain of testnet or mainnet.
We observe effects on the chain using public RPC endpoints to ensure canaries are running a fork.

python3 endtoend/endtoend.py
    --ips <ip_node1,ip_node2>
    --accounts <account_node1,account_node2>
    --interval-sec 60
    --port 3030
    --public-key <public_key of test accounts>
    --private-key <private key of test account>
    --rpc-server-addr rpc.testnet.near.org
    --rpc-server-port 80
    --metrics-port 3040
"""
import argparse
import random
import sys
import time
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[1] / 'lib'))

import account as account_mod
import key as key_mod
import mocknet_helpers
from concurrent.futures import ThreadPoolExecutor

from configured_logger import logger
import prometheus_client

balance_gauge = prometheus_client.Gauge(
    'near_e2e_account_balance',
    'Balance of the test accounts running an end-to-end test. Measured in yoctonear.',
    ['account'])

def do_ping(account, rpc_server):
    account_id = account.key.account_id
    base_block_hash = mocknet_helpers.get_latest_block_hash(addr=rpc_server[0],
                                                            port=rpc_server[1])
    balance = mocknet_helpers.retry_and_ignore_errors(
        lambda: mocknet_helpers.get_amount_yoctonear(
            account_id, addr=rpc_server[0], port=rpc_server[1]))
    if balance is not None:
        balance_gauge.labels(account=account_id).set(balance)
    logger.info(f'Sending 0 tokens from {account_id} to itself')
    tx_res = mocknet_helpers.retry_and_ignore_errors(
        lambda: account.send_transfer_tx(
            account_id, transfer_amount=0, base_block_hash=base_block_hash))
    logger.info(
        f'Sending 0 tokens from {account_id} to itself, tx result: {tx_res}')

def pinger(account, interval, rpc_server):
    account_id = account.key.account_id
    logger.info(f'pinger {account_id}')
    time.sleep(random.random() * interval)
    logger.info(f'pinger {account_id} woke up')
    while True:
        try:
            do_ping(account, rpc_server)
        except Exception as ex:
            logger.warning(f'Error pinging account {account_id}', exc_info=ex)
        time.sleep(interval)

if __name__ == '__main__':
    logger.info('Starting end-to-end test.')
    parser = argparse.ArgumentParser(description='Run an end-to-end test')
    parser.add_argument('--ips', required=True)
    parser.add_argument('--accounts', required=True)
    parser.add_argument('--interval-sec', type=float, required=True)
    parser.add_argument('--port', type=int, required=True)
    parser.add_argument('--public-key', required=True)
    parser.add_argument('--private-key', required=True)
    parser.add_argument('--rpc-server-addr', required=True)
    parser.add_argument('--rpc-server-port', required=True)
    parser.add_argument('--metrics-port', type=int, required=True)

    args = parser.parse_args()

    assert args.ips
    ips = args.ips.split(',')
    assert args.accounts
    account_ids = args.accounts.split(',')
    assert len(account_ids) == len(
        ips), 'List of test accounts must match the list of node IP addresses'
    interval_sec = args.interval_sec
    assert interval_sec > 1, 'Need at least 1 second between pings'
    port = args.port
    pk, sk = args.public_key, args.private_key
    rpc_server = (args.rpc_server_addr, args.rpc_server_port)

    keys = [key_mod.Key(account_id, pk, sk) for account_id in account_ids]
    base_block_hash = mocknet_helpers.get_latest_block_hash(addr=rpc_server[0],
                                                            port=rpc_server[1])
    accounts = []
    for ip, key in zip(ips, keys):
        nonce = mocknet_helpers.get_nonce_for_pk(key.account_id,
                                                 key.pk,
                                                 addr=rpc_server[0],
                                                 port=rpc_server[1])
        accounts.append(
            account_mod.Account(key,
                                nonce,
                                base_block_hash,
                                rpc_infos=[(ip, port)]))

    prometheus_client.start_http_server(args.metrics_port)

    with ThreadPoolExecutor() as executor:
        for account in accounts:
            executor.submit(pinger, account, interval_sec, rpc_server)

'''
'''--- pytest/lib/__init__.py ---

'''
'''--- pytest/lib/account.py ---
import base64
import json
import requests
import random
import time

from transaction import (
    sign_payment_tx, sign_deploy_contract_tx, sign_function_call_tx,
    sign_create_account_with_full_access_key_and_balance_tx, sign_staking_tx)
from key import Key
from utils import load_binary_file
from configured_logger import logger

# Constant for 1 NEAR
NEAR_BASE = 10**24
TGAS = 10**12

class Account:

    def __init__(self,
                 key,
                 init_nonce,
                 base_block_hash,
                 rpc_info=None,
                 rpc_infos=None):
        """
        `rpc_info` takes precedence over `rpc_infos` for compatibility. One of them must be set.
        If `rpc_info` is set, only this RPC node will be contacted.
        Otherwise if `rpc_infos` is set, an RPC node will be selected randomly
        from that set for each transaction attempt.
        """
        self.key = key
        self.nonce = init_nonce
        self.base_block_hash = base_block_hash
        assert rpc_info or rpc_infos
        if rpc_info:
            assert not rpc_infos
            rpc_infos = [rpc_info]
        self.rpc_infos = rpc_infos
        assert key.account_id
        self.tx_timestamps = []
        logger.debug(
            f'Creating Account {key.account_id} {init_nonce} {self.rpc_infos[0]} {key.pk} {key.sk}'
        )

    # Returns an address of a random known RPC node.
    def get_rpc_node_address(self):
        rpc_addr, rpc_port = random.choice(self.rpc_infos)
        return f'http://{rpc_addr}:{rpc_port}'

    def json_rpc(self, method, params):
        j = {
            'method': method,
            'params': params,
            'id': 'dontcare',
            'jsonrpc': '2.0'
        }
        r = requests.post(self.get_rpc_node_address(), json=j, timeout=30)
        return json.loads(r.content)

    def send_tx(self, signed_tx):
        return self.json_rpc('broadcast_tx_async',
                             [base64.b64encode(signed_tx).decode('utf-8')])

    def send_tx_sync(self, signed_tx):
        return self.json_rpc('broadcast_tx_commit',
                             [base64.b64encode(signed_tx).decode('utf-8')])

    def prep_tx(self):
        self.tx_timestamps.append(time.time())
        self.nonce += 1

    def send_transfer_tx(self,
                         dest_account_id,
                         transfer_amount=100,
                         base_block_hash=None):
        self.prep_tx()
        tx = sign_payment_tx(self.key, dest_account_id, transfer_amount,
                             self.nonce, base_block_hash or
                             self.base_block_hash)
        return self.send_tx(tx)

    def send_deploy_contract_tx(self, wasm_filename, base_block_hash=None):
        wasm_binary = load_binary_file(wasm_filename)
        self.prep_tx()
        tx = sign_deploy_contract_tx(self.key, wasm_binary, self.nonce,
                                     base_block_hash or self.base_block_hash)
        return self.send_tx(tx)

    def send_call_contract_tx(self, method_name, args, base_block_hash=None):
        return self.send_call_contract_raw_tx(self.key.account_id,
                                              method_name,
                                              args,
                                              0,
                                              base_block_hash=base_block_hash)

    def send_call_contract_raw_tx(self,
                                  contract_id,
                                  method_name,
                                  args,
                                  deposit,
                                  base_block_hash=None):
        self.prep_tx()
        tx = sign_function_call_tx(self.key, contract_id, method_name, args,
                                   300 * TGAS, deposit, self.nonce,
                                   base_block_hash or self.base_block_hash)
        return self.send_tx(tx)

    def send_call_contract_raw_tx_sync(self,
                                       contract_id,
                                       method_name,
                                       args,
                                       deposit,
                                       base_block_hash=None):
        self.prep_tx()
        tx = sign_function_call_tx(self.key, contract_id, method_name, args,
                                   300 * TGAS, deposit, self.nonce,
                                   base_block_hash or self.base_block_hash)
        return self.send_tx_sync(tx)

    def send_create_account_tx(self, new_account_id, base_block_hash=None):
        self.prep_tx()
        new_key = Key(new_account_id, self.key.pk, self.key.sk)
        tx = sign_create_account_with_full_access_key_and_balance_tx(
            self.key, new_account_id, new_key, 100 * NEAR_BASE, self.nonce,
            base_block_hash or self.base_block_hash)
        return self.send_tx(tx)

    def send_stake_tx(self, stake_amount, base_block_hash=None):
        self.prep_tx()
        tx = sign_staking_tx(self.key, self.key, stake_amount, self.nonce,
                             base_block_hash or self.base_block_hash)
        return self.send_tx(tx)

    def get_amount_yoctonear(self):
        j = self.json_rpc(
            'query', {
                'request_type': 'view_account',
                'finality': 'optimistic',
                'account_id': self.key.account_id
            })
        return int(j.get('result', {}).get('amount', 0))

'''
'''--- pytest/lib/branches.py ---
import os
import pathlib
import subprocess
import sys
import tempfile
import typing

import requests
import semver
from configured_logger import logger

_UNAME = os.uname()[0]
_IS_DARWIN = _UNAME == 'Darwin'
_BASEHREF = 'https://s3-us-west-1.amazonaws.com/build.nearprotocol.com'
_REPO_DIR = pathlib.Path(__file__).resolve().parents[2]
_OUT_DIR = _REPO_DIR / 'target/debug'
_IS_NAYDUCK = bool(os.getenv('NAYDUCK'))

def current_branch() -> str:
    """Returns checked out branch name or sha if we’re on detached head."""
    branch = os.environ.get('BUILDKITE_BRANCH')
    if branch:
        return branch
    try:
        return subprocess.check_output(
            ('git', 'symbolic-ref', '--short', '-q', 'HEAD')).strip().decode()
    except subprocess.CalledProcessError as ex:
        if ex.returncode != 1:
            raise
        # We’re on detached HEAD
    return subprocess.check_output(('git', 'rev-parse', '@')).strip().decode()

def __get_latest_deploy(chain_id: str) -> typing.Tuple[str, str]:
    """Returns latest (release, deploy) for given chain.

    Gets latest release and deploy identifier from S3 for given chain.  Those
    can be used to uniquely identify a neard executable running on the chain.
    """

    def download(url: str) -> str:
        logger.info(f"download {url}")
        res = requests.get(url)
        res.raise_for_status()
        return res.text

    basehref = f'{_BASEHREF}/nearcore-deploy/{chain_id}'
    release = download(f'{basehref}/latest_release')
    deploy = download(f'{basehref}/latest_deploy')

    if release != 'master':
        # Make sure it parses as a version
        release = str(semver.VersionInfo.parse(release).finalize_version())

    return release, deploy

class Executables(typing.NamedTuple):
    root: pathlib.Path
    neard: pathlib.Path

    def node_config(self) -> typing.Dict[str, typing.Any]:
        return {
            'local': True,
            'neard_root': self.root,
            'binary_name': self.neard.name
        }

def _compile_binary(branch: str) -> Executables:
    """For given branch, compile binary.

    Stashes current changes, switches branch and then returns everything back.
    """
    # TODO: download pre-compiled binary from github for beta/stable?
    prev_branch = current_branch()
    stash_output = subprocess.check_output(['git', 'stash'])
    try:
        subprocess.check_output([
            'git',
            # When checking out old releases we end up in a detached head state
            # and git prints a scary warning about that. This config silences it.
            '-c',
            'advice.detachedHead=false',
            'checkout',
            str(branch),
        ])
        try:
            subprocess.check_output(['git', 'pull', 'origin', str(branch)])
            result = _compile_current(branch)
        finally:
            subprocess.check_output(['git', 'checkout', prev_branch])
    finally:
        if stash_output != b"No local changes to save\n":
            subprocess.check_output(['git', 'stash', 'pop'])
    return result

def escaped(branch):
    return branch.replace('/', '-')

def _compile_current(branch: str) -> Executables:
    """Compile current branch."""
    prebuilt_neard = os.environ.get("CURRENT_NEARD")
    if prebuilt_neard is not None:
        logger.info(
            f'Using `CURRENT_NEARD={prebuilt_neard}` neard for branch {branch}')
        try:
            path = pathlib.Path(prebuilt_neard).resolve()
            return Executables(path.parent, path)
        except OSError as e:
            logger.exception('Could not use `CURRENT_NEARD`, will build…')

    logger.info(f'Building neard for branch {branch}')
    subprocess.check_call(['cargo', 'build', '-p', 'neard', '--bin', 'neard'],
                          cwd=_REPO_DIR)
    subprocess.check_call(['cargo', 'build', '-p', 'near-test-contracts'],
                          cwd=_REPO_DIR)
    branch = escaped(branch)
    neard = _OUT_DIR / f'neard-{branch}'
    (_OUT_DIR / 'neard').rename(neard)
    return Executables(_OUT_DIR, neard)

def patch_binary(binary: pathlib.Path) -> None:
    """
    Patch a specified external binary if doing so is necessary for the host
    system to be able to run it.

    Currently only supports NixOS.
    """
    # Are we running on NixOS and require patching…?
    try:
        with open('/etc/os-release', 'r') as f:
            if not any(line.strip() == 'ID=nixos' for line in f):
                return
    except FileNotFoundError:
        return
    if os.path.exists('/lib'):
        return
    # Build an output with patchelf and interpreter in it
    nix_expr = '''
    with (import <nixpkgs> {});
    symlinkJoin {
      name = "nearcore-dependencies";
      paths = [patchelf stdenv.cc.bintools gcc.cc.lib];
    }
    '''
    path = subprocess.run(('nix-build', '-E', nix_expr),
                          capture_output=True,
                          encoding='utf-8').stdout.strip()
    # Set the interpreter for the binary to NixOS'.
    patchelf = f'{path}/bin/patchelf'
    linker = (pathlib.Path(path) / "nix-support" /
              "dynamic-linker").read_text().strip()
    cmd = (patchelf, '--set-interpreter', linker, binary)
    logger.debug('Patching NixOS interpreter {}'.format(cmd))
    subprocess.check_call(cmd)
    cmd = (patchelf, '--set-rpath', '$ORIGIN:{}/lib'.format(path), binary)
    logger.debug('Patching DSO rpath {}'.format(cmd))
    subprocess.check_call(cmd)

def __download_file_if_missing(filename: pathlib.Path, url: str) -> None:
    """Downloads a file from given URL if it does not exist already.

    Does nothing if file `filename` already exists.  Otherwise, downloads data
    from `url` and saves them in `filename`.  Downloading is done with `curl`
    tool and on failure (i.e. if it returns non-zero exit code) `filename` is
    not created.  On success, the file’s mode is set to 0x555 (i.e. readable and
    executable by anyone).

    Args:
        filename: Path to the file.
        url: URL of the file to download (if the file is missing).
    """
    if filename.exists():
        if not filename.is_file():
            sys.exit(f'{filename} exists but is not a file')
        return

    proto = '"=https"' if _IS_DARWIN else '=https'
    cmd = ('curl', '--proto', proto, '--tlsv1.2', '-sSfL', url)
    name = None
    filename.parent.mkdir(parents=True, exist_ok=True)
    try:
        with tempfile.NamedTemporaryFile(dir=filename.parent,
                                         delete=False) as tmp:
            name = pathlib.Path(tmp.name)
            logger.debug('Executing ' + ' '.join(cmd))
            subprocess.check_call(cmd, stdout=tmp)
        patch_binary(name)
        name.chmod(0o555)
        name.rename(filename)
        name = None
    finally:
        if name:
            name.unlink()

def __download_binary(release: str, deploy: str) -> Executables:
    """Download binary for given release and deploy."""
    logger.info(f'Getting neard for {release}@{_UNAME} (deploy={deploy})')
    neard = _OUT_DIR / f'neard-{release}-{deploy}'
    basehref = f'{_BASEHREF}/nearcore/{_UNAME}/{release}/{deploy}'
    __download_file_if_missing(neard, f'{basehref}/neard')
    return Executables(_OUT_DIR, neard)

class ABExecutables(typing.NamedTuple):
    stable: Executables
    current: Executables
    release: str
    deploy: str

def prepare_ab_test(chain_id: str = 'mainnet') -> ABExecutables:
    """Prepares executable at HEAD and latest deploy at given chain.

    Args:
        chain_id: Chain id to get latest deployed executable for.  Can be
            ‘master’, ‘testnet’ or ‘betanet’.
    Returns:
        An ABExecutables object where `current` describes executable built at
        current HEAD while `stable` points at executable which is deployed in
        production at given chain.  `release` and `deploy` of the returned
        object specify, well, the latest release and deploy running in
        production at the chain.
    """
    release, deploy, stable = __get_executables_for(chain_id)

    if _IS_NAYDUCK:
        # On NayDuck the file is fetched from a builder host so there’s no need
        # to build it.
        current = Executables(_OUT_DIR, _OUT_DIR / 'neard')
    else:
        current = _compile_current(current_branch())

    return ABExecutables(stable=stable,
                         current=current,
                         release=release,
                         deploy=deploy)

def __get_executables_for(chain_id: str) -> typing.Tuple[str, str, Executables]:
    """Returns latest deploy at given chain."""
    if chain_id not in ('mainnet', 'testnet', 'betanet'):
        raise ValueError(f'Unexpected chain_id: {chain_id}; '
                         'expected mainnet, testnet or betanet')

    release, deploy = __get_latest_deploy(chain_id)
    try:
        executable = __download_binary(release, deploy)
    except Exception as e:
        if _IS_NAYDUCK:
            logger.exception('RC binary should be downloaded for NayDuck.', e)
        else:
            logger.exception(e)
        executable = _compile_binary(release)
    return release, deploy, executable

def get_executables_for(chain_id: str) -> Executables:
    """Prepares executable at HEAD and latest deploy at given chain.

    Args:
        chain_id: Chain id to get latest deployed executable for.  Can be
            ‘master’, ‘testnet’ or ‘betanet’.
    Returns:
        An Executables object where pointing at executable which is deployed in
        production at given chain.
    """
    _, _, executable = __get_executables_for(chain_id)
    return executable

'''
'''--- pytest/lib/cluster.py ---
import atexit
import base64
import json
import os
import pathlib
import rc
import requests
import shutil
import signal
import subprocess
import sys
import threading
import time
import traceback
import typing
import uuid
from rc import gcloud
from retrying import retry

import base58

import network
from configured_logger import logger
from key import Key
from proxy import NodesProxy

os.environ["ADVERSARY_CONSENT"] = "1"

remote_nodes = []
remote_nodes_lock = threading.Lock()
cleanup_remote_nodes_atexit_registered = False

class DownloadException(Exception):
    pass

def atexit_cleanup(node):
    logger.info("Cleaning up node %s:%s on script exit" % node.addr())
    logger.info("Executed store validity tests: %s" % node.store_tests)
    try:
        node.cleanup()
    except:
        logger.info("Cleaning failed!")
        traceback.print_exc()
        pass

def atexit_cleanup_remote():
    with remote_nodes_lock:
        if remote_nodes:
            rc.pmap(atexit_cleanup, remote_nodes)

# custom retry that is used in wait_for_rpc() and get_status()
def nretry(fn, timeout):
    started = time.time()
    delay = 0.05
    while True:
        try:
            return fn()
        except:
            if time.time() - started >= timeout:
                raise
            time.sleep(delay)
            delay *= 1.2

BootNode = typing.Union[None, 'BaseNode', typing.Iterable['BaseNode']]

def make_boot_nodes_arg(boot_node: BootNode) -> typing.Tuple[str]:
    """Converts `boot_node` argument to `--boot-nodes` command line argument.

    If the argument is `None` returns an empty tuple.  Otherwise, returns
    a tuple representing arguments to be added to `neard` invocation for setting
    boot nodes according to `boot_node` argument.

    Apart from `None` as described above, `boot_node` can be a [`BaseNode`]
    object, or an iterable (think list) of [`BaseNode`] objects.  The boot node
    address of a BaseNode object is contstructed using [`BaseNode.addr_with_pk`]
    method.

    If iterable of nodes is given, the `neard` is going to be configured with
    multiple boot nodes.

    Args:
        boot_node: Specification of boot node(s).
    Returns:
        A tuple to add to `neard` invocation specifying boot node(s) if any
        specified.
    """
    if not boot_node:
        return ()
    try:
        it = iter(boot_node)
    except TypeError:
        it = iter((boot_node,))
    nodes = ','.join(node.addr_with_pk() for node in it)
    if not nodes:
        return ()
    return ('--boot-nodes', nodes)

class BlockId(typing.NamedTuple):
    """Stores block’s height and hash.

    The values can be accessed either through properties or by structural
    deconstruction, e.g.:

        block_height, block_hash = block_id
        assert block_height == block_id.height
        assert block_hash == block_id.hash

    Attributes:
        height: Block’s height.
        hash: Block’s hash encoding using base58.
        hash_bytes: Block’s hash decoded as raw bytes.  Note that this attribute
            cannot be accessed through aforementioned deconstruction.
    """
    height: int
    hash: str

    @classmethod
    def from_header(cls, header: typing.Dict[str, typing.Any]) -> 'BlockId':
        return cls(height=int(header['height']), hash=header['hash'])

    @property
    def hash_bytes(self) -> bytes:
        return base58.b58decode(self.hash.encode('ascii'))

    def __str__(self) -> str:
        return f'#{self.height} {self.hash}'

    def __eq__(self, rhs) -> bool:
        return (isinstance(rhs, BlockId) and self.height == rhs.height and
                self.hash == rhs.hash)

class BaseNode(object):

    def __init__(self):
        self._start_proxy = None
        self._proxy_local_stopped = None
        self.proxy = None
        self.store_tests = 0
        self.is_check_store = True

    def change_config(self, overrides: typing.Dict[str, typing.Any]) -> None:
        """Change client config.json of a node by applying given overrides.

        Changes to the configuration need to be made while the node is stopped.
        More precisely, while the changes may be made at any point, the node
        reads the time at startup only.

        The overrides are a dictionary specifying new values for configuration
        keys.  Non-dictionary values are applied directly, while dictionaries
        are non-recursively merged.  For example if the original config is:

            {
                'foo': 42,
                'bar': {'a': 1, 'b': 2, 'c': {'A': 3}},
            }

        and overrides are:

            {
                'foo': 24,
                'bar': {'a': -1, 'c': {'D': 3}, 'd': 1},
            }

        then resulting configuration file will be:

            {
                'foo': 24,
                'bar': {'a': -1, 'b': 2, 'c': {'D': 3}, 'd': 1},
            }

        Args:
            overrides: A dictionary of config overrides.  Non-dictionary values
                are set as is, dictionaries are non-recursively merged.
        Raises:
            NotImplementedError: Currently changing the configuration is
                supported on local node only.
        """
        name = type(self).__name__
        raise NotImplementedError('change_config not supported by ' + name)

    def _get_command_line(self,
                          near_root,
                          node_dir,
                          boot_node: BootNode,
                          binary_name='neard'):
        cmd = (os.path.join(near_root, binary_name), '--home', node_dir, 'run')
        return cmd + make_boot_nodes_arg(boot_node)

    def get_command_for_subprogram(self,
                                   cmd: tuple,
                                   near_root,
                                   node_dir,
                                   binary_name='neard'):
        return (os.path.join(near_root, binary_name), '--home', node_dir) + cmd

    def addr_with_pk(self) -> str:
        pk_hash = self.node_key.pk.split(':')[1]
        host, port = self.addr()
        return '{}@{}:{}'.format(pk_hash, host, port)

    def wait_for_rpc(self, timeout=1):
        nretry(lambda: self.get_status(), timeout=timeout)

    def json_rpc(self, method, params, timeout=2):
        j = {
            'method': method,
            'params': params,
            'id': 'dontcare',
            'jsonrpc': '2.0'
        }
        r = requests.post("http://%s:%s" % self.rpc_addr(),
                          json=j,
                          timeout=timeout)
        r.raise_for_status()
        return json.loads(r.content)

    def send_tx(self, signed_tx):
        return self.json_rpc('broadcast_tx_async',
                             [base64.b64encode(signed_tx).decode('utf8')])

    def send_tx_and_wait(self, signed_tx, timeout):
        return self.json_rpc('broadcast_tx_commit',
                             [base64.b64encode(signed_tx).decode('utf8')],
                             timeout=timeout)

    def get_status(self,
                   check_storage: bool = True,
                   timeout: float = 4,
                   verbose: bool = False):
        r = requests.get("http://%s:%s/status" % self.rpc_addr(),
                         timeout=timeout)
        r.raise_for_status()
        status = json.loads(r.content)
        if verbose:
            logger.info(f'Status: {status}')
        if check_storage and status['sync_info']['syncing'] == False:
            # Storage is not guaranteed to be in consistent state while syncing
            self.check_store()
        if verbose:
            logger.info(status)
        return status

    def get_metrics(self, timeout: float = 4):
        r = requests.get("http://%s:%s/metrics" % self.rpc_addr(),
                         timeout=timeout)
        r.raise_for_status()
        return r.content

    def get_latest_block(self, **kw) -> BlockId:
        sync_info = self.get_status(**kw)['sync_info']
        return BlockId(height=sync_info['latest_block_height'],
                       hash=sync_info['latest_block_hash'])

    def get_all_heights(self):
        hash_ = self.get_latest_block().hash
        heights = []

        while True:
            block = self.get_block(hash_)
            if 'error' in block and 'data' in block[
                    'error'] and 'DB Not Found Error: BLOCK:' in block['error'][
                        'data']:
                break
            elif 'result' not in block:
                logger.info(block)

            height = block['result']['header']['height']
            if height == 0:
                break
            heights.append(height)
            hash_ = block['result']['header']['prev_hash']

        return reversed(heights)

    def get_validators(self, epoch_id=None):
        if epoch_id is None:
            args = [None]
        else:
            args = {'epoch_id': epoch_id}
        return self.json_rpc('validators', args)

    def get_account(self, acc, finality='optimistic', do_assert=True, **kwargs):
        res = self.json_rpc('query', {
            "request_type": "view_account",
            "account_id": acc,
            "finality": finality
        }, **kwargs)
        if do_assert:
            assert 'error' not in res, res

        return res

    def call_function(self,
                      acc,
                      method,
                      args,
                      finality='optimistic',
                      timeout=2):
        return self.json_rpc('query', {
            "request_type": "call_function",
            "account_id": acc,
            "method_name": method,
            "args_base64": args,
            "finality": finality
        },
                             timeout=timeout)

    def get_access_key_list(self, acc, finality='optimistic'):
        return self.json_rpc(
            'query', {
                "request_type": "view_access_key_list",
                "account_id": acc,
                "finality": finality
            })

    def get_nonce_for_pk(self, acc, pk, finality='optimistic'):
        for access_key in self.get_access_key_list(acc,
                                                   finality)['result']['keys']:
            if access_key['public_key'] == pk:
                return access_key['access_key']['nonce']
        return None

    def get_block(self, block_id, **kwargs):
        return self.json_rpc('block', [block_id], **kwargs)

    def get_block_by_height(self, block_height, **kwargs):
        return self.json_rpc('block', {'block_id': block_height}, **kwargs)

    def get_final_block(self, **kwargs):
        return self.json_rpc('block', {'finality': 'final'}, **kwargs)

    def get_chunk(self, chunk_id):
        return self.json_rpc('chunk', [chunk_id])

    def get_tx(self, tx_hash, tx_recipient_id):
        return self.json_rpc('tx', [tx_hash, tx_recipient_id])

    def get_changes_in_block(self, changes_in_block_request):
        return self.json_rpc('EXPERIMENTAL_changes_in_block',
                             changes_in_block_request)

    def get_changes(self, changes_request):
        return self.json_rpc('EXPERIMENTAL_changes', changes_request)

    def validators(self):
        return set(
            map(lambda v: v['account_id'],
                self.get_status()['validators']))

    def stop_checking_store(self):
        logger.warning("Stopping checking Storage for inconsistency for %s:%s" %
                       self.addr())
        self.is_check_store = False

    def check_store(self):
        if self.is_check_store:
            res = self.json_rpc('adv_check_store', [])
            if not 'result' in res:
                # cannot check Storage Consistency for the node, possibly not Adversarial Mode is running
                pass
            else:
                if res['result'] == 0:
                    logger.error(
                        "Storage for %s:%s in inconsistent state, stopping" %
                        self.addr())
                    self.kill()
                self.store_tests += res['result']

class RpcNode(BaseNode):
    """ A running node only interact by rpc queries """

    def __init__(self, host, rpc_port):
        super(RpcNode, self).__init__()
        self.host = host
        self.rpc_port = rpc_port

    def rpc_addr(self):
        return (self.host, self.rpc_port)

class LocalNode(BaseNode):

    def __init__(self,
                 port,
                 rpc_port,
                 near_root,
                 node_dir,
                 blacklist,
                 binary_name=None,
                 single_node=False):
        super(LocalNode, self).__init__()
        self.port = port
        self.rpc_port = rpc_port
        self.near_root = str(near_root)
        self.node_dir = node_dir
        self.binary_name = binary_name or 'neard'
        self.cleaned = False
        self.validator_key = Key.from_json_file(
            os.path.join(node_dir, "validator_key.json"))
        self.node_key = Key.from_json_file(
            os.path.join(node_dir, "node_key.json"))
        self.signer_key = Key.from_json_file(
            os.path.join(node_dir, "validator_key.json"))
        self._process = None

        self.change_config({
            'network': {
                'addr': f'0.0.0.0:{port}',
                'blacklist': blacklist
            },
            'rpc': {
                'addr': f'0.0.0.0:{rpc_port}',
            },
            'consensus': {
                'min_num_peers': int(not single_node)
            },
        })

        atexit.register(atexit_cleanup, self)

    def change_config(self, overrides: typing.Dict[str, typing.Any]) -> None:
        apply_config_changes(self.node_dir, overrides)

    def addr(self):
        return ("127.0.0.1", self.port)

    def rpc_addr(self):
        return ("127.0.0.1", self.rpc_port)

    def start_proxy_if_needed(self):
        if self._start_proxy is not None:
            self._proxy_local_stopped = self._start_proxy()

    def output_logs(self):
        stdout = pathlib.Path(self.node_dir) / 'stdout'
        stderr = pathlib.Path(self.node_dir) / 'stderr'
        if os.environ.get('BUILDKITE'):
            logger.info('=== stdout: ')
            logger.info(stdout.read_text('utf-8', 'replace'))
            logger.info('=== stderr: ')
            logger.info(stderr.read_text('utf-8', 'replace'))
        else:
            logger.info(f'=== stdout: available at {stdout}')
            logger.info(f'=== stderr: available at {stderr}')

    def start(self,
              *,
              boot_node: BootNode = None,
              skip_starting_proxy=False,
              extra_env: typing.Dict[str, str] = dict()):
        cmd = self._get_command_line(
            self.near_root,
            self.node_dir,
            boot_node,
            self.binary_name,
        )

        if self._proxy_local_stopped is not None:
            while self._proxy_local_stopped.value != 2:
                logger.info(f'Waiting for previous proxy instance to close')
                time.sleep(1)

        self.run_cmd(cmd=cmd, extra_env=extra_env)

        if not skip_starting_proxy:
            self.start_proxy_if_needed()

        try:
            self.wait_for_rpc(10)
        except:
            logger.error(
                '=== failed to start node, rpc is not ready in 10 seconds')

    def run_cmd(self, *, cmd: tuple, extra_env: typing.Dict[str, str] = dict()):

        env = os.environ.copy()
        env["RUST_BACKTRACE"] = "1"
        env["RUST_LOG"] = "actix_web=warn,mio=warn,tokio_util=warn,actix_server=warn,actix_http=warn," + env.get(
            "RUST_LOG", "debug")
        env.update(extra_env)
        node_dir = pathlib.Path(self.node_dir)
        self.stdout_name = node_dir / 'stdout'
        self.stderr_name = node_dir / 'stderr'
        with open(self.stdout_name, 'ab') as stdout, \
             open(self.stderr_name, 'ab') as stderr:
            self._process = subprocess.Popen(cmd,
                                             stdin=subprocess.DEVNULL,
                                             stdout=stdout,
                                             stderr=stderr,
                                             env=env)
        self._pid = self._process.pid

    def kill(self, *, gentle=False):
        """Kills the process.  If `gentle` sends SIGINT before killing."""
        if self._proxy_local_stopped is not None:
            self._proxy_local_stopped.value = 1
        if self._process and gentle:
            self._process.send_signal(signal.SIGINT)
            try:
                self._process.wait(5)
                self._process = None
            except subprocess.TimeoutExpired:
                pass
        if self._process:
            self._process.kill()
            self._process.wait(5)
            self._process = None

    def reset_data(self):
        shutil.rmtree(os.path.join(self.node_dir, "data"))

    def reset_validator_key(self, new_key):
        self.validator_key = new_key
        with open(os.path.join(self.node_dir, "validator_key.json"), 'w+') as f:
            json.dump(new_key.to_json(), f)

    def reset_node_key(self, new_key):
        self.node_key = new_key
        with open(os.path.join(self.node_dir, "node_key.json"), 'w+') as f:
            json.dump(new_key.to_json(), f)

    def cleanup(self):
        if self.cleaned:
            return

        try:
            self.kill()
        except:
            logger.critical('Kill failed on cleanup!', exc_info=sys.exc_info())

        # move the node dir to avoid weird interactions with multiple serial test invocations
        target_path = self.node_dir + '_finished'
        if os.path.exists(target_path) and os.path.isdir(target_path):
            shutil.rmtree(target_path)
        os.rename(self.node_dir, target_path)
        self.node_dir = target_path
        self.output_logs()
        self.cleaned = True

    def stop_network(self):
        logger.info(f'Stopping network for process {self._pid}')
        network.stop(self._pid)

    def resume_network(self):
        logger.info(f'Resuming network for process {self._pid}')
        network.resume_network(self._pid)

class GCloudNode(BaseNode):

    def __init__(self, *args, username=None, project=None, ssh_key_path=None):
        if len(args) == 1:
            name = args[0]
            # Get existing instance assume it's ready to run.
            self.instance_name = name
            self.port = 24567
            self.rpc_port = 3030
            self.machine = gcloud.get(name,
                                      username=username,
                                      project=project,
                                      ssh_key_path=ssh_key_path)
            self.ip = self.machine.ip
        elif len(args) == 4:
            # Create new instance from scratch
            instance_name, zone, node_dir, binary = args
            self.instance_name = instance_name
            self.port = 24567
            self.rpc_port = 3030
            self.node_dir = node_dir
            self.machine = gcloud.create(
                name=instance_name,
                machine_type='n1-standard-2',
                disk_size='50G',
                image_project='gce-uefi-images',
                image_family='ubuntu-1804-lts',
                zone=zone,
                firewall_allows=['tcp:3030', 'tcp:24567'],
                min_cpu_platform='Intel Skylake',
                preemptible=False,
            )
            # self.ip = self.machine.ip
            self._upload_config_files(node_dir)
            self._download_binary(binary)
            with remote_nodes_lock:
                global cleanup_remote_nodes_atexit_registered
                if not cleanup_remote_nodes_atexit_registered:
                    atexit.register(atexit_cleanup_remote)
                    cleanup_remote_nodes_atexit_registered = True
        else:
            raise Exception()

    def _upload_config_files(self, node_dir):
        self.machine.run('bash', input='mkdir -p ~/.near')
        self.machine.upload(os.path.join(node_dir, '*.json'),
                            f'/home/{self.machine.username}/.near/')
        self.validator_key = Key.from_json_file(
            os.path.join(node_dir, "validator_key.json"))
        self.node_key = Key.from_json_file(
            os.path.join(node_dir, "node_key.json"))
        self.signer_key = Key.from_json_file(
            os.path.join(node_dir, "validator_key.json"))

    @retry(wait_fixed=1000, stop_max_attempt_number=3)
    def _download_binary(self, binary):
        p = self.machine.run('bash',
                             input=f'''
/snap/bin/gsutil cp gs://nearprotocol_nearcore_release/{binary} neard
chmod +x neard
''')
        if p.returncode != 0:
            raise DownloadException(p.stderr)

    def addr(self):
        return (self.ip, self.port)

    def rpc_addr(self):
        return (self.ip, self.rpc_port)

    def start(self,
              *,
              boot_node: BootNode = None,
              extra_env: typing.Dict[str, str] = dict()):
        if "RUST_BACKTRACE" not in extra_env:
            extra_env["RUST_BACKTRACE"] = "1"
        extra_env = [f"{k}={v}" for (k, v) in extra_env]
        extra_env = " ".join(extra_env)
        self.machine.run_detach_tmux(
            extra_env +
            " ".join(self._get_command_line('.', '.near', boot_node)))
        self.wait_for_rpc(timeout=30)

    def kill(self):
        self.machine.run('tmux send-keys -t python-rc C-c')
        time.sleep(3)
        self.machine.kill_detach_tmux()

    def destroy_machine(self):
        self.machine.delete()

    def cleanup(self):
        self.kill()
        # move the node dir to avoid weird interactions with multiple serial test invocations
        target_path = self.node_dir + '_finished'
        if os.path.exists(target_path) and os.path.isdir(target_path):
            shutil.rmtree(target_path)
        os.rename(self.node_dir, target_path)

        # Get log and delete machine
        rc.run(f'mkdir -p /tmp/pytest_remote_log')
        self.machine.download(
            '/tmp/python-rc.log',
            f'/tmp/pytest_remote_log/{self.machine.name}.log')
        self.destroy_machine()

    def json_rpc(self, method, params, timeout=15):
        return super().json_rpc(method, params, timeout=timeout)

    def get_status(self):
        r = nretry(lambda: requests.get("http://%s:%s/status" % self.rpc_addr(),
                                        timeout=15),
                   timeout=45)
        r.raise_for_status()
        return json.loads(r.content)

    def stop_network(self):
        rc.run(
            f'gcloud compute firewall-rules create {self.machine.name}-stop --direction=EGRESS --priority=1000 --network=default --action=DENY --rules=all --target-tags={self.machine.name}'
        )

    def resume_network(self):
        rc.run(f'gcloud compute firewall-rules delete {self.machine.name}-stop',
               input='yes\n')

    def reset_validator_key(self, new_key):
        self.validator_key = new_key
        with open(os.path.join(self.node_dir, "validator_key.json"), 'w+') as f:
            json.dump(new_key.to_json(), f)
        self.machine.upload(os.path.join(self.node_dir, 'validator_key.json'),
                            f'/home/{self.machine.username}/.near/')

def spin_up_node(config,
                 near_root,
                 node_dir,
                 ordinal,
                 *,
                 boot_node: BootNode = None,
                 blacklist=[],
                 proxy=None,
                 skip_starting_proxy=False,
                 single_node=False):
    is_local = config['local']

    args = make_boot_nodes_arg(boot_node)
    logger.info("Starting node %s %s" %
                (ordinal,
                 ('with ' + '='.join(args) if args else 'as BOOT NODE')))
    if is_local:
        blacklist = [
            "127.0.0.1:%s" % (24567 + 10 + bl_ordinal)
            for bl_ordinal in blacklist
        ]
        node = LocalNode(24567 + 10 + ordinal, 3030 + 10 + ordinal,
                         near_root, node_dir, blacklist,
                         config.get('binary_name'), single_node)
    else:
        # TODO: Figure out how to know IP address beforehand for remote deployment.
        assert len(
            blacklist) == 0, "Blacklist is only supported in LOCAL deployment."

        instance_name = '{}-{}-{}'.format(
            config['remote'].get('instance_name', 'near-pytest'), ordinal,
            uuid.uuid4())
        zones = config['remote']['zones']
        zone = zones[ordinal % len(zones)]
        node = GCloudNode(instance_name, zone, node_dir,
                          config['remote']['binary'])
        with remote_nodes_lock:
            remote_nodes.append(node)
        logger.info(f"node {ordinal} machine created")

    if proxy is not None:
        proxy.proxify_node(node)

    node.start(boot_node=boot_node, skip_starting_proxy=skip_starting_proxy)
    time.sleep(3)
    logger.info(f"node {ordinal} started")
    return node

def init_cluster(num_nodes,
                 num_observers,
                 num_shards,
                 config,
                 genesis_config_changes,
                 client_config_changes,
                 prefix="test"):
    """
    Create cluster configuration
    """
    if 'local' not in config and 'nodes' in config:
        logger.critical(
            "Attempt to launch a regular test with a mocknet config")
        sys.exit(1)

    if not prefix.startswith("test"):
        logger.critical(f"The prefix must begin with 'test'. prefix = {prefix}")
        sys.exit(1)

    is_local = config['local']
    near_root = config['near_root']
    binary_name = config.get('binary_name', 'neard')

    logger.info("Creating %s cluster configuration with %s nodes" %
                ("LOCAL" if is_local else "REMOTE", num_nodes + num_observers))

    binary_path = os.path.join(near_root, binary_name)
    process = subprocess.Popen(
        [
            binary_path,
            "localnet",
            "--validators",
            str(num_nodes),
            "--non-validators",
            str(num_observers),
            "--shards",
            str(num_shards),
            "--tracked-shards",
            "none",
            "--prefix",
            prefix,
        ],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
    )
    out, err = process.communicate()
    assert 0 == process.returncode, err

    node_dirs = [
        line.split()[-1]
        for line in err.decode('utf8').split('\n')
        if '/test' in line
    ]
    assert len(
        node_dirs
    ) == num_nodes + num_observers, "node dirs: %s num_nodes: %s num_observers: %s" % (
        len(node_dirs), num_nodes, num_observers)

    logger.info("Search for stdout and stderr in %s" % node_dirs)
    # apply config changes
    for i, node_dir in enumerate(node_dirs):
        apply_genesis_changes(node_dir, genesis_config_changes)
        overrides = client_config_changes.get(i)
        if overrides:
            apply_config_changes(node_dir, overrides)

    return near_root, node_dirs

def apply_genesis_changes(node_dir, genesis_config_changes):
    # apply genesis.json changes
    fname = os.path.join(node_dir, 'genesis.json')
    with open(fname) as fd:
        genesis_config = json.load(fd)
    for change in genesis_config_changes:
        cur = genesis_config
        for s in change[:-2]:
            cur = cur[s]
        assert change[-2] in cur
        cur[change[-2]] = change[-1]
    with open(fname, 'w') as fd:
        json.dump(genesis_config, fd, indent=2)

def apply_config_changes(node_dir, client_config_change):
    # apply config.json changes
    fname = os.path.join(node_dir, 'config.json')
    with open(fname) as fd:
        config_json = json.load(fd)

    # ClientConfig keys which are valid but may be missing from the config.json
    # file.  Those are often Option<T> types which are not stored in JSON file
    # when None.
    allowed_missing_configs = ('archive', 'consensus.block_fetch_horizon',
                               'consensus.min_block_production_delay',
                               'consensus.max_block_production_delay',
                               'consensus.max_block_wait_delay',
                               'consensus.state_sync_timeout',
                               'expected_shutdown', 'log_summary_period',
                               'max_gas_burnt_view', 'rosetta_rpc',
                               'save_trie_changes', 'split_storage',
                               'state_sync', 'state_sync_enabled',
                               'store.state_snapshot_enabled',
                               'tracked_shard_schedule', 'cold_store')

    for k, v in client_config_change.items():
        if not (k in allowed_missing_configs or k in config_json):
            raise ValueError(f'Unknown configuration option: {k}')
        if k in config_json and isinstance(v, dict):
            config_json[k].update(v)
        else:
            # Support keys in the form of "a.b.c".
            parts = k.split('.')
            current = config_json
            for part in parts[:-1]:
                if part not in current:
                    raise ValueError(
                        f'{part} is not found in config.json. Key={k}, Value={v}'
                    )
                current = current[part]
            current[parts[-1]] = v

    with open(fname, 'w') as fd:
        json.dump(config_json, fd, indent=2)

def get_config_json(node_dir):
    fname = os.path.join(node_dir, 'config.json')
    with open(fname) as fd:
        return json.load(fd)

def set_config_json(node_dir, config_json):
    fname = os.path.join(node_dir, 'config.json')
    with open(fname, 'w') as fd:
        json.dump(config_json, fd, indent=2)

def start_cluster(num_nodes,
                  num_observers,
                  num_shards,
                  config,
                  genesis_config_changes,
                  client_config_changes,
                  message_handler=None):
    if not config:
        config = load_config()

    dot_near = pathlib.Path.home() / '.near'
    if (dot_near / 'test0').exists():
        near_root = config['near_root']
        node_dirs = [
            str(dot_near / name)
            for name in os.listdir(dot_near)
            if name.startswith('test') and not name.endswith('_finished')
        ]
    else:
        near_root, node_dirs = init_cluster(num_nodes, num_observers,
                                            num_shards, config,
                                            genesis_config_changes,
                                            client_config_changes)

    proxy = NodesProxy(message_handler) if message_handler is not None else None
    ret = []

    def spin_up_node_and_push(i, boot_node: BootNode):
        single_node = (num_nodes == 1) and (num_observers == 0)
        node = spin_up_node(config,
                            near_root,
                            node_dirs[i],
                            i,
                            boot_node=boot_node,
                            proxy=proxy,
                            skip_starting_proxy=True,
                            single_node=single_node)
        ret.append((i, node))
        return node

    boot_node = spin_up_node_and_push(0, None)

    handles = []
    for i in range(1, num_nodes + num_observers):
        handle = threading.Thread(target=spin_up_node_and_push,
                                  args=(i, boot_node))
        handle.start()
        handles.append(handle)

    for handle in handles:
        handle.join()

    nodes = [node for _, node in sorted(ret)]
    for node in nodes:
        node.start_proxy_if_needed()

    return nodes

ROOT_DIR = pathlib.Path(__file__).resolve().parents[2]

def get_near_root():
    cargo_target_dir = os.environ.get('CARGO_TARGET_DIR', 'target')
    default_root = (ROOT_DIR / cargo_target_dir / 'debug').resolve()
    return os.environ.get('NEAR_ROOT', str(default_root))

DEFAULT_CONFIG = {
    'local': True,
    'near_root': get_near_root(),
    'binary_name': 'neard',
    'release': False,
}
CONFIG_ENV_VAR = 'NEAR_PYTEST_CONFIG'

def load_config():
    config = DEFAULT_CONFIG

    config_file = os.environ.get(CONFIG_ENV_VAR, '')
    if config_file:
        try:
            with open(config_file) as f:
                new_config = json.load(f)
                config.update(new_config)
                logger.info(f"Load config from {config_file}, config {config}")
        except FileNotFoundError:
            logger.info(
                f"Failed to load config file, use default config {config}")
    else:
        logger.info(f"Use default config {config}")
    return config

# Returns the protocol version of the binary.
def get_binary_protocol_version(config) -> typing.Optional[int]:
    binary_name = config.get('binary_name', 'neard')
    near_root = config.get('near_root')
    binary_path = os.path.join(near_root, binary_name)

    # Get the protocol version of the binary
    # The --version output looks like this:
    # neard (release trunk) (build 1.1.0-3884-ge93793a61-modified) (rustc 1.71.0) (protocol 137) (db 37)
    out = subprocess.check_output([binary_path, "--version"], text=True)
    out = out.replace('(', '')
    out = out.replace(')', '')
    tokens = out.split()
    n = len(tokens)
    for i in range(n):
        if tokens[i] == "protocol" and i + 1 < n:
            return int(tokens[i + 1])
    return None

def corrupt_state_snapshot(config, node_dir, shard_layout_version):
    near_root = config['near_root']
    binary_name = config.get('binary_name', 'neard')
    binary_path = os.path.join(near_root, binary_name)

    cmd = [
        binary_path,
        "--home",
        node_dir,
        "database",
        "corrupt-state-snapshot",
        "--shard-layout-version",
        str(shard_layout_version),
    ]

    env = os.environ.copy()
    env["RUST_BACKTRACE"] = "1"
    env["RUST_LOG"] = "db=warn,db_opener=warn," + env.get("RUST_LOG", "debug")

    out = subprocess.check_output(cmd, text=True, env=env)

    return out

'''
'''--- pytest/lib/configured_logger.py ---
import logging
import uuid
import sys

from typing import Optional

# LogLevel type since logging lib doesn't define its own enum/type for it
LogLevel = int

def new_logger(
    name: Optional[str] = None,
    level: LogLevel = logging.INFO,
    outfile: Optional[str] = None,
    stderr: Optional[bool] = None,
) -> logging.Logger:
    """
    Create a new configured logger. Used mainly by pytests.

    :param name: The name of the logger. Defaults to "test".
    :param level: The logging level. Defaults to DEBUG to log everything.
    :param outfile: Optional to set. When set, will log to a file instead of stdout.
    :param stderr: Optional to set. If outfile is not set, and stderr is set to True, then will log to stderr instead of stdout.
    :return: The configured logger.
    """
    # If name is not specified, create one so that this can be a separate logger.
    if name is None:
        name = f"logger_{uuid.uuid1()}"

    log = logging.getLogger(name)
    log.setLevel(level)
    fmt = logging.Formatter('[%(asctime)s] %(levelname)s: %(message)s',
                            '%Y-%m-%d %H:%M:%S')

    if outfile is not None:
        handler = logging.FileHandler(outfile)
    elif stderr:
        handler = logging.StreamHandler(sys.stderr)
    else:
        handler = logging.StreamHandler(sys.stdout)
    handler.setLevel(level)
    handler.setFormatter(fmt)

    log.addHandler(handler)
    log.propagate = False

    return log

# global logger for testing purposes:
logger = new_logger("test")

'''
'''--- pytest/lib/data.py ---
import numpy as np
from sklearn.linear_model import LinearRegression

def flatten(ll):
    '''
    Flattens a list of lists into a single list
    '''
    return [item for sublist in ll for item in sublist]

def compute_cumulative(xs):
    '''
    Computes a running total (i.e. cumulative sum)
    for the given list.
    E.g. given [1, 2, 3, 4] the return value will be
    [1, 3, 6, 10].
    '''
    total = xs[0]
    result = [total]
    for x in xs[1:]:
        total += x
        result.append(total)
    return result

def linear_regression(xs, ys):
    '''
    Fits a line `y = mx + b` to the given data points
    '''
    x = np.array(xs).reshape((-1, 1))
    y = np.array(ys)
    model = LinearRegression().fit(x, y)
    return {
        'slope': model.coef_[0],
        'intercept': model.intercept_,
        'R^2': model.score(x, y),
    }

def compute_rate(timestamps):
    '''
    Given a list of timestamps indicating the times
    some event occurred, returns the average rate at
    which the events happen. If the units of the
    timestamps are seconds, then the output units will
    be `events/s`.
    '''
    cumulative_events = [i for i in range(1, len(timestamps) + 1)]
    fit = linear_regression(timestamps, cumulative_events)
    return fit['slope']

'''
'''--- pytest/lib/key.py ---
import base58
import json
import os
import typing

import ed25519

class Key:
    account_id: str
    pk: str
    sk: str

    def __init__(self, account_id: str, pk: str, sk: str) -> None:
        super(Key, self).__init__()
        self.account_id = account_id
        self.pk = pk
        self.sk = sk

    @classmethod
    def from_random(cls, account_id: str) -> 'Key':
        keys = ed25519.create_keypair(entropy=os.urandom)
        return cls.from_keypair(account_id, keys)

    @classmethod
    def implicit_account(cls) -> 'Key':
        keys = ed25519.create_keypair(entropy=os.urandom)
        account_id = keys[1].to_bytes().hex()
        return cls.from_keypair(account_id, keys)

    @classmethod
    def from_json(cls, j: typing.Dict[str, str]):
        return cls(j['account_id'], j['public_key'], j['secret_key'])

    @classmethod
    def from_json_file(cls, filename: str):
        with open(filename) as rd:
            return cls.from_json(json.load(rd))

    @classmethod
    def from_seed_testonly(cls, account_id: str, seed: str = None) -> 'Key':
        """
        Deterministically produce an **insecure** signer pair from a seed.
        
        If no seed is provided, the account id is used as seed.
        """
        if seed is None:
            seed = account_id
        # use the repeated seed string as secret key by injecting fake entropy
        fake_entropy = lambda length: (seed * (1 + int(length / len(seed)))
                                      ).encode()[:length]
        keys = ed25519.create_keypair(entropy=fake_entropy)
        return cls.from_keypair(account_id, keys)

    @classmethod
    def from_keypair(cls, account_id, keys):
        sk = 'ed25519:' + base58.b58encode(keys[0].to_bytes()).decode('ascii')
        pk = 'ed25519:' + base58.b58encode(keys[1].to_bytes()).decode('ascii')
        return cls(account_id, pk, sk)

    def decoded_pk(self) -> bytes:
        key = self.pk.split(':')[1] if ':' in self.pk else self.pk
        return base58.b58decode(key.encode('ascii'))

    def decoded_sk(self) -> bytes:
        key = self.sk.split(':')[1] if ':' in self.sk else self.sk
        return base58.b58decode(key.encode('ascii'))

    def to_json(self):
        return {
            'account_id': self.account_id,
            'public_key': self.pk,
            'secret_key': self.sk
        }

    def sign_bytes(self, data: typing.Union[bytes, bytearray]) -> bytes:
        sk = self.decoded_sk()
        return ed25519.SigningKey(sk).sign(bytes(data))

'''
'''--- pytest/lib/lightclient.py ---
from serializer import BinarySerializer
import hashlib, base58
import nacl.signing
from utils import combine_hash

ED_PREFIX = "ed25519:"

class BlockHeaderInnerLite:
    pass

inner_lite_schema = dict([
    [
        BlockHeaderInnerLite, {
            'kind':
                'struct',
            'fields': [
                ['height', 'u64'],
                ['epoch_id', [32]],
                ['next_epoch_id', [32]],
                ['prev_state_root', [32]],
                ['outcome_root', [32]],
                ['timestamp', 'u64'],
                ['next_bp_hash', [32]],
                ['block_merkle_root', [32]],
            ]
        }
    ],
])

def compute_block_hash(inner_lite_view, inner_rest_hash, prev_hash):
    inner_rest_hash = base58.b58decode(inner_rest_hash)
    prev_hash = base58.b58decode(prev_hash)

    inner_lite = BlockHeaderInnerLite()
    inner_lite.height = inner_lite_view['height']
    inner_lite.epoch_id = base58.b58decode(inner_lite_view['epoch_id'])
    inner_lite.next_epoch_id = base58.b58decode(
        inner_lite_view['next_epoch_id'])
    inner_lite.prev_state_root = base58.b58decode(
        inner_lite_view['prev_state_root'])
    inner_lite.outcome_root = base58.b58decode(inner_lite_view['outcome_root'])
    inner_lite.timestamp = int(inner_lite_view['timestamp_nanosec'])
    inner_lite.next_bp_hash = base58.b58decode(inner_lite_view['next_bp_hash'])
    inner_lite.block_merkle_root = base58.b58decode(
        inner_lite_view['block_merkle_root'])

    msg = BinarySerializer(inner_lite_schema).serialize(inner_lite)
    inner_lite_hash = hashlib.sha256(msg).digest()
    inner_hash = combine_hash(inner_lite_hash, inner_rest_hash)
    final_hash = combine_hash(inner_hash, prev_hash)

    return base58.b58encode(final_hash)

# follows the spec from NEP 25 (https://github.com/nearprotocol/NEPs/pull/25)
def validate_light_client_block(last_known_block,
                                new_block,
                                block_producers_map,
                                panic=False):
    new_block_hash = compute_block_hash(new_block['inner_lite'],
                                        new_block['inner_rest_hash'],
                                        new_block['prev_block_hash'])
    next_block_hash_decoded = combine_hash(
        base58.b58decode(new_block['next_block_inner_hash']),
        base58.b58decode(new_block_hash))

    if new_block['inner_lite']['epoch_id'] not in [
            last_known_block['inner_lite']['epoch_id'],
            last_known_block['inner_lite']['next_epoch_id']
    ]:
        if panic:
            assert False
        return False

    block_producers = block_producers_map[new_block['inner_lite']['epoch_id']]
    if len(new_block['approvals_after_next']) != len(block_producers):
        if panic:
            assert False
        return False

    total_stake = 0
    approved_stake = 0

    for approval, stake in zip(new_block['approvals_after_next'],
                               block_producers):
        total_stake += int(stake['stake'])

        if approval is None:
            continue

        approved_stake += int(stake['stake'])

        public_key = stake['public_key']

        signature = base58.b58decode(approval[len(ED_PREFIX):])
        verify_key = nacl.signing.VerifyKey(
            base58.b58decode(public_key[len(ED_PREFIX):]))

        approval_message = bytearray()
        approval_message.append(0)
        approval_message += next_block_hash_decoded
        approval_message.append(new_block['inner_lite']['height'] + 2)
        for i in range(7):
            approval_message.append(0)
        approval_message = bytes(approval_message)
        verify_key.verify(approval_message, signature)

    threshold = total_stake * 2 // 3
    if approved_stake <= threshold:
        if panic:
            assert False
        return False

    if new_block['inner_lite']['epoch_id'] == last_known_block['inner_lite'][
            'next_epoch_id']:
        if new_block['next_bps'] is None:
            if panic:
                assert False
            return False

        print(new_block['next_bps'])
        serialized_next_bp = bytearray()
        serialized_next_bp.append(len(new_block['next_bps']))
        for i in range(3):
            serialized_next_bp.append(0)
        for bp in new_block['next_bps']:
            version = 0
            if 'validator_stake_struct_version' in bp:
                # version of ValidatorStake enum
                version = int(bp['validator_stake_struct_version'][1:]) - 1
                serialized_next_bp.append(version)
            serialized_next_bp.append(5)
            for i in range(3):
                serialized_next_bp.append(0)
            serialized_next_bp += bp['account_id'].encode('utf-8')
            serialized_next_bp.append(0)  # public key type
            serialized_next_bp += base58.b58decode(
                bp['public_key'][len(ED_PREFIX):])
            stake = int(bp['stake'])
            for i in range(16):
                serialized_next_bp.append(stake & 255)
                stake >>= 8

        serialized_next_bp = bytes(serialized_next_bp)

        computed_hash = base58.b58encode(
            hashlib.sha256(serialized_next_bp).digest())
        if computed_hash != new_block['inner_lite']['next_bp_hash'].encode(
                'utf-8'):
            if panic:
                assert False
            return False

        block_producers_map[new_block['inner_lite']
                            ['next_epoch_id']] = new_block['next_bps']

'''
'''--- pytest/lib/messages/__init__.py ---
from .block import block_schema
from .bridge import bridge_schema
from .crypto import crypto_schema
from .network import network_schema
from .shard import shard_schema
from .tx import tx_schema

schema = dict(block_schema + bridge_schema + crypto_schema + network_schema +
              shard_schema + tx_schema)

'''
'''--- pytest/lib/messages/block.py ---
from messages.crypto import PublicKey, Signature, MerklePath, ShardProof
from messages.tx import Receipt, SignedTransaction

class Block:

    def header(self):
        if self.enum == 'BlockV1':
            return self.BlockV1.header
        elif self.enum == 'BlockV2':
            return self.BlockV2.header
        elif self.enum == 'BlockV3':
            return self.BlockV3.header
        elif self.enum == 'BlockV4':
            return self.BlockV4.header
        assert False, "header is called on Block, but the enum variant `%s` is unknown" % self.enum

    def chunks(self):
        if self.enum == 'BlockV1':
            return self.BlockV1.chunks
        elif self.enum == 'BlockV2':
            return self.BlockV2.chunks
        elif self.enum == 'BlockV3':
            return self.BlockV3.body.chunks
        elif self.enum == 'BlockV4':
            return self.BlockV3.body.chunks
        assert False, "chunks is called on Block, but the enum variant `%s` is unknown" % self.enum

class BlockV1:
    pass

class BlockV2:
    pass

class BlockV3:
    pass

class BlockV4:
    pass

class BlockBody:
    pass

class BlockBodyV1:
    pass

class BlockBodyV2:
    pass

class BlockHeader:

    def inner_lite(self):
        if self.enum == 'BlockHeaderV4':
            return self.BlockHeaderV4.inner_lite
        elif self.enum == 'BlockHeaderV3':
            return self.BlockHeaderV3.inner_lite
        elif self.enum == 'BlockHeaderV2':
            return self.BlockHeaderV2.inner_lite
        elif self.enum == 'BlockHeaderV1':
            return self.BlockHeaderV1.inner_lite
        assert False, "inner_lite is called on BlockHeader, but the enum variant `%s` is unknown" % self.enum

class BlockHeaderV1:
    pass

class BlockHeaderV2:
    pass

class BlockHeaderV3:
    pass

class BlockHeaderV4:
    pass

class BlockHeaderInnerLite:
    pass

class BlockHeaderInnerRest:
    pass

class BlockHeaderInnerRestV2:
    pass

class BlockHeaderInnerRestV3:
    pass

class BlockHeaderInnerRestV4:
    pass

class ShardChunk:
    pass

class ShardChunkV1:
    pass

class ShardChunkV2:
    pass

class ShardChunkHeader:

    @property
    def signature(self):
        if self.enum == 'V1':
            return self.V1.signature
        elif self.enum == 'V2':
            return self.V2.signature
        elif self.enum == 'V3':
            return self.V3.signature
        assert False, "signature is called on ShardChunkHeader, but the enum variant `%s` is unknown" % self.enum

class ShardChunkHeaderV1:

    @staticmethod
    def chunk_hash(inner):
        import hashlib
        from messages.crypto import crypto_schema
        from serializer import BinarySerializer
        inner_serialized = BinarySerializer(
            dict(block_schema + crypto_schema)).serialize(inner)
        return hashlib.sha256(inner_serialized).digest()

class ShardChunkHeaderV2:

    @staticmethod
    def chunk_hash(inner):
        import hashlib
        from messages.crypto import crypto_schema
        from serializer import BinarySerializer
        inner_serialized = BinarySerializer(
            dict(block_schema + crypto_schema)).serialize(inner)
        inner_hash = hashlib.sha256(inner_serialized).digest()

        return hashlib.sha256(inner_hash + inner.encoded_merkle_root).digest()

class ShardChunkHeaderV3:

    @staticmethod
    def chunk_hash(inner):
        import hashlib
        from messages.crypto import crypto_schema
        from serializer import BinarySerializer
        inner_serialized = BinarySerializer(
            dict(block_schema + crypto_schema)).serialize(inner)
        inner_hash = hashlib.sha256(inner_serialized).digest()

        return hashlib.sha256(inner_hash +
                              inner.V2.encoded_merkle_root).digest()

class ShardChunkHeaderInner:
    pass

class ShardChunkHeaderInnerV1:
    pass

class ShardChunkHeaderInnerV2:
    pass

class PartialEncodedChunkPart:
    pass

class ReceiptProof:
    pass

class PartialEncodedChunk:

    def inner_header(self):
        version = self.enum
        if version == 'V1':
            return self.V1.header.inner
        elif version == 'V2':
            header = self.V2.header
            header_version = header.enum
            if header_version == 'V1':
                return header.V1.inner
            elif header_version == 'V2':
                return header.V2.inner
            elif header_version == 'V3':
                return header.V3.inner
            assert False, "unknown header version"

    def header_version(self):
        version = self.enum
        if version == 'V1':
            return version
        elif version == 'V2':
            return self.V2.header.enum
        assert False, "unknown partial encoded chunk version"

class PartialEncodedChunkV1:
    pass

class PartialEncodedChunkV2:
    pass

class PartialEncodedChunkRequestMsg:
    pass

class PartialEncodedChunkResponseMsg:
    pass

class PartialEncodedChunkForwardMsg:
    pass

class ValidatorStake:
    pass

class ValidatorStakeV1:
    pass

class ValidatorStakeV2:
    pass

class Approval:
    pass

class ApprovalInner:
    pass

block_schema = [
    [
        Block, {
            'kind':
                'enum',
            'field':
                'enum',
            'values': [
                ['BlockV1', BlockV1],
                ['BlockV2', BlockV2],
                ['BlockV3', BlockV3],
                ['BlockV4', BlockV4],
            ]
        }
    ],
    [
        BlockV1,
        {
            'kind':
                'struct',
            'fields': [
                ['header', BlockHeader],
                ['chunks', [ShardChunkHeaderV1]],
                ['challenges', [()]],  # TODO
                ['vrf_value', [32]],
                ['vrf_proof', [64]],
            ]
        }
    ],
    [
        BlockV2,
        {
            'kind':
                'struct',
            'fields': [
                ['header', BlockHeader],
                ['chunks', [ShardChunkHeader]],
                ['challenges', [()]],  # TODO
                ['vrf_value', [32]],
                ['vrf_proof', [64]],
            ]
        }
    ],
    [
        BlockV3, {
            'kind': 'struct',
            'fields': [
                ['header', BlockHeader],
                ['body', BlockBodyV1],
            ]
        }
    ],
    [
        BlockV4, {
            'kind': 'struct',
            'fields': [
                ['header', BlockHeader],
                ['body', BlockBody],
            ]
        }
    ],
    [
        BlockBody, {
            'kind': 'enum',
            'field': 'enum',
            'values': [
                ['V1', BlockBodyV1],
                ['V2', BlockBodyV2],
            ]
        }
    ],
    [
        BlockBodyV1,
        {
            'kind':
                'struct',
            'fields': [
                ['chunks', [ShardChunkHeader]],
                ['challenges', [()]],  # TODO
                ['vrf_value', [32]],
                ['vrf_proof', [64]],
            ]
        }
    ],
    [
        BlockBodyV2,
        {
            'kind':
                'struct',
            'fields': [
                ['chunks', [ShardChunkHeader]],
                ['challenges', [()]],  # TODO
                ['vrf_value', [32]],
                ['vrf_proof', [64]],
                [
                    'chunk_endorsements',
                    [[{
                        'kind': 'option',
                        'type': Signature
                    }]]
                ],
            ]
        }
    ],
    [
        BlockHeader, {
            'kind':
                'enum',
            'field':
                'enum',
            'values': [['BlockHeaderV1', BlockHeaderV1],
                       ['BlockHeaderV2', BlockHeaderV2],
                       ['BlockHeaderV3', BlockHeaderV3],
                       ['BlockHeaderV4', BlockHeaderV4]]
        }
    ],
    [
        BlockHeaderV1, {
            'kind':
                'struct',
            'fields': [
                ['prev_hash', [32]],
                ['inner_lite', BlockHeaderInnerLite],
                ['inner_rest', BlockHeaderInnerRest],
                ['signature', Signature],
            ]
        }
    ],
    [
        BlockHeaderV2, {
            'kind':
                'struct',
            'fields': [
                ['prev_hash', [32]],
                ['inner_lite', BlockHeaderInnerLite],
                ['inner_rest', BlockHeaderInnerRestV2],
                ['signature', Signature],
            ]
        }
    ],
    [
        BlockHeaderV3, {
            'kind':
                'struct',
            'fields': [
                ['prev_hash', [32]],
                ['inner_lite', BlockHeaderInnerLite],
                ['inner_rest', BlockHeaderInnerRestV3],
                ['signature', Signature],
            ]
        }
    ],
    [
        BlockHeaderV4, {
            'kind':
                'struct',
            'fields': [
                ['prev_hash', [32]],
                ['inner_lite', BlockHeaderInnerLite],
                ['inner_rest', BlockHeaderInnerRestV4],
                ['signature', Signature],
            ]
        }
    ],
    [
        BlockHeaderInnerLite, {
            'kind':
                'struct',
            'fields': [
                ['height', 'u64'],
                ['epoch_id', [32]],
                ['next_epoch_id', [32]],
                ['prev_state_root', [32]],
                ['outcome_root', [32]],
                ['timestamp', 'u64'],
                ['next_bp_hash', [32]],
                ['block_merkle_root', [32]],
            ]
        }
    ],
    [
        BlockHeaderInnerRest,
        {
            'kind':
                'struct',
            'fields': [
                ['chunk_receipts_root', [32]],
                ['chunk_headers_root', [32]],
                ['chunk_tx_root', [32]],
                ['chunks_included', 'u64'],
                ['challenges_root', [32]],
                ['random_value', [32]],
                ['validator_proposals', [ValidatorStakeV1]],
                ['chunk_mask', ['u8']],
                ['gas_price', 'u128'],
                ['total_supply', 'u128'],
                ['challenges_result', [()]],  # TODO
                ['last_final_block', [32]],
                ['last_ds_final_block', [32]],
                ['approvals', [{
                    'kind': 'option',
                    'type': Signature
                }]],
                ['latest_protocol_version', 'u32'],
            ]
        }
    ],
    [
        BlockHeaderInnerRestV2,
        {
            'kind':
                'struct',
            'fields': [
                ['chunk_receipts_root', [32]],
                ['chunk_headers_root', [32]],
                ['chunk_tx_root', [32]],
                ['challenges_root', [32]],
                ['random_value', [32]],
                ['validator_proposals', [ValidatorStakeV1]],
                ['chunk_mask', ['u8']],
                ['gas_price', 'u128'],
                ['total_supply', 'u128'],
                ['challenges_result', [()]],  # TODO
                ['last_final_block', [32]],
                ['last_ds_final_block', [32]],
                ['approvals', [{
                    'kind': 'option',
                    'type': Signature
                }]],
                ['latest_protocol_version', 'u32'],
            ]
        }
    ],
    [
        BlockHeaderInnerRestV3,
        {
            'kind':
                'struct',
            'fields': [
                ['chunk_receipts_root', [32]],
                ['chunk_headers_root', [32]],
                ['chunk_tx_root', [32]],
                ['challenges_root', [32]],
                ['random_value', [32]],
                ['validator_proposals', [ValidatorStake]],
                ['chunk_mask', ['u8']],
                ['gas_price', 'u128'],
                ['total_supply', 'u128'],
                ['challenges_result', [()]],  # TODO
                ['last_final_block', [32]],
                ['last_ds_final_block', [32]],
                ['block_ordinal', 'u64'],
                ['prev_height', 'u64'],
                ['epoch_sync_data_hash', {
                    'kind': 'option',
                    'type': [32]
                }],
                ['approvals', [{
                    'kind': 'option',
                    'type': Signature
                }]],
                ['latest_protocol_version', 'u32'],
            ]
        }
    ],
    [
        BlockHeaderInnerRestV4,
        {
            'kind':
                'struct',
            'fields': [
                ['block_body_hash', [32]],
                ['chunk_receipts_root', [32]],
                ['chunk_headers_root', [32]],
                ['chunk_tx_root', [32]],
                ['challenges_root', [32]],
                ['random_value', [32]],
                ['validator_proposals', [ValidatorStake]],
                ['chunk_mask', ['u8']],
                ['gas_price', 'u128'],
                ['total_supply', 'u128'],
                ['challenges_result', [()]],  # TODO
                ['last_final_block', [32]],
                ['last_ds_final_block', [32]],
                ['block_ordinal', 'u64'],
                ['prev_height', 'u64'],
                ['epoch_sync_data_hash', {
                    'kind': 'option',
                    'type': [32]
                }],
                ['approvals', [{
                    'kind': 'option',
                    'type': Signature
                }]],
                ['latest_protocol_version', 'u32'],
            ]
        }
    ],
    [
        ShardChunkHeader, {
            'kind':
                'enum',
            'field':
                'enum',
            'values': [['V1', ShardChunkHeaderV1], ['V2', ShardChunkHeaderV2],
                       ['V3', ShardChunkHeaderV3]]
        }
    ],
    [
        ShardChunkHeaderV1, {
            'kind':
                'struct',
            'fields': [
                ['inner', ShardChunkHeaderInnerV1],
                ['height_included', 'u64'],
                ['signature', Signature],
            ]
        }
    ],
    [
        ShardChunkHeaderV2, {
            'kind':
                'struct',
            'fields': [
                ['inner', ShardChunkHeaderInnerV1],
                ['height_included', 'u64'],
                ['signature', Signature],
            ]
        }
    ],
    [
        ShardChunkHeaderV3, {
            'kind':
                'struct',
            'fields': [
                ['inner', ShardChunkHeaderInner],
                ['height_included', 'u64'],
                ['signature', Signature],
            ]
        }
    ],
    [
        ShardChunkHeaderInner, {
            'kind':
                'enum',
            'field':
                'enum',
            'values': [['V1', ShardChunkHeaderInnerV1],
                       ['V2', ShardChunkHeaderInnerV2]]
        }
    ],
    [
        ShardChunkHeaderInnerV1, {
            'kind':
                'struct',
            'fields': [
                ['prev_block_hash', [32]],
                ['prev_state_root', [32]],
                ['outcome_root', [32]],
                ['encoded_merkle_root', [32]],
                ['encoded_length', 'u64'],
                ['height_created', 'u64'],
                ['shard_id', 'u64'],
                ['gas_used', 'u64'],
                ['gas_limit', 'u64'],
                ['balance_burnt', 'u128'],
                ['outgoing_receipt_root', [32]],
                ['tx_root', [32]],
                ['validator_proposals', [ValidatorStakeV1]],
            ]
        }
    ],
    [
        ShardChunkHeaderInnerV2, {
            'kind':
                'struct',
            'fields': [
                ['prev_block_hash', [32]],
                ['prev_state_root', [32]],
                ['outcome_root', [32]],
                ['encoded_merkle_root', [32]],
                ['encoded_length', 'u64'],
                ['height_created', 'u64'],
                ['shard_id', 'u64'],
                ['gas_used', 'u64'],
                ['gas_limit', 'u64'],
                ['balance_burnt', 'u128'],
                ['outgoing_receipt_root', [32]],
                ['tx_root', [32]],
                ['validator_proposals', [ValidatorStake]],
            ]
        }
    ],
    [
        ShardChunk, {
            'kind': 'enum',
            'field': 'enum',
            'values': [['V1', ShardChunkV1], ['V2', ShardChunkV2]]
        }
    ],
    [
        ShardChunkV1, {
            'kind':
                'struct',
            'fields': [
                ['chunk_hash', [32]],
                ['header', ShardChunkHeaderV1],
                ['transactions', [SignedTransaction]],
                ['receipts', [Receipt]],
            ]
        }
    ],
    [
        ShardChunkV2, {
            'kind':
                'struct',
            'fields': [
                ['chunk_hash', [32]],
                ['header', ShardChunkHeader],
                ['transactions', [SignedTransaction]],
                ['receipts', [Receipt]],
            ]
        }
    ],
    [
        PartialEncodedChunkPart, {
            'kind':
                'struct',
            'fields': [
                ['part_ord', 'u64'],
                ['part', ['u8']],
                ['merkle_proof', MerklePath],
            ]
        }
    ],
    [
        ReceiptProof, {
            'kind': 'struct',
            'fields': [
                ['f1', [Receipt]],
                ['f2', ShardProof],
            ]
        }
    ],
    [
        PartialEncodedChunk, {
            'kind':
                'enum',
            'field':
                'enum',
            'values': [['V1', PartialEncodedChunkV1],
                       ['V2', PartialEncodedChunkV2]]
        }
    ],
    [
        PartialEncodedChunkV1, {
            'kind':
                'struct',
            'fields': [['header', ShardChunkHeaderV1],
                       ['parts', [PartialEncodedChunkPart]],
                       ['receipts', [ReceiptProof]]]
        }
    ],
    [
        PartialEncodedChunkV2, {
            'kind':
                'struct',
            'fields': [['header', ShardChunkHeader],
                       ['parts', [PartialEncodedChunkPart]],
                       ['receipts', [ReceiptProof]]]
        }
    ],
    [
        PartialEncodedChunkRequestMsg, {
            'kind':
                'struct',
            'fields': [['chunk_hash', [32]], ['part_ords', ['u64']],
                       ['tracking_shards', ['u64']]]
        }
    ],
    [
        PartialEncodedChunkResponseMsg, {
            'kind':
                'struct',
            'fields': [['chunk_hash', [32]],
                       ['parts', [PartialEncodedChunkPart]],
                       ['receipts', [ReceiptProof]]]
        }
    ],
    [
        PartialEncodedChunkForwardMsg, {
            'kind':
                'struct',
            'fields': [['chunk_hash', [32]], ['inner_header_hash', [32]],
                       ['merkle_root', [32]], ['signature', Signature],
                       ['prev_block_hash', [32]], ['height_created', 'u64'],
                       ['shard_id', 'u64'],
                       ['parts', [PartialEncodedChunkPart]]]
        }
    ],
    [
        ValidatorStake, {
            'kind': 'enum',
            'field': 'enum',
            'values': [['V1', ValidatorStakeV1], ['V2', ValidatorStakeV2]]
        }
    ],
    [
        ValidatorStakeV1, {
            'kind':
                'struct',
            'fields': [
                ['account_id', 'string'],
                ['public_key', PublicKey],
                ['stake', 'u128'],
            ]
        }
    ],
    [
        Approval, {
            'kind':
                'struct',
            'fields': [
                ['inner', ApprovalInner],
                ['target_height', 'u64'],
                ['signature', Signature],
                ['account_id', 'string'],
            ]
        }
    ],
    [
        ApprovalInner, {
            'kind': 'enum',
            'field': 'enum',
            'values': [
                ['Endorsement', [32]],
                ['Skip', 'u64'],
            ]
        }
    ],
]

'''
'''--- pytest/lib/messages/bridge.py ---
class Proof:
    pass

bridge_schema = [[
    Proof, {
        'kind':
            'struct',
        'fields': [
            ['log_index', 'u64'],
            ['log_entry_data', ['u8']],
            ['receipt_index', 'u64'],
            ['receipt_data', ['u8']],
            ['header_data', ['u8']],
            ['proof', [['u8']]],
        ]
    }
]]

'''
'''--- pytest/lib/messages/crypto.py ---
import typing

import base58

class Signature:
    _KEY_TYPES = {
        'ed25519': 0,
        'secp256k1': 1,
    }

    def __init__(self, signature: typing.Optional[str] = None) -> None:
        if signature:
            keyType, data = signature.split(':')
            self.keyType = self._KEY_TYPES[keyType]
            self.data = base58.b58decode(data)

class PublicKey:
    pass

class AccessKey:
    pass

class AccessKeyPermission:
    pass

class FunctionCallPermission:
    pass

class FullAccessPermission:
    pass

class Direction:
    pass

class MerklePath:
    pass

class ShardProof:
    pass

crypto_schema = [
    [
        Signature, {
            'kind': 'struct',
            'fields': [['keyType', 'u8'], ['data', [64]]]
        }
    ],
    [
        PublicKey, {
            'kind': 'struct',
            'fields': [['keyType', 'u8'], ['data', [32]]]
        }
    ],
    [
        AccessKey, {
            'kind': 'struct',
            'fields': [
                ['nonce', 'u64'],
                ['permission', AccessKeyPermission],
            ]
        }
    ],
    [
        AccessKeyPermission, {
            'kind':
                'enum',
            'field':
                'enum',
            'values': [
                ['functionCall', FunctionCallPermission],
                ['fullAccess', FullAccessPermission],
            ]
        }
    ],
    [
        FunctionCallPermission, {
            'kind':
                'struct',
            'fields': [
                ['allowance', {
                    'kind': 'option',
                    'type': 'u128'
                }],
                ['receiverId', 'string'],
                ['methodNames', ['string']],
            ]
        }
    ],
    [FullAccessPermission, {
        'kind': 'struct',
        'fields': []
    }],
    [
        Direction, {
            'kind': 'enum',
            'field': 'enum',
            'values': [['Left', ()], ['Right', ()]],
        }
    ],
    [MerklePath, {
        'kind': 'struct',
        'fields': [['f1', [([32], Direction)]]],
    }],
    [
        ShardProof, {
            'kind':
                'struct',
            'fields': [['from_shard_id', 'u64'], ['to_shard_id', 'u64'],
                       ['proof', MerklePath]],
        }
    ],
]

'''
'''--- pytest/lib/messages/network.py ---
from messages.crypto import Signature, PublicKey, MerklePath, ShardProof
from messages.tx import SignedTransaction, Receipt
from messages.block import Block, Approval, PartialEncodedChunk, PartialEncodedChunkV1, PartialEncodedChunkRequestMsg, PartialEncodedChunkResponseMsg, PartialEncodedChunkForwardMsg, BlockHeader, ShardChunk, ShardChunkHeader, ShardChunkHeaderV1
from messages.shard import StateRootNode

class SocketAddr:
    pass

class PeerMessage:
    pass

class Handshake:
    pass

class HandshakeV2:
    pass

class HandshakeFailureReason:
    pass

class ProtocolVersionMismatch:
    pass

class PeerInfo:
    pass

class PeerChainInfo:
    pass

class PeerChainInfoV2:
    pass

class EdgeInfo:
    pass

class GenesisId:
    pass

class Edge:
    pass

class SyncData:
    pass

class AnnounceAccount:
    pass

class RoutedMessage:
    pass

class PeerIdOrHash:
    pass

class RoutedMessageBody:
    pass

class PingPong:
    pass

class StateResponseInfo:
    pass

class StateResponseInfoV1:
    pass

class StateResponseInfoV2:
    pass

class ShardStateSyncResponse:
    pass

class ShardStateSyncResponseV1:
    pass

class ShardStateSyncResponseV2:
    pass

class ShardStateSyncResponseHeader:
    pass

class ShardStateSyncResponseHeaderV1:
    pass

class ShardStateSyncResponseHeaderV2:
    pass

class RoutingTableSyncV2:
    pass

class IbfElem:
    pass

class RoutingVersion2:
    pass

class RoutingState:
    pass

class PartialSync:
    pass

network_schema = [
    [
        SocketAddr, {
            'kind': 'enum',
            'field': 'enum',
            'values': [['V4', ([4], 'u16')], ['V6', ([16], 'u16')]]
        }
    ],
    [
        PeerMessage,
        {
            'kind':
                'enum',
            'field':
                'enum',
            'values': [
                ['Handshake', Handshake],
                ['HandshakeFailure', (PeerInfo, HandshakeFailureReason)],
                ['LastEdge', Edge],
                ['Sync', SyncData],
                ['RequestUpdateNonce', EdgeInfo],
                ['ResponseUpdateNonce', Edge],
                ['PeersRequest', ()],
                ['PeersResponse', [PeerInfo]],
                ['BlockHeadersRequest', [[32]]],
                ['BlockHeaders', [BlockHeader]],
                ['BlockRequest', [32]],
                ['Block', Block],
                ['Transaction', SignedTransaction],
                ['Routed', RoutedMessage],
                ['Disconnect'],
                ['Challenge', None],  # TODO
                ['HandshakeV2', HandshakeV2],
                ['EpochSyncRequest', None],  # TODO
                ['EpochSyncResponse', None],  # TODO
                ['EpochSyncFinalizationRequest', None],  # TODO
                ['EpochSyncFinalizationResponse', None],  # TODO
                ['RoutingTableSyncV2', RoutingTableSyncV2],
            ]
        }
    ],
    [
        Handshake, {
            'kind':
                'struct',
            'fields': [
                ['version', 'u32'],
                ['oldest_supported_version', 'u32'],
                ['peer_id', PublicKey],
                ['target_peer_id', PublicKey],
                ['listen_port', {
                    'kind': 'option',
                    'type': 'u16'
                }],
                ['chain_info', PeerChainInfoV2],
                ['edge_info', EdgeInfo],
            ]
        }
    ],
    [
        HandshakeV2, {
            'kind':
                'struct',
            'fields': [
                ['version', 'u32'],
                ['oldest_supported_version', 'u32'],
                ['peer_id', PublicKey],
                ['target_peer_id', PublicKey],
                ['listen_port', {
                    'kind': 'option',
                    'type': 'u16'
                }],
                ['chain_info', PeerChainInfo],
                ['edge_info', EdgeInfo],
            ]
        }
    ],
    [
        HandshakeFailureReason, {
            'kind':
                'enum',
            'field':
                'enum',
            'values': [
                ['ProtocolVersionMismatch', ProtocolVersionMismatch],
                ['GenesisMismatch', GenesisId],
                ['InvalidTarget', ()],
            ]
        }
    ],
    [
        ProtocolVersionMismatch, {
            'kind': 'struct',
            'fields': [
                ['version', 'u32'],
                ['oldest_supported_version', 'u32'],
            ]
        }
    ],
    [
        PeerInfo, {
            'kind':
                'struct',
            'fields': [['id', PublicKey],
                       ['addr', {
                           'kind': 'option',
                           'type': SocketAddr
                       }], ['account_id', {
                           'kind': 'option',
                           'type': 'string'
                       }]]
        }
    ],
    [
        PeerChainInfo, {
            'kind':
                'struct',
            'fields': [['genesis_id', GenesisId], ['height', 'u64'],
                       ['tracked_shards', ['u64']]]
        }
    ],
    [
        PeerChainInfoV2, {
            'kind':
                'struct',
            'fields': [['genesis_id', GenesisId], ['height', 'u64'],
                       ['tracked_shards', ['u64']], ['archival', 'bool']]
        }
    ],
    [
        Edge, {
            'kind':
                'struct',
            'fields': [
                ['peer0', PublicKey],
                ['peer1', PublicKey],
                ['nonce', 'u64'],
                ['signature0', Signature],
                ['signature1', Signature],
                ['removal_info', {
                    'kind': 'option',
                    'type': ('u8', Signature)
                }],
            ]
        }
    ],
    [
        SyncData, {
            'kind': 'struct',
            'fields': [
                ['edges', [Edge]],
                ['accounts', [AnnounceAccount]],
            ]
        }
    ],
    [
        EdgeInfo, {
            'kind': 'struct',
            'fields': [
                ['nonce', 'u64'],
                ['signature', Signature],
            ]
        }
    ],
    [
        GenesisId, {
            'kind': 'struct',
            'fields': [
                ['chain_id', 'string'],
                ['hash', [32]],
            ]
        }
    ],
    [
        AnnounceAccount, {
            'kind':
                'struct',
            'fields': [
                ['account_id', 'string'],
                ['peer_id', PublicKey],
                ['epoch_id', [32]],
                ['signature', Signature],
            ]
        }
    ],
    [
        RoutedMessage, {
            'kind':
                'struct',
            'fields': [
                ['target', PeerIdOrHash],
                ['author', PublicKey],
                ['signature', Signature],
                ['ttl', 'u8'],
                ['body', RoutedMessageBody],
            ]
        }
    ],
    [
        PeerIdOrHash, {
            'kind': 'enum',
            'field': 'enum',
            'values': [
                ['PeerId', PublicKey],
                ['Hash', [32]],
            ]
        }
    ],
    [
        RoutedMessageBody,
        {
            'kind':
                'enum',
            'field':
                'enum',
            'values': [
                ['BlockApproval', Approval],
                ['ForwardTx', SignedTransaction],
                ['TxStatusRequest', ('string', [32])],
                ['TxStatusResponse', None],  # TODO
                ['QueryRequest', None],  # TODO
                ['QueryResponse', None],  # TODO
                ['ReceiptOutcomeRequest', [32]],
                ['ReceiptOutcomeResponse', None],  # TODO
                ['StateRequestHeader', ('u64', [32])],
                ['StateRequestPart', ('u64', [32], 'u64')],
                ['StateResponseInfo', StateResponseInfoV1],
                ['PartialEncodedChunkRequest', PartialEncodedChunkRequestMsg],
                ['PartialEncodedChunkResponse', PartialEncodedChunkResponseMsg],
                ['PartialEncodedChunk', PartialEncodedChunkV1],
                ['Ping', PingPong],
                ['Pong', PingPong],
                ['VersionedPartialEncodedChunk', PartialEncodedChunk],
                ['VersionedStateResponse', StateResponseInfo],
                ['PartialEncodedChunkForward', PartialEncodedChunkForwardMsg]
            ]
        }
    ],
    [
        PingPong, {
            'kind': 'struct',
            'fields': [['nonce', 'u64'], ['source', PublicKey]]
        }
    ],
    [
        StateResponseInfo, {
            'kind': 'enum',
            'field': 'enum',
            'values': [['V1', StateResponseInfoV1], ['V2', StateResponseInfoV2]]
        }
    ],
    [
        StateResponseInfoV1, {
            'kind':
                'struct',
            'fields': [['shard_id', 'u64'], ['sync_hash', [32]],
                       ['state_response', ShardStateSyncResponseV1]]
        }
    ],
    [
        StateResponseInfoV2, {
            'kind':
                'struct',
            'fields': [['shard_id', 'u64'], ['sync_hash', [32]],
                       ['state_response', ShardStateSyncResponse]]
        }
    ],
    [
        ShardStateSyncResponse, {
            'kind':
                'enum',
            'field':
                'enum',
            'values': [['V1', ShardStateSyncResponseV1],
                       ['V2', ShardStateSyncResponseV2]]
        }
    ],
    [
        ShardStateSyncResponseV1, {
            'kind':
                'struct',
            'fields': [[
                'header', {
                    'kind': 'option',
                    'type': ShardStateSyncResponseHeaderV1
                }
            ], ['part', {
                'kind': 'option',
                'type': ('u64', ['u8'])
            }]]
        }
    ],
    [
        ShardStateSyncResponseV2, {
            'kind':
                'struct',
            'fields': [[
                'header', {
                    'kind': 'option',
                    'type': ShardStateSyncResponseHeaderV2
                }
            ], ['part', {
                'kind': 'option',
                'type': ('u64', ['u8'])
            }]]
        }
    ],
    [
        ShardStateSyncResponseHeader, {
            'kind':
                'enum',
            'field':
                'enum',
            'values': [['V1', ShardStateSyncResponseHeaderV1],
                       ['V2', ShardStateSyncResponseHeaderV2]]
        }
    ],
    [
        ShardStateSyncResponseHeaderV1, {
            'kind':
                'struct',
            'fields': [['chunk', ShardChunk], ['chunk_proof', MerklePath],
                       [
                           'prev_chunk_header', {
                               'kind': 'option',
                               'type': ShardChunkHeaderV1
                           }
                       ],
                       [
                           'prev_chunk_proof', {
                               'kind': 'option',
                               'type': MerklePath
                           }
                       ],
                       [
                           'incoming_receipts_proofs',
                           [([32], [([Receipt], ShardProof)])]
                       ], ['root_proofs', [[([32], MerklePath)]]],
                       ['state_root_node', StateRootNode]]
        }
    ],
    [
        ShardStateSyncResponseHeaderV2, {
            'kind':
                'struct',
            'fields': [['chunk', ShardChunk], ['chunk_proof', MerklePath],
                       [
                           'prev_chunk_header', {
                               'kind': 'option',
                               'type': ShardChunkHeader
                           }
                       ],
                       [
                           'prev_chunk_proof', {
                               'kind': 'option',
                               'type': MerklePath
                           }
                       ],
                       [
                           'incoming_receipts_proofs',
                           [([32], [([Receipt], ShardProof)])]
                       ], ['root_proofs', [[([32], MerklePath)]]],
                       ['state_root_node', StateRootNode]]
        }
    ],
    [
        RoutingTableSyncV2, {
            'kind': 'enum',
            'field': 'enum',
            'values': [['Version2', RoutingVersion2]]
        }
    ],
    [
        IbfElem, {
            'kind': 'struct',
            'fields': [
                ['xor_elem', 'u64'],
                ['xor_hash', 'u64'],
            ]
        }
    ],
    [
        RoutingVersion2, {
            'kind':
                'struct',
            'fields': [
                ['known_edges', 'u64'],
                ['seed', 'u64'],
                ['edges', [Edge]],
                ['routing_state', RoutingState],
            ]
        }
    ],
    [
        RoutingState, {
            'kind':
                'enum',
            'field':
                'enum',
            'values': [
                ['PartialSync', PartialSync],
                ['RequestAllEdges', ()],
                ['Done', ()],
                ['RequestMissingEdges', ['u64']],
                ['InitializeIbf', ()],
            ]
        }
    ],
    [
        PartialSync, {
            'kind': 'struct',
            'fields': [
                ['ibf_level', 'u64'],
                ['ibf', [IbfElem]],
            ]
        }
    ]
]

'''
'''--- pytest/lib/messages/shard.py ---
class StateRootNode:
    pass

shard_schema = [[
    StateRootNode, {
        'kind': 'struct',
        'fields': [['data', ['u8']], ['memory_usage', 'u64']]
    }
]]

'''
'''--- pytest/lib/messages/tx.py ---
from messages.crypto import Signature, PublicKey, AccessKey

class SignedTransaction:
    pass

class Transaction:
    pass

class Action:
    pass

class CreateAccount:
    pass

class DeployContract:
    pass

class FunctionCall:
    pass

class Transfer:
    pass

class Stake:
    pass

class AddKey:
    pass

class DeleteKey:
    pass

class DeleteAccount:
    pass

class SignedDelegate:
    pass

class DelegateAction:
    pass

class Receipt:
    pass

class ReceiptEnum:
    pass

class ActionReceipt:
    pass

class DataReceipt:
    pass

class DataReceiver:
    pass

tx_schema = [
    [
        SignedTransaction, {
            'kind': 'struct',
            'fields': [['transaction', Transaction], ['signature', Signature]]
        }
    ],
    [
        Transaction, {
            'kind':
                'struct',
            'fields': [['signerId', 'string'], ['publicKey', PublicKey],
                       ['nonce', 'u64'], ['receiverId', 'string'],
                       ['blockHash', [32]], ['actions', [Action]]]
        }
    ],
    [
        Action, {
            'kind':
                'enum',
            'field':
                'enum',
            'values': [
                ['createAccount', CreateAccount],
                ['deployContract', DeployContract],
                ['functionCall', FunctionCall],
                ['transfer', Transfer],
                ['stake', Stake],
                ['addKey', AddKey],
                ['deleteKey', DeleteKey],
                ['deleteAccount', DeleteAccount],
                ['delegate', SignedDelegate],
            ]
        }
    ],
    [CreateAccount, {
        'kind': 'struct',
        'fields': []
    }],
    [DeployContract, {
        'kind': 'struct',
        'fields': [['code', ['u8']]]
    }],
    [
        FunctionCall, {
            'kind':
                'struct',
            'fields': [['methodName', 'string'], ['args', ['u8']],
                       ['gas', 'u64'], ['deposit', 'u128']]
        }
    ],
    [
        SignedDelegate, {
            'kind':
                'struct',
            'fields': [['delegateAction', DelegateAction],
                       ['signature', Signature]]
        }
    ],
    [
        DelegateAction, {
            'kind':
                'struct',
            'fields': [['senderId', 'string'], ['receiverId', 'string'],
                       ['actions', [Action]], ['nonce', 'u64'],
                       ['maxBlockHeight', 'u64'], ['publicKey', PublicKey]]
        }
    ],
    [Transfer, {
        'kind': 'struct',
        'fields': [['deposit', 'u128']]
    }],
    [
        Stake, {
            'kind': 'struct',
            'fields': [['stake', 'u128'], ['publicKey', PublicKey]]
        }
    ],
    [
        AddKey, {
            'kind': 'struct',
            'fields': [['publicKey', PublicKey], ['accessKey', AccessKey]]
        }
    ],
    [DeleteKey, {
        'kind': 'struct',
        'fields': [['publicKey', PublicKey]]
    }],
    [
        DeleteAccount, {
            'kind': 'struct',
            'fields': [['beneficiaryId', 'string']]
        }
    ],
    [
        Receipt, {
            'kind':
                'struct',
            'fields': [
                ['predecessor_id', 'string'],
                ['receiver_id', 'string'],
                ['receipt_id', [32]],
                ['receipt', ReceiptEnum],
            ]
        }
    ],
    [
        ReceiptEnum, {
            'kind': 'enum',
            'field': 'enum',
            'values': [
                ['Action', ActionReceipt],
                ['Data', DataReceipt],
            ]
        }
    ],
    [
        ActionReceipt, {
            'kind':
                'struct',
            'fields': [
                ['signer_id', 'string'],
                ['signer_public_key', PublicKey],
                ['gas_price', 'u128'],
                ['output_data_receivers', [DataReceiver]],
                ['input_data_ids', [[32]]],
                ['actions', [Action]],
            ],
        }
    ],
    [
        DataReceipt, {
            'kind':
                'struct',
            'fields': [
                ['data_id', [32]],
                ['data', {
                    'kind': 'option',
                    'type': ['u8']
                }],
            ]
        }
    ],
    [
        DataReceiver, {
            'kind': 'struct',
            'fields': [
                ['data_id', [32]],
                ['receiver_id', 'string'],
            ]
        }
    ],
]

'''
'''--- pytest/lib/metrics.py ---
from prometheus_client import parser
import requests
import time

BLOCK_TIME_BINS = [
    '0.005', '0.01', '0.025', '0.05', '0.1', '0.25', '0.5', '1', '2.5', '5',
    '10', '+Inf'
]

def fold(collection, key, f, default):
    if key in collection:
        return f(collection[key])
    else:
        return default

class Metrics:

    def __init__(self, total_blocks, memory_usage, total_transactions,
                 block_processing_time, timestamp, blocks_per_second):
        self.total_blocks = total_blocks
        self.memory_usage = memory_usage
        self.total_transactions = total_transactions
        self.block_processing_time = block_processing_time
        self.timestamp = timestamp
        self.blocks_per_second = blocks_per_second

    @classmethod
    def from_url(cls, metrics_url):
        response = requests.get(metrics_url, timeout=10)
        timestamp = time.time()
        response.raise_for_status()
        prometheus_string = response.content.decode('utf8')
        prometheus_metrics = dict(
            map(lambda m: (m.name, m),
                parser.text_string_to_metric_families(prometheus_string)))

        fold_sample = lambda key: fold(prometheus_metrics, key, lambda m: int(
            m.samples[0].value), 0)

        total_blocks = fold_sample('near_block_processed')
        memory_usage = fold_sample('near_memory_usage_bytes')
        total_transactions = fold_sample('near_transaction_processed')
        blocks_per_second = fold_sample('near_blocks_per_minute') / 60.0

        def extract_block_processing_time(m):
            block_processing_time_samples = m.samples
            block_processing_time = {}
            for sample in block_processing_time_samples:
                if 'le' in sample.labels:
                    bound = sample.labels['le']
                    block_processing_time[f'le {bound}'] = int(sample.value)
            return block_processing_time

        block_processing_time = fold(
            prometheus_metrics, 'near_block_processing_time',
            extract_block_processing_time,
            dict(map(lambda bin: ('le ' + bin, 0), BLOCK_TIME_BINS)))

        return cls(total_blocks, memory_usage, total_transactions,
                   block_processing_time, timestamp, blocks_per_second)

    @classmethod
    def diff(cls, final_metrics, initial_metrics):
        total_blocks = final_metrics.total_blocks - initial_metrics.total_blocks
        memory_usage = final_metrics.memory_usage - initial_metrics.memory_usage
        total_transactions = final_metrics.total_transactions - initial_metrics.total_transactions
        timestamp = final_metrics.timestamp - initial_metrics.timestamp
        blocks_per_second = (final_metrics.blocks_per_second +
                             initial_metrics.blocks_per_second) / 2.0
        block_processing_time = {}
        for sample in final_metrics.block_processing_time.keys():
            block_processing_time[sample] = final_metrics.block_processing_time[
                sample] - initial_metrics.block_processing_time[sample]

        return cls(total_blocks, memory_usage, total_transactions,
                   block_processing_time, timestamp, blocks_per_second)

'''
'''--- pytest/lib/mocknet.py ---
import json
import os
import random
import os.path
import shlex
import subprocess
import tempfile
import time
import functools

import base58
import requests
from rc import run, pmap, gcloud

import data
from cluster import GCloudNode
from configured_logger import logger
from key import Key
from metrics import Metrics
from transaction import sign_payment_tx_and_get_hash, sign_staking_tx_and_get_hash

DEFAULT_KEY_TARGET = '/tmp/mocknet'
KEY_TARGET_ENV_VAR = 'NEAR_PYTEST_KEY_TARGET'
# NODE_SSH_KEY_PATH = '~/.ssh/near_ops'
NODE_SSH_KEY_PATH = None
NODE_USERNAME = 'ubuntu'
NUM_ACCOUNTS = 26 * 2
PROJECT = 'near-mocknet'
PUBLIC_KEY = 'ed25519:76NVkDErhbP1LGrSAf5Db6BsFJ6LBw6YVA4BsfTBohmN'
SECRET_KEY = 'ed25519:3cCk8KUWBySGCxBcn1syMoY5u73wx5eaPLRbQcMi23LwBA3aLsqEbA33Ww1bsJaFrchmDciGe9otdn45SrDSkow2'
TX_OUT_FILE = '/home/ubuntu/tx_events'
WASM_FILENAME = 'simple_contract.wasm'

TREASURY_ACCOUNT = 'test.near'
MASTER_ACCOUNT = 'near'
SKYWARD_ACCOUNT = 'skyward.near'
SKYWARD_TOKEN_ACCOUNT = 'token.skyward.near'
TOKEN1_ACCOUNT = 'token1.near'
TOKEN2_ACCOUNT = 'token2.near'
TOKEN2_OWNER_ACCOUNT = 'account.token2.near'
ACCOUNT1_ACCOUNT = 'account1.near'

TMUX_STOP_SCRIPT = '''
while tmux has-session -t near; do
tmux kill-session -t near || true
done
'''

PYTHON_DIR = '/home/ubuntu/.near/pytest/'

PYTHON_SETUP_SCRIPT = f'''
rm -rf {PYTHON_DIR}
mkdir -p {PYTHON_DIR}
python3 -m pip install pip --upgrade
python3 -m pip install virtualenv --upgrade
cd {PYTHON_DIR}
python3 -m virtualenv venv -p $(which python3)
'''

INSTALL_PYTHON_REQUIREMENTS = f'''
cd {PYTHON_DIR}
./venv/bin/pip install -r requirements.txt
'''

ONE_NEAR = 10**24
MIN_STAKE = 64 * (10**3)
STAKE_STEP = 15 * (10**3)
OTHER_STAKE = 10**6
MAINNET_STAKES = [
    43566361, 20091202, 19783811, 18990335, 18196731, 12284685, 10770734,
    10769428, 9858038, 9704977, 8871933, 8296476, 7731153, 7499051, 7322703,
    7307458, 6477856, 6293083, 6242196, 6093107, 6085802, 5553788, 5508664,
    5286843, 5056137, 4944414, 4859235, 4732286, 4615542, 4565243, 4468179,
    4451510, 4444888, 4412264, 4221909, 4219451, 4210541, 4161553, 4116102,
    4085627, 4075090, 3988387, 3932601, 3923842, 3921959, 3915353, 3907857,
    3905980, 3898791, 3886957, 3851553, 3831536, 3790646, 3784485, 3777647,
    3760931, 3746129, 3741225, 3727313, 3699201, 3620341
]

ACCOUNTS = {
    TREASURY_ACCOUNT: (10**7) * ONE_NEAR,
    MASTER_ACCOUNT: (10**7) * ONE_NEAR,
    SKYWARD_ACCOUNT: (10**6) * ONE_NEAR,
    TOKEN1_ACCOUNT: (10**6) * ONE_NEAR,
    TOKEN2_ACCOUNT: (10**6) * ONE_NEAR,
    TOKEN2_OWNER_ACCOUNT: (10**6) * ONE_NEAR,
    ACCOUNT1_ACCOUNT: (10**6) * ONE_NEAR,
}

def get_node(hostname):
    instance_name = hostname
    n = GCloudNode(
        instance_name,
        username=NODE_USERNAME,
        project=PROJECT,
        ssh_key_path=NODE_SSH_KEY_PATH,
    )
    return n

def get_nodes(pattern=None):
    machines = gcloud.list(
        pattern=pattern,
        project=PROJECT,
        username=NODE_USERNAME,
        ssh_key_path=NODE_SSH_KEY_PATH,
    )
    nodes = pmap(
        lambda machine: GCloudNode(
            machine.name,
            username=NODE_USERNAME,
            project=PROJECT,
            ssh_key_path=NODE_SSH_KEY_PATH,
        ),
        machines,
    )
    return nodes

# Needs to be in-sync with init.sh.tmpl in terraform.
def node_account_name(node_name):
    # Assuming node_name is a hostname and looks like
    # 'mocknet-betanet-spoon-abcd' or 'mocknet-zxcv'.
    parts = node_name.split('-')
    return f'{parts[-1]}-load-test.near'

# Constructs an account name given the basic account name.
# Accounts are generated in this order:
# `a00.base`, `b00.base`, .., `z00.base`, `a01.base`, ...
def load_testing_account_id(node_account_id, i):
    NUM_LETTERS = 26
    letter = i % NUM_LETTERS
    num = i // NUM_LETTERS
    return '%s%02d.%s' % (chr(ord('a') + letter), num, node_account_id)

def get_validator_account(node):
    return Key.from_json(
        download_and_read_json(node, '/home/ubuntu/.near/validator_key.json'))

def list_validators(node):
    validators = node.get_validators()['result']
    validator_accounts = set(
        map(lambda v: v['account_id'], validators['current_validators']))
    return validator_accounts

def setup_python_environment(node, wasm_contract):
    m = node.machine
    logger.info(f'Setting up python environment on {m.name}')
    m.run('bash', input=PYTHON_SETUP_SCRIPT)
    m.upload('lib', PYTHON_DIR, switch_user='ubuntu')
    m.upload('requirements.txt', PYTHON_DIR, switch_user='ubuntu')
    m.upload(wasm_contract,
             os.path.join(PYTHON_DIR, WASM_FILENAME),
             switch_user='ubuntu')
    m.upload('tests/mocknet/helpers/*.py', PYTHON_DIR, switch_user='ubuntu')
    m.run('bash', input=INSTALL_PYTHON_REQUIREMENTS)
    logger.info(f'{m.name} python setup complete')

def setup_python_environments(nodes, wasm_contract):
    pmap(lambda n: setup_python_environment(n, wasm_contract), nodes)

def start_load_test_helper_script(
    script,
    node_account_id,
    rpc_nodes,
    num_nodes,
    max_tps,
    test_timeout,
    contract_deploy_time,
):
    s = '''
        cd {dir}
        nohup ./venv/bin/python {script} \\
            --node-account-id {node_account_id} \\
            --rpc-nodes {rpc_nodes} \\
            --num-nodes {num_nodes} \\
            --max-tps {max_tps} \\
            --test-timeout {test_timeout} \\
            --contract-deploy-time {contract_deploy_time} \\
            1>load_test.out 2>load_test.err < /dev/null &
    '''.format(
        dir=shlex.quote(PYTHON_DIR),
        script=shlex.quote(script),
        node_account_id=shlex.quote(node_account_id),
        rpc_nodes=shlex.quote(rpc_nodes),
        num_nodes=shlex.quote(str(num_nodes)),
        max_tps=shlex.quote(str(max_tps)),
        test_timeout=shlex.quote(str(test_timeout)),
        contract_deploy_time=shlex.quote(str(contract_deploy_time)),
    )
    logger.info(
        f'Starting load test helper. Node accound id: {node_account_id}.')
    logger.debug(f'The load test helper script is:{s}')
    return s

def start_load_test_helper(
    script,
    node,
    rpc_nodes,
    num_nodes,
    max_tps,
    test_timeout,
    contract_deploy_time,
):
    logger.info(f'Starting load_test_helper on {node.instance_name}')
    rpc_node_ips = ','.join([rpc_node.ip for rpc_node in rpc_nodes])
    node.machine.run(
        'bash',
        input=start_load_test_helper_script(
            script,
            node_account_name(node.instance_name),
            rpc_node_ips,
            num_nodes,
            max_tps,
            test_timeout,
            contract_deploy_time,
        ),
    )

def start_load_test_helpers(
    script,
    validator_nodes,
    rpc_nodes,
    max_tps,
    test_timeout,
    contract_deploy_time,
):
    pmap(
        lambda node: start_load_test_helper(
            script,
            node,
            rpc_nodes,
            len(validator_nodes),
            max_tps,
            test_timeout,
            contract_deploy_time,
        ),
        validator_nodes,
    )

def get_log(node):
    target_file = f'./logs/{node.instance_name}.log'
    node.machine.download('/home/ubuntu/near.log', target_file)

def get_logs(nodes):
    pmap(get_log, nodes)

def get_epoch_length_in_blocks(node):
    config = download_and_read_json(node, '/home/ubuntu/.near/genesis.json')
    epoch_length_in_blocks = config['epoch_length']
    return epoch_length_in_blocks

def get_metrics(node):
    (addr, port) = node.rpc_addr()
    metrics_url = f'http://{addr}:{port}/metrics'
    return Metrics.from_url(metrics_url)

def get_timestamp(block):
    return block['header']['timestamp'] / 1e9

def get_chunk_txn(index, chunks, archival_node, result):
    chunk = chunks[index]
    chunk_hash = chunk['chunk_hash']
    result[index] = len(
        archival_node.get_chunk(chunk_hash)['result']['transactions'])

# Measure bps and tps by directly checking block timestamps and number of transactions
# in each block.
def chain_measure_bps_and_tps(
    archival_node,
    start_time,
    end_time,
    duration=None,
):
    latest_block_hash = archival_node.get_latest_block().hash
    curr_block = archival_node.get_block(latest_block_hash)['result']
    curr_time = get_timestamp(curr_block)

    if end_time is None:
        end_time = curr_time
    if start_time is None:
        start_time = end_time - duration
    logger.info(
        f'Measuring BPS and TPS in the time range {start_time} to {end_time}')

    # One entry per block, equal to the timestamp of that block.
    block_times = []
    # One entry per block, containing the count of transactions in all chunks of the block.
    tx_count = []
    while curr_time > start_time:
        if curr_time < end_time:
            block_times.append(curr_time)
            gas_per_chunk = []
            for chunk in curr_block['chunks']:
                gas_per_chunk.append(chunk['gas_used'] * 1e-12)
            gas_block = sum(gas_per_chunk)
            tx_per_chunk = [None] * len(curr_block['chunks'])
            pmap(
                lambda i: get_chunk_txn(i, curr_block['chunks'], archival_node,
                                        tx_per_chunk),
                range(len(curr_block['chunks'])))
            txs = sum(tx_per_chunk)
            tx_count.append(txs)
            logger.info(
                f'Processed block at time {curr_time} height #{curr_block["header"]["height"]}, # txs in a block: {txs}, per chunk: {tx_per_chunk}, gas in block: {gas_block}, gas per chunk: {gas_per_chunk}'
            )
        prev_hash = curr_block['header']['prev_hash']
        curr_block = archival_node.get_block(prev_hash)['result']
        curr_time = get_timestamp(curr_block)
    block_times.reverse()
    tx_count.reverse()
    assert block_times
    tx_cumulative = data.compute_cumulative(tx_count)
    bps = data.compute_rate(block_times)
    tps_fit = data.linear_regression(block_times, tx_cumulative)
    logger.info(
        f'Num blocks: {len(block_times)}, num transactions: {len(tx_count)}, bps: {bps}, tps_fit: {tps_fit}'
    )
    return {'bps': bps, 'tps': tps_fit['slope']}

def get_tx_events_single_node(node, tx_filename):
    try:
        target_file = f'./logs/{node.instance_name}_txs'
        node.machine.download(tx_filename, target_file)
        with open(target_file, 'r') as f:
            return [float(line.strip()) for line in f.readlines()]
    except Exception as e:
        logger.exception(f'Getting tx_events from {node.instance_name} failed')
        return []

def get_tx_events(nodes, tx_filename):
    run('mkdir ./logs/')
    run('rm -rf ./logs/*_txs')
    all_events = pmap(
        lambda node: get_tx_events_single_node(node, tx_filename),
        nodes,
    )
    return sorted(data.flatten(all_events))

# Sends the transaction to the network via `node` and checks for success.
# Some retrying is done when the node returns a Timeout error.
def send_transaction(node, tx, tx_hash, account_id, timeout=120):
    response = node.send_tx_and_wait(tx, timeout)
    loop_start = time.time()
    missing_count = 0
    while 'error' in response.keys():
        error_data = response['error']['data']
        if 'timeout' in error_data.lower():
            logger.warning(
                f'transaction {tx_hash} returned Timout, checking status again.'
            )
            time.sleep(5)
            response = node.get_tx(tx_hash, account_id)
        elif 'does not exist' in error_data:
            missing_count += 1
            logger.warning(
                f'transaction {tx_hash} failed to be received by the node, checking again.'
            )
            if missing_count < 20:
                time.sleep(5)
                response = node.get_tx(tx_hash, account_id)
            else:
                logger.warning(f're-sending transaction {tx_hash}.')
                response = node.send_tx_and_wait(tx, timeout)
                missing_count = 0
        else:
            raise RuntimeError(
                f'Error in processing transaction {tx_hash}: {response}')
        if time.time() - loop_start > timeout:
            raise TimeoutError(
                f'Transaction {tx_hash} did not complete successfully within the timeout'
            )

    if 'SuccessValue' not in response['result']['status']:
        raise RuntimeError(
            f'ERROR: Failed transaction {tx_hash}. Response: {response}')

def transfer_between_nodes(nodes):
    logger.info('Testing transfer between mocknet validators')
    node = nodes[0]
    alice = get_validator_account(nodes[1])
    bob = get_validator_account(nodes[0])
    transfer_amount = 100
    get_balance = lambda account: int(
        node.get_account(account.account_id)['result']['amount'])

    alice_initial_balance = get_balance(alice)
    alice_nonce = node.get_nonce_for_pk(alice.account_id, alice.pk)
    bob_initial_balance = get_balance(bob)
    logger.info(f'Alice initial balance: {alice_initial_balance}')
    logger.info(f'Bob initial balance: {bob_initial_balance}')

    last_block_hash = node.get_latest_block().hash_bytes

    tx, tx_hash = sign_payment_tx_and_get_hash(alice, bob.account_id,
                                               transfer_amount, alice_nonce + 1,
                                               last_block_hash)
    send_transaction(node, tx, tx_hash, alice.account_id)

    alice_final_balance = get_balance(alice)
    bob_final_balance = get_balance(bob)
    logger.info(f'Alice final balance: {alice_final_balance}')
    logger.info(f'Bob final balance: {bob_final_balance}')

    # Check mod 1000 to ignore the cost of the transaction itself
    assert (alice_initial_balance -
            alice_final_balance) % 1000 == transfer_amount
    assert bob_final_balance - bob_initial_balance == transfer_amount

def stake_node(node):
    account = get_validator_account(node)
    logger.info(f'Staking {account.account_id}.')
    nonce = node.get_nonce_for_pk(account.account_id, account.pk)

    validators = node.get_validators(timeout=None)['result']
    if account.account_id in validators['current_validators']:
        return
    stake_amount = max(
        map(lambda v: int(v['stake']), validators['current_validators']))

    latest_block_hash = node.get_status()['sync_info']['latest_block_hash']
    last_block_hash_decoded = base58.b58decode(latest_block_hash.encode('utf8'))

    staking_tx, staking_tx_hash = sign_staking_tx_and_get_hash(
        account, account, stake_amount, nonce + 1, last_block_hash_decoded)
    send_transaction(node, staking_tx, staking_tx_hash, account.account_id)

def accounts_from_nodes(nodes):
    return pmap(get_validator_account, nodes)

def kill_proccess_script(pid):
    return f'''
        sudo kill {pid}
        while kill -0 {pid}; do
            sleep 1
        done
    '''

def get_near_pid(machine):
    p = machine.run(
        "ps aux | grep 'near.* run' | grep -v grep | awk '{print $2}'")
    return p.stdout.strip()

def is_binary_running(binary_name: str, node) -> bool:
    result = node.machine.run(f'ps -A -o comm= | grep {binary_name}')
    return result.returncode == 0

def is_binary_running_all_nodes(binary_name: str, all_nodes) -> bool:
    return pmap(functools.partial(is_binary_running, binary_name), all_nodes)

def stop_node(node):
    m = node.machine
    logger.info(f'Stopping node {m.name}')
    pids = get_near_pid(m).split()

    for pid in pids:
        m.run('bash', input=kill_proccess_script(pid))
        m.run('sudo -u ubuntu -i', input=TMUX_STOP_SCRIPT)

def upload_and_extract(node, src_filename, dst_filename):
    node.machine.upload(f'{src_filename}.gz',
                        f'{dst_filename}.gz',
                        switch_user='ubuntu')
    node.machine.run('gunzip -f {dst_filename}.gz'.format(
        dst_filename=shlex.quote(dst_filename)))

def compress_and_upload(nodes, src_filename, dst_filename):
    res = run(f'gzip {src_filename}')
    assert res.returncode == 0
    pmap(lambda node: upload_and_extract(node, src_filename, dst_filename),
         nodes)

def redownload_neard(nodes, binary_url):
    pmap(
        lambda node: node.machine.
        run('sudo -u ubuntu -i',
            input='wget -O /home/ubuntu/neard "{}"; chmod +x /home/ubuntu/neard'
            .format(binary_url)), nodes)

# Check /home/ubuntu/neard.upgrade to see whether the amend-genesis command is
# available. Returns the path to the neard binary or throws an exception.
def neard_amend_genesis_path(node):
    r = node.machine.run(
        '/home/ubuntu/neard.upgrade amend-genesis --help',
        stdout=subprocess.DEVNULL,
        stderr=subprocess.DEVNULL,
    )
    if r.exitcode == 0:
        return '/home/ubuntu/neard.upgrade'
    raise Exception(f'`neard.upgrade amend-genesis` not available')

# We assume that the nodes already have the .near directory with the files
# node_key.json, validator_key.json and config.json.
def create_and_upload_genesis(
    validator_nodes,
    chain_id,
    rpc_nodes=None,
    epoch_length=20000,
    node_pks=None,
    increasing_stakes=0.0,
    num_seats=100,
    single_shard=False,
    all_node_pks=None,
    node_ips=None,
):
    logger.info('Uploading genesis and config files')
    assert chain_id
    logger.info(
        'Assuming that genesis_updater.py is available on the instances.')
    validator_keys = dict(pmap(get_validator_key, validator_nodes))
    rpc_node_names = [node.instance_name for node in rpc_nodes]
    assert '-spoon' in chain_id, f'Expecting chain_id like "testnet-spoon" or "mainnet-spoon", got {chain_id}'
    chain_id_in = chain_id.split('-spoon')[0]
    genesis_filename_in = f'/home/ubuntu/.near/{chain_id_in}-genesis/genesis.json'
    records_filename_in = f'/home/ubuntu/.near/{chain_id_in}-genesis/records.json'
    config_filename_in = f'/home/ubuntu/.near/{chain_id_in}-genesis/config.json'
    stamp = time.strftime('%Y%m%d-%H%M%S', time.gmtime())
    done_filename = f'/home/ubuntu/genesis_update_done_{stamp}.txt'
    neard = neard_amend_genesis_path(validator_nodes[1])
    pmap(
        lambda node: start_genesis_updater(
            node=node,
            script='genesis_updater.py',
            genesis_filename_in=genesis_filename_in,
            records_filename_in=records_filename_in,
            config_filename_in=config_filename_in,
            out_dir='/home/ubuntu/.near/',
            chain_id=chain_id,
            validator_keys=validator_keys,
            rpc_nodes=rpc_node_names,
            done_filename=done_filename,
            epoch_length=epoch_length,
            node_pks=node_pks,
            increasing_stakes=increasing_stakes,
            num_seats=num_seats,
            single_shard=single_shard,
            all_node_pks=all_node_pks,
            node_ips=node_ips,
            neard=neard,
        ),
        validator_nodes + rpc_nodes,
    )
    pmap(lambda node: wait_genesis_updater_done(node, done_filename),
         validator_nodes + rpc_nodes)

def extra_genesis_records(validator_keys, rpc_node_names, node_pks,
                          seen_accounts, num_seats, increasing_stakes):
    records = []

    VALIDATOR_BALANCE = (10**2) * ONE_NEAR
    RPC_BALANCE = (10**1) * ONE_NEAR
    LOAD_TESTER_BALANCE = (10**4) * ONE_NEAR

    for account_id, balance in ACCOUNTS.items():
        if account_id not in seen_accounts:
            records.append({
                'Account': {
                    'account_id': account_id,
                    'account': {
                        'amount': str(balance),
                        'locked': '0',
                        'code_hash': '11111111111111111111111111111111',
                        'storage_usage': 0,
                        'version': 'V1'
                    }
                }
            })
        pkeys = [PUBLIC_KEY]
        if node_pks:
            pkeys += node_pks
        for pk in pkeys:
            records.append({
                'AccessKey': {
                    'account_id': account_id,
                    'public_key': pk,
                    'access_key': {
                        'nonce': 0,
                        'permission': 'FullAccess'
                    }
                }
            })

    stakes = []
    prev_stake = None
    for i, account_id in enumerate(validator_keys):
        logger.info(f'Adding account {account_id}')
        if increasing_stakes:
            if i * 5 < num_seats * 3 and i < len(MAINNET_STAKES):
                staked = MAINNET_STAKES[i] * ONE_NEAR
            elif prev_stake is None:
                prev_stake = MIN_STAKE - STAKE_STEP
                staked = prev_stake * ONE_NEAR
            else:
                prev_stake = prev_stake + STAKE_STEP
                staked = prev_stake * ONE_NEAR
        else:
            staked = MIN_STAKE
        stakes.append((staked, account_id))
        records.append({
            'Account': {
                'account_id': account_id,
                'account': {
                    'amount': str(VALIDATOR_BALANCE),
                    'locked': str(staked),
                    'code_hash': '11111111111111111111111111111111',
                    'storage_usage': 0,
                    'version': 'V1'
                }
            }
        })
        records.append({
            'AccessKey': {
                'account_id': account_id,
                'public_key': PUBLIC_KEY,
                'access_key': {
                    'nonce': 0,
                    'permission': 'FullAccess'
                }
            }
        })
        for i in range(NUM_ACCOUNTS):
            load_testing_account = load_testing_account_id(account_id, i)
            logger.info(f'Adding load testing account {load_testing_account}')
            records.append({
                'Account': {
                    'account_id': load_testing_account,
                    'account': {
                        'amount': str(LOAD_TESTER_BALANCE),
                        'locked': str(0),
                        'code_hash': '11111111111111111111111111111111',
                        'storage_usage': 0,
                        'version': 'V1'
                    }
                }
            })
            records.append({
                'AccessKey': {
                    'account_id': load_testing_account,
                    'public_key': PUBLIC_KEY,
                    'access_key': {
                        'nonce': 0,
                        'permission': 'FullAccess'
                    }
                }
            })
    for node_name in rpc_node_names:
        account_id = node_account_name(node_name)
        logger.info(f'Adding rpc node account {account_id}')
        records.append({
            'Account': {
                'account_id': account_id,
                'account': {
                    'amount': str(RPC_BALANCE),
                    'locked': str(0),
                    'code_hash': '11111111111111111111111111111111',
                    'storage_usage': 0,
                    'version': 'V1'
                }
            }
        })
        records.append({
            'AccessKey': {
                'account_id': account_id,
                'public_key': PUBLIC_KEY,
                'access_key': {
                    'nonce': 0,
                    'permission': 'FullAccess'
                }
            }
        })

    validators = []
    seats = compute_seats(stakes, num_seats)
    seats_taken = 0
    for seats, staked, account_id in seats:
        if seats + seats_taken > num_seats:
            break
        validators.append({
            'account_id': account_id,
            'public_key': validator_keys[account_id],
            'amount': str(staked),
        })
        seats_taken += seats

    return records, validators

def neard_amend_genesis(neard, validator_keys, genesis_filename_in,
                        records_filename_in, out_dir, rpc_node_names, chain_id,
                        epoch_length, node_pks, increasing_stakes, num_seats,
                        single_shard):
    extra_records, validators = extra_genesis_records(validator_keys,
                                                      rpc_node_names, node_pks,
                                                      set(), num_seats,
                                                      increasing_stakes)

    validators_filename = os.path.join(out_dir, 'validators.json')
    extra_records_filename = os.path.join(out_dir, 'extra-records.json')
    genesis_filename_out = os.path.join(out_dir, 'genesis.json')
    records_filename_out = os.path.join(out_dir, 'records.json')

    with open(validators_filename, 'w') as f:
        json.dump(validators, f)
    with open(extra_records_filename, 'w') as f:
        json.dump(extra_records, f)

    cmd = [
        neard,
        'amend-genesis',
        '--genesis-file-in',
        genesis_filename_in,
        '--records-file-in',
        records_filename_in,
        '--extra-records',
        extra_records_filename,
        '--validators',
        validators_filename,
        '--genesis-file-out',
        genesis_filename_out,
        '--records-file-out',
        records_filename_out,
        '--num-seats',
        str(int(num_seats)),
        '--transaction-validity-period',
        str(10**9),
        '--protocol-version',
        '49',
        '--protocol-reward-rate',
        '1/10',
        '--block-producer-kickout-threshold',
        '10',
    ]
    if chain_id is not None:
        cmd.extend(['--chain-id', chain_id])
    if epoch_length is not None:
        cmd.extend(['--epoch-length', str(epoch_length)])
    if single_shard:
        shard_layout_filename = os.path.join(out_dir, 'shard_layout.json')
        with open(shard_layout_filename, 'w') as f:
            json.dump({'V0': {'num_shards': 1, 'version': 0}}, f)

        cmd.extend(['--shard-layout-file', shard_layout_filename])

    subprocess.run(cmd, text=True)

def create_and_upload_genesis_file_from_empty_genesis(
    validator_node_and_stakes,
    rpc_nodes,
    chain_id=None,
    epoch_length=None,
    num_seats=None,
):
    node0 = validator_node_and_stakes[0][0]
    node0.machine.run(
        'rm -rf /home/ubuntu/.near-tmp && mkdir /home/ubuntu/.near-tmp && /home/ubuntu/neard --home /home/ubuntu/.near-tmp init --chain-id {}'
        .format(chain_id))
    genesis_config = download_and_read_json(
        node0, "/home/ubuntu/.near-tmp/genesis.json")
    records = []

    VALIDATOR_BALANCE = (10**2) * ONE_NEAR
    RPC_BALANCE = (10**1) * ONE_NEAR
    TREASURY_ACCOUNT = 'test.near'
    TREASURY_BALANCE = (10**7) * ONE_NEAR
    LOAD_TESTER_BALANCE = (10**8) * ONE_NEAR

    SKYWARD_CONTRACT_BALANCE = (10**6) * ONE_NEAR
    TOKEN1_BALANCE = (10**6) * ONE_NEAR
    TOKEN2_BALANCE = (10**6) * ONE_NEAR
    TOKEN2_OWNER_BALANCE = (10**6) * ONE_NEAR
    ACCOUNT1_BALANCE = (10**6) * ONE_NEAR

    genesis_config['chain_id'] = chain_id

    master_balance = 10**7
    assert master_balance > 0
    accounts = {
        TREASURY_ACCOUNT: TREASURY_BALANCE,
        MASTER_ACCOUNT: master_balance,
        SKYWARD_ACCOUNT: SKYWARD_CONTRACT_BALANCE,
        TOKEN1_ACCOUNT: TOKEN1_BALANCE,
        TOKEN2_ACCOUNT: TOKEN2_BALANCE,
        TOKEN2_OWNER_ACCOUNT: TOKEN2_OWNER_BALANCE,
        ACCOUNT1_ACCOUNT: ACCOUNT1_BALANCE
    }

    for account_id, balance in accounts.items():
        records.append({
            'Account': {
                'account_id': account_id,
                'account': {
                    'amount': str(balance),
                    'locked': '0',
                    'code_hash': '11111111111111111111111111111111',
                    'storage_usage': 0,
                    'version': 'V1'
                }
            }
        })
        records.append({
            'AccessKey': {
                'account_id': account_id,
                'public_key': PUBLIC_KEY,
                'access_key': {
                    'nonce': 0,
                    'permission': 'FullAccess'
                }
            }
        })

    stakes = []
    account_id_to_validator_pk = {}
    for i, (node, stake_multiplier) in enumerate(validator_node_and_stakes):
        validator = get_validator_account(node)
        logger.info(f'Adding account {validator.account_id}')
        account_id_to_validator_pk[validator.account_id] = validator.pk
        staked = MIN_STAKE * stake_multiplier
        stakes.append((staked, validator.account_id))
        records.append({
            'Account': {
                'account_id': validator.account_id,
                'account': {
                    'amount': str(VALIDATOR_BALANCE),
                    'locked': str(staked),
                    'code_hash': '11111111111111111111111111111111',
                    'storage_usage': 0,
                    'version': 'V1'
                }
            }
        })
        records.append({
            'AccessKey': {
                'account_id': validator.account_id,
                'public_key': PUBLIC_KEY,
                'access_key': {
                    'nonce': 0,
                    'permission': 'FullAccess'
                }
            }
        })

    load_testing_account = 'loadtester'
    logger.info(f'Adding load testing account {load_testing_account}')
    records.append({
        'Account': {
            'account_id': load_testing_account,
            'account': {
                'amount': str(LOAD_TESTER_BALANCE),
                'locked': str(0),
                'code_hash': '11111111111111111111111111111111',
                'storage_usage': 0,
                'version': 'V1'
            }
        }
    })
    records.append({
        'AccessKey': {
            'account_id': load_testing_account,
            'public_key': PUBLIC_KEY,
            'access_key': {
                'nonce': 0,
                'permission': 'FullAccess'
            }
        }
    })
    genesis_config['validators'] = []
    seats = compute_seats(stakes, num_seats)
    seats_taken = 0
    for seats, staked, account_id in seats:
        if seats + seats_taken > num_seats:
            break
        genesis_config['validators'].append({
            'account_id': account_id,
            'public_key': account_id_to_validator_pk[account_id],
            'amount': str(staked),
        })
        seats_taken += seats

    total_supply = 0
    for record in records:
        account = record.get('Account', {}).get('account', {})
        total_supply += int(account.get('locked', 0))
        total_supply += int(account.get('amount', 0))
    genesis_config['total_supply'] = str(total_supply)
    genesis_config['protocol_version'] = 57
    genesis_config['epoch_length'] = int(epoch_length)
    genesis_config['num_block_producer_seats'] = int(num_seats)
    genesis_config['protocol_reward_rate'] = [1, 10]
    # Loadtest helper signs all transactions using the same block.
    # Extend validity period to allow the same hash to be used for the whole duration of the test.
    genesis_config['transaction_validity_period'] = 10**9
    # Protocol upgrades require downtime, therefore make it harder to kickout validators.
    # The default value of this parameter is 90.
    genesis_config['block_producer_kickout_threshold'] = 10

    genesis_config['shard_layout'] = {'V0': {'num_shards': 4, 'version': 0}}
    genesis_config['simple_nightshade_shard_layout'] = {}
    genesis_config['num_block_producer_seats_per_shard'] = [int(num_seats)] * 4

    genesis_config['records'] = records
    pmap(
        lambda node: upload_json(node, '/home/ubuntu/.near/genesis.json',
                                 genesis_config),
        [node for (node, _) in validator_node_and_stakes] + rpc_nodes)

def download_and_read_json(node, filename):
    try:
        tmp_file = tempfile.NamedTemporaryFile(mode='r+', delete=False)
        node.machine.download(filename, tmp_file.name)
        tmp_file.close()
        with open(tmp_file.name, 'r') as f:
            return json.load(f)
    except:
        logger.error(
            f'Failed to download json file. Node: {node.instance_name}. Filename: {filename}'
        )
        raise

def upload_json(node, filename, data):
    try:
        logger.info(f'Upload file {filename} to {node.instance_name}')
        tmp_file = tempfile.NamedTemporaryFile(mode='r+', delete=False)
        with open(tmp_file.name, 'w') as f:
            json.dump(data, f, indent=2)
        node.machine.upload(tmp_file.name, filename)
        tmp_file.close()
    except:
        logger.error(
            f'Failed to upload json file. Node: {node.instance_name}. Filename: {filename}'
        )
        raise

def get_node_addr(node, port):
    node_key_json = download_and_read_json(node,
                                           '/home/ubuntu/.near/node_key.json')
    return f'{node_key_json["public_key"]}@{node.ip}:{port}'

def get_validator_account_id(node):
    node_key_json = download_and_read_json(
        node, '/home/ubuntu/.near/validator_key.json')
    return node_key_json["account_id"]

def get_validator_key(node):
    node_key_json = download_and_read_json(
        node, '/home/ubuntu/.near/validator_key.json')
    return node_key_json["account_id"], node_key_json["public_key"]

def get_node_keys(node):
    logger.info(f'get_node_keys from {node.instance_name}')
    node_key_json = download_and_read_json(
        node,
        '/home/ubuntu/.near/node_key.json',
    )
    return node_key_json['public_key'], node_key_json['secret_key']

def init_validator_key(node):
    account_id = node_account_name(node.instance_name)
    node.machine.run(
        f'dir=$(mktemp -d) && /home/ubuntu/neard --home $dir init --account-id {account_id} && mv $dir/validator_key.json /home/ubuntu/.near/validator_key.json'
    )

def update_config_file(
    config_filename_in,
    config_filename_out,
    all_node_pks,
    node_ips,
):
    with open(config_filename_in) as f:
        config_json = json.load(f)

    # Usually the port is 24567
    port = config_json['network']['addr'].split(':')[1]
    node_addresses = [
        f'{node_key}@{node_ip}:{port}'
        for node_key, node_ip in zip(all_node_pks, node_ips)
    ]

    config_json['tracked_shards'] = [0]
    config_json['archive'] = True
    config_json['archival_peer_connections_lower_bound'] = 1
    config_json['network']['boot_nodes'] = ','.join(node_addresses)
    config_json['rpc']['addr'] = '0.0.0.0:3030'
    if 'telemetry' in config_json:
        config_json['telemetry']['endpoints'] = []

    with open(config_filename_out, 'w') as f:
        json.dump(config_json, f, indent=2)

def upload_config(node, config_json, overrider):
    copied_config = json.loads(json.dumps(config_json))
    if overrider:
        overrider(node, copied_config)
    upload_json(node, '/home/ubuntu/.near/config.json', copied_config)

def create_and_upload_config_file_from_default(nodes, chain_id, overrider=None):
    nodes[0].machine.run(
        'rm -rf /home/ubuntu/.near-tmp && mkdir /home/ubuntu/.near-tmp && /home/ubuntu/neard --home /home/ubuntu/.near-tmp init --chain-id {}'
        .format(chain_id))
    config_json = download_and_read_json(
        nodes[0],
        '/home/ubuntu/.near-tmp/config.json',
    )
    config_json['tracked_shards'] = [0, 1, 2, 3]
    config_json['archive'] = True
    config_json['archival_peer_connections_lower_bound'] = 1
    node_addresses = [get_node_addr(node, 24567) for node in nodes]
    config_json['network']['boot_nodes'] = ','.join(node_addresses)
    config_json['network']['skip_sync_wait'] = False
    config_json['rpc']['addr'] = '0.0.0.0:3030'
    config_json['rpc']['enable_debug_rpc'] = True
    if 'telemetry' in config_json:
        config_json['telemetry']['endpoints'] = []

    pmap(lambda node: upload_config(node, config_json, overrider), nodes)

def update_existing_config_file(node, overrider=None):
    config_json = download_and_read_json(
        node,
        '/home/ubuntu/.near/config.json',
    )
    overrider(node, config_json)
    upload_json(node, '/home/ubuntu/.near/config.json', config_json)

def update_existing_config_files(nodes, overrider=None):
    pmap(
        lambda node: update_existing_config_file(node, overrider=overrider),
        nodes,
    )

def start_nodes(nodes, upgrade_schedule=None):
    pmap(
        lambda node: start_node(node, upgrade_schedule=upgrade_schedule),
        nodes,
    )

def stop_nodes(nodes):
    pmap(stop_node, nodes)

def clear_data(nodes):
    pmap(lambda node: node.machine.run('rm -rf /home/ubuntu/.near/data'), nodes)

def neard_start_script(node, upgrade_schedule=None, epoch_height=None):
    if upgrade_schedule and upgrade_schedule.get(node.instance_name,
                                                 0) <= epoch_height:
        neard_binary = '/home/ubuntu/neard.upgrade'
    else:
        neard_binary = '/home/ubuntu/neard'
    return '''
        sudo mv /home/ubuntu/near.log /home/ubuntu/near.log.1 2>/dev/null
        sudo mv /home/ubuntu/near.upgrade.log /home/ubuntu/near.upgrade.log.1 2>/dev/null
        tmux new -s near -d bash
        sudo rm -rf /home/ubuntu/neard.log
        tmux send-keys -t near 'RUST_BACKTRACE=full RUST_LOG=debug,actix_web=info {neard_binary} run 2>&1 | tee -a {neard_binary}.log' C-m
    '''.format(neard_binary=shlex.quote(neard_binary))

def start_node(node, upgrade_schedule=None):
    m = node.machine
    logger.info(f'Starting node {m.name}')
    attempt = 0
    success = False
    while attempt < 3:
        pid = get_near_pid(m)
        if pid != '':
            success = True
            break
        start_process = m.run(
            'sudo -u ubuntu -i',
            input=neard_start_script(
                node,
                upgrade_schedule=upgrade_schedule,
                epoch_height=0,
            ),
        )
        if start_process.returncode == 0:
            success = True
            break
        logger.warn(
            f'Failed to start process, returncode: {start_process.returncode}\n{node.instance_name}\n{start_process.stderr}'
        )
        attempt += 1
        time.sleep(1)
    if not success:
        raise Exception(f'Could not start node {node.instance_name}')

def reset_data(node, retries=0):
    try:
        m = node.machine
        stop_node(node)
        logger.info(f'Clearing data directory of node {m.name}')
        start_process = m.run('bash', input='rm -r /home/ubuntu/.near')
        assert start_process.returncode == 0, m.name + '\n' + start_process.stderr
    except:
        if retries < 3:
            logger.warning(
                'And error occurred while clearing data directory, retrying')
            reset_data(node, retries=retries + 1)
        else:
            raise Exception(
                f'ERROR: Could not clear data directory for {node.machine.name}'
            )

def start_genesis_updater_script(
    script,
    genesis_filename_in,
    records_filename_in,
    config_filename_in,
    out_dir,
    chain_id,
    validator_keys,
    rpc_nodes,
    done_filename,
    epoch_length,
    node_pks,
    increasing_stakes,
    num_seats,
    single_shard,
    all_node_pks,
    node_ips,
    neard,
):
    validators = ','.join(
        [f'{account_id}={key}' for (account_id, key) in validator_keys.items()])
    cmd = ' '.join([
        shlex.quote(str(arg)) for arg in [
            'nohup',
            './venv/bin/python',
            script,
            genesis_filename_in,
            records_filename_in,
            config_filename_in,
            out_dir,
            chain_id,
            validators,
            ','.join(rpc_nodes),
            done_filename,
            epoch_length,
            ','.join(node_pks),
            increasing_stakes,
            num_seats,
            single_shard,
            ','.join(all_node_pks),
            ','.join(node_ips),
            neard if neard is not None else 'None',
        ]
    ])
    return '''
        cd {dir}
        {cmd} 1> genesis_updater.out 2> genesis_updater.err < /dev/null &
        '''.format(dir=shlex.quote(PYTHON_DIR), cmd=cmd)

def start_genesis_updater(node, script, genesis_filename_in,
                          records_filename_in, config_filename_in, out_dir,
                          chain_id, validator_keys, rpc_nodes, done_filename,
                          epoch_length, node_pks, increasing_stakes, num_seats,
                          single_shard, all_node_pks, node_ips, neard):
    logger.info(f'Starting genesis_updater on {node.instance_name}')
    node.machine.run(
        'bash',
        input=start_genesis_updater_script(
            script,
            genesis_filename_in,
            records_filename_in,
            config_filename_in,
            out_dir,
            chain_id,
            validator_keys,
            rpc_nodes,
            done_filename,
            epoch_length,
            node_pks,
            increasing_stakes,
            num_seats,
            single_shard,
            all_node_pks,
            node_ips,
            neard,
        ),
    )

def wait_genesis_updater_done(node, done_filename):
    msg = f'Waiting for the genesis updater on {node.instance_name}'
    logger.info(msg)

    # Wait until the neard process stops running.
    while True:
        time.sleep(5)
        if not is_binary_running('neard', node):
            break

    # Check if the done_filename was produced.
    result = node.machine.run(f'ls {done_filename}')
    if result.returncode != 0:
        # Get the stderr of the genesis updater.
        script = f'cat {shlex.quote(PYTHON_DIR)}/genesis_updater.err'
        stderr = node.machine.run(script).stdout
        error_msg = f'The {done_filename} is missing! The stderr is \n\n\n{stderr}\n'
        raise Exception(f'{msg} - failed. {error_msg}')

    logger.info(f'{msg} -- done')

# Waits until the node becomes responsive to RPC requests. It works the same as
# list_validators but assumes that the node can still be starting and expects
# connection errors and the data not being available.
def wait_node_up(node):
    msg = f'Waiting for node {node.instance_name} to start'
    logger.info(msg)
    attempt = 0
    while True:
        try:
            if not is_binary_running('neard', node):
                raise Exception(f'{msg} - failed. The neard process crashed.')

            response = node.get_validators()

            if 'error' in response:
                attempt += 1
                logger.info(f'{msg}, attempt {attempt} error response.')
                continue
            if 'result' not in response:
                attempt += 1
                logger.info(f'{msg}, attempt {attempt} result missing.')
                continue

            logger.info(f'{msg} - done.')
            return True
        except (ConnectionRefusedError, requests.exceptions.ConnectionError):
            attempt += 1
            logger.info(f'{msg}, attempt {attempt} connection refused.')
        time.sleep(30)

def wait_all_nodes_up(all_nodes):
    pmap(lambda node: wait_node_up(node), all_nodes)

def create_upgrade_schedule(
    rpc_nodes,
    validator_nodes,
    progressive_upgrade,
    increasing_stakes,
    num_block_producer_seats,
):
    schedule = {}
    if progressive_upgrade:
        # Re-create stakes assignment.
        stakes = []
        if increasing_stakes:
            prev_stake = None
            for i, node in enumerate(validator_nodes):
                if (i * 5 < num_block_producer_seats * 3 and
                        i < len(MAINNET_STAKES)):
                    staked = MAINNET_STAKES[i] * ONE_NEAR
                elif prev_stake is None:
                    prev_stake = MIN_STAKE - STAKE_STEP
                    staked = prev_stake * ONE_NEAR
                else:
                    prev_stake = prev_stake + STAKE_STEP
                    staked = prev_stake * ONE_NEAR
                stakes.append((staked, node.instance_name))

        else:
            for node in validator_nodes:
                stakes.append((MIN_STAKE, node.instance_name))
        logger.info(f'create_upgrade_schedule')
        for stake, instance_name in stakes:
            logger.debug(f'instance_name: {instance_name} - stake: {stake}')

        # Compute seat assignments.
        seats = compute_seats(stakes, num_block_producer_seats)

        seats_upgraded = 0
        for seat, stake, instance_name in seats:
            # As the protocol upgrade takes place after 80% of the nodes are
            # upgraded, stop a bit earlier to start in a non-upgraded state.
            if (seats_upgraded + seat) * 100 > 75 * num_block_producer_seats:
                break
            schedule[instance_name] = 0
            logger.info(
                f'validator node {node.instance_name} will start upgraded')
            seats_upgraded += seat

        # Upgrade the remaining validators during 4 epochs.
        for node in validator_nodes:
            if node.instance_name not in schedule:
                schedule[node.instance_name] = random.randint(1, 4)
                logger.info(
                    f'validator node {node.instance_name} will upgrade at {schedule[node.instance_name]}'
                )

        for node in rpc_nodes:
            schedule[node.instance_name] = random.randint(0, 4)
            if not schedule[node.instance_name]:
                logger.info(
                    f'rpc node {node.instance_name} will start upgraded')
            else:
                logger.info(
                    f'rpc node {node.instance_name} will upgrade at {schedule[node.instance_name]}'
                )
    else:
        # Start all nodes upgraded.
        for node in rpc_nodes:
            schedule[node.instance_name] = 0
        for node in validator_nodes:
            schedule[node.instance_name] = 0

    return schedule

def compute_seats(stakes, num_block_producer_seats):
    max_stake = 0
    for i in stakes:
        max_stake = max(max_stake, i[0])

    # Compute seats assignment.
    l = 0
    r = max_stake + 1
    seat_price = -1
    while r - l > 1:
        tmp_seat_price = (l + r) // 2
        num_seats = 0
        for i in range(len(stakes)):
            num_seats += stakes[i][0] // tmp_seat_price
        if num_seats <= num_block_producer_seats:
            r = tmp_seat_price
        else:
            l = tmp_seat_price
    seat_price = r
    logger.info(f'compute_seats seat_price: {seat_price}')

    seats = []
    for stake, item in stakes:
        seats.append((stake // seat_price, stake, item))
    seats.sort(reverse=True)
    return seats

def upgrade_nodes(epoch_height, upgrade_schedule, all_nodes):
    logger.info(f'Upgrading nodes for epoch height {epoch_height}')
    for node in all_nodes:
        if upgrade_schedule.get(node.instance_name, 0) == epoch_height:
            upgrade_node(node)

def get_epoch_height(rpc_nodes, prev_epoch_height):
    nodes = rpc_nodes.copy()
    random.shuffle(nodes)
    max_height = prev_epoch_height
    for node in nodes:
        (addr, port) = node.rpc_addr()
        j = {
            'method': 'validators',
            'params': [None],
            'id': 'dontcare',
            'jsonrpc': '2.0'
        }
        try:
            r = requests.post('http://%s:%s' % (addr, port), json=j, timeout=15)
            if r.ok:
                response = r.json()
                max_height = max(
                    max_height,
                    int(response.get('result', {}).get('epoch_height', 0)),
                )
        except Exception as e:
            continue
    return max_height

def neard_restart_script(node):
    neard_binary = '/home/ubuntu/neard.upgrade'
    return '''
        tmux send-keys -t near C-c
        sudo mv /home/ubuntu/near.log /home/ubuntu/near.log.1 2>/dev/null
        sudo mv /home/ubuntu/near.upgrade.log /home/ubuntu/near.upgrade.log.1 2>/dev/null
        tmux send-keys -t near 'RUST_BACKTRACE=full RUST_LOG=debug,actix_web=info {neard_binary} run 2>&1 | tee -a {neard_binary}.log' C-m
    '''.format(neard_binary=shlex.quote(neard_binary))

def upgrade_node(node):
    logger.info(f'Upgrading node {node.instance_name}')
    attempt = 0
    success = False
    while attempt < 3:
        start_process = node.machine.run(
            'sudo -u ubuntu -i',
            input=neard_restart_script(node),
        )
        if start_process.returncode == 0:
            success = True
            break
        logger.warn(
            f'Failed to upgrade neard, return code: {start_process.returncode}\n{node.instance_name}\n{start_process.stderr}'
        )
        attempt += 1
        time.sleep(1)
    if not success:
        raise Exception(f'Could not upgrade node {node.instance_name}')

STAKING_TIMEOUT = 60

# If the available amount of whole NEAR tokens is above 10**3, then stakes all available amount.
# Runs only if `last_staking` is at least `STAKING_TIMEOUT` seconds in the past.
def stake_available_amount(node_account, last_staking):
    # Repeat the staking transactions in case the validator selection algorithm changes.
    # Don't query the balance too often, avoid overloading the RPC node.
    if time.time() - last_staking > STAKING_TIMEOUT:
        # Make several attempts just in case the RPC node doesn't respond.
        for attempt in range(3):
            try:
                stake_amount = node_account.get_amount_yoctonear()
                logger.info(
                    f'Amount of {node_account.key.account_id} is {stake_amount}'
                )
                if stake_amount > (10**3) * ONE_NEAR:
                    logger.info(
                        f'Staking {stake_amount} for {node_account.key.account_id}'
                    )
                    node_account.send_stake_tx(stake_amount)
                logger.info(
                    f'Staked {stake_amount} for {node_account.key.account_id}')
                return time.time()
            except Exception as e:
                logger.info('Failed to stake')
    return None

'''
'''--- pytest/lib/mocknet_helpers.py ---
#!/usr/bin/env python3
import time
import base58
import requests
from configured_logger import logger
from key import Key

LOCAL_ADDR = '127.0.0.1'
RPC_PORT = '3030'

def get_status(addr=LOCAL_ADDR, port=RPC_PORT):
    r = requests.get(f'http://{addr}:{port}/status', timeout=10)
    r.raise_for_status()
    return r.json()

def json_rpc(method, params, addr=LOCAL_ADDR, port=RPC_PORT):
    j = {'method': method, 'params': params, 'id': 'dontcare', 'jsonrpc': '2.0'}
    r = requests.post(f'http://{addr}:{port}', json=j, timeout=10)
    return r.json()

def get_nonce_for_key(key: Key, **kwargs) -> int:
    return get_nonce_for_pk(key.account_id, key.pk, **kwargs)

def get_nonce_for_pk(account_id,
                     pk,
                     finality='optimistic',
                     addr=LOCAL_ADDR,
                     port=RPC_PORT,
                     logger=logger):
    access_keys = json_rpc(
        'query',
        {
            'request_type': 'view_access_key_list',
            'account_id': account_id,
            'finality': finality
        },
        addr=addr,
        port=port,
    )
    logger.info(f'get_nonce_for_pk {account_id}')
    logger.info(access_keys)
    if not access_keys['result']['keys']:
        raise KeyError(account_id)

    nonce = next((key['access_key']['nonce']
                  for key in access_keys['result']['keys']
                  if key['public_key'] == pk), None)
    if nonce is None:
        raise KeyError(f'Nonce for {account_id} {pk} not found')
    return nonce

def get_latest_block_hash(addr=LOCAL_ADDR, port=RPC_PORT):
    last_block_hash = get_status(
        addr=addr,
        port=port,
    )['sync_info']['latest_block_hash']
    return base58.b58decode(last_block_hash.encode('utf-8'))

def throttle_txns(send_txns, total_tx_sent, elapsed_time, test_state):
    start_time = time.monotonic()
    send_txns(test_state)
    duration = time.monotonic() - start_time
    total_tx_sent += test_state.num_test_accounts()

    excess_transactions = total_tx_sent - (test_state.max_tps_per_node *
                                           (elapsed_time + duration))
    if excess_transactions > 0:
        delay = excess_transactions / test_state.max_tps_per_node
        logger.info(f'Sleeping for {delay} seconds to throttle transactions')
        time.sleep(delay)

    return total_tx_sent

def retry_and_ignore_errors(f):
    for attempt in range(3):
        try:
            return f()
        except Exception as e:
            time.sleep(0.1 * 2**attempt)
    return None

def wait_at_least_one_block():
    status = get_status()
    start_height = status['sync_info']['latest_block_height']
    timeout_sec = 5
    started = time.monotonic()
    while time.monotonic() - started < timeout_sec:
        status = get_status()
        height = status['sync_info']['latest_block_height']
        if height > start_height:
            break
        time.sleep(1.0)

def get_amount_yoctonear(account_id, addr=LOCAL_ADDR, port=RPC_PORT):
    j = json_rpc('query', {
        'request_type': 'view_account',
        'finality': 'optimistic',
        'account_id': account_id
    },
                 addr=addr,
                 port=port)
    return int(j.get('result', {}).get('amount', 0))

# Returns the transaction result for the given txn_id.
# Might return None - if transaction is not present and wait_for_success is false.
def tx_result(txn_id, account_id, wait_for_success=False, **kwargs):
    while True:
        status = json_rpc("EXPERIMENTAL_tx_status", [txn_id, account_id],
                          **kwargs)
        if 'error' in status:
            print("tx error: tx not ready yet")
            if not wait_for_success:
                return None
            time.sleep(3)
        else:
            return status['result']

'''
'''--- pytest/lib/network.py ---
import subprocess, sys
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))
from configured_logger import logger

def _run_process(cmd):
    process = subprocess.Popen(cmd,
                               stdout=subprocess.PIPE,
                               stderr=subprocess.PIPE)
    out, err = process.communicate()
    return (process.returncode, out, err)

def init_network_pillager():
    _run_process(["mkdir", "-p", "/sys/fs/cgroup/net_cls/block"])
    try:
        with open("/sys/fs/cgroup/net_cls/block/net_cls.classid", 'w') as f:
            f.write("42")
    except IOError as e:
        if e[0] == 13:
            logger.critical(
                "Failed to modify `/sys/fs/cgroup/net_cls/block/net_cls.classid`."
            )
            logger.critical(
                "Make sure the current user has access to it, e.g. by changing the owner:\n"
            )
            logger.critical(
                "    chown <group>.<user> /sys/fs/cgroup/net_cls/block/net_cls.classid"
            )
            sys.exit(1)
    _run_process([
        "iptables", "-A", "OUTPUT", "-m", "cgroup", "--cgroup", "42", "-j",
        "DROP"
    ])

def stop_network(pid):
    with open('/sys/fs/cgroup/net_cls/block/tasks', 'w') as f:
        f.write(str(pid))

def resume_network(pid):
    try:
        with open('/sys/fs/cgroup/net_cls/tasks', 'w') as f:
            f.write(str(pid))
    except ProcessLookupError:
        # the process was killed in the meantime
        pass

if __name__ == "__main__":
    import time
    init_network_pillager()
    handle = subprocess.Popen(["ping", "8.8.8.8"],
                              stdout=subprocess.PIPE,
                              stderr=subprocess.PIPE,
                              universal_newlines=True)
    logger.info(handle.pid)
    time.sleep(3)
    stop_network(handle.pid)
    time.sleep(3)
    resume_network(handle.pid)
    time.sleep(3)
    handle.kill()
    out, err = handle.communicate()
    logger.info("STDOUT (expect ~6 entries if all goes well):")
    logger.info(out)
    logger.info("STDERR (expect ~3 entries if all goes well):")
    logger.info(err)

'''
'''--- pytest/lib/peer.py ---
import asyncio
import concurrent
import hashlib
import struct

import base58

from configured_logger import logger
from messages import schema
from messages.crypto import PublicKey, Signature
from messages.network import (EdgeInfo, GenesisId, Handshake, PeerChainInfoV2,
                              PeerMessage, RoutedMessage, PeerIdOrHash)
from serializer import BinarySerializer
from nacl.signing import SigningKey
from typing import Optional

ED_PREFIX = "ed25519:"

class Connection:

    def __init__(self, reader: asyncio.StreamReader,
                 writer: asyncio.StreamWriter):
        self.reader = reader
        self.writer = writer
        self.is_closed = False

    async def send(self, message):
        raw_message = BinarySerializer(schema).serialize(message)
        await self.send_raw(raw_message)

    async def send_raw(self, raw_message):
        length = struct.pack('I', len(raw_message))
        self.writer.write(length)
        self.writer.write(raw_message)
        await self.writer.drain()

    # returns None on timeout
    async def recv(self, expected=None):
        while True:
            response_raw = await self.recv_raw()

            # Connection was closed on the other side
            if response_raw is None:
                return None
            # TODO(CP-85): when removing borsh support, fix this to use protobufs,
            # (or preferably reimplement the test in rust).
            try:
                response = BinarySerializer(schema).deserialize(
                    response_raw, PeerMessage)
            except IndexError:
                # unparsable message, ignore.
                continue

            if expected is None or response.enum == expected or (
                    callable(expected) and expected(response)):
                return response

    async def recv_raw(self):
        length = await self.reader.read(4)

        if len(length) == 0:
            self.is_closed = True
            return None
        else:
            length = struct.unpack('I', length)[0]
            response = b''

            while len(response) < length:
                response += await self.reader.read(length - len(response))
                if len(response) < length:
                    logger.info(f"Downloading message {len(response)}/{length}")

            return response

    async def close(self):
        self.writer.close()
        await self.writer.wait_closed()

    def do_send(self, message):
        loop = asyncio.get_event_loop()
        loop.create_task(self.send(message))

    def do_send_raw(self, raw_message):
        loop = asyncio.get_event_loop()
        loop.create_task(self.send_raw(raw_message))

async def connect(addr) -> Connection:
    reader, writer = await asyncio.open_connection(*addr)
    conn = Connection(reader, writer)
    return conn

def create_handshake(my_key_pair_nacl,
                     their_pk_serialized,
                     listen_port,
                     version=0):
    """
    Create handshake message but with placeholders in:
        - version
        - genesis_id.chain_id
        - genesis_id.hash
        - edge_info.signature
    """
    handshake = Handshake()
    handshake.version = version
    handshake.oldest_supported_version = version
    handshake.peer_id = PublicKey()
    handshake.target_peer_id = PublicKey()
    handshake.listen_port = listen_port
    handshake.chain_info = PeerChainInfoV2()
    handshake.edge_info = EdgeInfo()

    handshake.peer_id.keyType = 0
    handshake.peer_id.data = bytes(my_key_pair_nacl.verify_key)

    handshake.target_peer_id.keyType = 0
    handshake.target_peer_id.data = base58.b58decode(
        their_pk_serialized[len(ED_PREFIX):])

    handshake.chain_info.genesis_id = GenesisId()
    handshake.chain_info.height = 0
    handshake.chain_info.tracked_shards = []
    handshake.chain_info.archival = False

    handshake.chain_info.genesis_id.chain_id = 'moo'
    handshake.chain_info.genesis_id.hash = bytes([0] * 32)

    handshake.edge_info.nonce = 1
    handshake.edge_info.signature = Signature()

    handshake.edge_info.signature.keyType = 0
    handshake.edge_info.signature.data = bytes([0] * 64)

    peer_message = PeerMessage()
    peer_message.enum = 'Handshake'
    peer_message.Handshake = handshake

    return peer_message

def create_peer_request():
    peer_message = PeerMessage()
    peer_message.enum = 'PeersRequest'
    peer_message.PeersRequest = ()
    return peer_message

def sign_handshake(my_key_pair_nacl, handshake):
    peer0 = handshake.peer_id
    peer1 = handshake.target_peer_id
    if peer1.data < peer0.data:
        peer0, peer1 = peer1, peer0

    arr = bytes(
        bytearray([0]) + peer0.data + bytearray([0]) + peer1.data +
        struct.pack('Q', handshake.edge_info.nonce))
    handshake.edge_info.signature.data = my_key_pair_nacl.sign(
        hashlib.sha256(arr).digest()).signature

async def run_handshake(conn: Connection,
                        target_public_key: PublicKey,
                        key_pair: SigningKey,
                        listen_port=12345):
    handshake = create_handshake(key_pair, target_public_key, listen_port)

    async def send_handshake():
        sign_handshake(key_pair, handshake.Handshake)
        await conn.send(handshake)
        # The peer might sent us an unsolicited message before replying to
        # a successful handshake.  This is because node is multi-threaded and
        # peers are added to PeerManager before the reply is sent.  Since we
        # don’t care about those messages, ignore them and wait for some kind of
        # Handshake reply.
        return await conn.recv(lambda msg: msg.enum.startswith('Handshake'))

    response = await send_handshake()

    if response.enum == 'HandshakeFailure' and response.HandshakeFailure[
            1].enum == 'ProtocolVersionMismatch':
        pvm = response.HandshakeFailure[1].ProtocolVersionMismatch.version
        handshake.Handshake.version = pvm
        response = await send_handshake()

    if response.enum == 'HandshakeFailure' and response.HandshakeFailure[
            1].enum == 'GenesisMismatch':
        gm = response.HandshakeFailure[1].GenesisMismatch
        handshake.Handshake.chain_info.genesis_id.chain_id = gm.chain_id
        handshake.Handshake.chain_info.genesis_id.hash = gm.hash
        response = await send_handshake()

    assert response.enum == 'Handshake', response.enum if response.enum != 'HandshakeFailure' else response.HandshakeFailure[
        1].enum

def create_and_sign_routed_peer_message(routed_msg_body, target_node,
                                        my_key_pair_nacl):
    routed_msg = RoutedMessage()
    routed_msg.target = PeerIdOrHash()
    routed_msg.target.enum = 'PeerId'
    routed_msg.target.PeerId = PublicKey()
    routed_msg.target.PeerId.keyType = 0
    routed_msg.target.PeerId.data = base58.b58decode(
        target_node.node_key.pk[len(ED_PREFIX):])
    routed_msg.author = PublicKey()
    routed_msg.author.keyType = 0
    routed_msg.author.data = bytes(my_key_pair_nacl.verify_key)
    routed_msg.ttl = 100
    routed_msg.body = routed_msg_body
    routed_msg.signature = Signature()
    routed_msg.signature.keyType = 0

    routed_msg_arr = bytes(
        bytearray([0, 0]) + routed_msg.target.PeerId.data + bytearray([0]) +
        routed_msg.author.data +
        BinarySerializer(schema).serialize(routed_msg.body))
    routed_msg_hash = hashlib.sha256(routed_msg_arr).digest()
    routed_msg.signature.data = my_key_pair_nacl.sign(routed_msg_hash).signature

    peer_message = PeerMessage()
    peer_message.enum = 'Routed'
    peer_message.Routed = routed_msg

    return peer_message

'''
'''--- pytest/lib/populate.py ---
import subprocess
from os.path import join
from shutil import copy2, rmtree

def genesis_populate(near_root, additional_accounts, node_dir):
    subprocess.check_call(
        (join(near_root, 'genesis-populate'), '--additional-accounts-num',
         str(additional_accounts), '--home', node_dir))
    rmtree(join(node_dir, 'data'), ignore_errors=True)

def copy_genesis(node_dir_source, node_dir_target):
    for file_name in ['genesis.json', 'genesis_roots', 'state_dump']:
        source_file = join(node_dir_source, file_name)
        target_file = join(node_dir_target, file_name)
        copy2(source_file, target_file)

def genesis_populate_all(near_root, additional_accounts, node_dirs):
    genesis_populate(near_root, additional_accounts, node_dirs[0])
    for node_dir in node_dirs[1:]:
        copy_genesis(node_dirs[0], node_dir)
        rmtree(join(node_dir, 'data'), ignore_errors=True)

'''
'''--- pytest/lib/proxy.py ---
# A library providing a node wrapper that intercepts all the incoming messages and
# outgoing responses (but not the initiated outgoing messages and incoming responses)
# from a node, and calls a handler on them. The handler can then decide to drop the
# message, deliver the message, or change the message.
#
# Usage:
# 1. Create a proxy = NodesProxy(handler).
#    handler takes two arguments, the message (deserialized), the ordinal of the sending
#    peer and the ordinal of the receiving peer. The ordinal is derived from the port,
#    and assumes no tampering with ports was done
# 2. Call proxy.proxify_node(node) before the node is started.
#    proxify_node will start a process that receives the connections on the port 100
#    larger than the original node port. It will change the port of the `Node` object
#    passed to it.
#
# Note that since the handler is called on incoming messages and outgoing responses, if
# all the nodes in the cluster are proxified, each message exchanged in the cluster will
# have the handler called exactly once.
#
# The proxy will register an atexit function that will gracefully shut down all the
# processes, and fail the test if any of the processes failed.
#
# `start_cluster` accepts a handler as its last argument, and automatically proxifies
# all the nodes if the parameter is not None
#
# See `tests/sanity/nodes_proxy.py` for an example usage

import asyncio
import atexit
import functools
import multiprocessing
import random
import select
import socket
import struct
import time
import logging

from configured_logger import logger
from messages import schema
from messages.crypto import PublicKey, Signature
from messages.network import PeerIdOrHash, PeerMessage
from serializer import BinarySerializer

MSG_TIMEOUT = 10
_MY_PORT = [None]

logging.basicConfig(format='%(asctime)s %(message)s', level=logging.INFO)

def proxy_cleanup(proxy):
    logging.debug(f'Cleaning process for {_MY_PORT}')
    proxy.global_stopped.value = 1
    for p in proxy.ps:
        p.terminate()
    logging.debug(f'Finish cleanup for {_MY_PORT}')
    if proxy.error.value != 0:
        assert False, "One of the proxy processes failed, search for the stacktraces above"

def port_holder_to_node_ord(holder):
    return None if holder[0] is None else (holder[0] - 24477) % 100

class ProxyHandler:

    def __init__(self, ordinal):
        self.ordinal = ordinal
        self.recv_from_map = {}
        self.send_to_map = {}
        self.loop = asyncio.get_event_loop()

    @property
    def me(self):
        return self.ordinal

    def other(self, ordinal_a, ordinal_b):
        assert self.me == ordinal_a or self.me == ordinal_b
        if ordinal_a == self.me:
            return ordinal_b
        else:
            return ordinal_a

    async def _handle(self, raw_message, *, writer, sender_port_holder,
                      receiver_port_holder, ordinal_to_writer):
        sender_ordinal = port_holder_to_node_ord(sender_port_holder)
        receiver_ordinal = port_holder_to_node_ord(receiver_port_holder)
        try:
            # TODO(CP-85): when removing borsh support, fix this to use protobufs,
            # (or preferably reimplement the test in rust).
            try:
                message = BinarySerializer(schema).deserialize(
                    raw_message, PeerMessage)
            except IndexError:
                # unparsable message, ignore.
                logging.warn(
                    f"Warning: could not proxy message {raw_message.hex()}")
                return
            assert BinarySerializer(schema).serialize(message) == raw_message

            if message.enum == 'Handshake':
                message.Handshake.listen_port += 100
                if sender_port_holder[0] is None:
                    sender_port_holder[0] = message.Handshake.listen_port

            other_ordinal = self.other(sender_ordinal, receiver_ordinal)

            if other_ordinal is not None and not other_ordinal in ordinal_to_writer:
                ordinal_to_writer[other_ordinal] = writer

            decision = await self.handle(message, sender_ordinal,
                                         receiver_ordinal)

            if decision is True and message.enum == 'Handshake':
                decision = message

            if not isinstance(decision, bool):
                decision = BinarySerializer(schema).serialize(decision)

            return decision
        except:
            # TODO: Remove this
            if raw_message[0] == 13:
                # raw_message[0] == 13 is RoutedMessage. Skip leading fields to get to the RoutedMessageBody
                ser = BinarySerializer(schema)
                ser.array = bytearray(raw_message)
                ser.offset = 1
                ser.deserialize_field(PeerIdOrHash)
                ser.deserialize_field(PublicKey)
                ser.deserialize_field(Signature)
                ser.deserialize_field('u8')

                # The next byte is the variant ordinal of the `RoutedMessageBody`.
                # Skip if it's the ordinal of a variant for which the schema is not ported yet
                if raw_message[ser.offset] in [3, 4, 5, 7]:
                    # Allow the handler determine if the message should be passed even when it couldn't be deserialized
                    return await self.handle(None, sender_ordinal,
                                             receiver_ordinal) is not False
                logger.info(f"ERROR 13 {int(raw_message[ser.offset])}")

            else:
                logger.info(f"ERROR {int(raw_message[0])}")

            raise

        return True

    def get_writer(self, to, fr=None):
        if to == self.ordinal:
            if fr is None and len(self.recv_from_map) > 0:
                fr = next(iter(self.recv_from_map.keys()))
            return self.recv_from_map.get(fr)
        else:
            return self.send_to_map.get(to)

    async def send_binary(self, raw_message, to, fr=None):
        writer = self.get_writer(to, fr)
        if writer is None:
            logger.info(
                f"Writer not known: to={to}, fr={fr}, send={self.send_to_map.keys()}, recv={self.recv_from_map.keys()}"
            )
        else:
            writer.write(struct.pack('I', len(raw_message)))
            writer.write(raw_message)
            await writer.drain()

    async def send_message(self, message, to, fr=None):
        raw_message = BinarySerializer(schema).serialize(message)
        await self.send_binary(raw_message, to, fr)

    def do_send_binary(self, raw_message, to, fr=None):
        self.loop.create_task(self.send_binary(raw_message, to, fr))

    def do_send_message(self, msg, to, fr=None):
        self.loop.create_task(self.send_message(msg, to, fr))

    async def handle(self, msg, fr, to):
        return True

class NodesProxy:

    def __init__(self, handler):
        assert handler is not None
        self.handler = handler
        self.global_stopped = multiprocessing.Value('i', 0)
        self.error = multiprocessing.Value('i', 0)
        self.ps = []
        atexit.register(proxy_cleanup, self)

    def proxify_node(self, node):
        proxify_node(node, self.ps, self.handler, self.global_stopped,
                     self.error, self)

async def stop_server(server):
    server.close()
    await server.wait_closed()

def check_finish(server, global_stopped, local_stopped, error):
    loop = asyncio.get_running_loop()
    if 0 == global_stopped.value and 0 >= local_stopped.value and 0 == error.value:
        loop.call_later(1, check_finish, server, global_stopped, local_stopped,
                        error)
    else:
        logging.debug(
            f"Stopping server. port={_MY_PORT}, global_stopped={global_stopped.value}, local_stopped={local_stopped.value}, error={error.value}"
        )
        server.close()
        local_stopped.value = 2

async def _read_exact(reader, length, *, allow_eof=False):
    """Reads exactly `length` bytes from reader."""
    data = await reader.read(length)
    if data or not allow_eof:
        while len(data) < length:
            data += await reader.read(length - len(data))
    return data

async def bridge(reader, writer, handler_fn, global_stopped, local_stopped,
                 bridge_stopped, error):
    bridge_id = random.randint(0, 10**10)
    logging.debug(f"Start bridge. port={_MY_PORT} bridge_id={bridge_id}")

    try:
        while 0 == global_stopped.value and 0 >= local_stopped.value and 0 == error.value and 0 == bridge_stopped[
                0]:
            header = await _read_exact(reader, 4, allow_eof=True)
            if not header:
                logging.debug(
                    f"Endpoint closed (Reader). port={_MY_PORT} bridge_id={bridge_id}"
                )
                break

            raw_message_len = struct.unpack('I', header)[0]
            raw_message = await _read_exact(reader, raw_message_len)

            logging.debug(
                f"Message size={len(raw_message)} port={_MY_PORT} bridge_id={bridge_id}"
            )
            decision = await handler_fn(raw_message)

            if isinstance(decision, bytes):
                raw_message = decision
                decision = True

            if decision:
                writer.write(struct.pack('I', len(raw_message)))
                writer.write(raw_message)
                await writer.drain()

        bridge_stopped[0] = 1
        writer.close()
        await writer.wait_closed()

        logging.debug(
            f"Gracefully close bridge. port={_MY_PORT} bridge_id={bridge_id}")
    except (ConnectionResetError, BrokenPipeError):
        bridge_stopped[0] = 1
        try:
            writer.close()
            await writer.wait_closed()
        except:
            pass
        logging.debug(
            f"Endpoint closed (Writer). port={_MY_PORT} bridge_id={bridge_id}")

async def handle_connection(outer_reader, outer_writer, inner_port, outer_port,
                            handler, global_stopped, local_stopped, error):
    connection_id = random.randint(0, 10**10)
    logging.debug(
        f"New connection. port={_MY_PORT} connection_id={connection_id}")
    try:
        inner_reader, inner_writer = await asyncio.open_connection(
            '127.0.0.1', inner_port)

        my_port = [outer_port]
        peer_port_holder = [None]
        bridge_stopped = [0]

        inner_to_outer = bridge(
            inner_reader, outer_writer,
            functools.partial(
                handler._handle,
                writer=inner_writer,
                sender_port_holder=my_port,
                receiver_port_holder=peer_port_holder,
                ordinal_to_writer=handler.recv_from_map,
            ), global_stopped, local_stopped, bridge_stopped, error)

        outer_to_inner = bridge(
            outer_reader, inner_writer,
            functools.partial(
                handler._handle,
                writer=outer_writer,
                sender_port_holder=peer_port_holder,
                receiver_port_holder=my_port,
                ordinal_to_writer=handler.send_to_map,
            ), global_stopped, local_stopped, bridge_stopped, error)

        await asyncio.gather(inner_to_outer, outer_to_inner)
        logging.debug(
            f"End of connection. port={_MY_PORT} peer_port={peer_port_holder} connection_id={connection_id}"
        )
    except asyncio.CancelledError:
        logging.debug(
            f"Cancelled Error (handle_connection). port={_MY_PORT} connection_id={connection_id} global_stopped={global_stopped.value} local_stopped={local_stopped.value} error={error.value}"
        )
        if local_stopped.value <= 0:
            global_stopped.value = 1
    except ConnectionRefusedError:
        logging.debug(
            f"ConnectionRefusedError (handle_connection). port={_MY_PORT} connection_id={connection_id} global_stopped={global_stopped.value} local_stopped={local_stopped.value} error={error.value}"
        )
    except:
        logging.debug(
            f"Other Error (handle_connection). port={_MY_PORT} connection_id={connection_id} global_stopped={global_stopped.value} local_stopped={local_stopped.value} error={error.value}"
        )
        error.value = 1
        raise

async def listener(inner_port, outer_port, handler_ctr, global_stopped,
                   local_stopped, error):
    logging.debug(f"Starting listener... port={_MY_PORT}")
    try:
        handler = handler_ctr(port_holder_to_node_ord([outer_port]))

        async def start_connection(reader, writer):
            await handle_connection(reader, writer, inner_port, outer_port,
                                    handler, global_stopped, local_stopped,
                                    error)

        attempts = 3

        # Possibly need to wait 1 second to start listener if node was killed and previous listener is on yet.
        while attempts > 0:
            try:
                server = await asyncio.start_server(start_connection,
                                                    '127.0.0.1', outer_port)
                logging.debug(f"Listener started. port={_MY_PORT}")
                break
            except OSError:
                attempts -= 1
                logging.debug(
                    f"Fail starting listener. Remaining attempts: {attempts} port={_MY_PORT}"
                )
                if attempts == 0:
                    raise
                await asyncio.sleep(1)

        check_finish(server, global_stopped, local_stopped, error)
        local_stopped.value = 0

        async with server:
            await server.serve_forever()
    except asyncio.CancelledError:
        logging.debug(
            f"Cancelled Error (listener). port={_MY_PORT} global_stopped={global_stopped.value} local_stopped={local_stopped.value} error={error.value}"
        )
        if local_stopped.value <= 0:
            global_stopped.value = 1
    except:
        logging.debug(
            f"Other Error (listener). port={_MY_PORT} global_stopped={global_stopped.value} local_stopped={local_stopped.value} error={error.value}"
        )
        error.value = 1
        raise

def start_server(inner_port, outer_port, handler_ctr, global_stopped,
                 local_stopped, error):
    _MY_PORT[0] = outer_port
    asyncio.run(
        listener(inner_port, outer_port, handler_ctr, global_stopped,
                 local_stopped, error))

def proxify_node(node, ps, handler, global_stopped, error, proxy):
    inner_port = node.port
    outer_port = inner_port + 100

    def start_proxy():
        # local_stopped denotes the current of state of the proxy:
        # -1: The proxy hasn't started yet
        # 0: The proxy is running
        # 1: The proxy is running but should be closed soon
        # 2: The proxy is closed
        local_stopped = multiprocessing.Value('i', -1)
        p = multiprocessing.Process(target=start_server,
                                    args=(inner_port, outer_port, handler,
                                          global_stopped, local_stopped, error))
        p.start()
        ps.append(p)
        for attempt in range(3):
            if local_stopped.value == 0:
                break
            time.sleep(1)
        else:
            error.value = 1
            assert False, "The proxy failed to start after 3 seconds"
        return local_stopped

    node.port = outer_port
    node._start_proxy = start_proxy
    node.proxy = proxy

'''
'''--- pytest/lib/proxy_instances.py ---
import logging, multiprocessing, random
from proxy import ProxyHandler, NodesProxy

class RejectListHandler(ProxyHandler):

    def __init__(self, reject_list, drop_probability, ordinal):
        super().__init__(ordinal)
        self.reject_list = reject_list
        self.drop_probability = drop_probability

    async def handle(self, msg, fr, to):
        msg_type = msg.enum if msg.enum != 'Routed' else msg.Routed.body.enum

        if (self.drop_probability > 0 and 'Handshake' not in msg_type and
                random.uniform(0, 1) < self.drop_probability):
            logging.info(
                f'NODE {self.ordinal} dropping message {msg_type} from {fr} to {to}'
            )
            return False

        if fr in self.reject_list or to in self.reject_list:
            logging.info(
                f'NODE {self.ordinal} blocking message {msg_type} from {fr} to {to}'
            )
            return False
        else:
            return True

class RejectListProxy(NodesProxy):

    def __init__(self, reject_list, drop_probability):
        self.reject_list = reject_list
        self.drop_probability = drop_probability
        handler = lambda ordinal: RejectListHandler(reject_list,
                                                    drop_probability, ordinal)
        super().__init__(handler)

    @staticmethod
    def create_reject_list(size):
        return multiprocessing.Array('i', [-1 for _ in range(size)])

'''
'''--- pytest/lib/resharding_lib.py ---
# A library with the common constants and functions for testing resharding.

# TODO(resharding) The resharding V2 is now stabilized so the V1 code is no
# longer exercised and can be removed.
import unittest

from cluster import get_binary_protocol_version, load_config
from utils import MetricsTracker

V1_PROTOCOL_VERSION = 48
V2_PROTOCOL_VERSION = 64
V3_PROTOCOL_VERSION = 65

V0_SHARD_LAYOUT = {
    "V0": {
        "num_shards": 1,
        "version": 0
    },
}
V1_SHARD_LAYOUT = {
    "V1": {
        "boundary_accounts": [
            "aurora", "aurora-0", "kkuuue2akv_1630967379.near"
        ],
        "shards_split_map": [[0, 1, 2, 3]],
        "to_parent_shard_map": [0, 0, 0, 0],
        "version": 1
    }
}
V2_SHARD_LAYOUT = {
    "V1": {
        "boundary_accounts": [
            "aurora",
            "aurora-0",
            "kkuuue2akv_1630967379.near",
            "tge-lockup.sweat",
        ],
        "shards_split_map": [[0], [1], [2], [3, 4]],
        "to_parent_shard_map": [0, 1, 2, 3, 3],
        "version": 2
    }
}

def get_genesis_config_changes(epoch_length,
                               binary_protocol_version,
                               logger=None):
    genesis_config_changes = [
        ["epoch_length", epoch_length],
    ]
    append_shard_layout_config_changes(
        genesis_config_changes,
        binary_protocol_version,
        logger,
    )
    return genesis_config_changes

# Append the genesis config changes that are required for testing resharding.
# This method will set the protocol version, shard layout and a few other
# configs so that it matches the protocol configuration as of right before the
# protocol version of the binary under test.
def append_shard_layout_config_changes(
    genesis_config_changes,
    binary_protocol_version,
    logger=None,
):
    genesis_config_changes.append(["use_production_config", True])

    if binary_protocol_version >= V3_PROTOCOL_VERSION:
        if logger:
            logger.info("Testing migration from V2 to V3.")
        # Set the initial protocol version to a version just before V3.
        genesis_config_changes.append([
            "protocol_version",
            V3_PROTOCOL_VERSION - 1,
        ])
        genesis_config_changes.append([
            "shard_layout",
            V2_SHARD_LAYOUT,
        ])
        genesis_config_changes.append([
            "num_block_producer_seats_per_shard",
            [1, 1, 1, 1],
        ])
        genesis_config_changes.append([
            "avg_hidden_validator_seats_per_shard",
            [0, 0, 0, 0],
        ])
        return

    if binary_protocol_version >= V2_PROTOCOL_VERSION:
        if logger:
            logger.info("Testing migration from V1 to V2.")
        # Set the initial protocol version to a version just before V2.
        genesis_config_changes.append([
            "protocol_version",
            V2_PROTOCOL_VERSION - 1,
        ])
        genesis_config_changes.append([
            "shard_layout",
            V1_SHARD_LAYOUT,
        ])
        genesis_config_changes.append([
            "num_block_producer_seats_per_shard",
            [1, 1, 1, 1],
        ])
        genesis_config_changes.append([
            "avg_hidden_validator_seats_per_shard",
            [0, 0, 0, 0],
        ])
        return

    if binary_protocol_version >= V1_PROTOCOL_VERSION:
        if logger:
            logger.info("Testing migration from V0 to V1.")
        # Set the initial protocol version to a version just before V1.
        genesis_config_changes.append([
            "protocol_version",
            V1_PROTOCOL_VERSION - 1,
        ])
        genesis_config_changes.append([
            "shard_layout",
            V0_SHARD_LAYOUT,
        ])
        genesis_config_changes.append([
            "num_block_producer_seats_per_shard",
            [100],
        ])
        genesis_config_changes.append([
            "avg_hidden_validator_seats_per_shard",
            [0],
        ])
        return

    assert False

def get_genesis_shard_layout_version(binary_protocol_version):
    if binary_protocol_version >= V3_PROTOCOL_VERSION:
        return 2
    if binary_protocol_version >= V2_PROTOCOL_VERSION:
        return 1
    if binary_protocol_version >= V1_PROTOCOL_VERSION:
        return 0

    assert False

def get_target_shard_layout_version(binary_protocol_version):
    if binary_protocol_version >= V3_PROTOCOL_VERSION:
        return 3
    if binary_protocol_version >= V2_PROTOCOL_VERSION:
        return 2
    if binary_protocol_version >= V1_PROTOCOL_VERSION:
        return 1

    assert False

def get_genesis_num_shards(binary_protocol_version):
    if binary_protocol_version >= V3_PROTOCOL_VERSION:
        return 5
    if binary_protocol_version >= V2_PROTOCOL_VERSION:
        return 4
    if binary_protocol_version >= V1_PROTOCOL_VERSION:
        return 1

    assert False

def get_target_num_shards(binary_protocol_version):
    if binary_protocol_version >= V2_PROTOCOL_VERSION:
        return 6
    if binary_protocol_version >= V2_PROTOCOL_VERSION:
        return 5
    if binary_protocol_version >= V1_PROTOCOL_VERSION:
        return 4

    assert False

def get_epoch_offset(binary_protocol_version):
    if binary_protocol_version >= V3_PROTOCOL_VERSION:
        return 1
    if binary_protocol_version >= V2_PROTOCOL_VERSION:
        return 1
    if binary_protocol_version >= V1_PROTOCOL_VERSION:
        return 0

    assert False

def get_client_config_changes(num_nodes, initial_delay=None):
    single = {
        "tracked_shards": [0],
        "resharding_config": {
            "batch_size": 1000000,
            # don't throttle resharding
            "batch_delay": {
                "secs": 0,
                "nanos": 0,
            },
            # retry often to start resharding as fast as possible
            "retry_delay": {
                "secs": 0,
                "nanos": 100_000_000
            }
        }
    }
    if initial_delay is not None:
        single["resharding_config"]["initial_delay"] = {
            "secs": initial_delay,
            "nanos": 0
        }
    return {i: single for i in range(num_nodes)}

class ReshardingTestBase(unittest.TestCase):

    def setUp(self, epoch_length):
        self.epoch_length = epoch_length
        self.config = load_config()
        self.binary_protocol_version = get_binary_protocol_version(self.config)
        assert self.binary_protocol_version is not None

        self.genesis_shard_layout_version = get_genesis_shard_layout_version(
            self.binary_protocol_version)
        self.target_shard_layout_version = get_target_shard_layout_version(
            self.binary_protocol_version)

        self.genesis_num_shards = get_genesis_num_shards(
            self.binary_protocol_version)
        self.target_num_shards = get_target_num_shards(
            self.binary_protocol_version)

        self.epoch_offset = get_epoch_offset(self.binary_protocol_version)

    def get_metric(self, metrics_tracker: MetricsTracker, metric_name):
        return metrics_tracker.get_int_metric_value(metric_name)

    def get_version(self, metrics_tracker: MetricsTracker):
        return self.get_metric(metrics_tracker, "near_shard_layout_version")

    def get_num_shards(self, metrics_tracker: MetricsTracker):
        return self.get_metric(metrics_tracker, "near_shard_layout_num_shards")

    def get_resharding_status(self, metrics_tracker: MetricsTracker):
        return metrics_tracker.get_metric_all_values("near_resharding_status")

    def get_flat_storage_head(self, metrics_tracker: MetricsTracker):
        return metrics_tracker.get_metric_all_values(
            "near_flat_storage_head_height")

'''
'''--- pytest/lib/serializer.py ---
class BinarySerializer:

    def __init__(self, schema):
        self.array = bytearray()
        self.schema = schema

    def read_bytes(self, n):
        assert n + self.offset <= len(
            self.array
        ), f'n: {n} offset: {self.offset}, length: {len(self.array)}'
        ret = self.array[self.offset:self.offset + n]
        self.offset += n
        return ret

    def serialize_num(self, value, n_bytes):
        assert value >= 0
        for i in range(n_bytes):
            self.array.append(value & 255)
            value //= 256
        assert value == 0

    def deserialize_num(self, n_bytes):
        value = 0
        bytes_ = self.read_bytes(n_bytes)
        for b in bytes_[::-1]:
            value = value * 256 + b
        return value

    def serialize_field(self, value, fieldType):
        if type(fieldType) == tuple:
            if len(fieldType) == 0:
                pass
            else:
                assert len(value) == len(fieldType)
                for (v, t) in zip(value, fieldType):
                    self.serialize_field(v, t)
        elif type(fieldType) == str:
            if fieldType == 'bool':
                assert isinstance(value, bool), str(type(value))
                self.serialize_num(int(value), 1)
            elif fieldType[0] == 'u':
                self.serialize_num(value, int(fieldType[1:]) // 8)
            elif fieldType == 'string':
                b = value.encode('utf8')
                self.serialize_num(len(b), 4)
                self.array += b
            else:
                assert False, fieldType
        elif type(fieldType) == list:
            assert len(fieldType) == 1
            if type(fieldType[0]) == int:
                assert type(value) == bytes
                assert len(value) == fieldType[0], "len(%s) = %s != %s" % (
                    value, len(value), fieldType[0])
                self.array += bytearray(value)
            else:
                self.serialize_num(len(value), 4)
                for el in value:
                    self.serialize_field(el, fieldType[0])
        elif type(fieldType) == dict:
            assert fieldType['kind'] == 'option'
            if value is None:
                self.serialize_num(0, 1)
            else:
                self.serialize_num(1, 1)
                self.serialize_field(value, fieldType['type'])
        elif type(fieldType) == type:
            assert type(value) == fieldType, "%s != type(%s)" % (fieldType,
                                                                 value)
            self.serialize_struct(value)
        else:
            assert False, type(fieldType)

    def deserialize_field(self, fieldType):
        if type(fieldType) == tuple:
            if len(fieldType) == 0:
                return None
            else:
                return tuple(self.deserialize_field(t) for t in fieldType)

        elif type(fieldType) == str:
            if fieldType == 'bool':
                value = self.deserialize_num(1)
                assert 0 <= value <= 1, f"Fail to deserialize bool: {value}"
                return bool(value)
            elif fieldType[0] == 'u':
                return self.deserialize_num(int(fieldType[1:]) // 8)
            elif fieldType == 'string':
                len_ = self.deserialize_num(4)
                return self.read_bytes(len_).decode('utf8')
            else:
                assert False, fieldType
        elif type(fieldType) == list:
            assert len(fieldType) == 1
            if type(fieldType[0]) == int:
                return bytes(self.read_bytes(fieldType[0]))
            else:
                len_ = self.deserialize_num(4)
                return [
                    self.deserialize_field(fieldType[0]) for _ in range(len_)
                ]
        elif type(fieldType) == dict:
            assert fieldType['kind'] == 'option'
            is_none = self.deserialize_num(1) == 0
            if is_none:
                return None
            else:
                return self.deserialize_field(fieldType['type'])
        elif type(fieldType) == type:
            return self.deserialize_struct(fieldType)
        else:
            assert False, type(fieldType)

    def serialize_struct(self, obj):
        structSchema = self.schema[type(obj)]
        if structSchema['kind'] == 'struct':
            for fieldName, fieldType in structSchema['fields']:
                try:
                    self.serialize_field(getattr(obj, fieldName), fieldType)
                except AssertionError as exc:
                    raise AssertionError(f"Error in field {fieldName}") from exc
        elif structSchema['kind'] == 'enum':
            name = getattr(obj, structSchema['field'])
            for idx, (fieldName,
                      fieldType) in enumerate(structSchema['values']):
                if fieldName == name:
                    self.serialize_num(idx, 1)
                    try:
                        self.serialize_field(getattr(obj, fieldName), fieldType)
                    except AssertionError as exc:
                        raise AssertionError(
                            f"Error in field {fieldName}") from exc
                    break
            else:
                assert False, name
        else:
            assert False, structSchema

    def deserialize_struct(self, type_):
        structSchema = self.schema[type_]
        if structSchema['kind'] == 'struct':
            ret = type_()
            for fieldName, fieldType in structSchema['fields']:
                setattr(ret, fieldName, self.deserialize_field(fieldType))
            return ret
        elif structSchema['kind'] == 'enum':
            ret = type_()
            value_ord = self.deserialize_num(1)
            value_schema = structSchema['values'][value_ord]
            setattr(ret, structSchema['field'], value_schema[0])
            setattr(ret, value_schema[0],
                    self.deserialize_field(value_schema[1]))

            return ret
        else:
            assert False, structSchema

    def serialize(self, obj):
        self.serialize_struct(obj)
        return bytes(self.array)

    def deserialize(self, bytes_, type_):
        self.array = bytearray(bytes_)
        self.offset = 0
        ret = self.deserialize_field(type_)
        assert self.offset == len(bytes_), "%s != %s" % (self.offset,
                                                         len(bytes_))
        return ret

'''
'''--- pytest/lib/state_sync_lib.py ---
import pathlib
import tempfile

def approximate_epoch_height(block_height, epoch_length):
    if block_height == 0:
        return 0
    if block_height <= epoch_length:
        # According to the protocol specifications, there are two epochs with height 1.
        return "1*"
    return int((block_height - 1) / epoch_length)

def get_state_sync_configs_pair():
    state_parts_dir = str(pathlib.Path(tempfile.gettempdir()) / "state_parts")

    config_dump = {
        "state_sync": {
            "dump": {
                "location": {
                    "Filesystem": {
                        "root_dir": state_parts_dir
                    }
                },
                "iteration_delay": {
                    "secs": 0,
                    "nanos": 100000000
                },
            }
        },
        "store.state_snapshot_enabled": True,
        "tracked_shards": [0],  # Track all shards
    }
    config_sync = {
        "consensus.state_sync_timeout": {
            "secs": 0,
            "nanos": 500000000
        },
        "state_sync": {
            "sync": {
                "ExternalStorage": {
                    "location": {
                        "Filesystem": {
                            "root_dir": state_parts_dir
                        }
                    }
                }
            }
        },
        "state_sync_enabled": True,
        "tracked_shards": [0],  # Track all shards
    }

    return (config_dump, config_sync)

def get_state_sync_config_combined():
    state_parts_dir = str(pathlib.Path(tempfile.gettempdir()) / "state_parts")
    config = {
        "consensus.state_sync_timeout": {
            "secs": 0,
            "nanos": 500000000
        },
        "state_sync": {
            "dump": {
                "location": {
                    "Filesystem": {
                        "root_dir": state_parts_dir
                    }
                },
                "iteration_delay": {
                    "secs": 0,
                    "nanos": 100000000
                },
            },
            "sync": {
                "ExternalStorage": {
                    "location": {
                        "Filesystem": {
                            "root_dir": state_parts_dir
                        }
                    }
                }
            }
        },
        "state_sync_enabled": True,
        "store.state_snapshot_enabled": True,
        "tracked_shards": [0],  # Track all shards
    }

    return config

'''
'''--- pytest/lib/transaction.py ---
from serializer import BinarySerializer
import hashlib
from ed25519 import SigningKey
import base58

from messages.tx import *
from messages.crypto import *
from messages.bridge import *

schema = dict(tx_schema + crypto_schema + bridge_schema)

def compute_tx_hash(receiverId, nonce, actions, blockHash, accountId, pk):
    tx = Transaction()
    tx.signerId = accountId
    tx.publicKey = PublicKey()
    tx.publicKey.keyType = 0
    tx.publicKey.data = pk
    tx.nonce = nonce
    tx.receiverId = receiverId
    tx.actions = actions
    tx.blockHash = blockHash

    msg = BinarySerializer(schema).serialize(tx)
    hash_ = hashlib.sha256(msg).digest()

    return tx, hash_

def sign_and_serialize_transaction(receiverId, nonce, actions, blockHash,
                                   accountId, pk, sk):
    tx, hash_ = compute_tx_hash(receiverId, nonce, actions, blockHash,
                                accountId, pk)

    signature = Signature()
    signature.keyType = 0
    signature.data = SigningKey(sk).sign(hash_)

    signedTx = SignedTransaction()
    signedTx.transaction = tx
    signedTx.signature = signature

    return BinarySerializer(schema).serialize(signedTx)

def compute_delegated_action_hash(senderId, receiverId, actions, nonce,
                                  maxBlockHeight, publicKey):
    delegateAction = DelegateAction()
    delegateAction.senderId = senderId
    delegateAction.receiverId = receiverId
    delegateAction.actions = actions
    delegateAction.nonce = nonce
    delegateAction.maxBlockHeight = maxBlockHeight
    delegateAction.publicKey = PublicKey()
    delegateAction.publicKey.keyType = 0
    delegateAction.publicKey.data = publicKey
    signableMessageDiscriminant = 2**30 + 366
    serializer = BinarySerializer(schema)
    serializer.serialize_num(signableMessageDiscriminant, 4)
    msg = serializer.serialize(delegateAction)
    hash_ = hashlib.sha256(msg).digest()

    return delegateAction, hash_

# Used by meta-transactions.
# Creates a SignedDelegate that is later put into the DelegateAction by relayer.
def create_signed_delegated_action(senderId, receiverId, actions, nonce,
                                   maxBlockHeight, publicKey, sk):
    delegated_action, hash_ = compute_delegated_action_hash(
        senderId, receiverId, actions, nonce, maxBlockHeight, publicKey)

    signature = Signature()
    signature.keyType = 0
    signature.data = SigningKey(sk).sign(hash_)

    signedDA = SignedDelegate()
    signedDA.delegateAction = delegated_action
    signedDA.signature = signature
    return signedDA

def create_create_account_action():
    createAccount = CreateAccount()
    action = Action()
    action.enum = 'createAccount'
    action.createAccount = createAccount
    return action

def create_full_access_key_action(pk):
    permission = AccessKeyPermission()
    permission.enum = 'fullAccess'
    permission.fullAccess = FullAccessPermission()
    accessKey = AccessKey()
    accessKey.nonce = 0
    accessKey.permission = permission
    publicKey = PublicKey()
    publicKey.keyType = 0
    publicKey.data = pk
    addKey = AddKey()
    addKey.accessKey = accessKey
    addKey.publicKey = publicKey
    action = Action()
    action.enum = 'addKey'
    action.addKey = addKey
    return action

def create_delete_access_key_action(pk):
    publicKey = PublicKey()
    publicKey.keyType = 0
    publicKey.data = pk
    deleteKey = DeleteKey()
    deleteKey.publicKey = publicKey
    action = Action()
    action.enum = 'deleteKey'
    action.deleteKey = deleteKey
    return action

def create_payment_action(amount):
    transfer = Transfer()
    transfer.deposit = amount
    action = Action()
    action.enum = 'transfer'
    action.transfer = transfer
    return action

def create_staking_action(amount, pk):
    stake = Stake()
    stake.stake = amount
    stake.publicKey = PublicKey()
    stake.publicKey.keyType = 0
    stake.publicKey.data = pk
    action = Action()
    action.enum = 'stake'
    action.stake = stake
    return action

def create_deploy_contract_action(code):
    deployContract = DeployContract()
    deployContract.code = code
    action = Action()
    action.enum = 'deployContract'
    action.deployContract = deployContract
    return action

def create_function_call_action(methodName, args, gas, deposit):
    functionCall = FunctionCall()
    functionCall.methodName = methodName
    functionCall.args = args
    functionCall.gas = gas
    functionCall.deposit = deposit
    action = Action()
    action.enum = 'functionCall'
    action.functionCall = functionCall
    return action

def create_delete_account_action(beneficiary):
    deleteAccount = DeleteAccount()
    deleteAccount.beneficiaryId = beneficiary
    action = Action()
    action.enum = 'deleteAccount'
    action.deleteAccount = deleteAccount
    return action

def create_delegate_action(signedDelegate):
    action = Action()
    action.enum = 'delegate'
    action.delegate = signedDelegate
    return action

def sign_delegate_action(signedDelegate, signer_key, contract_id, nonce,
                         blockHash):
    action = create_delegate_action(signedDelegate)
    return sign_and_serialize_transaction(contract_id, nonce, [action],
                                          blockHash, signer_key.account_id,
                                          signer_key.decoded_pk(),
                                          signer_key.decoded_sk())

def sign_create_account_tx(creator_key, new_account_id, nonce, block_hash):
    action = create_create_account_action()
    return sign_and_serialize_transaction(new_account_id, nonce, [action],
                                          block_hash, creator_key.account_id,
                                          creator_key.decoded_pk(),
                                          creator_key.decoded_sk())

def sign_create_account_with_full_access_key_and_balance_tx(
        creator_key, new_account_id, new_key, balance, nonce, block_hash):
    create_account_action = create_create_account_action()
    full_access_key_action = create_full_access_key_action(new_key.decoded_pk())
    payment_action = create_payment_action(balance)
    actions = [create_account_action, full_access_key_action, payment_action]
    return sign_and_serialize_transaction(new_account_id, nonce, actions,
                                          block_hash, creator_key.account_id,
                                          creator_key.decoded_pk(),
                                          creator_key.decoded_sk())

def sign_delete_access_key_tx(signer_key, target_account_id, key_for_deletion,
                              nonce, block_hash):
    action = create_delete_access_key_action(key_for_deletion.decoded_pk())
    return sign_and_serialize_transaction(target_account_id, nonce, [action],
                                          block_hash, signer_key.account_id,
                                          signer_key.decoded_pk(),
                                          signer_key.decoded_sk())

def sign_payment_tx(key, to, amount, nonce, blockHash):
    action = create_payment_action(amount)
    return sign_and_serialize_transaction(to, nonce, [action], blockHash,
                                          key.account_id, key.decoded_pk(),
                                          key.decoded_sk())

def sign_payment_tx_and_get_hash(key, to, amount, nonce, block_hash):
    action = create_payment_action(amount)
    _, hash_bytes = compute_tx_hash(to, nonce, [action], block_hash,
                                    key.account_id, key.decoded_pk())
    signed_tx = sign_payment_tx(key, to, amount, nonce, block_hash)
    return signed_tx, base58.b58encode(hash_bytes).decode('utf8')

def sign_staking_tx(signer_key, validator_key, amount, nonce, blockHash):
    action = create_staking_action(amount, validator_key.decoded_pk())
    return sign_and_serialize_transaction(signer_key.account_id, nonce,
                                          [action], blockHash,
                                          signer_key.account_id,
                                          signer_key.decoded_pk(),
                                          signer_key.decoded_sk())

def sign_staking_tx_and_get_hash(signer_key, validator_key, amount, nonce,
                                 block_hash):
    action = create_staking_action(amount, validator_key.decoded_pk())
    _, hash_bytes = compute_tx_hash(signer_key.account_id, nonce, [action],
                                    block_hash, signer_key.account_id,
                                    signer_key.decoded_pk())
    signed_tx = sign_staking_tx(signer_key, validator_key, amount, nonce,
                                block_hash)
    return signed_tx, base58.b58encode(hash_bytes).decode('utf8')

def sign_deploy_contract_tx(signer_key, code, nonce, blockHash):
    action = create_deploy_contract_action(code)
    return sign_and_serialize_transaction(signer_key.account_id, nonce,
                                          [action], blockHash,
                                          signer_key.account_id,
                                          signer_key.decoded_pk(),
                                          signer_key.decoded_sk())

def sign_function_call_tx(signer_key, contract_id, methodName, args, gas,
                          deposit, nonce, blockHash):
    action = create_function_call_action(methodName, args, gas, deposit)
    return sign_and_serialize_transaction(contract_id, nonce, [action],
                                          blockHash, signer_key.account_id,
                                          signer_key.decoded_pk(),
                                          signer_key.decoded_sk())

def sign_delete_account_tx(key, to, beneficiary, nonce, block_hash):
    action = create_delete_account_action(beneficiary)
    return sign_and_serialize_transaction(to, nonce, [action], block_hash,
                                          key.account_id, key.decoded_pk(),
                                          key.decoded_sk())

'''
'''--- pytest/lib/utils.py ---
import base58
import hashlib
import json
import os
import pathlib
import random
import re
import shutil
import sys
import tempfile
import time
import typing
import requests
from prometheus_client.parser import text_string_to_metric_families
from retrying import retry
from rc import gcloud

import cluster
from configured_logger import logger
from transaction import sign_payment_tx

class TxContext:

    def __init__(self, act_to_val, nodes):
        self.next_nonce = 2
        self.num_nodes = len(nodes)
        self.nodes = nodes
        self.act_to_val = act_to_val
        self.expected_balances = self.get_balances()
        assert len(act_to_val) == self.num_nodes
        assert self.num_nodes >= 2

    @retry(stop_max_attempt_number=10, wait_exponential_multiplier=1.2)
    def get_balance(self, whose):
        r = self.nodes[self.act_to_val[whose]].get_account("test%s" % whose)
        assert 'result' in r, r
        return int(r['result']['amount']) + int(r['result']['locked'])

    def get_balances(self):
        return [self.get_balance(i) for i in range(self.num_nodes)]

    def send_moar_txs(self, last_block_hash, num, use_routing):
        last_balances = [x for x in self.expected_balances]
        for i in range(num):
            while True:
                from_ = random.randint(0, self.num_nodes - 1)
                if self.nodes[from_] is not None:
                    break
            to = random.randint(0, self.num_nodes - 2)
            if to >= from_:
                to += 1
            amt = random.randint(0, 500)
            if self.expected_balances[from_] >= amt:
                logger.info("Sending a tx from %s to %s for %s" %
                            (from_, to, amt))
                tx = sign_payment_tx(
                    self.nodes[from_].signer_key, 'test%s' % to, amt,
                    self.next_nonce,
                    base58.b58decode(last_block_hash.encode('utf8')))
                if use_routing:
                    self.nodes[0].send_tx(tx)
                else:
                    self.nodes[self.act_to_val[from_]].send_tx(tx)
                self.expected_balances[from_] -= amt
                self.expected_balances[to] += amt
                self.next_nonce += 1

class LogTracker:
    """Opens up a log file, scrolls to the end and allows to check for patterns.

    The tracker works only on local nodes.

    PLEASE AVOID USING THE TRACKER IN NEW TESTS.
    As depending on the exact log wording is making tests very fragile.
    Try depending on a metric instead.
    """

    def __init__(self, node: cluster.BaseNode) -> None:
        """Initialises the tracker for given local node.

        Args:
            node: Node to create tracker for.
        Raises:
            NotImplementedError: If trying to create a tracker for non-local
                node.
        """
        if not isinstance(node, cluster.LocalNode):
            raise NotImplementedError()
        self.fname = node.stderr_name
        with open(self.fname) as f:
            f.seek(0, 2)
            self.offset = f.tell()

    # Pattern matching ANSI escape codes starting with a Control Sequence
    # Introducer (CSI) sequence.  Most notably Select Graphic Rendition (SGR)
    # such as ‘\x1b[35;41m’.
    _CSI_RE = re.compile('\x1b\\[[^\x40-\x7E]*[\x40-\x7E]')

    def _read_file(self) -> str:
        """Returns data from the file starting from the offset."""
        with open(self.fname) as rd:
            rd.seek(self.offset)
            data = rd.read()
            self.offset = rd.tell()
        # Strip ANSI codes
        return self._CSI_RE.sub('', data)

    def check(self, pattern: str) -> bool:
        """Check whether the pattern can be found in the logs."""
        return pattern in self._read_file()

    def check_re(self, pattern: str) -> bool:
        """Check whether the regex pattern can be found in the logs."""
        return re.search(pattern, self._read_file()) != None

    def reset(self) -> None:
        """Resets log offset to beginning of the file."""
        self.offset = 0

    def count(self, pattern: str) -> int:
        """Count number of occurrences of pattern in new logs."""
        return self._read_file().count(pattern)

class MetricsTracker:
    """Helper class to collect prometheus metrics from the node.
    
    Usage:
        tracker = MetricsTracker(node)
        assert tracker.get_int_metric_value("near-connections") == 2
    """

    def __init__(self, node: cluster.BaseNode) -> None:
        if not isinstance(node, cluster.LocalNode):
            raise NotImplementedError()
        host, port = node.rpc_addr()

        self.addr = f"http://{host}:{port}/metrics"

    def get_all_metrics(self) -> str:
        response = requests.get(self.addr)
        if not response.ok:
            raise RuntimeError(
                f"Could not fetch metrics from {self.addr}: {response}")
        return response.content.decode('utf-8')

    def get_metric_all_values(
            self, metric_name: str) -> typing.List[typing.Tuple[str, str]]:
        for family in text_string_to_metric_families(self.get_all_metrics()):
            if family.name == metric_name:
                return [
                    (sample.labels, sample.value) for sample in family.samples
                ]
        return []

    def get_metric_value(
        self,
        metric_name: str,
        labels: typing.Optional[typing.Dict[str, str]] = None
    ) -> typing.Optional[str]:
        all_samples = self.get_metric_all_values(metric_name)
        if not labels:
            if len(all_samples) > 1:
                raise AssertionError(
                    f"Too many metric values ({len(all_samples)}) for {metric_name} - please specify a label"
                )
            if not all_samples:
                return None
            (sample_labels, sample_value) = all_samples[0]
            return sample_value
        for (sample_labels, sample_value) in all_samples:
            if sample_labels == labels:
                return sample_value
        return None

    def get_int_metric_value(
        self,
        metric_name: str,
        labels: typing.Optional[typing.Dict[str, str]] = None
    ) -> typing.Optional[int]:
        """Helper function to return the integer value of the metric (as function above returns strings)."""
        value = self.get_metric_value(metric_name, labels)
        if value is None:
            return None
        return round(float(value))

def chain_query(node, block_handler, *, block_hash=None, max_blocks=-1):
    """
    Query chain block approvals and chunks preceding of block of block_hash.
    If block_hash is None, it query latest block hash
    It query at most max_blocks, or if it's -1, all blocks back to genesis
    """
    block_hash = block_hash or node.get_latest_block().hash
    initial_validators = node.validators()

    if max_blocks == -1:
        while True:
            validators = node.validators()
            if validators != initial_validators:
                logger.critical(
                    f'Fatal: validator set of node {node} changes, from {initial_validators} to {validators}'
                )
                sys.exit(1)
            block = node.get_block(block_hash)['result']
            block_handler(block)
            block_hash = block['header']['prev_hash']
            block_height = block['header']['height']
            if block_height == 0:
                break
    else:
        for _ in range(max_blocks):
            validators = node.validators()
            if validators != initial_validators:
                logger.critical(
                    f'Fatal: validator set of node {node} changes, from {initial_validators} to {validators}'
                )
                sys.exit(1)
            block = node.get_block(block_hash)['result']
            block_handler(block)
            block_hash = block['header']['prev_hash']
            block_height = block['header']['height']
            if block_height == 0:
                break

def get_near_tempdir(subdir=None, *, clean=False):
    tempdir = pathlib.Path(tempfile.gettempdir()) / 'near'
    if subdir:
        tempdir = tempdir / subdir
    if clean and tempdir.exists():
        shutil.rmtree(tempdir)
    tempdir.mkdir(parents=True, exist_ok=True)
    return tempdir

def load_binary_file(filepath):
    with open(filepath, "rb") as binaryfile:
        return bytearray(binaryfile.read())

def load_test_contract(
        filename: str = 'backwards_compatible_rs_contract.wasm') -> bytearray:
    """Loads a WASM file from near-test-contracts package.

    This is just a convenience function around load_binary_file which loads
    files from ../runtime/near-test-contracts/res directory.  By default
    test_contract_rs.wasm is loaded.
    """
    repo_dir = pathlib.Path(__file__).resolve().parents[2]
    path = repo_dir / 'runtime/near-test-contracts/res' / filename
    return load_binary_file(path)

def user_name():
    username = os.getlogin()
    if username == 'root':  # digitalocean
        username = gcloud.list()[0].username.replace('_nearprotocol_com', '')
    return username

def collect_gcloud_config(num_nodes):
    tempdir = get_near_tempdir()
    keys = []
    for i in range(num_nodes):
        node_dir = tempdir / f'node{i}'
        if not node_dir.exists():
            # TODO: avoid hardcoding the username
            logger.info(f'downloading node{i} config from gcloud')
            node_dir.mkdir(parents=True, exist_ok=True)
            host = gcloud.get(f'pytest-node-{user_name()}-{i}')
            for filename in ('config.json', 'signer0_key.json',
                             'validator_key.json', 'node_key.json'):
                host.download(f'/home/bowen_nearprotocol_com/.near/{filename}',
                              str(node_dir))
        with open(node_dir / 'signer0_key.json') as f:
            key = json.load(f)
        keys.append(key)
    with open(tempdir / 'node0' / 'config.json') as f:
        config = json.load(f)
    ip_addresses = map(lambda x: x.split('@')[-1],
                       config['network']['boot_nodes'].split(','))
    res = {
        'nodes':
            list(
                map(lambda x: {
                    'ip': x.split(':')[0],
                    'port': 3030
                }, ip_addresses)),
        'accounts':
            keys
    }
    outfile = tempdir / 'gcloud_config.json'
    with open(outfile, 'w') as f:
        json.dump(res, f)
    os.environ[cluster.CONFIG_ENV_VAR] = str(outfile)

def obj_to_string(obj, extra='    ', full=False):
    if type(obj) in [tuple, list]:
        return "tuple" + '\n' + '\n'.join(
            (extra + obj_to_string(x, extra + '    ')) for x in obj)
    elif hasattr(obj, "__dict__"):
        return str(obj.__class__) + '\n' + '\n'.join(
            extra + (str(item) + ' = ' +
                     obj_to_string(obj.__dict__[item], extra + '    '))
            for item in sorted(obj.__dict__))
    elif isinstance(obj, bytes):
        if not full:
            if len(obj) > 10:
                obj = obj[:7] + b"..."
        return str(obj)
    else:
        return str(obj)

def combine_hash(hash1, hash2):
    return hashlib.sha256(hash1 + hash2).digest()

def compute_merkle_root_from_path(path, leaf_hash):
    res = base58.b58decode(leaf_hash) if type(leaf_hash) is str else leaf_hash
    for node in path:
        if node['direction'] == 'Left':
            res = combine_hash(base58.b58decode(node['hash']), res)
        else:
            res = combine_hash(res, base58.b58decode(node['hash']))
    return res

def poll_epochs(node: cluster.LocalNode,
                *,
                epoch_length,
                num_blocks_per_year: int = 31536000,
                timeout: float = 300) -> typing.Iterable[int]:
    """Polls a node about the latest epoch and yields it when it changes.

    The function continues yielding epoch heights indefinitely (so long as the node
    continues reporting them) until timeout is reached or the caller stops
    reading yielded values.  Reaching the timeout is considered to be a failure
    condition and thus it results in an `AssertionError`.  The expected usage is
    that caller reads epoch heights until some condition is met at which point it stops
    iterating over the generator.

    Args:
        node: Node to query about its latest epoch.
        timeout: Total timeout from the first status request sent to the node.
        epoch_length: epoch_length genesis config value
        num_blocks_per_year: num_blocks_per_year genesis config value
    Yields:
        An int for each new epoch height reported. Note that there
        is no guarantee that there will be no skipped epochs.
    Raises:
        AssertionError: If more than `timeout` seconds passes from the start of
            the iteration, or the response from the node is not as expected.
    """
    end = time.time() + timeout
    start_height = -1
    epoch_start = -1
    count = 0
    previous = -1

    while time.time() < end:
        response = node.get_validators()
        assert 'error' not in response, response

        latest = response['result']
        height = latest['epoch_height']
        assert isinstance(height, int) and height >= 1, height

        if start_height == -1:
            start_height = height

        if previous != height:
            yield height

            count += 1
            previous = height
            epoch_start = latest['epoch_start_height']
            assert isinstance(epoch_start,
                              int) and epoch_start >= 1, epoch_start

        blocks_left = epoch_start + epoch_length - node.get_latest_block(
        ).height
        seconds_left = blocks_left / (num_blocks_per_year / 31536000)
        time.sleep(max(seconds_left, 2))

    msg = 'Timed out polling epochs from a node\n'
    if count:
        msg += (f'First epoch: {start_height}; last epoch: {previous}\n'
                f'Total epochs returned: {count}')
    else:
        msg += 'No epochs were returned'
    raise AssertionError(msg)

def poll_blocks(node: cluster.LocalNode,
                *,
                timeout: float = 120,
                poll_interval: float = 0.25,
                __target: typing.Optional[int] = None,
                **kw) -> typing.Iterable[cluster.BlockId]:
    """Polls a node about the latest block and yields it when it changes.

    The function continues yielding blocks indefinitely (so long as the node
    continues reporting its status) until timeout is reached or the caller stops
    reading yielded values.  Reaching the timeout is considered to be a failure
    condition and thus it results in an `AssertionError`.  The expected usage is
    that caller reads blocks until some condition is met at which point it stops
    iterating over the generator.

    Args:
        node: Node to query about its latest block.
        timeout: Total timeout from the first status request sent to the node.
        poll_interval: How long to wait in seconds between each status request
            sent to the node.
        kw: Keyword arguments passed to `BaseDone.get_latest_block` method.
    Yields:
        A `cluster.BlockId` object for each each time node’s latest block
        changes including the first block when function starts.  Note that there
        is no guarantee that there will be no skipped blocks.
    Raises:
        AssertionError: If more than `timeout` seconds passes from the start of
            the iteration.
    """
    end = time.monotonic() + timeout
    start_height = -1
    count = 0
    previous = -1

    while time.monotonic() < end:
        latest = node.get_latest_block(**kw)
        if latest.height != previous:
            if __target:
                msg = f'{latest}  (waiting for #{__target})'
            else:
                msg = str(latest)
            logger.info(msg)
            yield latest
            previous = latest.height
            if start_height == -1:
                start_height = latest.height
            count += 1
        time.sleep(poll_interval)

    msg = 'Timed out polling blocks from a node\n'
    if count > 0:
        msg += (f'First block: {start_height}; last block: {previous}\n'
                f'Total blocks returned: {count}')
    else:
        msg += 'No blocks were returned'
    if __target:
        msg += f'\nWaiting for block: {__target}'
    raise AssertionError(msg)

def wait_for_blocks(node: cluster.LocalNode,
                    *,
                    target: typing.Optional[int] = None,
                    count: typing.Optional[int] = None,
                    timeout: typing.Optional[float] = None,
                    **kw) -> cluster.BlockId:
    """Waits until given node reaches expected target block height.

    Exactly one of `target` or `count` arguments must be specified.  Specifying
    `count` is equivalent to setting `target` to node’s current height plus the
    given count.

    Args:
        node: Node to query about its latest block.
        target: Target height of the latest block known by the node.
        count: How many new blocks to wait for.  If this argument is given,
            target is calculated as node’s current block height plus the given
            count.
        timeout: Total timeout from the first status request sent to the node.
            If not specified, the default is to assume that overall each block
            takes no more than five seconds to generate.
        kw: Keyword arguments passed to `poll_blocks`.  `timeout` and
            `poll_interval` are likely of most interest.
    Returns:
        A `cluster.BlockId` of the block at target height.
    Raises:
        AssertionError: If the node does not reach given block height before
            timeout passes.
    """
    if target is None:
        if count is None:
            raise TypeError('Expected `count` or `target` keyword argument')
        target = node.get_latest_block().height + count
    else:
        if count is not None:
            raise TypeError(
                'Expected at most one of `count` or `target` arguments')
        if timeout is None:
            count = max(0, target - node.get_latest_block().height)
    if timeout is None:
        timeout = max(10, count * 5)
    for latest in poll_blocks(node, timeout=timeout, __target=target, **kw):
        if latest.height >= target:
            return latest

def figure_out_sandbox_binary():
    config = {
        'local': True,
        'release': False,
    }
    repo_dir = pathlib.Path(__file__).resolve().parents[2]
    # When run on NayDuck we end up with a binary called neard in target/debug
    # but when run locally the binary might be neard-sandbox or near-sandbox
    # instead.  Try to figure out whichever binary is available and use that.
    for release in ('release', 'debug'):
        root = repo_dir / 'target' / release
        for exe in ('neard-sandbox', 'near-sandbox', 'neard'):
            if (root / exe).exists():
                logger.info(
                    f'Using {(root / exe).relative_to(repo_dir)} binary')
                config['near_root'] = str(root)
                config['binary_name'] = exe
                return config

    assert False, ('Unable to figure out location of neard-sandbox binary; '
                   'Did you forget to run `make sandbox`?')

'''
'''--- pytest/requirements.txt ---
PyGithub
base58
cython
deepdiff
ed25519
numpy
prometheus-client
psutil
pynacl
python-rc==0.3.9
requests
retrying
scikit-learn
scipy
semver
toml
tqdm
urllib3<2

'''
'''--- pytest/tests/__init__.py ---

'''
'''--- pytest/tests/adversarial/fork_sync.py ---
#!/usr/bin/env python3
# Spins up four validating nodes. Wait until they produce 20 blocks.
# Kill the first two nodes, let the rest two produce 30 blocks.
# Kill the remaining two and restart the first two. Let them produce also 30 blocks
# Restart the two that were killed and make sure they can sync with the other chain
# and produce blocks

import sys, time
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import start_cluster
from configured_logger import logger
import utils

TIMEOUT = 120
FIRST_STEP_WAIT = 20
SECOND_STEP_WAIT = 30
FINAL_HEIGHT_THRESHOLD = 80

nodes = start_cluster(
    4, 0, 4, None,
    [["epoch_length", 200], ["block_producer_kickout_threshold", 10]], {})
time.sleep(3)
cur_height = 0
fork1_height = 0
fork2_height = 0

for i in range(0, 4):
    res = nodes[i].json_rpc('adv_disable_doomslug', [])
    assert 'result' in res, res

# step 1, let nodes run for some time
utils.wait_for_blocks(nodes[0], target=FIRST_STEP_WAIT)

for i in range(2):
    nodes[i].kill()

logger.info("killing node 0 and 1")
utils.wait_for_blocks(nodes[2], target=FIRST_STEP_WAIT + SECOND_STEP_WAIT)

for i in range(2, 4):
    nodes[i].kill()

logger.info("killing node 2 and 3")

for i in range(2):
    nodes[i].start(boot_node=nodes[0])
    res = nodes[i].json_rpc('adv_disable_doomslug', [])
    assert 'result' in res, res

time.sleep(1)

utils.wait_for_blocks(nodes[0], target=FIRST_STEP_WAIT + SECOND_STEP_WAIT)

for i in range(2, 4):
    nodes[i].start(boot_node=nodes[0])
    res = nodes[i].json_rpc('adv_disable_doomslug', [])
    assert 'result' in res, res

time.sleep(1)

logger.info("all nodes restarted")

while cur_height < TIMEOUT:
    statuses = sorted((enumerate(node.get_latest_block() for node in nodes)),
                      key=lambda element: element[1].height)
    last = statuses.pop()
    cur_height = last[1].height
    node = nodes[last[0]]
    succeed = True
    for _, block in statuses:
        try:
            node.get_block(block.hash)
        except Exception:
            succeed = False
            break
    if statuses[0][1].height > FINAL_HEIGHT_THRESHOLD and succeed:
        sys.exit(0)
    time.sleep(0.5)

assert False, "timed out waiting for forks to resolve"

'''
'''--- pytest/tests/adversarial/gc_rollback.py ---
#!/usr/bin/env python3
# Builds the following graph:
# -------
#    \
#     ------
#       \
#        --------
#             \
#              ----------
# checks that GC not failing

import sys, time
import random
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import start_cluster
from configured_logger import logger

EPOCH_LENGTH = 30
NUM_BLOCKS_TOTAL = 200
FORK_EACH_BLOCKS = 10

consensus_config = {"consensus": {"min_num_peers": 0}}
nodes = start_cluster(2, 0, 1, None, [["epoch_length", EPOCH_LENGTH]],
                      {0: consensus_config})
time.sleep(2)

res = nodes[0].json_rpc('adv_disable_doomslug', [])
assert 'result' in res, res
res = nodes[1].json_rpc('adv_disable_doomslug', [])
assert 'result' in res, res

cur_height = 0
last_fork = FORK_EACH_BLOCKS * 2
while cur_height < NUM_BLOCKS_TOTAL:
    cur_height = nodes[0].get_latest_block(verbose=True).height
    if cur_height > last_fork:
        new_height = cur_height - random.randint(1, FORK_EACH_BLOCKS)
        nodes[1].kill()
        nodes[1].reset_data()

        logger.info("Rolling back from %d to %d" % (cur_height, new_height))
        res = nodes[0].json_rpc('adv_switch_to_height', [new_height])
        assert 'result' in res, res
        #res = nodes[1].json_rpc('adv_switch_to_height', [new_height])
        #assert 'result' in res, res

        nodes[1].start(boot_node=nodes[0])
        res = nodes[1].json_rpc('adv_disable_doomslug', [])
        assert 'result' in res, res

        last_fork += FORK_EACH_BLOCKS
    time.sleep(0.9)

saved_blocks = nodes[0].json_rpc('adv_get_saved_blocks', [])
logger.info(saved_blocks)

'''
'''--- pytest/tests/adversarial/malicious_chain.py ---
#!/usr/bin/env python3
# starts two validators and one extra RPC node, and directs one of the validators
# to produce invalid blocks. Then we check that the other two nodes have banned this peer.

import sys, time
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import start_cluster
from configured_logger import logger
import utils

valid_blocks_only = False  # creating invalid blocks, should be banned instantly
if "valid_blocks_only" in sys.argv:
    valid_blocks_only = True  # creating valid blocks, should be fixed by doom slug

BLOCKS = 25
MALICIOUS_BLOCKS = 50

nodes = start_cluster(
    2, 1, 2, None,
    [["epoch_length", 1000], ["block_producer_kickout_threshold", 80]], {})

started = time.time()

logger.info(f'Waiting for {BLOCKS} blocks...')
height, _ = utils.wait_for_blocks(nodes[1], target=BLOCKS)
logger.info(f'Got to {height} blocks, getting to fun stuff')

# first check that nodes 0 and 2 have two peers each, before we check later that
# they've dropped to just one peer
network_info0 = nodes[0].json_rpc('network_info', {})['result']
network_info2 = nodes[2].json_rpc('network_info', {})['result']
assert network_info0['num_active_peers'] == 2, network_info0['num_active_peers']
assert network_info2['num_active_peers'] == 2, network_info2['num_active_peers']

nodes[1].get_status(verbose=True)

res = nodes[1].json_rpc('adv_produce_blocks',
                        [MALICIOUS_BLOCKS, valid_blocks_only])
assert 'result' in res, res
logger.info("Generated %s malicious blocks" % MALICIOUS_BLOCKS)

time.sleep(10)

height = nodes[0].get_latest_block(verbose=True).height

assert height < 40

network_info0 = nodes[0].json_rpc('network_info', {})['result']
network_info2 = nodes[2].json_rpc('network_info', {})['result']

# Since we have 3 nodes, they should all have started with 2 peers. After the above
# invalid blocks sent by node1, the other two should have banned it, leaving them
# with one active peer

assert network_info0['num_active_peers'] == 1, network_info0['num_active_peers']
assert network_info2['num_active_peers'] == 1, network_info2['num_active_peers']

logger.info("Epic")

'''
'''--- pytest/tests/adversarial/start_from_genesis.py ---
#!/usr/bin/env python3
import sys, time
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import start_cluster
from configured_logger import logger
import utils

overtake = False  # create a new chain which is shorter than current one
if "overtake" in sys.argv:
    overtake = True  # create a new chain which is longer than current one

doomslug = True
if "doomslug_off" in sys.argv:
    doomslug = False  # turn off doomslug

BLOCKS = 30

# Low sync_check_period to sync from a new peer with greater height
client_config_change = {
    "consensus": {
        "sync_check_period": {
            "secs": 0,
            "nanos": 100000000
        }
    }
}

nodes = start_cluster(
    2, 0, 2, None,
    [["epoch_length", 100], ["block_producer_kickout_threshold", 80]],
    {0: client_config_change})
if not doomslug:
    # we expect inconsistency in store in node 0
    # because we're going to turn off doomslug
    # and allow applying blocks without proper validation
    nodes[0].stop_checking_store()

started = time.time()

time.sleep(2)
logger.info(f'Waiting for {BLOCKS} blocks...')
height, _ = utils.wait_for_blocks(nodes[0], target=BLOCKS)
logger.info(f'Got to {height} blocks, getting to fun stuff')

status = nodes[0].get_status()
logger.info(f"STATUS OF HONEST {status}")
saved_blocks = nodes[0].json_rpc('adv_get_saved_blocks', [])
logger.info(f"SAVED BLOCKS {saved_blocks}")

nodes[0].kill()  # to disallow syncing
nodes[1].kill()

# Switch node1 to an adversarial chain
nodes[1].reset_data()
nodes[1].start(boot_node=nodes[0])

num_produce_blocks = BLOCKS // 2 - 5
if overtake:
    num_produce_blocks += 10

res = nodes[1].json_rpc('adv_produce_blocks', [num_produce_blocks, True])
assert 'result' in res, res
time.sleep(2)
nodes[1].kill()

# Restart both nodes.
# Disabling doomslug must happen before starting node1
nodes[0].start(boot_node=nodes[0])
if not doomslug:
    res = nodes[0].json_rpc('adv_disable_doomslug', [])
    assert 'result' in res, res
nodes[1].start(boot_node=nodes[0])

time.sleep(3)
status = nodes[1].get_status()
logger.info(f"STATUS OF MALICIOUS {status}")

status = nodes[0].get_status()
logger.info(f"STATUS OF HONEST AFTER {status}")
height = status['sync_info']['latest_block_height']

saved_blocks_2 = nodes[0].json_rpc('adv_get_saved_blocks', [])
logger.info(f"SAVED BLOCKS AFTER MALICIOUS INJECTION {saved_blocks_2}")
logger.info(f"HEIGHT {height}")

assert saved_blocks['result'] < BLOCKS + 10
if overtake and not doomslug:
    # node 0 should accept additional blocks from node 1 because of new chain is longer and doomslug is turned off
    assert saved_blocks_2['result'] >= BLOCKS + num_produce_blocks
else:
    assert saved_blocks_2['result'] < saved_blocks['result'] + 10

logger.info("Epic")

'''
'''--- pytest/tests/contracts/deploy_call_smart_contract.py ---
#!/usr/bin/env python3
"""Deploy a smart contract on one node and call it on another."""

import sys
import time
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))
from cluster import start_cluster
from transaction import sign_deploy_contract_tx, sign_function_call_tx
from utils import load_test_contract

GGAS = 10**9

def test_deploy_contract():
    nodes = start_cluster(
        2, 0, 1, None,
        [["epoch_length", 10], ["block_producer_kickout_threshold", 80]], {})

    last_block_hash = nodes[0].get_latest_block().hash_bytes
    tx = sign_deploy_contract_tx(nodes[0].signer_key, load_test_contract(), 10,
                                 last_block_hash)
    nodes[0].send_tx(tx)
    time.sleep(3)

    last_block_hash = nodes[1].get_latest_block().hash_bytes
    tx = sign_function_call_tx(nodes[0].signer_key,
                               nodes[0].signer_key.account_id, 'log_something',
                               [], 150 * GGAS, 1, 20, last_block_hash)
    res = nodes[1].send_tx_and_wait(tx, 20)
    import json
    print(json.dumps(res, indent=2))
    assert res['result']['receipts_outcome'][0]['outcome']['logs'][0] == 'hello'

if __name__ == '__main__':
    test_deploy_contract()

'''
'''--- pytest/tests/contracts/gibberish.py ---
#!/usr/bin/env python3
# Experiments with deploying gibberish contracts. Specifically,
# 1. Deploys completely gibberish contracts
# 2. Gets an existing wasm contract, and tries to arbitrarily pertrurb bytes in it

import sys, time, random
import base58
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))
from cluster import start_cluster
from configured_logger import logger
from transaction import sign_deploy_contract_tx, sign_function_call_tx
from utils import load_test_contract

nodes = start_cluster(
    3, 0, 4, None,
    [["epoch_length", 1000], ["block_producer_kickout_threshold", 80]], {})

wasm_blob_1 = load_test_contract()

hash_ = nodes[0].get_latest_block().hash_bytes

for iter_ in range(10):
    logger.info("Deploying garbage contract #%s" % iter_)
    wasm_blob = bytes(
        [random.randint(0, 255) for _ in range(random.randint(200, 500))])
    tx = sign_deploy_contract_tx(nodes[0].signer_key, wasm_blob, 10 + iter_,
                                 hash_)
    nodes[0].send_tx_and_wait(tx, 20)

for iter_ in range(10):
    hash_ = nodes[0].get_latest_block().hash_bytes
    logger.info("Deploying perturbed contract #%s" % iter_)

    new_name = '%s_mething' % iter_
    new_output = '%s_llo' % iter_

    wasm_blob = wasm_blob_1.replace(bytes('something', 'utf8'),
                                    bytes(new_name, 'utf8')).replace(
                                        bytes('hello', 'utf8'),
                                        bytes(new_output, 'utf8'))
    assert len(wasm_blob) == len(wasm_blob_1)

    pos = random.randint(0, len(wasm_blob_1) - 1)
    val = random.randint(0, 255)
    wasm_blob = wasm_blob[:pos] + bytes([val]) + wasm_blob[pos + 1:]
    tx = sign_deploy_contract_tx(nodes[0].signer_key, wasm_blob, 20 + iter_ * 2,
                                 hash_)
    res = nodes[0].send_tx_and_wait(tx, 20)
    assert 'result' in res
    logger.info(res)

    logger.info("Invoking perturbed contract #%s" % iter_)

    tx2 = sign_function_call_tx(nodes[0].signer_key,
                                nodes[0].signer_key.account_id, new_name, [],
                                3000000000000, 100000000000, 20 + iter_ * 2 + 1,
                                hash_)
    # don't have any particular expectation for the call result, but the transaction should at least go through
    res = nodes[1].send_tx_and_wait(tx2, 20)
    assert 'result' in res

hash_ = nodes[0].get_latest_block().hash_bytes

logger.info("Real thing!")
tx = sign_deploy_contract_tx(nodes[0].signer_key, wasm_blob_1, 60, hash_)
nodes[0].send_tx(tx)

time.sleep(3)

hash_2 = nodes[1].get_latest_block().hash_bytes
tx2 = sign_function_call_tx(nodes[0].signer_key, nodes[0].signer_key.account_id,
                            'log_something', [], 3000000000000, 100000000000,
                            62, hash_2)
res = nodes[1].send_tx_and_wait(tx2, 20)
logger.info(res)
assert res['result']['receipts_outcome'][0]['outcome']['logs'][0] == 'hello'

'''
'''--- pytest/tests/contracts/infinite_loops.py ---
#!/usr/bin/env python3
# Spins up four nodes, deploy an smart contract to one node,
# Call a smart contract method in another node
import sys, time
import base58
import concurrent.futures
import requests
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))
from cluster import start_cluster
from transaction import sign_deploy_contract_tx, sign_function_call_tx
from utils import load_test_contract

nodes = start_cluster(
    4, 0, 1, None,
    [["epoch_length", 40], ["block_producer_kickout_threshold", 80],
     ["transaction_validity_period", 10000]], {
         0: {
             "tracked_shards": [0]
         },
         1: {
             "tracked_shards": [0]
         },
         2: {
             "tracked_shards": [0]
         },
         3: {
             "tracked_shards": [0]
         },
     })

hash_ = nodes[0].get_latest_block().hash_bytes
tx = sign_deploy_contract_tx(nodes[0].signer_key, load_test_contract(), 10,
                             hash_)
nodes[0].send_tx(tx)

time.sleep(3)

# send num_tx function calls from node[i]'s account
def send_transactions(i, num_tx):
    print("Sending Transactions {}".format(i))
    hash_2 = nodes[i].get_latest_block().hash_bytes
    nonce = 20
    for _ in range(num_tx):
        tx = sign_function_call_tx(nodes[i].signer_key,
                                   nodes[0].signer_key.account_id,
                                   'loop_forever', [], 300000000000, 0, nonce,
                                   hash_2)
        nonce += 1
        res = nodes[i].send_tx_and_wait(tx, 20)
        assert 'result' in res, res
        assert res['result']['status']['Failure']['ActionError']['kind'][
            'FunctionCallError'][
                'ExecutionError'] == 'Exceeded the prepaid gas.', "result: {}".format(
                    res)

with concurrent.futures.ThreadPoolExecutor() as executor:
    futures = []
    for i in range(len(nodes)):
        futures.append(executor.submit(send_transactions, i, 20))
    for future in concurrent.futures.as_completed(futures):
        future.result()

'''
'''--- pytest/tests/delete_remote_nodes.py ---
#!/usr/bin/env python3

# When script exit with traceback, remote node is not deleted. This script is
# to delete remote machines so test can be rerun
# DANGER: make sure not delete production nodes!

from rc import gcloud, pmap
from distutils.util import strtobool
import sys
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[1] / 'lib'))
from utils import user_name
from configured_logger import logger

machines = gcloud.list()
to_delete_prefix = sys.argv[1] if len(
    sys.argv) >= 2 else f"pytest-node-{user_name()}-"
to_delete = list(filter(lambda m: m.name.startswith(to_delete_prefix),
                        machines))

if to_delete:
    a = input(
        f"going to delete {list(map(lambda m: m.name, to_delete))}\ny/n: ")
    if strtobool(a):

        def delete_machine(m):
            logger.info(f'deleting {m.name}')
            m.delete()
            logger.info(f'{m.name} deleted')

        pmap(delete_machine, to_delete)

'''
'''--- pytest/tests/loadtest/loadtest.py ---
from tqdm import tqdm
import mocknet_helpers
import account
import key
import base64
import argparse
from os.path import join

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Setup loadtest')

    parser.add_argument('--home', type=str, required=True)
    parser.add_argument(
        '--sender_key_file',
        type=str,
        default="validator_key.json",
        help=
        "File in home directory that contains the key for the account that should be used to send all the requests."
    )
    parser.add_argument(
        '--num_accounts',
        type=int,
        default=5,
        help=
        "Accounts that contain the contract to run (either set the --num_accounts or comma separated list in --account_ids)"
    )
    parser.add_argument('--account_ids', type=str, default=None)

    parser.add_argument('--num_requests', type=int, default=50)
    parser.add_argument('--host', type=str, default='127.0.0.1')
    # Contract types:
    #  - storage - tries to do maximum amount of reads & writes (increments a large vector)
    #  - compute - tries to do maximum amount of compute (infinite loop)
    parser.add_argument('--contract_type',
                        type=str,
                        default='storage',
                        help="""# Contract types:
    #  - storage - tries to do maximum amount of reads & writes (increments a large vector)
    #  - compute - tries to do maximum amount of compute (infinite loop)
    #  - write - tries to write a lot of data to the contract state.""")
    args = parser.parse_args()

    accounts = [args.account_ids.split(",")] if args.account_ids else [
        f"shard{i}" for i in range(args.num_accounts)
    ]

    validator_key = key.Key.from_json_file(join(args.home,
                                                args.sender_key_file))

    base_block_hash = mocknet_helpers.get_latest_block_hash(addr=args.host)
    nonce = mocknet_helpers.get_nonce_for_key(validator_key, addr=args.host)

    my_account = account.Account(validator_key,
                                 init_nonce=nonce,
                                 base_block_hash=base_block_hash,
                                 rpc_infos=[(args.host, "3030")])

    # First - 'reset' the counters in the contract.
    for y in accounts:
        my_account.send_call_contract_raw_tx(
            contract_id=y,
            method_name="reset_increment_many",
            args=f'{{"how_many": 400}}'.encode("utf-8"),
            deposit=0)

    results = []

    contract_type = args.contract_type
    assert (contract_type in ['storage', 'compute', 'write'])

    if contract_type == "storage":
        method_name = "increment_many"
    if contract_type == "write":
        method_name = "write_many"
    if contract_type == "compute":
        method_name = "infinite_loop"

    for i in tqdm(range(args.num_requests)):
        for y in accounts:
            result = my_account.send_call_contract_raw_tx(
                contract_id=y,
                method_name=method_name,
                args=f'{{"how_many": {min(400 + i, 400)}}}'.encode("utf-8"),
                deposit=0)
            results.append(result)

    if contract_type == 'storage':
        # For 'storage' contracts - we can also check that all were executed successfully.
        for y in accounts:
            res = my_account.send_call_contract_raw_tx(
                contract_id=y,
                method_name="get_increment_many",
                args='',
                deposit=0)
            print(f"Shard {y} asking for result: {res}")
            result = mocknet_helpers.tx_result(res["result"],
                                               validator_key.account_id,
                                               addr=args.host,
                                               wait_for_success=True)
            outcome = base64.b64decode(result['status']['SuccessValue'])
            if int(outcome) == args.num_requests:
                print(f"Shard {y}: PASS")
            else:
                print(f"Shard {y} : FAIL {outcome} vs {args.num_requests}")

'''
'''--- pytest/tests/loadtest/loadtest2.py ---
#!/usr/bin/env python3
"""
This is a benchmark in which a network with a single fungible_token contract is
deployed, and then a variable number of users (see `N_ACCOUNTS`) send each other
those fungible tokens.

At the time this benchmark is written, the intent is to observe the node metrics
and traces for the block duration, potentially adding any additional
instrumentation as needed.

To run:

```
env NEAR_ROOT=../target/release/ \
    python3 tests/loadtest/loadtest2.py \
    --fungible-token-wasm=$PWD/../../FT/res/fungible_token.wasm \
    --setup-cluster --accounts=1000 --executors=4
```
"""

import argparse
import sys
import os
import time
import pathlib
import base58
import requests
import random
import logging
import json
import multiprocessing
import multiprocessing.queues
import ctypes
import ed25519
import queue
import string

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

import cluster
import utils
import account
import transaction
import key
import account
import mocknet_helpers
from configured_logger import new_logger

DEFAULT_TRANSACTION_TTL_SECONDS = 10
MAX_INFLIGHT_TRANSACTIONS_PER_EXECUTOR = 1000
SEED = random.uniform(0, 0xFFFFFFFF)
logger = new_logger(level=logging.INFO)

class Transaction:
    """
    A transaction future.
    """

    ID = 0

    def __init__(self):
        self.id = Transaction.ID
        Transaction.ID += 1

        # Number of times we are going to check this transaction for completion before retrying
        # submission
        self.ttl = 0
        self.expiration = 0
        # The transaction id hash
        #
        # str if the transaction has been submitted and may eventually conclude.
        self.transaction_id = None
        # The transaction caller (used for checking the transaction status.
        #
        # `str` if the transaction has been submitted and may eventually conclude.
        self.caller = None
        # The outcome of a successful transaction execution
        self.outcome = None

    def poll(self, node, block_hash):
        """
        Returns True if transaction has completed.
        """
        if self.is_complete():
            return True
        # Send the transaction if the previous expired or we didn't send one in the first place.
        if self.transaction_id is None or self.ttl <= 0:
            if self.transaction_id is not None:
                logger.warning(
                    f"transaction {self.transaction_id} expired, submitting a new one!"
                )
            (self.transaction_id, self.caller) = self.send(node, block_hash)
            self.expiration = time.time() + DEFAULT_TRANSACTION_TTL_SECONDS
            self.ttl = DEFAULT_TRANSACTION_TTL_SECONDS
            return False  # almost guaranteed to not produce any results right now.
        caller = ACCOUNTS[self.caller]
        logger.debug(
            f"checking {self.transaction_id} from {caller.key.account_id}")
        tx_result = node.json_rpc('tx',
                                  [self.transaction_id, caller.key.account_id])
        if self.is_success(tx_result):
            self.outcome = tx_result
            return True
        return False

    def send(self, block_hash):
        return (self.transaction_id, self.caller)

    def is_complete(self):
        return self.outcome is not None

    def is_success(self, tx_result):
        success = 'error' not in tx_result
        if not success:
            logger.debug(
                f"transaction {self.transaction_id} for {self.caller} is not successful: {tx_result}"
            )
        # only set TTL if we managed to check for success or failure...
        self.ttl = self.expiration - time.time()
        return success

class DeployFT(Transaction):

    def __init__(self, account, contract):
        super().__init__()
        self.account = account
        self.contract = contract

    def send(self, node, block_hash):
        account = ACCOUNTS[self.account]
        logger.warning(f"deploying FT to {account.key.account_id}")
        wasm_binary = utils.load_binary_file(self.contract)
        tx = transaction.sign_deploy_contract_tx(account.key, wasm_binary,
                                                 account.use_nonce(),
                                                 block_hash)
        result = node.send_tx(tx)
        return (result["result"], self.account)

class TransferFT(Transaction):

    def __init__(self, ft, sender, recipient, how_much=1, tgas=300):
        super().__init__()
        self.ft = ft
        self.sender = sender
        self.recipient = recipient
        self.how_much = how_much
        self.tgas = tgas

    def send(self, node, block_hash):
        (ft, sender, recipient
        ) = ACCOUNTS[self.ft], ACCOUNTS[self.sender], ACCOUNTS[self.recipient]
        logger.debug(
            f"sending {self.how_much} FT from {sender.key.account_id} to {recipient.key.account_id}"
        )
        args = {
            "receiver_id": recipient.key.account_id,
            "amount": str(int(self.how_much)),
        }
        tx = transaction.sign_function_call_tx(
            sender.key,
            ft.key.account_id,
            "ft_transfer",
            json.dumps(args).encode('utf-8'),
            # About enough gas per call to fit N such transactions into an average block.
            self.tgas * account.TGAS,
            # Gotta attach exactly 1 yoctoNEAR according to NEP-141 to avoid calls from restricted access keys
            1,
            sender.use_nonce(),
            block_hash)
        result = node.send_tx(tx)
        return (result["result"], self.sender)

class TransferNear(Transaction):

    def __init__(self, sender, recipient_id, how_much=2.0):
        super().__init__()
        self.recipient_id = recipient_id
        self.sender = sender
        self.how_much = how_much

    def send(self, node, block_hash):
        sender = ACCOUNTS[self.sender]
        logger.debug(
            f"sending {self.how_much} NEAR from {sender.key.account_id} to {self.recipient_id}"
        )
        tx = transaction.sign_payment_tx(sender.key, self.recipient_id,
                                         int(self.how_much * 1E24),
                                         sender.use_nonce(), block_hash)
        result = node.send_tx(tx)
        return (result["result"], self.sender)

class CreateSubAccount(Transaction):

    def __init__(self, sender, sub, balance=50.0):
        super().__init__()
        self.sender = sender
        self.sub = sub
        self.balance = balance

    def send(self, node, block_hash):
        sender = ACCOUNTS[self.sender]
        sub = ACCOUNTS[self.sub]
        new_account_id = f"{sub.key.account_id}.{sender.key.account_id}"
        logger.debug(f"creating {new_account_id}")
        tx = transaction.sign_create_account_with_full_access_key_and_balance_tx(
            sender.key, sub.key.account_id, sub.key, int(self.balance * 1E24),
            sender.use_nonce(), block_hash)
        result = node.send_tx(tx)
        return (result["result"], self.sender)

class InitFT(Transaction):

    def __init__(self, contract):
        super().__init__()
        self.contract = contract

    def send(self, node, block_hash):
        contract = ACCOUNTS[self.contract]
        args = json.dumps({
            "owner_id": contract.key.account_id,
            "total_supply": str(10**33)
        })
        tx = transaction.sign_function_call_tx(contract.key,
                                               contract.key.account_id,
                                               "new_default_meta",
                                               args.encode('utf-8'), int(3E14),
                                               0, contract.use_nonce(),
                                               block_hash)
        result = node.send_tx(tx)
        return (result["result"], self.contract)

class InitFTAccount(Transaction):

    def __init__(self, contract, account):
        super().__init__()
        self.contract = contract
        self.account = account

    def send(self, node, block_hash):
        contract, account = ACCOUNTS[self.contract], ACCOUNTS[self.account]
        args = json.dumps({"account_id": account.key.account_id})
        tx = transaction.sign_function_call_tx(contract.key,
                                               contract.key.account_id,
                                               "storage_deposit",
                                               args.encode('utf-8'), int(3E14),
                                               int(1E23), contract.use_nonce(),
                                               block_hash)
        result = node.send_tx(tx)
        return (result["result"], self.contract)

class TxQueue(multiprocessing.queues.Queue):

    def __init__(self, size, *args, **kwargs):
        super().__init__(size,
                         ctx=multiprocessing.get_context(),
                         *args,
                         **kwargs)
        self.pending = multiprocessing.Value(ctypes.c_ulong, 0)

    def add(self, tx):
        with self.pending.get_lock():
            self.pending.value += 1
        self.put(tx)

    def complete(self):
        with self.pending.get_lock():
            self.pending.value -= 1

class Account:

    def __init__(self, key):
        self.key = key
        self.nonce = multiprocessing.Value(ctypes.c_ulong, 0)

    def refresh_nonce(self, node):
        with self.nonce.get_lock():
            self.nonce.value = mocknet_helpers.get_nonce_for_key(
                self.key,
                addr=node.rpc_addr()[0],
                port=node.rpc_addr()[1],
            )

    def use_nonce(self):
        with self.nonce.get_lock():
            new_nonce = self.nonce.value + 1
            self.nonce.value = new_nonce
            return new_nonce

def transaction_executor(nodes, tx_queue, accounts):
    global ACCOUNTS
    ACCOUNTS = accounts
    last_block_hash_update = 0
    my_transactions = queue.SimpleQueue()
    rng = random.Random()
    while True:
        node = rng.choice(nodes)
        try:
            now = time.time()
            if now - last_block_hash_update >= 0.5:
                block_hash = base58.b58decode(node.get_latest_block().hash)
                last_block_hash_update = now
        except (requests.exceptions.ReadTimeout,
                requests.exceptions.ConnectionError):
            continue

        while my_transactions.qsize() < MAX_INFLIGHT_TRANSACTIONS_PER_EXECUTOR:
            try:
                tx = tx_queue.get_nowait()
            except queue.Empty:
                break
            # Send out the transaction immediately.
            try:
                tx.poll(node, block_hash)
            except (requests.exceptions.ReadTimeout,
                    requests.exceptions.ConnectionError):
                pass
            my_transactions.put(tx)

        try:
            tx = my_transactions.get_nowait()
        except queue.Empty:
            time.sleep(0.1)
            continue
        try:
            poll_result = tx.poll(node, block_hash)
        except (requests.exceptions.ReadTimeout,
                requests.exceptions.ConnectionError):
            my_transactions.put(tx)
            continue
        if not poll_result:
            my_transactions.put(tx)
            if tx.ttl != DEFAULT_TRANSACTION_TTL_SECONDS:
                time.sleep(0.1)  # don't spam RPC too hard...
        else:
            tx_queue.complete()

def main():
    parser = argparse.ArgumentParser(description='FT transfer benchmark.')
    parser.add_argument('--fungible-token-wasm',
                        required=True,
                        help='Path to the compiled Fungible Token contract')
    parser.add_argument(
        '--setup-cluster',
        default=False,
        help=
        'Whether to start a dedicated cluster instead of connecting to an existing local node',
        action='store_true')
    parser.add_argument(
        '--contracts',
        default='0,2,4,6,8,a,c,e',
        help=
        'Number of contract accounts, or alternatively list of subnames, separated by commas'
    )
    parser.add_argument(
        '--contract-key',
        required='--setup-cluster' not in sys.argv,
        help=
        'account to deploy contract to and use as source of NEAR for account creation'
    )
    parser.add_argument('--accounts',
                        default=1000,
                        help='Number of accounts to use')
    parser.add_argument(
        '--no-account-topup',
        default=False,
        action='store_true',
        help='Fill accounts with additional NEAR prior to testing')
    parser.add_argument('--shards', default=10, help='number of shards')
    parser.add_argument('--executors',
                        default=2,
                        help='number of transaction executors')
    parser.add_argument('--tx-tgas',
                        default=30,
                        help='amount of Tgas to attach to each transaction')
    args = parser.parse_args()

    logger.warning(f"SEED is {SEED}")
    rng = random.Random(SEED)

    if args.setup_cluster:
        config = cluster.load_config()
        nodes = cluster.start_cluster(
            2, 0, args.shards, config, [["epoch_length", 100]], {
                shard: {
                    "tracked_shards": list(range(args.shards))
                } for shard in range(args.shards + 1)
            })
        if args.contract_key is None:
            signer_key = nodes[0].signer_key
        else:
            signer_key = key.Key.from_json_file(args.contract_key)

    else:
        nodes = [
            cluster.RpcNode("127.0.0.1", 3030),
        ]
        # The `nearup` localnet setup stores the keys in this directory.
        key_path = args.contract_key
        signer_key = key.Key.from_json_file(key_path)

    ACCOUNTS = []
    ACCOUNTS.append(Account(signer_key))
    ACCOUNTS[0].refresh_nonce(nodes[0])
    funding_account = 0
    start_of_accounts = len(ACCOUNTS) - 1
    contract_accounts = []

    try:
        for sub in sorted(
                rng.sample(string.ascii_lowercase + string.digits,
                           k=int(args.contracts))):
            funding_key = ACCOUNTS[funding_account].key
            sub_key = key.Key(f"{sub}.{funding_key.account_id}", funding_key.pk,
                              funding_key.sk)
            contract_accounts.append(len(ACCOUNTS))
            ACCOUNTS.append(Account(sub_key))
    except ValueError:
        for sub in args.contracts.split(","):
            funding_key = ACCOUNTS[funding_account].key
            sub_key = key.Key(f"{sub}.{funding_key.account_id}", funding_key.pk,
                              funding_key.sk)
            contract_accounts.append(len(ACCOUNTS))
            ACCOUNTS.append(Account(sub_key))

    for i in range(int(args.accounts)):
        keys = ed25519.create_keypair(entropy=rng.randbytes)
        account_id = keys[1].to_bytes().hex()
        sk = 'ed25519:' + base58.b58encode(keys[0].to_bytes()).decode('ascii')
        pk = 'ed25519:' + base58.b58encode(keys[1].to_bytes()).decode('ascii')
        ACCOUNTS.append(Account(key.Key(account_id, pk, sk)))

    executors = int(args.executors)
    queue_size = 16 + max(MAX_INFLIGHT_TRANSACTIONS_PER_EXECUTOR,
                          int(args.accounts) * len(contract_accounts))
    tx_queue = TxQueue(queue_size)
    subargs = (
        nodes,
        tx_queue,
        ACCOUNTS,
    )
    for executor in range(executors):
        multiprocessing.Process(target=transaction_executor,
                                args=subargs,
                                daemon=True).start()

    for contract_account in contract_accounts:
        tx_queue.add(CreateSubAccount(funding_account, contract_account))
    wait_empty(tx_queue, "creating contract sub accounts")
    for contract_account in contract_accounts:
        ACCOUNTS[contract_account].refresh_nonce(nodes[0])
        tx_queue.add(DeployFT(contract_account, args.fungible_token_wasm))
    wait_empty(tx_queue, "deployment")
    for contract_account in contract_accounts:
        tx_queue.add(InitFT(contract_account))
    wait_empty(tx_queue, "contract initialization")

    if not args.no_account_topup:
        for test_account in ACCOUNTS[start_of_accounts:]:
            tx_queue.add(
                TransferNear(funding_account, test_account.key.account_id, 2.0))
        wait_empty(tx_queue, "account creation and top-up")

    for contract_account in contract_accounts:
        for test_account_idx in range(start_of_accounts, len(ACCOUNTS)):
            # Initialize nonces for all real accounts. But we only want to do that for the first
            # iteration... Otherwise there's a risk of a race. And O(n^2) doesn't help...
            if contract_account == contract_accounts[0]:
                ACCOUNTS[test_account_idx].refresh_nonce(nodes[0])
            tx_queue.add(InitFTAccount(contract_account, test_account_idx))
    wait_empty(tx_queue, "registeration of accounts with the FT contracts")

    for contract_account in contract_accounts:
        for test_account_idx in range(start_of_accounts, len(ACCOUNTS)):
            tx_queue.add(
                TransferFT(contract_account,
                           contract_account,
                           test_account_idx,
                           how_much=1E8))
    wait_empty(tx_queue, "distribution of initial FT")

    transfers = 0
    while True:
        sender_idx, receiver_idx = rng.sample(range(start_of_accounts,
                                                    len(ACCOUNTS)),
                                              k=2)
        ft_contract = rng.choice(contract_accounts)
        tgas = int(args.tx_tgas)
        tx_queue.add(
            TransferFT(ft_contract,
                       sender_idx,
                       receiver_idx,
                       how_much=1,
                       tgas=tgas))
        transfers += 1
        if transfers % 10000 == 0:
            logger.info(
                f"{transfers} so far ({tx_queue.pending.value} pending)")
        while tx_queue.pending.value >= MAX_INFLIGHT_TRANSACTIONS_PER_EXECUTOR * executors:
            time.sleep(0.25)

def wait_empty(queue, why):
    with queue.pending.get_lock():
        remaining = queue.pending.value
    while remaining != 0:
        logger.info(f"waiting for {why} ({remaining} remain)")
        time.sleep(0.5)
        with queue.pending.get_lock():
            remaining = queue.pending.value
    logger.info(f"wait for {why} completed!")

if __name__ == "__main__":
    main()

'''
'''--- pytest/tests/loadtest/locust/common/base.py ---
from datetime import datetime, timedelta
from locust import User, events, runners
from retrying import retry
import abc
import base64
import json
import base58
import ctypes
import logging
import multiprocessing
import pathlib
import requests
import sys
import threading
import time
import typing
import unittest

sys.path.append(str(pathlib.Path(__file__).resolve().parents[4] / 'lib'))

from account import TGAS
from common.sharding import AccountGenerator
from configured_logger import new_logger
import cluster
import key
import mocknet_helpers
import transaction
import utils

DEFAULT_TRANSACTION_TTL = timedelta(minutes=30)
logger = new_logger(level=logging.WARN)

def is_key_error(exception):
    return isinstance(exception, KeyError)

def is_tx_unknown_error(exception):
    return isinstance(exception, TxUnknownError)

class Account:

    def __init__(self, key):
        self.key = key
        self.current_nonce = multiprocessing.Value(ctypes.c_ulong, 0)

    # Race condition: maybe the account was created but the RPC interface
    # doesn't display it yet, which returns an empty result and raises a
    # `KeyError`.
    # (not quite sure how this happens but it did happen to me on localnet)
    @retry(wait_exponential_multiplier=500,
           wait_exponential_max=10000,
           stop_max_attempt_number=5,
           retry_on_exception=is_key_error)
    def refresh_nonce(self, node):
        with self.current_nonce.get_lock():
            self.current_nonce.value = mocknet_helpers.get_nonce_for_key(
                self.key,
                addr=node.rpc_addr()[0],
                port=node.rpc_addr()[1],
                logger=logger,
            )

    def use_nonce(self):
        with self.current_nonce.get_lock():
            new_nonce = self.current_nonce.value + 1
            self.current_nonce.value = new_nonce
            return new_nonce

    def fast_forward_nonce(self, ak_nonce):
        with self.current_nonce.get_lock():
            self.current_nonce.value = max(self.current_nonce.value,
                                           ak_nonce + 1)

class Transaction:
    """
    A transaction future.
    """

    ID = 0

    def __init__(self):
        self.id = Transaction.ID
        Transaction.ID += 1

        # The transaction id hash
        #
        # str if the transaction has been submitted and may eventually conclude.
        # FIXME: this is currently not set in some cases
        self.transaction_id = None

    @abc.abstractmethod
    def sign_and_serialize(self, block_hash) -> bytes:
        """
        Each transaction class is supposed to define this method to serialize and
        sign the transaction and return the raw message to be sent.
        """

    @abc.abstractmethod
    def sender_account(self) -> Account:
        """
        Account of the sender that signs the tx, which must be known to map
        the tx-result request to the right shard.
        """

class FunctionCall(Transaction):

    def __init__(self,
                 sender: Account,
                 receiver_id: str,
                 method: str,
                 balance: int = 0):
        super().__init__()
        self.sender = sender
        self.receiver_id = receiver_id
        self.method = method
        # defensive cast to avoid serialization bugs when float balance is
        # provided despite type hint
        self.balance = int(balance)

    @abc.abstractmethod
    def args(self) -> typing.Union[dict, typing.List[dict]]:
        """
        Function call arguments to be serialized and sent with the call.
        Return a single dict for `FunctionCall` but a list of dict for `MultiFunctionCall`.
        """

    def sign_and_serialize(self, block_hash) -> bytes:
        return transaction.sign_function_call_tx(
            self.sender.key, self.receiver_id, self.method,
            json.dumps(self.args()).encode('utf-8'), 300 * TGAS, self.balance,
            self.sender.use_nonce(), block_hash)

    def sender_account(self) -> Account:
        return self.sender

class MultiFunctionCall(FunctionCall):
    """
    Batches multiple function calls into a single transaction.
    """

    def __init__(self,
                 sender: Account,
                 receiver_id: str,
                 method: str,
                 balance: int = 0):
        super().__init__(sender, receiver_id, method, balance=balance)

    def sign_and_serialize(self, block_hash) -> bytes:
        all_args = self.args()
        gas = 300 * TGAS // len(all_args)

        def create_action(args):
            return transaction.create_function_call_action(
                self.method,
                json.dumps(args).encode('utf-8'), gas, int(self.balance))

        actions = [create_action(args) for args in all_args]
        return transaction.sign_and_serialize_transaction(
            self.receiver_id, self.sender.use_nonce(), actions, block_hash,
            self.sender.key.account_id, self.sender.key.decoded_pk(),
            self.sender.key.decoded_sk())

class Deploy(Transaction):

    def __init__(self, account, contract, name):
        super().__init__()
        self.account = account
        self.contract = contract
        self.name = name

    def sign_and_serialize(self, block_hash) -> bytes:
        account = self.account
        logger.info(f"deploying {self.name} to {account.key.account_id}")
        wasm_binary = utils.load_binary_file(self.contract)
        return transaction.sign_deploy_contract_tx(account.key, wasm_binary,
                                                   account.use_nonce(),
                                                   block_hash)

    def sender_account(self) -> Account:
        return self.account

class CreateSubAccount(Transaction):

    def __init__(self, sender, sub_key, balance: float = 50):
        super().__init__()
        self.sender = sender
        self.sub_key = sub_key
        self.balance = balance

    def sign_and_serialize(self, block_hash) -> bytes:
        sender = self.sender
        sub = self.sub_key
        logger.debug(f"creating {sub.account_id}")
        return transaction.sign_create_account_with_full_access_key_and_balance_tx(
            sender.key, sub.account_id, sub, int(self.balance * 1E24),
            sender.use_nonce(), block_hash)

    def sender_account(self) -> Account:
        return self.sender

class AddFullAccessKey(Transaction):

    def __init__(self, parent: Account, new_key: key.Key):
        super().__init__()
        self.sender = parent
        self.new_key = new_key

    def sign_and_serialize(self, block_hash) -> bytes:
        action = transaction.create_full_access_key_action(
            self.new_key.decoded_pk())
        return transaction.sign_and_serialize_transaction(
            self.sender.key.account_id, self.sender.use_nonce(),
            [action], block_hash, self.sender.key.account_id,
            self.sender.key.decoded_pk(), self.sender.key.decoded_sk())

    def sender_account(self) -> Account:
        return self.sender

class NearNodeProxy:
    """
    Wrapper around a RPC node connection that tracks requests on locust.
    """

    def __init__(self, environment):
        self.request_event = environment.events.request
        [url, port] = environment.host.rsplit(":", 1)
        self.node = cluster.RpcNode(url, port)
        self.session = requests.Session()

    def send_tx_retry(self, tx: Transaction, locust_name) -> dict:
        """
        Send a transaction and retry until it succeeds
        
        This method retries no matter the kind of error, but it tries to be
        smart about what to do depending on the error.
        
        Expected error: UnknownTransactionError means TX has not been executed yet.
        Expected error: InvalidNonceError means we are using an outdated nonce.
        Other errors: Probably bugs in the test setup (e.g. invalid signer).
        """
        while True:
            meta = self.send_tx(tx, locust_name)
            error = meta["exception"]
            if error is None:
                return meta
            elif isinstance(error, InvalidNonceError):
                logger.debug(
                    f"{error} for {tx.sender_account().key.account_id}, updating nonce and retrying"
                )
                tx.sender_account().fast_forward_nonce(error.ak_nonce)
            else:
                logger.warn(
                    f"transaction {tx.transaction_id} failed: {error}, retrying in 0.25s"
                )
                time.sleep(0.25)

    def send_tx(self, tx: Transaction, locust_name: str) -> dict:
        """
        Send a transaction and return the result, no retry attempted.
        """
        block_hash = self.final_block_hash()
        signed_tx = tx.sign_and_serialize(block_hash)

        meta = self.new_locust_metadata(locust_name)
        start_perf_counter = time.perf_counter()

        try:
            try:
                # To get proper errors on invalid transaction, we need to use sync api first
                result = self.post_json(
                    "broadcast_tx_commit",
                    [base64.b64encode(signed_tx).decode('utf8')])
                evaluate_rpc_result(result.json())
            except TxUnknownError as err:
                # This means we time out in one way or another.
                # In that case, the stateless transaction validation was
                # successful, we can now use async API without missing errors.
                submit_raw_response = self.post_json(
                    "broadcast_tx_async",
                    [base64.b64encode(signed_tx).decode('utf8')])
                meta["response_length"] = len(submit_raw_response.text)
                submit_response = submit_raw_response.json()
                # extract transaction ID from response, it should be "{ "result": "id...." }"
                if not "result" in submit_response:
                    meta["exception"] = RpcError(message="Didn't get a TX ID",
                                                 details=submit_response)
                    meta["response"] = submit_response.content
                else:
                    tx.transaction_id = submit_response["result"]
                    # using retrying lib here to poll until a response is ready
                    self.poll_tx_result(meta, tx)
        except NearError as err:
            logging.warn(f"marking an error {err.message}, {err.details}")
            meta["exception"] = err

        meta["response_time"] = (time.perf_counter() -
                                 start_perf_counter) * 1000

        # Track request + response in Locust
        self.request_event.fire(**meta)
        return meta

    def final_block_hash(self):
        return base58.b58decode(
            self.node.get_final_block()['result']['header']['hash'])

    def new_locust_metadata(self, locust_name: str):
        return {
            "request_type": "near-rpc",
            "name": locust_name,
            "start_time": time.time(),
            "response_time": 0,  # overwritten later  with end-to-end time
            "response_length": 0,  # overwritten later
            "response": None,  # overwritten later
            "context": {},  # not used  right now
            "exception": None,  # maybe overwritten later
        }

    def post_json(self, method: str, params: typing.List[str]):
        j = {
            "method": method,
            "params": params,
            "id": "dontcare",
            "jsonrpc": "2.0"
        }
        return self.session.post(url="http://%s:%s" % self.node.rpc_addr(),
                                 json=j)

    @retry(wait_fixed=500,
           stop_max_delay=DEFAULT_TRANSACTION_TTL / timedelta(milliseconds=1),
           retry_on_exception=is_tx_unknown_error)
    def poll_tx_result(self, meta: dict, tx: Transaction):
        params = [tx.transaction_id, tx.sender_account().key.account_id]
        # poll for tx result, using "EXPERIMENTAL_tx_status" which waits for
        # all receipts to finish rather than just the first one, as "tx" would do
        result_response = self.post_json("EXPERIMENTAL_tx_status", params)
        # very verbose, but very useful to see what's happening when things are stuck
        logger.debug(
            f"polling, got: {result_response.status_code} {result_response.json()}"
        )

        try:
            meta["response"] = evaluate_rpc_result(result_response.json())
        except:
            # Store raw response to improve error-reporting.
            meta["response"] = result_response.content
            raise

    def account_exists(self, account_id: str) -> bool:
        return "error" not in self.node.get_account(account_id, do_assert=False)

    def prepare_account(self, account: Account, parent: Account, balance: float,
                        msg: str) -> bool:
        """
        Creates the account if it doesn't exist and refreshes the nonce.
        """
        exists = self.account_exists(account.key.account_id)
        if not exists:
            self.send_tx_retry(
                CreateSubAccount(parent, account.key, balance=balance), msg)
        account.refresh_nonce(self.node)
        return exists

    def prepare_accounts(self,
                         accounts: typing.List[Account],
                         parent: Account,
                         balance: float,
                         msg: str,
                         timeout: timedelta = timedelta(minutes=3)):
        """
        Creates accounts if they don't exist and refreshes their nonce.
        Accounts must share the parent account.
        
        This implementation attempts on-chain parallelization, hence it should
        be faster than calling `prepare_account` in a loop.
        
        Note that error-handling in this variant isn't quite as smooth. Errors
        that are only reported by the sync API of RPC nodes will not be caught
        here. Instead, we do a best-effort retry and stop after a fixed timeout.
        """

        # To avoid blocking, each account goes though a FSM independently.
        #
        # FSM outline:
        #
        #    [INIT] -----------------(account exists already)-------------
        #       |                                                        |
        #       V                                                        V
        #  [TO_CREATE] --(post tx)--> [INFLIGHT] --(poll result)--> [TO_REFRESH]
        #   ^                    |                                       |
        #   |                    |                                       |
        #   |--(fail to submit)<--                                   (refresh)
        #                                                                |
        #                                                                V
        #                                                              [DONE]
        #
        to_create: typing.List[Account] = []
        inflight: typing.List[Transaction, dict, Account] = []
        to_refresh: typing.List[Account] = []

        for account in accounts:
            if self.account_exists(account.key.account_id):
                to_refresh.append(account)
            else:
                to_create.append(account)

        block_hash = self.final_block_hash()
        start = datetime.now()
        while len(to_create) + len(to_refresh) + len(inflight) > 0:
            logger.info(
                f"preparing {len(accounts)} accounts, {len(to_create)} to create, {len(to_refresh)} to refresh, {len(inflight)} inflight"
            )
            if start + timeout < datetime.now():
                raise SystemExit("Account preparation timed out")
            try_again = []
            for account in to_create:
                meta = self.new_locust_metadata(msg)
                tx = CreateSubAccount(parent, account.key, balance=balance)
                signed_tx = tx.sign_and_serialize(block_hash)
                submit_raw_response = self.post_json(
                    "broadcast_tx_async",
                    [base64.b64encode(signed_tx).decode('utf8')])
                meta["response_length"] = len(submit_raw_response.text)
                submit_response = submit_raw_response.json()
                if not "result" in submit_response:
                    # something failed, let's not block, just try again later
                    logger.debug(
                        f"couldn't submit account creation TX, got {submit_raw_response.text}"
                    )
                    try_again.append(account)
                else:
                    tx.transaction_id = submit_response["result"]
                    inflight.append((tx, meta, account))
            to_create = try_again

            # while requests are processed on-chain, refresh nonces for existing accounts
            for account in to_refresh:
                account.refresh_nonce(self.node)
            to_refresh.clear()

            # poll all pending requests
            for tx, meta, account in inflight:
                # Using retrying lib here to poll until a response is ready.
                # This is blocking on a single request, but we expect requests
                # to be processed in order so it shouldn't matter.
                self.poll_tx_result(meta, tx)
                meta["response_time"] = (time.time() -
                                         meta["start_time"]) * 1000
                to_refresh.append(account)
                # Locust tracking
                self.request_event.fire(**meta)
            inflight.clear()

        logger.info(f"done preparing {len(accounts)} accounts")

class NearUser(User):
    abstract = True
    id_counter = 0
    INIT_BALANCE = 100.0
    funding_account: Account

    @classmethod
    def get_next_id(cls):
        cls.id_counter += 1
        return cls.id_counter

    @classmethod
    def generate_account_id(cls, account_generator, id) -> str:
        return account_generator.random_account_id(
            cls.funding_account.key.account_id, f'_user{id}')

    def __init__(self, environment):
        super().__init__(environment)
        assert self.host is not None, "Near user requires the RPC node address"
        self.node = NearNodeProxy(environment)
        self.id = NearUser.get_next_id()
        user_suffix = f"{self.id}_run{environment.parsed_options.run_id}"
        self.account_id = NearUser.generate_account_id(
            environment.account_generator, user_suffix)

    def on_start(self):
        """
        Called once per user, creating the account on chain
        """
        self.account = Account(key.Key.from_random(self.account_id))
        if not self.node.account_exists(self.account_id):
            self.send_tx_retry(
                CreateSubAccount(NearUser.funding_account,
                                 self.account.key,
                                 balance=NearUser.INIT_BALANCE))
        self.account.refresh_nonce(self.node.node)

    def send_tx(self, tx: Transaction, locust_name="generic send_tx"):
        """
        Send a transaction and return the result, no retry attempted.
        """
        return self.node.send_tx(tx, locust_name)["response"]

    def send_tx_retry(self,
                      tx: Transaction,
                      locust_name="generic send_tx_retry"):
        """
        Send a transaction and retry until it succeeds
        """
        return self.node.send_tx_retry(tx, locust_name=locust_name)["response"]

class NearError(Exception):

    def __init__(self, message, details):
        """
        The `message` is used in locust to aggregate errors and is also displayed in the UI.
        The `details` are logged as additional information in the console.
        """
        self.message = message
        self.details = details
        super().__init__(message)

class RpcError(NearError):

    def __init__(self, message="RPC returned an error", details=None):
        super().__init__(message, details)

class TxUnknownError(RpcError):

    def __init__(
        self,
        message="RPC does not know the result of this TX, probably it is not executed yet"
    ):
        super().__init__(message=message)

class InvalidNonceError(RpcError):

    def __init__(
        self,
        used_nonce,
        ak_nonce,
    ):
        super().__init__(
            message="Nonce too small",
            details=
            f"Tried to use nonce {used_nonce} but access key nonce is {ak_nonce}"
        )
        self.ak_nonce = ak_nonce

class TxError(NearError):

    def __init__(self,
                 status,
                 message="Transaction to receipt conversion failed"):
        super().__init__(message, status)

class ReceiptError(NearError):

    def __init__(self, status, receipt_id, message="Receipt execution failed"):
        super().__init__(message, f"id={receipt_id} {status}")

class SmartContractPanic(ReceiptError):

    def __init__(self, status, receipt_id, message="Smart contract panicked"):
        super().__init__(status, receipt_id, message)

class FunctionExecutionError(ReceiptError):

    def __init__(self,
                 status,
                 receipt_id,
                 message="Smart contract function execution failed"):
        super().__init__(status, receipt_id, message)

def evaluate_rpc_result(rpc_result):
    """
    Take the json RPC response and translate it into success
    and failure cases. Failures are raised as exceptions.
    """
    if "error" in rpc_result:
        err_name = rpc_result["error"]["cause"]["name"]
        # The sync API returns "UNKNOWN_TRANSACTION" after a timeout.
        # The async API returns "TIMEOUT_ERROR" if the tx was not accepted in the chain after 10s.
        # In either case, the identical transaction should be retried.
        if err_name in ["UNKNOWN_TRANSACTION", "TIMEOUT_ERROR"]:
            raise TxUnknownError(err_name)
        # When reusing keys across test runs, the nonce is higher than expected.
        elif err_name == "INVALID_TRANSACTION":
            err_description = rpc_result["error"]["data"]["TxExecutionError"][
                "InvalidTxError"]
            if "InvalidNonce" in err_description:
                raise InvalidNonceError(
                    err_description["InvalidNonce"]["tx_nonce"],
                    err_description["InvalidNonce"]["ak_nonce"])
        raise RpcError(details=rpc_result["error"])

    result = rpc_result["result"]
    transaction_outcome = result["transaction_outcome"]
    if not "SuccessReceiptId" in transaction_outcome["outcome"]["status"]:
        raise TxError(transaction_outcome["outcome"]["status"])

    receipt_outcomes = result["receipts_outcome"]
    for receipt in receipt_outcomes:
        # For each receipt, we get
        # `{ "outcome": { ..., "status": { <ExecutionStatusView>: "..." } } }`
        # and the key for `ExecutionStatusView` tells us whether it was successful
        status = list(receipt["outcome"]["status"].keys())[0]

        if status == "Unknown":
            raise ReceiptError(receipt["outcome"],
                               receipt["id"],
                               message="Unknown receipt result")
        if status == "Failure":
            failure = receipt["outcome"]["status"]["Failure"]
            panic_msg = as_smart_contract_panic_message(failure)
            if panic_msg:
                raise SmartContractPanic(receipt["outcome"],
                                         receipt["id"],
                                         message=panic_msg)
            exec_failed = as_execution_error(failure)
            if exec_failed:
                raise FunctionExecutionError(receipt["outcome"],
                                             receipt["id"],
                                             message=exec_failed)
            raise ReceiptError(receipt["outcome"], receipt["id"])
        if not status in ["SuccessReceiptId", "SuccessValue"]:
            raise ReceiptError(receipt["outcome"],
                               receipt["id"],
                               message="Unexpected status")

    return result

def as_action_error(failure: dict) -> typing.Optional[dict]:
    return failure.get("ActionError", None)

def as_function_call_error(failure: dict) -> typing.Optional[dict]:
    action_error = as_action_error(failure)
    if action_error and "FunctionCallError" in action_error["kind"]:
        return action_error["kind"]["FunctionCallError"]
    return None

def as_execution_error(failure: dict) -> typing.Optional[dict]:
    function_call_error = as_function_call_error(failure)
    if function_call_error and "ExecutionError" in function_call_error:
        return function_call_error["ExecutionError"]
    return None

def as_smart_contract_panic_message(failure: dict) -> typing.Optional[str]:
    execution_error = as_execution_error(failure)
    known_prefix = "Smart contract panicked: "
    if execution_error and execution_error.startswith(known_prefix):
        return execution_error[len(known_prefix):]
    return None

def init_account_generator(parsed_options):
    if parsed_options.shard_layout_file is not None:
        with open(parsed_options.shard_layout_file, 'r') as f:
            shard_layout = json.load(f)
    elif parsed_options.shard_layout_chain_id is not None:
        if parsed_options.shard_layout_chain_id not in ['mainnet', 'testnet']:
            sys.exit(
                f'unexpected --shard-layout-chain-id: {parsed_options.shard_layout_chain_id}'
            )

        shard_layout = {
            "V1": {
                "fixed_shards": [],
                "boundary_accounts": [
                    "aurora", "aurora-0", "kkuuue2akv_1630967379.near"
                ],
                "shards_split_map": [[0, 1, 2, 3]],
                "to_parent_shard_map": [0, 0, 0, 0],
                "version": 1
            }
        }
    else:
        shard_layout = {
            "V0": {
                "num_shards": 1,
                "version": 0,
            },
        }

    return AccountGenerator(shard_layout)

# called once per process before user initialization
def do_on_locust_init(environment):
    node = NearNodeProxy(environment)

    master_funding_key = key.Key.from_json_file(
        environment.parsed_options.funding_key)
    master_funding_account = Account(master_funding_key)

    if not node.account_exists(master_funding_account.key.account_id):
        raise SystemExit(
            f"account {master_funding_account.key.account_id} of the provided master funding key does not exist"
        )
    master_funding_account.refresh_nonce(node.node)

    environment.account_generator = init_account_generator(
        environment.parsed_options)
    funding_account = None
    # every worker needs a funding account to create its users, eagerly create them in the master
    if isinstance(environment.runner, runners.MasterRunner):
        num_funding_accounts = environment.parsed_options.max_workers
        funding_balance = 10000 * NearUser.INIT_BALANCE

        def create_account(id):
            account_id = f"funds_worker_{id}.{master_funding_account.key.account_id}"
            return Account(key.Key.from_seed_testonly(account_id))

        funding_accounts = [
            create_account(id) for id in range(num_funding_accounts)
        ]
        node.prepare_accounts(funding_accounts, master_funding_account,
                              funding_balance, "create funding account")
        funding_account = master_funding_account
    elif isinstance(environment.runner, runners.WorkerRunner):
        worker_id = environment.runner.worker_index
        worker_account_id = f"funds_worker_{worker_id}.{master_funding_account.key.account_id}"
        worker_key = key.Key.from_seed_testonly(worker_account_id)
        funding_account = Account(worker_key)
        funding_account.refresh_nonce(node.node)
    elif isinstance(environment.runner, runners.LocalRunner):
        funding_account = master_funding_account
    else:
        raise SystemExit(
            f"unexpected runner class {environment.runner.__class__.__name__}")

    NearUser.funding_account = funding_account
    environment.master_funding_account = master_funding_account

INIT_DONE = threading.Event()

@events.init.add_listener
def on_locust_init(environment, **kwargs):
    do_on_locust_init(environment)
    INIT_DONE.set()

# Add custom CLI args here, will be available in `environment.parsed_options`
@events.init_command_line_parser.add_listener
def _(parser):
    parser.add_argument(
        "--funding-key",
        required=True,
        help="account to use as source of NEAR for account creation")
    parser.add_argument(
        "--max-workers",
        type=int,
        required=False,
        default=16,
        help="How many funding accounts to generate for workers")
    parser.add_argument(
        "--shard-layout-file",
        required=False,
        help="file containing a shard layout JSON object for the target chain")
    parser.add_argument(
        "--shard-layout-chain-id",
        required=False,
        help=
        "chain ID whose shard layout we should consult when generating account IDs. Convenience option to avoid using --shard-layout-file for mainnet and testnet"
    )
    parser.add_argument(
        "--run-id",
        default="",
        help="Unique index to append to static account ids. "
        "Change between runs if you need a new state. Keep at default if you want to reuse the old state"
    )

class TestEvaluateRpcResult(unittest.TestCase):

    def test_smart_contract_panic(self):
        input = """{
          "result": {
            "transaction_outcome": { "outcome": {"status": { "SuccessReceiptId": "" } } },
            "receipts_outcome": [ {
              "id": "J3EVpgJXgLQ5f33ammArtewYBAg3KmDgVf47HtapBtua",
              "outcome": {
                "logs": [],
                "receipt_ids": [
                  "HxL55zV91tEgpPKg8QPkoWo53Ue1x9yhfRQTgdfQ11mc",
                  "467VVuaNz9igj74Zs9wFpeYmtfpRorbZugJKDHymCN1Q"
                ],
                "gas_burnt": 2658479078129,
                "tokens_burnt": "265847907812900000000",
                "executor_id": "vy0zxd_ft.funds_worker_3.node0",
                "status": {
                  "Failure": {
                    "ActionError": {
                      "index": 0,
                      "kind": {
                        "FunctionCallError": {
                          "ExecutionError": "Smart contract panicked: The account doesnt have enough balance"
                        }
                      }
                    }
                  }
                },
                "metadata": {
                  "version": 3,
                  "gas_profile": []
                }
              }
            } ]
          }
        }"""
        self.assertRaises(SmartContractPanic, evaluate_rpc_result,
                          json.loads(input))

'''
'''--- pytest/tests/loadtest/locust/common/congestion.py ---
import json
import pathlib
import sys

from locust import events, runners

sys.path.append(str(pathlib.Path(__file__).resolve().parents[4] / "lib"))

import account
import common.base as base
import key
import transaction

class ComputeSha256(base.FunctionCall):
    """Transaction with a large input size."""

    def __init__(
        self,
        contract_account_id: str,
        sender: base.Account,
        size_bytes: int,
    ):
        super().__init__(sender, contract_account_id, "ext_sha256")
        self.contract_account_id = contract_account_id
        self.sender = sender
        self.size_bytes = size_bytes

    def args(self) -> dict:
        return ["a" * self.size_bytes]

    def sender_account(self) -> base.Account:
        return self.sender

class ComputeSum(base.Transaction):
    """Large computation that consumes a specified amount of gas."""

    def __init__(
        self,
        contract_account_id: str,
        sender: base.Account,
        usage_tgas: int,
    ):
        super().__init__()
        self.contract_account_id = contract_account_id
        self.sender = sender
        self.usage_tgas = usage_tgas

    def sign_and_serialize(self, block_hash) -> bytes:
        return transaction.sign_function_call_tx(
            self.sender.key,
            self.contract_account_id,
            "sum_n",
            # 1000000 is around 12 TGas.
            ((1000000 * self.usage_tgas) // 12).to_bytes(8, byteorder="little"),
            300 * account.TGAS,
            0,
            self.sender.use_nonce(),
            block_hash,
        )

    def sender_account(self) -> base.Account:
        return self.sender

@events.init.add_listener
def on_locust_init(environment, **kwargs):
    base.INIT_DONE.wait()
    # `master_funding_account` is the same on all runners, allowing to share a
    # single instance of congestion contract.
    funding_account = environment.master_funding_account
    environment.congestion_account_id = f"congestion.{funding_account.key.account_id}"

    # Only create congestion contract on master.
    if isinstance(environment.runner, runners.WorkerRunner):
        return

    node = base.NearNodeProxy(environment)
    funding_account = base.NearUser.funding_account
    funding_account.refresh_nonce(node.node)

    account = base.Account(
        key.Key.from_seed_testonly(environment.congestion_account_id))
    node.prepare_account(account, funding_account, 50000,
                         "create contract account")
    node.send_tx_retry(
        base.Deploy(
            account,
            environment.parsed_options.congestion_wasm,
            "Congestion",
        ), "deploy congestion contract")

# Congestion specific CLI args
@events.init_command_line_parser.add_listener
def _(parser):
    parser.add_argument(
        "--congestion-wasm",
        default="res/congestion.wasm",
        help="Path to the compiled congestion contract",
    )

'''
'''--- pytest/tests/loadtest/locust/common/ft.py ---
import random
import string
import sys
import pathlib
import typing
from locust import events

sys.path.append(str(pathlib.Path(__file__).resolve().parents[4] / 'lib'))

import key
from common.base import Account, Deploy, NearNodeProxy, NearUser, FunctionCall, INIT_DONE

class FTContract:
    # NEAR balance given to contracts, doesn't have to be much since users are
    # going to pay for storage
    INIT_BALANCE = NearUser.INIT_BALANCE

    def __init__(self, account: Account, ft_distributor: Account, code: str):
        self.account = account
        self.ft_distributor = ft_distributor
        self.registered_users = []
        self.code = code

    def install(self, node: NearNodeProxy, parent: Account):
        """
        Deploy and initialize the contract on chain.
        The account is created if it doesn't exist yet.
        """
        existed = node.prepare_account(self.account, parent,
                                       FTContract.INIT_BALANCE,
                                       "create contract account")
        if not existed:
            node.send_tx_retry(Deploy(self.account, self.code, "FT"),
                               "deploy ft")
            self.init_contract(node)

    def init_contract(self, node: NearNodeProxy):
        node.send_tx_retry(InitFT(self.account), "init ft")

    def register_user(self, user: NearUser):
        user.send_tx_retry(InitFTAccount(self.account, user.account),
                           locust_name="Init FT Account")
        user.send_tx_retry(TransferFT(self.account,
                                      self.ft_distributor,
                                      user.account_id,
                                      how_much=10**8),
                           locust_name="FT Funding")
        self.registered_users.append(user.account_id)

    def register_passive_user(self, node: NearNodeProxy, account: Account):
        """
        Passive users are only used as receiver, not as signer.
        """
        node.send_tx_retry(InitFTAccount(self.account, account),
                           locust_name="Init FT Account")
        self.registered_users.append(account.key.account_id)

    def random_receiver(self, sender: str) -> str:
        return self.random_receivers(sender, 1)[0]

    def random_receivers(self, sender: str, num) -> typing.List[str]:
        rng = random.Random()
        receivers = rng.sample(self.registered_users, num)
        # Sender must be != receiver but maybe there is no other registered user
        # yet, so we just send to the ft_distributor account which is registered
        # from the start
        return list(
            map(lambda a: a.replace(sender, self.ft_distributor.key.account_id),
                receivers))

    def create_passive_users(self,
                             num: int,
                             node: NearNodeProxy,
                             parent: Account,
                             max_account_id_len=64):
        """
        Create on-chain accounts and register them as FT users.
        Note that these are not locust users and they are not able to sign
        transactions. They are only used as targets of transactions and as a
        side-effect they also increase the state size of the contract.
        """
        prefix_len = max_account_id_len - len(parent.key.account_id) - 1
        assert prefix_len > 4, f"user key {parent.key.account_id} is too long"
        chars = string.ascii_lowercase + string.digits

        def create_account():
            prefix = ''.join(random.choices(chars, k=prefix_len))
            account_id = f"{prefix}.{parent.key.account_id}"
            return Account(key.Key.from_seed_testonly(account_id))

        accounts = [create_account() for _ in range(num)]
        node.prepare_accounts(accounts,
                              parent,
                              balance=0.3,
                              msg="create passive user")
        # TODO: this could also be done in parallel, actually in very simple
        # ways since there are no nonce conflicts (transactions are signed by
        # different users)
        for account in accounts:
            self.register_passive_user(node, account)

class TransferFT(FunctionCall):

    def __init__(self,
                 ft: Account,
                 sender: Account,
                 recipient_id: str,
                 how_much=1):
        # Attach exactly 1 yoctoNEAR according to NEP-141 to avoid calls from restricted access keys
        super().__init__(sender, ft.key.account_id, "ft_transfer", balance=1)
        self.ft = ft
        self.sender = sender
        self.recipient_id = recipient_id
        self.how_much = how_much

    def args(self) -> dict:
        return {
            "receiver_id": self.recipient_id,
            "amount": str(int(self.how_much)),
        }

    def sender_account(self) -> Account:
        return self.sender

class InitFT(FunctionCall):

    def __init__(self, contract: Account):
        super().__init__(contract, contract.key.account_id, "new_default_meta")
        self.contract = contract

    def args(self) -> dict:
        return {
            "owner_id": self.contract.key.account_id,
            "total_supply": str(10**33)
        }

    def sender_account(self) -> Account:
        return self.contract

class InitFTAccount(FunctionCall):

    def __init__(self, contract: Account, account: Account):
        super().__init__(account,
                         contract.key.account_id,
                         "storage_deposit",
                         balance=int(1E23))
        self.contract = contract
        self.account = account

    def args(self) -> dict:
        return {"account_id": self.account.key.account_id}

    def sender_account(self) -> Account:
        return self.account

@events.init.add_listener
def on_locust_init(environment, **kwargs):
    INIT_DONE.wait()
    node = NearNodeProxy(environment)
    ft_contract_code = environment.parsed_options.fungible_token_wasm
    num_ft_contracts = environment.parsed_options.num_ft_contracts
    funding_account = NearUser.funding_account
    parent_id = funding_account.key.account_id

    funding_account.refresh_nonce(node.node)

    environment.ft_contracts = []
    # TODO: Create accounts in parallel
    for i in range(num_ft_contracts):
        account_id = environment.account_generator.random_account_id(
            parent_id, '_ft')
        contract_key = key.Key.from_random(account_id)
        ft_account = Account(contract_key)
        ft_contract = FTContract(ft_account, ft_account, ft_contract_code)
        ft_contract.install(node, funding_account)
        environment.ft_contracts.append(ft_contract)

# FT specific CLI args
@events.init_command_line_parser.add_listener
def _(parser):
    parser.add_argument("--fungible-token-wasm",
                        default="res/fungible_token.wasm",
                        help="Path to the compiled Fungible Token contract")
    parser.add_argument(
        "--num-ft-contracts",
        type=int,
        required=False,
        default=4,
        help=
        "How many different FT contracts to spawn from this worker (FT contracts are never shared between workers)"
    )

'''
'''--- pytest/tests/loadtest/locust/common/sharding.py ---
"""Account name generator

Provides tools to generate account names that are distributed evenly across the shards.

For account naming rules and conventions see https://nomicon.io/DataStructures/Account
"""

import os
import sys
import random
import re
import unittest

def char_range(lower, upper, upper_inclusive=True):
    l = ord(lower)
    u = ord(upper) + 1 if upper_inclusive else ord(upper)
    return [chr(i) for i in range(l, u)]

def alpha_num_str(length):
    return ''.join(
        random.choices(char_range('0', '9') + char_range('a', 'z'), k=length))

def random_char_below(upper, upper_inclusive):
    if upper >= 'a':
        chars = char_range('0', '9') + char_range('a', upper, upper_inclusive)
    elif upper == '_':
        chars = char_range('0', '9')
        if upper_inclusive:
            chars.append('_')
    elif upper >= '1':
        chars = char_range('0', upper, upper_inclusive)
    elif upper == '0':
        # here just return a - if upper_inclusive is False, since that's called
        # only when we want to finish a prefix of the upper string. Otherwise, don't bother
        # with returning a - to avoid handling account ID validity in that case
        if upper_inclusive:
            chars = ['0']
        else:
            chars = ['-']
    else:
        assert upper == '-' or upper == '.'
        return upper
    return random.choice(chars)

def random_char_above(lower):
    if lower >= 'a':
        chars = char_range(lower, 'z')
    elif lower == '_':
        chars = ['_'] + char_range('a', 'z')
    elif lower >= '0':
        chars = char_range(lower, '9') + char_range('a', 'z')
    elif lower == '.':
        chars = ['.'] + char_range('0', '9') + char_range('a', 'z')
    else:
        assert lower == '-'
        chars = ['-'] + char_range(lower, '9') + char_range('a', 'z')
    return random.choice(chars)

def random_valid_char(lower, upper, upper_inclusive=True):
    if lower is None and upper is None:
        chars = char_range('0', '9') + char_range('a', 'z')
        return random.choice(chars)
    if lower is None:
        return random_char_below(upper, upper_inclusive)
    if upper is None:
        return random_char_above(lower)

    assert (upper_inclusive and lower <= upper) or (not upper_inclusive and
                                                    lower < upper)

    if lower >= 'a':
        assert lower <= 'z'
        chars = char_range(lower, upper, upper_inclusive)
    elif lower == '_':
        chars = ['_']
        if upper != '_':
            chars += char_range('a', upper, upper_inclusive)
    elif lower >= '0':
        if upper <= '9':
            chars = char_range(lower, upper, upper_inclusive)
        elif upper == '_':
            chars = char_range(lower, '9')
            if upper_inclusive:
                chars += ['_']
        else:
            chars = char_range(lower, '9') + char_range('a', upper,
                                                        upper_inclusive)
    elif lower == '.':
        if upper == '.':
            # upper_inclusive must be false here because of the assert above
            chars = ['.']
        elif upper <= '9':
            assert upper >= '0'
            chars = ['.'] + char_range('0', upper, upper_inclusive)
        elif upper == '_':
            chars = ['.'] + char_range('0', '9')
            if upper_inclusive:
                chars += '_'
        else:
            assert upper >= 'a' and upper <= 'z'
            chars = ['.'] + char_range('0', '9') + char_range(
                'a', upper, upper_inclusive)
    else:
        assert lower == '-'
        if upper == '-' or upper == '.':
            chars = ['-']
            # if upper == '.', we don't include the '.' in the list of chars even if upper_inclusive
            # is True, because we want to make it easy for ourselves in the case we're generating a prefix
            # between b-0 and b.0, where if we choose a '.' in position 2, we're out of luck because the next
            # char is a 0
        elif upper <= '9':
            assert upper >= '0'
            chars = ['-'] + char_range('0', upper, upper_inclusive)
        elif upper == '_':
            chars = ['-'] + char_range('0', '9')
            if upper_inclusive:
                chars += '_'
        else:
            assert upper >= 'a' and upper <= 'z'
            chars = ['-'] + char_range('0', '9') + char_range(
                'a', upper, upper_inclusive)
    return random.choice(chars)

def char_at(s, i):
    if s is None or i >= len(s):
        return None
    return s[i]

# when picking a random char between two chars, we'll consider it as counting toward the number
# of free chars we wanted to generate if the range is large. We could try and be clever and add fractional
# "freeness" but it's not a big deal, and the initial choice of free_chars=6 is arbitrary anyway
def char_range_is_large(l, u):
    return (l is None or l < '4') and (u is None or u > 't')

# if we so far generated a prefix exactly equal to the upper boundary string up to its second to last char,
# handle it specially here, because we have to make sure we generate a prefix strictly lower than it
def finish_upper(lower, upper, prefix, free_chars, free_length):
    if len(upper) > 1:
        # e.g. lower = "aurora" and upper = "aurora-0". In that case "aurora" is the only valid account ID in the shard
        assert upper[-2] != '-' or upper[
            -1] != '0', f'Cannot build account ID prefix less than {upper}'
    else:
        assert upper[
            0] > '0', f'Cannot build account ID prefix less than {upper}'

    l = char_at(lower, len(upper) - 1)
    c = random_valid_char(l, upper[-1], upper_inclusive=False)
    prefix += c
    if char_range_is_large(l, upper[-1]):
        free_chars += 1
    if l is not None and c != l:
        # we've generated a prefix strictly greater than lower, so no need to consider it anymore
        lower = None
    extra_chars_needed = c in ['.', '-', '_'] or lower is not None
    if lower is not None:
        for i in range(len(upper), len(lower)):
            c = random_char_above(lower[i])
            prefix += c
            if char_range_is_large(lower[i], None):
                free_chars += 1
            if c != lower[i]:
                extra_chars_needed = False
                break
    if extra_chars_needed:
        extra_len = max(1, free_length - free_chars)
    else:
        extra_len = max(0, free_length - free_chars)
    if extra_len > 0:
        prefix += alpha_num_str(extra_len)
    return prefix

# generate a string that's a valid account ID between lower and upper
# free_length refers to the number of characters in the result that are free
# to be chosen from a large range. For example, if lower='aaa', upper='aaa0',
# then we want a string of length 4 + free_length, because the first 4 characters
# are constricted
# TODO: This could hopefully be made simpler by successively appending either an
# alphanumeric character or one of ['-', '.', '_'] followed by an alphanumeric character,
# choosing one of the ones that keeps us between the bounds each time.
# See https://github.com/near/nearcore/pull/9194#pullrequestreview-1488492798
def random_prefix_between(lower, upper, free_length=6):
    assert lower is None or upper is None or lower < upper, (lower, upper)

    # 1 shard case
    if lower is None and upper is None:
        return alpha_num_str(free_length)

    prefix = ''
    free_chars = 0
    max_len = 0
    if upper is not None:
        max_len = len(upper)
    if lower is not None:
        max_len = max(max_len, len(lower))

    for i in range(max_len):
        l = char_at(lower, i)
        u = char_at(upper, i)

        if l is not None and l == u:
            prefix += l
            continue

        if l is None and u is None:
            # we get here when lower is shorter than upper, and we have generated
            # a string equal to lower. Just add anything at the end
            extra_len = max(1, free_length - free_chars)
            prefix += alpha_num_str(extra_len)
            return prefix

        if upper is not None and i == len(upper) - 1:
            return finish_upper(lower, upper, prefix, free_chars, free_length)

        c = random_valid_char(l, u)
        prefix += c
        if char_range_is_large(l, u):
            free_chars += 1
        if c != u and c != l:
            if free_chars < free_length:
                prefix += alpha_num_str(free_length - free_chars)
            return prefix

        if c != u:
            upper = None
        if c != l:
            lower = None

        if free_chars >= free_length:
            # here only one of lower/upper is not None, meaning we have a prefix of it so far
            # if it's a prefix of upper, it's strictly smaller, since we call finish_upper() before hitting
            # the end of upper in this loop.
            # If it's a prefix of lower, we add chars here to make it larger. Just add z's
            # to be lazy and make the code simpler, since we already got the number of free chars we wanted
            if lower is not None:
                prefix += 'z'
                for j in range(i + 1, len(lower)):
                    if lower[j] != 'z':
                        break
                    prefix += 'z'
            return prefix

    assert lower is not None and upper is None
    # here we happened to generate a prefix equal to lower, but still strictly less than upper
    # no matter what we do, so just add anything to the end.
    extra_len = max(1, free_length - free_chars)
    prefix += alpha_num_str(extra_len)
    return prefix

def random_account_between(base_name, suffix, lower, upper, free_length=6):
    prefix = random_prefix_between(lower, upper, free_length)
    return f'{prefix}{suffix}.{base_name}'

# Given a shard layout, generates accounts distributed evenly across the shards
class AccountGenerator:

    def __init__(self, shard_layout):
        assert len(shard_layout) == 1
        assert 'V0' in shard_layout or 'V1' in shard_layout

        self.shard_map = {}

        # If the shard layout is V0, we can just skip this, and random_account_id()
        # will see an empty self.shard_map and generate a random prefix, which should
        # distribute the accounts evenly across shards in that case
        shard_layout_v1 = shard_layout.get('V1')
        if shard_layout_v1 is not None:
            # taken from a comment in core/account-id/src/lib.rs
            account_regex = re.compile(
                r'^(([a-z\d]+[-_])*[a-z\d]+\.)*([a-z\d]+[-_])*[a-z\d]+$')
            # doesn't actually matter that much to get the shard IDs right, since we're just
            # picking one at random, and not actually doing anything with the shard ID itself, but
            # add the right offset to the shard IDs below just for cleanliness, and in case we
            # want to print out shard IDs or something
            shard_offset = len(shard_layout_v1['fixed_shards'])
            accounts = shard_layout_v1['boundary_accounts']
            if len(accounts) == 0:
                self.shard_map[shard_offset] = (None, None)
                return

            if accounts[0] != '00':
                self.shard_map[shard_offset] = (None, accounts[0])

            for i, account_id in enumerate(accounts):
                # should of course be true, but assert it since we let the user pass a shard layout file, so
                # verify it at least a little bit, along with the check below that the list is increasing
                assert account_regex.fullmatch(account_id) is not None

                if i + 1 < len(accounts):
                    next_account = accounts[i + 1]
                    assert account_id < next_account

                    # like "aurora" and "aurora-0", in this case we can't generate any accounts there
                    if next_account != account_id + '-0':
                        self.shard_map[shard_offset + i + 1] = (account_id,
                                                                next_account)
                else:
                    if account_id != 'z' * 64:
                        self.shard_map[shard_offset + i + 1] = (account_id,
                                                                None)

            # This should be true no matter what boundary accounts we have, but just sanity check it
            assert len(self.shard_map) > 0

    # generate a valid subaccount ID of `base_name`` between lower and upper, with the first part of
    # the account ID ending with `suffix`
    # TODO: check the resulting length somewhere. Right now it's not checked and could be too large
    # if `base_name` is large
    def random_account_id(self, base_name, suffix):
        if len(self.shard_map) == 0:
            return random_account_between(base_name, suffix, None, None)
        else:
            shard_id, (lower,
                       upper) = random.choice(list(self.shard_map.items()))
            return random_account_between(base_name, suffix, lower, upper)

class TestRandomAccount(unittest.TestCase):

    def test_random_account(self):
        account_regex = re.compile(
            r'^(([a-z\d]+[-_])*[a-z\d]+\.)*([a-z\d]+[-_])*[a-z\d]+$')
        test_cases = [
            (None, None),
            ('aa', None),
            (None, 'aa'),
            ('aa', 'bb'),
            ('56', 'bb'),
            ('a-1', 'a-1-1'),
            ('a-0', 'a-01'),
            ('a-0', 'a-00'),
            ('b-b', 'bb'),
            ('aa', 'aa000'),
            ('b-0', 'b.0'),
        ]
        for (lower, upper) in test_cases:
            # sanity check the test case itself
            if lower is not None:
                assert account_regex.fullmatch(lower) is not None
            if upper is not None:
                assert account_regex.fullmatch(upper) is not None

            for _ in range(10):
                account_id = random_account_between('foo.near', '_ft', lower,
                                                    upper)
                assert account_regex.fullmatch(account_id) is not None, (
                    account_id, lower, upper)
                if lower is not None:
                    assert account_id >= lower, (account_id, lower, upper)
                if upper is not None:
                    assert account_id < upper, (account_id, lower, upper)

'''
'''--- pytest/tests/loadtest/locust/common/social.py ---
from abc import abstractmethod
import json
import sys
import pathlib
import typing
import unittest

sys.path.append(str(pathlib.Path(__file__).resolve().parents[4] / 'lib'))
sys.path.append(str(pathlib.Path(__file__).resolve().parents[1]))

import transaction

from account import TGAS, NEAR_BASE
import key
from common.base import Account, Deploy, NearNodeProxy, Transaction, FunctionCall, INIT_DONE
from locust import events, runners
from transaction import create_function_call_action

class SocialDbSet(FunctionCall):

    def __init__(self, contract_id: str, sender: Account):
        super().__init__(sender, contract_id, "set")
        self.contract_id = contract_id
        self.sender = sender

    def sender_account(self) -> Account:
        return self.sender

class SubmitPost(SocialDbSet):

    def __init__(self, contract_id: str, sender: Account, content: str):
        super().__init__(contract_id, sender)
        self.content = content

    def args(self) -> dict:
        return social_post_args(self.sender.key.account_id, self.content)

class Follow(SocialDbSet):

    def __init__(self, contract_id: str, sender: Account,
                 follow_list: typing.List[str]):
        super().__init__(contract_id, sender)
        self.follow_list = follow_list

    def args(self) -> dict:
        follow_list = self.follow_list
        sender = self.sender.key.account_id
        return social_follow_args(sender, follow_list)

class InitSocialDB(Transaction):

    def __init__(self, contract: Account):
        super().__init__()
        self.contract = contract

    def sign_and_serialize(self, block_hash) -> bytes:
        # Call the #[init] function, no arguments
        call_new_action = create_function_call_action("new", "", 100 * TGAS, 0)

        # Set the status to "Live" to enable normal usage
        args = json.dumps({"status": "Live"}).encode('utf-8')
        call_set_status_action = create_function_call_action(
            "set_status", args, 100 * TGAS, 0)

        # Batch the two actions above into one transaction
        nonce = self.contract.use_nonce()
        key = self.contract.key
        return transaction.sign_and_serialize_transaction(
            key.account_id, nonce, [call_new_action, call_set_status_action],
            block_hash, key.account_id, key.decoded_pk(), key.decoded_sk())

    def sender_account(self) -> Account:
        return self.contract

class InitSocialDbAccount(FunctionCall):
    """
    Send initial storage balance to ensure the account can use social DB.

    Technically, we could also rely on lazy initialization and just send enough
    balance with each request. But a typical user sends balance ahead of time.
    """

    def __init__(self, contract_id: str, account: Account):
        super().__init__(account,
                         contract_id,
                         "storage_deposit",
                         balance=1 * NEAR_BASE)
        self.contract_id = contract_id
        self.account = account

    def args(self) -> dict:
        return {"account_id": self.account.key.account_id}

    def sender_account(self) -> Account:
        return self.account

def social_db_build_index_obj(key_list_pairs: dict) -> dict:
    """
    JSON serializes the key - value-list pairs to be included in a SocialDB set message.

    To elaborate a bit more, SocialDB expects for example
    ```json
    "index": { "graph": value_string } 
    ```
    where value_string = 
    ```json
    "[{\"key\":\"follow\",\"value\":{\"type\":\"follow\",\"accountId\":\"pagodaplatform.near\"}}]"
    ```
    So it's really JSON nested inside a JSON string.
    And worse, the nested JSON is always a list of objects with "key" and "value" fields.
    This method unfolds this format from a leaner definition, using a list of pairs to
    define each `value_string`.
    A dict instead of a list of tuples doesn't work because keys can be duplicated.
    """

    def serialize_values(values: typing.List[typing.Tuple[str, dict]]):
        return json.dumps([{"key": k, "value": v} for k, v in values])

    return {
        key: serialize_values(values) for key, values in key_list_pairs.items()
    }

def social_db_set_msg(sender: str, values: dict, index: dict) -> dict:
    """
    Construct a SocialDB `set` function argument.

    The output will be of the form:
    ```json
    {
      "data": {
        "key1": value1,
        "key3": value2,
        "key4": value3,
        "index": {
          "index_key1": "[{\"index_key_1_key_A\":\"index_key_1_value_A\"}]",
          "index_key2": "[{\"index_key_2_key_A\":\"index_key_2_value_A\"},{\"index_key_2_key_B\":\"index_key_2_value_B\"}]",
        }
      }
    }
    ```
    """
    updates = values.copy()
    updates["index"] = social_db_build_index_obj(index)
    msg = {"data": {sender: updates}}
    return msg

def social_follow_args(sender: str, follow_list: typing.List[str]) -> dict:
    follow_map = {}
    graph = []
    notify = []
    for user in follow_list:
        follow_map[user] = ""
        graph.append(("follow", {"type": "follow", "accountId": user}))
        notify.append((user, {"type": "follow"}))

    values = {
        "graph": {
            "follow": follow_map
        },
    }
    index = {"graph": graph, "notify": notify}
    return social_db_set_msg(sender, values, index)

def social_post_args(sender: str, text: str) -> dict:
    values = {"post": {"main": json.dumps({"type": "md", "text": text})}}
    index = {"post": [("main", {"type": "md"})]}
    msg = social_db_set_msg(sender, values, index)
    return msg

class TestSocialDbSetMsg(unittest.TestCase):

    def test_follow(self):
        sender = "alice.near"
        follow_list = ["bob.near"]
        parsed_msg = social_follow_args(sender, follow_list)
        expected_msg = {
            "data": {
                "alice.near": {
                    "graph": {
                        "follow": {
                            "bob.near": ""
                        }
                    },
                    "index": {
                        "graph":
                            "[{\"key\": \"follow\", \"value\": {\"type\": \"follow\", \"accountId\": \"bob.near\"}}]",
                        "notify":
                            "[{\"key\": \"bob.near\", \"value\": {\"type\": \"follow\"}}]"
                    }
                }
            }
        }
        self.maxDiff = 2000  # print large diffs
        self.assertEqual(parsed_msg, expected_msg)

    def test_mass_follow(self):
        sender = "alice.near"
        follow_list = ["bob.near", "caroline.near", "david.near"]
        parsed_msg = social_follow_args(sender, follow_list)
        expected_msg = {
            "data": {
                "alice.near": {
                    "graph": {
                        "follow": {
                            "bob.near": "",
                            "caroline.near": "",
                            "david.near": "",
                        }
                    },
                    "index": {
                        "graph":
                            "[{\"key\": \"follow\", \"value\": {\"type\": \"follow\", \"accountId\": \"bob.near\"}},"
                            " {\"key\": \"follow\", \"value\": {\"type\": \"follow\", \"accountId\": \"caroline.near\"}},"
                            " {\"key\": \"follow\", \"value\": {\"type\": \"follow\", \"accountId\": \"david.near\"}}]",
                        "notify":
                            "[{\"key\": \"bob.near\", \"value\": {\"type\": \"follow\"}},"
                            " {\"key\": \"caroline.near\", \"value\": {\"type\": \"follow\"}},"
                            " {\"key\": \"david.near\", \"value\": {\"type\": \"follow\"}}]"
                    }
                }
            }
        }
        self.maxDiff = 2000  # print large diffs
        self.assertEqual(parsed_msg, expected_msg)

    def test_post(self):
        sender = "alice.near"
        text = "#Title\n\nbody"
        parsed_msg = social_post_args(sender, text)
        expected_msg = {
            "data": {
                "alice.near": {
                    "post": {
                        "main":
                            "{\"type\": \"md\", \"text\": \"#Title\\n\\nbody\"}"
                    },
                    "index": {
                        "post":
                            "[{\"key\": \"main\", \"value\": {\"type\": \"md\"}}]"
                    }
                }
            }
        }
        self.maxDiff = 2000  # print large diffs
        self.assertEqual(parsed_msg, expected_msg)

@events.init.add_listener
def on_locust_init(environment, **kwargs):
    INIT_DONE.wait()
    # `master_funding_account` is the same on all runners, allowing to share a
    # single instance of SocialDB in its `social` sub account
    funding_account = environment.master_funding_account
    environment.social_account_id = f"social{environment.parsed_options.run_id}.{funding_account.key.account_id}"

    # Create SocialDB account, unless we are a worker, in which case the master already did it
    if not isinstance(environment.runner, runners.WorkerRunner):
        social_contract_code = environment.parsed_options.social_db_wasm
        contract_key = key.Key.from_seed_testonly(environment.social_account_id)
        social_account = Account(contract_key)

        node = NearNodeProxy(environment)
        existed = node.prepare_account(social_account, funding_account, 50000,
                                       "create contract account")
        if not existed:
            node.send_tx_retry(
                Deploy(social_account, social_contract_code, "Social DB"),
                "deploy socialDB contract")
            node.send_tx_retry(InitSocialDB(social_account),
                               "init socialDB contract")

# Social specific CLI args
@events.init_command_line_parser.add_listener
def _(parser):
    parser.add_argument(
        "--social-db-wasm",
        default="res/social_db.wasm",
        help=
        "Path to the compiled SocialDB contract, get it from https://github.com/NearSocial/social-db/tree/aa7fafaac92a7dd267993d6c210246420a561370/res"
    )

'''
'''--- pytest/tests/loadtest/locust/common/sweat.py ---
import typing
from common.ft import FTContract, InitFTAccount
from common.base import Account, Deploy, NearNodeProxy, NearUser, FunctionCall, MultiFunctionCall, INIT_DONE
import locust
import sys
import pathlib
from locust import events
from collections import namedtuple

sys.path.append(str(pathlib.Path(__file__).resolve().parents[4] / 'lib'))

import key

RecipientSteps = namedtuple('RecipientSteps', ['account', 'steps'])

class SweatContract(FTContract):

    def __init__(self, main_account: Account, claim_account: Account,
                 oracle_account: Account, code: str):
        super().__init__(main_account, oracle_account, code)
        self.claim = claim_account
        self.oracle = oracle_account

    def init_contract(self, node: NearNodeProxy):
        node.send_tx_retry(InitSweat(self.account), "init sweat")
        # Unlike FT initialization that starts with a total supply and assigns
        # it to the user, the sweat main account doesn't start with tokens. We
        # need to register an account and then mint the tokens, so that it can
        # distribute them later.
        node.send_tx_retry(InitFTAccount(self.account, self.ft_distributor),
                           locust_name="Init Sweat Account")
        self.register_oracle(node, self.oracle.key.account_id)
        self.top_up(node, self.ft_distributor.key.account_id)

    def top_up(self, node: NearNodeProxy, receiver_id: str):
        """
        Adds a large amount of tokens to an account.
        Note: This should only be called on the master runner, as it requires
        the private key of the sweat contract account. (Multiple runners using
        this leads to frequent nonce invalidations.)
        """
        node.send_tx_retry(
            SweatMint(self.account, receiver_id, 1_000_000_000_000),
            "top up sweat")

    def register_oracle(self, node: NearNodeProxy, oracle_id: str):
        """
        Register an account as oracle to give it the power to mint tokens for steps.
        """
        node.send_tx_retry(SweatAddOracle(self.account, oracle_id),
                           "add sweat oracle")

class InitSweat(FunctionCall):

    def __init__(self, sweat_account: Account):
        super().__init__(sweat_account, sweat_account.key.account_id, "new")

    def args(self) -> dict:
        # Technical details about Sweat contract initialization:
        #
        # A postfix is used by the smart contract to decide whether or not to
        # hash an account id. It's an optimization that makes storage keys for
        # implicit accounts shorter, while preserving short keys of named
        # accounts.
        #
        # More specifically, any account id that matches .*<postfix> will be
        # stored in the trie normally, like in a unmodified FT contract. Any
        # other account ids are hashed.
        #
        # As an example, the postfix can be `.u.sweat.testnet`.
        #
        # Source code for reference:
        # https://github.com/sweatco/near-sdk-rs/blob/af6ba3cb75e0bbfc26e346e61aa3a0d1d7f5ac7b/near-contract-standards/src/fungible_token/core_impl.rs#L249-L259
        #
        # Here we don't provide a postfix, so everything will be hashed. This is
        # fine for new contracts we create. And when we reuse a contract, we
        # won't need to initialise it at all.
        return {"postfix": None}

class InitClaim(FunctionCall):

    def __init__(self, claim_account: Account, token_account_id: str):
        super().__init__(claim_account, claim_account.key.account_id, "init")
        self.token_account_id = token_account_id

    def args(self) -> dict:
        return {"token_account_id": self.token_account_id}

class SweatAddOracle(FunctionCall):
    """
    Oracle accounts are allowed to mint new tokens and can only be added by the
    account id of the contract itself.
    """

    def __init__(self, sweat_account: Account, oracle_id: str):
        super().__init__(sweat_account, sweat_account.key.account_id,
                         "add_oracle")
        self.oracle_id = oracle_id

    def args(self) -> dict:
        return {"account_id": self.oracle_id}

class SweatMint(FunctionCall):
    """
    A call to `sweat::tge_mint`.
    Token Generation Event (TGE) was day 0 when SWEAT launched.
    This is the transaction to get initial balance into accounts.
    """

    def __init__(self, sweat: Account, user_id: str, amount: int):
        super().__init__(sweat, sweat.key.account_id, "tge_mint")
        self.user_id = user_id
        self.amount = amount

    def args(self) -> dict:
        return {
            "account_id": self.user_id,
            "amount": f"{self.amount}",
        }

class SweatMintBatch(MultiFunctionCall):
    """
    A call to `sweat::record_batch`.
    Mints new tokens for walked steps for a batch of users.
    Might get split into multiple function calls to avoid log output limits.
    """

    def __init__(self, sweat_id: str, oracle: Account,
                 recipient_step_pairs: typing.List[RecipientSteps]):
        super().__init__(oracle, sweat_id, "record_batch")
        self.recipient_step_pairs = recipient_step_pairs

    def args(self) -> typing.List[dict]:
        # above a threshold, we hit the log output limit of 16kB
        # this depends a bit on the exact account id names
        chunk_len = 150
        chunks = [
            self.recipient_step_pairs[s:s + chunk_len]
            for s in range(0, len(self.recipient_step_pairs), chunk_len)
        ]

        return [{"steps_batch": chunk} for chunk in chunks]

class SweatDeferBatch(FunctionCall):
    """
    A call to `sweat::defer_batch`.
    """

    def __init__(self, sweat_id: str, oracle: Account, holding_account_id: str,
                 steps_batch: typing.List[RecipientSteps]):
        super().__init__(oracle, sweat_id, "defer_batch")
        self.holding_account_id = holding_account_id
        self.steps_batch = steps_batch

    def args(self) -> dict:
        return {
            "holding_account_id": self.holding_account_id,
            "steps_batch": self.steps_batch,
        }

class SweatGetClaimableBalanceForAccount(FunctionCall):
    """
    A call to `sweat.claim::get_claimable_balance_for_account`.

    We use it instead of `sweat.claim::claim` as it does not require to wait for funds to become
    claimable and performs similar amount of computation.
    """

    def __init__(self, sweat_claim_id: str, user: Account, account_id: str):
        super().__init__(user, sweat_claim_id,
                         "get_claimable_balance_for_account")
        self.account_id = account_id

    def args(self) -> dict:
        return {
            "account_id": self.account_id,
        }

@events.init.add_listener
def on_locust_init(environment, **kwargs):
    INIT_DONE.wait()
    node = NearNodeProxy(environment)
    worker_id = getattr(environment.runner, "worker_index", "_master")
    run_id = environment.parsed_options.run_id

    funding_account = NearUser.funding_account
    funding_account.refresh_nonce(node.node)
    sweat_account_id = f"sweat{run_id}.{environment.master_funding_account.key.account_id}"
    sweat_claim_account_id = f"sweat-claim{run_id}.{environment.master_funding_account.key.account_id}"
    oracle_account_id = worker_oracle_id(worker_id, run_id,
                                         environment.master_funding_account)

    sweat_account = Account(key.Key.from_seed_testonly(sweat_account_id))
    sweat_claim_account = Account(
        key.Key.from_seed_testonly(sweat_claim_account_id))
    oracle_account = Account(key.Key.from_seed_testonly(oracle_account_id))

    environment.sweat = SweatContract(sweat_account, sweat_claim_account,
                                      oracle_account,
                                      environment.parsed_options.sweat_wasm)

    # Create Sweat contract, unless we are a worker, in which case the master already did it
    if not isinstance(environment.runner, locust.runners.WorkerRunner):
        node.prepare_account(oracle_account, environment.master_funding_account,
                             FTContract.INIT_BALANCE, "create oracle account")
        environment.sweat.install(node, environment.master_funding_account)

        existed = node.prepare_account(sweat_claim_account, funding_account,
                                       100000, "create contract account")
        if not existed:
            node.send_tx_retry(
                Deploy(sweat_claim_account,
                       environment.parsed_options.sweat_claim_wasm,
                       "Sweat-Claim"), "deploy Sweat-Claim contract")
            node.send_tx_retry(InitClaim(sweat_claim_account, sweat_account_id),
                               "init Sweat-Claim contract")
            # `sweat` account also must be registered as oracle to call methods on `sweat.claim`.
            node.send_tx_retry(
                SweatAddOracle(sweat_claim_account, sweat_account_id),
                "add sweat.claim oracle")

    # on master, register oracles for workers
    if isinstance(environment.runner, locust.runners.MasterRunner):
        num_oracles = int(environment.parsed_options.max_workers)
        oracle_accounts = [
            Account(
                key.Key.from_seed_testonly(
                    worker_oracle_id(id, run_id,
                                     environment.master_funding_account)))
            for id in range(num_oracles)
        ]
        node.prepare_accounts(oracle_accounts,
                              environment.master_funding_account, 100000,
                              "create contract account")
        for oracle in oracle_accounts:
            id = oracle.key.account_id
            environment.sweat.top_up(node, id)
            environment.sweat.register_oracle(node, id)
            node.send_tx_retry(SweatAddOracle(sweat_claim_account, id),
                               "add sweat.claim oracle")

def worker_oracle_id(worker_id, run_id, funding_account):
    return f"sweat{run_id}_oracle{worker_id}.{funding_account.key.account_id}"

@events.init_command_line_parser.add_listener
def _(parser):
    parser.add_argument("--sweat-wasm",
                        default="res/sweat.wasm",
                        help="Path to the compiled Sweat contract")
    parser.add_argument("--sweat-claim-wasm",
                        default="res/sweat_claim.wasm",
                        help="Path to the compiled Sweat-Claim contract")

'''
'''--- pytest/tests/loadtest/locust/locustfiles/congestion.py ---
"""
A workload that generates congestion.
"""

import logging
import pathlib
import sys

sys.path.append(str(pathlib.Path(__file__).resolve().parents[4] / 'lib'))

from configured_logger import new_logger
from locust import between, task
from common.base import NearUser
from common.congestion import ComputeSha256, ComputeSum

logger = new_logger(level=logging.WARN)

class CongestionUser(NearUser):
    """
    Runs a resource-heavy workload that is likely to cause congestion.
    """
    wait_time = between(1, 3)  # random pause between transactions

    @task
    def compute_sha256(self):
        self.send_tx(ComputeSha256(self.contract_account_id, self.account,
                                   100000),
                     locust_name="SHA256, 100 KiB")

    @task
    def compute_sum(self):
        self.send_tx(ComputeSum(self.contract_account_id, self.account, 250),
                     locust_name="Sum, 250 TGas")

    def on_start(self):
        super().on_start()
        self.contract_account_id = self.environment.congestion_account_id

'''
'''--- pytest/tests/loadtest/locust/locustfiles/ft.py ---
"""
A workload with Fungible Token operations.
"""

import logging
import pathlib
import random
import sys

sys.path.append(str(pathlib.Path(__file__).resolve().parents[4] / 'lib'))

from configured_logger import new_logger
from locust import between, task
from common.base import NearUser
from common.ft import TransferFT

logger = new_logger(level=logging.WARN)

class FTTransferUser(NearUser):
    """
    Registers itself on an FT contract in the setup phase, then just sends FTs to
    random users.
    """
    wait_time = between(1, 3)  # random pause between transactions

    @task
    def ft_transfer(self):
        receiver = self.ft.random_receiver(self.account_id)
        tx = TransferFT(self.ft.account, self.account, receiver, how_much=1)
        self.send_tx(tx, locust_name="FT transfer")

    def on_start(self):
        super().on_start()
        self.ft = random.choice(self.environment.ft_contracts)
        self.ft.register_user(self)
        logger.debug(
            f"{self.account_id} ready to use FT contract {self.ft.account.key.account_id}"
        )

'''
'''--- pytest/tests/loadtest/locust/locustfiles/social.py ---
"""
A workload that simulates SocialDB traffic.
"""

import logging
import pathlib
import random
import sys

sys.path.append(str(pathlib.Path(__file__).resolve().parents[4] / 'lib'))

from configured_logger import new_logger
from locust import between, task
from common.base import NearUser
from common.social import Follow, InitSocialDbAccount, SubmitPost

logger = new_logger(level=logging.WARN)

class SocialDbUser(NearUser):
    """
    Registers itself on near.social in the setup phase, then starts posting,
    following, and liking posts.
    """
    wait_time = between(1, 3)  # random pause between transactions
    registered_users = []

    @task
    def follow(self):
        users_to_follow = [random.choice(SocialDbUser.registered_users)]
        self.send_tx(Follow(self.contract_account_id, self.account,
                            users_to_follow),
                     locust_name="Social Follow")

    @task
    def post(self):
        seed = random.randrange(2**32)
        len = random.randrange(100, 1000)
        post = self.generate_post(len, seed)
        self.send_tx(SubmitPost(self.contract_account_id, self.account, post),
                     locust_name="Social Post")

    def on_start(self):
        super().on_start()
        self.contract_account_id = self.environment.social_account_id

        self.send_tx(InitSocialDbAccount(self.contract_account_id,
                                         self.account),
                     locust_name="Init Social Account")
        logger.debug(
            f"user {self.account_id} ready to use SocialDB on {self.contract_account_id}"
        )

        SocialDbUser.registered_users.append(self.account_id)

    def generate_post(self, length: int, seed: int) -> str:
        sample_quotes = [
            "Despite the constant negative press covfefe",
            "Sorry losers and haters, but my I.Q. is one of the highest - and you all know it! Please don't feel so stupid or insecure, it's not your fault",
            "Windmills are the greatest threat in the US to both bald and golden eagles. Media claims fictional 'global warming' is worse.",
        ]
        quote = sample_quotes[seed % len(sample_quotes)]
        post = f"I, {self.account.key.account_id}, cannot resists to declare with pride: \n_{quote}_"
        while length > len(post):
            post = f"{post}\nI'll say it again: \n**{quote}**"

        return post[:length]

'''
'''--- pytest/tests/loadtest/locust/locustfiles/sweat.py ---
"""
A workload with Sweat operations.

Sweat is a slightly modified version of the standard fungible token contract.
  - The lookup map is slightly modified to make storage keys shorter
  - There is a record_batch method which can update many users' balances at once
  - The "oracles" concept was added, a list of privileged accounts that can mint tokens

This workload is similar to the FT workload with 2 major differences:
  - Single account with larger state (larger state still TODO)
  - Periodic batches that adds steps (mints new tokens)
"""

from common.sweat import RecipientSteps, SweatGetClaimableBalanceForAccount, SweatMintBatch, SweatDeferBatch
from common.ft import TransferFT
from common.base import Account, AddFullAccessKey, NearUser
from locust import between, tag, task
import logging
import pathlib
import random
import sys

sys.path.append(str(pathlib.Path(__file__).resolve().parents[4] / 'lib'))

from configured_logger import new_logger
import key

logger = new_logger(level=logging.WARN)

class SweatOracle(NearUser):
    """
    Registers itself as an oracle in the setup phase, then records Sweat minting events.
    """
    wait_time = between(1, 3)  # random pause between transactions
    fixed_count = 1  # Oracle can do only one operation at a time.

    @task(1)
    def record_single_batch(self):
        rng = random.Random()
        # just around the log limit
        batch_size = min(rng.randint(100, 150),
                         len(self.sweat.registered_users))
        receivers = self.sweat.random_receivers(self.account_id, batch_size)
        tx = SweatMintBatch(self.sweat.account.key.account_id, self.oracle, [
            RecipientSteps(account_id, steps=rng.randint(1000, 3000))
            for account_id in receivers
        ])
        self.send_tx(tx, locust_name="Sweat record batch")

    @tag("storage-stress-test")
    @task
    def record_batch_of_large_batches(self):
        # create more sweat users to allow for a decent record_batch size
        while len(self.sweat.registered_users) < 1000:
            # creating 20 accounts in parallel, about 200 should fit in a chunk
            # but we don't want to assume we get to use all the chunk space for
            # ourself. Also, other SweatUsers will be in the same loop and at
            # some point the local CPU becomes a bottleneck, too.
            self.sweat.create_passive_users(
                20,
                self.node,
                self.account,
                # protocol enforced max length is 64 but we want shorter names to
                # not hit the log limits too soon
                max_account_id_len=48)

        rng = random.Random()
        # just around 300Tgas
        batch_size = rng.randint(700, 750)
        receivers = self.sweat.random_receivers(self.account_id, batch_size)
        tx = SweatMintBatch(
            self.sweat.account.key.account_id, self.oracle,
            [[account_id, rng.randint(1000, 3000)] for account_id in receivers])
        self.send_tx(tx, locust_name="Sweat record batch (stress test)")

    @tag("claim-test")
    @task
    def defer_batch(self):
        rng = random.Random()
        # just around the log limit
        batch_size = min(rng.randint(100, 150),
                         len(self.sweat.registered_users))
        receivers = self.sweat.random_receivers(self.account_id, batch_size)
        tx = SweatDeferBatch(
            self.sweat.account.key.account_id, self.oracle,
            self.sweat.claim.key.account_id, [
                RecipientSteps(account_id, steps=rng.randint(1000, 3000))
                for account_id in receivers
            ])
        self.send_tx(tx, locust_name="Sweat defer batch")

    def on_start(self):
        super().on_start()
        # We have one oracle account per worker. Sharing a single access key
        # means potential conflicts in nonces when we mint new tokens through
        # batches. Hence, let's add a new access key to the oracle account for
        # each sweat user.
        self.sweat = self.environment.sweat
        oracle = self.environment.sweat.oracle
        user_oracle_key = key.Key.from_random(oracle.key.account_id)
        self.send_tx_retry(AddFullAccessKey(oracle, user_oracle_key),
                           "add user key to oracle")
        self.oracle = Account(user_oracle_key)
        self.oracle.refresh_nonce(self.node.node)

        logger.debug(
            f"{self.account_id} ready to use Sweat contract {self.sweat.account.key.account_id}"
        )

class SweatUser(NearUser):
    """
    Registers itself on an FT contract in the setup phase, then does one of two things:
      - Sends Sweat to random users.
      - Checks its' balance in sweat.claim contract.
    """
    wait_time = between(1, 3)  # random pause between transactions

    @task(3)
    def ft_transfer(self):
        receiver = self.sweat.random_receiver(self.account_id)
        tx = TransferFT(self.sweat.account, self.account, receiver)
        self.send_tx(tx, locust_name="Sweat transfer")

    @tag("claim-test")
    @task
    def get_claimable_balance_for_account(self):
        tx = SweatGetClaimableBalanceForAccount(self.sweat.claim.key.account_id,
                                                self.account,
                                                self.account.key.account_id)
        self.send_tx(tx, locust_name="Sweat.claim get balance")

    def on_start(self):
        super().on_start()
        self.sweat = self.environment.sweat
        self.sweat.register_user(self)
        logger.debug(
            f"{self.account_id} ready to use Sweat contract {self.sweat.account.key.account_id}"
        )

'''
'''--- pytest/tests/loadtest/setup.py ---
import subprocess
import mocknet_helpers
import account
import key
import argparse
from os.path import join

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Setup loadtest')

    parser.add_argument('--home', type=str, required=True)
    parser.add_argument('--num_accounts', type=int, default=5)
    parser.add_argument('--host', type=str, default='127.0.0.1')
    parser.add_argument('--account_id', type=str, default=None)
    parser.add_argument('--contract_dir',
                        type=str,
                        default='pytest/tests/loadtest/contract')
    args = parser.parse_args()

    print("Compiling contract")
    subprocess.check_call(args=[
        "cargo", "build", "--target", "wasm32-unknown-unknown", "--release"
    ],
                          cwd=args.contract_dir)

    for i in range(args.num_accounts):
        account_name = args.account_id or f"shard{i}"

        shard_key = key.Key.from_json_file(
            join(args.home, f"{account_name}_key.json"))

        base_block_hash = mocknet_helpers.get_latest_block_hash(addr=args.host)
        nonce = mocknet_helpers.get_nonce_for_key(shard_key, addr=args.host)

        shard_account = account.Account(shard_key,
                                        init_nonce=nonce,
                                        base_block_hash=base_block_hash,
                                        rpc_infos=[(args.host, "3030")])

        shard_account.send_deploy_contract_tx(
            join(
                args.contract_dir,
                "target/wasm32-unknown-unknown/release/loadtest_contract.wasm"))

'''
'''--- pytest/tests/mocknet/__init__.py ---

'''
'''--- pytest/tests/mocknet/cmd_utils.py ---
import sys

LOG_DIR = '/home/ubuntu/logs'
STATUS_DIR = '/home/ubuntu/logs/status'

def run_cmd(node, cmd):
    r = node.machine.run(cmd)
    if r.exitcode != 0:
        sys.exit(
            f'failed running {cmd} on {node.instance_name}:\nstdout: {r.stdout}\nstderr: {r.stderr}'
        )
    return r

def run_in_background(node, cmd, log_filename, env='', pre_cmd=None):
    setup_cmd = f'truncate --size 0 {STATUS_DIR}/{log_filename} '
    setup_cmd += f'&& for i in {{8..0}}; do if [ -f {LOG_DIR}/{log_filename}.$i ]; then mv {LOG_DIR}/{log_filename}.$i {LOG_DIR}/{log_filename}.$((i+1)); fi done'
    if pre_cmd is not None:
        pre_cmd += ' && '
    else:
        pre_cmd = ''
    run_cmd(
        node,
        f'( {pre_cmd}{setup_cmd} && {env} nohup {cmd} > {LOG_DIR}/{log_filename}.0 2>&1; nohup echo "$?" ) > {STATUS_DIR}/{log_filename} 2>&1 &'
    )

def init_node(node):
    run_cmd(node, f'mkdir -p {LOG_DIR} && mkdir -p {STATUS_DIR}')

'''
'''--- pytest/tests/mocknet/helpers/__init__.py ---

'''
'''--- pytest/tests/mocknet/helpers/genesis_updater.py ---
#!/usr/bin/env python3
"""
Creates a genesis file from a template.
This file is uploaded to each mocknet node and run on the node, producing identical genesis files across all nodes.
This approach is significantly faster than the alternative, of uploading the genesis file to all mocknet nodes.
Currently testnet state is a 17GB json file, and uploading that file to 100 machines over a 1Gbit/s connection would
need at 4 hours.
"""

import os
import pathlib
import sys

# Don't use the pathlib magic because this file runs on a remote machine.
sys.path.append('lib')
import mocknet
from configured_logger import logger

def str_to_bool(arg):
    return arg.lower() == 'true'

def main(argv):
    logger.info(argv)
    assert len(argv) == 17

    genesis_filename_in = argv[1]
    records_filename_in = argv[2]
    config_filename_in = argv[3]
    out_dir = argv[4]

    chain_id = argv[5]
    validator_keys = None
    if argv[6]:
        validator_keys = dict(map(lambda x: x.split('='), argv[6].split(',')))
    rpc_node_names = None
    if argv[7]:
        rpc_node_names = argv[7].split(',')
    done_filename = argv[8]
    epoch_length = int(argv[9])
    node_pks = None
    if argv[10]:
        node_pks = argv[10].split(',')
    increasing_stakes = float(argv[11])
    num_seats = float(argv[12])
    single_shard = str_to_bool(argv[13])
    all_node_pks = None
    if argv[14]:
        all_node_pks = argv[14].split(',')
    node_ips = None
    if argv[15]:
        node_ips = argv[15].split(',')
    if argv[16].lower() == 'none':
        neard = None
    else:
        neard = argv[16]

    assert genesis_filename_in
    assert records_filename_in
    assert config_filename_in
    assert out_dir
    assert chain_id
    assert validator_keys
    assert done_filename
    assert epoch_length
    assert node_pks
    assert rpc_node_names
    assert num_seats
    assert all_node_pks
    assert node_ips

    mocknet.neard_amend_genesis(
        neard=neard,
        validator_keys=validator_keys,
        genesis_filename_in=genesis_filename_in,
        records_filename_in=records_filename_in,
        out_dir=out_dir,
        rpc_node_names=rpc_node_names,
        chain_id=chain_id,
        epoch_length=epoch_length,
        node_pks=node_pks,
        increasing_stakes=increasing_stakes,
        num_seats=num_seats,
        single_shard=single_shard,
    )
    config_filename_out = os.path.join(out_dir, 'config.json')
    mocknet.update_config_file(
        config_filename_in,
        config_filename_out,
        all_node_pks,
        node_ips,
    )

    logger.info(f'done_filename: {done_filename}')
    pathlib.Path(done_filename).write_text('DONE')

if __name__ == '__main__':
    main(sys.argv)

'''
'''--- pytest/tests/mocknet/helpers/load_test_spoon_helper.py ---
#!/usr/bin/env python3
"""
Generates transactions on a mocknet node.
This file is uploaded to each mocknet node and run there.
"""

import random
import sys
import time
import argparse
import requests

# Don't use the pathlib magic because this file runs on a remote machine.
sys.path.append('lib')
import account
import key as key_mod
import load_test_utils
import mocknet
import mocknet_helpers

from configured_logger import logger

def parse_args():
    parser = argparse.ArgumentParser(
        description='Generates transactions on a mocknet node.')
    parser.add_argument('--node-account-id', required=True, type=str)
    parser.add_argument('--rpc-nodes', required=True, type=str)
    parser.add_argument('--num-nodes', required=True, type=int)
    parser.add_argument('--max-tps', required=True, type=float)

    parser.add_argument('--test-timeout', type=int, default=12 * 60 * 60)
    parser.add_argument(
        '--contract-deploy-time',
        type=int,
        default=10 * mocknet.NUM_ACCOUNTS,
        help=
        'We need to slowly deploy contracts, otherwise we stall out the nodes',
    )

    return parser.parse_args()

def get_test_accounts_from_args(args):

    node_account_id = args.node_account_id
    rpc_nodes = args.rpc_nodes.split(',')
    num_nodes = args.num_nodes
    max_tps = args.max_tps

    logger.info(f'node_account_id: {node_account_id}')
    logger.info(f'rpc_nodes: {rpc_nodes}')
    logger.info(f'num_nodes: {num_nodes}')
    logger.info(f'max_tps: {max_tps}')

    node_account_key = key_mod.Key(
        node_account_id,
        mocknet.PUBLIC_KEY,
        mocknet.SECRET_KEY,
    )
    test_account_keys = [
        key_mod.Key(
            mocknet.load_testing_account_id(node_account_id, i),
            mocknet.PUBLIC_KEY,
            mocknet.SECRET_KEY,
        ) for i in range(mocknet.NUM_ACCOUNTS)
    ]

    base_block_hash = mocknet_helpers.get_latest_block_hash()

    rpc_infos = [(rpc_addr, mocknet_helpers.RPC_PORT) for rpc_addr in rpc_nodes]
    node_account = account.Account(
        node_account_key,
        mocknet_helpers.get_nonce_for_pk(
            node_account_key.account_id,
            node_account_key.pk,
        ),
        base_block_hash,
        rpc_infos=rpc_infos,
    )
    accounts = [
        account.Account(
            key,
            mocknet_helpers.get_nonce_for_pk(
                key.account_id,
                key.pk,
            ),
            base_block_hash,
            rpc_infos=rpc_infos,
        ) for key in test_account_keys
    ]
    max_tps_per_node = max_tps / num_nodes
    return load_test_utils.TestState(
        node_account,
        accounts,
        max_tps_per_node,
        rpc_infos,
    )

def main():
    logger.info(" ".join(sys.argv))

    args = parse_args()
    test_state = get_test_accounts_from_args(args)

    # Ensure load testing contract is deployed to all accounts before
    # starting to send random transactions (ensures we do not try to
    # call the contract before it is deployed).
    delay = args.contract_deploy_time / test_state.num_test_accounts()
    logger.info(f'Start deploying, delay between deployments: {delay}')
    assert delay >= 1

    time.sleep(random.random() * delay)
    start_time = time.monotonic()
    load_test_utils.init_ft(test_state.node_account)
    for i, account in enumerate(test_state.test_accounts):
        logger.info(f'Deploying contract for account {account.key.account_id}')
        mocknet_helpers.retry_and_ignore_errors(
            lambda: account.send_deploy_contract_tx(mocknet.WASM_FILENAME))
        load_test_utils.init_ft_account(test_state.node_account, account)
        balance = mocknet_helpers.retry_and_ignore_errors(
            lambda: account.get_amount_yoctonear())
        logger.info(
            f'Account {account.key.account_id} balance after initialization: {balance}'
        )
        time.sleep(max(1.0, start_time + (i + 1) * delay - time.monotonic()))

    logger.info('Done deploying')

    # begin with only transfers for TPS measurement
    total_tx_sent, elapsed_time = 0, 0
    logger.info(
        f'Start the test, expected TPS {test_state.max_tps_per_node} over the next {args.test_timeout} seconds'
    )
    last_staking = 0
    start_time = time.monotonic()
    while time.monotonic() - start_time < args.test_timeout:
        try:
            # Repeat the staking transactions in case the validator selection algorithm changes.
            staked_time = mocknet.stake_available_amount(
                test_state.node_account,
                last_staking,
            )
            if staked_time is not None:
                last_staking = staked_time

            elapsed_time = time.monotonic() - start_time
            total_tx_sent = mocknet_helpers.throttle_txns(
                load_test_utils.send_random_transactions,
                total_tx_sent,
                elapsed_time,
                test_state,
            )
        except (
                ConnectionRefusedError,
                requests.exceptions.ConnectionError,
        ) as e:
            # If this happens only occasionally the loop will retry immediately,
            # eventually pick a healthy rpc node and all will be fine. If it
            # happens every time, it indicates that something is wrong and
            # should be visible in grafana tx rate.
            logger.warning(
                f'Error when staking or sending tx. This may happen when the '
                f'selected RPC node is being upgraded. The exception is {e}')
    logger.info('Stop the test')

if __name__ == '__main__':
    main()

'''
'''--- pytest/tests/mocknet/helpers/load_test_utils.py ---
import random
import sys
import time

import base58
from rc import pmap

# Don't use the pathlib magic because this file runs on a remote machine.
sys.path.append('lib')
import mocknet_helpers
import mocknet
import transaction

from configured_logger import logger

class TestState:

    def __init__(
        self,
        node_account,
        test_accounts,
        max_tps_per_node,
        rpc_infos,
    ):
        self.node_account = node_account
        self.test_accounts = test_accounts
        self.max_tps_per_node = max_tps_per_node
        self.rpc_infos = rpc_infos
        self.function_call_state = [[]] * len(self.test_accounts)

    def random_account(self):
        return random.choice(self.test_accounts)

    def num_test_accounts(self):
        return len(self.test_accounts)

def send_transfer(account, test_state, base_block_hash=None):
    dest_account = test_state.random_account()
    amount = 1
    mocknet_helpers.retry_and_ignore_errors(
        lambda: account.send_transfer_tx(dest_account.key.account_id,
                                         transfer_amount=amount,
                                         base_block_hash=base_block_hash))
    logger.info(
        f'Account {account.key.account_id} transfers {amount} yoctoNear to {dest_account.key.account_id}'
    )

def function_call_set_state_then_delete_state(i,
                                              test_state,
                                              base_block_hash=None):
    if not test_state.function_call_state[i]:
        action = "add"
    elif len(test_state.function_call_state[i]) >= 100:
        action = "delete"
    else:
        action = random.choice(["add", "delete"])

    account = test_state.test_accounts[i]
    if action == "add":
        next_id = random.randrange(test_state.num_test_accounts())
        next_val = random.randint(0, 1000)
        next_account_id = mocknet.load_testing_account_id(
            test_state.node_account.key.account_id, next_id)
        s = f'{{"account_id": "account_{next_val}", "message":"{next_val}"}}'
        logger.info(
            f'Calling function "set_state" of account {next_account_id} with arguments {s} from account {account.key.account_id}'
        )
        tx_res = mocknet_helpers.retry_and_ignore_errors(
            lambda: account.send_call_contract_raw_tx(next_account_id,
                                                      'set_state',
                                                      s.encode('utf-8'),
                                                      0,
                                                      base_block_hash=
                                                      base_block_hash))
        logger.info(
            f'{account.key.account_id} set_state on {next_account_id} {tx_res}')
        test_state.function_call_state[i].append((next_id, next_val))
    else:
        assert test_state.function_call_state[i]
        item = random.choice(test_state.function_call_state[i])
        next_id, next_val = item
        next_account_id = mocknet.load_testing_account_id(
            test_state.node_account.key.account_id, next_id)
        s = f'{{"account_id": "account_{next_val}"}}'
        logger.info(
            f'Calling function "delete_state" of account {next_account_id} with arguments {s} from account {account.key.account_id}'
        )
        tx_res = mocknet_helpers.retry_and_ignore_errors(
            lambda: account.send_call_contract_raw_tx(next_account_id,
                                                      'delete_state',
                                                      s.encode('utf-8'),
                                                      0,
                                                      base_block_hash=
                                                      base_block_hash))
        logger.info(
            f'{account.key.account_id} delete_state on {next_account_id} {tx_res}'
        )
        if item in test_state.function_call_state[i]:
            test_state.function_call_state[i].remove(item)
            logger.info(
                f'Successfully removed {item} from function_call_state. New #items: {len(test_state.function_call_state[i])}'
            )
        else:
            logger.info(
                f'{item} is not in function_call_state even though this is impossible. #Items: {len(test_state.function_call_state[i])}'
            )

def function_call_ft_transfer_call(account, test_state, base_block_hash=None):
    dest_account = test_state.random_account()
    contract = test_state.node_account.key.account_id

    s = f'{{"receiver_id": "{dest_account.key.account_id}", "amount": "3", "msg": "\\"hi\\""}}'
    logger.info(
        f'Calling function "ft_transfer_call" with arguments {s} on account {account.key.account_id} contract {contract} with destination {dest_account.key.account_id}'
    )
    tx_res = mocknet_helpers.retry_and_ignore_errors(
        lambda: account.send_call_contract_raw_tx(contract,
                                                  'ft_transfer_call',
                                                  s.encode('utf-8'),
                                                  1,
                                                  base_block_hash=
                                                  base_block_hash))
    logger.info(
        f'{account.key.account_id} ft_transfer to {dest_account.key.account_id} {tx_res}'
    )

# See https://near.github.io/nearcore/architecture/how/meta-tx.html to understand what is going on.
# Alice pays the costs of Relayer sending 1 yoctoNear to Receiver
def meta_transaction_transfer(alice_account, test_state, base_block_hash,
                              base_block_height):
    relayer_account = test_state.random_account()
    receiver_account = test_state.random_account()

    yocto_near_amount = 1
    transfer_action = transaction.create_payment_action(yocto_near_amount)
    # Use (relayer_account.nonce + 2) as a nonce to deal with the case of Alice
    # and Relayer being the same account. The outer transaction needs to have
    # a lower nonce.
    # Make the delegated action valid for 10**6 blocks to avoid DelegateActionExpired errors.
    # Value of 10 should probably be enough.
    # DelegateAction is signed with the keys of the Relayer.
    signed_meta_tx = transaction.create_signed_delegated_action(
        relayer_account.key.account_id, receiver_account.key.account_id,
        [transfer_action], relayer_account.nonce + 2, base_block_height + 10**6,
        relayer_account.key.decoded_pk(), relayer_account.key.decoded_sk())

    # Outer transaction is signed with the keys of Alice.
    meta_tx = transaction.sign_delegate_action(signed_meta_tx,
                                               alice_account.key,
                                               relayer_account.key.account_id,
                                               alice_account.nonce + 1,
                                               base_block_hash)
    alice_account.send_tx(meta_tx)
    alice_account.nonce += 1
    relayer_account.nonce += 2

    logger.info(
        f'meta-transaction from {alice_account.key.account_id} to transfer {yocto_near_amount} yoctoNear from {relayer_account.key.account_id} to {receiver_account.key.account_id}'
    )

def random_transaction(i, test_state, base_block_hash, base_block_height):
    account = test_state.test_accounts[i]
    time.sleep(random.random() * test_state.num_test_accounts() /
               test_state.max_tps_per_node / 3)
    choice = random.randint(0, 3)
    if choice == 0:
        send_transfer(account, test_state, base_block_hash=base_block_hash)
    elif choice == 1:
        function_call_set_state_then_delete_state(
            i, test_state, base_block_hash=base_block_hash)
    elif choice == 2:
        function_call_ft_transfer_call(account,
                                       test_state,
                                       base_block_hash=base_block_hash)
    elif choice == 3:
        meta_transaction_transfer(account, test_state, base_block_hash,
                                  base_block_height)

def send_random_transactions(test_state):
    logger.info("===========================================")
    logger.info("New iteration of 'send_random_transactions'")
    addr = random.choice(test_state.rpc_infos)[0]
    status = mocknet_helpers.get_status(addr=addr)
    base_block_hash = base58.b58decode(
        status['sync_info']['latest_block_hash'].encode('utf-8'))
    base_block_height = status['sync_info']['latest_block_height']
    pmap(
        lambda index_and_account: random_transaction(
            index_and_account[0],
            test_state,
            base_block_hash=base_block_hash,
            base_block_height=base_block_height),
        enumerate(test_state.test_accounts))

def init_ft(node_account):
    tx_res = node_account.send_deploy_contract_tx(
        '/home/ubuntu/fungible_token.wasm')
    logger.info(f'ft deployment {tx_res}')
    mocknet_helpers.wait_at_least_one_block()

    s = f'{{"owner_id": "{node_account.key.account_id}", "total_supply": "{10**33}"}}'
    tx_res = node_account.send_call_contract_raw_tx(node_account.key.account_id,
                                                    'new_default_meta',
                                                    s.encode('utf-8'), 0)
    logger.info(f'ft new_default_meta {tx_res}')

def init_ft_account(node_account, account):
    s = f'{{"account_id": "{account.key.account_id}"}}'
    tx_res = account.send_call_contract_raw_tx(node_account.key.account_id,
                                               'storage_deposit',
                                               s.encode('utf-8'),
                                               (10**24) // 800)
    logger.info(f'Account {account.key.account_id} storage_deposit {tx_res}')

    # The next transaction depends on the previous transaction succeeded.
    # Sleeping for 1 second is the poor man's solution for waiting for that transaction to succeed.
    # This works because the contracts are being deployed slow enough to keep block production above 1 bps.
    mocknet_helpers.wait_at_least_one_block()

    s = f'{{"receiver_id": "{account.key.account_id}", "amount": "{10**18}"}}'
    logger.info(
        f'Calling function "ft_transfer" with arguments {s} on account {account.key.account_id}'
    )
    tx_res = node_account.send_call_contract_raw_tx(node_account.key.account_id,
                                                    'ft_transfer',
                                                    s.encode('utf-8'), 1)
    logger.info(
        f'{node_account.key.account_id} ft_transfer to {account.key.account_id} {tx_res}'
    )

'''
'''--- pytest/tests/mocknet/helpers/neard_runner.py ---
# TODO: reimplement this logic using standard tooling like systemd instead of relying on this
# python script to handle neard process management.

import argparse
import datetime
from enum import Enum
import fcntl
import json
import jsonrpc
import logging
import os
import psutil
import re
import requests
import shutil
import signal
import socket
import subprocess
import sys
import threading
import time
import http
import http.server

def get_lock(home):
    lock_file = os.path.join(home, 'LOCK')

    fd = os.open(lock_file, os.O_CREAT | os.O_RDWR)
    try:
        fcntl.flock(fd, fcntl.LOCK_EX | fcntl.LOCK_NB)
    except BlockingIOError:
        raise Exception(f'{lock_file} is currently locked by another process')
    return fd

class JSONHandler(http.server.BaseHTTPRequestHandler):

    def __init__(self, request, client_address, server):
        self.dispatcher = jsonrpc.Dispatcher()
        self.dispatcher.add_method(server.neard_runner.do_new_test,
                                   name="new_test")
        self.dispatcher.add_method(server.neard_runner.do_network_init,
                                   name="network_init")
        self.dispatcher.add_method(server.neard_runner.do_update_config,
                                   name="update_config")
        self.dispatcher.add_method(server.neard_runner.do_ready, name="ready")
        self.dispatcher.add_method(server.neard_runner.do_version,
                                   name="version")
        self.dispatcher.add_method(server.neard_runner.do_start, name="start")
        self.dispatcher.add_method(server.neard_runner.do_stop, name="stop")
        self.dispatcher.add_method(server.neard_runner.do_reset, name="reset")
        self.dispatcher.add_method(server.neard_runner.do_update_binaries,
                                   name="update_binaries")
        self.dispatcher.add_method(server.neard_runner.do_make_backup,
                                   name="make_backup")
        self.dispatcher.add_method(server.neard_runner.do_ls_backups,
                                   name="ls_backups")
        super().__init__(request, client_address, server)

    def do_GET(self):
        if self.path == '/status':
            body = 'OK\n'.encode('UTF-8')
            self.send_response(http.HTTPStatus.OK)
            self.send_header("Content-Type", 'application/json')
            self.send_header("Content-Length", str(len(body)))
            self.end_headers()
            self.wfile.write(body)
        else:
            self.send_error(http.HTTPStatus.NOT_FOUND)

    def do_POST(self):
        l = self.headers.get('content-length')
        if l is None:
            self.send_error(http.HTTPStatus.BAD_REQUEST,
                            "Content-Length missing")
            return

        body = self.rfile.read(int(l))
        response = jsonrpc.JSONRPCResponseManager.handle(body, self.dispatcher)
        response_body = response.json.encode('UTF-8')

        self.send_response(http.HTTPStatus.OK)
        self.send_header("Content-Type", 'application/json')
        self.send_header("Content-Length", str(len(response_body)))
        self.end_headers()
        self.wfile.write(response_body)

class RpcServer(http.server.HTTPServer):

    def __init__(self, addr, neard_runner):
        self.neard_runner = neard_runner
        super().__init__(addr, JSONHandler)

class TestState(Enum):
    NONE = 1
    AWAITING_NETWORK_INIT = 2
    AMEND_GENESIS = 3
    STATE_ROOTS = 4
    RUNNING = 5
    STOPPED = 6
    RESETTING = 7
    ERROR = 8
    MAKING_BACKUP = 9
    SET_VALIDATORS = 10

backup_id_pattern = re.compile(r'^[0-9a-zA-Z.][0-9a-zA-Z_\-.]+$')

class NeardRunner:

    def __init__(self, args):
        self.home = args.home
        self.neard_home = args.neard_home
        self.neard_logs_dir = args.neard_logs_dir
        try:
            os.mkdir(self.neard_logs_dir)
        except FileExistsError:
            pass
        with open(self.home_path('config.json'), 'r') as f:
            self.config = json.load(f)
        self.neard = None
        self.num_restarts = 0
        self.last_start = time.time()
        # self.data will contain data that we want to persist so we can
        # start where we left off if this process is killed and restarted
        # for now we save info on the neard binaries (their paths and epoch
        # heights where we want to run them), a bool that tells whether neard
        # should be running, and info on the currently running neard process
        try:
            with open(self.home_path('data.json'), 'r') as f:
                self.data = json.load(f)
                self.data['binaries'].sort(key=lambda x: x['epoch_height'])
        except FileNotFoundError:
            self.data = {
                'binaries': [],
                'neard_process': None,
                'current_neard_path': None,
                'state': TestState.NONE.value,
                'backups': {},
                'state_data': None,
            }
        self.legacy_records = self.is_legacy()
        # protects self.data, and its representation on disk,
        # because both the rpc server and the main loop touch them concurrently
        # TODO: consider locking the TestState variable separately, since there
        # is no need to block reading that when inside the update_binaries rpc for example
        self.lock = threading.Lock()

    def is_legacy(self):
        if os.path.exists(os.path.join(self.neard_home, 'setup', 'data')):
            if os.path.exists(
                    os.path.join(self.neard_home, 'setup', 'records.json')):
                logging.warning(
                    f'found both records.json and data/ in {os.path.join(self.neard_home, "setup")}'
                )
            return False
        if os.path.exists(os.path.join(
                self.neard_home, 'setup', 'records.json')) and os.path.exists(
                    os.path.join(self.neard_home, 'setup', 'genesis.json')):
            return True
        sys.exit(
            f'did not find either records.json and genesis.json or data/ in {os.path.join(self.neard_home, "setup")}'
        )

    def is_traffic_generator(self):
        return self.config.get('is_traffic_generator', False)

    def save_data(self):
        with open(self.home_path('data.json'), 'w') as f:
            json.dump(self.data, f)

    def parse_binaries_config(self):
        if 'binaries' not in self.config:
            raise ValueError('config does not have a "binaries" section')

        if not isinstance(self.config['binaries'], list):
            raise ValueError('config "binaries" section not a list')

        if len(self.config['binaries']) == 0:
            raise ValueError('no binaries in the config')

        self.config['binaries'].sort(key=lambda x: x['epoch_height'])
        last_epoch_height = -1

        binaries = []
        for i, b in enumerate(self.config['binaries']):
            epoch_height = b['epoch_height']
            if not isinstance(epoch_height, int) or epoch_height < 0:
                raise ValueError(f'bad epoch height in config: {epoch_height}')
            if last_epoch_height == -1:
                if epoch_height != 0:
                    # TODO: maybe it could make sense to allow this, meaning don't run any binary
                    # on this node until the network reaches that epoch, then we bring this node online
                    raise ValueError(
                        f'config should contain one binary with epoch_height 0')
            else:
                if epoch_height == last_epoch_height:
                    raise ValueError(
                        f'repeated epoch height in config: {epoch_height}')
            last_epoch_height = epoch_height
            binaries.append({
                'url': b['url'],
                'epoch_height': b['epoch_height'],
                'system_path': self.home_path('binaries', f'neard{i}')
            })
        return binaries

    def set_current_neard_path(self, path):
        self.data['current_neard_path'] = path

    def reset_current_neard_path(self):
        self.set_current_neard_path(self.data['binaries'][0]['system_path'])

    # tries to download the binaries specified in config.json, saving them in $home/binaries/
    # if force is set to true all binaries will be downloaded, otherwise only the missing ones
    def download_binaries(self, force):
        binaries = self.parse_binaries_config()

        try:
            os.mkdir(self.home_path('binaries'))
        except FileExistsError:
            pass

        if force:
            # always start from start_index = 0 and download all binaries
            self.data['binaries'] = []

        # start at the index of the first missing binary
        # typically it's all or nothing
        start_index = len(self.data['binaries'])

        # for now we assume that the binaries recorded in data.json as having been
        # dowloaded are still valid and were not touched. Also this assumes that their
        # filenames are neard0, neard1, etc. in the right order and with nothing skipped
        for i in range(start_index, len(binaries)):
            b = binaries[i]
            logging.info(f'downloading binary from {b["url"]}')
            with open(b['system_path'], 'wb') as f:
                r = requests.get(b['url'], stream=True)
                r.raise_for_status()
                for chunk in r.iter_content(chunk_size=8192):
                    f.write(chunk)
            os.chmod(b['system_path'], 0o755)
            logging.info(f'downloaded binary from {b["url"]}')

            self.data['binaries'].append(b)
            if self.data['current_neard_path'] is None:
                self.reset_current_neard_path()
            self.save_data()

    def target_near_home_path(self, *args):
        if self.is_traffic_generator():
            args = ('target',) + args
        return os.path.join(self.neard_home, *args)

    def home_path(self, *args):
        return os.path.join(self.home, *args)

    def tmp_near_home_path(self, *args):
        args = ('tmp-near-home',) + args
        return os.path.join(self.home, *args)

    def neard_init(self, rpc_port, protocol_port, validator_id):
        # We make neard init save files to self.tmp_near_home_path() just to make it
        # a bit cleaner, so we can init to a non-existent directory and then move
        # the files we want to the real near home without having to remove it first
        cmd = [
            self.data['binaries'][0]['system_path'], '--home',
            self.tmp_near_home_path(), 'init'
        ]
        if not self.is_traffic_generator():
            if validator_id is None:
                validator_id = f'{socket.gethostname()}.near'
            cmd += ['--account-id', validator_id]
        else:
            if validator_id is not None:
                logging.warning(
                    f'ignoring validator ID "{validator_id}" for traffic generator node'
                )
        subprocess.check_call(cmd)

        with open(self.tmp_near_home_path('config.json'), 'r') as f:
            config = json.load(f)
        config['rpc']['addr'] = f'0.0.0.0:{rpc_port}'
        config['network']['addr'] = f'0.0.0.0:{protocol_port}'
        self.data['neard_addr'] = config['rpc']['addr']
        config['tracked_shards'] = [0, 1, 2, 3]
        config['log_summary_style'] = 'plain'
        config['network']['skip_sync_wait'] = False
        if self.legacy_records:
            config['genesis_records_file'] = 'records.json'
        config['rpc']['enable_debug_rpc'] = True
        config['consensus']['min_block_production_delay']['secs'] = 1
        config['consensus']['min_block_production_delay']['nanos'] = 300000000
        config['consensus']['max_block_production_delay']['secs'] = 3
        config['consensus']['max_block_production_delay']['nanos'] = 0
        if self.is_traffic_generator():
            config['archive'] = True
        with open(self.tmp_near_home_path('config.json'), 'w') as f:
            json.dump(config, f, indent=2)

    def reset_starting_data_dir(self):
        try:
            shutil.rmtree(self.target_near_home_path('data'))
        except FileNotFoundError:
            pass
        if not self.legacy_records:
            cmd = [
                self.data['binaries'][0]['system_path'],
                '--home',
                os.path.join(self.neard_home, 'setup'),
                'database',
                'make-snapshot',
                '--destination',
                self.target_near_home_path(),
            ]
            logging.info(f'running {" ".join(cmd)}')
            subprocess.check_call(cmd)

    def move_init_files(self):
        try:
            os.mkdir(self.target_near_home_path())
        except FileExistsError:
            pass
        for p in os.listdir(self.target_near_home_path()):
            filename = self.target_near_home_path(p)
            if os.path.isfile(filename):
                os.remove(filename)
        self.reset_starting_data_dir()

        paths = ['config.json', 'node_key.json']
        if not self.is_traffic_generator():
            paths.append('validator_key.json')
        for path in paths:
            shutil.move(self.tmp_near_home_path(path),
                        self.target_near_home_path(path))
        if not self.legacy_records:
            shutil.copyfile(
                os.path.join(self.neard_home, 'setup', 'genesis.json'),
                self.target_near_home_path('genesis.json'))

    # This RPC method tells to stop neard and re-initialize its home dir. This returns the
    # validator and node key that resulted from the initialization. We can't yet call amend-genesis
    # and compute state roots, because the caller of this method needs to hear back from
    # each node before it can build the list of initial validators. So after this RPC method returns,
    # we'll be waiting for the network_init RPC.
    # TODO: add a binaries argument that tells what binaries we want to use in the test. Before we do
    # this, it is pretty mandatory to implement some sort of client authentication, because without it,
    # anyone would be able to get us to download and run arbitrary code
    def do_new_test(self,
                    rpc_port=3030,
                    protocol_port=24567,
                    validator_id=None):
        if not isinstance(rpc_port, int):
            raise jsonrpc.exceptions.JSONRPCDispatchException(
                code=-32600, message='rpc_port argument not an int')
        if not isinstance(protocol_port, int):
            raise jsonrpc.exceptions.JSONRPCDispatchException(
                code=-32600, message='protocol_port argument not an int')
        if validator_id is not None and not isinstance(validator_id, str):
            raise jsonrpc.exceptions.JSONRPCDispatchException(
                code=-32600, message='validator_id argument not a string')

        with self.lock:
            self.kill_neard()
            try:
                shutil.rmtree(self.tmp_near_home_path())
            except FileNotFoundError:
                pass
            try:
                os.remove(self.home_path('validators.json'))
            except FileNotFoundError:
                pass
            try:
                os.remove(self.home_path('network_init.json'))
            except FileNotFoundError:
                pass

            self.neard_init(rpc_port, protocol_port, validator_id)
            self.move_init_files()

            with open(self.target_near_home_path('config.json'), 'r') as f:
                config = json.load(f)
            with open(self.target_near_home_path('node_key.json'), 'r') as f:
                node_key = json.load(f)
            if not self.is_traffic_generator():
                with open(self.target_near_home_path('validator_key.json'),
                          'r') as f:
                    validator_key = json.load(f)
                    validator_account_id = validator_key['account_id']
                    validator_public_key = validator_key['public_key']
            else:
                validator_account_id = None
                validator_public_key = None

            self.data['backups'] = {}
            self.set_state(TestState.AWAITING_NETWORK_INIT)
            self.save_data()

            return {
                'validator_account_id': validator_account_id,
                'validator_public_key': validator_public_key,
                'node_key': node_key['public_key'],
                'listen_port': config['network']['addr'].split(':')[1],
            }

    # After the new_test RPC, we wait to get this RPC that gives us the list of validators
    # and boot nodes for the test network. After this RPC call, we run amend-genesis and
    # start neard to compute genesis state roots.
    def do_network_init(self,
                        validators,
                        boot_nodes,
                        epoch_length=1000,
                        num_seats=100,
                        protocol_version=None,
                        genesis_time=None):
        if not isinstance(validators, list):
            raise jsonrpc.exceptions.JSONRPCDispatchException(
                code=-32600, message='validators argument not a list')
        if not isinstance(boot_nodes, list):
            raise jsonrpc.exceptions.JSONRPCDispatchException(
                code=-32600, message='boot_nodes argument not a list')

        # TODO: maybe also check validity of these arguments?
        if len(validators) == 0:
            raise jsonrpc.exceptions.JSONRPCDispatchException(
                code=-32600, message='validators argument must not be empty')
        if len(boot_nodes) == 0:
            raise jsonrpc.exceptions.JSONRPCDispatchException(
                code=-32600, message='boot_nodes argument must not be empty')

        if not self.legacy_records and genesis_time is None:
            raise jsonrpc.exceptions.JSONRPCDispatchException(
                code=-32600,
                message=
                'genesis_time argument required for nodes running via neard fork-network'
            )

        with self.lock:
            state = self.get_state()
            if state != TestState.AWAITING_NETWORK_INIT:
                raise jsonrpc.exceptions.JSONRPCDispatchException(
                    code=-32600,
                    message='Can only call network_init after a call to init')

            if len(validators) < 3:
                with open(self.target_near_home_path('config.json'), 'r') as f:
                    config = json.load(f)
                config['consensus']['min_num_peers'] = len(validators) - 1
                with open(self.target_near_home_path('config.json'), 'w') as f:
                    json.dump(config, f)
            with open(self.home_path('validators.json'), 'w') as f:
                json.dump(validators, f)
            with open(self.home_path('network_init.json'), 'w') as f:
                json.dump(
                    {
                        'boot_nodes': boot_nodes,
                        'epoch_length': epoch_length,
                        'num_seats': num_seats,
                        'protocol_version': protocol_version,
                        'genesis_time': genesis_time,
                    }, f)

    def do_update_config(self, key_value):
        with self.lock:
            logging.info(f'updating config with {key_value}')
            with open(self.target_near_home_path('config.json'), 'r') as f:
                config = json.load(f)

            [key, value] = key_value.split("=", 1)
            key_item_list = key.split(".")

            object = config
            for key_item in key_item_list[:-1]:
                if key_item not in object:
                    object[key_item] = {}
                object = object[key_item]

            value = json.loads(value)

            object[key_item_list[-1]] = value

            with open(self.target_near_home_path('config.json'), 'w') as f:
                json.dump(config, f, indent=2)

        return True

    def do_start(self, batch_interval_millis=None):
        if batch_interval_millis is not None and not isinstance(
                batch_interval_millis, int):
            raise ValueError(
                f'batch_interval_millis: {batch_interval_millis} not an int')
        with self.lock:
            state = self.get_state()
            if state == TestState.STOPPED:
                if batch_interval_millis is not None and not self.is_traffic_generator(
                ):
                    logging.warn(
                        f'got batch_interval_millis = {batch_interval_millis} on non traffic generator node. Ignoring it.'
                    )
                    batch_interval_millis = None
                # TODO: restart it if we get a different batch_interval_millis than last time
                self.start_neard(batch_interval_millis)
                self.set_state(TestState.RUNNING)
                self.save_data()
            elif state != TestState.RUNNING:
                raise jsonrpc.exceptions.JSONRPCDispatchException(
                    code=-32600,
                    message=
                    'Cannot start node as test state has not been initialized yet'
                )

    # right now only has an effect if the test setup has been initialized. Should it also mean stop setting up
    # the test if we're in the middle of initializing it?
    def do_stop(self):
        with self.lock:
            state = self.get_state()
            if state == TestState.RUNNING:
                self.kill_neard()
                self.set_state(TestState.STOPPED)
                self.save_data()

    def do_reset(self, backup_id=None):
        with self.lock:
            state = self.get_state()
            logging.info(f"do_reset {state}")
            if state != TestState.RUNNING and state != TestState.STOPPED:
                raise jsonrpc.exceptions.JSONRPCDispatchException(
                    code=-32600,
                    message='Cannot reset data dir as test state is not ready')

            backups = self.data.get('backups', {})
            if backup_id is not None and backup_id != 'start' and backup_id not in backups:
                raise jsonrpc.exceptions.JSONRPCDispatchException(
                    code=-32600, message=f'backup ID {backup_id} not known')

            if backup_id is None or backup_id == 'start':
                path = self.data['binaries'][0]['system_path']
            else:
                path = backups[backup_id]['neard_path']

            if state == TestState.RUNNING:
                self.kill_neard()
            self.set_state(TestState.RESETTING, data=backup_id)
            self.set_current_neard_path(path)
            self.save_data()

    def do_make_backup(self, backup_id, description=None):
        with self.lock:
            state = self.get_state()
            if state != TestState.RUNNING and state != TestState.STOPPED:
                raise jsonrpc.exceptions.JSONRPCDispatchException(
                    code=-32600,
                    message='Cannot make backup as test state is not ready')

            if backup_id_pattern.match(backup_id) is None:
                raise jsonrpc.exceptions.JSONRPCDispatchException(
                    code=-32600, message=f'invalid backup ID: {backup_id}')

            if backup_id in self.data.get('backups', {}):
                raise jsonrpc.exceptions.JSONRPCDispatchException(
                    code=-32600, message=f'backup {backup_id} already exists')
            if state == TestState.RUNNING:
                self.kill_neard()
            self.making_backup(backup_id, description)
            self.save_data()

    def do_ls_backups(self):
        with self.lock:
            return self.data.get('backups', {})

    def do_update_binaries(self):
        with self.lock:
            logging.info('update binaries')
            try:
                self.download_binaries(force=True)
            except ValueError as e:
                raise jsonrpc.exceptions.JSONRPCDispatchException(
                    code=-32603,
                    message=f'Internal error downloading binaries: {e}')
                self.set_state(TestState.ERROR)
                self.save_data()
            logging.info('update binaries finished')

    def do_version(self):
        if self.legacy_records:
            node_setup_version = '0'
        else:
            node_setup_version = '1'
        return {'node_setup_version': node_setup_version}

    def do_ready(self):
        with self.lock:
            state = self.get_state()
            return state == TestState.RUNNING or state == TestState.STOPPED

    # check the current epoch height, and return the binary path that we should
    # be running given the epoch heights specified in config.json
    # TODO: should we update it at a random time in the middle of the
    # epoch instead of the beginning?
    def wanted_neard_path(self):
        j = {
            'method': 'validators',
            'params': [None],
            'id': 'dontcare',
            'jsonrpc': '2.0'
        }
        try:
            r = requests.post(f'http://{self.data["neard_addr"]}',
                              json=j,
                              timeout=5)
            r.raise_for_status()
            response = json.loads(r.content)
            epoch_height = response['result']['epoch_height']
            path = self.data['binaries'][0]['system_path']
            for b in self.data['binaries']:
                # this logic assumes the binaries are sorted by epoch height
                if b['epoch_height'] <= epoch_height:
                    path = b['system_path']
                else:
                    break
            return path
        except (requests.exceptions.ConnectionError,
                requests.exceptions.ReadTimeout, KeyError):
            return self.data['current_neard_path']

    def run_neard(self, cmd, out_file=None):
        assert (self.neard is None)
        assert (self.data['neard_process'] is None)
        env = os.environ.copy()
        if 'RUST_LOG' not in env:
            env['RUST_LOG'] = 'actix_web=warn,mio=warn,tokio_util=warn,actix_server=warn,actix_http=warn,indexer=info,debug'
        logging.info(f'running {" ".join(cmd)}')
        self.neard = subprocess.Popen(
            cmd,
            stdin=subprocess.DEVNULL,
            stdout=out_file,
            stderr=out_file,
            env=env,
        )
        # we save the create_time so we can tell if the process with pid equal
        # to the one we saved is the same process. It's not that likely, but
        # if we don't do this (or maybe something else like it), then it's possble
        # that if this process is killed and restarted and we see a process with that PID,
        # it could actually be a different process that was started later after neard exited
        try:
            create_time = int(psutil.Process(self.neard.pid).create_time())
        except psutil.NoSuchProcess:
            # not that likely, but if it already exited, catch the exception so
            # at least this process doesn't die
            create_time = 0

        self.data['neard_process'] = {
            'pid': self.neard.pid,
            'create_time': create_time,
            'path': cmd[0],
        }
        self.save_data()

    def poll_neard(self):
        if self.neard is not None:
            code = self.neard.poll()
            path = self.data['neard_process']['path']
            running = code is None
            if not running:
                self.neard = None
                self.data['neard_process'] = None
                self.save_data()
            return path, running, code
        elif self.data['neard_process'] is not None:
            path = self.data['neard_process']['path']
            # we land here if this process previously died and is now restarted,
            # and the old neard process is still running
            try:
                p = psutil.Process(self.data['neard_process']['pid'])
                if int(p.create_time()
                      ) == self.data['neard_process']['create_time']:
                    return path, True, None
            except psutil.NoSuchProcess:
                self.neard = None
                self.data['neard_process'] = None
                self.save_data()
                return path, False, None
        else:
            return None, False, None

    def kill_neard(self):
        if self.neard is not None:
            logging.info('stopping neard')
            self.neard.send_signal(signal.SIGINT)
            self.neard.wait()
            self.neard = None
            self.data['neard_process'] = None
            self.save_data()
            return

        if self.data['neard_process'] is None:
            return

        try:
            p = psutil.Process(self.data['neard_process']['pid'])
            if int(p.create_time()
                  ) == self.data['neard_process']['create_time']:
                logging.info('stopping neard')
                p.send_signal(signal.SIGINT)
                p.wait()
        except psutil.NoSuchProcess:
            pass
        self.neard = None
        self.data['neard_process'] = None
        self.save_data()

    # If this is a regular node, starts neard run. If it's a traffic generator, starts neard mirror run
    def start_neard(self, batch_interval_millis=None):
        for i in range(20, -1, -1):
            old_log = os.path.join(self.neard_logs_dir, f'log-{i}.txt')
            new_log = os.path.join(self.neard_logs_dir, f'log-{i+1}.txt')
            try:
                os.rename(old_log, new_log)
            except FileNotFoundError:
                pass

        with open(os.path.join(self.neard_logs_dir, 'log-0.txt'), 'ab') as out:
            if self.is_traffic_generator():
                cmd = [
                    self.data['current_neard_path'],
                    'mirror',
                    'run',
                    '--source-home',
                    self.neard_home,
                    '--target-home',
                    self.target_near_home_path(),
                    '--no-secret',
                ]
                if batch_interval_millis is not None:
                    with open(self.target_near_home_path('mirror-config.json'),
                              'w') as f:
                        secs = batch_interval_millis // 1000
                        nanos = (batch_interval_millis % 1000) * 1000000
                        json.dump(
                            {
                                'tx_batch_interval': {
                                    'secs': secs,
                                    'nanos': nanos
                                }
                            },
                            f,
                            indent=2)
                    cmd.append('--config-path')
                    cmd.append(self.target_near_home_path('mirror-config.json'))
            else:
                cmd = [
                    self.data['current_neard_path'], '--log-span-events',
                    '--home', self.neard_home, '--unsafe-fast-startup', 'run'
                ]
            self.run_neard(
                cmd,
                out_file=out,
            )
            self.last_start = time.time()

    # returns a bool that tells whether we should attempt a restart
    def on_neard_died(self):
        if self.is_traffic_generator():
            # TODO: This is just a lazy way to deal with the fact
            # that the mirror command is expected to exit after it finishes sending all the traffic.
            # For now just don't restart neard on the traffic generator. Here we should be smart
            # about restarting only if it makes sense, and we also shouldn't restart over and over.
            # Right now we can't just check the exit_code because there's a bug that makes the
            # neard mirror run command not exit cleanly when it's finished
            return False

        now = time.time()
        if now - self.last_start > 600:
            self.num_restarts = 1
            return True
        if self.num_restarts >= 5:
            self.set_state(TestState.STOPPED)
            self.save_data()
            return False
        else:
            self.num_restarts += 1
            return True

    def check_upgrade_neard(self):
        neard_path = self.wanted_neard_path()

        path, running, exit_code = self.poll_neard()
        if path is None:
            start_neard = self.on_neard_died()
        elif not running:
            if exit_code is not None:
                logging.info(f'neard exited with code {exit_code}.')
            start_neard = self.on_neard_died()
        else:
            if path == neard_path:
                start_neard = False
            else:
                logging.info('upgrading neard upon new epoch')
                self.kill_neard()
                start_neard = True

        if start_neard:
            self.set_current_neard_path(neard_path)
            self.start_neard()

    def get_state(self):
        return TestState(self.data['state'])

    def set_state(self, state, data=None):
        self.data['state'] = state.value
        self.data['state_data'] = data

    def making_backup(self, backup_id, description=None):
        backup_data = {'backup_id': backup_id, 'description': description}
        self.set_state(TestState.MAKING_BACKUP, data=backup_data)

    def network_init(self):
        # wait til we get a network_init RPC
        if not os.path.exists(self.home_path('validators.json')):
            return

        with open(self.home_path('network_init.json'), 'r') as f:
            n = json.load(f)
        with open(self.target_near_home_path('node_key.json'), 'r') as f:
            node_key = json.load(f)
        with open(self.target_near_home_path('config.json'), 'r') as f:
            config = json.load(f)
        boot_nodes = []
        for b in n['boot_nodes']:
            if node_key['public_key'] != b.split('@')[0]:
                boot_nodes.append(b)

        config['network']['boot_nodes'] = ','.join(boot_nodes)
        with open(self.target_near_home_path('config.json'), 'w') as f:
            config = json.dump(config, f, indent=2)

        if self.legacy_records:
            cmd = [
                self.data['binaries'][0]['system_path'],
                'amend-genesis',
                '--genesis-file-in',
                os.path.join(self.neard_home, 'setup', 'genesis.json'),
                '--records-file-in',
                os.path.join(self.neard_home, 'setup', 'records.json'),
                '--genesis-file-out',
                self.target_near_home_path('genesis.json'),
                '--records-file-out',
                self.target_near_home_path('records.json'),
                '--validators',
                self.home_path('validators.json'),
                '--chain-id',
                'mocknet',
                '--transaction-validity-period',
                '10000',
                '--epoch-length',
                str(n['epoch_length']),
                '--num-seats',
                str(n['num_seats']),
            ]
            if n['protocol_version'] is not None:
                cmd.append('--protocol-version')
                cmd.append(str(n['protocol_version']))

            self.run_neard(cmd)
            self.set_state(TestState.AMEND_GENESIS)
        else:
            cmd = [
                self.data['binaries'][0]['system_path'], '--home',
                self.target_near_home_path(), 'fork-network', 'set-validators',
                '--validators',
                self.home_path('validators.json'), '--chain-id-suffix',
                '_mocknet', '--epoch-length',
                str(n['epoch_length']), '--genesis-time',
                str(n['genesis_time'])
            ]

            self.run_neard(cmd)
            self.set_state(TestState.SET_VALIDATORS)
        self.save_data()

    def check_set_validators(self):
        path, running, exit_code = self.poll_neard()
        if path is None:
            logging.error(
                'state is SET_VALIDATORS, but no amend-genesis process is known'
            )
            self.set_state(TestState.AWAITING_NETWORK_INIT)
            self.save_data()
        elif not running:
            if exit_code is not None and exit_code != 0:
                logging.error(
                    f'neard fork-network set-validators exited with code {exit_code}'
                )
                # for now just set the state to ERROR, and if this ever happens, the
                # test operator will have to intervene manually. Probably shouldn't
                # really happen in practice
                self.set_state(TestState.ERROR)
                self.save_data()
            else:
                cmd = [
                    self.data['binaries'][0]['system_path'],
                    '--home',
                    self.target_near_home_path(),
                    'fork-network',
                    'finalize',
                ]
                logging.info(f'running {" ".join(cmd)}')
                subprocess.check_call(cmd)
                logging.info(
                    f'neard fork-network finalize succeeded. Node is ready')
                self.make_initial_backup()

    def check_amend_genesis(self):
        path, running, exit_code = self.poll_neard()
        if path is None:
            logging.error(
                'state is AMEND_GENESIS, but no amend-genesis process is known')
            self.set_state(TestState.AWAITING_NETWORK_INIT)
            self.save_data()
        elif not running:
            if exit_code is not None and exit_code != 0:
                logging.error(
                    f'neard amend-genesis exited with code {exit_code}')
                # for now just set the state to ERROR, and if this ever happens, the
                # test operator will have to intervene manually. Probably shouldn't
                # really happen in practice
                self.set_state(TestState.ERROR)
                self.save_data()
            else:
                # TODO: if exit_code is None then we were interrupted and restarted after starting
                # the amend-genesis command. We assume here that the command was successful. Ok for now since
                # the command probably won't fail. But should somehow check that it was OK

                logging.info('setting use_production_config to true')
                genesis_path = self.target_near_home_path('genesis.json')
                with open(genesis_path, 'r') as f:
                    genesis_config = json.load(f)
                with open(genesis_path, 'w') as f:
                    genesis_config['use_production_config'] = True
                    # with the normal min_gas_price (10x higher than this one)
                    # many mirrored mainnet transactions fail with too little balance
                    # One way to fix that would be to increase everybody's balances in
                    # the amend-genesis command. But we can also just make this change here.
                    genesis_config['min_gas_price'] = 10000000
                    # protocol_versions in range [56, 63] need to have these
                    # genesis parameters, otherwise nodes get stuck because at
                    # some point it produces an incompatible EpochInfo.
                    # TODO: remove these changes once mocknet tests will probably
                    # only ever be run with binaries including https://github.com/near/nearcore/pull/10722
                    genesis_config['num_block_producer_seats'] = 100
                    genesis_config['num_block_producer_seats_per_shard'] = [
                        100, 100, 100, 100
                    ]
                    genesis_config['block_producer_kickout_threshold'] = 80
                    genesis_config['chunk_producer_kickout_threshold'] = 80
                    genesis_config['shard_layout'] = {
                        'V1': {
                            'boundary_accounts': [
                                'aurora', 'aurora-0',
                                'kkuuue2akv_1630967379.near'
                            ],
                            'shards_split_map': [[0, 1, 2, 3]],
                            'to_parent_shard_map': [0, 0, 0, 0],
                            'version': 1
                        }
                    }
                    genesis_config['num_chunk_only_producer_seats'] = 200
                    genesis_config['max_kickout_stake_perc'] = 30
                    json.dump(genesis_config, f, indent=2)
                initlog_path = os.path.join(self.neard_logs_dir, 'initlog.txt')
                with open(initlog_path, 'ab') as out:
                    cmd = [
                        self.data['binaries'][0]['system_path'],
                        '--home',
                        self.target_near_home_path(),
                        '--unsafe-fast-startup',
                        'run',
                    ]
                    self.run_neard(
                        cmd,
                        out_file=out,
                    )
                self.set_state(TestState.STATE_ROOTS)
                self.save_data()

    def make_backup(self):
        now = str(datetime.datetime.now())
        backup_data = self.data['state_data']
        name = backup_data['backup_id']
        description = backup_data.get('description', None)

        backup_dir = self.home_path('backups', name)
        if os.path.exists(backup_dir):
            # we already checked that this backup ID didn't already exist, so if this path
            # exists, someone probably manually added it. for now just set the state to ERROR
            # and make the human intervene, but it shouldn't happen in practice
            logging.warn(f'{backup_dir} already exists')
            self.set_state(TestState.ERROR)
            return
        logging.info(f'copying data dir to {backup_dir}')
        shutil.copytree(self.target_near_home_path('data'),
                        backup_dir,
                        dirs_exist_ok=True)
        logging.info(f'copied data dir to {backup_dir}')

        backups = self.data.get('backups', {})
        if name in backups:
            # shouldn't happen if we check this in do_make_backups(), but fine to be paranoid and at least warn here
            logging.warn(
                f'backup {name} already existed in data.json, but it was not present before'
            )
        backups[name] = {
            'time': now,
            'description': description,
            'neard_path': self.data['current_neard_path']
        }
        self.data['backups'] = backups
        self.set_state(TestState.STOPPED)
        self.save_data()

    def make_initial_backup(self):
        try:
            shutil.rmtree(self.home_path('backups'))
        except FileNotFoundError:
            pass
        os.mkdir(self.home_path('backups'))
        self.making_backup(
            'start',
            description='initial test state after state root computation')
        self.save_data()
        self.make_backup()

    def check_genesis_state(self):
        path, running, exit_code = self.poll_neard()
        if not running:
            logging.error(
                f'neard exited with code {exit_code} on the first run')
            # For now just set the state to ERROR, because if this happens, there is something pretty wrong with
            # the setup, so a human needs to investigate and fix the bug
            self.set_state(TestState.ERROR)
            self.save_data()
        try:
            r = requests.get(f'http://{self.data["neard_addr"]}/status',
                             timeout=5)
        except requests.exceptions.ConnectionError:
            return
        if r.status_code == 200:
            logging.info('neard finished computing state roots')
            self.kill_neard()
            self.make_initial_backup()

    def reset_near_home(self):
        backup_id = self.data['state_data']
        if backup_id is None:
            backup_id = 'start'
        backup_path = self.home_path('backups', backup_id)
        if not os.path.exists(backup_path):
            logging.error(f'backup dir {backup_path} does not exist')
            self.set_state(TestState.ERROR)
            self.save_data()
        try:
            logging.info("removing the old directory")
            shutil.rmtree(self.target_near_home_path('data'))
        except FileNotFoundError:
            pass
        logging.info(f'restoring data dir from backup at {backup_path}')
        shutil.copytree(backup_path, self.target_near_home_path('data'))
        logging.info('data dir restored')
        self.set_state(TestState.STOPPED)
        self.save_data()

    # periodically check if we should update neard after a new epoch
    def main_loop(self):
        while True:
            with self.lock:
                state = self.get_state()
                if state == TestState.AWAITING_NETWORK_INIT:
                    self.network_init()
                elif state == TestState.AMEND_GENESIS:
                    self.check_amend_genesis()
                elif state == TestState.SET_VALIDATORS:
                    self.check_set_validators()
                elif state == TestState.STATE_ROOTS:
                    self.check_genesis_state()
                elif state == TestState.RUNNING:
                    self.check_upgrade_neard()
                elif state == TestState.RESETTING:
                    self.reset_near_home()
                elif state == TestState.MAKING_BACKUP:
                    self.make_backup()
            time.sleep(10)

    def serve(self, port):
        # TODO: maybe use asyncio? kind of silly to use multiple threads for
        # something so lightweight
        main_loop = threading.Thread(target=self.main_loop)
        main_loop.start()
        # this will listen only on the loopback interface and won't be accessible
        # over the internet. If connecting to another machine, we can SSH and then make
        # the request locally
        s = RpcServer(('localhost', port), self)
        s.serve_forever()

def main():
    parser = argparse.ArgumentParser(description='run neard')
    parser.add_argument('--home', type=str, required=True)
    parser.add_argument('--neard-home', type=str, required=True)
    parser.add_argument('--neard-logs-dir', type=str, required=True)
    parser.add_argument('--port', type=int, required=True)
    args = parser.parse_args()

    logging.basicConfig(format='[%(asctime)s] %(levelname)s: %(message)s',
                        level=logging.INFO)

    config_path = os.path.join(args.home, 'config.json')
    if not os.path.isdir(args.home) or not os.path.exists(config_path):
        sys.exit(
            f'please create the directory at {args.home} and write a config file at {config_path}'
        )

    # only let one instance of this code run at a time
    _fd = get_lock(args.home)

    logging.info("creating neard runner")
    runner = NeardRunner(args)

    logging.info("downloading binaries")
    with runner.lock:
        runner.download_binaries(force=False)

    logging.info("serve")
    runner.serve(args.port)

if __name__ == '__main__':
    main()

'''
'''--- pytest/tests/mocknet/helpers/requirements.txt ---
json-rpc
psutil
requests
'''
'''--- pytest/tests/mocknet/load_test_betanet.py ---
#!/usr/bin/env python3
import pathlib
import random
import sys
import time

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))
import account as account_mod
import key as key_mod
import mocknet
import mocknet_helpers
from helpers import load_test_utils

from configured_logger import logger

NUM_LETTERS = 26
NUM_ACCOUNTS = NUM_LETTERS * 5

def get_test_accounts_from_args(argv):
    assert len(argv) == 7
    node_account_id = argv[1]
    pk = argv[2]
    sk = argv[3]
    assert argv[4]
    rpc_nodes = argv[4].split(',')
    logger.info(f'rpc_nodes: {rpc_nodes}')
    max_tps = float(argv[5])
    need_deploy = (argv[6] == 'deploy')

    logger.info(f'need_deploy: {need_deploy}')

    rpc_infos = [(rpc_addr, mocknet_helpers.RPC_PORT) for rpc_addr in rpc_nodes]

    node_account_key = key_mod.Key(node_account_id, pk, sk)
    test_account_keys = [
        key_mod.Key(mocknet.load_testing_account_id(node_account_id, i), pk, sk)
        for i in range(NUM_ACCOUNTS)
    ]

    base_block_hash = mocknet_helpers.get_latest_block_hash(
        addr=random.choice(rpc_nodes))
    node_account = account_mod.Account(node_account_key,
                                       mocknet_helpers.get_nonce_for_pk(
                                           node_account_key.account_id,
                                           node_account_key.pk,
                                           addr=random.choice(rpc_nodes)),
                                       base_block_hash,
                                       rpc_infos=rpc_infos)

    test_accounts = []
    for key in test_account_keys:
        account = account_mod.Account(key,
                                      mocknet_helpers.get_nonce_for_pk(
                                          key.account_id,
                                          key.pk,
                                          addr=random.choice(rpc_nodes)),
                                      base_block_hash,
                                      rpc_infos=rpc_infos)
        test_accounts.append(account)

    return load_test_utils.TestState(node_account, test_accounts, max_tps,
                                     rpc_infos), need_deploy

def main(argv):
    logger.info(argv)
    test_state, need_deploy = get_test_accounts_from_args(argv)

    if need_deploy:
        load_test_utils.init_ft(test_state.node_account)
        for account in test_state.test_accounts:
            logger.info(
                f'Deploying contract for account {account.key.account_id}')
            tx_res = account.send_deploy_contract_tx('betanet_state.wasm')
            logger.info(f'Deploying result: {tx_res}')
            load_test_utils.init_ft_account(test_state.node_account, account)
            logger.info(
                f'Account {account.key.account_id} balance after initialization: {account.get_amount_yoctonear()}'
            )
            mocknet_helpers.wait_at_least_one_block()

    total_tx_sent = 0
    start_time = time.monotonic()
    while True:
        elapsed_time = time.monotonic() - start_time
        total_tx_sent = mocknet_helpers.throttle_txns(
            load_test_utils.send_random_transactions, total_tx_sent,
            elapsed_time, test_state)

if __name__ == '__main__':
    main(sys.argv)

'''
'''--- pytest/tests/mocknet/load_test_spoon.py ---
#!/usr/bin/env python3
"""
Runs a loadtest on mocknet.

The setup requires you to have a few nodes that will be validators and at least one node that will be an RPC node.
Specify the number of validator nodes using the `--num-nodes` flag.

Use https://github.com/near/near-ops/tree/master/provisioning/terraform/network/mocknet to bring up a set of VM
instances for the test. It also takes care of downloading necessary binaries and smart contracts.

Depending on the `--chain-id` flag, the nodes will be initialized with an empty state, or with the most recent state
dump of `mainnet`, `testnet` and `betanet`. Note that `mainnet` and `testnet` state dumps are huge. Starting a node with
`testnet` state dump takes about 1.5 hours.

You can run multiple tests in parallel, use `--pattern` to disambiguate.

Configure the desirable generated load with the `--max-tps` flag, or disable load altogether with `--skip-load`.

Example from the recent loadtest run:
1) terraform apply -var="chain_id=mainnet" -var="size=big" -var="override_chain_id=rc3-22" -var="neard_binary_url=https://s3.us-west-1.amazonaws.com/build.nearprotocol.com/nearcore/Linux/1.23.0/1eaa01d6abc76757b2ef50a1a127f98576b750c4/neard" -var="upgrade_neard_binary_url=https://near-protocol-public.s3.ca-central-1.amazonaws.com/mocknet/neard.rc3-22"
2) python3 tests/mocknet/load_test_spoon.py --chain-id=mainnet-spoon --pattern=rc3-22 --epoch-length=1000 --num-nodes=120 --max-tps=100 --script=add_and_delete --increasing-stakes=1 --progressive-upgrade --num-seats=100

Things to look out for when running the test:
1) Test init phase completes before any binary upgrades start.
2) At the beginning of each of the first epochs some nodes get upgraded.
3) If the protocol upgrades becomes effective at epoch T, check that binaries upgraded at epochs T-1, T-2 have started successfully.
4) BPS and TPS need to be at some sensible values.
5) CPU usage and RAM usage need to be reasonable as well.
6) Ideally we should check the percentage of generated transactions that succeed, but there is no easy way to do that. Maybe replay some blocks using `neard view_state apply_range --help`.

Other notes:
1) This grafana dashboard can help: https://grafana.near.org/d/jHbiNgSnz/mocknet
2) Logs are in /home/ubuntu/neard.log and /home/ubuntu/neard.upgrade.log

For the terraform command to work, please do the following:
1) Install terraform cli: https://learn.hashicorp.com/tutorials/terraform/install-cli
2) git clone https://github.com/near/near-ops
3) cd provisioning/terraform/network/mocknet
4) Run `terraform init`
5) Run `terraform apply` as specified above and check the output.
6) If you don't have permissions for the GCP project `near-mocknet`, please ask your friendly SREs to give you permissions.
7) Make the pair of neard binaries available for download. The pair of binaries is the baseline version, e.g. the current mainnet binary version, and your experimental binary. The simplest way to make the binaries available is to upload them to https://s3.console.aws.amazon.com/s3/buckets/near-protocol-public?region=ca-central-1&prefix=mocknet/
8) If you don't have access to S3 AWS, please ask your friendly SREs to give you access.
9) A reliable way to make an experimental binary compatible with mocknet instances, is to build it on machine "builder" in project "near-mocknet". You may need to "Resume" the instance in the GCP console: https://console.cloud.google.com/compute/instancesDetail/zones/us-central1-a/instances/builder?project=near-mocknet

For the `load_test_spoon.py` to work, please do the following:
1) git clone https://github.com/near/nearcore
2) cd pytest
3) Follow instructions in https://github.com/near/nearcore/blob/master/pytest/README.md to setup your Python environment
4) Run load_test_spoon.py as specified above.

"""
import argparse
import random
import sys
import time
from rc import pmap
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

import mocknet
import data

from metrics import Metrics
from configured_logger import logger

def measure_tps_bps(nodes, tx_filename):
    input_tx_events = mocknet.get_tx_events(nodes, tx_filename)
    # drop first and last 5% of events to avoid edges of test
    n = int(0.05 * len(input_tx_events))
    input_tx_events = input_tx_events[n:-n]
    input_tps = data.compute_rate(input_tx_events)
    measurement = mocknet.chain_measure_bps_and_tps(
        nodes[-1],
        input_tx_events[0],
        input_tx_events[-1],
    )
    result = {
        'bps': measurement['bps'],
        'in_tps': input_tps,
        'out_tps': measurement['tps']
    }
    logger.info(f'{result}')
    return result

def check_tps(measurement, expected_in, expected_out=None, tolerance=0.05):
    if expected_out is None:
        expected_out = expected_in
    almost_equal = lambda x, y: (abs(x - y) / y) <= tolerance
    return (almost_equal(measurement['in_tps'], expected_in) and
            almost_equal(measurement['out_tps'], expected_out))

def check_memory_usage(node):
    metrics = mocknet.get_metrics(node)
    mem_usage = metrics.memory_usage / 1e6
    logger.info(f'Memory usage (MB) = {mem_usage}')
    return mem_usage < 4500

def check_slow_blocks(initial_metrics, final_metrics):
    delta = Metrics.diff(final_metrics, initial_metrics)
    slow_process_blocks = delta.block_processing_time[
        'le +Inf'] - delta.block_processing_time['le 1']
    logger.info(
        f'Number of blocks processing for more than 1s: {slow_process_blocks}')
    return slow_process_blocks == 0

class LoadTestSpoon:
    EPOCH_HEIGHT_CHECK_DELAY = 30

    def run(self):
        logger.info('Starting load test spoon.')

        self.__parse_args()

        self.__prepare_nodes()

        self.__prepare_upgrade_schedule()

        if not self.skip_setup:
            self.__setup_remote_python_environments()

        if not self.skip_restart:
            self.__upload_genesis_and_restart()

        mocknet.wait_all_nodes_up(self.all_nodes)

        initial_metrics = mocknet.get_metrics(self.archival_node)
        initial_validator_accounts = mocknet.list_validators(self.archival_node)

        if not self.skip_load:
            self.__load()

        time.sleep(5)

        final_metrics = mocknet.get_metrics(self.archival_node)
        final_validator_accounts = mocknet.list_validators(self.archival_node)

        self.__final_checks(
            initial_metrics,
            final_metrics,
            initial_validator_accounts,
            final_validator_accounts,
        )

        logger.info('Load test complete.')

    def __parse_args(self):
        parser = argparse.ArgumentParser(description='Run a load test')
        parser.add_argument('--chain-id', required=True)
        parser.add_argument('--pattern', required=False)
        parser.add_argument('--epoch-length', type=int, required=True)
        parser.add_argument('--num-nodes', type=int, required=True)
        parser.add_argument('--max-tps', type=float, required=True)
        parser.add_argument('--increasing-stakes', type=float, default=0.0)
        parser.add_argument('--progressive-upgrade', action='store_true')

        parser.add_argument('--skip-load', action='store_true')
        parser.add_argument('--skip-setup', action='store_true')
        parser.add_argument('--skip-restart', action='store_true')
        parser.add_argument('--no-sharding', action='store_true')
        parser.add_argument('--num-seats', type=int, required=True)

        parser.add_argument('--test-timeout', type=int, default=12 * 60 * 60)
        parser.add_argument(
            '--contract-deploy-time',
            type=int,
            default=10 * mocknet.NUM_ACCOUNTS,
            help=
            'We need to slowly deploy contracts, otherwise we stall out the nodes',
        )

        parser.add_argument('--log-level', default="INFO")

        # The flag is no longer needed but is kept for backwards-compatibility.
        parser.add_argument('--script', required=False)

        args = parser.parse_args()

        logger.setLevel(args.log_level)

        self.chain_id = args.chain_id
        self.pattern = args.pattern
        self.epoch_length = args.epoch_length
        self.num_nodes = args.num_nodes
        self.max_tps = args.max_tps

        self.increasing_stakes = args.increasing_stakes
        self.progressive_upgrade = args.progressive_upgrade
        self.num_seats = args.num_seats

        self.skip_setup = args.skip_setup
        self.skip_restart = args.skip_restart
        self.skip_load = args.skip_load
        self.no_sharding = args.no_sharding

        self.test_timeout = args.test_timeout
        self.contract_deploy_time = args.contract_deploy_time

        self.log_level = args.log_level

        assert self.epoch_length > 0
        assert self.num_nodes > 0
        assert self.max_tps > 0

    def __prepare_nodes(self):
        all_nodes = mocknet.get_nodes(pattern=self.pattern)
        random.shuffle(all_nodes)
        assert len(all_nodes) > self.num_nodes, \
            f'Need at least one RPC node. ' \
            f'Nodes available in mocknet: {len(all_nodes)} ' \
            f'Requested validator nodes num: {self.num_nodes}'

        self.all_nodes = all_nodes
        self.validator_nodes = all_nodes[:self.num_nodes]
        self.rpc_nodes = all_nodes[self.num_nodes:]
        self.archival_node = self.rpc_nodes[0]

        logger.info(
            f'validator_nodes: {[n.instance_name for n in self.validator_nodes]}'
        )
        logger.info(
            f'rpc_nodes      : {[n.instance_name for n in self.rpc_nodes]}')
        logger.info(f'archival node  : {self.archival_node.instance_name}')

    def __prepare_upgrade_schedule(self):
        logger.info(f'Preparing upgrade schedule')
        self.upgrade_schedule = mocknet.create_upgrade_schedule(
            self.rpc_nodes,
            self.validator_nodes,
            self.progressive_upgrade,
            self.increasing_stakes,
            self.num_seats,
        )
        logger.info(f'Preparing upgrade schedule -- done.')

    def __setup_remote_python_environments(self):
        logger.info('Setting remote python environments')
        mocknet.setup_python_environments(
            self.all_nodes,
            'add_and_delete_state.wasm',
        )
        logger.info('Setting remote python environments -- done')

    def __upload_genesis_and_restart(self):
        logger.info(f'Restarting')
        # Make sure nodes are running by restarting them.
        mocknet.stop_nodes(self.all_nodes)
        time.sleep(10)
        validator_node_pks = pmap(
            lambda node: mocknet.get_node_keys(node)[0],
            self.validator_nodes,
        )
        all_node_pks = pmap(
            lambda node: mocknet.get_node_keys(node)[0],
            self.all_nodes,
        )
        pmap(lambda node: mocknet.init_validator_key(node), self.all_nodes)
        node_ips = [node.machine.ip for node in self.all_nodes]
        mocknet.create_and_upload_genesis(
            self.validator_nodes,
            self.chain_id,
            rpc_nodes=self.rpc_nodes,
            epoch_length=self.epoch_length,
            node_pks=validator_node_pks,
            increasing_stakes=self.increasing_stakes,
            num_seats=self.num_seats,
            single_shard=self.no_sharding,
            all_node_pks=all_node_pks,
            node_ips=node_ips,
        )
        mocknet.clear_data(self.all_nodes)
        mocknet.start_nodes(self.all_nodes, self.upgrade_schedule)
        time.sleep(60)

        logger.info(f'Restarting -- done')

    def __start_load_test_helpers(self):
        logger.info('Starting transaction spamming scripts.')
        script = 'load_test_spoon_helper.py'
        mocknet.start_load_test_helpers(
            script,
            self.validator_nodes,
            self.rpc_nodes,
            self.max_tps,
            # Make the helper run a bit longer - there is significant delay
            # between the time when the first helper is started and when this
            # scipts begins the test.
            self.test_timeout + 4 * self.EPOCH_HEIGHT_CHECK_DELAY,
            self.contract_deploy_time,
        )
        logger.info('Starting transaction spamming scripts -- done.')

    def __check_neard_and_helper_are_running(self):
        neard_running = mocknet.is_binary_running_all_nodes(
            'neard',
            self.all_nodes,
        )
        helper_running = mocknet.is_binary_running_all_nodes(
            'load_test_spoon_helper.py',
            self.validator_nodes,
        )

        logger.debug(
            f'neard is running on {neard_running.count(True)}/{len(neard_running)} nodes'
        )
        logger.debug(
            f'helper is running on {helper_running.count(True)}/{len(helper_running)} validator nodes'
        )

        for node, is_running in zip(self.all_nodes, neard_running):
            if not is_running:
                raise Exception(
                    'The neard process is not running on some of the nodes! '
                    f'The neard is not running on {node.instance_name}')

        for node, is_running in zip(self.validator_nodes, helper_running):
            if not is_running:
                raise Exception(
                    'The helper process is not running on some of the nodes! '
                    f'The helper is not running on {node.instance_name}')

    def __wait_to_deploy_contracts(self):
        msg = 'Waiting for the helper to deploy contracts'
        logger.info(f'{msg}: {self.contract_deploy_time}s')

        start_time = time.monotonic()
        while True:
            self.__check_neard_and_helper_are_running()

            now = time.monotonic()
            remaining_time = self.contract_deploy_time + start_time - now
            logger.info(f'{msg}: {round(remaining_time)}s')
            if remaining_time < 0:
                break

            time.sleep(self.EPOCH_HEIGHT_CHECK_DELAY)

    def __wait_to_complete(self):
        msg = 'Waiting for the loadtest to complete'
        logger.info(f'{msg}: {self.test_timeout}s')

        initial_epoch_height = mocknet.get_epoch_height(self.rpc_nodes, -1)
        logger.info(f'initial_epoch_height: {initial_epoch_height}')
        assert initial_epoch_height >= 0

        prev_epoch_height = initial_epoch_height
        start_time = time.monotonic()
        while True:
            self.__check_neard_and_helper_are_running()

            epoch_height = mocknet.get_epoch_height(
                self.rpc_nodes,
                prev_epoch_height,
            )
            if epoch_height > prev_epoch_height:
                mocknet.upgrade_nodes(
                    epoch_height - initial_epoch_height,
                    self.upgrade_schedule,
                    self.all_nodes,
                )
                prev_epoch_height = epoch_height

            remaining_time = self.test_timeout + start_time - time.monotonic()
            logger.info(f'{msg}: {round(remaining_time)}s')
            if remaining_time < 0:
                break

            time.sleep(self.EPOCH_HEIGHT_CHECK_DELAY)

        logger.info(f'{msg} -- done')

    def __load(self):
        msg = 'Running load test'
        logger.info(f'{msg}')

        self.__start_load_test_helpers()

        self.__wait_to_deploy_contracts()

        self.__wait_to_complete()

        logger.info(f'{msg} -- done')

    def __final_checks(
        self,
        initial_metrics,
        final_metrics,
        initial_validator_accounts,
        final_validator_accounts,
    ):
        msg = 'Running final checks'
        logger.info(msg)

        test_passed = True
        if not check_memory_usage(self.validator_nodes[0]):
            test_passed = False
            logger.error('Memory usage too large')
        if not check_slow_blocks(initial_metrics, final_metrics):
            test_passed = False
            logger.error('Too many slow blocks')

        if set(initial_validator_accounts) != set(final_validator_accounts):
            logger.warning(
                f'Mismatching set of validators:\n'
                f'initial_validator_accounts: {initial_validator_accounts}\n'
                f'final_validator_accounts  : {final_validator_accounts}')

        assert test_passed

        logger.info(f'{msg} -- done.')

if __name__ == '__main__':
    load_test_spoon = LoadTestSpoon()
    load_test_spoon.run()

'''
'''--- pytest/tests/mocknet/local_test_node.py ---
#!/usr/bin/env python3
"""
defines the LocalTestNeardRunner class meant to to test mocknet itself locally
"""
from argparse import ArgumentParser
import http.server
import json
import os
import pathlib
import psutil
import re
import requests
import shutil
import signal
import subprocess
import sys
import threading
import time

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from configured_logger import logger
from node_handle import NodeHandle

# return the process with pid listed in `pid_path`, if it's running
def get_process(pid_path):
    try:
        with open(pid_path, 'r') as f:
            pid = int(f.read().strip())
    except FileNotFoundError:
        return None
    try:
        return psutil.Process(pid)
    except psutil.NoSuchProcess:
        return None

# kill the process with pid listed in `pid_path`
def kill_process(pid_path):
    p = get_process(pid_path)
    if p is not None:
        logger.info(f'killing process with pid {p.pid} indicated in {pid_path}')
        p.send_signal(signal.SIGTERM)
        p.wait()
    try:
        pid_path.unlink()
    except FileNotFoundError:
        pass

def http_post(addr, port, body):
    r = requests.post(f'http://{addr}:{port}', json=body, timeout=5)
    if r.status_code != 200:
        logger.warning(
            f'bad response {r.status_code} trying to post {body} to http://{addr}:{port}:\n{r.content}'
        )
    r.raise_for_status()
    return r.json()

class LocalTestNeardRunner:

    def __init__(self, home, port, neard_rpc_port, neard_protocol_port):
        # last part of the path. e.g. ~/.near/local-mocknet/traffic-generator -> traffic-generator
        self._name = os.path.basename(os.path.normpath(home))
        self.home = home
        self.port = port
        self.neard_rpc_port = neard_rpc_port
        self.neard_protocol_port = neard_protocol_port

    def name(self):
        return self._name

    def ip_addr(self):
        return '0.0.0.0'

    def neard_port(self):
        return self.neard_rpc_port

    def init(self):
        return

    def mk_neard_runner_home(self, remove_home_dir):
        # handled by local_test_setup_cmd()
        return

    def upload_neard_runner(self):
        return

    def upload_neard_runner_config(self, config):
        # handled by local_test_setup_cmd()
        return

    def init_python(self):
        return

    def _pid_path(self):
        return self.home / 'pid.txt'

    def stop_neard_runner(self):
        kill_process(self._pid_path())

    def start_neard_runner(self):
        if get_process(self._pid_path()) is not None:
            return

        with open(self.home / 'stdout', 'ab') as stdout, \
            open(self.home / 'stderr', 'ab') as stderr:
            args = [
                sys.executable, 'tests/mocknet/helpers/neard_runner.py',
                '--home', self.home / 'neard-runner', '--neard-home',
                self.home / '.near', '--neard-logs', self.home / 'neard-logs',
                '--port',
                str(self.port)
            ]
            process = subprocess.Popen(args,
                                       stdin=subprocess.DEVNULL,
                                       stdout=stdout,
                                       stderr=stderr,
                                       process_group=0)
        with open(self._pid_path(), 'w') as f:
            f.write(f'{process.pid}\n')
        logger.info(
            f'started neard runner process with pid {process.pid} listening on port {self.port}'
        )

    def neard_runner_post(self, body):
        return http_post(self.ip_addr(), self.port, body)

    def new_test_params(self):
        return {
            'rpc_port': self.neard_rpc_port,
            'protocol_port': self.neard_protocol_port,
            'validator_id': self._name,
        }

    def get_validators(self):
        body = {
            'method': 'validators',
            'params': [None],
            'id': 'dontcare',
            'jsonrpc': '2.0'
        }
        return http_post(self.ip_addr(), self.neard_rpc_port, body)

def prompt_flags(args):
    if args.num_nodes is None:
        print(
            'number of validating nodes? One instance of neard_runner.py will be run for each one, plus a traffic generator: '
        )
        args.num_nodes = int(sys.stdin.readline().strip())
        assert args.num_nodes > 0

    if args.neard_binary_path is None:
        print('neard binary path?: ')
        args.neard_binary_path = sys.stdin.readline().strip()
        assert len(args.neard_binary_path) > 0

    if args.fork_height is not None:
        if not args.legacy_records:
            print(
                '--legacy-records not given. Assuming it based on --fork-height'
            )
            args.legacy_records = True
    elif args.target_home_dir is not None:
        if args.legacy_records:
            sys.exit('cannot give --target-home-dir and --legacy-records')
    elif not args.legacy_records:
        print(
            'prepare nodes with fork-network tool instead of genesis records JSON? [yes/no]:'
        )
        while True:
            r = sys.stdin.readline().strip().lower()
            if r == 'yes':
                args.legacy_records = False
                break
            elif r == 'no':
                args.legacy_records = True
                break
            else:
                print('please say yes or no')

    if args.source_home_dir is None:
        if args.legacy_records:
            print('source home dir: ')
        else:
            print(
                'source home dir containing the HEAD block of target home, plus more blocks after that: '
            )
        args.source_home_dir = sys.stdin.readline().strip()
        assert len(args.source_home_dir) > 0

    if args.target_home_dir is None and not args.legacy_records:
        print('target home dir whose HEAD is contained in --source-home-dir: ')
        args.target_home_dir = sys.stdin.readline().strip()
        assert len(args.target_home_dir) > 0

    if args.legacy_records and args.fork_height is None:
        print('fork height: ')
        args.fork_height = sys.stdin.readline().strip()
        assert len(args.fork_height) > 0

def run_cmd(cmd):
    try:
        subprocess.check_output(cmd, stderr=subprocess.STDOUT)
    except subprocess.CalledProcessError as e:
        sys.exit(
            f'running `{" ".join([str(a) for a in cmd])}` returned {e.returncode}. output:\n{e.output.decode("utf-8")}'
        )

# dumps records from `traffic_home_dir` and prepares records with keys changed
# for mirroring traffic
def make_records(neard_binary_path, traffic_home_dir, start_height):
    run_cmd([
        neard_binary_path, '--home', traffic_home_dir, 'view-state',
        'dump-state', '--stream', '--height',
        str(start_height)
    ])
    shutil.copyfile(traffic_home_dir / 'output/genesis.json',
                    traffic_home_dir / 'setup/genesis.json')
    run_cmd([
        neard_binary_path,
        'mirror',
        'prepare',
        '--records-file-in',
        traffic_home_dir / 'output/records.json',
        '--records-file-out',
        traffic_home_dir / 'setup/records.json',
        '--secret-file-out',
        '/dev/null',
        '--no-secret',
    ])

def make_legacy_records(neard_binary_path, traffic_generator_home, node_homes,
                        start_height):
    make_records(neard_binary_path, traffic_generator_home / '.near',
                 start_height)
    for node_home in node_homes:
        shutil.copyfile(traffic_generator_home / '.near/setup/genesis.json',
                        node_home / '.near/setup/genesis.json')
        shutil.copyfile(traffic_generator_home / '.near/setup/records.json',
                        node_home / '.near/setup/records.json')

def fork_db(neard_binary_path, target_home_dir, home_dir, setup_dir):
    copy_source_home(target_home_dir, setup_dir)

    run_cmd([
        neard_binary_path,
        '--home',
        setup_dir,
        'fork-network',
        'init',
    ])
    run_cmd([
        neard_binary_path,
        '--home',
        setup_dir,
        'fork-network',
        'amend-access-keys',
    ])
    shutil.rmtree(setup_dir / 'data/fork-snapshot')

def make_forked_network(neard_binary_path, traffic_generator_home, node_homes,
                        source_home_dir, target_home_dir):
    for (home_dir, setup_dir) in [
        (h / '.near', h / '.near/setup') for h in node_homes
    ] + [(traffic_generator_home / '.near/target',
          traffic_generator_home / '.near/setup')]:
        fork_db(neard_binary_path, target_home_dir, home_dir, setup_dir)

def mkdirs(local_mocknet_path):
    traffic_generator_home = local_mocknet_path / 'traffic-generator'
    traffic_generator_home.mkdir()
    os.mkdir(traffic_generator_home / 'neard-runner')
    os.mkdir(traffic_generator_home / '.near')
    os.mkdir(traffic_generator_home / '.near/setup')
    node_homes = []
    for i in range(args.num_nodes):
        node_home = local_mocknet_path / f'node{i}'
        node_home.mkdir()
        os.mkdir(node_home / f'neard-runner')
        os.mkdir(node_home / f'.near')
        os.mkdir(node_home / f'.near/setup')
        node_homes.append(node_home)
    return traffic_generator_home, node_homes

def copy_source_home(source_home_dir, traffic_generator_home):
    shutil.copyfile(source_home_dir / 'config.json',
                    traffic_generator_home / 'config.json')
    shutil.copyfile(source_home_dir / 'node_key.json',
                    traffic_generator_home / 'node_key.json')
    shutil.copyfile(source_home_dir / 'genesis.json',
                    traffic_generator_home / 'genesis.json')
    try:
        shutil.copyfile(source_home_dir / 'records.json',
                        traffic_generator_home / 'records.json')
    except FileNotFoundError:
        pass
    shutil.copytree(source_home_dir / 'data', traffic_generator_home / 'data')

def make_binaries_dir(local_mocknet_path, neard_binary_path):
    binaries_path = local_mocknet_path / 'binaries'
    binaries_path.mkdir()
    binary_path = binaries_path / 'neard'
    binary_path.symlink_to(neard_binary_path)
    return binaries_path

class Server(http.server.HTTPServer):

    def __init__(self, addr, directory):
        self.directory = directory
        super().__init__(addr, http.server.SimpleHTTPRequestHandler)

    def finish_request(self, request, client_address):
        self.RequestHandlerClass(request,
                                 client_address,
                                 self,
                                 directory=self.directory)

def write_config(home, config):
    with open(home / 'neard-runner' / 'config.json', 'w') as f:
        json.dump(config, f)

# looks for directories called node{i} in `local_mocknet_path`
def get_node_homes(local_mocknet_path):
    dirents = os.listdir(local_mocknet_path)
    node_homes = []
    for p in dirents:
        m = re.match(r'node(\d+)', p)
        if m is None:
            continue
        node_homes.append((p, int(m.groups()[0])))
    node_homes.sort(key=lambda x: x[1])
    idx = -1
    for (home, node_index) in node_homes:
        if node_index != idx + 1:
            raise ValueError(
                f'some neard runner node dirs missing? found: {[n[0] for n in node_homes]}'
            )
        idx = node_index
    return [local_mocknet_path / x[0] for x in node_homes]

# return a NodeHandle for each of the neard runner directories in `local_mocknet_path`
def get_nodes(local_mocknet_path=pathlib.Path.home() / '.near/local-mocknet'):
    runner_port = 3000
    neard_rpc_port = 3040
    neard_protocol_port = 24577
    traffic_generator = NodeHandle(
        LocalTestNeardRunner(local_mocknet_path / 'traffic-generator',
                             runner_port, neard_rpc_port, neard_protocol_port))

    node_homes = get_node_homes(local_mocknet_path)
    nodes = []
    for home in node_homes:
        runner_port += 1
        neard_rpc_port += 1
        neard_protocol_port += 1
        nodes.append(
            NodeHandle(
                LocalTestNeardRunner(home, runner_port, neard_rpc_port,
                                     neard_protocol_port)))

    return traffic_generator, nodes

def kill_neard_runner(home):
    kill_process(home / 'pid.txt')

def kill_neard_runners(local_mocknet_path):
    kill_neard_runner(local_mocknet_path / 'traffic-generator')
    node_homes = get_node_homes(local_mocknet_path)
    for home in node_homes:
        kill_neard_runner(home)

def wait_node_serving(node):
    while True:
        try:
            node.neard_runner_ready()
            return
        except requests.exceptions.ConnectionError:
            pass
        time.sleep(0.5)

def local_test_setup_cmd(args):
    prompt_flags(args)
    if args.source_home_dir is None:
        sys.exit(f'must give --source-home-dir')
    if args.legacy_records:
        if args.target_home_dir is not None:
            sys.exit(f'cannot give --target-home-dir with --legacy-records')
        if args.fork_height is None:
            sys.exit('must give --fork-height with --legacy-records')
    else:
        if args.target_home_dir is None:
            sys.exit(f'must give --target-home-dir')
        if args.fork_height is not None:
            sys.exit('cannot give --fork-height without --legacy-records')

    local_mocknet_path = pathlib.Path.home() / '.near/local-mocknet'
    if os.path.exists(local_mocknet_path):
        if not args.yes:
            print(
                f'{local_mocknet_path} already exists. This command will delete and reinitialize it. Continue? [yes/no]:'
            )
            if sys.stdin.readline().strip() != 'yes':
                return
        kill_neard_runners(local_mocknet_path)
        shutil.rmtree(local_mocknet_path)

    neard_binary_path = pathlib.Path(args.neard_binary_path)
    source_home_dir = pathlib.Path(args.source_home_dir)

    os.mkdir(local_mocknet_path)
    traffic_generator_home, node_homes = mkdirs(local_mocknet_path)
    copy_source_home(source_home_dir, traffic_generator_home / '.near')
    if args.legacy_records:
        make_legacy_records(neard_binary_path, traffic_generator_home,
                            node_homes, args.fork_height)
    else:
        target_home_dir = pathlib.Path(args.target_home_dir)
        make_forked_network(neard_binary_path, traffic_generator_home,
                            node_homes, source_home_dir, target_home_dir)
    # now set up an HTTP server to serve the binary that each neard_runner.py will request
    binaries_path = make_binaries_dir(local_mocknet_path, neard_binary_path)
    binaries_server_addr = 'localhost'
    binaries_server_port = 8000
    binaries_server = Server(addr=(binaries_server_addr, binaries_server_port),
                             directory=binaries_path)
    server_thread = threading.Thread(
        target=lambda: binaries_server.serve_forever(), daemon=True)
    server_thread.start()

    node_config = {
        'is_traffic_generator':
            False,
        'binaries': [{
            'url':
                f'http://{binaries_server_addr}:{binaries_server_port}/neard',
            'epoch_height':
                0
        }]
    }
    traffic_generator_config = {
        'is_traffic_generator':
            True,
        'binaries': [{
            'url':
                f'http://{binaries_server_addr}:{binaries_server_port}/neard',
            'epoch_height':
                0
        }]
    }

    write_config(traffic_generator_home, traffic_generator_config)
    for node_home in node_homes:
        write_config(node_home, node_config)

    traffic_generator, nodes = get_nodes(local_mocknet_path)
    traffic_generator.start_neard_runner()
    for node in nodes:
        node.start_neard_runner()

    for node in [traffic_generator] + nodes:
        wait_node_serving(node)

    print(
        f'All directories initialized. neard runners are running in dirs: {[str(traffic_generator.node.home)] + [str(n.node.home) for n in nodes]}, listening on respective ports: {[traffic_generator.node.port] + [n.node.port for n in nodes]}'
    )

if __name__ == '__main__':
    parser = ArgumentParser(description='Set up a local instance of mocknet')
    subparsers = parser.add_subparsers(title='subcommands',
                                       description='valid subcommands')

    local_test_setup_parser = subparsers.add_parser('local-test-setup',
                                                    help='''
        Setup several instances of neard-runner to run locally. Then the mirror.py --local-test
        argument can be used to test these test scripts themselves.
        ''')
    local_test_setup_parser.add_argument('--num-nodes', type=int)
    # TODO: add a --neard-upgrade-binary-path flag too
    local_test_setup_parser.add_argument('--neard-binary-path', type=str)
    local_test_setup_parser.add_argument('--source-home-dir',
                                         type=str,
                                         help='''
    Near home directory containing some transactions that can be used to create a forked state
    for transaction mirroring. This could be a home dir from a pytest in tests/sanity, for example.
    ''')
    local_test_setup_parser.add_argument('--fork-height',
                                         type=int,
                                         help='''
    Height where state should be forked from in the directory indicated by --source-home-dir. Ideally this should
    be a height close to the node's tail. This is something that could be automated if there were an easy
    way to get machine-readable valid heights in a near data directory, but for now this flag must be given manually.
    ''')
    local_test_setup_parser.add_argument('--yes', action='store_true')
    local_test_setup_parser.add_argument('--legacy-records',
                                         action='store_true',
                                         help='''
    If given, setup a records.json file with forked state instead of using the neard fork-network command
    ''')
    local_test_setup_parser.add_argument('--target-home-dir',
                                         type=str,
                                         help='''
    todo
    ''')
    local_test_setup_parser.set_defaults(func=local_test_setup_cmd)

    args = parser.parse_args()
    args.func(args)

'''
'''--- pytest/tests/mocknet/locust.py ---
import argparse
import cmd_utils
import pathlib
from rc import pmap, run
import sys

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

import mocknet

from configured_logger import logger

def init_locust_node(instance_name):
    node = mocknet.get_node(instance_name)
    if node is None:
        sys.exit(f'could not find node {instance_name}')
    cmd_utils.init_node(node)
    commands = [
        'sudo apt update',
        'sudo apt-get install -y git virtualenv build-essential python3-dev',
        'git clone https://github.com/near/nearcore /home/ubuntu/nearcore',
        'mkdir /home/ubuntu/locust',
        'cd /home/ubuntu/locust && python3 -m virtualenv venv -p $(which python3)',
        './venv/bin/pip install -r /home/ubuntu/nearcore/pytest/requirements.txt',
        './venv/bin/pip install locust',
    ]
    init_command = ' && '.join(commands)
    cmd_utils.run_cmd(node, init_command)

def init_cmd(args):
    nodes = [x for x in args.instance_names.split(',') if len(x) > 0]
    pmap(init_locust_node, nodes)

def parse_instsance_names(args):
    if args.master is None:
        print('master instance name?: ')
        args.master = sys.stdin.readline().strip()

    if args.workers is None:
        print('''
worker instance names? Give a comma separated list. It is also valid to
have the machine used for the master process in this list as well:''')
        args.workers = sys.stdin.readline().strip()

    master = mocknet.get_node(args.master)
    if master is None:
        sys.exit(f'could not find node {args.master}')

    worker_names = [x for x in args.workers.split(',') if len(x) > 0]
    workers = [
        mocknet.get_node(instance_name) for instance_name in worker_names
    ]
    for (name, node) in zip(worker_names, workers):
        if node is None:
            sys.exit(f'could not find node {name}')

    return master, workers

def upload_key(node, filename):
    node.machine.upload(args.funding_key,
                        '/home/ubuntu/locust/funding_key.json',
                        switch_user='ubuntu')

def run_master(args, node, num_workers):
    upload_key(node, args.funding_key)
    cmd = f'/home/ubuntu/locust/venv/bin/python3 -m locust --web-port 3030 --master-bind-port 3000 -H {args.node_ip_port} -f locustfiles/{args.locustfile} --shard-layout-chain-id mainnet --funding-key=/home/ubuntu/locust/funding_key.json --max-workers {args.max_workers} --master'
    if args.num_users is not None:
        cmd += f' --users {args.num_users}'
    if args.run_time is not None:
        cmd += f' --run-time {args.run_time}'
    if not args.web_ui:
        cmd += f' --headless --expect-workers {num_workers}'

    logger.info(f'running "{cmd}" on master node {node.instance_name}')
    cmd_utils.run_in_background(
        node,
        cmd,
        'locust-master.txt',
        pre_cmd=
        'ulimit -S -n 100000 && cd /home/ubuntu/nearcore/pytest/tests/loadtest/locust'
    )

def wait_locust_inited(node, log_filename):
    # We want to wait for the locust process to finish the initialization steps. Is there a better way than
    # just waiting for the string "Starting Locust" to appear in the logs?
    cmd_utils.run_cmd(
        node,
        f'tail -f {cmd_utils.LOG_DIR}/{log_filename}.0 | grep --line-buffered -m 1 -q "Starting Locust"'
    )

def wait_master_inited(node):
    wait_locust_inited(node, 'locust-master.txt')
    logger.info(f'master locust node initialized')

def wait_worker_inited(node):
    wait_locust_inited(node, 'locust-worker.txt')
    logger.info(f'worker locust node {node.instance_name} initialized')

def run_worker(args, node, master_ip):
    cmd = f'/home/ubuntu/locust/venv/bin/python3 -m locust --web-port 3030 -H {args.node_ip_port} -f locustfiles/{args.locustfile} --shard-layout-chain-id mainnet --funding-key=/home/ubuntu/locust/funding_key.json --worker --master-port 3000'
    if master_ip != node.machine.ip:
        # if this node is also the master node, the key has already been uploaded
        upload_key(node, args.funding_key)
        cmd += f' --master-host {master_ip}'
    logger.info(f'running "{cmd}" on worker node {node.instance_name}')
    cmd_utils.run_in_background(
        node,
        cmd,
        'locust-worker.txt',
        pre_cmd=
        'ulimit -S -n 100000 && cd /home/ubuntu/nearcore/pytest/tests/loadtest/locust'
    )

def run_cmd(args):
    if not args.web_ui and args.num_users is None:
        sys.exit('unless you pass --web-ui, --num-users must be set')

    master, workers = parse_instsance_names(args)

    run_master(args, master, len(workers))
    if args.web_ui:
        wait_master_inited(master)
    pmap(lambda n: run_worker(args, n, master.machine.ip), workers)
    if args.web_ui:
        pmap(wait_worker_inited, workers)
        logger.info(
            f'All locust workers initialized. Visit http://{master.machine.ip}:3030/ to start and control the test'
        )
    else:
        logger.info('All workers started.')

def stop_cmd(args):
    master, workers = parse_instsance_names(args)
    # TODO: this feels kind of imprecise and heavy-handed, since we're just looking for a command that matches "python3.*locust.*master" and killing it,
    # instead of remembering what the process' IP was. Should be possible to do this right, but this will work for now
    cmd_utils.run_cmd(
        master,
        'pids=$(ps -C python3 -o pid=,cmd= | grep "locust" | cut -d " " -f 2) && if [ ! -z "$pids" ]; then kill $pids; fi'
    )

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Run a locust load test')

    subparsers = parser.add_subparsers(title='subcommands',
                                       description='valid subcommands',
                                       help='additional help')

    init_parser = subparsers.add_parser('init',
                                        help='''
    Sets up the python environment and downloads the code on each node.
    ''')
    init_parser.add_argument('--instance-names', type=str)
    init_parser.set_defaults(func=init_cmd)

    run_parser = subparsers.add_parser('run',
                                       help='''
    Runs the locust load test on each node.
    ''')
    run_parser.add_argument('--master',
                            type=str,
                            required=True,
                            help='instance name of master node')
    run_parser.add_argument(
        '--workers',
        type=str,
        required=True,
        help='comma-separated list of instance names of worker nodes')
    run_parser.add_argument(
        '--node-ip-port',
        type=str,
        required=True,
        help='IP address and port of a node in the network under test')
    run_parser.add_argument(
        '--funding-key',
        type=str,
        required=True,
        help=
        'local path to a key file for the base account to be used in the test')
    run_parser.add_argument(
        '--locustfile',
        type=str,
        default='ft.py',
        help=
        'locustfile name in nearcore/pytest/tests/loadtest/locust/locustfiles')
    run_parser.add_argument(
        '--max-workers',
        type=int,
        default=16,
        help='max number of workers the test should support')
    run_parser.add_argument(
        '--web-ui',
        action='store_true',
        help=
        'if given, sets up a web UI to control the test, otherwise starts automatically'
    )
    run_parser.add_argument(
        '--num-users',
        type=int,
        help=
        'number of users to run the test with. Required unless --web-ui is given.'
    )
    run_parser.add_argument(
        '--run-time',
        type=str,
        help=
        'A string specifying the total run time of the test, passed to the locust --run-time argument. e.g. (300s, 20m, 3h, 1h30m, etc.)'
    )
    run_parser.set_defaults(func=run_cmd)

    stop_parser = subparsers.add_parser('stop',
                                        help='''
    Stops the locust load test on each node.
    ''')
    stop_parser.add_argument('--master',
                             type=str,
                             help='instance name of master node')
    stop_parser.add_argument(
        '--workers',
        type=str,
        help='comma-separated list of instance names of worker nodes')
    stop_parser.set_defaults(func=stop_cmd)

    args = parser.parse_args()

    args.func(args)

'''
'''--- pytest/tests/mocknet/mirror.py ---
#!/usr/bin/env python3
"""

"""
from argparse import ArgumentParser, BooleanOptionalAction
import datetime
import pathlib
import json
import random
from rc import pmap
import re
import sys
import time

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from configured_logger import logger
import local_test_node
import remote_node

def prompt_setup_flags(args):
    if not args.yes:
        print(
            'this will reset all nodes\' home dirs and initialize them with new state. continue? [yes/no]'
        )
        if sys.stdin.readline().strip() != 'yes':
            sys.exit()

    if args.epoch_length is None:
        print('epoch length for the initialized genesis file?: ')
        args.epoch_length = int(sys.stdin.readline().strip())

    if args.num_validators is None:
        print('number of validators?: ')
        args.num_validators = int(sys.stdin.readline().strip())

    if args.num_seats is None:
        print('number of block producer seats?: ')
        args.num_seats = int(sys.stdin.readline().strip())

    if args.genesis_protocol_version is None:
        print('genesis protocol version?: ')
        args.genesis_protocol_version = int(sys.stdin.readline().strip())

def prompt_init_flags(args):
    if args.neard_binary_url is None:
        print('neard binary URL?: ')
        args.neard_binary_url = sys.stdin.readline().strip()
        assert len(args.neard_binary_url) > 0

    if args.neard_upgrade_binary_url is None:
        print(
            'add a second neard binary URL to upgrade to mid-test? enter nothing here to skip: '
        )
        url = sys.stdin.readline().strip()
        if len(url) > 0:
            args.neard_upgrade_binary_url = url

def init_neard_runners(args, traffic_generator, nodes, remove_home_dir=False):
    prompt_init_flags(args)
    if args.neard_upgrade_binary_url is None:
        configs = [{
            "is_traffic_generator": False,
            "binaries": [{
                "url": args.neard_binary_url,
                "epoch_height": 0
            }]
        }] * len(nodes)
        traffic_generator_config = {
            "is_traffic_generator": True,
            "binaries": [{
                "url": args.neard_binary_url,
                "epoch_height": 0
            }]
        }
    else:
        # for now this test starts all validators with the same stake, so just make the upgrade
        # epoch random. If we change the stakes, we should change this to choose how much stake
        # we want to upgrade during each epoch
        configs = []
        for i in range(len(nodes)):
            configs.append({
                "is_traffic_generator":
                    False,
                "binaries": [{
                    "url": args.neard_binary_url,
                    "epoch_height": 0
                }, {
                    "url": args.neard_upgrade_binary_url,
                    "epoch_height": random.randint(1, 4)
                }]
            })
        traffic_generator_config = {
            "is_traffic_generator":
                True,
            "binaries": [{
                "url": args.neard_upgrade_binary_url,
                "epoch_height": 0
            }]
        }

    traffic_generator.init_neard_runner(traffic_generator_config,
                                        remove_home_dir)
    pmap(lambda x: x[0].init_neard_runner(x[1], remove_home_dir),
         zip(nodes, configs))

def init_cmd(args, traffic_generator, nodes):
    init_neard_runners(args, traffic_generator, nodes, remove_home_dir=False)

def hard_reset_cmd(args, traffic_generator, nodes):
    print("""
        WARNING!!!!
        WARNING!!!!
        This will undo all chain state, which will force a restart from the beginning,
        icluding the genesis state computation which takes several hours.
        Continue? [yes/no]""")
    if sys.stdin.readline().strip() != 'yes':
        return
    init_neard_runners(args, traffic_generator, nodes, remove_home_dir=True)

def restart_cmd(args, traffic_generator, nodes):
    all_nodes = nodes + [traffic_generator]
    pmap(lambda node: node.stop_neard_runner(), all_nodes)
    if args.upload_program:
        pmap(lambda node: node.upload_neard_runner(), all_nodes)
    pmap(lambda node: node.start_neard_runner(), all_nodes)

def stop_runner_cmd(args, traffic_generator, nodes):
    pmap(lambda node: node.stop_neard_runner(), nodes + [traffic_generator])

# returns boot nodes and validators we want for the new test network
def get_network_nodes(new_test_rpc_responses, num_validators):
    validators = []
    boot_nodes = []
    for ip_addr, response in new_test_rpc_responses:
        if len(validators) < num_validators:
            if response['validator_account_id'] is not None:
                # we assume here that validator_account_id is not null, validator_public_key
                # better not be null either
                validators.append({
                    'account_id': response['validator_account_id'],
                    'public_key': response['validator_public_key'],
                    'amount': str(10**33),
                })
        if len(boot_nodes) < 20:
            boot_nodes.append(
                f'{response["node_key"]}@{ip_addr}:{response["listen_port"]}')

        if len(validators) >= num_validators and len(boot_nodes) >= 20:
            break
    # neither of these should happen, since we check the number of available nodes in new_test(), and
    # only the traffic generator will respond with null validator_account_id and validator_public_key
    if len(validators) == 0:
        sys.exit('no validators available after new_test RPCs')
    if len(validators) < num_validators:
        logger.warning(
            f'wanted {num_validators} validators, but only {len(validators)} available'
        )
    return validators, boot_nodes

def new_genesis_timestamp(node):
    version = node.neard_runner_version()
    err = version.get('error')
    if err is not None:
        if err['code'] != -32601:
            sys.exit(
                f'bad response calling version RPC on {node.name()}: {err}')
        return None
    genesis_time = None
    result = version.get('result')
    if result is not None:
        if result.get('node_setup_version') == '1':
            genesis_time = str(datetime.datetime.now(tz=datetime.UTC))
    return genesis_time

def new_test(args, traffic_generator, nodes):
    prompt_setup_flags(args)

    if args.epoch_length <= 0:
        sys.exit(f'--epoch-length should be positive')
    if args.num_validators <= 0:
        sys.exit(f'--num-validators should be positive')
    if len(nodes) < args.num_validators:
        sys.exit(
            f'--num-validators is {args.num_validators} but only found {len(nodes)} under test'
        )

    genesis_time = new_genesis_timestamp(nodes[0])

    all_nodes = nodes + [traffic_generator]

    logger.info(f'resetting/initializing home dirs')
    test_keys = pmap(lambda node: node.neard_runner_new_test(), all_nodes)

    validators, boot_nodes = get_network_nodes(
        zip([n.ip_addr() for n in all_nodes], test_keys), args.num_validators)

    logger.info("""setting validators: {0}
Then running neard amend-genesis on all nodes, and starting neard to compute genesis \
state roots. This will take a few hours. Run `status` to check if the nodes are \
ready. After they're ready, you can run `start-traffic`""".format(validators))
    pmap(
        lambda node: node.neard_runner_network_init(
            validators,
            boot_nodes,
            args.epoch_length,
            args.num_seats,
            args.genesis_protocol_version,
            genesis_time=genesis_time), all_nodes)

def status_cmd(args, traffic_generator, nodes):
    all_nodes = nodes + [traffic_generator]
    statuses = pmap(lambda node: node.neard_runner_ready(), all_nodes)
    num_ready = 0
    not_ready = []
    for ready, node in zip(statuses, all_nodes):
        if not ready:
            not_ready.append(node.name())

    if len(not_ready) == 0:
        print(f'all {len(all_nodes)} nodes ready')
    else:
        print(
            f'{len(all_nodes)-len(not_ready)}/{len(all_nodes)} ready. Nodes not ready: {not_ready[:3]}'
        )

def reset_cmd(args, traffic_generator, nodes):
    if not args.yes:
        print(
            'this will reset all nodes\' home dirs to their initial states right after test initialization finished. continue? [yes/no]'
        )
        if sys.stdin.readline().strip() != 'yes':
            sys.exit()
    if args.backup_id is None:
        backups = nodes[0].neard_runner_ls_backups()
        backups_msg = 'ID |  Time  | Description\n'
        if 'start' not in backups:
            backups_msg += 'start | None | initial test state after state root computation\n'
        for backup_id, backup_data in backups.items():
            backups_msg += f'{backup_id} | {backup_data.get("time")} | {backup_data.get("description")}\n'

        print(f'Backups as reported by {nodes[0].name()}):\n\n{backups_msg}')
        print('please enter a backup ID here:')
        args.backup_id = sys.stdin.readline().strip()
        if args.backup_id != 'start' and args.backup_id not in backups:
            print(
                f'Given backup ID ({args.backup_id}) was not in the list given')
            sys.exit()

    all_nodes = nodes + [traffic_generator]
    pmap(lambda node: node.neard_runner_reset(backup_id=args.backup_id),
         all_nodes)
    logger.info(
        'Data dir reset in progress. Run the `status` command to see when this is finished. Until it is finished, neard runners may not respond to HTTP requests.'
    )

def make_backup_cmd(args, traffic_generator, nodes):
    if not args.yes:
        print(
            'this will stop all nodes and create a new backup of their home dirs. continue? [yes/no]'
        )
        if sys.stdin.readline().strip() != 'yes':
            sys.exit()

    if args.backup_id is None:
        print('please enter a backup ID:')
        args.backup_id = sys.stdin.readline().strip()
        if re.match(r'^[0-9a-zA-Z.][0-9a-zA-Z_\-.]+$', args.backup_id) is None:
            sys.exit('invalid backup ID')
        if args.description is None:
            print('please enter a description (enter nothing to skip):')
            description = sys.stdin.readline().strip()
            if len(description) > 0:
                args.description = description

    all_nodes = nodes + [traffic_generator]
    pmap(
        lambda node: node.neard_runner_make_backup(
            backup_id=args.backup_id, description=args.description), all_nodes)

def stop_nodes_cmd(args, traffic_generator, nodes):
    pmap(lambda node: node.neard_runner_stop(), nodes + [traffic_generator])

def stop_traffic_cmd(args, traffic_generator, nodes):
    traffic_generator.neard_runner_stop()

def update_config_cmd(args, traffic_generator, nodes):
    nodes = nodes + [traffic_generator]
    results = pmap(
        lambda node: node.neard_update_config(args.set),
        nodes,
    )
    if not all(results):
        logger.warning('failed to update configs for some nodes')
        return

def start_nodes_cmd(args, traffic_generator, nodes):
    if not all(pmap(lambda node: node.neard_runner_ready(), nodes)):
        logger.warning(
            'not all nodes are ready to start yet. Run the `status` command to check their statuses'
        )
        return
    pmap(lambda node: node.neard_runner_start(), nodes)
    pmap(lambda node: node.wait_node_up(), nodes)

def start_traffic_cmd(args, traffic_generator, nodes):
    if not all(
            pmap(lambda node: node.neard_runner_ready(),
                 nodes + [traffic_generator])):
        logger.warning(
            'not all nodes are ready to start yet. Run the `status` command to check their statuses'
        )
        return
    pmap(lambda node: node.neard_runner_start(), nodes)
    logger.info("waiting for validators to be up")
    pmap(lambda node: node.wait_node_up(), nodes)
    logger.info(
        "waiting a bit after validators started before starting traffic")
    time.sleep(10)
    traffic_generator.neard_runner_start(
        batch_interval_millis=args.batch_interval_millis)
    logger.info(
        f'test running. to check the traffic sent, try running "curl --silent http://{traffic_generator.ip_addr()}:{traffic_generator.neard_port()}/metrics | grep near_mirror"'
    )

def update_binaries_cmd(args, traffic_generator, nodes):
    pmap(lambda node: node.neard_runner_update_binaries(),
         nodes + [traffic_generator])

if __name__ == '__main__':
    parser = ArgumentParser(description='Control a mocknet instance')
    parser.add_argument('--chain-id', type=str)
    parser.add_argument('--start-height', type=int)
    parser.add_argument('--unique-id', type=str)
    parser.add_argument('--local-test', action='store_true')

    subparsers = parser.add_subparsers(title='subcommands',
                                       description='valid subcommands',
                                       help='additional help')

    init_parser = subparsers.add_parser('init-neard-runner',
                                        help='''
    Sets up the helper servers on each of the nodes. Doesn't start initializing the test
    state, which is done with the `new-test` command.
    ''')
    init_parser.add_argument('--neard-binary-url', type=str)
    init_parser.add_argument('--neard-upgrade-binary-url', type=str)
    init_parser.set_defaults(func=init_cmd)

    update_config_parser = subparsers.add_parser(
        'update-config',
        help='''Update config.json with given flags for all nodes.''')
    update_config_parser.add_argument(
        '--set',
        help='''
        A key value pair to set in the config. The key will be interpreted as a
        json path to the config to be updated. The value will be parsed as json.   
        e.g.
        --set 'aaa.bbb.ccc=5'
        --set 'aaa.bbb.ccc="5"'
        --set 'aaa.bbb.ddd={"eee":6,"fff":"7"}' # no spaces!
        ''',
    )
    update_config_parser.set_defaults(func=update_config_cmd)

    restart_parser = subparsers.add_parser(
        'restart-neard-runner',
        help='''Restarts the neard runner on all nodes.''')
    restart_parser.add_argument('--upload-program', action='store_true')
    restart_parser.set_defaults(func=restart_cmd, upload_program=False)

    stop_runner_parser = subparsers.add_parser(
        'stop-neard-runner', help='''Stops the neard runner on all nodes.''')
    stop_runner_parser.set_defaults(func=stop_runner_cmd)

    hard_reset_parser = subparsers.add_parser(
        'hard-reset',
        help='''Stops neard and clears all test state on all nodes.''')
    hard_reset_parser.add_argument('--neard-binary-url', type=str)
    hard_reset_parser.add_argument('--neard-upgrade-binary-url', type=str)
    hard_reset_parser.set_defaults(func=hard_reset_cmd)

    new_test_parser = subparsers.add_parser('new-test',
                                            help='''
    Sets up new state from the prepared records and genesis files with the number
    of validators specified. This calls neard amend-genesis to create the new genesis
    and records files, and then starts the neard nodes and waits for them to be online
    after computing the genesis state roots. This step takes a long time (a few hours).
    ''')
    new_test_parser.add_argument('--epoch-length', type=int)
    new_test_parser.add_argument('--num-validators', type=int)
    new_test_parser.add_argument('--num-seats', type=int)
    new_test_parser.add_argument('--genesis-protocol-version', type=int)
    new_test_parser.add_argument('--yes', action='store_true')
    new_test_parser.set_defaults(func=new_test)

    status_parser = subparsers.add_parser(
        'status',
        help='''Checks the status of test initialization on each node''')
    status_parser.set_defaults(func=status_cmd)

    start_traffic_parser = subparsers.add_parser(
        'start-traffic',
        help=
        'Starts all nodes and starts neard mirror run on the traffic generator.'
    )
    start_traffic_parser.add_argument(
        '--batch-interval-millis',
        type=int,
        help=
        '''Interval in millis between sending each mainnet block\'s worth of transactions.
        Without this flag, the traffic generator will try to match the per-block load on mainnet.
        So, transactions from consecutive mainnet blocks will be be sent with delays
        between them such that they will probably appear in consecutive mocknet blocks.
        ''')
    start_traffic_parser.set_defaults(func=start_traffic_cmd)

    start_nodes_parser = subparsers.add_parser(
        'start-nodes',
        help='Starts all nodes, but does not start the traffic generator.')
    start_nodes_parser.set_defaults(func=start_nodes_cmd)

    stop_parser = subparsers.add_parser('stop-nodes',
                                        help='kill all neard processes')
    stop_parser.set_defaults(func=stop_nodes_cmd)

    stop_parser = subparsers.add_parser(
        'stop-traffic',
        help='stop the traffic generator, but leave the other nodes running')
    stop_parser.set_defaults(func=stop_traffic_cmd)

    backup_parser = subparsers.add_parser('make-backup',
                                          help='''
    Stops all nodes and haves them make a backup of the data dir that can later be restored to with the reset command
    ''')
    backup_parser.add_argument('--yes', action='store_true')
    backup_parser.add_argument('--backup-id', type=str)
    backup_parser.add_argument('--description', type=str)
    backup_parser.set_defaults(func=make_backup_cmd)

    reset_parser = subparsers.add_parser('reset',
                                         help='''
    The new_test command saves the data directory after the genesis state roots are computed so that
    the test can be reset from the start without having to do that again. This command resets all nodes'
    data dirs to what was saved then, so that start-traffic will start the test all over again.
    ''')
    reset_parser.add_argument('--yes', action='store_true')
    reset_parser.add_argument('--backup-id', type=str)
    reset_parser.set_defaults(func=reset_cmd)

    # It re-uses the same binary urls because it's quite easy to do it with the
    # nearcore-release buildkite and urls in the following format without commit
    # but only with the branch name:
    # https://s3-us-west-1.amazonaws.com/build.nearprotocol.com/nearcore/Linux/<branch-name>/neard"
    update_binaries_parser = subparsers.add_parser(
        'update-binaries',
        help=
        'Update the neard binaries by re-downloading them. The same urls are used.'
    )
    update_binaries_parser.set_defaults(func=update_binaries_cmd)

    args = parser.parse_args()

    if args.local_test:
        if args.chain_id is not None or args.start_height is not None or args.unique_id is not None:
            sys.exit(
                f'cannot give --chain-id --start-height or --unique-id along with --local-test'
            )
        traffic_generator, nodes = local_test_node.get_nodes()
    else:
        if args.chain_id is None or args.start_height is None or args.unique_id is None:
            sys.exit(
                f'must give all of --chain-id --start-height and --unique-id')
        traffic_generator, nodes = remote_node.get_nodes(
            args.chain_id, args.start_height, args.unique_id)
    args.func(args, traffic_generator, nodes)

'''
'''--- pytest/tests/mocknet/node_handle.py ---
import pathlib
import requests
import sys
import time

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from configured_logger import logger

class NodeHandle:

    def __init__(self, node):
        self.node = node

    def name(self):
        return self.node.name()

    def ip_addr(self):
        return self.node.ip_addr()

    def neard_port(self):
        return self.node.neard_port()

    def stop_neard_runner(self):
        self.node.stop_neard_runner()

    def start_neard_runner(self):
        self.node.start_neard_runner()

    def upload_neard_runner(self):
        self.node.upload_neard_runner()

    def init_neard_runner(self, config, remove_home_dir=False):
        self.node.stop_neard_runner()
        self.node.init()
        self.node.mk_neard_runner_home(remove_home_dir)
        self.node.upload_neard_runner()
        # TODO: this config file should just be replaced by parameters to the new-test
        # rpc method. This was originally made a config file instead because the rpc port
        # was open to the internet, but now that we call it via ssh instead (which we should
        # have done from the beginning), it's not really necessary and just an arbitrary difference
        self.node.upload_neard_runner_config(config)
        self.node.init_python()
        self.node.start_neard_runner()

    # TODO: is the validators RPC the best way to do this? What are we trying to
    # test for exactly? The use of this is basically just cargo culted from a while ago,
    # but maybe we should consider something else
    def wait_node_up(self):
        while True:
            try:
                res = self.node.get_validators()
                if 'error' not in res:
                    assert 'result' in res
                    logger.info(f'Node {self.node.name()} is up')
                    return
            except (ConnectionRefusedError,
                    requests.exceptions.ConnectionError) as e:
                pass
            time.sleep(10)

    # Same as neard_runner_jsonrpc() without checking the error
    # This should maybe be the behavior everywhere, and callers
    # should handle errors themselves
    def neard_runner_jsonrpc_nocheck(self, method, params=[]):
        body = {
            'method': method,
            'params': params,
            'id': 'dontcare',
            'jsonrpc': '2.0'
        }
        return self.node.neard_runner_post(body)

    def neard_runner_jsonrpc(self, method, params=[]):
        response = self.neard_runner_jsonrpc_nocheck(method, params)
        if 'error' in response:
            # TODO: errors should be handled better here in general but just exit for now
            sys.exit(
                f'bad response trying to send {method} JSON RPC to neard runner on {self.node.name()}:\n{response}'
            )
        return response['result']

    def neard_runner_start(self, batch_interval_millis=None):
        if batch_interval_millis is None:
            params = []
        else:
            params = {'batch_interval_millis': batch_interval_millis}
        return self.neard_runner_jsonrpc('start', params=params)

    def neard_runner_stop(self):
        return self.neard_runner_jsonrpc('stop')

    def neard_runner_new_test(self):
        params = self.node.new_test_params()
        return self.neard_runner_jsonrpc('new_test', params)

    def neard_runner_network_init(self,
                                  validators,
                                  boot_nodes,
                                  epoch_length,
                                  num_seats,
                                  protocol_version,
                                  genesis_time=None):
        params = {
            'validators': validators,
            'boot_nodes': boot_nodes,
            'epoch_length': epoch_length,
            'num_seats': num_seats,
            'protocol_version': protocol_version,
        }
        if genesis_time is not None:
            params['genesis_time'] = genesis_time
        return self.neard_runner_jsonrpc('network_init', params=params)

    def neard_runner_ready(self):
        return self.neard_runner_jsonrpc('ready')

    def neard_runner_version(self):
        return self.neard_runner_jsonrpc_nocheck('version')

    def neard_runner_make_backup(self, backup_id, description=None):
        return self.neard_runner_jsonrpc('make_backup',
                                         params={
                                             'backup_id': backup_id,
                                             'description': description
                                         })

    def neard_runner_ls_backups(self):
        return self.neard_runner_jsonrpc('ls_backups')

    def neard_runner_reset(self, backup_id=None):
        return self.neard_runner_jsonrpc('reset',
                                         params={'backup_id': backup_id})

    def neard_runner_update_binaries(self):
        return self.neard_runner_jsonrpc('update_binaries')

    def neard_update_config(self, key_value):
        return self.neard_runner_jsonrpc(
            'update_config',
            params={
                "key_value": key_value,
            },
        )

'''
'''--- pytest/tests/mocknet/remote_node.py ---
#!/usr/bin/env python3
"""
defines the RemoteNeardRunner class meant to be interacted with over ssh
"""
import pathlib
import json
import sys

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

import cmd_utils
from node_handle import NodeHandle
import mocknet

class RemoteNeardRunner:

    def __init__(self, node):
        self.node = node

    def name(self):
        return self.node.instance_name

    def ip_addr(self):
        return self.node.machine.ip

    def neard_port(self):
        return 3030

    def init(self):
        cmd_utils.init_node(self.node)

    def mk_neard_runner_home(self, remove_home_dir):
        if remove_home_dir:
            cmd_utils.run_cmd(
                self.node,
                'rm -rf /home/ubuntu/.near/neard-runner && mkdir -p /home/ubuntu/.near/neard-runner'
            )
        else:
            cmd_utils.run_cmd(self.node,
                              'mkdir -p /home/ubuntu/.near/neard-runner')

    def upload_neard_runner(self):
        self.node.machine.upload('tests/mocknet/helpers/neard_runner.py',
                                 '/home/ubuntu/.near/neard-runner',
                                 switch_user='ubuntu')
        self.node.machine.upload('tests/mocknet/helpers/requirements.txt',
                                 '/home/ubuntu/.near/neard-runner',
                                 switch_user='ubuntu')

    def upload_neard_runner_config(self, config):
        mocknet.upload_json(self.node,
                            '/home/ubuntu/.near/neard-runner/config.json',
                            config)

    def init_python(self):
        cmd = 'cd /home/ubuntu/.near/neard-runner && python3 -m virtualenv venv -p $(which python3)' \
        ' && ./venv/bin/pip install -r requirements.txt'
        cmd_utils.run_cmd(self.node, cmd)

    def stop_neard_runner(self):
        # this looks for python processes with neard_runner.py in the command line. the first word will
        # be the pid, which we extract with the last awk command
        self.node.machine.run(
            'kill $(ps -C python -o pid=,cmd= | grep neard_runner.py | awk \'{print $1};\')'
        )

    def start_neard_runner(self):
        cmd_utils.run_in_background(self.node, f'/home/ubuntu/.near/neard-runner/venv/bin/python /home/ubuntu/.near/neard-runner/neard_runner.py ' \
            '--home /home/ubuntu/.near/neard-runner --neard-home /home/ubuntu/.near ' \
            '--neard-logs /home/ubuntu/neard-logs --port 3000', 'neard-runner.txt')

    def neard_runner_post(self, body):
        body = json.dumps(body)
        # '"'"' will be interpreted as ending the first quote and then concatenating it with "'",
        # followed by a new quote started with ' and the rest of the string, to get any single quotes
        # in method or params into the command correctly
        body = body.replace("'", "'\"'\"'")
        r = cmd_utils.run_cmd(self.node, f'curl localhost:3000 -d \'{body}\'')
        return json.loads(r.stdout)

    def new_test_params(self):
        return []

    def get_validators(self):
        return self.node.get_validators()

def get_nodes(chain_id, start_height, unique_id):
    pattern = chain_id + '-' + str(start_height) + '-' + unique_id
    all_nodes = mocknet.get_nodes(pattern=pattern)
    if len(all_nodes) < 1:
        sys.exit(f'no known nodes matching {pattern}')

    traffic_generator = None
    nodes = []
    for n in all_nodes:
        if n.instance_name.endswith('traffic'):
            if traffic_generator is not None:
                sys.exit(
                    f'more than one traffic generator instance found. {traffic_generator.instance_name} and {n.instance_name}'
                )
            traffic_generator = n
        else:
            nodes.append(n)

    if traffic_generator is None:
        sys.exit(f'no traffic generator instance found')
    return NodeHandle(RemoteNeardRunner(traffic_generator)), [
        NodeHandle(RemoteNeardRunner(node)) for node in nodes
    ]

'''
'''--- pytest/tests/mocknet/run_adversenet.py ---
#!/usr/bin/env python3

help_str = """
This script starts or updates a network of adversenet, in which some validators may be malicious.

The setup requires you to have a few nodes that will be validators and at least one node that will be an RPC node.
The script will recognize any node that has "validator" in its name as a validator, of which any node that has a
"bad" in its name as an adversarial node.

Use https://github.com/near/near-ops/tree/master/provisioning/terraform/network/adversenet to bring up a set of VM
instances for the test.

Example command to drive the cluster:
  python3 pytest/tests/mocknet/run_adversenet.py new \
    --chain-id mynetwork \
    --pattern mynetwork-node- \
    --epoch-length 100 \
    --num-seats 5 \
    --binary-url http://something/neard

Example command to send load to the cluster:
  KEY=~/nearcore/pytest/tests/mocknet/adversenet_loadtest_key.json
  cd pytest/tests/loadtest/locust
  locust -H <rpc_node>:3030 -f locustfiles/ft.py --funding-key=$KEY
"""

import argparse
import sys
import time
from enum import Enum
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

import mocknet
import data

from metrics import Metrics
from configured_logger import logger

def measure_tps_bps(nodes, tx_filename):
    input_tx_events = mocknet.get_tx_events(nodes, tx_filename)
    # drop first and last 5% of events to avoid edges of test
    n = int(0.05 * len(input_tx_events))
    input_tx_events = input_tx_events[n:-n]
    input_tps = data.compute_rate(input_tx_events)
    measurement = mocknet.chain_measure_bps_and_tps(nodes[-1],
                                                    input_tx_events[0],
                                                    input_tx_events[-1])
    result = {
        'bps': measurement['bps'],
        'in_tps': input_tps,
        'out_tps': measurement['tps']
    }
    logger.info(f'{result}')
    return result

def check_tps(measurement, expected_in, expected_out=None, tolerance=0.05):
    if expected_out is None:
        expected_out = expected_in
    almost_equal = lambda x, y: (abs(x - y) / y) <= tolerance
    return (almost_equal(measurement['in_tps'], expected_in) and
            almost_equal(measurement['out_tps'], expected_out))

def check_memory_usage(node):
    metrics = mocknet.get_metrics(node)
    mem_usage = metrics.memory_usage / 1e6
    logger.info(f'Memory usage (MB) = {mem_usage}')
    return mem_usage < 4500

def check_slow_blocks(initial_metrics, final_metrics):
    delta = Metrics.diff(final_metrics, initial_metrics)
    slow_process_blocks = delta.block_processing_time[
        'le +Inf'] - delta.block_processing_time['le 1']
    logger.info(
        f'Number of blocks processing for more than 1s: {slow_process_blocks}')
    return slow_process_blocks == 0

def override_config(node, config):
    # Add config here depending on the specific node build.
    pass
    """
    if "bad" in node.instance_name:
        config["adversarial"] = {
            "produce_duplicate_blocks": True
        }
    """

class Role(Enum):
    Rpc = 0
    GoodValidator = 1
    BadValidator = 2

def get_role(node):
    if "validator" not in node.instance_name:
        return Role.Rpc
    elif "bad" in node.instance_name:
        return Role.BadValidator
    else:
        return Role.GoodValidator

if __name__ == '__main__':
    logger.info('Starting adversenet.')
    parser = argparse.ArgumentParser(description=help_str)
    parser.add_argument(
        'mode',
        choices=["new", "update"],
        help=
        "new: start a new network from scratch, update: update existing network"
    )
    parser.add_argument('--chain-id', required=False, default="adversenet")
    parser.add_argument('--pattern',
                        required=False,
                        default="adversenet-node-",
                        help="pattern to filter the gcp instance names")
    parser.add_argument(
        '--epoch-length',
        type=int,
        required=False,
        default=60,
        help="epoch length of the network. Only used when mode == new")
    parser.add_argument(
        '--num-seats',
        type=int,
        required=False,
        default=100,
        help="number of validator seats. Only used when mode == new")
    parser.add_argument('--bad-stake',
                        required=False,
                        default=5,
                        type=int,
                        help="Total stake percentage for bad validators")
    parser.add_argument('--binary-url',
                        required=False,
                        help="url to download neard binary")

    args = parser.parse_args()

    chain_id = args.chain_id
    pattern = args.pattern
    epoch_length = args.epoch_length
    assert epoch_length > 0

    all_nodes = mocknet.get_nodes(pattern=pattern)
    rpcs = [n.instance_name for n in all_nodes if get_role(n) == Role.Rpc]
    good_validators = [
        n.instance_name for n in all_nodes if get_role(n) == Role.GoodValidator
    ]
    bad_validators = [
        n.instance_name for n in all_nodes if get_role(n) == Role.BadValidator
    ]
    print("Good validators: ", good_validators)
    print("Bad validators: ", bad_validators)
    TOTAL_STAKE = 1000000
    bad_validator_stake = int(
        TOTAL_STAKE * args.bad_stake /
        (100 * len(bad_validators))) if len(bad_validators) > 0 else 0
    good_validator_stake = int(TOTAL_STAKE * (100 - args.bad_stake) /
                               (100 * len(good_validators)))

    logger.info(f'Starting chain {chain_id} with {len(all_nodes)} nodes. \n\
        Good validators: {good_validators} each with stake {good_validator_stake} NEAR\n\
        Bad validators: {bad_validators} each with stake {bad_validator_stake} NEAR\n\
        RPC nodes: {rpcs}\n')

    answer = input("Enter y to continue: ")
    if answer != "y":
        exit(0)

    mocknet.stop_nodes(all_nodes)
    time.sleep(10)
    validator_nodes = [n for n in all_nodes if get_role(n) != Role.Rpc]
    rpc_nodes = [n for n in all_nodes if get_role(n) == Role.Rpc]
    if args.binary_url:
        mocknet.redownload_neard(all_nodes, args.binary_url)
    if args.mode == "new":
        logger.info(f'Configuring nodes from scratch')
        mocknet.clear_data(all_nodes)
        mocknet.create_and_upload_genesis_file_from_empty_genesis(
            # Give bad validators less stake.
            [(node, bad_validator_stake * mocknet.ONE_NEAR if get_role(node)
              == Role.BadValidator else good_validator_stake * mocknet.ONE_NEAR)
             for node in validator_nodes],
            rpc_nodes,
            chain_id,
            epoch_length=epoch_length,
            num_seats=args.num_seats)
        mocknet.create_and_upload_config_file_from_default(
            all_nodes, chain_id, override_config)
    else:
        mocknet.update_existing_config_files(all_nodes, override_config)
    mocknet.start_nodes(all_nodes)
    mocknet.wait_all_nodes_up(all_nodes)

'''
'''--- pytest/tests/mocknet/stop.py ---
# Stop all mocknet nodes, wait 1s, then start all nodes again.
# Nodes should be responsive again after this operation.

import sys
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))
import mocknet

nodes = mocknet.get_nodes()

# stop nodes
mocknet.stop_nodes(nodes)

'''
'''--- pytest/tests/replay/replay.py ---
# TODO(#7132): Convert this test to a nightly regression test.

from account import Account
from collections import OrderedDict
from key import Key
from messages.tx import *
from messages.crypto import AccessKey, crypto_schema, PublicKey, Signature
from messages.bridge import bridge_schema
from serializer import BinarySerializer
import mocknet_helpers

import argparse
import base58
import base64
import hashlib
import json
import os

LOCALHOST = '127.0.0.1'

def generate_new_key():
    return Key.implicit_account()

def save_genesis_with_new_key_pair(genesis_path, key_pair, output_path):
    node0_dir = os.path.join(output_path, 'node0/')
    if not os.path.exists(node0_dir):
        os.makedirs(node0_dir)
    with open(genesis_path) as fin:
        genesis = json.load(fin)

    new_key = key_pair.pk.split(':')[1] if ':' in key_pair.pk else key_pair.pk
    # TODO(jc-near): Use different key pairs for different accounts.
    for i in range(len(genesis['validators'])):
        genesis['validators'][i]['public_key'] = new_key
    for record in genesis['records']:
        if 'AccessKey' in record:
            record['AccessKey']['public_key'] = new_key
    with open(os.path.join(node0_dir, 'genesis.json'), 'w') as fout:
        json.dump(genesis, fout, indent=2)

    key_pair.account_id = genesis['validators'][0]['account_id']
    key_json = dict()
    key_json['account_id'] = genesis['validators'][0]['account_id']
    key_json['public_key'] = key_pair.pk
    key_json['secret_key'] = key_pair.sk
    with open(os.path.join(node0_dir, 'node_key.json'), 'w') as fout:
        json.dump(key_json, fout, indent=2)
    with open(os.path.join(node0_dir, 'validator_key.json'), 'w') as fout:
        json.dump(key_json, fout, indent=2)

def prompt_to_launch_localnet(hint_dir):
    print('New genesis ready.')
    print('Please launch your localnet node now.')
    print('Hint: nearup run localnet --home %s --num-nodes 1' % hint_dir)

def convert_snack_case_to_camel_case(s):
    words = s.split('_')
    return words[0] + ''.join(w.title() for w in words[1:])

def convert_json_rust_instance_to_py_object(ordered_dict_instance, class_name):
    ans = class_name()
    for attr_key, attr_value in ordered_dict_instance.items():
        setattr(ans, convert_snack_case_to_camel_case(attr_key), attr_value)
    return ans

def convert_transaction_type_string_to_class(name):
    tx_class_by_type = {
        "CreateAccount": CreateAccount,
        "DeleteAccount": DeleteAccount,
        "DeployContract": DeployContract,
        "FunctionCall": FunctionCall,
        "Transfer": Transfer,
        "Stake": Stake,
        "AddKey": AddKey,
        "DeleteKey": DeleteKey,
    }
    if name in tx_class_by_type:
        return tx_class_by_type[name]
    raise ValueError('Unknown tx type: %s' % name)

def convert_json_public_key_to_py_public_key(json_public_key):
    pk_str = json_public_key.split(
        ':')[1] if ':' in json_public_key else json_public_key
    ans = PublicKey()
    ans.keyType = 0
    ans.data = base58.b58decode(pk_str.encode('ascii'))
    return ans

'''
Get tx fields read from json ready to be serialized.
See `pytest/lib/messages/` for tx schemas desired.
'''

def fix_json_fields_by_tx_type(py_tx, tx_type):
    if tx_type == "CreateAccount":
        pass
    elif tx_type == "DeleteAccount":
        pass
    elif tx_type == "DeployContract":
        py_tx.code = base64.b64decode(py_tx.code.encode('ascii'))
    elif tx_type == "FunctionCall":
        py_tx.args = base64.b64decode(py_tx.args.encode('ascii'))
        py_tx.deposit = int(py_tx.deposit)
    elif tx_type == "Transfer":
        py_tx.deposit = int(py_tx.deposit)
    elif tx_type == "Stake":
        py_tx.stake = int(py_tx.stake)
        py_tx.publicKey = convert_json_public_key_to_py_public_key(
            py_tx.publicKey)
    else:
        raise ValueError('Unsupported tx type: %s' % tx_type)
    return py_tx

def convert_json_action_to_py_action(action_dict):
    assert (len(action_dict) == 1)
    for tx_type, value in action_dict.items():
        contents = convert_json_rust_instance_to_py_object(
            value, convert_transaction_type_string_to_class(tx_type))
        contents = fix_json_fields_by_tx_type(contents, tx_type)
        action = Action()
        action.enum = tx_type[0].lower() + tx_type[1:]
        setattr(action, action.enum, contents)
        return action

def send_resigned_transactions(tx_path, home_dir):
    with open(os.path.join(home_dir, 'node0/', 'node_key.json'), 'r') as fin:
        key_pair_json = json.load(fin)
    key_pair = Key(key_pair_json['account_id'], key_pair_json['public_key'],
                   key_pair_json['secret_key'])
    base_block_hash = mocknet_helpers.get_latest_block_hash(addr=LOCALHOST)
    my_account = Account(key_pair,
                         init_nonce=0,
                         base_block_hash=base_block_hash,
                         rpc_infos=[(LOCALHOST, "3030")])

    schema = dict(tx_schema + crypto_schema + bridge_schema)
    with open(tx_path) as fin:
        txs = json.load(fin, object_pairs_hook=OrderedDict)
    for original_signed_tx in txs:
        tx = convert_json_rust_instance_to_py_object(
            original_signed_tx['transaction'], Transaction)
        if hasattr(tx, 'blockHash'):
            tx.blockHash = base_block_hash
        if hasattr(tx, 'actions'):
            try:
                tx.actions = [
                    convert_json_action_to_py_action(action_dict)
                    for action_dict in tx.actions
                ]
            except ValueError:
                continue
        tx.publicKey = PublicKey()
        tx.publicKey.keyType = 0
        tx.publicKey.data = key_pair.decoded_pk()
        msg = BinarySerializer(schema).serialize(tx)
        hash_ = hashlib.sha256(msg).digest()
        signature = Signature()
        signature.keyType = 0
        signature.data = key_pair.sign_bytes(hash_)
        resigned_tx = SignedTransaction()
        resigned_tx.transaction = tx
        resigned_tx.signature = signature
        resigned_tx.hash = hash_
        my_account.send_tx(BinarySerializer(schema).serialize(resigned_tx))

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Setup replay')
    parser.add_argument('operation',
                        type=str,
                        help="choose between [generate/send]")
    parser.add_argument('--tx-json',
                        type=str,
                        required=False,
                        help="Path of tx history json")
    parser.add_argument('--genesis',
                        type=str,
                        required=False,
                        help="Path of genesis")
    parser.add_argument('--home-dir',
                        type=str,
                        required=True,
                        help="Path of the new home directory")
    args = parser.parse_args()

    if args.operation == 'generate':
        if args.genesis:
            key_pair = generate_new_key()
            save_genesis_with_new_key_pair(args.genesis, key_pair,
                                           args.home_dir)
            prompt_to_launch_localnet(args.home_dir)
        else:
            parser.error('Cannot run generate without genesis')
    elif args.operation == 'send':
        if args.tx_json:
            send_resigned_transactions(args.tx_json, args.home_dir)
        else:
            parser.error('Cannot run send without tx history')
    else:
        parser.error(
            'Unsupported positional argument: replay [operation] where operation = %s'
            % args.opeartion)

'''
'''--- pytest/tests/sandbox/fast_forward.py ---
#!/usr/bin/env python3
# test fast fowarding by a specific block height within a sandbox node. This will
# fail if the block height is not past the forwarded height. Also we will test
# for the timestamps and epoch height being adjusted correctly after the block
# height is changed.

import datetime
import sys
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

import utils
from cluster import start_cluster

# startup a RPC node
MIN_BLOCK_PROD_TIME = 1  # seconds
MAX_BLOCK_PROD_TIME = 2  # seconds
EPOCH_LENGTH = 100
BLOCKS_TO_FASTFORWARD = 4 * EPOCH_LENGTH
CONFIG = utils.figure_out_sandbox_binary()
CONFIG.update({
    "consensus": {
        "min_block_production_delay": {
            "secs": MIN_BLOCK_PROD_TIME,
            "nanos": 0,
        },
        "max_block_production_delay": {
            "secs": MAX_BLOCK_PROD_TIME,
            "nanos": 0,
        },
    }
})

nodes = start_cluster(1, 0, 1, CONFIG, [["epoch_length", EPOCH_LENGTH]], {})
sync_info = nodes[0].get_status()['sync_info']
pre_forward_block_hash = sync_info['latest_block_hash']

# request to fast forward
nodes[0].json_rpc('sandbox_fast_forward',
                  {"delta_height": BLOCKS_TO_FASTFORWARD},
                  timeout=60)

# wait a little for it to fast forward
# if this call times out, then the fast_forward failed somewhere
utils.wait_for_blocks(nodes[0], target=BLOCKS_TO_FASTFORWARD + 10, timeout=10)

# Assert that we're within the bounds of fast forward timestamp between range of min and max:
sync_info = nodes[0].get_status()['sync_info']
earliest = datetime.datetime.strptime(sync_info['earliest_block_time'][:-4],
                                      '%Y-%m-%dT%H:%M:%S.%f')
latest = datetime.datetime.strptime(sync_info['latest_block_time'][:-4],
                                    '%Y-%m-%dT%H:%M:%S.%f')

min_forwarded_secs = datetime.timedelta(
    0, BLOCKS_TO_FASTFORWARD * MIN_BLOCK_PROD_TIME)
max_forwarded_secs = datetime.timedelta(
    0, BLOCKS_TO_FASTFORWARD * MAX_BLOCK_PROD_TIME)
min_forwarded_time = earliest + min_forwarded_secs
max_forwarded_time = earliest + max_forwarded_secs

assert min_forwarded_time < latest < max_forwarded_time

# Check to see that the epoch height has been updated correctly:
epoch_height = nodes[0].get_validators()['result']['epoch_height']
assert epoch_height == BLOCKS_TO_FASTFORWARD // EPOCH_LENGTH

# Check if queries aren't failing after fast forwarding:
resp = nodes[0].json_rpc("block", {"finality": "optimistic"})
assert resp['result']['chunks'][0]['height_created'] > BLOCKS_TO_FASTFORWARD
resp = nodes[0].json_rpc("block", {"finality": "final"})
assert resp['result']['chunks'][0]['height_created'] > BLOCKS_TO_FASTFORWARD

# Not necessarily a requirement, but current implementation should be able to retrieve
# one of the blocks before fast-forwarding:
resp = nodes[0].json_rpc("block", {"block_id": pre_forward_block_hash})
assert resp['result']['chunks'][0]['height_created'] < BLOCKS_TO_FASTFORWARD

# do one more fast forward request just so we make sure consecutive requests
# don't crash anything on the node
nodes[0].json_rpc('sandbox_fast_forward',
                  {"delta_height": BLOCKS_TO_FASTFORWARD},
                  timeout=60)
resp = nodes[0].json_rpc("block", {"finality": "optimistic"})
assert resp['result']['chunks'][0]['height_created'] > 2 * BLOCKS_TO_FASTFORWARD

'''
'''--- pytest/tests/sandbox/fast_forward_epoch_boundary.py ---
#!/usr/bin/env python3
# test fast fowarding on epoch boundaries just so we can see that epoch heights
# are being updated accordingly once we get near the boundary.

import datetime
import sys
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

import utils
from cluster import start_cluster

# startup a RPC node
MIN_BLOCK_PROD_TIME = 1  # seconds
MAX_BLOCK_PROD_TIME = 2  # seconds
EPOCH_LENGTH = 100
CONFIG = utils.figure_out_sandbox_binary()
CONFIG.update({
    "consensus": {
        "min_block_production_delay": {
            "secs": MIN_BLOCK_PROD_TIME,
            "nanos": 0,
        },
        "max_block_production_delay": {
            "secs": MAX_BLOCK_PROD_TIME,
            "nanos": 0,
        },
    }
})

nodes = start_cluster(1, 0, 1, CONFIG, [["epoch_length", EPOCH_LENGTH]], {})

# start at block_height = 10
utils.wait_for_blocks(nodes[0], target=10)
# fast forward to about block_height=190 and then test for boundaries
nodes[0].json_rpc('sandbox_fast_forward', {"delta_height": 180}, timeout=60)
for i in range(20):
    utils.wait_for_blocks(nodes[0], target=190 + i)
    block_height = nodes[0].get_latest_block().height
    epoch_height = nodes[0].get_validators()['result']['epoch_height']
    assert epoch_height == 2 if block_height > 200 else 1

# check that we still have correct epoch heights after consecutive fast forwards:
utils.wait_for_blocks(nodes[0], target=220)
nodes[0].json_rpc('sandbox_fast_forward', {"delta_height": 70}, timeout=60)
for i in range(20):
    utils.wait_for_blocks(nodes[0], target=290 + i)
    block_height = nodes[0].get_latest_block().height
    epoch_height = nodes[0].get_validators()['result']['epoch_height']
    assert epoch_height == 3 if block_height > 300 else 2

'''
'''--- pytest/tests/sandbox/patch_state.py ---
#!/usr/bin/env python3
# Patch contract states in a sandbox node

import sys, time
import base64
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

import utils
from cluster import start_cluster
from transaction import sign_deploy_contract_tx, sign_function_call_tx

CONFIG = utils.figure_out_sandbox_binary()

# start node
nodes = start_cluster(1, 0, 1, CONFIG, [["epoch_length", 10]], {})

# deploy contract
hash_ = nodes[0].get_latest_block().hash_bytes
tx = sign_deploy_contract_tx(nodes[0].signer_key, utils.load_test_contract(),
                             10, hash_)
nodes[0].send_tx(tx)
time.sleep(3)

# store a key value
hash_ = nodes[0].get_latest_block().hash_bytes
k = (10).to_bytes(8, byteorder="little")
v = (20).to_bytes(8, byteorder="little")
tx2 = sign_function_call_tx(nodes[0].signer_key, nodes[0].signer_key.account_id,
                            'write_key_value', k + v, 1000000000000, 0, 20,
                            hash_)
res = nodes[0].send_tx_and_wait(tx2, 20)
assert ('SuccessValue' in res['result']['status'])
res = nodes[0].call_function("test0", "read_value",
                             base64.b64encode(k).decode('ascii'))
assert (res['result']['result'] == list(v))

# patch it
new_v = (30).to_bytes(8, byteorder="little")
res = nodes[0].json_rpc(
    'sandbox_patch_state', {
        "records": [{
            'Data': {
                'account_id': "test0",
                'data_key': base64.b64encode(k).decode('ascii'),
                'value': base64.b64encode(new_v).decode('ascii'),
            }
        }, {
            "Account": {
                "account_id": "01.near",
                "account": {
                    "amount": "49999999958035075000000000",
                    "locked": "0",
                    "code_hash": "11111111111111111111111111111111",
                    "storage_usage": 264
                }
            }
        }, {
            "Account": {
                "account_id": "alex.near",
                "account": {
                    "amount": "9999000000000000000000000",
                    "locked": "0",
                    "code_hash": "11111111111111111111111111111111",
                    "storage_usage": 182
                }
            }
        }, {
            "Account": {
                "account_id": "bo.near",
                "account": {
                    "amount": "50000000000000000000000000",
                    "locked": "0",
                    "code_hash": "11111111111111111111111111111111",
                    "storage_usage": 182
                }
            }
        }, {
            "Account": {
                "account_id": "bot.pulse.near",
                "account": {
                    "amount": "791373397694044304600000",
                    "locked": "0",
                    "code_hash": "11111111111111111111111111111111",
                    "storage_usage": 182
                }
            }
        }, {
            "Account": {
                "account_id": "bowen.near",
                "account": {
                    "amount": "49999999506363398300200000",
                    "locked": "0",
                    "code_hash": "11111111111111111111111111111111",
                    "storage_usage": 182
                }
            }
        }, {
            "Account": {
                "account_id": "contributors.near",
                "account": {
                    "amount": "418000000000000000000000000000000",
                    "locked": "0",
                    "code_hash": "11111111111111111111111111111111",
                    "storage_usage": 182
                }
            }
        }, {
            "Account": {
                "account_id": "erik.near",
                "account": {
                    "amount": "10000000000000000000000000",
                    "locked": "0",
                    "code_hash": "11111111111111111111111111111111",
                    "storage_usage": 182
                }
            }
        }, {
            "Account": {
                "account_id": "foundation.near",
                "account": {
                    "amount": "581779979999999955363487500000000",
                    "locked": "0",
                    "code_hash": "11111111111111111111111111111111",
                    "storage_usage": 182
                }
            }
        }, {
            "Account": {
                "account_id": "illia.near",
                "account": {
                    "amount": "9909124991408763970627200000",
                    "locked": "0",
                    "code_hash": "11111111111111111111111111111111",
                    "storage_usage": 321
                }
            }
        }, {
            "Account": {
                "account_id": "kendall.near",
                "account": {
                    "amount": "49998999710140992484400000",
                    "locked": "0",
                    "code_hash": "11111111111111111111111111111111",
                    "storage_usage": 462
                }
            }
        }, {
            "Account": {
                "account_id": "ledger.vlad.near",
                "account": {
                    "amount": "999999957937258742200000",
                    "locked": "0",
                    "code_hash": "11111111111111111111111111111111",
                    "storage_usage": 327
                }
            }
        }, {
            "Account": {
                "account_id": "mike.near",
                "account": {
                    "amount": "30999999915088987500000000",
                    "locked": "0",
                    "code_hash": "11111111111111111111111111111111",
                    "storage_usage": 182
                }
            }
        }, {
            "Account": {
                "account_id": "mikemikemikemikemikemikemikemike",
                "account": {
                    "amount": "19000000000000000000000000",
                    "locked": "0",
                    "code_hash": "11111111111111111111111111111111",
                    "storage_usage": 182
                }
            }
        }, {
            "Account": {
                "account_id": "near",
                "account": {
                    "amount": "8700003991476791004803600000",
                    "locked": "0",
                    "code_hash": "23tqXYRdbJVuvpLB14Pe9Su9bQBwfn3njKN6EBbKTQwh",
                    "storage_usage": 197868
                }
            }
        }, {
            "Account": {
                "account_id": "nfvalidator1.near",
                "account": {
                    "amount": "0",
                    "locked": "50000000000000000000000000000",
                    "code_hash": "11111111111111111111111111111111",
                    "storage_usage": 182
                }
            }
        }, {
            "Account": {
                "account_id": "nfvalidator2.near",
                "account": {
                    "amount": "0",
                    "locked": "50000000000000000000000000000",
                    "code_hash": "11111111111111111111111111111111",
                    "storage_usage": 182
                }
            }
        }, {
            "Account": {
                "account_id": "nfvalidator3.near",
                "account": {
                    "amount": "0",
                    "locked": "50000000000000000000000000000",
                    "code_hash": "11111111111111111111111111111111",
                    "storage_usage": 182
                }
            }
        }, {
            "Account": {
                "account_id": "nfvalidator4.near",
                "account": {
                    "amount": "0",
                    "locked": "50000000000000000000000000000",
                    "code_hash": "11111111111111111111111111111111",
                    "storage_usage": 182
                }
            }
        }, {
            "Account": {
                "account_id": "patrick.near",
                "account": {
                    "amount": "9998999875468925000000000",
                    "locked": "0",
                    "code_hash": "11111111111111111111111111111111",
                    "storage_usage": 263
                }
            }
        }, {
            "Account": {
                "account_id": "peter.near",
                "account": {
                    "amount": "1000874999955363487500000000",
                    "locked": "0",
                    "code_hash": "11111111111111111111111111111111",
                    "storage_usage": 182
                }
            }
        }, {
            "Account": {
                "account_id": "pulse.near",
                "account": {
                    "amount": "48001118054588063403800000",
                    "locked": "0",
                    "code_hash": "2pMwiHggCBQAv3eFEPtJozDpbHpD8KkL3o3qRv6qs6DT",
                    "storage_usage": 26061
                }
            }
        }, {
            "Account": {
                "account_id": "registrar",
                "account": {
                    "amount": "10000000000000000000000000",
                    "locked": "0",
                    "code_hash": "11111111111111111111111111111111",
                    "storage_usage": 182
                }
            }
        }, {
            "Account": {
                "account_id": "treasury.near",
                "account": {
                    "amount": "10000000000000000000000000",
                    "locked": "0",
                    "code_hash": "11111111111111111111111111111111",
                    "storage_usage": 182
                }
            }
        }, {
            "Account": {
                "account_id": "vlad.near",
                "account": {
                    "amount": "8998999831159137500000000",
                    "locked": "0",
                    "code_hash": "11111111111111111111111111111111",
                    "storage_usage": 346
                }
            }
        }, {
            "Account": {
                "account_id": "wallet.pulse.near",
                "account": {
                    "amount": "999899913398562500000000",
                    "locked": "0",
                    "code_hash": "11111111111111111111111111111111",
                    "storage_usage": 264
                }
            }
        }, {
            "Account": {
                "account_id": "yifang.near",
                "account": {
                    "amount": "50000000000000000000000000",
                    "locked": "0",
                    "code_hash": "11111111111111111111111111111111",
                    "storage_usage": 182
                }
            }
        }, {
            "Contract": {
                "account_id":
                    "near",
                "code":
                    "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"
            }
        }, {
            "Contract": {
                "account_id":
                    "pulse.near",
                "code":
                    "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"
            }
        }, {
            "AccessKey": {
                "account_id":
                    "01.near",
                "public_key":
                    "ed25519:6GxYiNnRLoKkjGeKA68hrfyrJC9tYSamGND5d23aXqRx",
                "access_key": {
                    "nonce": 0,
                    "permission": "FullAccess"
                }
            }
        }, {
            "AccessKey": {
                "account_id":
                    "01.near",
                "public_key":
                    "ed25519:E837NUYQLFgP9cLQou3nBSYzqFFhGffhYQLVzbwL5jtY",
                "access_key": {
                    "nonce": 1,
                    "permission": "FullAccess"
                }
            }
        }, {
            "AccessKey": {
                "account_id":
                    "alex.near",
                "public_key":
                    "ed25519:8fohZQ3DwXgVUXKJSoU9vi6iPyXKUKKff1T7sw4xj4wW",
                "access_key": {
                    "nonce": 0,
                    "permission": "FullAccess"
                }
            }
        }, {
            "AccessKey": {
                "account_id":
                    "bo.near",
                "public_key":
                    "ed25519:C5kXZP86M3DoWjPUwYr2QXkP7RoLj1hcF3kPFyoYcC4h",
                "access_key": {
                    "nonce": 0,
                    "permission": "FullAccess"
                }
            }
        }, {
            "AccessKey": {
                "account_id":
                    "bot.pulse.near",
                "public_key":
                    "ed25519:9x5kkFynLRojfwoVGbuZPSoRHEP5urze5xAbkybXHFBS",
                "access_key": {
                    "nonce": 422638,
                    "permission": "FullAccess"
                }
            }
        }, {
            "AccessKey": {
                "account_id":
                    "bowen.near",
                "public_key":
                    "ed25519:5LaQTGEqGZMrSQuypgR8zS3fQJRhVLgMtjFw7qBmWb8X",
                "access_key": {
                    "nonce": 1,
                    "permission": "FullAccess"
                }
            }
        }, {
            "AccessKey": {
                "account_id":
                    "contributors.near",
                "public_key":
                    "ed25519:BCCMGbV9FzTMTcwS67QCW1TrTmjuwFR1SrFPiG744kio",
                "access_key": {
                    "nonce": 0,
                    "permission": "FullAccess"
                }
            }
        }, {
            "AccessKey": {
                "account_id":
                    "erik.near",
                "public_key":
                    "ed25519:8fohZQ3DwXgVUXKJSoU9vi6iPyXKUKKff1T7sw4xj4wW",
                "access_key": {
                    "nonce": 0,
                    "permission": "FullAccess"
                }
            }
        }, {
            "AccessKey": {
                "account_id":
                    "foundation.near",
                "public_key":
                    "ed25519:GmtTh6yhWz6BmkA9AfnoQESKanDbBJDGfWVpW5wq9Uz",
                "access_key": {
                    "nonce": 1,
                    "permission": "FullAccess"
                }
            }
        }, {
            "AccessKey": {
                "account_id":
                    "illia.near",
                "public_key":
                    "ed25519:dQMV9776YrjHYLysrpQ7abkUmqiA5XLupvveVofYnvy",
                "access_key": {
                    "nonce": 0,
                    "permission": {
                        "FunctionCall": {
                            "allowance": "0",
                            "receiver_id": "illia.near",
                            "method_names": ["__wallet__metadata"]
                        }
                    }
                }
            }
        }, {
            "AccessKey": {
                "account_id":
                    "illia.near",
                "public_key":
                    "ed25519:8fohZQ3DwXgVUXKJSoU9vi6iPyXKUKKff1T7sw4xj4wW",
                "access_key": {
                    "nonce": 11,
                    "permission": "FullAccess"
                }
            }
        }, {
            "AccessKey": {
                "account_id":
                    "kendall.near",
                "public_key":
                    "ed25519:3Puiccgti9iExBUucUxEdbVgeecRibK5FgENQVLHTg5t",
                "access_key": {
                    "nonce": 0,
                    "permission": {
                        "FunctionCall": {
                            "allowance": "0",
                            "receiver_id": "kendall.near",
                            "method_names": ["__wallet__metadata"]
                        }
                    }
                }
            }
        }, {
            "AccessKey": {
                "account_id":
                    "kendall.near",
                "public_key":
                    "ed25519:DvabrRhC1TKXG8hWTGG2U3Ra5E4YXAF1azHdwSc61fs9",
                "access_key": {
                    "nonce": 5,
                    "permission": "FullAccess"
                }
            }
        }, {
            "AccessKey": {
                "account_id":
                    "kendall.near",
                "public_key":
                    "ed25519:J7PuMuFm34c19f324gFSQwMkaBG1DmwPaSEVZEbZw1nX",
                "access_key": {
                    "nonce": 0,
                    "permission": {
                        "FunctionCall": {
                            "allowance": "0",
                            "receiver_id": "kendall.near",
                            "method_names": ["__wallet__metadata"]
                        }
                    }
                }
            }
        }, {
            "AccessKey": {
                "account_id":
                    "ledger.vlad.near",
                "public_key":
                    "ed25519:8g7GvgccAaub68HeSrmp6Aw2vYAvRYbLQZdEa6hZiG9X",
                "access_key": {
                    "nonce": 1,
                    "permission": "FullAccess"
                }
            }
        }, {
            "AccessKey": {
                "account_id":
                    "ledger.vlad.near",
                "public_key":
                    "ed25519:GgK5WqBhrhdwYDUgqsjKwDpnFWad4BgpJSNfH2VPs94v",
                "access_key": {
                    "nonce": 0,
                    "permission": {
                        "FunctionCall": {
                            "allowance": "0",
                            "receiver_id": "ledger.vlad.near",
                            "method_names": ["__wallet__metadata"]
                        }
                    }
                }
            }
        }, {
            "AccessKey": {
                "account_id":
                    "mike.near",
                "public_key":
                    "ed25519:AhiKooGnQsw8S8WZ2V2xRGvpbZDY3yHFcTp4iCHYP8jo",
                "access_key": {
                    "nonce": 1,
                    "permission": "FullAccess"
                }
            }
        }, {
            "AccessKey": {
                "account_id":
                    "mikemikemikemikemikemikemikemike",
                "public_key":
                    "ed25519:AhiKooGnQsw8S8WZ2V2xRGvpbZDY3yHFcTp4iCHYP8jo",
                "access_key": {
                    "nonce": 0,
                    "permission": "FullAccess"
                }
            }
        }, {
            "AccessKey": {
                "account_id":
                    "near",
                "public_key":
                    "ed25519:5zset1JX4qp4PcR3N9KDSY6ATdgkrbBW5wFBGWC4ZjnU",
                "access_key": {
                    "nonce": 8,
                    "permission": "FullAccess"
                }
            }
        }, {
            "AccessKey": {
                "account_id":
                    "nfvalidator1.near",
                "public_key":
                    "ed25519:Fd2TW6TtTDL5hiY58pbTVYfTBSNyWLgHGxiD9mcHgQ92",
                "access_key": {
                    "nonce": 0,
                    "permission": "FullAccess"
                }
            }
        }, {
            "AccessKey": {
                "account_id":
                    "nfvalidator2.near",
                "public_key":
                    "ed25519:4rg9rmbxuSM7bX8z8989LTmBiM6JNnE4w9LZ8KkuCcfq",
                "access_key": {
                    "nonce": 0,
                    "permission": "FullAccess"
                }
            }
        }, {
            "AccessKey": {
                "account_id":
                    "nfvalidator3.near",
                "public_key":
                    "ed25519:EVyX7KE6e2KD3CzpoN1kvzJATsS5KxkjbMCCYHbM3vRr",
                "access_key": {
                    "nonce": 0,
                    "permission": "FullAccess"
                }
            }
        }, {
            "AccessKey": {
                "account_id":
                    "nfvalidator4.near",
                "public_key":
                    "ed25519:CrLQzMvfSDWnTYzfbEzcJ3hdetnpYdsQnvbhuzwHBtAG",
                "access_key": {
                    "nonce": 0,
                    "permission": "FullAccess"
                }
            }
        }, {
            "AccessKey": {
                "account_id":
                    "patrick.near",
                "public_key":
                    "ed25519:8MPLjkG12V5AQfCogZhjrWe5k6PoRzNtLUb2eD1r7fyU",
                "access_key": {
                    "nonce": 3,
                    "permission": "FullAccess"
                }
            }
        }, {
            "AccessKey": {
                "account_id":
                    "patrick.near",
                "public_key":
                    "ed25519:BHTmjrvg2UWxBjzSwDyhkc2FYJseSduWVe7YXBS2Rms1",
                "access_key": {
                    "nonce": 0,
                    "permission": "FullAccess"
                }
            }
        }, {
            "AccessKey": {
                "account_id":
                    "peter.near",
                "public_key":
                    "ed25519:HDybq3JWgmbaiCKtE27T75iYVkEoA8cH6rfnut77ZVY1",
                "access_key": {
                    "nonce": 1,
                    "permission": "FullAccess"
                }
            }
        }, {
            "AccessKey": {
                "account_id":
                    "pulse.near",
                "public_key":
                    "ed25519:3BWDipnJmNfWT7YSBGZu63dkfMBoZDUqWJsctNGBDinE",
                "access_key": {
                    "nonce": 3,
                    "permission": "FullAccess"
                }
            }
        }, {
            "AccessKey": {
                "account_id":
                    "registrar",
                "public_key":
                    "ed25519:Fm9g4GQeQrnwknCVexuPvn3owgrYvMbZhPRoXKpj2wX6",
                "access_key": {
                    "nonce": 0,
                    "permission": "FullAccess"
                }
            }
        }, {
            "AccessKey": {
                "account_id":
                    "treasury.near",
                "public_key":
                    "ed25519:CzAXM8NcumuHPYJYnjq5tUX5v92GHdbYZfmfKFwDNzBZ",
                "access_key": {
                    "nonce": 0,
                    "permission": "FullAccess"
                }
            }
        }, {
            "AccessKey": {
                "account_id":
                    "vlad.near",
                "public_key":
                    "ed25519:2nE29FtYYZrT2owygL3FN9CLVBs9wdUy1r6pdpuScazs",
                "access_key": {
                    "nonce": 2,
                    "permission": "FullAccess"
                }
            }
        }, {
            "AccessKey": {
                "account_id":
                    "vlad.near",
                "public_key":
                    "ed25519:4GnS8L8hnCNWh4saWPPAVxto1VFtVdmY27mkrXLeSxgp",
                "access_key": {
                    "nonce": 1,
                    "permission": "FullAccess"
                }
            }
        }, {
            "AccessKey": {
                "account_id":
                    "vlad.near",
                "public_key":
                    "ed25519:9xLURZGus8bU4Qnf9AC3jmJhHNBo7Ydh17w7nJAY2L78",
                "access_key": {
                    "nonce": 0,
                    "permission": "FullAccess"
                }
            }
        }, {
            "AccessKey": {
                "account_id":
                    "wallet.pulse.near",
                "public_key":
                    "ed25519:9783PHB4mZXYFopqXcypm4TCv2LoAbAdmj24AA9YJ2C6",
                "access_key": {
                    "nonce": 2,
                    "permission": "FullAccess"
                }
            }
        }, {
            "AccessKey": {
                "account_id":
                    "wallet.pulse.near",
                "public_key":
                    "ed25519:BJ3wDgNtiMa22d8iCKmzbGA7YTiSWv9J33NTftekUcoZ",
                "access_key": {
                    "nonce": 0,
                    "permission": "FullAccess"
                }
            }
        }, {
            "AccessKey": {
                "account_id":
                    "yifang.near",
                "public_key":
                    "ed25519:CKUb9VneyN1XFMXcvEc55aKiDpirdDim8Dd4cAyzefF1",
                "access_key": {
                    "nonce": 0,
                    "permission": "FullAccess"
                }
            }
        }, {
            "Data": {
                "account_id":
                    "near",
                "data_key":
                    "U1RBVEU=",
                "value":
                    "CQAAAAAAAAAAAAAAaQAAAAAAAAAACQAAAAAAAAAAAAAAawAAAAAAAAAACQAAAAAAAAAAAAAAdg=="
            }
        }]
    })

# patch should succeed
res = nodes[0].call_function("test0", "read_value",
                             base64.b64encode(k).decode('ascii'))
assert (res['result']['result'] == list(new_v))

'''
'''--- pytest/tests/sanity/__init__.py ---

'''
'''--- pytest/tests/sanity/backward_compatible.py ---
#!/usr/bin/env python3
"""
This script runs node from stable branch and from current branch and makes
sure they are backward compatible.
"""

import sys
import os
import subprocess
import time
import base58
import json
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

import branches
import cluster
from transaction import sign_deploy_contract_tx, sign_function_call_tx, sign_payment_tx, sign_create_account_with_full_access_key_and_balance_tx
import utils

def main():
    node_root = utils.get_near_tempdir('backward', clean=True)
    executables = branches.prepare_ab_test()

    # Setup local network.
    subprocess.check_call((executables.stable.neard, f'--home={node_root}',
                           'localnet', '-v', '2', '--prefix', 'test'))

    # Run both binaries at the same time.
    config = executables.stable.node_config()
    stable_node = cluster.spin_up_node(config, executables.stable.root,
                                       str(node_root / 'test0'), 0)
    config = executables.current.node_config()
    current_node = cluster.spin_up_node(config,
                                        executables.current.root,
                                        str(node_root / 'test1'),
                                        1,
                                        boot_node=stable_node)

    # Check it all works.
    BLOCKS = 100
    max_height = -1
    started = time.time()

    # Create account, transfer tokens, deploy contract, invoke function call
    block_hash = stable_node.get_latest_block().hash_bytes

    new_account_id = 'test_account.test0'
    new_signer_key = cluster.Key(new_account_id, stable_node.signer_key.pk,
                                 stable_node.signer_key.sk)
    create_account_tx = sign_create_account_with_full_access_key_and_balance_tx(
        stable_node.signer_key, new_account_id, new_signer_key, 10**24, 1,
        block_hash)
    res = stable_node.send_tx_and_wait(create_account_tx, timeout=20)
    assert 'error' not in res, res
    assert 'Failure' not in res['result']['status'], res

    transfer_tx = sign_payment_tx(stable_node.signer_key, new_account_id,
                                  10**25, 2, block_hash)
    res = stable_node.send_tx_and_wait(transfer_tx, timeout=20)
    assert 'error' not in res, res

    block_height = stable_node.get_latest_block().height
    nonce = block_height * 1_000_000 - 1

    tx = sign_deploy_contract_tx(new_signer_key, utils.load_test_contract(),
                                 nonce, block_hash)
    res = stable_node.send_tx_and_wait(tx, timeout=20)
    assert 'error' not in res, res

    tx = sign_deploy_contract_tx(stable_node.signer_key,
                                 utils.load_test_contract(), 3, block_hash)
    res = stable_node.send_tx_and_wait(tx, timeout=20)
    assert 'error' not in res, res

    tx = sign_function_call_tx(new_signer_key, new_account_id,
                               'write_random_value', [], 10**13, 0, nonce + 1,
                               block_hash)
    res = stable_node.send_tx_and_wait(tx, timeout=20)
    assert 'error' not in res, res
    assert 'Failure' not in res['result']['status'], res

    data = json.dumps([{
        "create": {
            "account_id": "test_account.test0",
            "method_name": "call_promise",
            "arguments": [],
            "amount": "0",
            "gas": 30000000000000,
        },
        "id": 0
    }, {
        "then": {
            "promise_index": 0,
            "account_id": "test0",
            "method_name": "call_promise",
            "arguments": [],
            "amount": "0",
            "gas": 30000000000000,
        },
        "id": 1
    }])

    tx = sign_function_call_tx(stable_node.signer_key,
                               new_account_id, 'call_promise',
                               bytes(data, 'utf-8'), 90000000000000, 0,
                               nonce + 2, block_hash)
    res = stable_node.send_tx_and_wait(tx, timeout=20)

    assert 'error' not in res, res
    assert 'Failure' not in res['result']['status'], res

    utils.wait_for_blocks(current_node, target=BLOCKS)

if __name__ == "__main__":
    main()

'''
'''--- pytest/tests/sanity/block_chunk_signature.py ---
#!/usr/bin/env python3
# Test for #3368
#
# Create a proxy that nullifies chunk signatures in blocks, test that blocks get rejected.
import sys, time, asyncio
import pathlib
import logging

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import start_cluster
from configured_logger import logger
from peer import *
from proxy import ProxyHandler

class Handler(ProxyHandler):

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.blocks = 0

    async def handle(self, msg, fr, to):
        if msg.enum == 'Block' and msg.Block.chunks(
        )[0].signature.data != bytes(64):
            msg.Block.chunks()[0].signature.data = bytes(64)
            self.blocks += 1
            # Node gets banned by the peer after sending one block
            assert self.blocks <= 2
            return msg
        return True

if __name__ == '__main__':
    nodes = start_cluster(2, 0, 1, None, [], {}, Handler)

    time.sleep(5)
    h0 = nodes[0].get_latest_block(verbose=True).height
    h1 = nodes[1].get_latest_block(verbose=True).height
    assert h0 <= 3 and h1 <= 3

'''
'''--- pytest/tests/sanity/block_production.py ---
#!/usr/bin/env python3
# Spins up four nodes, and waits until they produce 50 blocks.
# Ensures that the nodes remained in sync throughout the process
# Sets epoch length to 20
# The nodes track all shards.

# Local:
# python tests/sanity/block_production.py
# Remote:
# NEAR_PYTEST_CONFIG=remote.json python tests/sanity/block_production.py

# Same for all tests that call start_cluster with a None config

import sys, time
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import start_cluster
from configured_logger import logger

TIMEOUT = 150
EPOCH_LENGTH = 20
BLOCKS = EPOCH_LENGTH * 5

node_config = {
    "tracked_shards": [0],  # Track all shards.
}

nodes = start_cluster(
    4, 0, 4, None,
    [["epoch_length", EPOCH_LENGTH], ["block_producer_kickout_threshold", 60],
     ["chunk_producer_kickout_threshold", 60]], {
         0: node_config,
         1: node_config,
         2: node_config,
         3: node_config,
     })

started = time.time()

max_height = 0
last_heights = [0 for _ in nodes]
seen_heights = [set() for _ in nodes]
last_common = [[0 for _ in nodes] for _ in nodes]

height_to_hash = {}

# the test relies on us being able to query heights faster
# than the blocks are produced. Validating store takes up
# to 350ms on slower hardware for this test, multiplied
# by four nodes querying heights every 1 second becomes
# unfeasible
for node in nodes:
    node.stop_checking_store()

def min_common():
    return min([min(x) for x in last_common])

def heights_report():
    for i, sh in enumerate(seen_heights):
        logger.info("Node %s: %s" % (i, sorted(list(sh))))

while max_height < BLOCKS:
    assert time.time() - started < TIMEOUT
    for i, node in enumerate(nodes):
        block = node.get_latest_block()
        height = block.height
        hash_ = block.hash

        if height > max_height:
            max_height = height
            if height % 10 == 0:
                logger.info("Reached height %s, min common: %s" %
                            (height, min_common()))

        if height not in height_to_hash:
            height_to_hash[height] = hash_
        else:
            assert height_to_hash[
                height] == hash_, "height: %s, h1: %s, h2: %s" % (
                    height, hash_, height_to_hash[height])

        last_heights[i] = height
        seen_heights[i].add(height)
        for j, _ in enumerate(nodes):
            if height in seen_heights[j]:
                last_common[i][j] = height
                last_common[j][i] = height

        # during the time it took to start the test some blocks could have been produced, so the first observed height
        # could be higher than 2, at which point for the nodes for which we haven't queried the height yet the
        # `min_common` is zero. Once we queried each node at least once, we expect the difference between the last
        # queried heights to never differ by more than two.
        if min_common() > 0:
            assert min_common() + 2 >= height, heights_report()

assert min_common() + 2 >= BLOCKS, heights_report()

doomslug_final_block = nodes[0].json_rpc('block', {'finality': 'near-final'})
assert (doomslug_final_block['result']['header']['height'] >= BLOCKS - 10)

nfg_final_block = nodes[0].json_rpc('block', {'finality': 'final'})
assert (nfg_final_block['result']['header']['height'] >= BLOCKS - 10)

'''
'''--- pytest/tests/sanity/block_sync.py ---
#!/usr/bin/env python3
# Spins up two validating nodes. Make one validator produce block every 100 seconds.
# Let the validators produce blocks for a while and then shut one of them down, remove data and restart.
# Check that it can sync to the validator through block sync.

import sys, time
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import start_cluster
from configured_logger import logger
import utils

BLOCKS = 10

consensus_config0 = {
    "consensus": {
        "block_fetch_horizon": 30,
        "block_header_fetch_horizon": 30
    }
}
consensus_config1 = {
    "consensus": {
        "min_block_production_delay": {
            "secs": 100,
            "nanos": 0
        },
        "max_block_production_delay": {
            "secs": 200,
            "nanos": 0
        },
        "max_block_wait_delay": {
            "secs": 1000,
            "nanos": 0
        }
    }
}
# give more stake to the bootnode so that it can produce the blocks alone
nodes = start_cluster(
    2, 0, 4, None,
    [["epoch_length", 100], ["num_block_producer_seats", 100],
     ["num_block_producer_seats_per_shard", [25, 25, 25, 25]],
     ["validators", 0, "amount", "110000000000000000000000000000000"],
     [
         "records", 0, "Account", "account", "locked",
         "110000000000000000000000000000000"
     ], ["total_supply", "3060000000000000000000000000000000"]], {
         0: consensus_config0,
         1: consensus_config1
     })
time.sleep(3)

utils.wait_for_blocks(nodes[0], target=BLOCKS)

logger.info("kill node 0")
nodes[0].kill()
nodes[0].reset_data()

logger.info("restart node 0")
nodes[0].start(boot_node=nodes[0])
time.sleep(3)

node1_height = nodes[1].get_latest_block().height
utils.wait_for_blocks(nodes[0], target=node1_height)

'''
'''--- pytest/tests/sanity/block_sync_archival.py ---
#!/usr/bin/env python3
"""Tests that archival node can sync up history from another archival node.

The overview of this test is that it starts archival nodes which need to sync
their state from already running archival nodes.  The test can be divided into
two stages:

1. The test first starts a validator and an observer node (let’s call it Fred).
   Both configured as archival nodes.  It then waits for several epochs worth of
   blocks to be generated and received by the observer node.  Once that happens,
   the test kills the validator node so that no new blocks are generated.

   At this stage, the test verifies that Fred can sync correctly and that the
   boot node serves all partial chunks requests from its in-memory cache (which
   is determined by looking at Prometheus metrics).

2. The test then restarts Fred so that its in-memory cache is cleared.  It
   finally starts a new observer (let’s call it Barney) and points it at Fred as
   a boot node.  The test waits for Barney to synchronise with Fred and then
   verifies that all the blocks have been correctly fetched.

   At this stage, the test verifies that Barney synchronises correctly and that
   Fred serves all requests from storage (since it's in-memory cache has been
   cleared).  This is again done through Prometheus metrics and in addition the
   test verifies that data from DBCol::Chunks and DBCol::PartialChunks was used.  This
   also implies that Fred correctly performed DBCol::PartialChunks garbage
   collection.
"""

import argparse
import datetime
import pathlib
import sys
import typing

import prometheus_client.parser
import requests

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

import cluster
from configured_logger import logger
import utils

EPOCH_LENGTH = 20
TARGET_HEIGHT = 20 * EPOCH_LENGTH

_DurationMaybe = typing.Optional[datetime.timedelta]

class Cluster:

    def __init__(self):
        node_config = {
            'archive': True,
            'tracked_shards': [0],
        }

        self._config = cluster.load_config()
        self._near_root, self._node_dirs = cluster.init_cluster(
            num_nodes=1,
            num_observers=2,
            num_shards=1,
            config=self._config,
            genesis_config_changes=[['epoch_length', EPOCH_LENGTH],
                                    ['block_producer_kickout_threshold', 80]],
            client_config_changes={
                0: node_config,
                1: node_config,
                2: node_config,
                3: node_config
            })
        self._nodes = [None] * len(self._node_dirs)

    def start_node(
            self, ordinal: int, *,
            boot_node: typing.Optional[cluster.BaseNode]) -> cluster.BaseNode:
        assert self._nodes[ordinal] is None
        self._nodes[ordinal] = node = cluster.spin_up_node(
            self._config,
            self._near_root,
            self._node_dirs[ordinal],
            ordinal,
            boot_node=boot_node,
            single_node=not ordinal)
        return node

    def __enter__(self):
        return self

    def __exit__(self, *_):
        for node in self._nodes:
            if node:
                node.cleanup()

# TODO(#6458): Move this to separate file and merge with metrics module.
def get_metrics(node_name: str,
                node: cluster.BootNode) -> typing.Dict[str, int]:
    """Fetches partial encoded chunk request count metrics from node.

    Args:
        node_name: Node’s name used when logging the counters.  This is purely
            for debugging.
        node: Node to fetch metrics from.

    Returns:
        A `{key: count}` dictionary where key is in ‘method/success’ format.
        The values correspond to the
        near_partial_encoded_chunk_request_processing_time_count Prometheus
        metric.
    """
    url = 'http://{}:{}/metrics'.format(*node.rpc_addr())
    response = requests.get(url)
    response.raise_for_status()

    metric_name = 'near_partial_encoded_chunk_request_processing_time'
    histogram = next(
        (metric
         for metric in prometheus_client.parser.text_string_to_metric_families(
             response.content.decode('utf8'))
         if metric.name == metric_name), None)
    if not histogram:
        return {}

    counts = dict((sample.labels['method'] + '/' + sample.labels['success'],
                   int(sample.value))
                  for sample in histogram.samples
                  if sample.name.endswith('_count'))
    logger.info(f'{node_name} counters: ' + '; '.join(
        f'{key}: {count}' for key, count in sorted(counts.items())))
    return counts

def assert_metrics(metrics: typing.Dict[str, int],
                   allowed_non_zero: typing.Sequence[str]) -> None:
    """Asserts that only given keys are non-zero.

    Args:
        metrics: Metrics as returned by get_metrics() function.
        allowed_non_zero: Keys that are expected to be non-zero in the metrics.
    """
    for key in allowed_non_zero:
        assert metrics.get(key), f'Expected {key} to be non-zero'
    for key, count in metrics.items():
        ok = key in allowed_non_zero or not count
        assert ok, f'Expected {key} to be zero but got {count}'

def get_all_blocks(node: cluster.BaseNode) -> typing.Sequence[cluster.BlockId]:
    """Returns all blocks from given head down to genesis block."""
    ids = []
    block_hash = node.get_latest_block().hash
    while block_hash != '11111111111111111111111111111111':
        block = node.get_block(block_hash)
        assert 'result' in block, block
        header = block['result']['header']
        ids.append(cluster.BlockId.from_header(header))
        block_hash = header.get('prev_hash')
    return list(reversed(ids))

def run_test(cluster: Cluster) -> None:
    # Start the validator and the first observer.  Wait until the observer
    # synchronises a few epoch’s worth of blocks to be generated and then kill
    # validator so no more blocks are generated.
    boot = cluster.start_node(0, boot_node=None)
    fred = cluster.start_node(1, boot_node=boot)
    utils.wait_for_blocks(fred, target=TARGET_HEIGHT, poll_interval=1)
    metrics = get_metrics('boot', boot)
    boot.kill()

    # We didn’t generate enough blocks to fill boot’s in-memory cache which
    # means all Fred’s requests should be served from it.
    assert_metrics(metrics, ('cache/ok',))

    # Restart Fred so that its cache is cleared.  Then start the second
    # observer, Barney, and wait for it to sync up.
    fred_blocks = get_all_blocks(fred)
    fred.kill(gentle=True)
    fred.start()

    barney = cluster.start_node(2, boot_node=fred)
    utils.wait_for_blocks(barney,
                          target=fred_blocks[-1].height,
                          poll_interval=1)
    barney_blocks = get_all_blocks(barney)
    if fred_blocks != barney_blocks:
        for f, b in zip(fred_blocks, barney_blocks):
            if f != b:
                logger.error(f'{f} != {b}')
        assert False

    # Since Fred’s in-memory cache is clear, all Barney’s requests are served
    # from storage.  Since DBCol::PartialChunks is garbage collected, some of the
    # requests are served from DBCol::Chunks.
    assert_metrics(get_metrics('fred', fred), (
        'chunk/ok',
        'partial/ok',
    ))

if __name__ == '__main__':
    with Cluster() as cl:
        run_test(cl)

'''
'''--- pytest/tests/sanity/block_sync_flat_storage.py ---
#!/usr/bin/env python3
# Spins up one validating node.
# Spins a non-validating node that tracks all shards.
# In the middle of an epoch, the node gets stopped, and the set of tracked shards gets reduced.
# Test that the node correctly handles chunks for the shards that it will care about in the next epoch.
# Spam transactions that require the node to use flat storage to process them correctly.

import pathlib
import random
import sys

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import init_cluster, spin_up_node, load_config, apply_config_changes
import account
import transaction
import utils

EPOCH_LENGTH = 30

config0 = {
    'tracked_shards': [0],
}
config1 = {
    'tracked_shards': [0],
}

config = load_config()
near_root, node_dirs = init_cluster(1, 1, 4, config,
                                    [["epoch_length", EPOCH_LENGTH]], {
                                        0: config0,
                                        1: config1
                                    })

boot_node = spin_up_node(config, near_root, node_dirs[0], 0)
node1 = spin_up_node(config, near_root, node_dirs[1], 1, boot_node=boot_node)

contract_key = boot_node.signer_key
contract = utils.load_test_contract()
latest_block_hash = boot_node.get_latest_block().hash_bytes
deploy_contract_tx = transaction.sign_deploy_contract_tx(
    contract_key, contract, 10, latest_block_hash)
result = boot_node.send_tx_and_wait(deploy_contract_tx, 10)
assert 'result' in result and 'error' not in result, (
    'Expected "result" and no "error" in response, got: {}'.format(result))

def random_workload_until(target, nonce, keys):
    while True:
        nonce += 1
        height = boot_node.get_latest_block().height
        if height > target:
            break
        if (len(keys) > 100 and random.random() < 0.2) or len(keys) > 1000:
            key = keys[random.randint(0, len(keys) - 1)]
            call_function(boot_node, 'read', key, nonce)
        else:
            key = random_u64()
            keys.append(key)
            call_function(boot_node, 'write', key, nonce)
    return (nonce, keys)

def random_u64():
    return bytes(random.randint(0, 255) for _ in range(8))

def call_function(node, op, key, nonce):
    last_block_hash = node.get_latest_block().hash_bytes
    if op == 'read':
        args = key
        fn = 'read_value'
    else:
        args = key + random_u64()
        fn = 'write_key_value'

    tx = transaction.sign_function_call_tx(node.signer_key,
                                           node.signer_key.account_id, fn, args,
                                           300 * account.TGAS, 0, nonce,
                                           last_block_hash)
    return node.send_tx(tx).get('result')

nonce, keys = random_workload_until(EPOCH_LENGTH + 5, 1, [])

node1.kill()
# Reduce the set of tracked shards and make it variable in time.
# The node is stopped in epoch_height = 1.
# Change the config of tracked shards such that after restart the node cares
# only about shard 0, and in the next epoch it will care about shards [1, 2, 3].
apply_config_changes(node_dirs[1], {
    "tracked_shards": [],
    "tracked_shard_schedule": [[0], [0], [1, 2, 3]]
})

# Run node0 more to trigger block sync in node1.
nonce, keys = random_workload_until(EPOCH_LENGTH * 2 + 1, nonce, keys)

# Node1 is now behind and needs to do header sync and block sync.
node1.start(boot_node=boot_node)
utils.wait_for_blocks(node1, target=EPOCH_LENGTH * 2 + 10)

'''
'''--- pytest/tests/sanity/catchup_flat_storage_deletions.py ---
#!/usr/bin/env python3
# Spins up a validator node tracking all shards and a non-validator node.
# Deletes an account.
# Restart the non-validator node.
# After the non-validator node does catchup, attempt to send a token to the deleted account.
# Observe that both nodes correctly execute the transaction and return an error.

import pathlib
import sys

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import start_cluster, load_config
import state_sync_lib
import transaction
import utils

from configured_logger import logger

EPOCH_LENGTH = 10

def print_balances(nodes, account_ids):
    for node in nodes:
        for account_id in account_ids:
            res = node.json_rpc(
                'query', {
                    'request_type': 'view_account',
                    'account_id': account_id,
                    'finality': 'optimistic'
                })
            logger.info(
                f"Lookup account '{account_id}' in node '{node.signer_key.account_id}': {res['result'] if 'result' in res else res['error']}"
            )

def main():
    node_config_dump, node_config_sync = state_sync_lib.get_state_sync_configs_pair(
    )
    # The schedule means that the node tracks all shards all the time except for epoch heights 2 and 3.
    # Those epochs correspond to block heights [EPOCH_LENGTH * 2 + 1, EPOCH_LENGTH * 4].
    node_config_sync["tracked_shard_schedule"] = [
        [0],  # epoch_height = 0 and 4
        [0],  # epoch_height = 1* and 1 and 5
        [],  # epoch_height = 2
        [],  # epoch_height = 3
    ]
    node_config_sync["tracked_shards"] = []

    config = load_config()
    nodes = start_cluster(1, 1, 1, config, [["epoch_length", EPOCH_LENGTH]], {
        0: node_config_dump,
        1: node_config_sync
    })
    [boot_node, node] = nodes

    logger.info('started the nodes')

    test_account_id = node.signer_key.account_id
    test_account_key = node.signer_key
    account_ids = [boot_node.signer_key.account_id, test_account_id]

    print_balances(nodes, account_ids)

    nonce = 10

    latest_block = utils.wait_for_blocks(boot_node,
                                         target=int(2.5 * EPOCH_LENGTH))
    epoch = state_sync_lib.approximate_epoch_height(latest_block.height,
                                                    EPOCH_LENGTH)
    assert epoch == 2, f"epoch: {epoch}"
    node.kill()
    # Restart the node to make it start without opening any flat storages.
    node.start(boot_node=boot_node)
    logger.info(f'We are in epoch {epoch}, and the node is restarted')

    print_balances(nodes, account_ids)

    # Delete the account.
    latest_block_hash = boot_node.get_latest_block().hash_bytes
    nonce += 1
    tx = transaction.sign_delete_account_tx(test_account_key, test_account_id,
                                            boot_node.signer_key.account_id,
                                            nonce, latest_block_hash)
    result = boot_node.send_tx(tx)
    logger.info(result)
    logger.info(f'Deleted {test_account_id}')

    # Wait until the node tracks the shard and probably does the catchup.
    latest_block = utils.wait_for_blocks(boot_node,
                                         target=int(4.5 * EPOCH_LENGTH))
    epoch = state_sync_lib.approximate_epoch_height(latest_block.height,
                                                    EPOCH_LENGTH)
    assert epoch == 4, f"epoch: {epoch}"
    logger.info(f'We are in epoch {epoch}')

    # Ensure the non-validator node has caught up.
    utils.wait_for_blocks(node, target=int(4.5 * EPOCH_LENGTH))
    logger.info(f'The other node is in sync')

    print_balances(nodes, account_ids)

    # Check that the lookup of a deleted account returns an error. Because it's deleted.
    test_account_balance = node.json_rpc(
        'query', {
            'request_type': 'view_account',
            'account_id': test_account_id,
            'finality': 'optimistic'
        })
    assert 'error' in test_account_balance, test_account_balance

    # Send tokens.
    # The transaction will be accepted and will detect that the receiver account was deleted.
    latest_block_hash = boot_node.get_latest_block().hash_bytes
    nonce += 1
    tx = transaction.sign_payment_tx(boot_node.signer_key, test_account_id, 1,
                                     nonce, latest_block_hash)
    logger.info(
        f'Sending a token from {boot_node.signer_key.account_id} to {test_account_id} now'
    )
    result = boot_node.send_tx_and_wait(tx, 10)
    assert 'result' in result and 'error' not in result, (
        'Expected "result" and no "error" in response, got: {}'.format(result))

    print_balances(nodes, account_ids)

    # Wait a bit more and check that the non-validator node is in sync.
    utils.wait_for_blocks(boot_node, target=int(5.8 * EPOCH_LENGTH))
    boot_node_latest_block_height = boot_node.get_latest_block().height
    node_latest_block_height = node.get_latest_block().height
    logger.info(
        f'The validator node is at block height {boot_node_latest_block_height} in epoch {state_sync_lib.approximate_epoch_height(boot_node_latest_block_height, EPOCH_LENGTH)}'
    )
    logger.info(
        f'The non-validator node is at block height {node_latest_block_height}')

    # Check that the non-validator node is not stuck, and is in-sync.
    assert boot_node_latest_block_height < int(
        0.5 * EPOCH_LENGTH) + node_latest_block_height

if __name__ == "__main__":
    main()

'''
'''--- pytest/tests/sanity/concurrent_function_calls.py ---
#!/usr/bin/env python3
# Spins up four nodes, deploy an smart contract to one node,
# Call a smart contract method in another node

import sys, time
import base58
import base64
import multiprocessing
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))
from cluster import start_cluster
from configured_logger import logger
from transaction import sign_deploy_contract_tx, sign_function_call_tx
from utils import load_test_contract

nodes = start_cluster(
    4, 1, 4, None,
    [["epoch_length", 10], ["block_producer_kickout_threshold", 80]],
    {4: {
        "tracked_shards": [0, 1, 2, 3]
    }})

# Deploy contract
hash_ = nodes[0].get_latest_block().hash_bytes
tx = sign_deploy_contract_tx(nodes[0].signer_key, load_test_contract(), 10,
                             hash_)
nodes[0].send_tx(tx)

time.sleep(3)

# Write 10 values to storage
for i in range(10):
    hash_ = nodes[1].get_latest_block().hash_bytes
    keyvalue = bytearray(16)
    keyvalue[0] = i
    keyvalue[8] = i
    tx2 = sign_function_call_tx(nodes[0].signer_key,
                                nodes[0].signer_key.account_id,
                                'write_key_value', bytes(keyvalue),
                                10000000000000, 100000000000, 20 + i * 10,
                                hash_)
    res = nodes[1].send_tx(tx2)

time.sleep(3)
acc_id = nodes[0].signer_key.account_id

def process():
    for i in range(100):
        key = bytearray(8)
        key[0] = i % 10
        res = nodes[4].call_function(
            acc_id, 'read_value',
            base64.b64encode(bytes(key)).decode("ascii"))
        res = int.from_bytes(res["result"]["result"], byteorder='little')
        assert res == (i % 10)
    logger.info("all done")

ps = [multiprocessing.Process(target=process, args=()) for i in range(6)]
for p in ps:
    p.start()

for p in ps:
    p.join()

'''
'''--- pytest/tests/sanity/db_migration.py ---
#!/usr/bin/python3
"""
Spins up a node with old version and wait until it produces some blocks.
Shutdowns the node and restarts with the same data folder with the new binary.
Makes sure that the node can still produce blocks.
"""

import json
import logging
import os
import sys
import time
import subprocess
import base58
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

import branches
import cluster
from transaction import sign_deploy_contract_tx, sign_function_call_tx
import utils

logging.basicConfig(level=logging.INFO)

def deploy_contract(node):
    hash_ = node.get_latest_block().hash_bytes
    tx = sign_deploy_contract_tx(node.signer_key, utils.load_test_contract(),
                                 10, hash_)
    node.send_tx_and_wait(tx, timeout=15)
    utils.wait_for_blocks(node, count=3)

def send_some_tx(node):
    # Write 10 values to storage
    nonce = node.get_nonce_for_pk(node.signer_key.account_id,
                                  node.signer_key.pk) + 10
    for i in range(10):
        hash_ = node.get_latest_block().hash_bytes
        keyvalue = bytearray(16)
        keyvalue[0] = (nonce // 10) % 256
        keyvalue[8] = (nonce // 10) % 255
        tx2 = sign_function_call_tx(node.signer_key, node.signer_key.account_id,
                                    'write_key_value', bytes(keyvalue),
                                    10000000000000, 100000000000, nonce, hash_)
        nonce += 10
        res = node.send_tx_and_wait(tx2, timeout=15)
        assert 'error' not in res, res
        assert 'Failure' not in res['result']['status'], res
    utils.wait_for_blocks(node, count=3)

def main():
    executables = branches.prepare_ab_test()
    node_root = utils.get_near_tempdir('db_migration', clean=True)

    logging.info(f"The near root is {executables.stable.root}...")
    logging.info(f"The node root is {node_root}...")

    # Init local node
    subprocess.call((
        executables.stable.neard,
        "--home=%s" % node_root,
        "init",
        "--fast",
    ))

    # Adjust changes required since #7486.  This is needed because current
    # stable release populates the deprecated migration configuration options.
    # TODO(mina86): Remove this once we get stable release which doesn’t
    # populate those fields by default.
    config_path = node_root / 'config.json'
    data = json.loads(config_path.read_text(encoding='utf-8'))
    data.pop('db_migration_snapshot_path', None)
    data.pop('use_db_migration_snapshot', None)
    config_path.write_text(json.dumps(data), encoding='utf-8')

    # Run stable node for few blocks.
    logging.info("Starting the stable node...")
    config = executables.stable.node_config()
    node = cluster.spin_up_node(config, executables.stable.root, str(node_root),
                                0)

    logging.info("Running the stable node...")
    utils.wait_for_blocks(node, count=20)
    logging.info("Blocks are being produced, sending some tx...")
    deploy_contract(node)
    send_some_tx(node)

    node.kill()

    logging.info(
        "Stable node has produced blocks... Stopping the stable node... ")

    # Run new node and verify it runs for a few more blocks.
    logging.info("Starting the current node...")
    config = executables.current.node_config()
    node.near_root = executables.current.root
    node.binary_name = executables.current.neard
    node.start(boot_node=node)

    logging.info("Running the current node...")
    utils.wait_for_blocks(node, count=20)
    logging.info("Blocks are being produced, sending some tx...")
    send_some_tx(node)

    logging.info(
        "Currnet node has produced blocks... Stopping the current node... ")

    node.kill()

    logging.info("Restarting the current node...")

    node.start(boot_node=node)
    utils.wait_for_blocks(node, count=20)

if __name__ == "__main__":
    main()

'''
'''--- pytest/tests/sanity/docker.py ---
#!/usr/bin/env python3
"""Verifies that node can be started inside of a Docker image.

The script builds Docker image using 'make docker-nearcore' command and then
starts a cluster with all nodes running inside of containers.  As sanity check
to see if the cluster works correctly, the test waits for a several blocks and
then interrogates each node about block with specified hash expecting all of
them to return the same data.

The purpose of the test is to verify that:
- `make docker-nearcore` builds a working Docker image,
- `docker run ... nearcore` (i.e. the `run_docker.sh` script) works,
- `docker run -eBOOT_NODE=... ... nearcore` (i.e. passing boot nodes via
  BOOT_NODE environment variable) works and
- `docker run ... nearcore sh -c 'neard ...'` works.
"""

import os
import pathlib
import shlex
import subprocess
import sys
import tempfile
import typing
import uuid

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

import cluster
from configured_logger import logger
import utils

BLOCKS = 42
NUM_NODES = 3

_REPO_DIR = pathlib.Path(__file__).resolve().parents[3]
_CD_PRINTED = False
_Command = typing.Sequence[typing.Union[str, pathlib.Path]]

_DOCKER_IMAGE_TAG = 'nearcore-testimage-' + uuid.uuid4().hex

def run(cmd: _Command, *, capture_output: bool = False) -> typing.Optional[str]:
    """Trivial wrapper around subprocess.check_({all,output}.

    Args:
        cmd: Command to execute.
        capture_output: If true, captures standard output of the command and
            returns it.
    Returns:
        Command's stripped standard output if `capture_output` is true, None
        otherwise.
    """
    global _CD_PRINTED
    if not _CD_PRINTED:
        logger.debug(f'+ cd {shlex.quote(str(_REPO_DIR))}')
        _CD_PRINTED = True
    logger.debug('+ ' + ' '.join(shlex.quote(str(arg)) for arg in cmd))
    if capture_output:
        return subprocess.check_output(cmd, cwd=_REPO_DIR,
                                       encoding='utf-8').strip()
    subprocess.check_call(cmd, cwd=_REPO_DIR)
    return None

def docker_run(shell_cmd: typing.Optional[str] = None,
               *,
               detach: bool = False,
               network: bool = False,
               volume: typing.Tuple[pathlib.Path, str],
               env: typing.Dict[str, str] = {}) -> typing.Optional[str]:
    """Runs a `docker run` command.

    Args:
        shell_cmd: Optionally a shell command to execute inside of the
            container.  It's going to be run using `sh -c` and it's caller's
            responsibility to make sure all data inside is properly sanitised.
            If not given, command configured via CMD when building the container
            will be executed as typical for Docker images.
        detach: Whether the `docker run` command should detach or not.  If
            False, standard output and standard error will be passed through to
            test outputs.  If True, the container will be detached and the
            function will return its container id.
        network: Whether to enable network.  If True, the container will be
            configured to use host's network.  This allows processes in
            different containers to connect with each other easily.
        volume: A (path, container_path) tuple denoting that local `path` should
            be mounted under `container_path` inside of the container.
        env: *Additional* environment variables set inside of the container.

    Returns:
        Command's stripped standard output if `detach` is true, None otherwise.
    """
    cmd = ['docker', 'run', '--read-only', f'-v{volume[0]}:{volume[1]}']

    # Either run detached or attach standard output and standard error so they
    # are visible.
    if detach:
        cmd.append('-d')
    else:
        cmd.extend(('-astdout', '-astderr'))

    if network:
        # Don’t create separate network.  This makes it much simpler for nodes
        # inside of the containers to communicate with each other.
        cmd.append('--network=host')
    else:
        # The command does not need networking so disable it.
        cmd.append('--network=none')

    # Use current user to run the code inside the container so that data saved
    # in home will be readable by us outside of the container (otherwise, the
    # command would be run as root and data would be owned by root).
    cmd.extend(('-u', f'{os.getuid()}:{os.getgid()}', '--userns=host'))

    # Set environment variables.
    rust_log = ('actix_web=warn,mio=warn,tokio_util=warn,'
                'actix_server=warn,actix_http=warn,' +
                os.environ.get('RUST_LOG', 'debug'))
    cmd.extend(('-eRUST_BACKTRACE=1', f'-eRUST_LOG={rust_log}'))
    for key, value in env.items():
        cmd.append((f'-e{key}={value}'))

    # Specify the image to run.
    cmd.append(_DOCKER_IMAGE_TAG)

    # And finally, specify the command.
    if shell_cmd:
        cmd.extend(('sh', '-c', shell_cmd))

    return run(cmd, capture_output=detach)

class DockerNode(cluster.LocalNode):
    """A node run inside of a Docker container."""

    def __init__(self, ordinal: int, node_dir: pathlib.Path) -> None:
        super().__init__(port=24567 + 10 + ordinal,
                         rpc_port=3030 + 10 + ordinal,
                         near_root='',
                         node_dir=str(node_dir),
                         blacklist=[])
        self._container_id = None

    def start(self, *, boot_node: cluster.BootNode = None) -> None:
        """Starts a node inside of a Docker container.

        Args:
            boot_node: Optional boot node to pass to the node.
        """
        assert self._container_id is None
        assert not self.cleaned

        env = {}
        if boot_node:
            env['BOOT_NODES'] = cluster.make_boot_nodes_arg(boot_node)[1]

        cid = docker_run(detach=True,
                         network=True,
                         volume=(self.node_dir, '/srv/near'),
                         env=env)
        self._container_id = cid
        logger.info(f'Node started in Docker container {cid}')

    def kill(self):
        cid = self._container_id
        if cid:
            self._container_id = None
            logger.info(f'Stopping container {cid}')
            run(('docker', 'stop', cid))

    __WARN_LOGS = True

    def output_logs(self):
        # Unfortunately because we’re running the containers as detached
        # anything neard writes to stdout and stderr is lost.  We could start
        # nodes using `docker run -d ... sh -c 'neard ... >stdout 2>stderr'` but
        # that would mean that we’re not testing the `run_docker.sh` script
        # which we do want to read.
        if self.__WARN_LOGS:
            logger.info(
                'Due to technical limitations logs from node is not available')
            type(self).__WARN_LOGS = False
        pass

def main():
    nodes = []

    logger.info("Build the container")
    run(('make', 'DOCKER_TAG=' + _DOCKER_IMAGE_TAG, 'docker-nearcore'))
    try:
        dot_near = pathlib.Path.home() / '.near'

        logger.info("Initialise local network nodes config.")
        cmd = f'neard --home /home/near localnet --v {NUM_NODES} --prefix test'
        docker_run(cmd, volume=(dot_near, '/home/near'), network=True)

        # Start all the nodes
        for ordinal in range(NUM_NODES):
            logger.info(f'Starting node {ordinal}')
            node = DockerNode(ordinal, dot_near / f'test{ordinal}')
            node.start(boot_node=nodes)
            nodes.append(node)

        # Wait for them to initialise
        for ordinal, node in enumerate(nodes):
            logger.info(f'Waiting for node {ordinal} to respond')
            node.wait_for_rpc(10)

        # Wait for BLOCKS blocks to be generated
        latest = utils.wait_for_blocks(nodes[0], target=BLOCKS)

        # Fetch latest block from all the nodes
        blocks = []
        for ordinal, node in enumerate(nodes):
            utils.wait_for_blocks(node, target=latest.height)
            response = node.get_block(latest.hash)
            assert 'result' in response, (ordinal, block)
            block = response['result']
            blocks.append(block)
            bid = cluster.BlockId.from_header(block['header'])
            logger.info(f'Node {ordinal} sees block: {bid}')

        # All blocks should be equal
        for ordinal in range(1, NUM_NODES):
            assert blocks[0] == blocks[ordinal], (ordinal, blocks)

        logger.info('All good')

    finally:
        # `docker stop` takes a few seconds so stop all containers in parallel.
        # atexit we’ll call DockerNode.cleanup method for each node as well and
        # it’ll handle all the other cleanups.
        cids = tuple(filter(None, (node._container_id for node in nodes)))
        if cids:
            logger.info('Stopping containers')
            run(('docker', 'rm', '-f') + cids)
        for node in nodes:
            node._container_id = None

        subprocess.check_call(
            ('docker', 'image', 'rm', '-f', _DOCKER_IMAGE_TAG))

if __name__ == '__main__':
    main()

'''
'''--- pytest/tests/sanity/epoch_switches.py ---
#!/usr/bin/env python3
# Spins up four nodes, and alternates [test1, test2] and [test3, test4] as block producers every epoch
# Makes sure that before the epoch switch each block is signed by all four

import sys, time, base58, random, datetime
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import start_cluster
from configured_logger import logger
from transaction import sign_staking_tx

EPOCH_LENGTH = 30
HEIGHT_GOAL = int(EPOCH_LENGTH * 7.5)
TIMEOUT = HEIGHT_GOAL * 3

config = None
nodes = start_cluster(
    2, 2, 1, config,
    [["epoch_length", EPOCH_LENGTH], ["block_producer_kickout_threshold", 40]],
    {
        0: {
            "view_client_throttle_period": {
                "secs": 0,
                "nanos": 0
            },
            "state_sync_enabled": True,
            "store.state_snapshot_enabled": True,
            "consensus": {
                "state_sync_timeout": {
                    "secs": 0,
                    "nanos": 500000000
                }
            }
        },
        1: {
            "view_client_throttle_period": {
                "secs": 0,
                "nanos": 0
            },
            "state_sync_enabled": True,
            "store.state_snapshot_enabled": True,
            "consensus": {
                "state_sync_timeout": {
                    "secs": 0,
                    "nanos": 500000000
                }
            }
        },
        2: {
            "tracked_shards": [0],
            "view_client_throttle_period": {
                "secs": 0,
                "nanos": 0
            },
            "state_sync_enabled": True,
            "store.state_snapshot_enabled": True,
            "consensus": {
                "state_sync_timeout": {
                    "secs": 0,
                    "nanos": 500000000
                }
            }
        },
        3: {
            "view_client_throttle_period": {
                "secs": 0,
                "nanos": 0
            },
            "state_sync_enabled": True,
            "store.state_snapshot_enabled": True,
            "consensus": {
                "state_sync_timeout": {
                    "secs": 0,
                    "nanos": 500000000
                }
            }
        }
    })

started = time.time()

def get_validators():
    return set([x['account_id'] for x in nodes[0].get_status()['validators']])

def get_stakes():
    return [
        int(nodes[2].get_account("test%s" % i)['result']['locked'])
        for i in range(3)
    ]

seen_epochs = set()
cur_vals = [0, 1]
next_vals = [2, 3]

height_to_num_approvals = {}

largest_height = 0

next_nonce = 1

epoch_switch_height = -2

blocks_by_height = {}

def wait_until_available(get_fn):
    while True:
        res = get_fn()
        logger.info(f"res: {res}")
        if 'result' in res:
            return res
        time.sleep(0.1)

for largest_height in range(2, HEIGHT_GOAL + 1):
    assert time.time() - started < TIMEOUT

    block = wait_until_available(
        lambda: nodes[0].get_block_by_height(largest_height, timeout=5))
    assert block is not None
    hash_ = block['result']['header']['hash']
    epoch_id = block['result']['header']['epoch_id']
    height = block['result']['header']['height']
    assert height == largest_height
    blocks_by_height[height] = block

    logger.info("... %s" % height)
    logger.info(block['result']['header']['approvals'])

    # we expect no skipped heights
    height_to_num_approvals[height] = len(
        block['result']['header']['approvals'])
    logger.info(
        f"Added height_to_num_approvals {height}={len(block['result']['header']['approvals'])}"
    )

    if height > epoch_switch_height + 2:
        prev_hash = None
        if (height - 1) in blocks_by_height:
            prev_hash = blocks_by_height[height - 1]['result']['header']['hash']
        if prev_hash:
            for val_ord in next_vals:
                tx = sign_staking_tx(nodes[val_ord].signer_key,
                                     nodes[val_ord].validator_key, 0,
                                     next_nonce,
                                     base58.b58decode(prev_hash.encode('utf8')))
                for target in range(0, 4):
                    nodes[target].send_tx(tx)
                next_nonce += 1

            for val_ord in cur_vals:
                tx = sign_staking_tx(nodes[val_ord].signer_key,
                                     nodes[val_ord].validator_key,
                                     50000000000000000000000000000000,
                                     next_nonce,
                                     base58.b58decode(prev_hash.encode('utf8')))
                for target in range(0, 4):
                    nodes[target].send_tx(tx)
                next_nonce += 1

    if epoch_id not in seen_epochs:
        seen_epochs.add(epoch_id)
        if height - 1 in blocks_by_height:
            prev_block = blocks_by_height[height - 1]
            assert prev_block['result']['header']['epoch_id'] != block[
                'result']['header']['epoch_id']

        logger.info("EPOCH %s, VALS %s" % (epoch_id, get_validators()))

        if len(seen_epochs) > 2:  # the first two epochs share the validator set
            logger.info(
                f"Checking height_to_num_approvals {height}, {height_to_num_approvals}"
            )
            assert height_to_num_approvals[height] == 2

            has_prev = height - 1 in height_to_num_approvals
            has_two_ago = height - 2 in height_to_num_approvals

            if has_prev:
                assert height_to_num_approvals[height - 1] == 4
            if has_two_ago:
                assert height_to_num_approvals[height - 2] == 4

            if has_prev and has_two_ago:
                for i in range(3, EPOCH_LENGTH):
                    if height - i in height_to_num_approvals:
                        assert height_to_num_approvals[height - i] == 2
        else:
            for i in range(height):
                if i in height_to_num_approvals:
                    assert height_to_num_approvals[i] == 2, (
                        i, height_to_num_approvals[i], height_to_num_approvals)

        cur_vals, next_vals = next_vals, cur_vals
        epoch_switch_height = height

assert len(seen_epochs) > 3

'''
'''--- pytest/tests/sanity/garbage_collection.py ---
#!/usr/bin/env python3
# Spins up two validating nodes. Stop one of them and make another one produce
# sufficient number of blocks. Restart the stopped node and check that it can
# still sync.

import sys, time
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import start_cluster
from configured_logger import logger
import utils

EPOCH_LENGTH = 20
TARGET_HEIGHT = EPOCH_LENGTH * 6

consensus_config = {
    "consensus": {
        "min_block_production_delay": {
            "secs": 0,
            "nanos": 100000000
        },
        "max_block_production_delay": {
            "secs": 0,
            "nanos": 400000000
        },
        "max_block_wait_delay": {
            "secs": 0,
            "nanos": 400000000
        }
    }
}

nodes = start_cluster(
    2, 0, 1, None,
    [["epoch_length", EPOCH_LENGTH], ["num_block_producer_seats", 5],
     ["num_block_producer_seats_per_shard", [5]],
     ["chunk_producer_kickout_threshold", 80],
     ["shard_layout", {
         "V0": {
             "num_shards": 1,
             "version": 1,
         }
     }], ["validators", 0, "amount", "110000000000000000000000000000000"],
     [
         "records", 0, "Account", "account", "locked",
         "110000000000000000000000000000000"
     ], ["total_supply", "3060000000000000000000000000000000"]], {
         0: consensus_config,
         1: consensus_config
     })

logger.info('Kill node 1')
nodes[1].kill()

node0_height, _ = utils.wait_for_blocks(nodes[0], target=TARGET_HEIGHT)

logger.info('Restart node 1')
nodes[1].start(boot_node=nodes[1])
time.sleep(3)

start_time = time.time()

utils.wait_for_blocks(nodes[1], target=node0_height)

'''
'''--- pytest/tests/sanity/garbage_collection1.py ---
#!/usr/bin/env python3
# Spins up three validating nodes with stake distribution 11, 5, 5.
# Stop the two nodes with stake 2
# Wait for sufficient number of blocks.
# Restart one of the stopped nodes and wait until it syncs with the running node.
# Restart the other one. Make sure it can sync as well.

import sys, time
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import start_cluster
from configured_logger import logger
import utils

EPOCH_LENGTH = 20
TARGET_HEIGHT = EPOCH_LENGTH * 6
TIMEOUT = 30

nodes_config = {
    "consensus": {
        "min_block_production_delay": {
            "secs": 0,
            "nanos": 100000000
        },
        "max_block_production_delay": {
            "secs": 0,
            "nanos": 400000000
        },
        "max_block_wait_delay": {
            "secs": 0,
            "nanos": 400000000
        }
    },
    # Enabling explicitly state sync, default value is False
    "state_sync_enabled": True
}

nodes = start_cluster(
    3, 0, 1, None,
    [["epoch_length", EPOCH_LENGTH], ["num_block_producer_seats", 5],
     ["num_block_producer_seats_per_shard", [5]],
     ["total_supply", "4210000000000000000000000000000000"],
     ["validators", 0, "amount", "260000000000000000000000000000000"],
     [
         "records", 0, "Account", "account", "locked",
         "260000000000000000000000000000000"
     ]], {
         0: nodes_config,
         1: nodes_config,
         2: nodes_config
     })

logger.info('kill node1 and node2')
nodes[1].kill()
nodes[2].kill()

node0_height, _ = utils.wait_for_blocks(nodes[0], target=TARGET_HEIGHT)

logger.info('Restart node 1')
nodes[1].start(boot_node=nodes[1])
time.sleep(2)

for height, _ in utils.poll_blocks(nodes[1], timeout=TIMEOUT):
    if height >= node0_height and len(nodes[0].validators()) < 3:
        break

logger.info('Restart node 2')
nodes[2].start(boot_node=nodes[2])
time.sleep(2)

target = nodes[0].get_latest_block().height
utils.wait_for_blocks(nodes[2], target=target)

'''
'''--- pytest/tests/sanity/garbage_collection_archival.py ---
#!/usr/bin/env python3
# Spins up two validator nodes and one archival node. Insert and delete data from a smart contract
# Verify that the archival node still has the old history.

import sys, time
import pathlib
import string, random, json, base64

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import start_cluster
from configured_logger import logger
from transaction import sign_create_account_with_full_access_key_and_balance_tx, sign_delete_account_tx
from utils import wait_for_blocks
from account import NEAR_BASE
from key import Key

EPOCH_LENGTH = 5
TARGET_HEIGHT = 300

client_config = {
    "consensus": {
        "min_block_production_delay": {
            "secs": 0,
            "nanos": 200000000
        },
        "max_block_production_delay": {
            "secs": 0,
            "nanos": 400000000
        },
        "max_block_wait_delay": {
            "secs": 0,
            "nanos": 400000000
        }
    },
    "gc_step_period": {
        "secs": 0,
        "nanos": 100000000
    },
    "rpc": {
        "polling_config": {
            "polling_interval": {
                "secs": 0,
                "nanos": 20000000
            },
            "polling_timeout": {
                "secs": 10,
                "nanos": 0
            }
        }
    },
    "tracked_shards": [0]
}

archival_config = {
    "gc_step_period": {
        "secs": 0,
        "nanos": 100000000
    },
    "rpc": {
        "polling_config": {
            "polling_interval": {
                "secs": 0,
                "nanos": 10000000
            },
            "polling_timeout": {
                "secs": 10,
                "nanos": 0
            }
        }
    },
    "archive": True,
    "save_trie_changes": True,
    "split_storage": {
        "enable_split_storage_view_client": True,
        "cold_store_initial_migration_loop_sleep_duration": {
            "secs": 0,
            "nanos": 100000000
        },
        "cold_store_loop_sleep_duration": {
            "secs": 0,
            "nanos": 100000000
        },
    },
    "cold_store": {
        "path": "cold-data",
    },
    "tracked_shards": [0]
}

nodes = start_cluster(
    2, 1, 1, None,
    [["epoch_length", EPOCH_LENGTH], ["num_block_producer_seats", 5],
     ["num_block_producer_seats_per_shard", [5]],
     ["chunk_producer_kickout_threshold", 80],
     ["transaction_validity_period", 100000]], {
         0: client_config,
         1: client_config,
         2: archival_config
     })

# generate 20 keys
keys = [''.join(random.choices(string.ascii_lowercase, k=3)) for _ in range(20)]
key_to_block_hash = {}
nonce = 1
time.sleep(1)
block_id = nodes[1].get_latest_block(check_storage=False)

# insert all accounts
for key in keys:
    print(f"inserting {key}")
    tx = sign_create_account_with_full_access_key_and_balance_tx(
        nodes[0].signer_key, f"{key}.{nodes[0].signer_key.account_id}",
        nodes[0].signer_key, NEAR_BASE, nonce, block_id.hash_bytes)
    res = nodes[1].send_tx_and_wait(tx, 2)
    assert 'SuccessValue' in res['result']['status']
    key_to_block_hash[key] = res['result']['receipts_outcome'][0]['block_hash']
    nonce += 1
print("keys inserted")

# delete all accounts
for key in keys:
    print(f"deleting {key}")
    block_id = nodes[1].get_latest_block(check_storage=False)
    account_id = f"{key}.{nodes[0].signer_key.account_id}"
    key = Key(account_id, nodes[0].signer_key.pk, nodes[0].signer_key.sk)
    tx = sign_delete_account_tx(key, account_id, nodes[0].signer_key.account_id,
                                int(block_id.height) * 1_000_000,
                                block_id.hash_bytes)
    res = nodes[1].send_tx_and_wait(tx, 2)
    assert 'result' in res, res
    assert 'SuccessValue' in res['result']['status'], res

# wait for the deletions to be garbage collected
deletion_finish_block_height = int(
    nodes[1].get_latest_block(check_storage=False).height)
wait_for_blocks(nodes[1],
                target=deletion_finish_block_height + EPOCH_LENGTH * 6)

# check that querying a validator node on the block at which the key inserted fails,
# but querying an archival node succeeeds
for key in keys:
    # check that it doesn't exist at the latest height
    res = nodes[1].json_rpc(
        'query', {
            "request_type": "view_account",
            "account_id": f"{key}.{nodes[0].signer_key.account_id}",
            "finality": "final"
        })
    assert 'error' in res, res

    # check that the history can be queried on archival node
    block_hash = key_to_block_hash[key]
    res = nodes[2].json_rpc(
        'query', {
            "request_type": "view_account",
            "account_id": f"{key}.{nodes[0].signer_key.account_id}",
            "block_id": block_hash
        })
    assert 'result' in res, res
    assert res['result']['amount'] == str(NEAR_BASE)

    # check that the history cannot be queried on a nonarchival node
    res = nodes[1].json_rpc(
        'query', {
            "request_type": "view_account",
            "account_id": f"{key}.{nodes[0].signer_key.account_id}",
            "block_id": block_hash
        })
    assert 'error' in res, res

nodes[1].check_store()
nodes[2].check_store()

'''
'''--- pytest/tests/sanity/garbage_collection_intense.py ---
#!/usr/bin/env python3
# Spins up two validating nodes. Deploy a contract that allows for insertion and deletion of keys
# Randomly insert keys or delete keys every block. Let it run until GC kicks in
# Then delete all keys and let garbage collection catch up

import sys, time
import pathlib
import string, random, json
import subprocess

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import start_cluster
from transaction import sign_deploy_contract_tx, sign_function_call_tx
from utils import load_test_contract, wait_for_blocks

EPOCH_LENGTH = 5
TARGET_HEIGHT = 300
GAS = 100_000_000_000_000

client_config = {
    "consensus": {
        "min_block_production_delay": {
            "secs": 0,
            "nanos": 100000000
        },
        "max_block_production_delay": {
            "secs": 0,
            "nanos": 400000000
        },
        "max_block_wait_delay": {
            "secs": 0,
            "nanos": 400000000
        }
    },
    "gc_step_period": {
        "secs": 0,
        "nanos": 100000000
    },
    "rpc": {
        "polling_config": {
            "polling_interval": {
                "secs": 0,
                "nanos": 10000000
            },
            "polling_timeout": {
                "secs": 10,
                "nanos": 0
            }
        }
    }
}

nodes = start_cluster(
    2, 0, 1, None,
    [["epoch_length", EPOCH_LENGTH], ["num_block_producer_seats", 5],
     ["num_block_producer_seats_per_shard", [5]],
     ["chunk_producer_kickout_threshold", 80],
     ["shard_layout", {
         "V0": {
             "num_shards": 1,
             "version": 1,
         }
     }], ["validators", 0, "amount", "110000000000000000000000000000000"],
     [
         "records", 0, "Account", "account", "locked",
         "110000000000000000000000000000000"
     ], ["total_supply", "3060000000000000000000000000000000"]], {
         0: client_config,
         1: client_config
     })

# generate 20 keys
keys = [''.join(random.choices(string.ascii_lowercase, k=3)) for _ in range(20)]
key_refcount = {x: 0 for x in keys}
nonce = 1
contract = load_test_contract()

last_block_hash = nodes[0].get_latest_block().hash_bytes
tx = sign_deploy_contract_tx(nodes[0].signer_key, contract, nonce,
                             last_block_hash)
res = nodes[0].send_tx_and_wait(tx, 2)
nonce += 1
assert 'SuccessValue' in res['result']['status']
time.sleep(1)

nodes[1].stop_checking_store()

while True:
    block_id = nodes[1].get_latest_block()
    if int(block_id.height) > TARGET_HEIGHT:
        break
    for i in range(1, 20):
        start = 0
        block_hash = nodes[1].get_latest_block().hash_bytes
        args = start.to_bytes(8, 'little') + i.to_bytes(8, 'little')
        if random.random() > 0.5:
            tx = sign_function_call_tx(nodes[0].signer_key,
                                       nodes[0].signer_key.account_id,
                                       'insert_strings', args, GAS, 0, nonce,
                                       block_hash)
        else:
            tx = sign_function_call_tx(nodes[0].signer_key,
                                       nodes[0].signer_key.account_id,
                                       'delete_strings', args, GAS, 0, nonce,
                                       block_hash)
        res = nodes[1].send_tx(tx)
        assert 'result' in res, res
        nonce += 1

# delete all keys
for i in range(1, 20):
    start = 0
    args = start.to_bytes(8, 'little') + i.to_bytes(8, 'little')
    block_id = nodes[1].get_latest_block()
    tx = sign_function_call_tx(nodes[0].signer_key,
                               nodes[0].signer_key.account_id, 'delete_strings',
                               args, GAS, 0, nonce, block_id.hash_bytes)
    res = nodes[1].send_tx_and_wait(tx, 2)
    assert 'result' in res, res
    assert 'SuccessValue' in res['result']['status'], res
    nonce += 1

# wait for the deletions to be garbage collected
deletion_finish_block_height = int(nodes[1].get_latest_block().height)
wait_for_blocks(nodes[1],
                target=deletion_finish_block_height + EPOCH_LENGTH * 6)

# check that querying a garbage collected block gives Error::GarbageCollected
res = nodes[1].json_rpc(
    'query', {
        "request_type": "view_account",
        "account_id": nodes[0].signer_key.account_id,
        "block_id": deletion_finish_block_height
    })
assert res['error']['cause']['name'] == "GARBAGE_COLLECTED_BLOCK", res

nodes[1].check_store()

'''
'''--- pytest/tests/sanity/gc_after_sync.py ---
#!/usr/bin/env python3
# Spins up three validating nodes. Stop one of them and make another one produce
# sufficient number of blocks. Restart the stopped node and check that it can
# still sync. Then check all old data is removed.

import pathlib
import sys

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import start_cluster
from configured_logger import logger
import state_sync_lib
import utils

EPOCH_LENGTH = 50
TARGET_HEIGHT = int(EPOCH_LENGTH * 2.5)
AFTER_SYNC_HEIGHT = EPOCH_LENGTH * 10
TIMEOUT = 300

node_config = state_sync_lib.get_state_sync_config_combined()
# We're generating many blocks here - lower the min production delay to speed
# up the test running time a little.
node_config["consensus.min_block_production_delay"] = {
    "secs": 0,
    "nanos": 100000000
}
node_config["consensus.max_block_production_delay"] = {
    "secs": 0,
    "nanos": 300000000
}
node_config["consensus.max_block_wait_delay"] = {"secs": 0, "nanos": 600000000}
node_config["consensus.block_fetch_horizon"] = 1
node_config["gc_step_period"] = {"secs": 0, "nanos": 100000000}

nodes = start_cluster(
    4, 0, 1,
    None, [["epoch_length", EPOCH_LENGTH],
           ["num_block_producer_seats_per_shard", [5]],
           ["validators", 0, "amount", "60000000000000000000000000000000"],
           ["block_producer_kickout_threshold", 50],
           ["chunk_producer_kickout_threshold", 50],
           [
               "records", 0, "Account", "account", "locked",
               "60000000000000000000000000000000"
           ], ["total_supply", "5010000000000000000000000000000000"]],
    {x: node_config for x in range(4)})

node0_height, _ = utils.wait_for_blocks(nodes[0], target=TARGET_HEIGHT)

logger.info('Kill node 1')
nodes[1].kill()

node0_height, _ = utils.wait_for_blocks(nodes[0], target=AFTER_SYNC_HEIGHT)

logger.info('Restart node 1')
nodes[1].start(boot_node=nodes[1])

node1_height, _ = utils.wait_for_blocks(nodes[1], target=node0_height)

epoch_id = nodes[1].json_rpc('block', [node1_height],
                             timeout=15)['result']['header']['epoch_id']
epoch_start_height = nodes[1].get_validators(
    epoch_id=epoch_id)['result']['epoch_start_height']

# all fresh data should be synced
blocks_count = 0
for height in range(max(node1_height - 10, epoch_start_height),
                    node1_height + 1):
    block0 = nodes[0].json_rpc('block', [height], timeout=15)
    block1 = nodes[1].json_rpc('block', [height], timeout=15)
    assert block0 == block1, (block0, block1)
    if 'result' in block0:
        blocks_count += 1
assert blocks_count > 0

# all old data should be GCed
blocks_count = 0
for height in range(1, EPOCH_LENGTH * 4):
    block0 = nodes[0].json_rpc('block', [height], timeout=15)
    block1 = nodes[1].json_rpc('block', [height], timeout=15)
    assert block0 == block1, (block0, block1)
    if 'result' in block0:
        blocks_count += 1
assert blocks_count == 0

'''
'''--- pytest/tests/sanity/gc_after_sync1.py ---
#!/usr/bin/env python3
# Spin up one validating node and one nonvalidating node
# stop the nonvalidating node in the second epoch and
# restart it in the fourth epoch to trigger state sync
# Check that after 10 epochs the node has properly garbage
# collected blocks.

import sys, time
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import start_cluster
import state_sync_lib
from configured_logger import logger
import utils

EPOCH_LENGTH = 30
TARGET_HEIGHT1 = EPOCH_LENGTH + (EPOCH_LENGTH // 2)
TARGET_HEIGHT2 = EPOCH_LENGTH * 3 + (EPOCH_LENGTH // 2)
TARGET_HEIGHT3 = EPOCH_LENGTH * 10 + (EPOCH_LENGTH // 2)

node0_config, node1_config = state_sync_lib.get_state_sync_configs_pair()

node0_config.update({"gc_blocks_limit": 10})

node1_config.update({
    "consensus": {
        "block_fetch_horizon": 10,
        "block_header_fetch_horizon": 10,
    },
    "tracked_shards": [0],
    "gc_blocks_limit": 10,
    "gc_step_period": {
        "secs": 0,
        "nanos": 100000000
    }
})

nodes = start_cluster(
    1, 1, 1, None,
    [["epoch_length", EPOCH_LENGTH], ["block_producer_kickout_threshold", 80],
     ["chunk_producer_kickout_threshold", 80]], {
         0: node0_config,
         1: node1_config
     })

height = nodes[1].get_latest_block().height

utils.wait_for_blocks(nodes[0], target=TARGET_HEIGHT1)

logger.info('Kill node 1')
nodes[1].kill()

utils.wait_for_blocks(nodes[0], target=TARGET_HEIGHT2)

logger.info('Restart node 1')
nodes[1].start(boot_node=nodes[0])

utils.wait_for_blocks(nodes[0], target=TARGET_HEIGHT3)

nodes[0].kill()

for i in range(1, EPOCH_LENGTH * 6):
    res = nodes[1].json_rpc('block', [i], timeout=10)
    assert 'error' in res, f'height {i}, {res}'

for i in range(EPOCH_LENGTH * 6, EPOCH_LENGTH * 10 + 1):
    res = nodes[1].json_rpc('block', [i], timeout=10)
    assert 'result' in res, f'height {i}, {res}'

'''
'''--- pytest/tests/sanity/gc_sync_after_sync.py ---
#!/usr/bin/env python3
# Spins up three validating nodes. Stop one of them and make another one produce
# sufficient number of blocks. Restart the stopped node and check that it can
# still sync. Repeat. Then check all old data is removed.

import pathlib
import sys
import tempfile
import time

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

swap_nodes = ("swap_nodes" in sys.argv)  # swap nodes 0 and 1 after first sync

from cluster import start_cluster
from configured_logger import logger
import state_sync_lib
import utils

EPOCH_LENGTH = 30
NUM_GC_EPOCHS = 3
# The gaps need to be longer than NUM_GC_EPOCHS epochs for the garbage collection to kick in.
TARGET_HEIGHT1 = EPOCH_LENGTH * (NUM_GC_EPOCHS + 1)
TARGET_HEIGHT2 = EPOCH_LENGTH * 2 * (NUM_GC_EPOCHS + 1)
TARGET_HEIGHT3 = EPOCH_LENGTH * 3 * (NUM_GC_EPOCHS + 1)

node_config = state_sync_lib.get_state_sync_config_combined()
node_config["gc_num_epochs_to_keep"] = NUM_GC_EPOCHS

nodes = start_cluster(
    4, 0, 1, None,
    [["epoch_length", EPOCH_LENGTH],
     ["validators", 0, "amount", "12500000000000000000000000000000"],
     [
         "records", 0, "Account", "account", "locked",
         "12500000000000000000000000000000"
     ], ["validators", 1, "amount", "12500000000000000000000000000000"],
     [
         "records", 2, "Account", "account", "locked",
         "12500000000000000000000000000000"
     ], ['total_supply', "4925000000000000000000000000000000"],
     ["block_producer_kickout_threshold", 40],
     ["chunk_producer_kickout_threshold", 40], ["num_block_producer_seats", 10],
     ["num_block_producer_seats_per_shard", [10]]],
    {x: node_config for x in range(4)})

logger.info('Kill node 1')
nodes[1].kill()

node0_height, _ = utils.wait_for_blocks(nodes[0], target=TARGET_HEIGHT1)

logger.info('Starting back node 1')
nodes[1].start(boot_node=nodes[1])
time.sleep(3)

node1_height, _ = utils.wait_for_blocks(nodes[1], target=node0_height)

if swap_nodes:
    logger.info('Swap nodes 0 and 1')
    nodes[0], nodes[1] = nodes[1], nodes[0]

logger.info('Kill node 1')
nodes[1].kill()

node0_height, _ = utils.wait_for_blocks(nodes[0], target=TARGET_HEIGHT2)

logger.info('Restart node 1')
nodes[1].start(boot_node=nodes[1])
# State Sync makes the storage seem inconsistent.
nodes[1].stop_checking_store()
time.sleep(3)

node1_height, _ = utils.wait_for_blocks(nodes[1],
                                        target=node0_height + EPOCH_LENGTH)

logger.info(f'node0_height: {node0_height}, node1_height: {node1_height}')

# all fresh data should be synced
blocks_count = 0
for height in range(node1_height - 10, node1_height):
    logger.info(f'Check block at height {height}')
    block0 = nodes[0].json_rpc('block', [height], timeout=15)
    block1 = nodes[1].json_rpc('block', [height], timeout=15)
    assert block0 == block1, (
        f'fresh block at height: {height}, block0: {block0}, block1: {block1}')
    if 'result' in block0:
        blocks_count += 1
assert blocks_count > 0
time.sleep(1)

# all old data should be GCed
blocks_count = 0
for height in range(1, 15):
    logger.info(f'Check old block at height {height}')
    block0 = nodes[0].json_rpc('block', [height], timeout=15)
    block1 = nodes[1].json_rpc('block', [height], timeout=15)
    assert block0 == block1, (
        f'old block at height: {height}, block0: {block0}, block1: {block1}')
    if 'result' in block0:
        blocks_count += 1
assert blocks_count == 0

# all data after first sync should be GCed
blocks_count = 0
for height in range(TARGET_HEIGHT1, TARGET_HEIGHT1 + 10):
    logger.info(f'Check block after first sync at height {height}')
    block1 = nodes[1].json_rpc('block', [height], timeout=15)
    if 'result' in block1:
        blocks_count += 1
assert blocks_count == 0

# all data before second sync should be GCed
blocks_count = 0
for height in range(TARGET_HEIGHT2 - 15, TARGET_HEIGHT2 - 5):
    logger.info(f'Check block before second sync at height {height}')
    block1 = nodes[1].json_rpc('block', [height], timeout=15)
    if 'result' in block1:
        logger.info(block1['result'])
        blocks_count += 1
assert blocks_count == 0

# check that node can GC normally after syncing
utils.wait_for_blocks(nodes[1], target=TARGET_HEIGHT3, verbose=True)

logger.info('EPIC')

'''
'''--- pytest/tests/sanity/handshake_tie_resolution.py ---
#!/usr/bin/env python3
"""
Spawn a cluster with four nodes. Check that no node tries to
connect to another node that is currently connected.
"""

import sys, time
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

import cluster
import utils

BLOCKS = 20

nodes = cluster.start_cluster(4, 0, 4, None, [], {})
trackers = [utils.LogTracker(node) for node in nodes]
utils.wait_for_blocks(nodes[0], target=BLOCKS)
assert all(not tracker.check('Dropping handshake (Active Peer).')
           for tracker in trackers)

'''
'''--- pytest/tests/sanity/large_messages.py ---
#!/usr/bin/env python3
import sys, time
import socket, struct, multiprocessing
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import start_cluster
from configured_logger import logger

PACKAGE_LEN = 16 * 1024 * 1024
N_PROCESSES = 16

buf = bytes([0] * PACKAGE_LEN)

nodes = start_cluster(2, 0, 4, None, [], {})

def one_process(ord_, seconds):
    started = time.time()
    sent = 0
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.connect(nodes[0].addr())
        while time.time() - started < seconds:
            s.send(struct.pack('I', PACKAGE_LEN))
            s.send(buf)
            sent += PACKAGE_LEN
            logger.info("PROCESS %s SENT %s BYTES" % (ord_, sent))

last_height = nodes[0].get_latest_block().height

for seconds in [20, 120]:
    ps = [
        multiprocessing.Process(target=one_process, args=(i, seconds))
        for i in range(N_PROCESSES)
    ]

    for p in ps:
        p.start()

    for p in ps:
        p.join()

    new_height = nodes[0].get_latest_block().height
    assert new_height - last_height > 5, "new height: %s, last_height: %s" % (
        new_height, last_height)
    last_height = new_height

'''
'''--- pytest/tests/sanity/lightclnt.py ---
#!/usr/bin/env python3
# Generates three epochs worth of blocks
# Requests next light client block until it reaches the last final block.
# Verifies that the returned blocks are what we expect, and runs the validation on them

import sys, time
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import start_cluster, load_config
from configured_logger import logger
from lightclient import compute_block_hash, validate_light_client_block
import utils

TIMEOUT = 150
config = load_config()
client_config_changes = {}
if not config['local']:
    client_config_changes = {
        "consensus": {
            "min_block_production_delay": {
                "secs": 4,
                "nanos": 0,
            },
            "max_block_production_delay": {
                "secs": 8,
                "nanos": 0,
            },
            "max_block_wait_delay": {
                "secs": 24,
                "nanos": 0,
            },
        }
    }
    TIMEOUT = 600

client_config_changes['archive'] = True
client_config_changes['tracked_shards'] = [0]  # Track all shards

no_state_snapshots_config = client_config_changes
no_state_snapshots_config['store.state_snapshot_enabled'] = False

nodes = start_cluster(
    4, 0, 4, None,
    [["epoch_length", 6], ["block_producer_kickout_threshold", 40],
     ["chunk_producer_kickout_threshold", 40]], {
         0: no_state_snapshots_config,
         1: client_config_changes,
         2: client_config_changes,
         3: client_config_changes
     })

for node in nodes:
    node.stop_checking_store()

started = time.time()

hash_to_height = {}
hash_to_epoch = {}
hash_to_next_epoch = {}
height_to_hash = {}
epochs = []

first_epoch_switch_height = None
last_epoch = None

block_producers_map = {}

def get_light_client_block(hash_, last_known_block):
    global block_producers_map

    ret = nodes[0].json_rpc('next_light_client_block', [hash_])
    if ret['result'] != {} and last_known_block is not None:
        validate_light_client_block(last_known_block,
                                    ret['result'],
                                    block_producers_map,
                                    panic=True)
    return ret

def get_up_to(from_, to):
    global first_epoch_switch_height, last_epoch

    for height, hash_ in utils.poll_blocks(nodes[0],
                                           timeout=TIMEOUT,
                                           poll_interval=0.01):
        block = nodes[0].get_block(hash_)

        hash_to_height[hash_] = height
        height_to_hash[height] = hash_

        cur_epoch = block['result']['header']['epoch_id']

        hash_to_epoch[hash_] = cur_epoch
        hash_to_next_epoch[hash_] = block['result']['header']['next_epoch_id']

        if (first_epoch_switch_height is None and last_epoch is not None and
                last_epoch != cur_epoch):
            first_epoch_switch_height = height
        last_epoch = cur_epoch

        if height >= to:
            break

    for i in range(from_, to + 1):
        hash_ = height_to_hash[i]
        logger.info(
            f"{i} {hash_} {hash_to_epoch[hash_]} {hash_to_next_epoch[hash_]}")

        if len(epochs) == 0 or epochs[-1] != hash_to_epoch[hash_]:
            epochs.append(hash_to_epoch[hash_])

# don't start from 1, since couple heights get produced while the nodes spin up
get_up_to(4, 15)
get_up_to(16, 22 + first_epoch_switch_height)

# since we already "know" the first block, the first light client block that will be returned
# will be for the second epoch. The second epoch spans blocks 7-12, and the last final block in
# it has height 10. Then blocks go in increments of 6.
# the last block returned will be the last final block, with height 27
heights = [
    None, 3 + first_epoch_switch_height, 9 + first_epoch_switch_height,
    15 + first_epoch_switch_height, 20 + first_epoch_switch_height
]

last_known_block_hash = height_to_hash[4]
last_known_block = None
iter_ = 1

while True:
    assert time.time() - started < TIMEOUT

    res = get_light_client_block(last_known_block_hash, last_known_block)

    if last_known_block_hash == height_to_hash[20 + first_epoch_switch_height]:
        assert res['result'] == {}
        break

    assert res['result']['inner_lite']['epoch_id'] == epochs[iter_]
    logger.info(f"{iter_} {heights[iter_]}")
    assert res['result']['inner_lite']['height'] == heights[iter_], (
        res['result']['inner_lite'], first_epoch_switch_height)

    last_known_block_hash = compute_block_hash(
        res['result']['inner_lite'], res['result']['inner_rest_hash'],
        res['result']['prev_block_hash']).decode('ascii')
    assert last_known_block_hash == height_to_hash[
        res['result']['inner_lite']['height']], "%s != %s" % (
            last_known_block_hash,
            height_to_hash[res['result']['inner_lite']['height']])

    if last_known_block is None:
        block_producers_map[res['result']['inner_lite']
                            ['next_epoch_id']] = res['result']['next_bps']
    last_known_block = res['result']

    iter_ += 1

res = get_light_client_block(height_to_hash[19 + first_epoch_switch_height],
                             last_known_block)
logger.info(res)
assert res['result']['inner_lite']['height'] == 20 + first_epoch_switch_height

get_up_to(23 + first_epoch_switch_height, 24 + first_epoch_switch_height)

# Test that the light client block is always in the same epoch as the block 2 heights onward (needed to make
# sure that the proofs can be verified using the keys of the block producers of the epoch of the block).
# Before the loop below the last block is 24 + C, which is in the new epoch, thus we expect the light client
# block during the first iteration to be 21 + C. After the first iteration we move one block onward, now
# having the head at 25 + C. At this point the last final block is 23 + C, still in the previous epoch, so
# we still expect 21 + C to be returned. We then move again to 26 + C, and (in the section after the loop)
# check that the light client block now corresponds to 24 + C, which is in the same epoch as 26 + C.
for i in range(2):
    res = get_light_client_block(height_to_hash[19 + first_epoch_switch_height],
                                 last_known_block)
    assert res['result']['inner_lite'][
        'height'] == 21 + first_epoch_switch_height, (
            res['result']['inner_lite']['height'],
            21 + first_epoch_switch_height)

    res = get_light_client_block(height_to_hash[20 + first_epoch_switch_height],
                                 last_known_block)
    assert res['result']['inner_lite'][
        'height'] == 21 + first_epoch_switch_height

    res = get_light_client_block(height_to_hash[21 + first_epoch_switch_height],
                                 last_known_block)
    assert res['result'] == {}

    get_up_to(i + 25 + first_epoch_switch_height,
              i + 25 + first_epoch_switch_height)

res = get_light_client_block(height_to_hash[21 + first_epoch_switch_height],
                             last_known_block)
assert res['result']['inner_lite']['height'] == 24 + first_epoch_switch_height

'''
'''--- pytest/tests/sanity/meta_tx.py ---
#!/usr/bin/env python3
# Tests the meta transaction flow.
# Creates a new account (candidate.test0) with a fixed amount of tokens.
# Afterwards, creates the meta transaction that adds a new key to this account, but the gas is paid by someone else (test0) account.
# At the end, verifies that key has been added successfully and that the amount of tokens in candidate didn't change.

import base58
import pathlib
import sys
import typing

import unittest

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import start_cluster, LocalNode
import utils
import transaction
import key

class Nonce:
    """ Helper class to manage nonces (automatically increase them when they are used. """

    def __init__(self, current_nonce: int = 0):
        self.nonce = current_nonce

    def use_nonce(self) -> int:
        self.nonce += 1
        return self.nonce

def create_nonce_from_node(node: LocalNode, account_id: str, pk: str) -> Nonce:
    nn = node.get_nonce_for_pk(account_id, pk)
    assert nn, "Nonce missing for the candidate account"
    return Nonce(nn)

# Returns the number of keys and current amount for a given account
def check_account_status(node: LocalNode,
                         account_id: str) -> typing.Tuple[int, int]:
    current_keys = node.get_access_key_list(account_id)['result']['keys']
    account_state = node.get_account(account_id)['result']
    return (len(current_keys), int(account_state['amount']))

class TestMetaTransactions(unittest.TestCase):

    def test_meta_tx(self):
        nodes: list[LocalNode] = start_cluster(2, 0, 1, None, [], {})
        _, hash_ = utils.wait_for_blocks(nodes[0], target=10)

        node0_nonce = Nonce()

        CANDIDATE_ACCOUNT = "candidate.test0"
        CANDIDATE_STARTING_AMOUNT = 123 * (10**24)

        # create new account
        candidate_key = key.Key.from_random(CANDIDATE_ACCOUNT)

        tx = transaction.sign_create_account_with_full_access_key_and_balance_tx(
            nodes[0].signer_key, candidate_key.account_id, candidate_key,
            CANDIDATE_STARTING_AMOUNT, node0_nonce.use_nonce(),
            base58.b58decode(hash_.encode('utf8')))
        nodes[0].send_tx_and_wait(tx, 100)

        self.assertEqual(check_account_status(nodes[0], CANDIDATE_ACCOUNT),
                         (1, CANDIDATE_STARTING_AMOUNT))

        candidate_nonce = create_nonce_from_node(nodes[0],
                                                 candidate_key.account_id,
                                                 candidate_key.pk)

        # Now let's prepare the meta transaction.
        new_key = key.Key.from_random("new_key")
        add_new_key_action = transaction.create_full_access_key_action(
            new_key.decoded_pk())
        signed_meta_tx = transaction.create_signed_delegated_action(
            CANDIDATE_ACCOUNT, CANDIDATE_ACCOUNT, [add_new_key_action],
            candidate_nonce.use_nonce(), 1000, candidate_key.decoded_pk(),
            candidate_key.decoded_sk())

        meta_tx = transaction.sign_delegate_action(
            signed_meta_tx, nodes[0].signer_key, CANDIDATE_ACCOUNT,
            node0_nonce.use_nonce(), base58.b58decode(hash_.encode('utf8')))

        nodes[0].send_tx_and_wait(meta_tx, 100)

        self.assertEqual(check_account_status(nodes[0], CANDIDATE_ACCOUNT),
                         (2, CANDIDATE_STARTING_AMOUNT))

if __name__ == '__main__':
    unittest.main()

'''
'''--- pytest/tests/sanity/network_drop_package.py ---
#!/usr/bin/env python3
import sys, time, random
import multiprocessing
import logging
import pathlib
from functools import partial

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import start_cluster
from peer import *
from proxy import ProxyHandler

from multiprocessing import Value

TIMEOUT = 90

# Ratio of message that are dropped to simulate bad network performance
DROP_RATIO = 0.05

class Handler(ProxyHandler):

    def __init__(self, *args, success=None, **kwargs):
        assert success is not None
        self.success = success
        super().__init__(*args, **kwargs)
        self.dropped = 0
        self.total = 0

    async def handle(self, msg, fr, to):
        if msg.enum == 'Block':
            h = msg.Block.header().inner_lite().height

            with self.success.get_lock():
                if h >= 10 and self.success.value == 0:
                    logging.info(
                        f'SUCCESS DROP={self.dropped} TOTAL={self.total}')
                    self.success.value = 1

        drop = random.random() < DROP_RATIO and 'Handshake' not in msg.enum

        if drop:
            self.dropped += 1
        self.total += 1

        return not drop

if __name__ == '__main__':
    success = Value('i', 0)

    start_cluster(3, 0, 1, None, [["epoch_length", 500]], {},
                  partial(Handler, success=success))

    started = time.time()

    while True:
        logging.info(f"Time: {time.time() - started:0.2}, Fin: {success.value}")
        assert time.time() - started < TIMEOUT
        time.sleep(1)

        if success.value == 1:
            break

    logging.info("Success")

'''
'''--- pytest/tests/sanity/one_val.py ---
#!/usr/bin/env python3
# Creates a genesis config with two block producers, and kills one right away after
# launch. Makes sure that the other block producer can produce blocks with chunks and
# process transactions. Makes large-ish number of block producers per shard to minimize
# the chance of the second block producer occupying all the seats in one of the shards

import sys, time, base58, random
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import start_cluster
from configured_logger import logger
import utils
from transaction import sign_payment_tx

TIMEOUT = 180

nightly = len(sys.argv) > 1
genesis_change = [
    ["num_block_producer_seats", 199],
    ["num_block_producer_seats_per_shard", [24, 25, 25, 25, 25, 25, 25, 25]],
    ["min_gas_price", 0], ["max_inflation_rate", [0, 1]], ["epoch_length", 10],
    ["block_producer_kickout_threshold", 60],
    ["chunk_producer_kickout_threshold", 60],
    ["validators", 0, "amount", "110000000000000000000000000000000"],
    [
        "records", 0, "Account", "account", "locked",
        "110000000000000000000000000000000"
    ], ["total_supply", "4060000000000000000000000000000000"]
]
nightly_genesis_change = [
    ["minimum_validators_per_shard", 2], ["min_gas_price", 0],
    ["max_inflation_rate", [0, 1]], ["epoch_length", 10],
    ["block_producer_kickout_threshold", 60],
    ["chunk_producer_kickout_threshold", 60],
    ["validators", 0, "amount", "110000000000000000000000000000000"],
    [
        "records", 0, "Account", "account", "locked",
        "110000000000000000000000000000000"
    ], ["total_supply", "4060000000000000000000000000000000"]
]

# give more stake to the bootnode so that it can produce the blocks alone
nodes = start_cluster(2, 1, 8, None,
                      nightly_genesis_change if nightly else genesis_change, {
                          0: {
                              "tracked_shards": [0]
                          },
                          1: {
                              "tracked_shards": [0]
                          }
                      })
time.sleep(3)
nodes[1].kill()

started = time.time()

act_to_val = [0, 0, 0]
ctx = utils.TxContext(act_to_val, nodes)

last_balances = [x for x in ctx.expected_balances]

sent_height = -1
caught_up_times = 0

for height, hash_ in utils.poll_blocks(nodes[0],
                                       timeout=TIMEOUT,
                                       poll_interval=0.1):
    logger.info(f'Got to height {height}')

    if ctx.get_balances() == ctx.expected_balances:
        logger.info('Balances caught up, took %s blocks, moving on',
                    height - sent_height)
        ctx.send_moar_txs(hash_, 10, use_routing=True)
        sent_height = height
        caught_up_times += 1
    else:
        assert height <= sent_height + 30, ('Balances before: {before}\n'
                                            'Expected balances: {expected}\n'
                                            'Current balances: {current}\n'
                                            'Sent at height: {height}').format(
                                                before=last_balances,
                                                expected=ctx.expected_balances,
                                                current=ctx.get_balances(),
                                                height=sent_height)

    if caught_up_times == 3:
        break

'''
'''--- pytest/tests/sanity/proxy_example.py ---
#!/usr/bin/env python3
# This test is an example about how to use the proxy features.
#
# Create two nodes and add a proxy between them.
# - Capture PeersRequest message from node 1 to node 0.
# - Let the message pass immediately so node 1 receives a PeersResponse
# - After 3 seconds send PeersRequest again so node 1 receives again a PeersResponse
import sys, time, asyncio
import multiprocessing
import functools
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import start_cluster
from configured_logger import logger
from peer import *
from proxy import ProxyHandler

from multiprocessing import Value

TIMEOUT = 30

class Handler(ProxyHandler):

    def __init__(self, *args, success=None, **kwargs):
        assert success is not None
        self.success = success
        super().__init__(*args, **kwargs)
        self.peers_response = 0

    async def handle(self, msg, fr, to):
        if msg.enum.startswith('Peers'):
            logger.info(f"{msg.enum} {fr} {to}")

        if to == 0 and msg.enum == 'PeersRequest':
            self.peers_request = msg
            loop = asyncio.get_running_loop()
            send = functools.partial(self.do_send_message, msg, 0)
            loop.call_later(3, send)

        if to == 1 and msg.enum == 'PeersResponse':
            self.peers_response += 1
            logger.info(f"Total PeersResponses = {self.peers_response}")
            if self.peers_response == 2:
                self.success.value = 1

        return True

if __name__ == '__main__':
    success = Value('i', 0)
    start_cluster(2, 0, 1, None, [], {},
                  functools.partial(Handler, success=success))

    started = time.time()

    while True:
        assert time.time() - started < TIMEOUT
        time.sleep(1)

        if success.value == 1:
            break

'''
'''--- pytest/tests/sanity/proxy_restart.py ---
#!/usr/bin/env python3
# Start two nodes. Proxify both nodes. Kill one of them, restart it
# and wait until block at height >= 20.
import sys, time
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import start_cluster
from configured_logger import logger
from peer import *
from proxy import ProxyHandler
import utils

TARGET_HEIGHT = 20

if __name__ == '__main__':
    nodes = start_cluster(2, 0, 1, None, [], {}, ProxyHandler)

    nodes[1].kill()
    nodes[1].start(boot_node=nodes[0])

    utils.wait_for_blocks(nodes[1], target=TARGET_HEIGHT)

'''
'''--- pytest/tests/sanity/proxy_simple.py ---
#!/usr/bin/env python3
# Start two nodes. Proxify both nodes
# and wait until block at height >= 10 pass through the proxy.
import sys, time
import multiprocessing
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import start_cluster
from configured_logger import logger
from functools import partial
from peer import *
from proxy import ProxyHandler

from multiprocessing import Value
from utils import obj_to_string

TIMEOUT = 30

class Handler(ProxyHandler):

    def __init__(self, *args, success=None, **kwargs):
        assert success is not None
        self.success = success
        super().__init__(*args, **kwargs)

    async def handle(self, msg, fr, to):
        if msg.enum == 'Block':
            h = msg.Block.header().inner_lite().height
            logger.info(f"Height: {h}")
            if h >= 10:
                logger.info('SUCCESS')
                self.success.value = 1
        return True

if __name__ == '__main__':
    success = Value('i', 0)

    start_cluster(2, 0, 1, None, [], {}, partial(Handler, success=success))

    started = time.time()

    while True:
        assert time.time() - started < TIMEOUT
        time.sleep(1)

        if success.value == 1:
            break

'''
'''--- pytest/tests/sanity/recompress_storage.py ---
#!/usr/bin/env python3
"""Tests whether node can continue working after its storage is recompressed.

The test starts a cluster of four nodes, submits a few transactions onto the
network.  The same transactions as the ones used in rpc_tx_status.py test.

Once that’s done, each node is stopped and its storage processed via the
‘recompress-storage’ command.  ‘view-state apply-range’ is then executed to
verify that the database has not been corrupted.

Finally, all the nodes are restarted and again a few transactions are sent to
verify that everything is working in order.

The above steps are done once for RPC nodes and again for archival nodes.
"""

import os
import pathlib
import subprocess
import sys
import threading
import time
import typing
import unittest

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from configured_logger import logger
import cluster
import key
import transaction

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'tests'))

import sanity.rpc_tx_status

class RecompressStorageTestCase(unittest.TestCase):

    def __init__(self, *args, **kw) -> None:
        super().__init__(*args, **kw)
        self.nodes = ()

    def tearDown(self) -> None:
        for node in self.nodes:
            node.cleanup()
        self.nodes = ()

    def do_test_recompress_storage(self, *, archive: bool) -> None:
        logger.info(f'start cluster')
        self.nodes = sanity.rpc_tx_status.start_cluster(archive=archive)

        # Give the network some time to generate a few blocks.  The same goes
        # for other sleeps in this method.
        time.sleep(5)

        # Execute a few transactions
        sanity.rpc_tx_status.test_tx_status(self.nodes)
        time.sleep(5)

        # Recompress storage on each node
        logger.info(f'Recompress storage on each node')
        for idx, node in enumerate(self.nodes):
            logger.info(f'Stopping node{idx}')
            node.kill(gentle=True)

            node_dir = pathlib.Path(node.node_dir)
            self._call(
                node,
                'recompress-storage-',
                'recompress-storage',
                '--output-dir=' + str(node_dir / 'data-new'),
            )
            (node_dir / 'data').rename(node_dir / 'data-old')
            (node_dir / 'data-new').rename(node_dir / 'data')

            self._call(
                node,
                'view-state-',
                'view-state',
                'apply-range',
                '--start-index=0',
                '--verbose-output',
                'sequential',
            )

        # Restart all nodes with the new database
        logger.info(f'Restart all nodes with the new database')
        for idx, node in enumerate(self.nodes):
            logger.info(f'Starting node{idx}')
            node.start(boot_node=self.nodes[0])

        # Execute a few more transactions
        time.sleep(5)
        sanity.rpc_tx_status.test_tx_status(self.nodes, nonce_offset=3)

    def _call(self, node: cluster.LocalNode, prefix: str, *args:
              typing.Union[str, pathlib.Path]) -> None:
        """Calls node’s neard with given arguments."""
        node_dir = pathlib.Path(node.node_dir)
        cmd = [
            pathlib.Path(node.near_root) / node.binary_name,
            f'--home={node_dir}',
        ] + list(args)
        logger.info('Running ' + ' '.join(str(arg) for arg in cmd))
        with open(node_dir / (prefix + 'stdout'), 'ab') as stdout, \
             open(node_dir / (prefix + 'stderr'), 'ab') as stderr:
            subprocess.check_call(cmd,
                                  stdin=subprocess.DEVNULL,
                                  stdout=stdout,
                                  stderr=stderr,
                                  env=dict(os.environ, RUST_LOG='debug'))

    def test_recompress_storage_rpc(self) -> None:
        self.do_test_recompress_storage(archive=False)

    def test_recompress_storage_archive(self) -> None:
        self.do_test_recompress_storage(archive=True)

if __name__ == '__main__':
    unittest.main()

'''
'''--- pytest/tests/sanity/repro_2916.py ---
#!/usr/bin/env python3
# Spins up two nodes with two shards, waits for couple blocks, snapshots the
# latest chunks, and requests both chunks from the first node, asking for
# receipts for both shards in both requests. We expect the first node have full
# responses for only one of these shards -- the shard it tracks (for the
# shard it doesn't track it will only have the receipts to the shard it does
# track).
#
# We then kill both nodes, and restart the first node, and do the same
# requests. We expect it to resond the same way. Before 2916 is fixed, it
# fails to respond to the request it was previously responding to due to
# incorrect reconstruction of the receipts.

import asyncio, sys, time
import base58
import nacl.signing
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import start_cluster
from configured_logger import logger
from peer import *
import utils

from messages.tx import *
from messages.block import *
from messages.crypto import *
from messages.network import *

async def main():
    # start a cluster with two shards
    nodes = start_cluster(2, 0, 2, None, [], {})

    height, hash_ = utils.wait_for_blocks(nodes[0], target=3)
    block = nodes[0].get_block(hash_)['result']
    chunk_hashes = [base58.b58decode(x['chunk_hash']) for x in block['chunks']]
    assert len(chunk_hashes) == 2
    assert all([len(x) == 32 for x in chunk_hashes])

    my_key_pair_nacl = nacl.signing.SigningKey.generate()
    tracking_shards_scenario = None  # will be either [0, 1] or [1, 0]; we'll detect

    # step = 0: before the node is killed
    # step = 1: after the node is killed
    for step in range(2):

        conn0 = await connect(nodes[0].addr())
        await run_handshake(conn0, nodes[0].node_key.pk, my_key_pair_nacl)
        for shard_ord, chunk_hash in enumerate(chunk_hashes):

            request = PartialEncodedChunkRequestMsg()
            request.chunk_hash = chunk_hash
            request.part_ords = []
            request.tracking_shards = [0, 1]

            routed_msg_body = RoutedMessageBody()
            routed_msg_body.enum = 'PartialEncodedChunkRequest'
            routed_msg_body.PartialEncodedChunkRequest = request

            peer_message = create_and_sign_routed_peer_message(
                routed_msg_body, nodes[0], my_key_pair_nacl)

            await conn0.send(peer_message)

            received_receipt_shards = set()

            def predicate(response):
                return response.enum == 'Routed' and response.Routed.body.enum == 'PartialEncodedChunkResponse'

            try:
                response = await asyncio.wait_for(conn0.recv(predicate), 5)
            except (concurrent.futures._base.TimeoutError,
                    asyncio.exceptions.TimeoutError):
                assert False, "A response is always expected for partial encoded chunk request."

            for receipt_proof in response.Routed.body.PartialEncodedChunkResponse.receipts:
                shard_proof = receipt_proof.f2
                assert shard_proof.from_shard_id == shard_ord, \
                    "Basic correctness check failed: the receipt for chunk of shard {} has the wrong from_shard_id {}".format(shard_ord, shard_proof.from_shard_id)
                received_receipt_shards.add(shard_proof.to_shard_id)

            if step == 0 and shard_ord == 0:
                # detect how the two validators decided who tracks which shard.
                if received_receipt_shards == set([1]):
                    # if the first validator only responded receipt to shard 1, then
                    # it's only tracking shard 1. (Otherwise it should respond with [0, 1]
                    # since it tracks all receipts coming from shard 0.)
                    tracking_shards_scenario = [1, 0]
                else:
                    tracking_shards_scenario = [0, 1]

            if tracking_shards_scenario == [0, 1]:
                if shard_ord == 0:
                    assert received_receipt_shards == set([0, 1]), \
                        "Request to node 0 (tracks shard 0), chunk 0, expected receipts to [0, 1], actual {}".format(received_receipt_shards)
                else:
                    assert received_receipt_shards == set([0]), \
                        "Request to node 0 (tracks shard 0), chunk 1, expected receipts to [0], actual {}".format(received_receipt_shards)
            else:  # [1, 0]
                if shard_ord == 0:
                    assert received_receipt_shards == set([1]), \
                        "Request to node 0 (tracks shard 1), chunk 0, expected receipts to [1], actual {}".format(received_receipt_shards)
                else:
                    assert received_receipt_shards == set([0, 1]), \
                        "Request to node 0 (tracks shard 1), chunk 1, expected receipts to [0, 1], actual {}".format(received_receipt_shards)

        if step == 0:
            logger.info("Killing and restarting nodes")
            nodes[1].kill()
            nodes[0].kill()
            nodes[0].start()
            time.sleep(1)

asyncio.run(main())

'''
'''--- pytest/tests/sanity/resharding.py ---
#!/usr/bin/env python3

# Small test for resharding. Spins up a few nodes from genesis with the previous
# shard layout, waits for a few epochs and verifies that the shard layout is
# upgraded.
# Usage:
# python3 pytest/tests/sanity/resharding.py
# RUST_LOG=info,resharding=debug,sync=debug,catchup=debug python3 pytest/tests/sanity/resharding.py

import pathlib
import sys

import unittest

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from configured_logger import logger
from cluster import init_cluster, spin_up_node
from utils import MetricsTracker, poll_blocks
from resharding_lib import ReshardingTestBase, get_genesis_config_changes, get_client_config_changes

class ReshardingTest(ReshardingTestBase):

    def setUp(self) -> None:
        super().setUp(epoch_length=20)

    def test_resharding(self):
        logger.info("The resharding test is starting.")
        num_nodes = 2

        genesis_config_changes = get_genesis_config_changes(
            self.epoch_length, self.binary_protocol_version, logger)
        client_config_changes = get_client_config_changes(num_nodes)

        near_root, [node0_dir, node1_dir] = init_cluster(
            num_nodes=num_nodes,
            num_observers=0,
            num_shards=1,
            config=self.config,
            genesis_config_changes=genesis_config_changes,
            client_config_changes=client_config_changes,
        )

        node0 = spin_up_node(
            self.config,
            near_root,
            node0_dir,
            0,
        )
        node1 = spin_up_node(
            self.config,
            near_root,
            node1_dir,
            1,
            boot_node=node0,
        )

        metrics_tracker = MetricsTracker(node0)

        for height, hash in poll_blocks(node0):
            version = self.get_version(metrics_tracker)
            num_shards = self.get_num_shards(metrics_tracker)

            protocol_config = node0.json_rpc(
                "EXPERIMENTAL_protocol_config",
                {"block_id": hash},
            )

            self.assertTrue('error' not in protocol_config)

            self.assertTrue('result' in protocol_config)
            protocol_config = protocol_config.get('result')

            self.assertTrue('shard_layout' in protocol_config)
            shard_layout = protocol_config.get('shard_layout')

            self.assertTrue('V1' in shard_layout)
            shard_layout = shard_layout.get('V1')

            self.assertTrue('boundary_accounts' in shard_layout)
            boundary_accounts = shard_layout.get('boundary_accounts')

            logger.info(
                f"#{height} shard layout version: {version}, num shards: {num_shards}"
            )
            logger.debug(f"#{height} shard layout: {shard_layout}")

            # check the shard layout versions from metrics and from json rpc are equal
            self.assertEqual(version, shard_layout.get('version'))
            # check the shard num from metrics and json rpc are equal
            self.assertEqual(num_shards, len(boundary_accounts) + 1)

            # This may be flaky - it shouldn't - but it may. We collect metrics
            # after the block is processed. If there is some delay the shard
            # layout may change and the assertions below will fail.

            # TODO(resharding) Why is epoch offset needed here?
            if height <= 2 * self.epoch_length + self.epoch_offset:
                self.assertEqual(version, self.genesis_shard_layout_version)
                self.assertEqual(num_shards, self.genesis_num_shards)
            else:
                self.assertEqual(version, self.target_shard_layout_version)
                self.assertEqual(num_shards, self.target_num_shards)

            if height >= 4 * self.epoch_length:
                break

        node0.kill()
        node1.kill()

        logger.info("The resharding test is finished.")

if __name__ == '__main__':
    unittest.main()

'''
'''--- pytest/tests/sanity/resharding_error_handling.py ---
#!/usr/bin/env python3

# Test for checking error handling during resharding. Spins up a few nodes from
# genesis with the previous shard layout. Stops the nodes in the middle of the
# epoch before resharding and corrupts the state snapshot. Resumes the nodes and
# verifies that the error is reported correctly.

# Usage:
# python3 pytest/tests/sanity/resharding_error_handling.py

import unittest
import sys
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from configured_logger import logger
from cluster import corrupt_state_snapshot, get_binary_protocol_version, init_cluster, load_config, spin_up_node
from utils import MetricsTracker, poll_blocks, wait_for_blocks
from resharding_lib import get_genesis_shard_layout_version, get_target_shard_layout_version, get_genesis_num_shards, get_target_num_shards, get_genesis_config_changes, get_client_config_changes

class ReshardingErrorHandlingTest(unittest.TestCase):

    def setUp(self) -> None:
        self.epoch_length = 10
        self.config = load_config()
        self.binary_protocol_version = get_binary_protocol_version(self.config)
        assert self.binary_protocol_version is not None

        self.genesis_shard_layout_version = get_genesis_shard_layout_version(
            self.binary_protocol_version)
        self.target_shard_layout_version = get_target_shard_layout_version(
            self.binary_protocol_version)

        self.genesis_num_shards = get_genesis_num_shards(
            self.binary_protocol_version)
        self.target_num_shards = get_target_num_shards(
            self.binary_protocol_version)

    # timeline by block number
    # epoch_length + 2 - snapshot is requested
    # epoch_length + 3 - snapshot is finished
    # epoch_length + 4 - stop the nodes, corrupt the snapshot, start nodes
    # epoch_length + 4 - resharding starts and fails
    # epoch_length * 2 + 1 - last block while node is still healthy before chain
    # upgrades to the new shard layout
    def test_resharding(self):
        logger.info("The resharding test is starting.")
        num_nodes = 2

        genesis_config_changes = get_genesis_config_changes(
            self.epoch_length, self.binary_protocol_version, logger)
        client_config_changes = get_client_config_changes(num_nodes, 10)

        near_root, [node0_dir, node1_dir] = init_cluster(
            num_nodes=num_nodes,
            num_observers=0,
            num_shards=1,
            config=self.config,
            genesis_config_changes=genesis_config_changes,
            client_config_changes=client_config_changes,
        )

        node0 = spin_up_node(
            self.config,
            near_root,
            node0_dir,
            0,
        )
        node1 = spin_up_node(
            self.config,
            near_root,
            node1_dir,
            1,
            boot_node=node0,
        )

        logger.info("wait until the snapshot is ready")
        wait_for_blocks(node0, target=self.epoch_length + 4)
        wait_for_blocks(node1, target=self.epoch_length + 4)

        logger.info("the snapshot should be ready, stopping nodes")
        node0.kill(gentle=True)
        node1.kill(gentle=True)

        logger.info("corrupting the state snapshot of node0")
        output = corrupt_state_snapshot(
            self.config,
            node0_dir,
            self.genesis_shard_layout_version,
        )
        logger.info(f"corrupted state snapshot\n{output}")

        # Update the initial delay to start resharding as soon as possible.
        client_config_changes = get_client_config_changes(1, 0)[0]
        node0.change_config(client_config_changes)
        node1.change_config(client_config_changes)

        logger.info("restarting nodes")
        node0.start()
        node1.start(boot_node=node0)

        all_failed_observed = False

        metrics = MetricsTracker(node0)
        for height, _ in poll_blocks(node0):
            status = metrics.get_metric_all_values("near_resharding_status")
            logger.info(f"#{height} resharding status {status}")

            if len(status) > 0:
                all_failed = all([s == -1.0 for (_, s) in status])
                all_failed_observed = all_failed_observed or all_failed

            # The node should be able to survive until the end of the epoch even
            # though resharding is broken. Only break after the last block of epoch.
            if height >= self.epoch_length * 2:
                break

        node0.kill(gentle=True)
        node1.kill(gentle=True)

        # Resharding should fail for all shards.
        self.assertTrue(all_failed_observed)

        logger.info("The resharding error handling test is finished.")

if __name__ == '__main__':
    unittest.main()

'''
'''--- pytest/tests/sanity/resharding_restart.py ---
#!/usr/bin/env python3
"""
Testing that resharding is correctly resumed after a node restart. The goal is
to make sure that if the node is restarted after resharding is finished that it
picks up where it left and doesn't try to start again.

Note that this is different than what happens if node is restared in the middle
of resharding. In this case resharding would get restarted from the beginning.

Usage:

python3 pytest/tests/sanity/resharding_restart.py

"""

from time import sleep
import unittest
import sys
import pathlib
import base58

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from configured_logger import logger
from cluster import BaseNode, init_cluster, spin_up_node
from utils import MetricsTracker, poll_blocks
from resharding_lib import ReshardingTestBase, get_genesis_config_changes, get_client_config_changes
from transaction import sign_payment_tx

class ReshardingTest(ReshardingTestBase):

    def setUp(self) -> None:
        # Note - this test has a really long epoch because it takes the nodes
        # forever to take the flat state snapshot. Ideally this should be
        # fixed and the test shortened.
        super().setUp(epoch_length=20)

    def test_resharding_restart(self):
        logger.info("The resharding restart test is starting.")
        num_nodes = 2

        genesis_config_changes = get_genesis_config_changes(
            self.epoch_length, self.binary_protocol_version, logger)
        client_config_changes = get_client_config_changes(num_nodes)

        near_root, [node0_dir, node1_dir] = init_cluster(
            num_nodes=num_nodes,
            num_observers=0,
            num_shards=1,
            config=self.config,
            genesis_config_changes=genesis_config_changes,
            client_config_changes=client_config_changes,
        )

        node0 = spin_up_node(
            self.config,
            near_root,
            node0_dir,
            0,
        )
        node1 = spin_up_node(
            self.config,
            near_root,
            node1_dir,
            1,
            boot_node=node0,
        )

        # The target height is the height where the new shard layout should
        # first be used.
        target_height = 2 * self.epoch_length + self.epoch_offset
        metrics_tracker = MetricsTracker(node0)

        for height, hash in poll_blocks(node0):
            version = self.get_version(metrics_tracker)
            num_shards = self.get_num_shards(metrics_tracker)
            resharding_status = self.get_resharding_status(metrics_tracker)
            flat_storage_head = self.get_flat_storage_head(metrics_tracker)

            logger.info(
                f"#{height} shard layout version: {version}, num shards: {num_shards}"
            )
            logger.info(
                f"${height} resharding status: {self.__pretty_metric_list(resharding_status)}"
            )
            logger.info(
                f"#{height} flat storage head: {self.__pretty_metric_list(flat_storage_head)}"
            )

            self.assertEqual(version, self.genesis_shard_layout_version)
            self.assertEqual(num_shards, self.genesis_num_shards)

            # - before resharding is finished flat storage should only exist for
            #   the old shards
            # - after resharding is finished flat storage should exists for both
            #   old and new shards.
            # Asserting that condition exactly is prone to race conditions. Here
            # we only check it briefly but after the restart we check it properly.
            self.assertGreaterEqual(len(flat_storage_head), num_shards)

            self.assertLess(height, 4 * self.epoch_length)

            # Resharding must finish before the end of the epoch, otherwise this
            # test wouldn't check anything.
            self.assertLess(height, target_height)

            # Send some transactions so that
            # a) the state gets changed
            # b) the flat storage gets updated
            self.__send_tx(node0, hash, height)

            if self.__is_resharding_finished(resharding_status):
                break

        # Wait a little bit to make sure that resharding is correctly
        # postprocessed.
        sleep(1)

        # Resharding is finished, restart the node now and check that resharding
        # was resumed and not restarted.
        logger.info("resharding finished")

        node0.kill()
        node0.start()

        check_count = 0
        for height, hash in poll_blocks(node0):
            if height > 4 * self.epoch_length:
                break

            version = self.get_version(metrics_tracker)
            num_shards = self.get_num_shards(metrics_tracker)
            resharding_status = self.get_resharding_status(metrics_tracker)

            flat_storage_head = self.get_flat_storage_head(metrics_tracker)

            # The node needs a short while to set the metrics.
            if version is None or num_shards is None or resharding_status is None:
                continue

            logger.info(
                f"#{height} shard layout version: {version}, num shards: {num_shards}"
            )
            logger.info(
                f"${height} resharding status: {self.__pretty_metric_list(resharding_status)}"
            )
            logger.info(
                f"#{height} flat storage head: {self.__pretty_metric_list(flat_storage_head)}"
            )

            # GOOD If resharding is correctly resumed the status should remain empty.
            # BAD  If resharding is restarted the status would be non-empty.
            # BAD  If resharding is neither restarted nor resumed the node would get
            # stuck at target_height - 1. This is checked below.
            self.assertEqual(resharding_status, [])

            # After the restart, which happens after the resharding is
            # finished, the flat storage should exist for both old and new
            # shards. Technically once we move to the new epoch the flat storage
            # for old shards is not needed anymore but we don't exactly clean up
            # the metrics so it kinda hangs around until the end of the test.
            expected_len = self.genesis_num_shards + self.target_num_shards
            self.assertEqual(len(flat_storage_head), expected_len)

            if height <= target_height:
                self.assertEqual(version, self.genesis_shard_layout_version)
                self.assertEqual(num_shards, self.genesis_num_shards)

            else:
                self.assertEqual(version, self.target_shard_layout_version)
                self.assertEqual(num_shards, self.target_num_shards)

            check_count += 1

            # Send some transactions so that
            # a) the state gets changed
            # b) the flat storage gets updated
            self.__send_tx(node0, hash, height)

        # Make sure that we actually checked something after the restart.
        self.assertGreater(check_count, 0)

        node0.kill()
        node1.kill()

        logger.info("The resharding restart test is finished.")

    def __pretty_metric_list(self, metric_list):
        result = dict()
        for metric in metric_list:
            (shard_uid, value) = metric
            shard_uid = shard_uid.get('shard_uid')
            result[shard_uid] = value

        return result

    def __is_resharding_finished(self, all_shards_status):
        if len(all_shards_status) == 0:
            logger.debug("is resharding finished: no shards")
            return False

        all_finished = True
        for shard_status in all_shards_status:
            (shard_uid, status) = shard_status
            if int(status) == 2:
                logger.debug(f"is resharding finished: {shard_uid} ready")
            else:
                logger.debug(f"is resharding finished: {shard_uid} not ready")
                all_finished = False

        return all_finished

    def __send_tx(self, node: BaseNode, block_hash: str, nonce: int):
        tx = sign_payment_tx(
            node.signer_key,
            'test1',
            1,
            nonce,
            base58.b58decode(block_hash.encode('utf8')),
        )
        result = node.send_tx(tx)
        self.assertTrue('result' in result)
        self.assertTrue('error' not in result)

if __name__ == '__main__':
    unittest.main()

'''
'''--- pytest/tests/sanity/resharding_rpc_tx.py ---
#!/usr/bin/env python3
"""
Testing RPC call to transaction status after a resharding event.
We create two account that we know would fall into different shards after resharding.
We submit a transfer transaction between the accounts and verify the transaction status after resharding.
"""

import sys
import unittest
import pathlib
import copy

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from configured_logger import logger
import cluster
from resharding_lib import ReshardingTestBase, get_genesis_config_changes, get_client_config_changes
import transaction
from utils import MetricsTracker, poll_blocks, wait_for_blocks
import key

STARTING_AMOUNT = 123 * (10**24)

class ReshardingRpcTx(ReshardingTestBase):

    def setUp(self) -> None:
        # The epoch needs to be quite long because we submit transactions and
        # wait for the response which takes some blocks. We need to make sure
        # that transactions are fully processed before resharding so that the
        # test checks the right things.
        super().setUp(epoch_length=20)

    def __setup_account(self, account_id, nonce):
        """ Create an account with full access key and balance. """
        encoded_block_hash = self.node.get_latest_block().hash_bytes
        account = key.Key.from_random(account_id)
        account_tx = transaction.sign_create_account_with_full_access_key_and_balance_tx(
            self.node.signer_key, account.account_id, account, STARTING_AMOUNT,
            nonce, encoded_block_hash)
        response = self.node.send_tx_and_wait(account_tx, timeout=20)
        assert 'error' not in response, response
        assert 'Failure' not in response['result']['status'], response
        return account

    def __submit_transfer_tx(self, from_key, to_account_id, nonce):
        logger.info(f"submit transfer tx from {from_key} to {to_account_id}")
        """ Submit a transfer transaction and wait for the response. """
        encoded_block_hash = self.node.get_latest_block().hash_bytes
        payment_tx = transaction.sign_payment_tx(from_key, to_account_id, 100,
                                                 nonce, encoded_block_hash)
        response = self.node.send_tx_and_wait(payment_tx, timeout=20)
        assert 'error' not in response, response
        assert 'Failure' not in response['result']['status'], response
        return response

    def __verify_tx_status(self, transfer_response, sender_account_id):
        tx_hash = transfer_response['result']['transaction']['hash']
        response = self.node.get_tx(tx_hash, sender_account_id)

        self.assertTrue(
            transfer_response['result']['final_execution_status']
            in ['EXECUTED_OPTIMISTIC', 'EXECUTED', 'FINAL'],)
        self.assertTrue(
            response['result']['final_execution_status']
            in ['EXECUTED_OPTIMISTIC', 'EXECUTED', 'FINAL'],)

        transfer_response = copy.deepcopy(transfer_response)
        transfer_response['result']['final_execution_status'] = "IGNORE_ME"
        response['result']['final_execution_status'] = "IGNORE_ME"

        assert response == transfer_response, response
        pass

    def test_resharding_rpc_tx(self):
        num_nodes = 2
        genesis_config_changes = get_genesis_config_changes(
            self.epoch_length, self.binary_protocol_version, logger)
        client_config_changes = get_client_config_changes(num_nodes)
        nodes = cluster.start_cluster(
            num_nodes=num_nodes,
            num_observers=0,
            num_shards=1,
            config=self.config,
            genesis_config_changes=genesis_config_changes,
            client_config_changes=client_config_changes,
        )
        self.node = nodes[0]

        # The shard boundaries are at "kkuuue2akv_1630967379.near" and "tge-lockup.sweat" for shard 3 and 4
        # We would like to create accounts that are in different shards
        # The first account before and after resharding is in shard 3
        # The second account after resharding is in shard 4
        account0 = self.__setup_account('setup_test_account.test0', 1)
        account1 = self.__setup_account('z_setup_test_account.test0', 2)

        logger.info("wait for one block to create the accounts")
        # Poll one block to create the accounts
        wait_for_blocks(self.node, count=1)

        # Submit a transfer transaction between the accounts, we would verify the transaction status later
        response0 = self.__submit_transfer_tx(
            account0,
            account1.account_id,
            6000001,
        )
        response1 = self.__submit_transfer_tx(
            account1,
            account0.account_id,
            12000001,
        )

        metrics_tracker = MetricsTracker(self.node)
        for height, _ in poll_blocks(self.node):
            # wait for resharding to complete
            if height <= 2 * self.epoch_length + self.epoch_offset:
                continue

            # Quick check whether resharding is completed
            version = self.get_version(metrics_tracker)
            num_shards = self.get_num_shards(metrics_tracker)
            self.assertEqual(version, self.target_shard_layout_version)
            self.assertEqual(num_shards, self.target_num_shards)

            # Verify the transaction status after resharding
            self.__verify_tx_status(response0, account0.account_id)
            self.__verify_tx_status(response0, account1.account_id)
            self.__verify_tx_status(response1, account0.account_id)
            self.__verify_tx_status(response1, account1.account_id)
            break

if __name__ == '__main__':
    unittest.main()

'''
'''--- pytest/tests/sanity/restart.py ---
#!/usr/bin/env python3
# Spins up two nodes, and waits until they produce 20 blocks.
# Kills the nodes, restarts them, makes sure they produce 20 more blocks
# Sets epoch length to 10

import sys, time
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import start_cluster
from configured_logger import logger

TIMEOUT = 150
BLOCKS1 = 20
BLOCKS2 = 40

# The epoch size is 10 and the validators will miss a few blocks during restarts, so the kickout threshold has to be adjusted.
# Otherwise validators will get kicked out and the blockchain will grind to a halt. The threshold is set to 0 to avoid any such problems.
# On production network this isn't a problem because the epoch size is a few orders of magnitude larger.
nodes = start_cluster(
    2, 0, 2, None,
    [["epoch_length", 10], ["block_producer_kickout_threshold", 0]], {})

started = time.time()

max_height = 0
last_heights = [0 for _ in nodes]
seen_heights = [set() for _ in nodes]
last_common = [[0 for _ in nodes] for _ in nodes]

height_to_hash = {}

def min_common():
    return min([min(x) for x in last_common])

def heights_report():
    for i, sh in enumerate(seen_heights):
        logger.info("Node %s: %s" % (i, sorted(list(sh))))

first_round = True
while max_height < BLOCKS1:
    assert time.time() - started < TIMEOUT
    for i, node in enumerate(nodes):
        height, hash_ = node.get_latest_block()

        if height > max_height:
            max_height = height
            if height % 10 == 0:
                logger.info("Reached height %s, min common: %s" %
                            (height, min_common()))

        if height not in height_to_hash:
            height_to_hash[height] = hash_
        else:
            assert height_to_hash[
                height] == hash_, "height: %s, h1: %s, h2: %s" % (
                    height, hash_, height_to_hash[height])

        last_heights[i] = height
        seen_heights[i].add(height)
        for j, _ in enumerate(nodes):
            if height in seen_heights[j]:
                last_common[i][j] = height
                last_common[j][i] = height
        if not first_round:
            # Don't check it in the first round - min_common will be 0, as we didn't
            # read the status from all nodes.
            assert min_common() + 2 >= height, heights_report()

    first_round = False

assert min_common() + 2 >= BLOCKS1, heights_report()

for node in nodes:
    node.kill()

nodes[0].start()
nodes[1].start(boot_node=nodes[0])

first_round = True
while max_height < BLOCKS2:
    assert time.time() - started < TIMEOUT
    for i, node in enumerate(nodes):
        height, hash_ = node.get_latest_block()

        if height > max_height:
            max_height = height
            if height % 10 == 0:
                logger.info("Reached height %s, min common: %s" %
                            (height, min_common()))

        if height not in height_to_hash:
            height_to_hash[height] = hash_
        else:
            assert height_to_hash[
                height] == hash_, "height: %s, h1: %s, h2: %s" % (
                    height, hash_, height_to_hash[height])

        last_heights[i] = height
        seen_heights[i].add(height)
        for j, _ in enumerate(nodes):
            if height in seen_heights[j]:
                last_common[i][j] = height
                last_common[j][i] = height

        if not first_round:
            # Don't check it in the first round - min_common will be 0, as we didn't
            # read the status from all nodes.
            assert min_common() + 2 >= height, heights_report()

assert min_common() + 2 >= BLOCKS2, heights_report()

'''
'''--- pytest/tests/sanity/rosetta.py ---
#!/usr/bin/env python3

import dataclasses
import json
import os
import pathlib
import sys
import time
import typing
import unittest

import base64
import base58
import ed25519
import requests
import requests.exceptions

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from utils import load_binary_file
from configured_logger import logger
import cluster
import key
import mocknet_helpers
import transaction

JsonDict = typing.Dict[str, typing.Any]
BlockIdentifier = typing.Union[str, int, JsonDict]
TxIdentifier = typing.Union[str, JsonDict]

def account_identifier(account_id: str) -> JsonDict:
    return {'address': account_id}

def block_identifier(block_id: BlockIdentifier) -> JsonDict:
    if isinstance(block_id, int):
        return {'index': block_id}
    if isinstance(block_id, str):
        return {'hash': block_id}
    if isinstance(block_id, dict):
        return block_id
    raise TypeError(f'{type(block_id).__name__} is not a block identifier')

def tx_identifier(tx_id: TxIdentifier) -> JsonDict:
    if isinstance(tx_id, str):
        return {'hash': tx_id}
    if isinstance(tx_id, dict):
        return tx_id
    raise TypeError(f'{type(tx_id).__name__} is not a transaction identifier')

class RosettaExecResult:
    identifier: typing.Dict[str, str]
    _rpc: 'RosettaRPC'
    _block_id: BlockIdentifier
    __block: typing.Optional[JsonDict] = None
    __transaction: typing.Optional[JsonDict] = None

    def __init__(self, rpc: 'RosettaRPC', block_id: BlockIdentifier,
                 identifier: typing.Dict[str, str]) -> None:
        self._rpc = rpc
        self._block_id = block_id
        self.identifier = identifier

    @property
    def hash(self) -> str:
        """The Rosetta hash of the transaction identifier.

        This will be a value in ‘<prefix>:<base58-hash>’ format as used in
        Rosetta RPC where prefix is either ‘tx’ for transactions or ‘receipt’
        for receipts.
        """
        return self.identifier['hash']

    @property
    def near_hash(self) -> str:
        """A NEAR transaction hash in base85.

        Compared to `hash` it’s just the `<base58-hash>’ part of the Rosetta
        identifier which is the NEAR transaction or receipt hash (depending on
        what comes before the colon).  This can be used to query NEAR through
        JSON RPC.
        """
        return self.identifier['hash'].split(':')[1]

    def block(self) -> JsonDict:
        """Returns the block in which the transaction was executed.

        When this method or `transaction` method is called the first time, it
        queries the node to find the node which includes the transaction.  The
        return value is memoised so subsequent calls won’t do the querying.

        Returns:
            A Rosetta RPC block data object.
        """
        return self.__get_transaction()[0]

    def transaction(self) -> JsonDict:
        """Returns the transaction details from Rosetta RPC.

        When this method or `block` method is called the first time, it queries
        the node to find the node which includes the transaction.  The return
        value is memoised so subsequent calls won’t do the querying.

        Returns:
            A Rosetta RPC transaction data object.
        """
        return self.__get_transaction()[1]

    def related(self, num: int) -> typing.Optional[JsonDict]:
        """Returns related transaction or None if there aren’t that many.

        The method uses `transaction` method so all comments regarding fetching
        the data from node apply to it as well.

        Returns:
            If the transaction has at least `num+1` related transactions returns
            a new `RosettaExecResult` object which can be used to fetch the
            transaction.  Otherwise, returns None.
        """
        block, transaction = self.__get_transaction()
        related = transaction.get('related_transactions', ())
        if len(related) <= num:
            return None
        return type(self)(self._rpc, block['block_identifier'],
                          related[num]['transaction_identifier'])

    def __get_transaction(self) -> typing.Tuple[JsonDict, JsonDict]:
        """Fetches transaction and its block from the node if not yet retrieved.

        Returns:
            (block, transaction) tuple where first element is Rosetta Block
            object and second Rosetta Transaction object.
        """
        if self.__block and self.__transaction:
            return self.__block, self.__transaction
        timeout = time.monotonic() + 10
        while time.monotonic() < timeout:
            while True:
                try:
                    block = self._rpc.get_block(block_id=self._block_id)
                except requests.exceptions.HTTPError as ex:
                    res = ex.response
                    try:
                        if res.status_code == 500 and res.json()['code'] == 404:
                            break
                    except:
                        pass
                    raise
                if not block:
                    break
                for tx in block['transactions']:
                    if tx['transaction_identifier']['hash'] == self.hash:
                        related = ', '.join(
                            related['transaction_identifier']['hash']
                            for related in tx.get('related_transactions',
                                                  ())) or 'none'
                        logger.info(f'Receipts of {self.hash}: {related}')
                        self.__memoised = (block, tx)
                        return self.__memoised
                self._block_id = int(block['block_identifier']['index']) + 1
            time.sleep(0.25)
        assert False, f'Transaction {self.hash} did not complete in 10 seconds'

class RosettaRPC:
    node: cluster.BaseNode
    href: str
    network_identifier: JsonDict

    def __init__(self,
                 *,
                 node: cluster.BaseNode,
                 host: str = '127.0.0.1',
                 port: int = 5040) -> None:
        self.node = node
        self.href = f'http://{host}:{port}'
        self.network_identifier = self.get_network_identifier()

    def get_network_identifier(self):
        result = requests.post(f'{self.href}/network/list',
                               headers={'content-type': 'application/json'},
                               data=json.dumps({'metadata': {}}))
        result.raise_for_status()
        return result.json()['network_identifiers'][0]

    def rpc(self, path: str, **data: typing.Any) -> JsonDict:
        data['network_identifier'] = self.network_identifier
        result = requests.post(f'{self.href}{path}',
                               headers={'content-type': 'application/json'},
                               data=json.dumps(data, indent=True))
        result.raise_for_status()
        return result.json()

    def exec_operations(self, signer: key.Key,
                        *operations) -> RosettaExecResult:
        """Sends given operations to Construction API.

        Args:
            signer: Account signing the operations.
            operations: List of operations to perform.
        Returns:
            A RosettaExecResult object which can be used to get hash of the
            submitted transaction or wait on the transaction completion.
        """
        height = self.node.get_latest_block().height

        public_key = {
            'hex_bytes': signer.decoded_pk().hex(),
            'curve_type': 'edwards25519'
        }
        options = self.rpc('/construction/preprocess',
                           operations=operations)['options']
        metadata = self.rpc('/construction/metadata',
                            options=options,
                            public_keys=[public_key])['metadata']
        payloads = self.rpc('/construction/payloads',
                            operations=operations,
                            public_keys=[public_key],
                            metadata=metadata)
        payload = payloads['payloads'][0]
        unsigned = payloads['unsigned_transaction']
        signature = signer.sign_bytes(bytearray.fromhex(payload['hex_bytes']))
        signed = self.rpc('/construction/combine',
                          unsigned_transaction=unsigned,
                          signatures=[{
                              'signing_payload': payload,
                              'hex_bytes': signature.hex(),
                              'signature_type': 'ed25519',
                              'public_key': public_key
                          }])['signed_transaction']
        tx = self.rpc('/construction/submit', signed_transaction=signed)
        tx_hash = tx['transaction_identifier']['hash']
        logger.info(f'Transaction hash: {tx_hash}')
        return RosettaExecResult(self, height, tx['transaction_identifier'])

    def transfer(self, *, src: key.Key, dst: key.Key, amount: int,
                 **kw) -> RosettaExecResult:
        currency = {'symbol': 'NEAR', 'decimals': 24}
        return self.exec_operations(
            src, {
                'operation_identifier': {
                    'index': 0
                },
                'type': 'TRANSFER',
                'account': {
                    'address': src.account_id
                },
                'amount': {
                    'value': str(-amount),
                    'currency': currency
                },
            }, {
                'operation_identifier': {
                    'index': 1
                },
                'related_operations': [{
                    'index': 0
                }],
                'type': 'TRANSFER',
                'account': {
                    'address': dst.account_id
                },
                'amount': {
                    'value': str(amount),
                    'currency': currency
                },
            }, **kw)

    def ft_transfer(self, *, src: key.Key, dst: key.Key, amount: int,
                    **kw) -> RosettaExecResult:
        currency = {'symbol': 'NEAR', 'decimals': 24}
        return self.exec_operations(
            src, {
                'operation_identifier': {
                    'index': 0
                },
                'type': 'INITIATE_FUNCTION_CALL',
                'account': {
                    'address': src.account_id
                }
            }, {
                'operation_identifier': {
                    'index': 0
                },
                'type': 'FUNCTION_CALL',
                'account': {
                    'address': src.account_id
                },
                'metadata': {
                    'method_name':
                        'ft_transfer',
                    'args':
                        bytes(
                            json.dumps({
                                'receiver_id': dst.account_id,
                                'amount': 1
                            }), 'utf-8').hex(),
                    'attached_gas':
                        "300000000000000",
                }
            }, **kw)

    def add_full_access_key(self, account: key.Key, public_key_hex: str,
                            **kw) -> RosettaExecResult:
        return self.exec_operations(
            account, {
                "operation_identifier": {
                    "index": 0
                },
                "type": "INITIATE_ADD_KEY",
                "account": {
                    "address": account.account_id
                }
            }, {
                "operation_identifier": {
                    "index": 1
                },
                "related_operations": [{
                    "index": 0
                }],
                "type": "ADD_KEY",
                "account": {
                    "address": account.account_id
                },
                "metadata": {
                    "public_key": {
                        "hex_bytes": public_key_hex,
                        "curve_type": "edwards25519"
                    }
                }
            }, **kw)

    def delete_account(self, account: key.Key, refund_to: key.Key,
                       **kw) -> RosettaExecResult:
        return self.exec_operations(
            account, {
                'operation_identifier': {
                    'index': 0
                },
                'type': 'INITIATE_DELETE_ACCOUNT',
                'account': {
                    'address': account.account_id
                },
            }, {
                'operation_identifier': {
                    'index': 0
                },
                'type': 'DELETE_ACCOUNT',
                'account': {
                    'address': account.account_id
                },
            }, {
                'operation_identifier': {
                    'index': 0
                },
                'type': 'REFUND_DELETE_ACCOUNT',
                'account': {
                    'address': refund_to.account_id
                },
            }, **kw)

    def get_block(self, *, block_id: BlockIdentifier) -> JsonDict:

        def fetch(block_id: BlockIdentifier) -> JsonDict:
            block_id = block_identifier(block_id)
            block = self.rpc('/block', block_identifier=block_id)['block']

            # Order of transactions on the list is not guaranteed so normalise
            # it by sorting by hash.
            # TODO(mina86): Verify that this is still true.
            block.get(
                'transactions',
                []).sort(key=lambda tx: tx['transaction_identifier']['hash'])

            return block

        block = fetch(block_id)

        # Verify that fetching block by index and hash produce the same result
        # as well as getting individual transactions produce the same object as
        # the one returned when fetching transactions individually.
        assert block == fetch(block['block_identifier']['index'])
        assert block == fetch(block['block_identifier']['hash'])
        for tx in block['transactions']:
            assert tx == self.get_transaction(
                block_id=block_id, tx_id=tx['transaction_identifier'])

        return block

    def get_transaction(self, *, block_id: BlockIdentifier,
                        tx_id: TxIdentifier) -> JsonDict:
        res = self.rpc('/block/transaction',
                       block_identifier=block_identifier(block_id),
                       transaction_identifier=tx_identifier(tx_id))
        return res['transaction']

    def get_account_balances(self, *, account_id: str) -> JsonDict:
        res = self.rpc('/account/balance',
                       account_identifier=account_identifier(account_id))
        return res['balances']

class RosettaTestCase(unittest.TestCase):
    node = None
    rosetta = None

    def __init__(self, *args, **kw) -> None:
        super().__init__(*args, **kw)
        self.maxDiff = None

    @classmethod
    def setUpClass(cls) -> None:
        cls.node = cluster.start_cluster(1, 0, 1, {}, {}, {
            0: {
                'rosetta_rpc': {
                    'addr': '0.0.0.0:5040',
                    'cors_allowed_origins': ['*']
                },
            }
        })[0]
        cls.rosetta = RosettaRPC(node=cls.node, host=cls.node.rpc_addr()[0])

    @classmethod
    def tearDownClass(cls) -> None:
        cls.node.cleanup()

    def test_zero_balance_account(self) -> None:
        """Tests storage staking requirements for low-storage accounts.

        Creates an implicit account by sending it 1 yoctoNEAR (not enough to
        cover storage). However, the zero-balance allowance established in
        NEP-448 should cover the storage staking requirement. Then, we
        transfer 10**22 yoctoNEAR to the account, which should be enough to
        cover the storage staking requirement for the 6 full-access keys we
        then add to the account, exceeding the zero-balance account allowance.
        """

        test_amount = 10**22
        key_space_cost = 41964925000000000000
        validator = self.node.validator_key
        implicit = key.Key.implicit_account()

        # first transfer 1 yoctoNEAR to create the account
        # not enough to cover storage, but the zero-balance allowance should cover it
        result = self.rosetta.transfer(src=validator, dst=implicit, amount=1)

        block = result.block()
        tx = result.transaction()
        json_res = self.node.get_tx(result.near_hash, implicit.account_id)
        json_res = json_res['result']
        receipt_ids = json_res['transaction_outcome']['outcome']['receipt_ids']
        receipt_id = {'hash': 'receipt:' + receipt_ids[0]}

        # Fetch the receipt through Rosetta RPC.
        result = RosettaExecResult(self.rosetta, block, receipt_id)
        related = result.related(0)

        balances = self.rosetta.get_account_balances(
            account_id=implicit.account_id)

        # even though 1 yoctoNEAR is not enough to cover the storage cost,
        # since the account should be consuming less than 770 bytes of storage,
        # it should be allowed nonetheless.
        self.assertEqual(balances, [{
            'value': '1',
            'currency': {
                'symbol': 'NEAR',
                'decimals': 24
            }
        }])

        # transfer the rest of the amount
        result = self.rosetta.transfer(src=validator,
                                       dst=implicit,
                                       amount=(test_amount - 1))

        block = result.block()
        tx = result.transaction()
        json_res = self.node.get_tx(result.near_hash, implicit.account_id)
        json_res = json_res['result']
        receipt_ids = json_res['transaction_outcome']['outcome']['receipt_ids']
        receipt_id = {'hash': 'receipt:' + receipt_ids[0]}

        # Fetch the receipt through Rosetta RPC.
        result = RosettaExecResult(self.rosetta, block, receipt_id)
        related = result.related(0)

        balances = self.rosetta.get_account_balances(
            account_id=implicit.account_id)

        self.assertEqual(balances, [{
            'value': str(test_amount),
            'currency': {
                'symbol': 'NEAR',
                'decimals': 24
            }
        }])

        # add 6 keys to go over the zero-balance account free storage allowance
        public_keys_hex = [
            "17595386a67d36afc73872e60916f83217d789dc60b5d037563998e6651111cf",
            "7940aac79a425f194621ab5c4e38b7841dddae90b20eaf28f7f78caec911bcf4",
            "0554fffef36614d7c49b3088c4c1fb66613ff05fb30927b582b43aed0b25b549",
            "09d36e25c5a3ac440a798252982dd92b67d8de60894df3177cb4ff30a890cafd",
            "e0ca119be7211f3dfed1768fc9ab235b6af06a205077ef23166dd1cbfd2ac7fc",
            "98f1a49296fb7156980d325a25e1bfeb4f123dd98c90fa0492699c55387f7ef3",
        ]
        for pk in public_keys_hex:
            result = self.rosetta.add_full_access_key(implicit, pk)

            block = result.block()
            tx = result.transaction()
            json_res = self.node.get_tx(result.near_hash, implicit.account_id)
            json_res = json_res['result']
            receipt_ids = json_res['transaction_outcome']['outcome'][
                'receipt_ids']
            receipt_id = {'hash': 'receipt:' + receipt_ids[0]}

            # Fetch the receipt through Rosetta RPC.
            result = RosettaExecResult(self.rosetta, block, receipt_id)
            related = result.related(0)

        balances = self.rosetta.get_account_balances(
            account_id=implicit.account_id)

        # no longer a zero-balance account
        self.assertEqual(test_amount - key_space_cost * len(public_keys_hex),
                         int(balances[0]['value']))

    def test_get_block(self) -> None:
        """Tests getting blocks and transactions.

        Fetches the first and second blocks to see if the responses look as they
        should.  Then fetches one transaction from each of those blocks to again
        see if the returned data looks as expected.  Since the exact hashes
        differ each time the test runs, those are assumed to be correct.
        """

        block_0 = self.rosetta.get_block(block_id=0)
        block_0_id = block_0['block_identifier']
        trans_0_id = 'block:' + block_0_id['hash']
        trans_0 = {
            'metadata': {
                'type': 'BLOCK'
            },
            'operations': [{
                'account': {
                    'address': 'near'
                },
                'amount': {
                    'currency': {
                        'decimals': 24,
                        'symbol': 'NEAR'
                    },
                    'value': '1000000000000000000000000000000000'
                },
                'operation_identifier': {
                    'index': 0
                },
                'status': 'SUCCESS',
                'type': 'TRANSFER'
            }, {
                'account': {
                    'address': 'test0'
                },
                'amount': {
                    'currency': {
                        'decimals': 24,
                        'symbol': 'NEAR'
                    },
                    'value': '950000000000000000000000000000000'
                },
                'operation_identifier': {
                    'index': 1
                },
                'status': 'SUCCESS',
                'type': 'TRANSFER'
            }, {
                'account': {
                    'address': 'test0',
                    'sub_account': {
                        'address': 'LOCKED'
                    }
                },
                'amount': {
                    'currency': {
                        'decimals': 24,
                        'symbol': 'NEAR'
                    },
                    'value': '50000000000000000000000000000000'
                },
                'operation_identifier': {
                    'index': 2
                },
                'status': 'SUCCESS',
                'type': 'TRANSFER'
            }],
            'transaction_identifier': {
                'hash': trans_0_id
            }
        }
        self.assertEqual(
            {
                'block_identifier': block_0_id,
                # Genesis block’s parent is genesis block itself.
                'parent_block_identifier': block_0_id,
                'timestamp': block_0['timestamp'],
                'transactions': [trans_0]
            },
            block_0)

        # Getting by hash should work and should return the exact same thing
        block = self.rosetta.get_block(block_id=block_0_id['hash'])
        self.assertEqual(block_0, block)

        # Get transaction from genesis block.
        tr = self.rosetta.get_transaction(block_id=block_0_id, tx_id=trans_0_id)
        self.assertEqual(trans_0, tr)

        # Block at height=1 should have genesis block as parent and only
        # validator update as a single operation.
        block_1 = self.rosetta.get_block(block_id=1)
        block_1_id = block_1['block_identifier']
        trans_1_id = {'hash': 'block-validators-update:' + block_1_id['hash']}
        trans_1 = {
            'metadata': {
                'type': 'TRANSACTION'
            },
            'operations': [],
            'transaction_identifier': trans_1_id,
        }
        self.assertEqual(
            {
                'block_identifier': {
                    'hash': block_1_id['hash'],
                    'index': 1
                },
                'parent_block_identifier':
                    block_0_id,
                'timestamp':
                    block_1['timestamp'],
                'transactions': [{
                    'metadata': {
                        'type': 'TRANSACTION'
                    },
                    'operations': [],
                    'transaction_identifier': trans_1_id
                }]
            }, block_1)

        # Get transaction from the second block
        self.assertEqual(
            trans_1,
            self.rosetta.get_transaction(block_id=block_1_id, tx_id=trans_1_id))

    def test_fungible_token_transfer(self) -> None:
        """Tests sending fungible token transfer.

        First sends some funds from validator’s account to an implicit account,
        then checks how the transaction looks through Data API and finally
        deletes that account refunding the validator account.
        """
        test_amount = 1
        validator = self.node.validator_key
        implicit = key.Key.implicit_account()
        contract_key = self.node.validator_key
        contract = load_binary_file(
            '../../../runtime/near-test-contracts/res/fungible_token.wasm')

        ### 1. Deploy the ft smart contract
        latest_block_hash = self.node.get_latest_block().hash
        deploy_contract_tx = transaction.sign_deploy_contract_tx(
            contract_key, contract, 10,
            base58.b58decode(latest_block_hash.encode('utf8')))
        deploy_contract_response = (self.node.send_tx_and_wait(
            deploy_contract_tx, 100))
        logger.info(f'Deploying contract: {deploy_contract_response}')

        ### 2. Create implicit account.
        logger.info(f'Creating implicit account: {implicit.account_id}')
        result = self.rosetta.transfer(src=validator,
                                       dst=implicit,
                                       amount=test_amount)
        ### 3. Storage deposit
        account_id_arg = f'"account_id" : "{implicit.account_id}"'
        data = json.dumps({'account_id': implicit.account_id})
        logger.info(f'Account id of implicit: {data}')
        s = f'{{"account_id": "{implicit.account_id}"}}'

        block_hash = self.node.get_latest_block().hash_bytes
        tx = transaction.sign_function_call_tx(contract_key,
                                               contract_key.account_id,
                                               'storage_deposit',
                                               bytes(s,
                                                     'utf-8'), 300000000000000,
                                               1250000000000000000000, 20,
                                               block_hash)
        res = self.node.send_tx_and_wait(tx, 100)
        logger.info(f'Storage deposit: {res}')
        mocknet_helpers.wait_at_least_one_block()

        ### 4. Rosetta ft_call
        ft_result = self.rosetta.ft_transfer(src=validator,
                                             dst=implicit,
                                             amount=test_amount)
        # logger.info(f'Ft_transfer result: {ft_result}')
        # tx = ft_result.transaction()
        # logger.info(f'Ft_transfer result: {tx}')
        json_res = self.node.get_tx(ft_result.near_hash, implicit.account_id)
        logger.info(f'Tx-result: {json_res}')

    def test_get_block_nonexistent(self) -> None:
        """Tests querying non-existent blocks and transactions.

        Queries for various blocks and transactions which do not exist on the
        chain to see if responses are what they should be.
        """
        block_0 = self.rosetta.get_block(block_id=0)
        block_0_id = block_0['block_identifier']
        trans_0_id = 'block:' + block_0_id['hash']

        block_1 = self.rosetta.get_block(block_id=1)
        block_1_id = block_1['block_identifier']
        trans_1_id = 'block:' + block_1_id['hash']

        def test(want_code, callback, *args, **kw) -> None:
            with self.assertRaises(requests.exceptions.HTTPError) as err:
                callback(*args, **kw)
            self.assertEqual(500, err.exception.response.status_code)
            resp = err.exception.response.json()
            self.assertFalse(resp['retriable'])
            self.assertEqual(want_code, resp['code'])

        # Query for non-existent blocks
        test(404, self.rosetta.get_block, block_id=123456789)
        test(400, self.rosetta.get_block, block_id=-123456789)
        bogus_hash = 'GJ92SsB76CvfaHHdaC4Vsio6xSHT7fR3EEUoK84tFe99'
        test(404, self.rosetta.get_block, block_id=bogus_hash)
        test(400, self.rosetta.get_block, block_id='malformed-hash')

        # Query for non-existent transactions
        test(404,
             self.rosetta.get_transaction,
             block_id=block_0_id,
             tx_id=trans_1_id)
        test(404,
             self.rosetta.get_transaction,
             block_id=block_1_id,
             tx_id=trans_0_id)

    def _get_account_balance(self,
                             account: key.Key,
                             require: bool = True) -> typing.Optional[int]:
        """Returns balance of given account or None if account doesn’t exist.

        Args:
            account: Account to get balance of.
            require: If True, require that the account exists.
        """
        account_id = account.account_id
        result = self.node.get_account(account_id, do_assert=False)
        error = result.get('error')
        if error is None:
            amount = int(result['result']['amount'])
            logger.info(f'Account {account_id} balance: {amount} yocto')
            return amount
        self.assertEqual('UNKNOWN_ACCOUNT', error['cause']['name'],
                         f'Error fetching account {account_id}: {error}')
        if require:
            self.fail(f'Account {account.account_id} does not exist')
        return None

    def test_implicit_account(self) -> None:
        """Tests creating and deleting implicit account

        First sends some funds from validator’s account to an implicit account,
        then checks how the transaction looks through Data API and finally
        deletes that account refunding the validator account.
        """
        test_amount = 10**22
        validator = self.node.validator_key
        implicit = key.Key.implicit_account()

        # 1. Create implicit account.
        logger.info(f'Creating implicit account: {implicit.account_id}')
        result = self.rosetta.transfer(src=validator,
                                       dst=implicit,
                                       amount=test_amount)
        # Get the transaction through Rosetta RPC.
        block = result.block()
        tx = result.transaction()
        # Also get it from JSON RPC to compare receipt ids.
        json_res = self.node.get_tx(result.near_hash, implicit.account_id)
        json_res = json_res['result']
        receipt_ids = json_res['transaction_outcome']['outcome']['receipt_ids']
        self.assertEqual(1, len(receipt_ids))
        receipt_id = {'hash': 'receipt:' + receipt_ids[0]}

        # There are two operations. The first subtracts `test_amount` from the account and the second one subtracts
        # gas fees
        value = -int(tx['operations'][0]['amount']['value'])
        logger.info(f'Took {value} from validator account')
        self.assertEqual(test_amount, value)
        gas_payment = -int(tx['operations'][1]['amount']['value'])
        self.assertGreater(gas_payment, 0)

        self.assertEqual([{
            'metadata': {
                'type': 'TRANSACTION'
            },
            'operations': [{
                'account': {
                    'address': 'test0'
                },
                'amount': {
                    'currency': {
                        'decimals': 24,
                        'symbol': 'NEAR'
                    },
                    'value': str(-value)
                },
                'operation_identifier': {
                    'index': 0
                },
                'status': 'SUCCESS',
                'type': 'TRANSFER',
                'metadata': {
                    'predecessor_id': {
                        'address': 'test0'
                    }
                }
            }, {
                'account': {
                    'address': 'test0'
                },
                'amount': {
                    'currency': {
                        'decimals': 24,
                        'symbol': 'NEAR'
                    },
                    'value': str(-gas_payment)
                },
                'operation_identifier': {
                    'index': 1
                },
                'status': 'SUCCESS',
                'type': 'TRANSFER',
                'metadata': {
                    'predecessor_id': {
                        'address': 'test0'
                    },
                    'transfer_fee_type': 'GAS_PREPAYMENT'
                }
            }],
            'related_transactions': [{
                'direction': 'forward',
                'transaction_identifier': receipt_id,
            }],
            'transaction_identifier': result.identifier,
        }], block['transactions'])

        # Fetch the receipt through Rosetta RPC.
        result = RosettaExecResult(self.rosetta, block, receipt_id)
        related = result.related(0)
        self.assertEqual(
            {
                'transaction_identifier': result.identifier,
                'operations': [{
                    'operation_identifier': {
                        'index': 0
                    },
                    'type': 'TRANSFER',
                    'status': 'SUCCESS',
                    'metadata': {
                        'predecessor_id': {
                            'address': 'test0'
                        }
                    },
                    'account': {
                        'address': implicit.account_id,
                    },
                    'amount': {
                        'value': '10000000000000000000000',
                        'currency': {
                            'symbol': 'NEAR',
                            'decimals': 24
                        }
                    }
                }],
                'related_transactions': [{
                    'direction': 'forward',
                    'transaction_identifier': related and related.identifier
                }],
                'metadata': {
                    'type': 'TRANSACTION'
                }
            }, result.transaction())

        # Fetch the next receipt through Rosetta RPC.
        self.assertEqual(
            {
                'metadata': {
                    'type': 'TRANSACTION'
                },
                'operations': [{
                    'account': {
                        'address': 'test0'
                    },
                    'amount': {
                        'currency': {
                            'decimals': 24,
                            'symbol': 'NEAR'
                        },
                        'value': '12524843062500000000'
                    },
                    'operation_identifier': {
                        'index': 0
                    },
                    'status': 'SUCCESS',
                    'type': 'TRANSFER',
                    'metadata': {
                        'predecessor_id': {
                            'address': 'system'
                        },
                        'transfer_fee_type': 'GAS_REFUND'
                    }
                }],
                'transaction_identifier': related.identifier
            }, related.transaction())

        # 2. Delete the account.
        logger.info(f'Deleting implicit account: {implicit.account_id}')
        result = self.rosetta.delete_account(implicit, refund_to=validator)

        self.assertEqual(
            test_amount, -sum(
                int(op['amount']['value'])
                for tx in result.block()['transactions']
                for op in tx['operations']))

        json_res = self.node.get_tx(result.near_hash, implicit.account_id)
        json_res = json_res['result']
        receipt_ids = json_res['transaction_outcome']['outcome']['receipt_ids']
        self.assertEqual(1, len(receipt_ids))
        receipt_id = {'hash': 'receipt:' + receipt_ids[0]}

        receipt_ids = json_res['receipts_outcome'][0]['outcome']['receipt_ids']
        self.assertEqual(1, len(receipt_ids))
        receipt_id_2 = {'hash': 'receipt:' + receipt_ids[0]}

        self.assertEqual(
            {
                'metadata': {
                    'type': 'TRANSACTION'
                },
                'operations': [{
                    'account': {
                        'address': implicit.account_id,
                    },
                    'amount': {
                        'currency': {
                            'decimals': 24,
                            'symbol': 'NEAR'
                        },
                        'value': '-51109700000000000000'
                    },
                    'operation_identifier': {
                        'index': 0
                    },
                    'status': 'SUCCESS',
                    'type': 'TRANSFER',
                    'metadata': {
                        'predecessor_id': {
                            'address': implicit.account_id
                        }
                    }
                }],
                'related_transactions': [{
                    'direction': 'forward',
                    'transaction_identifier': receipt_id,
                }],
                'transaction_identifier': result.identifier
            }, result.transaction())

        # Fetch the receipt
        result = RosettaExecResult(self.rosetta, block, receipt_id)
        self.assertEqual(
            {
                'metadata': {
                    'type': 'TRANSACTION'
                },
                'operations': [{
                    'account': {
                        'address': implicit.account_id,
                    },
                    'amount': {
                        'currency': {
                            'decimals': 24,
                            'symbol': 'NEAR'
                        },
                        'value': '-9948890300000000000000'
                    },
                    'operation_identifier': {
                        'index': 0
                    },
                    'status': 'SUCCESS',
                    'type': 'TRANSFER'
                }],
                'related_transactions': [{
                    'direction': 'forward',
                    'transaction_identifier': receipt_id_2
                }],
                'transaction_identifier': receipt_id
            }, result.transaction())

        # Fetch receipt’s receipt
        result = RosettaExecResult(self.rosetta, block, receipt_id_2)
        self.assertEqual(
            {
                'metadata': {
                    'type': 'TRANSACTION'
                },
                'operations': [{
                    'account': {
                        'address': 'test0'
                    },
                    'amount': {
                        'currency': {
                            'decimals': 24,
                            'symbol': 'NEAR'
                        },
                        'value': '9948890300000000000000'
                    },
                    'operation_identifier': {
                        'index': 0
                    },
                    'status': 'SUCCESS',
                    'type': 'TRANSFER',
                    'metadata': {
                        'predecessor_id': {
                            'address': "system"
                        },
                        'transfer_fee_type': 'GAS_REFUND'
                    }
                }],
                'transaction_identifier': receipt_id_2
            }, result.transaction())

if __name__ == '__main__':
    unittest.main()

'''
'''--- pytest/tests/sanity/rpc_finality.py ---
#!/usr/bin/env python3
# The test does the token transfer between the accounts, and tries to
# stop the network just in the right moment (so that the block with the refund receipt
# is not finalized).
# This way, we can verify that our json RPC returns correct values for different finality requests.

import sys
import pathlib

import unittest
from typing import List

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))
import utils
from cluster import start_cluster, LocalNode
from configured_logger import logger
from transaction import sign_payment_tx

class TestRpcFinality(unittest.TestCase):

    def test_finality(self):
        # set higher block delay to make test more reliable.
        min_block_delay = 3

        consensus = {
            "min_block_production_delay": {
                "secs": min_block_delay,
                "nanos": 0,
            },
            "max_block_production_delay": {
                "secs": min_block_delay * 2,
                "nanos": 0,
            },
            "max_block_wait_delay": {
                "secs": min_block_delay * 3,
                "nanos": 0,
            }
        }

        config = {node_id: {"consensus": consensus} for node_id in range(3)}

        nodes: List[LocalNode] = start_cluster(3, 0, 1, None, [
            ["min_gas_price", 0],
            ["epoch_length", 100],
        ], config)

        utils.wait_for_blocks(nodes[0], target=3)

        balances = {
            account: int(nodes[0].get_account('test0')['result']['amount'])
            for account in ['test0', 'test1']
        }

        token_transfer = 10
        latest_block_hash = nodes[0].get_latest_block().hash_bytes
        tx = sign_payment_tx(nodes[0].signer_key, 'test1', token_transfer, 1,
                             latest_block_hash)
        logger.info("About to send payment")
        # this transaction will be added to the block (probably around block 5)
        # and the receipts & transfers will happen in the next block (block 6).
        # This function should return as soon as block 6 arrives in node0.
        logger.info(nodes[0].send_tx_and_wait(tx, timeout=10))
        logger.info("Done")

        # kill one validating node so that block cannot be finalized.
        nodes[2].kill()

        print(
            f"Block height is {nodes[0].get_latest_block().height} (should be 6)"
        )

        # So now the situation is following:
        # Block 6 (head) - has the final receipt (that adds state to test1)
        # Block 5 (doomslug) - has the transaction (so this is the moment when state is removed from test0)
        # Block 4 (final) - has no information about the transaction.

        # So with optimistic finality: test0 = -10, test1 = +10
        # with doomslug (state as of block 5): test0 = -10, test1 = 0
        # with final (state as of block 4): test0 = 0, test1 = 0

        for acc_id in ['test0', 'test1']:
            amounts = [
                int(nodes[0].get_account(acc_id, finality)['result']['amount'])
                - balances[acc_id]
                for finality in ["optimistic", "near-final", "final"]
            ]
            print(f"Account amounts: {acc_id}: {amounts}")

            if acc_id == 'test0':
                self.assertEqual([-10, -10, 0], amounts)
            else:
                self.assertEqual([10, 0, 0], amounts)

if __name__ == '__main__':
    unittest.main()

'''
'''--- pytest/tests/sanity/rpc_hash.py ---
#!/usr/bin/env python3
# Test computing block hash from data provided by JSON RPCs.

import base58
import hashlib
import pathlib
import sys
import typing
import unittest

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from configured_logger import logger
import cluster
import utils
import serializer

import messages
import messages.crypto
import messages.block

config = cluster.load_config()
binary_protocol_version = cluster.get_binary_protocol_version(config)
logger.info(f"binary_protocol_version: {binary_protocol_version}")
assert binary_protocol_version is not None
BLOCK_HEADER_V4_PROTOCOL_VERSION = 63

def serialize(msg: typing.Any) -> bytes:
    return serializer.BinarySerializer(messages.schema).serialize(msg)

def compute_block_hash(header: typing.Dict[str, typing.Any],
                       msg_version: int) -> str:
    """Computes block hash based on given block header.

    Args:
        header: JSON representation of the block header.
        msg_version: Version of the BlockHeaderInnerRest to use when serialising
            and computing hash.  This depends on protocol version used when the
            block was generated.
    Returns:
        Base58-encoded block hash.
    """

    def get_int(key: str) -> typing.Optional[int]:
        value = header.get(key)
        if value is None:
            return None
        return int(value)

    def get_hash(key: str) -> typing.Optional[bytes]:
        value = header.get(key)
        if value is None:
            return value
        result = base58.b58decode(value)
        assert len(result) == 32, (key, value, len(result))
        return result

    def sha256(*chunks: bytes) -> bytes:
        return hashlib.sha256(b''.join(chunks)).digest()

    prev_hash = get_hash('prev_hash')

    inner_lite = messages.block.BlockHeaderInnerLite()
    inner_lite.height = get_int('height')
    inner_lite.epoch_id = get_hash('epoch_id')
    inner_lite.next_epoch_id = get_hash('next_epoch_id')
    inner_lite.prev_state_root = get_hash('prev_state_root')
    inner_lite.outcome_root = get_hash('outcome_root')
    inner_lite.timestamp = get_int('timestamp_nanosec')
    inner_lite.next_bp_hash = get_hash('next_bp_hash')
    inner_lite.block_merkle_root = get_hash('block_merkle_root')
    inner_lite_blob = serialize(inner_lite)
    inner_lite_hash = sha256(inner_lite_blob)

    inner_rest_msg = {
        1: messages.block.BlockHeaderInnerRest,
        2: messages.block.BlockHeaderInnerRestV2,
        3: messages.block.BlockHeaderInnerRestV3,
        4: messages.block.BlockHeaderInnerRestV4,
    }[msg_version]

    inner_rest = inner_rest_msg()
    # Some of the fields are superfluous in some of the versions of the message
    # but that’s quite all right.  Serialiser will ignore them and
    # unconditionally setting them makes the code simpler.
    inner_rest.block_body_hash = get_hash('block_body_hash')
    inner_rest.chunk_receipts_root = get_hash('chunk_receipts_root')
    inner_rest.chunk_headers_root = get_hash('chunk_headers_root')
    inner_rest.chunk_tx_root = get_hash('chunk_tx_root')
    inner_rest.chunks_included = get_int('chunks_included')
    inner_rest.challenges_root = get_hash('challenges_root')
    inner_rest.random_value = get_hash('random_value')
    # TODO: Handle non-empty list.
    inner_rest.validator_proposals = header['validator_proposals']
    inner_rest.chunk_mask = bytes(header['chunk_mask'])
    inner_rest.gas_price = get_int('gas_price')
    inner_rest.total_supply = get_int('total_supply')
    # TODO: Handle non-empty list.
    inner_rest.challenges_result = header['challenges_result']
    inner_rest.last_final_block = get_hash('last_final_block')
    inner_rest.last_ds_final_block = get_hash('last_ds_final_block')
    inner_rest.block_ordinal = get_int('block_ordinal')
    inner_rest.prev_height = get_int('prev_height')
    inner_rest.epoch_sync_data_hash = get_hash('epoch_sync_data_hash')
    inner_rest.approvals = [
        approval and messages.crypto.Signature(approval)
        for approval in header['approvals']
    ]
    inner_rest.latest_protocol_version = get_int('latest_protocol_version')
    inner_rest_blob = serialize(inner_rest)
    inner_rest_hash = sha256(inner_rest_blob)

    inner_hash = sha256(inner_lite_hash + inner_rest_hash)
    block_hash = sha256(inner_hash + prev_hash)

    return base58.b58encode(block_hash).decode('ascii')

class HashTestCase(unittest.TestCase):

    def __init__(self, *args, **kw) -> None:
        super().__init__(*args, **kw)
        self.maxDiff = None

    def test_compute_block_hash(self):
        """Tests that compute_block_hash function works correctly.

        This is a sanity check for cases when someone modifies the test code
        itself.  If this test breaks than all the other tests will break as
        well.  This test runs the compute_block_hash function on a well-known
        static input checking expected hash.
        """
        header = {
            'height':
                4,
            'prev_height':
                3,
            'epoch_id':
                '11111111111111111111111111111111',
            'next_epoch_id':
                'AuatKw3hiGmXed3uT2u4Die6ZRGZhEHn34kTyVpGnYLM',
            'prev_hash':
                'BUcVEkMq3DcZzDGgeh1sb7FFuD86XYcXpEt25Cf34LuP',
            'prev_state_root':
                'Bn786g4GdJJigSP4qRSaVCfeMotWVX88cV1LTZhD6o3z',
            'block_body_hash':
                '4K3NiGuqYGqKPnYp6XeGd2kdN4P9veL6rYcWkLKWXZCu',  # some arbitrary string
            'chunk_receipts_root':
                '9ETNjrt6MkwTgSVMMbpukfxRshSD1avBUUa4R4NuqwHv',
            'chunk_headers_root':
                'Fk7jeakmi8eruvv4L4ToKs7MV1YG64ETZtASQYjGBWK1',
            'chunk_tx_root':
                '7tkzFg8RHBmMw1ncRJZCCZAizgq4rwCftTKYLce8RU8t',
            'outcome_root':
                '7tkzFg8RHBmMw1ncRJZCCZAizgq4rwCftTKYLce8RU8t',
            'chunks_included':
                1,
            'challenges_root':
                '11111111111111111111111111111111',
            'timestamp':
                1642022757141096960,
            'timestamp_nanosec':
                '1642022757141096960',
            'random_value':
                'GxYrjCxQtfG2K7hX6w4aPs3usTskzfCkbVc2icSQMF7h',
            'validator_proposals': [],
            'chunk_mask': [True],
            'gas_price':
                '1000000000',
            'block_ordinal':
                4,
            'rent_paid':
                '0',
            'validator_reward':
                '0',
            'total_supply':
                '3000000000000000000000000000000000',
            'challenges_result': [],
            'last_final_block':
                'GTudmqKJQjEVCrdi31vcHqXoEpvEScmZ9BhBf3gPJ4pp',
            'last_ds_final_block':
                'BUcVEkMq3DcZzDGgeh1sb7FFuD86XYcXpEt25Cf34LuP',
            'next_bp_hash':
                '236RGxQc2xSqukyiBkixtZSqKu679ZxeS6vP8zzAL9vW',
            'block_merkle_root':
                'Gf3uWgULzc5WDuaAq4feehh7M1TFRFxTWVv2xH6AsnpA',
            'epoch_sync_data_hash':
                '4JTQn5LGcxdx4xstsAXgXHcP3oHKatzdzHBw6atBDSWV',
            'approvals': [
                'ed25519:5Jdeg8rk5hAbcooyxXQSTcxBgUK39Z8Qtfkhqmpi26biU26md5wBiFvkAEGXrMyn3sgq3cTMG8Lr3HD7RxWPjkPh',
                'ed25519:4vqTaN6bucu6ALsb1m15e8HWGGxLQeKJhWrcU8zPRrzfkZbakaSzW8rfas2ZG89rFKheZUyrnZRKooRny6YKFyKi'
            ],
            'signature':
                'ed25519:5mGi9dyuyt7TnSpPFjbEWSJThDdiEV9NNQB11knXvRbxSv8XfBT5tdVVFypeqpZjeB3fD7qgJpWhTj3KvdGbcXdu',
            'latest_protocol_version':
                50
        }

        for msg_ver, block_hash in (
            (1, '3ckGjcedZiN3RnvfiuEN83BtudDTVa9Pub4yZ8R737qt'),
            (2, 'Hezx56VTH815G6JTzWqJ7iuWxdR9X4ZqGwteaDF8q2z'),
            (3, 'Finjr87adnUqpFHVXbmAWiVAY12EA9G4DfUw27XYHox'),
            (4, '2QfdGyGWByEeL2ZSy8u2LoBa4pdDwf5KoDrr94W6oeB6'),
        ):
            self.assertEqual(block_hash, compute_block_hash(header, msg_ver))

        # Now try with a different block body hash
        header[
            'block_body_hash'] = '4rMxTeTF9LehPbzB2xhVa4xWVtbyjRfvL7qsxc8sL7WP'
        for msg_ver, block_hash in (
            (1, '3ckGjcedZiN3RnvfiuEN83BtudDTVa9Pub4yZ8R737qt'),
            (2, 'Hezx56VTH815G6JTzWqJ7iuWxdR9X4ZqGwteaDF8q2z'),
            (3, 'Finjr87adnUqpFHVXbmAWiVAY12EA9G4DfUw27XYHox'),
            (4, '3Cdm4sS9b4jdypMezP8ta6p2ecyRSJC9uaGUJTY18MUH'),
        ):
            self.assertEqual(block_hash, compute_block_hash(header, msg_ver))

        # Now try witohut epoch_sync_data_hash
        header['epoch_sync_data_hash'] = None
        for msg_ver, block_hash in (
            (1, '3ckGjcedZiN3RnvfiuEN83BtudDTVa9Pub4yZ8R737qt'),
            (2, 'Hezx56VTH815G6JTzWqJ7iuWxdR9X4ZqGwteaDF8q2z'),
            (3, '82v8RAc66tWpdjRCsoSrgnzpU6JMhpjbWKmUEcfkzX6T'),
            (4, '9BYhkbWkKTLJj46goq5WPEzUJDf5juHJnBu2jjoHL7yc'),
        ):
            self.assertEqual(block_hash, compute_block_hash(header, msg_ver))

        # Now try with one approval missing
        header['approvals'][1] = None
        for msg_ver, block_hash in (
            (1, 'EE2JtxdqWLBDKqNARxdFDqH36mEJ1xJ6LjQ9f7qCSDRE'),
            (2, '2WdpJD5dYPjEMn3EYbm1BhGgCAX7ksxJGTQm4xHazBxt'),
            (3, '3bx6vfbH8GrYp8UFMagiBgYyKMH63D7Qo5J7jCsNbh9o'),
            (4, 'CTDBpUpCdhdCjCMfaFD5r96PyKDK756aXw69toLYEaSH'),
        ):
            self.assertEqual(block_hash, compute_block_hash(header, msg_ver))

    def _test_block_hash(self,
                         msg_version: int,
                         protocol_version: typing.Optional[int] = None) -> None:
        """Starts a cluster, fetches blocks and computes their hashes.

        The cluster is started with genesis configured to use given protocol
        version.  The code fetches blocks until: 1) a block with all approvals
        set is encountered, 2) another block with at least one approval missing
        and 3) at least ten blocks total are checked.

        Args:
            msg_version: Version of the BlockHeaderInnerRest to use when
                serialising and computing hash.
            protocol_version: If given, protocol version to use in the cluster
                (which will be set in genesis); If not given, cluster will be
                started with the newest supported protocol version.
        """
        genesis_overrides = []
        if protocol_version:
            genesis_overrides = [['protocol_version', protocol_version]]

        nodes = ()
        try:
            nodes = cluster.start_cluster(4, 0, 4, None, genesis_overrides, {})
            got_all_set = False
            got_some_unset = False
            count = 0
            for block_id in utils.poll_blocks(nodes[0]):
                header = nodes[0].get_block(block_id.hash)['result']['header']
                self.assertEqual((block_id.height, block_id.hash),
                                 (header['height'], header['hash']),
                                 (block_id, header))
                got = compute_block_hash(header, msg_version)
                self.assertEqual(header['hash'], got, header)

                if all(header['approvals']):
                    if not got_all_set:
                        nodes[1].kill()
                        got_all_set = True
                elif any(approval is None for approval in header['approvals']):
                    got_some_unset = True

                count += 1
                if got_all_set and got_some_unset and count >= 10:
                    break
        finally:
            for node in nodes:
                node.cleanup()

    def test_block_hash_v1(self):
        """Starts a cluster using protocol version 24 and verifies block hashes.

        The cluster is started with a protocol version in which the first
        version of the BlockHeaderInnerRest has been used.
        """
        self._test_block_hash(1, 24)

    def test_block_hash_v2(self):
        """Starts a cluster using protocol version 42 and verifies block hashes.

        The cluster is started with a protocol version in which the second
        version of the BlockHeaderInnerRest has been used.
        """
        self._test_block_hash(2, 42)

    def test_block_hash_v3(self):
        """Starts a cluster using protocol version 50 and verifies block hashes.

        The cluster is started with a protocol version in which the third
        version of the BlockHeaderInnerRest has been used.
        """
        self._test_block_hash(3, 50)

    if binary_protocol_version >= BLOCK_HEADER_V4_PROTOCOL_VERSION:

        def test_block_hash_v4(self):
            """Starts a cluster using protocol version BLOCK_HEADER_V4_PROTOCOL_VERSION and verifies block hashes.

            The cluster is started with a protocol version in which the fourth
            version of the BlockHeaderInnerRest has been used.
            """
            self._test_block_hash(4, BLOCK_HEADER_V4_PROTOCOL_VERSION)

    def test_block_hash_latest(self):
        """Starts a cluster using latest protocol and verifies block hashes.

        The cluster is started with the newest protocol version supported by the
        node.  If this test fails while others pass this may indicate that a new
        BlockHeaderInnerRest message has been introduced and this test needs to
        be updated to support it.
        """
        if binary_protocol_version >= BLOCK_HEADER_V4_PROTOCOL_VERSION:
            self._test_block_hash(4)
        else:
            self._test_block_hash(3)

if __name__ == '__main__':
    unittest.main()

'''
'''--- pytest/tests/sanity/rpc_light_client_execution_outcome_proof.py ---
#!/usr/bin/env python3
# Spins up two nodes, deploy a smart contract to one node,
# Send a transaction to call a contract method. Check that
# the transaction and receipts execution outcome proof for
# light client works

import base58, base64
import hashlib
import json
import struct
import sys
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))
from cluster import start_cluster, Key
from serializer import BinarySerializer
import transaction
import time
import utils
from lightclient import compute_block_hash

class PartialExecutionOutcome:
    pass

class PartialExecutionStatus:
    pass

class Unknown:
    pass

class Failure:
    pass

partial_execution_outcome_schema = dict([
    [
        PartialExecutionOutcome,
        {
            'kind':
                'struct',
            'fields': [
                ['receipt_ids', [[32]]],
                ['gas_burnt', 'u64'],
                ['tokens_burnt', 'u128'],
                ['executor_id', 'string'],
                ['status', PartialExecutionStatus],
            ]
        },
    ],
    [
        PartialExecutionStatus, {
            'kind':
                'enum',
            'field':
                'enum',
            'values': [
                ['unknown', Unknown],
                ['failure', Failure],
                ['successValue', ['u8']],
                ['successReceiptId', [32]],
            ]
        }
    ],
    [Unknown, {
        'kind': 'struct',
        'fields': []
    }],
    [Failure, {
        'kind': 'struct',
        'fields': []
    }],
])

def serialize_execution_outcome_with_id(outcome, id):
    partial_outcome = PartialExecutionOutcome()
    partial_outcome.receipt_ids = [
        base58.b58decode(x) for x in outcome['receipt_ids']
    ]
    partial_outcome.gas_burnt = outcome['gas_burnt']
    partial_outcome.tokens_burnt = int(outcome['tokens_burnt'])
    partial_outcome.executor_id = outcome['executor_id']
    execution_status = PartialExecutionStatus()
    if 'SuccessValue' in outcome['status']:
        execution_status.enum = 'successValue'
        execution_status.successValue = base64.b64decode(
            outcome['status']['SuccessValue'])
    elif 'SuccessReceiptId' in outcome['status']:
        execution_status.enum = 'successReceiptId'
        execution_status.successReceiptId = base58.b58decode(
            outcome['status']['SuccessReceiptId'])
    elif 'Failure' in outcome['status']:
        execution_status.enum = 'failure'
        execution_status.failure = Failure()
    elif 'Unknown' in outcome['status']:
        execution_status.enum = 'unknown'
        execution_status.unknown = Unknown
    else:
        assert False, f'status not supported: {outcome["status"]}'
    partial_outcome.status = execution_status
    msg = BinarySerializer(partial_execution_outcome_schema).serialize(
        partial_outcome)
    partial_outcome_hash = hashlib.sha256(msg).digest()
    outcome_hashes = [partial_outcome_hash]
    for log_entry in outcome['logs']:
        outcome_hashes.append(
            hashlib.sha256(bytes(log_entry, 'utf-8')).digest())
    res = [base58.b58decode(id)]
    res.extend(outcome_hashes)
    borsh_res = bytearray()
    length = len(res)
    for i in range(4):
        borsh_res.append(length & 255)
        length //= 256
    for hash_result in res:
        borsh_res += bytearray(hash_result)
    return borsh_res

def check_transaction_outcome_proof(nodes, should_succeed, nonce):
    latest_block_hash = nodes[1].get_latest_block().hash_bytes
    function_caller_key = nodes[0].signer_key
    gas = 300000000000000 if should_succeed else 1000

    function_call_1_tx = transaction.sign_function_call_tx(
        function_caller_key, nodes[0].signer_key.account_id, 'write_key_value',
        struct.pack('<QQ', 42, 10), gas, 100000000000, nonce, latest_block_hash)
    function_call_result = nodes[1].send_tx_and_wait(function_call_1_tx, 15)
    assert 'error' not in function_call_result

    latest_block_height = nodes[0].get_latest_block().height

    # wait for finalization
    light_client_request_block_hash = None
    for cur_height, hash_ in utils.poll_blocks(nodes[0]):
        if (cur_height > latest_block_height + 2 and
                light_client_request_block_hash is None):
            light_client_request_block_hash = hash_
        if cur_height > latest_block_height + 7:
            break

    light_client_block = nodes[0].json_rpc(
        'next_light_client_block', [light_client_request_block_hash])['result']
    light_client_block_hash = compute_block_hash(
        light_client_block['inner_lite'], light_client_block['inner_rest_hash'],
        light_client_block['prev_block_hash']).decode('utf-8')

    queries = [{
        "type":
            "transaction",
        "transaction_hash":
            function_call_result['result']['transaction_outcome']['id'],
        "sender_id":
            "test0",
        "light_client_head":
            light_client_block_hash
    }]
    outcomes = [
        (function_call_result['result']['transaction_outcome']['outcome'],
         function_call_result['result']['transaction_outcome']['id'])
    ]
    for receipt_outcome in function_call_result['result']['receipts_outcome']:
        outcomes.append((receipt_outcome['outcome'], receipt_outcome['id']))
        queries.append({
            "type": "receipt",
            "receipt_id": receipt_outcome['id'],
            "receiver_id": "test0",
            "light_client_head": light_client_block_hash
        })

    for query, (outcome, id) in zip(queries, outcomes):
        res = nodes[0].json_rpc('light_client_proof', query, timeout=10)
        assert 'error' not in res, res
        light_client_proof = res['result']
        # check that execution outcome root proof is valid
        execution_outcome_hash = hashlib.sha256(
            serialize_execution_outcome_with_id(outcome, id)).digest()
        outcome_root = utils.compute_merkle_root_from_path(
            light_client_proof['outcome_proof']['proof'],
            execution_outcome_hash)
        block_outcome_root = utils.compute_merkle_root_from_path(
            light_client_proof['outcome_root_proof'],
            hashlib.sha256(outcome_root).digest())
        block = nodes[0].json_rpc(
            'block',
            {"block_id": light_client_proof['outcome_proof']['block_hash']})
        expected_root = block['result']['header']['outcome_root']
        assert base58.b58decode(
            expected_root
        ) == block_outcome_root, f'expected outcome root {expected_root} actual {base58.b58encode(block_outcome_root)}'
        # check that the light block header is valid
        block_header_lite = light_client_proof['block_header_lite']
        computed_block_hash = compute_block_hash(
            block_header_lite['inner_lite'],
            block_header_lite['inner_rest_hash'],
            block_header_lite['prev_block_hash'])
        assert light_client_proof['outcome_proof'][
            'block_hash'] == computed_block_hash.decode(
                'utf-8'
            ), f'expected block hash {light_client_proof["outcome_proof"]["block_hash"]} actual {computed_block_hash}'
        # check that block proof is valid
        block_merkle_root = utils.compute_merkle_root_from_path(
            light_client_proof['block_proof'],
            light_client_proof['outcome_proof']['block_hash'])
        assert base58.b58decode(
            light_client_block['inner_lite']['block_merkle_root']
        ) == block_merkle_root, f'expected block merkle root {light_client_block["inner_lite"]["block_merkle_root"]} actual {base58.b58encode(block_merkle_root)}'

def test_outcome_proof():
    nodes = start_cluster(
        2, 0, 1, None,
        [["epoch_length", 1000], ["block_producer_kickout_threshold", 80]], {})

    latest_block_hash = nodes[0].get_latest_block().hash_bytes
    deploy_contract_tx = transaction.sign_deploy_contract_tx(
        nodes[0].signer_key, utils.load_test_contract(), 10, latest_block_hash)
    deploy_contract_response = nodes[0].send_tx_and_wait(deploy_contract_tx, 15)
    assert 'error' not in deploy_contract_response, deploy_contract_response

    check_transaction_outcome_proof(nodes, True, 20)
    check_transaction_outcome_proof(nodes, False, 30)

if __name__ == '__main__':
    test_outcome_proof()

'''
'''--- pytest/tests/sanity/rpc_max_gas_burnt.py ---
#!/usr/bin/env python3
"""Test max_gas_burnt_view client configuration.

Spins up two nodes with different max_gas_burnt_view client configuration,
deploys a smart contract and finally calls a view function against both nodes
expecting the one with low max_gas_burnt_view limit to fail.
"""

import sys
import base58
import base64
import json
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))
import cluster
import utils
import transaction

GGAS = 10**9

def test_max_gas_burnt_view():
    nodes = cluster.start_cluster(
        2,
        0,
        1,
        config=None,
        genesis_config_changes=[],
        client_config_changes={1: {
            'max_gas_burnt_view': 130 * GGAS,
        }})

    contract_key = nodes[0].signer_key
    contract = utils.load_test_contract()

    # Deploy the fib smart contract
    latest_block_hash = nodes[0].get_latest_block().hash_bytes
    deploy_contract_tx = transaction.sign_deploy_contract_tx(
        contract_key, contract, 10, latest_block_hash)
    deploy_contract_response = nodes[0].send_tx_and_wait(deploy_contract_tx, 10)

    def call_fib(node, n):
        args = base64.b64encode(bytes([n])).decode('ascii')
        return node.call_function(contract_key.account_id,
                                  'fibonacci',
                                  args,
                                  timeout=10).get('result')

    # Call view function of the smart contract via the first node.  This should
    # succeed.
    result = call_fib(nodes[0], 25)
    assert 'result' in result and 'error' not in result, (
        'Expected "result" and no "error" in response, got: {}'.format(result))
    n = int.from_bytes(bytes(result['result']), 'little')
    assert n == 75025, 'Expected result to be 75025 but got: {}'.format(n)

    # Same but against the second node.  This should fail because of gas limit.
    result = call_fib(nodes[1], 25)
    assert 'result' not in result and 'error' in result, (
        'Expected "error" and no "result" in response, got: {}'.format(result))
    error = result['error']
    assert 'HostError(GasLimitExceeded)' in error, (
        'Expected error due to GasLimitExceeded but got: {}'.format(error))

    # It should still succeed for small arguments.
    result = call_fib(nodes[1], 5)
    assert 'result' in result and 'error' not in result, (
        'Expected "result" and no "error" in response, got: {}'.format(result))
    n = int.from_bytes(bytes(result['result']), 'little')
    assert n == 5, 'Expected result to be 5 but got: {}'.format(n)

if __name__ == '__main__':
    test_max_gas_burnt_view()

'''
'''--- pytest/tests/sanity/rpc_state_changes.py ---
#!/usr/bin/env python3
# Spins up four nodes, deploy a smart contract to one node,
# and call various scenarios to trigger store changes.
# Check that the key changes are observable via `changes` RPC call.

import base58, base64
import json
import struct
import sys
import threading
import pathlib

import deepdiff

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))
from cluster import start_cluster
from key import Key
from utils import load_test_contract
import transaction

nodes = start_cluster(
    4, 0, 1, None,
    [["epoch_length", 1000], ["block_producer_kickout_threshold", 80]], {})

def assert_changes_in_block_response(request, expected_response):
    for node_index, node in enumerate(nodes):
        response = node.get_changes_in_block(request)
        assert 'result' in response, "the request did not succeed: %r" % response
        response = response['result']
        diff = deepdiff.DeepDiff(expected_response, response)
        assert not diff, \
            "query node #%d same changes gives different results %r (expected VS actual):\n%r\n%r" \
            % (node_index, diff, expected_response, response)

def assert_changes_response(request, expected_response, **kwargs):
    for node_index, node in enumerate(nodes):
        response = node.get_changes(request)
        assert 'result' in response, "the request did not succeed: %r" % response
        response = response['result']
        diff = deepdiff.DeepDiff(expected_response, response, **kwargs)
        assert not diff, \
            "query node #%d same changes gives different results %r (expected VS actual):\n%r\n%r" \
            % (node_index, diff, expected_response, response)

def test_changes_with_new_account_with_access_key():
    """
    Plan:
    1. Create a new account with an access key.
    2. Observe the changes in the block where the receipt lands.
    3. Remove the access key.
    4. Observe the changes in the block where the receipt lands.
    """

    base_account_id = nodes[0].signer_key.account_id
    # re-use the key as a new account access key
    new_key = Key(
        account_id=f'rpc_key_value_changes.{base_account_id}',
        pk=nodes[1].signer_key.pk,
        sk=nodes[1].signer_key.sk,
    )

    # Step 1
    latest_block_hash = nodes[0].get_latest_block().hash_bytes
    create_account_tx = transaction.sign_create_account_with_full_access_key_and_balance_tx(
        creator_key=nodes[0].signer_key,
        new_account_id=new_key.account_id,
        new_key=new_key,
        balance=10**24,
        nonce=7,
        block_hash=latest_block_hash)
    new_account_response = nodes[0].send_tx_and_wait(create_account_tx, 10)

    # Step 2
    block_hash = new_account_response['result']['receipts_outcome'][0][
        'block_hash']
    assert_changes_in_block_response(request={"block_id": block_hash},
                                     expected_response={
                                         "block_hash":
                                             block_hash,
                                         "changes": [{
                                             "type": "account_touched",
                                             "account_id": new_key.account_id,
                                         }, {
                                             "type": "access_key_touched",
                                             "account_id": new_key.account_id,
                                         }]
                                     })

    base_request = {
        "block_id": block_hash,
        "changes_type": "all_access_key_changes",
    }
    for request in [
            # Test empty account_ids
        {
            **base_request, "account_ids": []
        },
            # Test an account_id that is a prefix of the original account_id.
        {
            **base_request, "account_ids": [new_key.account_id[:-1]]
        },
            # Test an account_id that has the original account_id as a prefix.
        {
            **base_request, "account_ids": [new_key.account_id + '_extra']
        },
    ]:
        assert_changes_response(request=request,
                                expected_response={
                                    "block_hash": block_hash,
                                    "changes": []
                                })

    # Test happy-path
    block_header = nodes[0].get_block(block_hash)['result']['header']
    prev_block_header = nodes[0].get_block(
        block_header['prev_hash'])['result']['header']
    nonce = prev_block_header['height'] * 1000000
    expected_response = {
        "block_hash":
            block_hash,
        "changes": [{
            "cause": {
                "type":
                    "receipt_processing",
                "receipt_hash":
                    new_account_response["result"]["receipts_outcome"][0]["id"],
            },
            "type": "access_key_update",
            "change": {
                "account_id": new_key.account_id,
                "public_key": new_key.pk,
                "access_key": {
                    "nonce": nonce,
                    "permission": "FullAccess"
                },
            }
        }]
    }
    for request in [
        {
            "block_id": block_hash,
            "changes_type": "all_access_key_changes",
            "account_ids": [new_key.account_id],
        },
        {
            "block_id":
                block_hash,
            "changes_type":
                "all_access_key_changes",
            "account_ids": [
                new_key.account_id + '_non_existing1', new_key.account_id,
                new_key.account_id + '_non_existing2'
            ],
        },
    ]:
        assert_changes_response(request=request,
                                expected_response=expected_response)

    # Step 3
    latest_block_hash = nodes[0].get_latest_block().hash_bytes
    nonce += 8
    delete_access_key_tx = transaction.sign_delete_access_key_tx(
        signer_key=new_key,
        target_account_id=new_key.account_id,
        key_for_deletion=new_key,
        nonce=nonce,
        block_hash=latest_block_hash)
    delete_access_key_response = nodes[1].send_tx_and_wait(
        delete_access_key_tx, 10)

    # Step 4
    block_hash = delete_access_key_response['result']['receipts_outcome'][0][
        'block_hash']
    assert_changes_in_block_response(request={"block_id": block_hash},
                                     expected_response={
                                         "block_hash":
                                             block_hash,
                                         "changes": [{
                                             "type": "account_touched",
                                             "account_id": new_key.account_id,
                                         }, {
                                             "type": "access_key_touched",
                                             "account_id": new_key.account_id,
                                         }]
                                     })

    base_request = {
        "block_id": block_hash,
        "changes_type": "all_access_key_changes",
    }
    for request in [
            # Test empty account_ids
        {
            **base_request, "account_ids": []
        },
            # Test an account_id that is a prefix of the original account_id
        {
            **base_request, "account_ids": [new_key.account_id[:-1]]
        },
            # Test an account_id that has the original account_id as a prefix
        {
            **base_request, "account_ids": [new_key.account_id + '_extra']
        },
            # Test empty keys in single_access_key_changes request
        {
            "block_id": block_hash,
            "changes_type": "single_access_key_changes",
            "keys": []
        },
            # Test non-existing account_id
        {
            "block_id":
                block_hash,
            "changes_type":
                "single_access_key_changes",
            "keys": [{
                "account_id": new_key.account_id + '_non_existing1',
                "public_key": new_key.pk
            },],
        },
            # Test non-existing public_key for an existing account_id
        {
            "block_id":
                block_hash,
            "changes_type":
                "single_access_key_changes",
            "keys": [{
                "account_id": new_key.account_id,
                "public_key": new_key.pk[:-3] + 'aaa'
            },],
        },
    ]:
        assert_changes_response(request=request,
                                expected_response={
                                    "block_hash": block_hash,
                                    "changes": []
                                })

    # Test happy-path
    expected_response = {
        "block_hash":
            block_hash,
        "changes": [{
            "cause": {
                'type':
                    'transaction_processing',
                'tx_hash':
                    delete_access_key_response['result']['transaction']['hash'],
            },
            "type": "access_key_update",
            "change": {
                "account_id": new_key.account_id,
                "public_key": new_key.pk,
                "access_key": {
                    "nonce": nonce,
                    "permission": "FullAccess"
                },
            }
        }, {
            "cause": {
                "type":
                    "receipt_processing",
                "receipt_hash":
                    delete_access_key_response["result"]["receipts_outcome"][0]
                    ["id"]
            },
            "type": "access_key_deletion",
            "change": {
                "account_id": new_key.account_id,
                "public_key": new_key.pk,
            }
        }]
    }

    for request in [
        {
            "block_id": block_hash,
            "changes_type": "all_access_key_changes",
            "account_ids": [new_key.account_id],
        },
        {
            "block_id":
                block_hash,
            "changes_type":
                "all_access_key_changes",
            "account_ids": [
                new_key.account_id + '_non_existing1', new_key.account_id,
                new_key.account_id + '_non_existing2'
            ],
        },
        {
            "block_id":
                block_hash,
            "changes_type":
                "single_access_key_changes",
            "keys": [{
                "account_id": new_key.account_id,
                "public_key": new_key.pk
            }],
        },
        {
            "block_id":
                block_hash,
            "changes_type":
                "single_access_key_changes",
            "keys": [
                {
                    "account_id": new_key.account_id + '_non_existing1',
                    "public_key": new_key.pk
                },
                {
                    "account_id": new_key.account_id,
                    "public_key": new_key.pk
                },
            ],
        },
    ]:
        assert_changes_response(request=request,
                                expected_response=expected_response)

def test_key_value_changes():
    """
    Plan:
    1. Deploy a contract.
    2. Observe the code changes in the block where the transaction outcome "lands".
    3. Send two transactions to be included into the same block setting and overriding the value of
       the same key.
    4. Observe the changes in the block where the transaction outcome "lands".
    """

    contract_key = nodes[0].signer_key
    contract_blob = load_test_contract()

    # Step 1
    latest_block_hash = nodes[0].get_latest_block().hash_bytes
    deploy_contract_tx = transaction.sign_deploy_contract_tx(
        contract_key, contract_blob, 10, latest_block_hash)
    deploy_contract_response = nodes[0].send_tx_and_wait(deploy_contract_tx, 10)

    # Step 2
    block_hash = deploy_contract_response['result']['transaction_outcome'][
        'block_hash']
    assert_changes_in_block_response(
        request={"block_id": block_hash},
        expected_response={
            "block_hash":
                block_hash,
            "changes": [{
                "type": "account_touched",
                "account_id": contract_key.account_id,
            }, {
                "type": "contract_code_touched",
                "account_id": contract_key.account_id,
            }, {
                "type": "access_key_touched",
                "account_id": contract_key.account_id,
            }]
        })

    base_request = {
        "block_id": block_hash,
        "changes_type": "contract_code_changes",
    }
    for request in [
            # Test empty account_ids
        {
            **base_request, "account_ids": []
        },
            # Test an account_id that is a prefix of the original account_id
        {
            **base_request, "account_ids": [contract_key.account_id[:-1]]
        },
            # Test an account_id that has the original account_id as a prefix
        {
            **base_request, "account_ids": [contract_key.account_id + '_extra']
        },
    ]:
        assert_changes_response(request=request,
                                expected_response={
                                    "block_hash": block_hash,
                                    "changes": []
                                })

    # Test happy-path
    expected_response = {
        "block_hash":
            block_hash,
        "changes": [{
            "cause": {
                "type":
                    "receipt_processing",
                "receipt_hash":
                    deploy_contract_response["result"]["receipts_outcome"][0]
                    ["id"],
            },
            "type": "contract_code_update",
            "change": {
                "account_id": contract_key.account_id,
                "code_base64": base64.b64encode(contract_blob).decode('utf-8'),
            }
        },]
    }
    base_request = {
        "block_id": block_hash,
        "changes_type": "contract_code_changes",
    }
    for request in [
        {
            **base_request, "account_ids": [contract_key.account_id]
        },
        {
            **base_request, "account_ids": [
                contract_key.account_id + '_non_existing1',
                contract_key.account_id,
                contract_key.account_id + '_non_existing2'
            ]
        },
    ]:
        assert_changes_response(request=request,
                                expected_response=expected_response)

    # Step 3
    latest_block_hash = nodes[1].get_latest_block().hash_bytes
    function_caller_key = nodes[0].signer_key

    key = struct.pack('<Q', 42)
    key_base64 = base64.b64encode(key).decode('ascii')

    def set_value(value, *, nounce):
        args = key + struct.pack('<Q', value)
        tx = transaction.sign_function_call_tx(function_caller_key,
                                               contract_key.account_id,
                                               'write_key_value', args,
                                               300000000000000, 100000000000,
                                               nounce, latest_block_hash)
        response = nodes[1].send_tx_and_wait(tx, 10)
        try:
            status = response['result']['receipts_outcome'][0]['outcome'][
                'status']
        except (KeyError, IndexError):
            status = ()
        assert 'SuccessValue' in status, (
            "Expected successful execution, but the output was: %s" % response)
        return response

    thread = threading.Thread(target=lambda: set_value(10, nounce=20))
    thread.start()
    response = set_value(20, nounce=30)
    thread.join()

    tx_block_hash = response['result']['transaction_outcome']['block_hash']

    # Step 4
    assert_changes_in_block_response(
        request={"block_id": tx_block_hash},
        expected_response={
            "block_hash":
                tx_block_hash,
            "changes": [
                {
                    "type": "account_touched",
                    "account_id": contract_key.account_id,
                },
                {
                    "type": "access_key_touched",
                    "account_id": contract_key.account_id,
                },
                {
                    "type": "data_touched",
                    "account_id": contract_key.account_id,
                },
            ]
        })

    base_request = {
        "block_id": block_hash,
        "changes_type": "data_changes",
        "key_prefix_base64": key_base64,
    }
    for request in [
            # Test empty account_ids
        {
            **base_request, "account_ids": []
        },
            # Test an account_id that is a prefix of the original account_id
        {
            **base_request, "account_ids": [contract_key.account_id[:-1]]
        },
            # Test an account_id that has the original account_id as a prefix
        {
            **base_request, "account_ids": [contract_key.account_id + '_extra']
        },
            # Test non-existing key prefix
        {
            **base_request,
            "account_ids": [contract_key.account_id],
            "key_prefix_base64":
                base64.b64encode(struct.pack('<Q', 24)).decode('ascii'),
        },
    ]:
        assert_changes_response(request=request,
                                expected_response={
                                    "block_hash": block_hash,
                                    "changes": []
                                })

    # Test happy-path
    expected_response = {
        "block_hash":
            tx_block_hash,
        "changes": [{
            "cause": {
                "type": "receipt_processing",
            },
            "type": "data_update",
            "change": {
                "account_id":
                    contract_key.account_id,
                "key_base64":
                    key_base64,
                "value_base64":
                    base64.b64encode(struct.pack('<Q', 10)).decode('ascii'),
            }
        }, {
            "cause": {
                "type": "receipt_processing",
                "receipt_hash": response["result"]["receipts_outcome"][0]["id"],
            },
            "type": "data_update",
            "change": {
                "account_id":
                    contract_key.account_id,
                "key_base64":
                    key_base64,
                "value_base64":
                    base64.b64encode(struct.pack('<Q', 20)).decode('ascii'),
            }
        }]
    }

    base_request = {
        "block_id": tx_block_hash,
        "changes_type": "data_changes",
        "key_prefix_base64": key_base64,
    }
    for request in [
        {
            **base_request, "account_ids": [contract_key.account_id]
        },
        {
            **base_request, "account_ids": [
                contract_key.account_id + '_non_existing1',
                contract_key.account_id,
                contract_key.account_id + '_non_existing2'
            ]
        },
        {
            **base_request,
            "account_ids": [contract_key.account_id],
            "key_prefix_base64": '',
        },
        {
            **base_request,
            "account_ids": [contract_key.account_id],
            "key_prefix_base64": base64.b64encode(key[:3]).decode('ascii'),
        },
    ]:
        assert_changes_response(
            request=request,
            expected_response=expected_response,
            exclude_paths={"root['changes'][0]['cause']['receipt_hash']"},
        )

if __name__ == '__main__':
    test_changes_with_new_account_with_access_key()
    test_key_value_changes()

'''
'''--- pytest/tests/sanity/rpc_tx_forwarding.py ---
#!/usr/bin/env python3
# The test launches two validating node and two observers
# The first observer tracks no shards, the second observer tracks all shards
# The second observer is used to query balances
# We then send one transaction synchronously through the first observer, and expect it to pass and apply due to rpc tx forwarding

import sys, time, base58, random
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import start_cluster
from configured_logger import logger
from utils import TxContext
from transaction import sign_payment_tx

nodes = start_cluster(
    2, 2, 4, None, [["min_gas_price", 0], ["epoch_length", 10],
                    ["block_producer_kickout_threshold", 70]], {
                        0: {
                            "consensus": {
                                "state_sync_timeout": {
                                    "secs": 2,
                                    "nanos": 0
                                }
                            }
                        },
                        1: {
                            "consensus": {
                                "state_sync_timeout": {
                                    "secs": 2,
                                    "nanos": 0
                                }
                            }
                        },
                        2: {
                            "consensus": {
                                "state_sync_timeout": {
                                    "secs": 2,
                                    "nanos": 0
                                }
                            }
                        },
                        3: {
                            "tracked_shards": [0, 1, 2, 3],
                            "consensus": {
                                "state_sync_timeout": {
                                    "secs": 2,
                                    "nanos": 0
                                }
                            }
                        }
                    })

time.sleep(3)
started = time.time()

old_balances = [
    int(nodes[-1].get_account("test%s" % x)['result']['amount'])
    for x in [0, 1, 2]
]
logger.info(f"BALANCES BEFORE {old_balances}")

hash_ = nodes[1].get_latest_block().hash

time.sleep(5)

tx = sign_payment_tx(nodes[0].signer_key, 'test1', 100, 1,
                     base58.b58decode(hash_.encode('utf8')))
logger.info(nodes[-2].send_tx_and_wait(tx, timeout=20))

new_balances = [
    int(nodes[-1].get_account("test%s" % x)['result']['amount'])
    for x in [0, 1, 2]
]
logger.info(f"BALANCES AFTER {new_balances}")

old_balances[0] -= 100
old_balances[1] += 100
assert old_balances == new_balances

'''
'''--- pytest/tests/sanity/rpc_tx_status.py ---
#!/usr/bin/env python3
import base58
import json
import struct
import sys
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))
import cluster
from utils import load_test_contract
import transaction

def submit_tx_and_check(nodes, node_index, tx):
    node = nodes[node_index]
    res = node.send_tx_and_wait(tx, timeout=20)
    assert 'error' not in res, res

    tx_hash = res['result']['transaction']['hash']
    query_res = nodes[0].json_rpc('EXPERIMENTAL_tx_status', [tx_hash, 'test0'])
    assert 'error' not in query_res, query_res

    receipt_id_from_outcomes = set(
        outcome['id'] for outcome in query_res['result']['receipts_outcome'])
    receipt_id_from_receipts = set(
        rec['receipt_id'] for rec in query_res['result']['receipts'])

    is_local_receipt = (res['result']['transaction']['signer_id'] ==
                        res['result']['transaction']['receiver_id'])
    if is_local_receipt:
        receipt_id_from_outcomes.remove(
            res['result']['transaction_outcome']['outcome']['receipt_ids'][0])

    assert receipt_id_from_outcomes == receipt_id_from_receipts, (
        f'receipt id from outcomes {receipt_id_from_outcomes}, '
        f'receipt id from receipts {receipt_id_from_receipts}')

def test_tx_status(nodes, *, nonce_offset: int = 0):
    signer_key = nodes[0].signer_key
    encoded_block_hash = nodes[0].get_latest_block().hash_bytes
    payment_tx = transaction.sign_payment_tx(signer_key, 'test1', 100,
                                             nonce_offset + 1,
                                             encoded_block_hash)
    submit_tx_and_check(nodes, 0, payment_tx)

    deploy_contract_tx = transaction.sign_deploy_contract_tx(
        signer_key, load_test_contract(), nonce_offset + 2, encoded_block_hash)
    submit_tx_and_check(nodes, 0, deploy_contract_tx)

    function_call_tx = transaction.sign_function_call_tx(
        signer_key, signer_key.account_id, 'write_key_value',
        struct.pack('<QQ', 42, 24), 300000000000000, 0, nonce_offset + 3,
        encoded_block_hash)
    submit_tx_and_check(nodes, 0, deploy_contract_tx)

def start_cluster(*, archive: bool = False):
    num_nodes = 4
    genesis_changes = [["epoch_length", 1000],
                       ["block_producer_kickout_threshold", 80],
                       ["transaction_validity_period", 10000]]
    config_changes = dict.fromkeys(range(num_nodes), {'archive': archive})
    return cluster.start_cluster(num_nodes=num_nodes,
                                 num_observers=0,
                                 num_shards=1,
                                 config=None,
                                 genesis_config_changes=genesis_changes,
                                 client_config_changes=config_changes)

def main():
    test_tx_status(start_cluster())

if __name__ == '__main__':
    main()

'''
'''--- pytest/tests/sanity/rpc_tx_submission.py ---
#!/usr/bin/env python3
# test various ways of submitting transactions (broadcast_tx_async, broadcast_tx_commit)

import sys, time, base58, base64
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import start_cluster
from configured_logger import logger
from utils import TxContext
from transaction import sign_payment_tx

nodes = start_cluster(
    2, 1, 1, None, [["min_gas_price", 0], ['max_inflation_rate', [0, 1]],
                    ["epoch_length", 100], ['transaction_validity_period', 200],
                    ["block_producer_kickout_threshold", 70]], {})

time.sleep(3)
started = time.time()

old_balances = [
    int(nodes[0].get_account("test%s" % x)['result']['amount']) for x in [0, 1]
]
logger.info(f"BALANCES BEFORE {old_balances}")

hash1 = nodes[0].get_latest_block().hash_bytes

tx = sign_payment_tx(nodes[0].signer_key, 'test1', 100, 1, hash1)
res = nodes[0].send_tx_and_wait(tx, timeout=20)
assert 'error' not in res, res
time.sleep(1)

tx = sign_payment_tx(nodes[0].signer_key, 'test1', 101, 2, hash1)
res = nodes[0].json_rpc('broadcast_tx_async',
                        [base64.b64encode(tx).decode('utf8')])
assert 'error' not in res, res
time.sleep(5)
tx_query_res = nodes[0].json_rpc('tx', [res['result'], 'test0'])
assert 'error' not in tx_query_res, tx_query_res
time.sleep(1)

new_balances = [
    int(nodes[0].get_account("test%s" % x)['result']['amount']) for x in [0, 1]
]
logger.info(f"BALANCES AFTER {new_balances}")
assert new_balances[0] == old_balances[0] - 201
assert new_balances[1] == old_balances[1] + 201

status = nodes[0].get_status()
hash_ = status['sync_info']['latest_block_hash']
tx = sign_payment_tx(nodes[0].signer_key, 'test1', 100, 1, hash1)

# tx status check should be idempotent
res = nodes[0].json_rpc('tx', [base64.b64encode(tx).decode('utf8')], timeout=10)
assert 'error' not in res, res

# broadcast_tx_commit should be idempotent
res = nodes[0].send_tx_and_wait(tx, timeout=15)
assert 'error' not in res, res

tx = sign_payment_tx(nodes[0].signer_key, 'test1', 100, 10,
                     base58.b58decode(hash_.encode('utf8')))
# check a transaction that doesn't exist yet
params = {
    "signed_tx_base64": base64.b64encode(tx).decode('utf8'),
    "wait_until": "NONE"
}
res = nodes[0].json_rpc('tx', params, timeout=10)
assert "doesn't exist" in res['error']['data'], res

'''
'''--- pytest/tests/sanity/simple.py ---
#!/usr/bin/env python3
# A simple test that is a good template for writing new tests.
# Spins up a few nodes and waits for a few blocks to be produced.
# Usage:
# python3 pytest/tests/sanity/simple.py
# python3 pytest/tests/sanity/simple.py SimpleTest.test_simple

import unittest
import sys
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from configured_logger import logger
from cluster import init_cluster, load_config, spin_up_node, start_cluster
from utils import wait_for_blocks

class SimpleTest(unittest.TestCase):

    # Spin up a single node and wait for a few blocks.
    # Please note that this uses the default neard config and is pretty slow.
    def test_simple(self):
        logger.info("The simple test is starting.")

        [node] = start_cluster(
            num_nodes=1,
            num_observers=0,
            num_shards=1,
            config=None,
            genesis_config_changes=[],
            client_config_changes={},
            message_handler=None,
        )

        wait_for_blocks(node, target=20)

        # It's important to kill the nodes at the end of the test when there are
        # multiple tests in the test suite. Otherwise the nodes from different
        # tests can be mixed up.
        node.kill()

        logger.info("The simple test is finished.")

    # Spin up a single node and wait for a few blocks. This test case is
    # customized to showcase the various configuration options.
    def test_custom(self):
        logger.info("The custom test is starting.")

        test_config = load_config()
        epoch_length = 10
        genesis_config_changes = [
            ("epoch_length", epoch_length),
        ]
        client_config_changes = {
            0: {
                'archive': True,
            },
        }

        near_root, [node_dir] = init_cluster(
            num_nodes=1,
            num_observers=0,
            num_shards=1,
            config=test_config,
            genesis_config_changes=genesis_config_changes,
            client_config_changes=client_config_changes,
            prefix="test_custom_",
        )

        node = spin_up_node(
            test_config,
            near_root,
            node_dir,
            0,
        )

        wait_for_blocks(node, target=2 * epoch_length)

        # It's important to kill the nodes at the end of the test when there are
        # multiple tests in the test suite. Otherwise the nodes from different
        # tests can be mixed up.
        node.kill()

        logger.info("The custom test is finished.")

if __name__ == '__main__':
    unittest.main()

'''
'''--- pytest/tests/sanity/skip_epoch.py ---
#!/usr/bin/env python3
# Tests a situation when in a given shard has all BPs offline
# Two specific cases:
#  - BPs never showed up to begin with, since genesis
#  - BPs went offline after some epoch
# Warn: this test may not clean up ~/.near if fails early

import base58
import pathlib
import sys
import time

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import init_cluster, spin_up_node, load_config
from configured_logger import logger
from transaction import sign_staking_tx
import state_sync_lib
import utils

TIMEOUT = 600

EPOCH_LENGTH = 30
# the height we spin up the second node
TARGET_HEIGHT = int(EPOCH_LENGTH * 2.8)

config = load_config()
node_config = state_sync_lib.get_state_sync_config_combined()
# give more stake to the bootnode so that it can produce the blocks alone
near_root, node_dirs = init_cluster(
    4, 1, 4, config,
    [["min_gas_price", 0], ["max_inflation_rate", [0, 1]],
     ["epoch_length", EPOCH_LENGTH], ["block_producer_kickout_threshold", 20],
     ["chunk_producer_kickout_threshold", 20]],
    {x: node_config for x in range(5)})

started = time.time()

boot_node = spin_up_node(config, near_root, node_dirs[0], 0)
boot_node.stop_checking_store()
node2 = spin_up_node(config, near_root, node_dirs[2], 2, boot_node=boot_node)
node3 = spin_up_node(config, near_root, node_dirs[3], 3, boot_node=boot_node)
observer = spin_up_node(config, near_root, node_dirs[4], 4, boot_node=boot_node)
observer.stop_checking_store()

ctx = utils.TxContext([4, 4, 4, 4, 4],
                      [boot_node, None, node2, node3, observer])
initial_balances = ctx.get_balances()
total_supply = sum(initial_balances)

logger.info("Initial balances: %s\nTotal supply: %s" %
            (initial_balances, total_supply))

sent_txs = False
largest_height = 0

# 1. Make the first node get to height 35. The second epoch will end around height 24-25,
#    which would already result in a stall if the first node can't sync the state from the
#    observer for the shard it doesn't care about
for height, hash_ in utils.poll_blocks(observer,
                                       timeout=TIMEOUT,
                                       poll_interval=0.1):
    if height >= TARGET_HEIGHT:
        break
    if height > 1 and not sent_txs:
        ctx.send_moar_txs(hash_, 10, False)
        logger.info(f'Sending txs at height {height}')
        sent_txs = True

logger.info("stage 1 done")

# 2. Spin up the second node and make sure it gets to TARGET_HEIGHT
node1 = spin_up_node(config, near_root, node_dirs[1], 1, boot_node=boot_node)
node1.stop_checking_store()

node1_height, _ = utils.wait_for_blocks(node1, target=TARGET_HEIGHT)

logger.info(f"stage 2 done, node1_height: {node1_height}")

# 3. During (1) we sent some txs. Make sure the state changed. We can't compare to the
#    expected balances directly, since the tx sent to the shard that node1 is responsible
#    for was never applied, but we can make sure that some change to the state was done,
#    and that the totals match (= the receipts was received)
#    What we are testing here specifically is that the first node received proper incoming
#    receipts during the state sync from the observer.
#    `max_inflation_rate` is set to zero, so the rewards do not mess up with the balances
balances = ctx.get_balances()
logger.info("New balances: %s\nNew total supply: %s" %
            (balances, sum(balances)))

assert balances != initial_balances
assert sum(balances) == total_supply

initial_balances = balances

logger.info("stage 3 done")

# 4. Stake for the second node to bring it back up as a validator and wait until it actually
#    becomes one

def get_validators():
    return set([x['account_id'] for x in boot_node.get_status()['validators']])

logger.info(get_validators())

# The stake for node1 must be higher than that of boot_node, so that it can produce blocks
# after the boot_node is brought down
tx = sign_staking_tx(node1.signer_key, node1.validator_key,
                     50000000000000000000000000000000, 20,
                     base58.b58decode(hash_.encode('utf8')))
boot_node.send_tx(tx)

validators = get_validators()
assert validators == set(["test0", "test2", "test3"]), validators

while True:
    if time.time() - started > TIMEOUT:
        logger.info(get_validators())
        assert False

    if get_validators() == set(["test0", "test1", "test2", "test3"]):
        break

    time.sleep(1)

logger.info("stage 4 done")

ctx.next_nonce = 100
# 5. Bring down the first node, then wait until epoch T+3
last_height = observer.get_latest_block().height

ctx.nodes = [boot_node, node1, node2, node3, observer]
ctx.act_to_val = [4, 4, 4, 4, 4]

boot_node.kill()

for height, hash_ in utils.poll_blocks(observer,
                                       timeout=TIMEOUT,
                                       poll_interval=0.1):
    logger.info(f'height: {height}')
    if height > last_height + 1:
        ctx.send_moar_txs(hash_, 10, False)
        logger.info(f'Sending txs at height {height}')
        break

start_epoch = -1
for epoch_height in utils.poll_epochs(observer,
                                      epoch_length=EPOCH_LENGTH,
                                      timeout=TIMEOUT):
    logger.info(f'epoch_height: {epoch_height}')
    if start_epoch == -1:
        start_epoch = epoch_height
    if epoch_height >= start_epoch + 3:
        break

balances = ctx.get_balances()
logger.info("New balances: %s\nNew total supply: %s" %
            (balances, sum(balances)))

ctx.nodes = [observer, node1]
ctx.act_to_val = [0, 0, 0, 0, 0]
logger.info("Observer sees: %s" % ctx.get_balances())

assert balances != initial_balances, "current balance %s, initial balance %s" % (
    balances, initial_balances)
assert sum(balances) == total_supply

'''
'''--- pytest/tests/sanity/spin_up_cluster.py ---
#!/usr/bin/env python3
"""Spins up a two-node cluster and wait for a few blocks to be produced."""

import sys
import time
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

import cluster
from configured_logger import logger
import utils

def test_sanity_spin_up():
    """Spins up a two-node cluster and wait for a few blocks to be produced.

    Sets store.path of one of the node to something other than `data` to test if
    that option works as well.

    This is just a sanity check that the neard binary isn’t borked too much.
    See <https://github.com/near/nearcore/issues/4993>.
    """
    nodes = cluster.start_cluster(
        2,
        0,
        1,
        None, [],
        client_config_changes={1: {
            'store': {
                'path': 'atad'
            }
        }})
    utils.wait_for_blocks(nodes[0], target=4)
    # Verify that second node created RocskDB in ‘atad’ directory rather than
    # ‘data’.
    assert not (pathlib.Path(nodes[1].node_dir) / 'data').exists()
    assert (pathlib.Path(nodes[1].node_dir) / 'atad').exists()

if __name__ == '__main__':
    test_sanity_spin_up()

'''
'''--- pytest/tests/sanity/split_storage.py ---
#!/usr/bin/python3
"""
 Spins up an archival node with cold store configured and verifies that blocks
 are copied from hot to cold store.
"""

import copy
import json
import os
import os.path as path
import pathlib
import shutil
import subprocess
import sys
import time
import unittest

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import (get_config_json, init_cluster, load_config,
                     set_config_json, spin_up_node)
from configured_logger import logger

from utils import wait_for_blocks

class TestSplitStorage(unittest.TestCase):

    def _steps(self):
        for name in dir(self):  # dir() result is implicitly sorted
            if name.startswith("step"):
                yield name, getattr(self, name)

    def test_steps(self):
        for _, step in self._steps():
            step()
            time.sleep(5)

    def _pretty_json(self, value):
        return json.dumps(value, indent=2)

    def _get_split_storage_info(self, node):
        return node.json_rpc("EXPERIMENTAL_split_storage_info", {})

    def _configure_cold_storage(self, node_dir):
        node_config = get_config_json(node_dir)

        # Need to create a deepcopy of the store config, otherwise
        # store and cold store will point to the same instance and
        # both will end up with the same path.
        node_config["cold_store"] = copy.deepcopy(node_config["store"])
        node_config["store"]["path"] = path.join(node_dir, 'data')
        node_config["cold_store"]["path"] = path.join(node_dir, 'cold_data')

        set_config_json(node_dir, node_config)

    def _configure_hot_storage(self, node_dir, new_path):
        node_config = get_config_json(node_dir)
        node_config["store"]["path"] = new_path
        set_config_json(node_dir, node_config)

    def _check_split_storage_info(
        self,
        msg,
        node,
        expected_head_height,
        expected_hot_db_kind,
        check_cold_head,
    ):
        info = self._get_split_storage_info(node)
        pretty_info = self._pretty_json(info)
        logger.info(f"Checking split storage info for the {msg}")
        logger.info(f"The split storage info is \n{pretty_info}")

        self.assertNotIn("error", info)
        self.assertIn("result", info)
        result = info["result"]
        head_height = result["head_height"]
        final_head_height = result["final_head_height"]
        cold_head_height = result["cold_head_height"]
        hot_db_kind = result["hot_db_kind"]

        self.assertEqual(hot_db_kind, expected_hot_db_kind)
        self.assertGreaterEqual(head_height, expected_head_height)
        self.assertGreaterEqual(final_head_height, expected_head_height - 10)
        if check_cold_head:
            self.assertGreaterEqual(cold_head_height, final_head_height - 10)
        else:
            self.assertIsNone(cold_head_height)

    # Migrate archival node using rpc node as source for hot db.
    # wait_period should be enough block for initial migration to finish, cold_head catch up to head and tail to appear.
    # for localnet tests it is enough to use epoch size * number of epochs to keep.
    def _migrate_to_split_storage(self, rpc, archival, archival_dir,
                                  wait_period):
        logger.info("Phase 1 - Running neard before migration.")

        # Wait until a few blocks are produced so that we're sure that the db is
        # properly created and populated with some data.
        n = 5
        n = wait_for_blocks(archival, target=n).height

        self._check_split_storage_info(
            "migration_phase_1",
            node=archival,
            expected_head_height=n,
            # The hot db kind should remain archive until fully migrated.
            expected_hot_db_kind="Archive",
            # The cold storage is not configured so cold head should be none.
            check_cold_head=False,
        )

        logger.info("Phase 2 - Setting the cold storage and restarting neard.")

        self._configure_cold_storage(archival_dir)
        archival.kill()
        archival.start()

        # Wait for a few seconds to:
        # - Give the node enough time to get started.
        # - Give the cold store loop enough time to run - it runs every 1s.
        # TODO(posvyatokum) this is a quick stop-gap solution to fix nayduck, this
        # should be solved by waiting in a loop until cold store head is at
        # expected proximity to final head.
        time.sleep(4)

        # Wait until enough blocks so that we produce enough blocks to fill 5
        # epochs to trigger GC - otherwise the tail won't be set.
        n = max(n, wait_period) + 5
        logger.info(f"Wait until RPC reaches #{n}")
        wait_for_blocks(rpc, target=n)
        logger.info(f"Wait until archival reaches #{n}")
        wait_for_blocks(archival, target=n)

        self._check_split_storage_info(
            "migration_phase_2",
            node=archival,
            expected_head_height=n,
            # The hot db kind should remain archive until fully migrated.
            expected_hot_db_kind="Archive",
            # The cold storage head should be fully caught up by now.
            check_cold_head=True,
        )

        logger.info("Phase 3 - Preparing hot storage from rpc backup.")

        # Stop the RPC node in order to dump the db to disk.
        rpc.kill()

        rpc_src = path.join(rpc.node_dir, "data")
        rpc_dst = path.join(archival.node_dir, "hot_data")
        logger.info(f"Copying rpc backup from {rpc_src} to {rpc_dst}")
        shutil.copytree(rpc_src, rpc_dst)

        archival_dir = pathlib.Path(archival.node_dir)
        with open(archival_dir / 'prepare-hot-stdout', 'w') as stdout, \
                open(archival_dir / 'prepare-hot-stderr', 'w') as stderr:
            cmd = [
                str(pathlib.Path(archival.near_root) / archival.binary_name),
                f'--home={archival_dir}',
                f'cold-store',
                f'prepare-hot',
                f'--store-relative-path',
                f'hot_data',
            ]
            logger.info(f"Calling '{' '.join(cmd)}'")
            subprocess.check_call(
                cmd,
                stdin=subprocess.DEVNULL,
                stdout=stdout,
                stderr=stderr,
                env=dict(
                    os.environ,
                    RUST_LOG='debug',
                ),
            )

        self._configure_hot_storage(archival_dir, rpc_dst)

        logger.info("Phase 4 - After migration.")

        archival.kill()
        archival.start()
        rpc.start()

        # Wait for a few seconds to:
        # - Give the node enough time to get started.
        # - Give the cold store loop enough time to run - it runs every 1s.
        # TODO(posvyatokum) this is a quick stop-gap solution to fix nayduck, this
        # should be solved by waiting in a loop until cold store head is at
        # expected proximity to final head.
        time.sleep(4)

        # Wait for just a few blocks to make sure neard correctly restarted.
        n += 5
        wait_for_blocks(archival, target=n)

        self._check_split_storage_info(
            "migration_phase_4",
            node=archival,
            expected_head_height=n,
            # The migration is over, the hot db kind should be set to hot.
            expected_hot_db_kind="Hot",
            # The cold storage head should be fully caught up.
            check_cold_head=True,
        )

    # Configure cold storage and start neard. Wait for a few blocks
    # and verify that cold head is moving and that it's close behind
    # final head.
    def step1_base_case_test(self):
        logger.info(f"starting test_base_case")

        config = load_config()
        client_config_changes = {
            0: {
                'archive': True,
                'save_trie_changes': True,
            },
        }

        epoch_length = 5
        genesis_config_changes = [
            ("epoch_length", epoch_length),
        ]

        near_root, [node_dir] = init_cluster(
            1,
            0,
            1,
            config,
            genesis_config_changes,
            client_config_changes,
            "test_base_case_",
        )

        self._configure_cold_storage(node_dir)

        node = spin_up_node(config, near_root, node_dir, 0, single_node=True)

        # Wait until enough blocks are produced so that we're guaranteed that
        # cold head has enough time to move and initial migration is finished.
        # The initial migration may fail at the beginning because FINAL_HEAD is
        # not set. It will retry after 30s.
        # TODO it would be better to configure the retry timeout above and set
        # it to a lower value to speed up the test.
        n = 50
        wait_for_blocks(node, target=n)

        self._check_split_storage_info(
            "base_case",
            node=node,
            expected_head_height=n,
            expected_hot_db_kind="Archive",
            check_cold_head=True,
        )

        node.kill()
        logger.info(f"Stopped node0 from base_chase_test")

    # Test the migration from single storage to split storage. This test spins
    # up two nodes, a validator node and an archival node. The validator stays
    # alive for the whole duration of the test. The archival node is migrated
    # to cold storage.
    # - phase 1 - have archival run with single storage for a few blocks
    # - phase 2 - configure cold storage and restart archival
    # - phase 3 - prepare hot storage from a rpc backup
    # - phase 4 - restart archival and check that it's correctly migrated
    def step2_migration_test(self):
        logger.info(f"Starting the migration test")

        # Archival nodes do not run state sync. This means that if peers
        # ran away further than epoch_lenght * gc_epoch_num, archival nodes
        # will not be able to further sync. In practice it means we need a long
        # enough epoch_length or more gc_epoch_num to keep.
        epoch_length = 10
        gc_epoch_num = 3

        genesis_config_changes = [
            ("epoch_length", epoch_length),
        ]
        client_config_changes = {
            # The validator node should be archival and be the source of
            # truth (and sync) for the other nodes. It needs to be archival
            # in case it runs away from the real archival node and the latter
            # won't be able to sync anymore.
            0: {
                'archive': True,
                'tracked_shards': [0],
            },
            # The rpc node will be used as the source of rpc backup db.
            1: {
                'archive': False,
                'tracked_shards': [0],
                'gc_num_epochs_to_keep': gc_epoch_num,
                'state_sync_enabled': True
            },
            # The archival node will be migrated to split storage.
            2: {
                'archive': True,
                'tracked_shards': [0],
                'save_trie_changes': True,
            },
        }

        config = load_config()
        near_root, [validator_dir, rpc_dir, archival_dir] = init_cluster(
            num_nodes=2,
            num_observers=1,
            num_shards=1,
            config=config,
            genesis_config_changes=genesis_config_changes,
            client_config_changes=client_config_changes,
            prefix="test_migration_",
        )

        validator = spin_up_node(
            config,
            near_root,
            validator_dir,
            0,
        )
        rpc = spin_up_node(
            config,
            near_root,
            rpc_dir,
            1,
            boot_node=validator,
        )
        archival = spin_up_node(
            config,
            near_root,
            archival_dir,
            2,
            boot_node=validator,
        )

        self._migrate_to_split_storage(rpc, archival, archival_dir,
                                       gc_epoch_num * epoch_length)

        validator.kill()
        rpc.kill()
        archival.kill()

    # Spin up legacy archival node and split storage node.
    # Pause legacy archival node, but continue running split storage node.
    # After split storage hot tail is bigger than legacy archival head, restart legacy archival node.
    # Make sure that legacy archival node is able to sync after that.
    def step3_archival_node_sync_test(self):
        logger.info(f"Starting the archival <- split storage sync test")

        # Archival nodes do not run state sync. This means that if peers
        # ran away further than epoch_lenght * gc_epoch_num, archival nodes
        # will not be able to further sync. In practice it means we need a long
        # enough epoch_length or more gc_epoch_num to keep.
        epoch_length = 10
        gc_epoch_num = 3

        genesis_config_changes = [
            ("epoch_length", epoch_length),
        ]
        client_config_changes = {
            0: {
                'archive': False,
                'tracked_shards': [0],
                'state_sync_enabled': True
            },
            1: {
                'archive': True,
                'tracked_shards': [0],
                'save_trie_changes': True,
                'split_storage': {
                    'enable_split_storage_view_client': True
                },
            },
            2: {
                'archive': True,
                'tracked_shards': [0],
            },
            3: {
                'archive': False,
                'tracked_shards': [0],
                'gc_num_epochs_to_keep': gc_epoch_num,
                'state_sync_enabled': True
            },
        }
        config = load_config()
        near_root, [validator_dir, split_dir, archival_dir,
                    rpc_dir] = init_cluster(
                        num_nodes=1,
                        num_observers=3,
                        num_shards=1,
                        config=config,
                        genesis_config_changes=genesis_config_changes,
                        client_config_changes=client_config_changes,
                        prefix="test_archival_node_sync_",
                    )

        validator = spin_up_node(
            config,
            near_root,
            validator_dir,
            0,
        )
        split = spin_up_node(
            config,
            near_root,
            split_dir,
            1,
            boot_node=validator,
        )
        archival = spin_up_node(
            config,
            near_root,
            archival_dir,
            2,
            boot_node=split,
        )
        rpc = spin_up_node(
            config,
            near_root,
            rpc_dir,
            3,
            boot_node=validator,
        )

        # First, migrate split node to split storage
        self._migrate_to_split_storage(rpc, split, split_dir,
                                       gc_epoch_num * epoch_length)

        # Remember ~where archival node stopped
        n = archival.get_latest_block().height

        logger.info("Kill legacy archival node.")
        # Now, kill legacy archival node, so it will be behind after restart and will be forced to sync from split node.
        archival.kill()

        logger.info(
            "Wait for split storage to have legacy archival head only in cold db."
        )
        # Wait for split storage to relly on cold db to sync archival node
        wait_for_blocks(split, target=n + epoch_length * gc_epoch_num * 2 + 1)
        # Kill validator and rpc so legacy archival doesn't have any peers that may accidentally have some useful data.
        validator.kill()
        rpc.kill()

        logger.info("Restart legacy archival node.")
        # Restart archival node. This should trigger sync.
        archival.start(boot_node=split)
        time.sleep(10)

        logger.info("Wait for legacy archival node to start syncing.")

        # Archival node should be able to sync to split storage without problems.
        wait_for_blocks(archival, target=n + epoch_length, timeout=120)
        archival.kill()
        split.kill()

if __name__ == "__main__":
    unittest.main()

'''
'''--- pytest/tests/sanity/staking1.py ---
#!/usr/bin/env python3
# Spins up with two validators, and one non-validator
# Stakes for the non-validators, ensures it becomes a validator
# Unstakes for them, makes sure they stop being a validator

import sys, time, base58, random, datetime
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import start_cluster
from configured_logger import logger
from transaction import sign_staking_tx
import utils

TIMEOUT = 150

config = None
nodes = start_cluster(
    2, 1, 1, config,
    [["epoch_length", 10], ["block_producer_kickout_threshold", 40]],
    {2: {
        "tracked_shards": [0],
        "store.state_snapshot_enabled": True,
    }})

started = time.time()

def get_validators():
    return set([x['account_id'] for x in nodes[0].get_status()['validators']])

def get_stakes():
    return [
        int(nodes[2].get_account("test%s" % i)['result']['locked'])
        for i in range(3)
    ]

hash_ = nodes[2].get_latest_block().hash_bytes

tx = sign_staking_tx(nodes[2].signer_key, nodes[2].validator_key,
                     100000000000000000000000000000000, 2, hash_)
nodes[0].send_tx(tx)

logger.info("Initial stakes: %s" % get_stakes())
for height, _ in utils.poll_blocks(nodes[0], timeout=TIMEOUT):
    if 'test2' in get_validators():
        logger.info("Normalin, normalin")
        assert 20 <= height <= 25, height
        break

tx = sign_staking_tx(nodes[2].signer_key, nodes[2].validator_key, 0, 3, hash_)
nodes[2].send_tx(tx)

for height, _ in utils.poll_blocks(nodes[0], timeout=TIMEOUT):
    if 'test2' not in get_validators():
        logger.info("DONE")
        assert 40 <= height <= 45, height
        break

'''
'''--- pytest/tests/sanity/staking2.py ---
#!/usr/bin/env python3
# Runs randomized staking transactions and makes some basic checks on the final `staked` values
# In each epoch sends two sets of staking transactions, one when (last_height % 12 == 4), called "fake", and
# one when (last_height % 12 == 7), called "real" (because the former will be overwritten by the later).
# Before the "fake" tx we expect the stakes to be equal to the largest of the last three "real" stakes for
# each node. Before "real" txs it is the largest of the same value, and the last "fake" stake.

import sys, time, base58, random, logging
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import start_cluster
from configured_logger import logger
from transaction import sign_staking_tx

TIMEOUT = 360
TIMEOUT_PER_ITER = 30

FAKE_OFFSET = 6
REAL_OFFSET = 12
EPOCH_LENGTH = 18

all_stakes = []
fake_stakes = [0, 0, 0]
next_nonce = 3

# other tests can set `sequence` to some sequence of triplets of stakes
# `do_moar_stakes` first uses the elements of `sequence` for stakes before switching to
# random. See `staking_repro1.py` for an example
sequence = []

def get_validators():
    return set([x['account_id'] for x in nodes[0].get_status()['validators']])

def get_stakes():
    return [
        int(nodes[2].get_account("test%s" % i)['result']['locked'])
        for i in range(3)
    ]

def get_expected_stakes():
    global all_stakes
    return [
        max(fake_stakes[i], max([x[i]
                                 for x in all_stakes[-3:]]))
        for i in range(3)
    ]

def do_moar_stakes(last_block_hash, update_expected):
    global next_nonce, all_stakes, fake_stakes, sequence

    if len(sequence) == 0:
        stakes = [0, 0, 0]
        # have 1-2 validators with stake, and the remaining without
        # make numbers dibisable by 1M so that we can easily distinguish a situation when the current locked amt has some reward added to it (not divisable by 1M) vs not (divisable by 1M)
        stakes[random.randint(0, 2)] = random.randint(
            70000000000000000000000000, 100000000000000000000000000) * 1000000
        stakes[random.randint(0, 2)] = random.randint(
            70000000000000000000000000, 100000000000000000000000000) * 1000000
    else:
        stakes = sequence[0]
        sequence = sequence[1:]

    vals = get_validators()
    val_id = int(list(vals)[0][4:])
    for i in range(3):
        tx = sign_staking_tx(nodes[i].signer_key, nodes[i].validator_key,
                             stakes[i], next_nonce,
                             base58.b58decode(last_block_hash.encode('utf8')))
        nodes[val_id].send_tx(tx)
        next_nonce += 1

    if update_expected:
        fake_stakes = [0, 0, 0]
        all_stakes.append(stakes)
    else:
        fake_stakes = stakes

    logger.info("Sent %s staking txs: %s" %
                ("REAL" if update_expected else "fake", stakes))

def doit(seq=[]):
    global nodes, all_stakes, sequence
    sequence = seq

    config = None
    nodes = start_cluster(
        2, 1, 1, config, [["epoch_length", EPOCH_LENGTH],
                          ["block_producer_kickout_threshold", 40],
                          ["chunk_producer_kickout_threshold", 40]], {
                              0: {
                                  "tracked_shards": [0],
                                  "view_client_throttle_period": {
                                      "secs": 0,
                                      "nanos": 0
                                  },
                                  "consensus": {
                                      "state_sync_timeout": {
                                          "secs": 2,
                                          "nanos": 0
                                      }
                                  }
                              },
                              1: {
                                  "tracked_shards": [0],
                                  "view_client_throttle_period": {
                                      "secs": 0,
                                      "nanos": 0
                                  },
                                  "consensus": {
                                      "state_sync_timeout": {
                                          "secs": 2,
                                          "nanos": 0
                                      }
                                  }
                              },
                              2: {
                                  "tracked_shards": [0],
                                  "view_client_throttle_period": {
                                      "secs": 0,
                                      "nanos": 0
                                  },
                                  "consensus": {
                                      "state_sync_timeout": {
                                          "secs": 2,
                                          "nanos": 0
                                      }
                                  },
                                  "store.state_snapshot_enabled": True,
                              }
                          })

    started = time.time()
    last_iter = started

    height, hash_ = nodes[2].get_latest_block()
    for i in range(3):
        nodes[i].stop_checking_store()

    logger.info("Initial stakes: %s" % get_stakes())
    all_stakes.append(get_stakes())

    do_moar_stakes(hash_, True)
    last_fake_stakes_height = FAKE_OFFSET
    last_staked_height = REAL_OFFSET

    while True:
        if time.time() - started >= TIMEOUT:
            break

        assert time.time() - last_iter < TIMEOUT_PER_ITER

        height, hash_ = nodes[0].get_latest_block()
        logging.info(
            f"Node 0 at height {height}; time since last staking iteration: {time.time() - last_iter} seconds"
        )
        send_fakes = send_reals = False

        if (height + EPOCH_LENGTH - FAKE_OFFSET) // EPOCH_LENGTH > (
                last_fake_stakes_height + EPOCH_LENGTH -
                FAKE_OFFSET) // EPOCH_LENGTH:
            last_iter = time.time()

            send_fakes = True

        if (height + EPOCH_LENGTH - REAL_OFFSET) // EPOCH_LENGTH > (
                last_staked_height + EPOCH_LENGTH -
                REAL_OFFSET) // EPOCH_LENGTH:

            send_reals = True

        if send_fakes or send_reals:
            cur_stakes = get_stakes()
            logger.info("Current stakes: %s" % cur_stakes)
            if len(all_stakes) > 1:
                expected_stakes = get_expected_stakes()
                logger.info("Expect  stakes: %s" % expected_stakes)
                for (cur, expected) in zip(cur_stakes, expected_stakes):
                    if cur % 1000000 == 0:
                        assert cur == expected
                    else:
                        assert expected <= cur <= expected * 1.1

            do_moar_stakes(hash_, update_expected=send_reals)

        if send_fakes:
            last_fake_stakes_height += EPOCH_LENGTH

        elif send_reals:
            last_staked_height += EPOCH_LENGTH

        time.sleep(1)

if __name__ == "__main__":
    doit()

'''
'''--- pytest/tests/sanity/staking_repro1.py ---
#!/usr/bin/env python3
# reproduces one of the bugs discovered by `staking2.py`

from staking2 import doit

# this repro was created before fake stakes were introduced, so each stake after the first one is duplicated to account for that
sequence = [
    [0, 73215476000000000000000000000000, 73203310000000000000000000000000],
    [0, 95711839000000000000000000000000, 75774073000000000000000000000000],
    [0, 95711839000000000000000000000000, 75774073000000000000000000000000],
    [0, 84382393000000000000000000000000, 98749818000000000000000000000000],
    [0, 84382393000000000000000000000000, 98749818000000000000000000000000],
    [0, 0, 90224511000000000000000000000000],
    [0, 0, 90224511000000000000000000000000],
    [86432337000000000000000000000000, 76422536000000000000000000000000, 0],
    [86432337000000000000000000000000, 76422536000000000000000000000000, 0],
    [75063341000000000000000000000000, 0, 80786582000000000000000000000000],
    [75063341000000000000000000000000, 0, 80786582000000000000000000000000],
    [86164122000000000000000000000000, 0, 73964803000000000000000000000000],
    [86164122000000000000000000000000, 0, 73964803000000000000000000000000],
    [70076530000000000000000000000000, 0, 0],
    [70076530000000000000000000000000, 0, 0],
    [0, 78553537000000000000000000000000, 0],
    [0, 78553537000000000000000000000000, 0],
    [85364825000000000000000000000000, 0, 81322978000000000000000000000000],
    [85364825000000000000000000000000, 0, 81322978000000000000000000000000],
    [98064272000000000000000000000000, 0, 0],
    [98064272000000000000000000000000, 0, 0],
    [0, 0, 78616653000000000000000000000000],
    [0, 0, 78616653000000000000000000000000],
    [99017960000000000000000000000000, 78066788000000000000000000000000, 0],
    [99017960000000000000000000000000, 78066788000000000000000000000000, 0],
    [83587257000000000000000000000000, 0, 0],
    [83587257000000000000000000000000, 0, 0],
    [81625375000000000000000000000000, 0, 90569985000000000000000000000000],
    [81625375000000000000000000000000, 0, 90569985000000000000000000000000],
    [76171853000000000000000000000000, 70811780000000000000000000000000, 0],
    [76171853000000000000000000000000, 70811780000000000000000000000000, 0],
]

doit(sequence)

'''
'''--- pytest/tests/sanity/staking_repro2.py ---
#!/usr/bin/env python3
# reproduces one of the bugs discovered by `staking2.py`

from staking2 import doit

sequence = [
    [0, 89807699575087731503000000000000, 99394381434196724661000000000000],
    [0, 78422984975403936749000000000000, 86824329382096781900000000000000],
    [96998170601935325415000000000000, 0, 96905754147675144777000000000000],
    [0, 90761562581683912450000000000000, 85484232564481713683000000000000],
    [0, 88770343317484058757000000000000, 94113873883235846136000000000000],
]

doit(sequence)

'''
'''--- pytest/tests/sanity/state_parts_dump_check.py ---
#!/usr/bin/env python3
# Spins up one validating node, one dumping node and one node to monitor the state parts dumped.
# Start all nodes.
# Before the end of epoch stop the dumping node.
# In the next epoch check the number of state sync files reported by the monitor. Should be 0
# Start the dumping node.
# In the following epoch check the number of state sync files reported.
# Should be non 0

import pathlib
import sys
import re
from itertools import islice, takewhile

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from utils import poll_blocks, poll_epochs
from cluster import init_cluster, spin_up_node, load_config
import state_sync_lib
from configured_logger import logger

EPOCH_LENGTH = 50
NUM_SHARDS = 4

def get_dump_check_metrics(target):
    metrics = target.get_metrics()
    metrics = [
        line.split(' ')
        for line in str(metrics).strip().split('\\n')
        if re.search('state_sync_dump_check.*{', line)
    ]
    metrics = {metric: int(val) for metric, val in metrics}
    return metrics

def main():
    node_config_dump, node_config = state_sync_lib.get_state_sync_configs_pair()
    config = load_config()
    near_root, node_dirs = init_cluster(1, 2, 4, config,
                                        [["epoch_length", EPOCH_LENGTH]], {
                                            0: node_config,
                                            1: node_config_dump,
                                            2: node_config
                                        })

    boot_node = spin_up_node(config, near_root, node_dirs[0], 0)
    logger.info('Started boot_node')
    dump_node = spin_up_node(config,
                             near_root,
                             node_dirs[1],
                             1,
                             boot_node=boot_node)
    dump_check = spin_up_node(config,
                              near_root,
                              node_dirs[2],
                              2,
                              boot_node=boot_node)
    dump_check.kill()
    chain_id = boot_node.get_status()['chain_id']
    dump_folder = node_config_dump["state_sync"]["dump"]["location"][
        "Filesystem"]["root_dir"]
    rpc_address, rpc_port = boot_node.rpc_addr()
    local_rpc_address, local_rpc_port = dump_check.rpc_addr()
    cmd = dump_check.get_command_for_subprogram(
        ('state-parts-dump-check', '--chain-id', chain_id, '--root-dir',
         dump_folder, 'loop-check', '--rpc-server-addr',
         f"http://{rpc_address}:{rpc_port}", '--prometheus-addr',
         f"{local_rpc_address}:{local_rpc_port}", '--interval', '2'),
        dump_check.near_root, dump_check.node_dir, dump_check.binary_name)
    dump_check.run_cmd(cmd=cmd)

    logger.info('Started nodes')

    # Close to the end of the epoch
    list(
        takewhile(lambda b: b.height < EPOCH_LENGTH - 5,
                  poll_blocks(boot_node)))
    # kill dumper node so that it can't dump state.
    node_height = dump_node.get_latest_block().height
    logger.info(f'Dump_node is @{node_height}')
    dump_node.kill()
    logger.info(f'Killed dump_node')

    # wait until next epoch starts
    list(
        takewhile(lambda b: b.height < EPOCH_LENGTH + 5,
                  poll_blocks(boot_node)))
    # Check the dumped stats
    metrics = get_dump_check_metrics(dump_check)
    assert sum([
        val for metric, val in metrics.items()
        if 'state_sync_dump_check_process_is_up' in metric
    ]) == NUM_SHARDS, f"Dumper process missing for some shards. {metrics}"
    assert sum([
        val for metric, val in metrics.items()
        if 'state_sync_dump_check_num_parts_dumped' in metric
    ]) == 0, f"No node was supposed to dump parts. {metrics}"
    assert sum([
        val for metric, val in metrics.items()
        if 'state_sync_dump_check_num_header_dumped' in metric
    ]) == 0, f"No node was supposed to dump headers. {metrics}"

    # wait for 10 more blocks.
    list(islice(poll_blocks(boot_node), 10))
    # Start state dumper node to keep up with the network and dump state next
    # epoch.
    logger.info(f'Starting dump_node')
    dump_node.start(boot_node=boot_node)

    # wait for the next epoch
    list(
        takewhile(lambda height: height < 2,
                  poll_epochs(boot_node, epoch_length=EPOCH_LENGTH)))
    list(islice(poll_blocks(boot_node), 10))
    # State should have been dumped and reported as dumped.
    metrics = get_dump_check_metrics(dump_check)
    assert sum([
        val for metric, val in metrics.items()
        if 'state_sync_dump_check_num_parts_dumped' in metric
    ]) >= NUM_SHARDS, f"Some parts are missing. {metrics}"
    assert sum([
        val for metric, val in metrics.items()
        if 'state_sync_dump_check_num_header_dumped' in metric
    ]) == NUM_SHARDS, f"Some headers are missing. {metrics}"

if __name__ == "__main__":
    main()

'''
'''--- pytest/tests/sanity/state_sync.py ---
#!/usr/bin/env python3
# Spins up a node, then waits for couple epochs
# and spins up another node
# Makes sure that eventually the second node catches up
# Three modes:
#   - notx: no transactions are sent, just checks that
#     the second node starts and catches up
#   - onetx: sends one series of txs at the beginning,
#     makes sure the second node balances reflect them
#   - manytx: constantly issues txs throughout the test
#     makes sure the balances are correct at the end

import sys, time
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

if len(sys.argv) < 3:
    logger.info("python state_sync.py [notx, onetx, manytx] <launch_at_block>")
    exit(1)

mode = sys.argv[1]
assert mode in ['notx', 'onetx', 'manytx']

from cluster import init_cluster, spin_up_node, load_config
from configured_logger import logger
import state_sync_lib
import utils

START_AT_BLOCK = int(sys.argv[2])
TIMEOUT = 150 + START_AT_BLOCK * 10

config = load_config()

node_config = state_sync_lib.get_state_sync_config_combined()

near_root, node_dirs = init_cluster(
    2, 1, 1, config,
    [["min_gas_price", 0], ["max_inflation_rate", [0, 1]], ["epoch_length", 10],
     ["block_producer_kickout_threshold", 80]],
    {x: node_config for x in range(3)})

started = time.time()

boot_node = spin_up_node(config, near_root, node_dirs[0], 0)
node1 = spin_up_node(config, near_root, node_dirs[1], 1, boot_node=boot_node)

ctx = utils.TxContext([0, 0], [boot_node, node1])

sent_txs = False

observed_height = 0
for height, block_hash in utils.poll_blocks(boot_node,
                                            timeout=TIMEOUT,
                                            poll_interval=0.1):
    observed_height = height
    if height >= START_AT_BLOCK:
        break

    if mode == 'onetx' and not sent_txs:
        ctx.send_moar_txs(block_hash, 3, False)
        sent_txs = True
    elif mode == 'manytx':
        if ctx.get_balances() == ctx.expected_balances:
            ctx.send_moar_txs(block_hash, 3, False)
            logger.info(f'Sending moar txs at height {height}')

if mode == 'onetx':
    assert ctx.get_balances() == ctx.expected_balances

node2 = spin_up_node(config, near_root, node_dirs[2], 2, boot_node=boot_node)
tracker = utils.LogTracker(node2)
time.sleep(3)

catch_up_height = 0
for height, block_hash in utils.poll_blocks(boot_node,
                                            timeout=TIMEOUT,
                                            poll_interval=0.1):
    catch_up_height = height
    if height >= observed_height:
        break
    if mode == 'manytx' and ctx.get_balances() == ctx.expected_balances:
        boot_height = boot_node.get_latest_block().height
        ctx.send_moar_txs(block_hash, 3, False)
        logger.info(f'Sending moar txs at height {boot_height}')

boot_heights = boot_node.get_all_heights()

assert catch_up_height in boot_heights, "%s not in %s" % (catch_up_height,
                                                          boot_heights)

tracker.reset(
)  # the transition might have happened before we initialized the tracker
if catch_up_height >= 100:
    assert tracker.check("transition to State Sync")
elif catch_up_height <= 30:
    assert not tracker.check("transition to State Sync")

if mode == 'manytx':
    while ctx.get_balances() != ctx.expected_balances:
        assert time.time() - started < TIMEOUT
        logger.info(
            "Waiting for the old node to catch up. Current balances: %s; Expected balances: %s"
            % (ctx.get_balances(), ctx.expected_balances))
        time.sleep(1)

    # requery the balances from the newly started node
    ctx.nodes.append(node2)
    ctx.act_to_val = [2, 2, 2]

    while ctx.get_balances() != ctx.expected_balances:
        assert time.time() - started < TIMEOUT
        logger.info(
            "Waiting for the new node to catch up. Current balances: %s; Expected balances: %s"
            % (ctx.get_balances(), ctx.expected_balances))
        time.sleep(1)

logger.info('EPIC')

'''
'''--- pytest/tests/sanity/state_sync1.py ---
#!/usr/bin/env python3
# Spins up two out of three validating nodes. Waits until they reach height 40.
# Start the last validating node and check that the second node can sync up before
# the end of epoch and produce blocks and chunks.

import sys, time
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import start_cluster
from configured_logger import logger
import state_sync_lib
import utils

BLOCK_WAIT = 40
EPOCH_LENGTH = 80

node_config = state_sync_lib.get_state_sync_config_combined()

nodes = start_cluster(
    4, 0, 1, None,
    [["epoch_length", EPOCH_LENGTH], ["block_producer_kickout_threshold", 10],
     ["chunk_producer_kickout_threshold", 10]],
    {x: node_config for x in range(4)})
time.sleep(2)
nodes[1].kill()

logger.info("step 1")
utils.wait_for_blocks(nodes[0], target=BLOCK_WAIT)
nodes[1].start(boot_node=nodes[1])
time.sleep(2)

logger.info("step 2")
synced = False
block_height0 = block_height1 = -1
while block_height0 <= EPOCH_LENGTH and block_height1 <= EPOCH_LENGTH:
    block_height0, block_hash0 = nodes[0].get_latest_block()
    block_height1, block_hash1 = nodes[1].get_latest_block()
    if block_height0 > BLOCK_WAIT:
        if block_height0 > block_height1:
            try:
                nodes[0].get_block(block_hash1)
                if synced and abs(block_height0 - block_height1) >= 5:
                    assert False, "Nodes fall out of sync"
                synced = abs(block_height0 - block_height1) < 5
            except Exception:
                pass
        else:
            try:
                nodes[1].get_block(block_hash0)
                if synced and abs(block_height0 - block_height1) >= 5:
                    assert False, "Nodes fall out of sync"
                synced = abs(block_height0 - block_height1) < 5
            except Exception:
                pass
    time.sleep(1)

if not synced:
    assert False, "Nodes are not synced"

validator_info = nodes[0].json_rpc('validators', 'latest')
if len(validator_info['result']['next_validators']) < 2:
    assert False, "Node 1 did not produce enough blocks"

for i in range(2):
    account0 = nodes[0].get_account("test%s" % i)['result']
    account1 = nodes[1].get_account("test%s" % i)['result']
    print(account0, account1)
    assert account0 == account1, "state diverged"

'''
'''--- pytest/tests/sanity/state_sync2.py ---
#!/usr/bin/env python3
# Spins up two block producing nodes. Uses a large number of block producer seats to ensure
# both block producers are validating both shards.
# Gets to 105 blocks and nukes + wipes one of the block producers. Makes sure it can recover
# and sync

import sys, time
import fcntl
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import start_cluster
from configured_logger import logger
import state_sync_lib
import utils

fcntl.fcntl(1, fcntl.F_SETFL, 0)  # no cache when execute from nightly runner

BLOCKS = 105  # should be enough to trigger state sync for node 1 later, see comments there

nightly = len(sys.argv) > 1

node_config = state_sync_lib.get_state_sync_config_combined()

nodes = start_cluster(
    2, 0, 2, None, [["minimum_validators_per_shard", 2], ["epoch_length", 10],
                    ["block_producer_kickout_threshold", 10],
                    ["chunk_producer_kickout_threshold", 10]],
    {x: node_config for x in range(4)}) if nightly else start_cluster(
        2, 0, 2,
        None, [["num_block_producer_seats", 199],
               ["num_block_producer_seats_per_shard", [99, 100]],
               ["epoch_length", 10], ["block_producer_kickout_threshold", 10],
               ["chunk_producer_kickout_threshold", 10]],
        {x: node_config for x in range(4)})
logger.info('cluster started')

started = time.time()

logger.info(f'Waiting for {BLOCKS} blocks...')
height = utils.wait_for_blocks(nodes[1], target=BLOCKS)
logger.info(f'Got to {height} blocks, rebooting the first node')

nodes[0].kill()
nodes[0].reset_data()
tracker = utils.LogTracker(nodes[0])
nodes[0].start(boot_node=nodes[1])
time.sleep(3)

utils.wait_for_blocks(nodes[0], target=BLOCKS)

# make sure `nodes[0]` actually state synced
assert tracker.check("transition to State Sync")

'''
'''--- pytest/tests/sanity/state_sync3.py ---
#!/usr/bin/env python3
# Spin up one validating node and make it produce blocks for more than one epoch
# spin up another node that tracks the shard, make sure that it can state sync into the first node

import sys, time
import pathlib
import tempfile

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import start_cluster
from configured_logger import logger
import state_sync_lib
import utils

EPOCH_LENGTH = 1000
MAX_SYNC_WAIT = 120

(node_config_dump,
 node_config_sync) = state_sync_lib.get_state_sync_configs_pair()
node_config_dump["consensus.min_block_production_delay"] = {
    "secs": 0,
    "nanos": 100000000
}

state_parts_dir = str(pathlib.Path(tempfile.gettempdir()) / 'state_parts')

nodes = start_cluster(
    1, 1, 1, None,
    [["epoch_length", EPOCH_LENGTH], ["block_producer_kickout_threshold", 10],
     ["chunk_producer_kickout_threshold", 10]], {
         0: node_config_dump,
         1: node_config_sync,
     })
time.sleep(2)
nodes[1].kill()

logger.info("step 1")

node0_height, _ = utils.wait_for_blocks(nodes[0],
                                        target=EPOCH_LENGTH * 2 + 1,
                                        poll_interval=5)

nodes[1].start(boot_node=nodes[1])
time.sleep(2)

logger.info("step 2")
state_sync_done_time = None
state_sync_done_height = None
for node1_height, _ in utils.poll_blocks(nodes[1],
                                         timeout=MAX_SYNC_WAIT,
                                         poll_interval=2):
    if node1_height > node0_height:
        break
    if node1_height >= EPOCH_LENGTH:
        if state_sync_done_time is None:
            state_sync_done_time = time.time()
            state_sync_done_height = node1_height
        elif time.time() - state_sync_done_time > 8:
            assert node1_height > state_sync_done_height, "No progress after state sync is done"

'''
'''--- pytest/tests/sanity/state_sync4.py ---
#!/usr/bin/env python3
# Spin up one node and create some accounts and make them stake
# Spin up another node that syncs from the first node.
# Check that the second node doesn't crash (with trie node missing)
# during state sync.

import pathlib
import sys
import tempfile
import time

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import start_cluster
from configured_logger import logger
from key import Key
from transaction import sign_staking_tx, sign_create_account_with_full_access_key_and_balance_tx
import state_sync_lib
import utils

MAX_SYNC_WAIT = 30
EPOCH_LENGTH = 10

(node_config_dump,
 node_config_sync) = state_sync_lib.get_state_sync_configs_pair()

nodes = start_cluster(
    1, 1, 1, None,
    [["epoch_length", EPOCH_LENGTH], ["block_producer_kickout_threshold", 10],
     ["chunk_producer_kickout_threshold", 10]], {
         0: node_config_dump,
         1: node_config_sync,
     })
time.sleep(2)
nodes[1].kill()
logger.info('node1 is killed')

block_hash = nodes[0].get_latest_block().hash_bytes

num_new_accounts = 10
balance = 50000000000000000000000000000000
account_keys = []
for i in range(num_new_accounts):
    account_name = f'test_account{i}.test0'
    signer_key = Key(account_name, nodes[0].signer_key.pk,
                     nodes[0].signer_key.sk)
    create_account_tx = sign_create_account_with_full_access_key_and_balance_tx(
        nodes[0].signer_key, account_name, signer_key,
        balance // num_new_accounts, i + 1, block_hash)
    account_keys.append(signer_key)
    res = nodes[0].send_tx_and_wait(create_account_tx, timeout=15)
    assert 'error' not in res, res

latest_block = utils.wait_for_blocks(nodes[0], target=50)
cur_height = latest_block.height
block_hash = latest_block.hash_bytes

for signer_key in account_keys:
    staking_tx = sign_staking_tx(signer_key, nodes[0].validator_key,
                                 balance // (num_new_accounts * 2),
                                 cur_height * 1_000_000 - 1, block_hash)
    res = nodes[0].send_tx_and_wait(staking_tx, timeout=15)
    assert 'error' not in res

cur_height, _ = utils.wait_for_blocks(nodes[0], target=80)

logger.info('restart node1')
nodes[1].start(boot_node=nodes[1])
logger.info('node1 restarted')
time.sleep(3)

utils.wait_for_blocks(nodes[1], target=cur_height)

'''
'''--- pytest/tests/sanity/state_sync5.py ---
#!/usr/bin/env python3
# Spin up one validator node and let it run for a while
# Spin up another node that does state sync. Keep sending
# transactions to that node and make sure it doesn't crash.

import sys, time, base58
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import start_cluster, Key
from configured_logger import logger
import state_sync_lib
from transaction import sign_payment_tx
import utils

MAX_SYNC_WAIT = 30
EPOCH_LENGTH = 20

(node_config_dump,
 node_config_sync) = state_sync_lib.get_state_sync_configs_pair()

nodes = start_cluster(
    1, 1, 1, None,
    [["epoch_length", EPOCH_LENGTH], ["block_producer_kickout_threshold", 10],
     ["chunk_producer_kickout_threshold", 10]], {
         0: node_config_dump,
         1: node_config_sync,
     })
time.sleep(2)
nodes[1].kill()
logger.info('node1 is killed')

cur_height, _ = utils.wait_for_blocks(nodes[0], target=60)

genesis_block = nodes[0].json_rpc('block', [0])
genesis_hash = genesis_block['result']['header']['hash']
genesis_hash = base58.b58decode(genesis_hash.encode('ascii'))

nodes[1].start(boot_node=nodes[1])
tracker = utils.LogTracker(nodes[1])
time.sleep(1)

start_time = time.time()
node1_height = 0
nonce = 1
while node1_height <= cur_height:
    if time.time() - start_time > MAX_SYNC_WAIT:
        assert False, "state sync timed out"
    if nonce % 5 == 0:
        node1_height = nodes[1].get_latest_block(verbose=True).height
    tx = sign_payment_tx(nodes[0].signer_key, 'test1', 1, nonce, genesis_hash)
    nodes[1].send_tx(tx)
    nonce += 1
    time.sleep(0.05)

assert tracker.check('transition to State Sync')

'''
'''--- pytest/tests/sanity/state_sync_epoch_boundary.py ---
#!/usr/bin/env python3
# Spins up one validating node.
# Spins a non-validating node that tracks some shards and the set of tracked
# shards changes regularly.
# The node gets stopped, and gets restarted close to an epoch boundary but in a
# way to trigger state sync.
#
# This test is a regression test to ensure that the node doesn't panic during
# function execution during block sync after a state sync.

import pathlib
import random
import sys

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import init_cluster, spin_up_node, load_config, apply_config_changes
import account
import state_sync_lib
import transaction
import utils

from configured_logger import logger

EPOCH_LENGTH = 30

(node_config_dump,
 node_config_sync) = state_sync_lib.get_state_sync_configs_pair()
node_config_sync["tracked_shards"] = []
node_config_sync["tracked_shard_schedule"] = [[0], [0], [1], [1]]

config = load_config()
near_root, node_dirs = init_cluster(1, 1, 2, config,
                                    [["epoch_length", EPOCH_LENGTH]], {
                                        0: node_config_dump,
                                        1: node_config_sync
                                    })

boot_node = spin_up_node(config, near_root, node_dirs[0], 0)
logger.info('started boot_node')
node1 = spin_up_node(config, near_root, node_dirs[1], 1, boot_node=boot_node)
# State sync makes the storage look inconsistent.
node1.stop_checking_store()
logger.info('started node1')

contract = utils.load_test_contract()

latest_block_hash = boot_node.get_latest_block().hash_bytes
deploy_contract_tx = transaction.sign_deploy_contract_tx(
    boot_node.signer_key, contract, 10, latest_block_hash)
result = boot_node.send_tx_and_wait(deploy_contract_tx, 10)
assert 'result' in result and 'error' not in result, (
    'Expected "result" and no "error" in response, got: {}'.format(result))

latest_block_hash = boot_node.get_latest_block().hash_bytes
deploy_contract_tx = transaction.sign_deploy_contract_tx(
    node1.signer_key, contract, 10, latest_block_hash)
result = boot_node.send_tx_and_wait(deploy_contract_tx, 10)
assert 'result' in result and 'error' not in result, (
    'Expected "result" and no "error" in response, got: {}'.format(result))

# Generates traffic for all possible shards.
# Assumes that `test0`, `test1`, `near` all belong to different shards.
def random_workload_until(target, nonce, keys, target_node):
    last_height = -1
    while True:
        nonce += 1

        last_block = target_node.get_latest_block()
        height = last_block.height
        if height > target:
            break
        if height != last_height:
            logger.info(
                f'@{height}, epoch_height: {state_sync_lib.approximate_epoch_height(height, EPOCH_LENGTH)}'
            )
            last_height = height

        last_block_hash = boot_node.get_latest_block().hash_bytes
        if (len(keys) > 100 and random.random() < 0.2) or len(keys) > 1000:
            key = keys[random.randint(0, len(keys) - 1)]
            call_function('read', key, nonce, boot_node.signer_key,
                          last_block_hash)
            call_function('read', key, nonce, node1.signer_key, last_block_hash)
        elif random.random() < 0.5:
            if random.random() < 0.3:
                key_from, account_to = boot_node.signer_key, node1.signer_key.account_id
            elif random.random() < 0.3:
                key_from, account_to = boot_node.signer_key, "near"
            elif random.random() < 0.5:
                key_from, account_to = node1.signer_key, boot_node.signer_key.account_id
            else:
                key_from, account_to = node1.signer_key, "near"
            payment_tx = transaction.sign_payment_tx(key_from, account_to, 1,
                                                     nonce, last_block_hash)
            boot_node.send_tx(payment_tx).get('result')
        else:
            key = random_u64()
            keys.append(key)
            call_function('write', key, nonce, boot_node.signer_key,
                          last_block_hash)
            call_function('write', key, nonce, node1.signer_key,
                          last_block_hash)
    return (nonce, keys)

def random_u64():
    return bytes(random.randint(0, 255) for _ in range(8))

def call_function(op, key, nonce, signer_key, last_block_hash):
    if op == 'read':
        args = key
        fn = 'read_value'
    else:
        args = key + random_u64()
        fn = 'write_key_value'

    tx = transaction.sign_function_call_tx(signer_key, signer_key.account_id,
                                           fn, args, 300 * account.TGAS, 0,
                                           nonce, last_block_hash)
    return boot_node.send_tx(tx).get('result')

nonce, keys = random_workload_until(EPOCH_LENGTH - 5, 1, [], boot_node)

node1_height = node1.get_latest_block().height
logger.info(f'node1@{node1_height}')
node1.kill()
logger.info(f'killed node1')

# Run node0 more to trigger block sync in node1.
nonce, keys = random_workload_until(int(EPOCH_LENGTH * 2.7), nonce, keys,
                                    boot_node)

# Node1 is now behind and needs to do header sync and block sync.
node1.start(boot_node=boot_node)
node1_height = node1.get_latest_block().height
logger.info(f'started node1@{node1_height}')

nonce, keys = random_workload_until(int(EPOCH_LENGTH * 3.9), nonce, keys,
                                    boot_node)
boot_node_height = boot_node.get_latest_block().height
node1_height = node1.get_latest_block().height
assert node1_height + int(EPOCH_LENGTH * 0.5) >= boot_node_height

'''
'''--- pytest/tests/sanity/state_sync_fail.py ---
#!/usr/bin/env python3

# Spins up 2 nodes, waits until sharding is upgraded and spins up another node.
# Check that the node can't be started because it cannot state sync to the epoch
# after the sharding upgrade.

# Depending on the version of the binary (default or nightly) it will perform
# resharding from V0 (1 shard) to V1 (4 shards) or from V1 (4 shards) to V2 (5
# shards).

import pathlib
import sys
import time

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import init_cluster, spin_up_node, load_config, get_binary_protocol_version
from configured_logger import logger
import requests
import resharding_lib
import state_sync_lib
import utils

EPOCH_LENGTH = 20
START_AT_BLOCK = int(EPOCH_LENGTH * 2.5)

def get_genesis_config_changes(binary_protocol_version):
    genesis_config_changes = [
        ["min_gas_price", 0],
        ["max_inflation_rate", [0, 1]],
        ["epoch_length", EPOCH_LENGTH],
        ["block_producer_kickout_threshold", 80],
    ]

    resharding_lib.append_shard_layout_config_changes(
        genesis_config_changes,
        binary_protocol_version,
        logger,
    )

    return genesis_config_changes

config = load_config()

binary_protocol_version = get_binary_protocol_version(config)
assert binary_protocol_version is not None

node_config = state_sync_lib.get_state_sync_config_combined()

near_root, node_dirs = init_cluster(
    num_nodes=2,
    num_observers=1,
    num_shards=4,
    config=config,
    genesis_config_changes=get_genesis_config_changes(binary_protocol_version),
    client_config_changes={x: node_config for x in range(3)},
)

started = time.time()

boot_node = spin_up_node(config, near_root, node_dirs[0], 0)
node1 = spin_up_node(config, near_root, node_dirs[1], 1, boot_node=boot_node)

utils.wait_for_blocks(boot_node, target=START_AT_BLOCK)

node2 = spin_up_node(config, near_root, node_dirs[2], 2, boot_node=boot_node)
time.sleep(3)

try:
    logger.info("Checking node2 status. It should not be running.")
    status = node2.get_status()
    sys.exit("node 2 successfully started while it should fail")
except requests.exceptions.ConnectionError:
    pass

logger.info("Checking node2 exit reason.")
node2_correct_exit_reason = False
node2_stderr_path = pathlib.Path(node2.node_dir) / 'stderr'
with open(node2_stderr_path) as stderr_file:
    for line in stderr_file:
        if "cannot sync to the first epoch after sharding upgrade" in line:
            logger.info("Found the correct exit reason in node2 stderr.")
            node2_correct_exit_reason = True
            break

assert node2_correct_exit_reason

logger.info("Test finished.")

'''
'''--- pytest/tests/sanity/state_sync_late.py ---
#!/usr/bin/env python3
# Spins up a node, then waits for five+ epochs
# and spins up another node
# Makes sure that eventually the second node catches up
# Three modes:
#   - notx: no transactions are sent, just checks that
#     the second node starts and catches up
#   - onetx: sends one series of txs at the beginning,
#     makes sure the second node balances reflect them
#   - manytx: constantly issues txs throughout the test
#     makes sure the balances are correct at the end

import pathlib
import sys
import time

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

if len(sys.argv) < 2:
    logger.info("python state_sync.py [notx, onetx, manytx]")
    exit(1)

mode = sys.argv[1]
assert mode in ['notx', 'onetx', 'manytx']

from cluster import init_cluster, spin_up_node, load_config
from configured_logger import logger
import state_sync_lib
import utils

START_AT_BLOCK = 75
TIMEOUT = 150 + START_AT_BLOCK * 10

config = load_config()
node_config = state_sync_lib.get_state_sync_config_combined()

near_root, node_dirs = init_cluster(
    2, 1, 1, config,
    [["min_gas_price", 0], ["max_inflation_rate", [0, 1]], ["epoch_length", 10],
     ["block_producer_kickout_threshold", 80]],
    {x: node_config for x in range(3)})

started = time.time()

boot_node = spin_up_node(config, near_root, node_dirs[0], 0)
node1 = spin_up_node(config, near_root, node_dirs[1], 1, boot_node=boot_node)

ctx = utils.TxContext([0, 0], [boot_node, node1])

sent_txs = False

observed_height = 0
for observed_height, hash_ in utils.poll_blocks(boot_node,
                                                timeout=TIMEOUT,
                                                poll_interval=0.1):
    if mode == 'onetx' and not sent_txs:
        ctx.send_moar_txs(hash_, 3, False)
        sent_txs = True
    elif mode == 'manytx':
        if ctx.get_balances() == ctx.expected_balances:
            logger.info(f'Sending moar txs at height {observed_height}')
            ctx.send_moar_txs(hash_, 3, False)

if mode == 'onetx':
    assert ctx.get_balances() == ctx.expected_balances

node2 = spin_up_node(config, near_root, node_dirs[2], 2, boot_node=boot_node)
tracker = utils.LogTracker(node2)
time.sleep(3)

catch_up_height = 0
for catch_up_height, hash_ in utils.poll_blocks(node2,
                                                timeout=TIMEOUT,
                                                poll_interval=0.1):
    if catch_up_height >= observed_height:
        break

    boot_height = boot_node.get_latest_block().height
    if mode == 'manytx':
        if ctx.get_balances() == ctx.expected_balances:
            ctx.send_moar_txs(hash_, 3, False)
            logger.info(f'Sending moar txs at height {boot_height}')

boot_heights = boot_node.get_all_heights()

assert catch_up_height in boot_heights, "%s not in %s" % (catch_up_height,
                                                          boot_heights)

tracker.offset = 0  # the transition might have happened before we initialized the tracker
if catch_up_height >= 100:
    assert tracker.check("transition to State Sync")
elif catch_up_height <= 30:
    assert not tracker.check("transition to State Sync")

if mode == 'manytx':
    while ctx.get_balances() != ctx.expected_balances:
        assert time.time() - started < TIMEOUT
        logger.info(
            "Waiting for the old node to catch up. Current balances: %s; Expected balances: %s"
            % (ctx.get_balances(), ctx.expected_balances))
        time.sleep(1)

    # requery the balances from the newly started node
    ctx.nodes.append(node2)
    ctx.act_to_val = [2, 2, 2]

    while ctx.get_balances() != ctx.expected_balances:
        assert time.time() - started < TIMEOUT
        logger.info(
            "Waiting for the new node to catch up. Current balances: %s; Expected balances: %s"
            % (ctx.get_balances(), ctx.expected_balances))
        time.sleep(1)

'''
'''--- pytest/tests/sanity/state_sync_massive.py ---
#!/usr/bin/env python3
# Survive massive state sync
#
# Create 3 nodes, 1 validator and 2 observers tracking the single shard 0.
# Generate a large state using genesis-populate. [*]
#
# Spawn validator and first observer and wait for them to make some progress.
# Spawn second observer and watch how it is able to sync state
# without degrading blocks per second.
#
# To run this test is important to compile genesis-populate tool first.
# In nearcore folder run:
#
# ```
# cargo build -p genesis-populate
# ```
#
# [*] This test might take a very large time generating the state.
# To speed up this between multiple executions, large state can be generated once
# saved, and reused on multiple executions. Steps to do this.
#
# 1. Run test for first time:
#
# ```
# python3 tests/sanity/state_sync_massive.py
# ```
#
# Stop at any point after seeing the message: "Genesis generated"
#
# 2. Save generated data:
#
# ```
# cp -r ~/.near/test0_finished ~/.near/backup_genesis
# ```
#
# 3. Run test passing path to backup_genesis
#
# ```
# python3 tests/sanity/state_sync_massive.py ~/.near/backup_genesis
# ```
#

import logging
import pathlib
import requests
from subprocess import check_output
import sys
import time

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import init_cluster, spin_up_node, load_config
from populate import genesis_populate_all, copy_genesis
import state_sync_lib
from utils import LogTracker

logging.basicConfig(format='%(asctime)s %(message)s', level=logging.DEBUG)

if len(sys.argv) >= 2:
    genesis_data = sys.argv[1]
else:
    genesis_data = None
    additional_accounts = 200000

config = load_config()
node_config = state_sync_lib.get_state_sync_config_combined()
near_root, node_dirs = init_cluster(
    1, 2, 1,
    config, [["min_gas_price", 0], ["max_inflation_rate", [0, 1]],
             ["epoch_length", 300], ["block_producer_kickout_threshold", 80]],
    {x: node_config for x in range(3)})

logging.info("Populating genesis")

if genesis_data is None:
    genesis_populate_all(near_root, additional_accounts, node_dirs)
else:
    for node_dir in node_dirs:
        copy_genesis(genesis_data, node_dir)

logging.info("Genesis generated")

for node_dir in node_dirs:
    result = check_output(['ls', '-la', node_dir]).decode()
    logging.info(f'Node directory: {node_dir}')
    for line in result.split('\n'):
        logging.info(line)

SMALL_HEIGHT = 600
LARGE_HEIGHT = 660
TIMEOUT = 3600
start = time.time()

boot_node = spin_up_node(config, near_root, node_dirs[0], 0)
observer = spin_up_node(config, near_root, node_dirs[1], 1, boot_node=boot_node)

def wait_for_height(target_height, rpc_node, sleep_time=2, bps_threshold=-1):
    queue = []
    latest_height = 0

    while latest_height < target_height:
        assert time.time() - start < TIMEOUT

        # Check current height
        try:
            new_height = rpc_node.get_latest_block(check_storage=False).height
            logging.info(f"Height: {latest_height} => {new_height}")
            latest_height = new_height
        except requests.ReadTimeout:
            logging.info("Timeout Error")

        # Computing bps
        cur_time = time.time()
        queue.append((cur_time, latest_height))

        while len(queue) > 2 and queue[0][0] <= cur_time - 7:
            queue.pop(0)

        if len(queue) <= 1:
            bps = None
        else:
            head = queue[-1]
            tail = queue[0]
            bps = (head[1] - tail[1]) / (head[0] - tail[0])

        logging.info(f"bps: {bps} queue length: {len(queue)}")
        time.sleep(sleep_time)
        assert bps is None or bps >= bps_threshold

wait_for_height(SMALL_HEIGHT, boot_node)

observer = spin_up_node(config, near_root, node_dirs[2], 2, boot_node=boot_node)
tracker = LogTracker(observer)

# Check that bps is not degraded
wait_for_height(LARGE_HEIGHT, boot_node)

# Make sure observer2 is able to sync
wait_for_height(SMALL_HEIGHT, observer)

tracker.reset()
assert tracker.check("transition to State Sync")

'''
'''--- pytest/tests/sanity/state_sync_massive_validator.py ---
#!/usr/bin/env python3
"""
Survive massive state sync for validator

Create 4 nodes, 3 validators and 1 observers tracking the single shard 0.
Generate a large state using genesis-populate. [*]

Spawn everything and wait for them to make some progress.
Kill one of the validators, delete state and rerun it

To run this test is important to compile genesis-populate tool first.
In nearcore folder run:

```
cargo build -p genesis-populate
```

[*] This test might take a very large time generating the state.
To speed up this between multiple executions, large state can be generated once
saved, and reused on multiple executions. Steps to do this.

1. Run test for first time:

```
python3 tests/sanity/state_sync_massive_validator.py
```

Stop at any point after seeing the message: "Genesis generated"

2. Save generated data:

```
cp -r ~/.near/test0_finished ~/.near/backup_genesis
```

3. Run test passing path to backup_genesis

```
python3 tests/sanity/state_sync_massive_validator.py ~/.near/backup_genesis
```
"""

from subprocess import check_output
import logging
import pathlib
import requests
import sys
import time

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import init_cluster, spin_up_node, load_config
from populate import genesis_populate_all, copy_genesis
import state_sync_lib

logging.basicConfig(format='%(asctime)s %(message)s', level=logging.DEBUG)

if len(sys.argv) >= 2:
    genesis_data = sys.argv[1]
else:
    genesis_data = None
    additional_accounts = 200000

EPOCH_LENGTH = 300

config = load_config()
node_config = state_sync_lib.get_state_sync_config_combined()
near_root, node_dirs = init_cluster(
    3, 1, 1, config,
    [["min_gas_price", 0], ["max_inflation_rate", [0, 1]],
     ["epoch_length", EPOCH_LENGTH], ["block_producer_kickout_threshold", 0],
     ["chunk_producer_kickout_threshold", 0]],
    {x: node_config for x in range(4)})

logging.info("Populating genesis")

if genesis_data is None:
    genesis_populate_all(near_root, additional_accounts, node_dirs)
else:
    for node_dir in node_dirs:
        copy_genesis(genesis_data, node_dir)

logging.info("Genesis generated")

for node_dir in node_dirs:
    result = check_output(['ls', '-la', node_dir], text=True)
    logging.info(f'Node directory: {node_dir}')
    for line in result.split('\n'):
        logging.info(line)

INTERMEDIATE_HEIGHT = EPOCH_LENGTH + 10
SMALL_HEIGHT = EPOCH_LENGTH * 2 + 10
LARGE_HEIGHT = SMALL_HEIGHT + 50
TIMEOUT = 3600
start = time.time()

boot_node = spin_up_node(config, near_root, node_dirs[0], 0)
validator = spin_up_node(config,
                         near_root,
                         node_dirs[1],
                         1,
                         boot_node=boot_node)
delayed_validator = spin_up_node(config,
                                 near_root,
                                 node_dirs[2],
                                 2,
                                 boot_node=boot_node)
observer = spin_up_node(config, near_root, node_dirs[3], 3, boot_node=boot_node)

def wait_for_height(target_height, rpc_node, sleep_time=2, bps_threshold=-1):
    queue = []
    latest_height = 0

    while latest_height < target_height:
        assert time.time() - start < TIMEOUT

        # Check current height
        try:
            new_height = rpc_node.get_latest_block(check_storage=False,
                                                   timeout=10).height
            logging.info(f"Height: {latest_height} => {new_height}")
            latest_height = new_height
        except requests.ReadTimeout:
            logging.info("Timeout Error")

        # Computing bps
        cur_time = time.time()
        queue.append((cur_time, latest_height))

        while len(queue) > 2 and queue[0][0] <= cur_time - 7:
            queue.pop(0)

        if len(queue) <= 1:
            bps = None
        else:
            head = queue[-1]
            tail = queue[0]
            bps = (head[1] - tail[1]) / (head[0] - tail[0])

        logging.info(f"bps: {bps} queue length: {len(queue)}")
        time.sleep(sleep_time)
        assert bps is None or bps >= bps_threshold

wait_for_height(INTERMEDIATE_HEIGHT, validator)

delayed_validator.kill()
delayed_validator.reset_data()
delayed_validator.start(boot_node=boot_node)

# Check that bps is not degraded
wait_for_height(LARGE_HEIGHT, validator)

wait_for_height(SMALL_HEIGHT, delayed_validator)

'''
'''--- pytest/tests/sanity/state_sync_routed.py ---
#!/usr/bin/env python3
# Spins two block producers and two observers.
# Wait several epochs and spin up another observer that
# is blacklisted by both block producers.
#
# Make sure the new observer sync via routing through other observers.
#
# Three modes:
#   - notx: no transactions are sent, just checks that
#     the second node starts and catches up
#   - onetx: sends one series of txs at the beginning,
#     makes sure the second node balances reflect them
#   - manytx: constantly issues txs throughout the test
#     makes sure the balances are correct at the end

import pathlib
import sys
import time

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))
from configured_logger import logger
from cluster import init_cluster, spin_up_node, load_config
import state_sync_lib
import utils

if len(sys.argv) < 3:
    logger.info("python state_sync.py [notx, onetx, manytx] <launch_at_block>")
    exit(1)

mode = sys.argv[1]
assert mode in ['notx', 'onetx', 'manytx']

START_AT_BLOCK = int(sys.argv[2])
TIMEOUT = 150 + START_AT_BLOCK * 10

config = load_config()
node_config = state_sync_lib.get_state_sync_config_combined()

near_root, node_dirs = init_cluster(
    2, 3, 1, config,
    [["min_gas_price", 0], ["max_inflation_rate", [0, 1]], ["epoch_length", 10],
     ["block_producer_kickout_threshold", 80]],
    {x: node_config for x in range(5)})

started = time.time()

# First observer
node2 = spin_up_node(config, near_root, node_dirs[2], 2)
# Boot from observer since block producer will blacklist third observer
boot_node = node2

# Second observer
node3 = spin_up_node(config, near_root, node_dirs[3], 3, boot_node=boot_node)

# Spin up validators
node0 = spin_up_node(config,
                     near_root,
                     node_dirs[0],
                     0,
                     boot_node=boot_node,
                     blacklist=[4])
node1 = spin_up_node(config,
                     near_root,
                     node_dirs[1],
                     1,
                     boot_node=boot_node,
                     blacklist=[4])

ctx = utils.TxContext([0, 0], [node0, node1])

sent_txs = False

observed_height = 0
for observed_height, hash_ in utils.poll_blocks(boot_node,
                                                timeout=TIMEOUT,
                                                poll_interval=0.1):
    if observed_height >= START_AT_BLOCK:
        break
    if mode == 'onetx' and not sent_txs:
        ctx.send_moar_txs(hash_, 3, False)
        sent_txs = True
    elif mode == 'manytx' and ctx.get_balances() == ctx.expected_balances:
        ctx.send_moar_txs(hash_, 3, False)
        logger.info(f'Sending moar txs at height {observed_height}')

if mode == 'onetx':
    assert ctx.get_balances() == ctx.expected_balances

node4 = spin_up_node(config,
                     near_root,
                     node_dirs[4],
                     4,
                     boot_node=boot_node,
                     blacklist=[0, 1])

metrics4 = utils.MetricsTracker(node4)
time.sleep(3)

catch_up_height = 0
for catch_up_height, hash_ in utils.poll_blocks(node4,
                                                timeout=TIMEOUT,
                                                poll_interval=0.1):
    if catch_up_height >= observed_height:
        break
    assert time.time() - started < TIMEOUT, "Waiting for node 4 to catch up"
    new_height = node4.get_latest_block().height
    logger.info(f"Latest block at: {new_height}")
    if new_height > catch_up_height:
        catch_up_height = new_height
        logger.info(f"Last observer got to height {new_height}")

    boot_height = boot_node.get_latest_block().height

    if mode == 'manytx':
        if ctx.get_balances() == ctx.expected_balances:
            ctx.send_moar_txs(hash_, 3, False)
            logger.info(f"Sending moar txs at height {boot_height}")
    time.sleep(0.1)

boot_heights = boot_node.get_all_heights()

assert catch_up_height in boot_heights, "%s not in %s" % (catch_up_height,
                                                          boot_heights)

while True:
    assert time.time(
    ) - started < TIMEOUT, "Waiting for node 4 to connect to two peers"
    if metrics4.get_int_metric_value("near_peer_connections_total") == 2:
        break
    time.sleep(0.1)

if mode == 'manytx':
    while ctx.get_balances() != ctx.expected_balances:
        assert time.time() - started < TIMEOUT
        logger.info(
            "Waiting for the old node to catch up. Current balances: %s; Expected balances: %s"
            % (ctx.get_balances(), ctx.expected_balances))
        time.sleep(1)

    # requery the balances from the newly started node
    ctx.nodes.append(node4)
    ctx.act_to_val = [2, 2, 2]

    while ctx.get_balances() != ctx.expected_balances:
        assert time.time() - started < TIMEOUT
        logger.info(
            "Waiting for the new node to catch up. Current balances: %s; Expected balances: %s"
            % (ctx.get_balances(), ctx.expected_balances))
        time.sleep(1)

'''
'''--- pytest/tests/sanity/state_sync_then_catchup.py ---
#!/usr/bin/env python3
# Spins up one validating node.
# Spins a non-validating node that tracks some shards and the set of tracked shards changes regularly.
# The node gets stopped, and gets restarted close to an epoch boundary but in a way to trigger state sync.
#
# After the state sync the node has to do a catchup.
#
# Note that the test must generate outgoing receipts for most shards almost
# every block in order to crash if creation of partial encoded chunks becomes
# non-deterministic.

import pathlib
import random
import sys

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import init_cluster, spin_up_node, load_config, apply_config_changes
import account
import state_sync_lib
import transaction
import utils

from configured_logger import logger

EPOCH_LENGTH = 50

def random_u64():
    return bytes(random.randint(0, 255) for _ in range(8))

# Generates traffic for all possible shards.
# Assumes that `test0`, `test1`, `near` all belong to different shards.
def random_workload_until(target, nonce, keys, node0, node1, target_node):
    last_height = -1
    while True:
        nonce += 1

        last_block = target_node.get_latest_block()
        height = last_block.height
        if height > target:
            break
        if height != last_height:
            logger.info(
                f'@{height}, epoch_height: {state_sync_lib.approximate_epoch_height(height, EPOCH_LENGTH)}'
            )
            last_height = height

        last_block_hash = node0.get_latest_block().hash_bytes
        if random.random() < 0.5:
            # Make a transfer between shards.
            # The goal is to generate cross-shard receipts.
            key_from = random.choice([node0, node1]).signer_key
            account_to = random.choice([
                node0.signer_key.account_id, node1.signer_key.account_id, "near"
            ])
            payment_tx = transaction.sign_payment_tx(key_from, account_to, 1,
                                                     nonce, last_block_hash)
            node0.send_tx(payment_tx).get('result')
        elif (len(keys) > 100 and random.random() < 0.5) or len(keys) > 1000:
            # Do some flat storage reads, but only if we have enough keys populated.
            key = keys[random.randint(0, len(keys) - 1)]
            for node in [node0, node1]:
                call_function('read', key, nonce, node.signer_key,
                              last_block_hash, node0)
                call_function('read', key, nonce, node.signer_key,
                              last_block_hash, node0)
        else:
            # Generate some data for flat storage reads
            key = random_u64()
            keys.append(key)
            for node in [node0, node1]:
                call_function('write', key, nonce, node.signer_key,
                              last_block_hash, node0)
    return nonce, keys

def call_function(op, key, nonce, signer_key, last_block_hash, node):
    if op == 'read':
        args = key
        fn = 'read_value'
    else:
        args = key + random_u64()
        fn = 'write_key_value'

    tx = transaction.sign_function_call_tx(signer_key, signer_key.account_id,
                                           fn, args, 300 * account.TGAS, 0,
                                           nonce, last_block_hash)
    return node.send_tx(tx).get('result')

def main():
    node_config_dump, node_config_sync = state_sync_lib.get_state_sync_configs_pair(
    )
    node_config_sync["tracked_shard_schedule"] = [[0], [0], [1], [2], [3], [1],
                                                  [2], [3]]
    node_config_sync["tracked_shards"] = []

    config = load_config()
    near_root, node_dirs = init_cluster(1, 1, 4, config,
                                        [["epoch_length", EPOCH_LENGTH]], {
                                            0: node_config_dump,
                                            1: node_config_sync
                                        })

    boot_node = spin_up_node(config, near_root, node_dirs[0], 0)
    logger.info('started boot_node')
    node1 = spin_up_node(config,
                         near_root,
                         node_dirs[1],
                         1,
                         boot_node=boot_node)
    logger.info('started node1')
    # State sync makes the storage look inconsistent.
    node1.stop_checking_store()

    contract = utils.load_test_contract()

    latest_block_hash = boot_node.get_latest_block().hash_bytes
    deploy_contract_tx = transaction.sign_deploy_contract_tx(
        boot_node.signer_key, contract, 10, latest_block_hash)
    result = boot_node.send_tx_and_wait(deploy_contract_tx, 10)
    assert 'result' in result and 'error' not in result, (
        'Expected "result" and no "error" in response, got: {}'.format(result))

    latest_block_hash = boot_node.get_latest_block().hash_bytes
    deploy_contract_tx = transaction.sign_deploy_contract_tx(
        node1.signer_key, contract, 10, latest_block_hash)
    result = boot_node.send_tx_and_wait(deploy_contract_tx, 10)
    assert 'result' in result and 'error' not in result, (
        'Expected "result" and no "error" in response, got: {}'.format(result))

    nonce, keys = random_workload_until(EPOCH_LENGTH - 5, 1, [], boot_node,
                                        node1, boot_node)

    node1_height = node1.get_latest_block().height
    logger.info(f'node1@{node1_height}')
    node1.kill()
    logger.info(f'killed node1')

    # Run node0 more to trigger block sync in node1.
    nonce, keys = random_workload_until(int(EPOCH_LENGTH * 3), nonce, keys,
                                        boot_node, node1, boot_node)

    # Node1 is now behind and needs to do header sync and block sync.
    node1.start(boot_node=boot_node)
    node1_height = node1.get_latest_block().height
    logger.info(f'started node1@{node1_height}')

    nonce, keys = random_workload_until(int(EPOCH_LENGTH * 3.7), nonce, keys,
                                        boot_node, node1, node1)

if __name__ == "__main__":
    main()

'''
'''--- pytest/tests/sanity/switch_node_key.py ---
#!/usr/bin/env python3
# Spin up two validating nodes. Stop one of them after one epoch, switch node key (peer id), and restart.
# Make sure that both node can still produce blocks.

import sys, time, base58, nacl.bindings
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import start_cluster
from key import Key
import utils

EPOCH_LENGTH = 40
STOP_HEIGHT1 = 35
TIMEOUT = 50

config1 = {"network": {"ttl_account_id_router": {"secs": 1, "nanos": 0},}}
nodes = start_cluster(
    2, 0, 1, None,
    [["epoch_length", EPOCH_LENGTH], ["block_producer_kickout_threshold", 30],
     ["chunk_producer_kickout_threshold", 30], ["num_block_producer_seats", 4],
     ["num_block_producer_seats_per_shard", [4]],
     ["validators", 0, "amount", "150000000000000000000000000000000"],
     [
         "records", 0, "Account", "account", "locked",
         "150000000000000000000000000000000"
     ], ["total_supply", "3100000000000000000000000000000000"]], {1: config1})
time.sleep(2)

block = nodes[1].get_block(nodes[1].get_latest_block().height)
epoch_id = block['result']['header']['epoch_id']

utils.wait_for_blocks(nodes[1], target=STOP_HEIGHT1)

nodes[1].kill()
for height, _ in utils.poll_blocks(nodes[0], timeout=TIMEOUT):
    cur_block = nodes[0].get_block(height)
    if cur_block['result']['header']['epoch_id'] != epoch_id:
        break

seed = bytes([1] * 32)
public_key, secret_key = nacl.bindings.crypto_sign_seed_keypair(seed)
node_key = Key("",
               base58.b58encode(public_key).decode('utf-8'),
               base58.b58encode(secret_key).decode('utf-8'))
nodes[1].reset_node_key(node_key)
nodes[1].start(boot_node=nodes[0])
time.sleep(2)

utils.wait_for_blocks(nodes[1], target=EPOCH_LENGTH * 2 + 5)

validators = nodes[1].get_validators()
assert len(
    validators['result']['next_validators']
) == 2, f'unexpected number of validators, next validators: {validators["result"]["next_validators"]}'

'''
'''--- pytest/tests/sanity/sync_ban.py ---
#!/usr/bin/env python3
# Spin up a validator node and a nonvalidator node.
# Stop the nonvalidator node and wait until the validator node reach height 100
# sync the nonvalidator node with controlled message passing between nodes.
# If `should_ban` is true, this should cause the nonvalidator node to ban the validator node
# due to not receiving any headers.
# If `should_ban` is false, the nonvalidator node should be able to sync without banning the
# validator node despite the slow connection.

import sys, time, functools, asyncio
import pathlib
from multiprocessing import Value

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import start_cluster
from configured_logger import logger
from peer import *
from proxy import ProxyHandler
import utils

TIMEOUT = 300
EPOCH_LENGTH = 50

BAN_STRING = 'ban a peer: [^\n]*, for not providing enough headers'

class Handler(ProxyHandler):

    def __init__(self, *args, should_sync=None, should_ban=None, **kwargs):
        assert should_sync is not None
        self.should_sync = should_sync
        assert should_ban is not None
        self.should_ban = should_ban
        super().__init__(*args, **kwargs)

    async def handle(self, msg, fr, to):
        if msg is not None:
            if msg.enum == 'Block':
                loop = asyncio.get_running_loop()
                send = functools.partial(self.do_send_message, msg, 1)
                if self.should_sync.value:
                    loop.call_later(1, send)
                return False
            elif msg.enum == 'BlockRequest':
                loop = asyncio.get_running_loop()
                send = functools.partial(self.do_send_message, msg, 0)
                if self.should_sync.value:
                    loop.call_later(6, send)
                return False
            elif msg.enum == 'BlockHeaders':
                if self.should_ban:
                    return False
                loop = asyncio.get_running_loop()
                send = functools.partial(self.do_send_message, msg, 1)
                if self.should_sync.value:
                    loop.call_later(2, send)
                return False
        return True

if __name__ == '__main__':
    should_ban = sys.argv[1] == 'true'
    node0_config = {
        "consensus": {
            "min_block_production_delay": {
                "secs": 0,
                "nanos": 1000000000
            },
        },
    }
    node1_config = {
        "consensus": {
            "header_sync_initial_timeout": {
                "secs": 3,
                "nanos": 0
            },
            "header_sync_stall_ban_timeout": {
                "secs": 5,
                "nanos": 0
            }
        },
        "tracked_shards": [0]
    }

    should_sync = Value('i', False)
    nodes = start_cluster(
        1, 1, 1, None, [["epoch_length", EPOCH_LENGTH]], {
            0: node0_config,
            1: node1_config
        },
        functools.partial(Handler,
                          should_sync=should_sync,
                          should_ban=should_ban))

    utils.wait_for_blocks(nodes[0], target=30, poll_interval=2)

    should_sync.value = True

    logger.info("sync node 1")

    start = time.time()

    tracker0 = utils.LogTracker(nodes[0])
    tracker1 = utils.LogTracker(nodes[1])

    while True:
        assert time.time() - start < TIMEOUT

        if should_ban:
            if tracker1.check_re(BAN_STRING):
                break
        else:
            cur_height = nodes[0].get_latest_block().height
            node1_height = nodes[1].get_latest_block().height
            status1 = nodes[1].get_status()
            print(
                f"Sync: node 1 at block {node1_height}, node 0 at block {cur_height}; waiting for node 1 to catch up"
            )
            if (abs(node1_height - cur_height) < 5 and
                    status1['sync_info']['syncing'] is False):
                break
        time.sleep(2)

    if not should_ban and (tracker0.check_re(BAN_STRING) or
                           tracker1.check_re(BAN_STRING)):
        assert False, "unexpected ban of peers"

    # logger.info('shutting down')
    # time.sleep(10)

'''
'''--- pytest/tests/sanity/sync_chunks_from_archival.py ---
#!/usr/bin/env python3
# Spins up two nodes; Let's them build the chain for several epochs;
# Spins up two more nodes, and makes the two new nodes to stake, and the old two to unstake;
# Makes the two new nodes to build for couple more epochs;
# Spins up one more node. Makes sure it can sync.
# For the last node to be able to sync, it needs to learn to download chunks from the other
# archival nodes, as opposed to the validators of the corresponding epoch, since the validators
# of the old epochs are long gone by the time the archival node is syncing.

import sys, time, logging, base58
import multiprocessing
from functools import partial
import pathlib

from requests.api import request

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import init_cluster, spin_up_node, load_config
from configured_logger import logger
from messages.block import ShardChunkHeaderV1, ShardChunkHeaderV2, ShardChunkHeaderV3
from transaction import sign_staking_tx
from proxy import ProxyHandler, NodesProxy
import utils

TIMEOUT = 200
EPOCH_LENGTH = 10
HEIGHTS_BEFORE_ROTATE = 35
HEIGHTS_BEFORE_CHECK = 25

class Handler(ProxyHandler):

    def __init__(self,
                 *args,
                 hash_to_metadata={},
                 requests={},
                 responses={},
                 **kwargs):
        super().__init__(*args, **kwargs)
        self.hash_to_metadata = hash_to_metadata
        self.requests = requests
        self.responses = responses

    async def handle(self, msg, fr, to):
        if msg.enum == 'Routed':
            msg_kind = msg.Routed.body.enum
            if msg_kind == 'PartialEncodedChunk':
                header = msg.Routed.body.PartialEncodedChunk.header
                height = header.inner.height_created
                shard_id = header.inner.shard_id
                hash_ = header.chunk_hash()
                self.hash_to_metadata[hash_] = (height, shard_id)

            if msg_kind == 'VersionedPartialEncodedChunk':
                header = msg.Routed.body.VersionedPartialEncodedChunk.inner_header(
                )
                header_version = msg.Routed.body.VersionedPartialEncodedChunk.header_version(
                )
                if header_version == 'V3':
                    height = header.V2.height_created
                    shard_id = header.V2.shard_id
                else:
                    height = header.height_created
                    shard_id = header.shard_id

                if header_version == 'V1':
                    hash_ = ShardChunkHeaderV1.chunk_hash(header)
                elif header_version == 'V2':
                    hash_ = ShardChunkHeaderV2.chunk_hash(header)
                elif header_version == 'V3':
                    hash_ = ShardChunkHeaderV3.chunk_hash(header)
                self.hash_to_metadata[hash_] = (height, shard_id)

            if msg_kind == 'PartialEncodedChunkRequest':
                if fr == 4:
                    hash_ = msg.Routed.body.PartialEncodedChunkRequest.chunk_hash
                    assert hash_ in self.hash_to_metadata, "chunk hash %s is not present" % base58.b58encode(
                        hash_)
                    (height, shard_id) = self.hash_to_metadata[hash_]
                    logger.info("REQ %s %s %s %s" % (height, shard_id, fr, to))
                    self.requests[(height, shard_id, to)] = 1

            if msg_kind == 'PartialEncodedChunkResponse':
                if to == 4:
                    hash_ = msg.Routed.body.PartialEncodedChunkResponse.chunk_hash
                    (height, shard_id) = self.hash_to_metadata[hash_]
                    logger.info("RESP %s %s %s %s" % (height, shard_id, fr, to))
                    self.responses[(height, shard_id, fr)] = 1

        return True

# TODO(mina86): Make it a utility class
class Timeout:

    def __init__(self, sesconds: float) -> None:
        if sesconds <= 0:
            raise ValueError('sesconds must be positive')
        self.__start = time.monotonic()
        self.__end = self.__start + sesconds

    def check(self) -> bool:
        return time.monotonic() < self.__end

    def elapsed_seconds(self) -> float:
        return time.monotonic() - self.__start

    def left_seconds(self) -> float:
        return self.__end - time.monotonic()

if __name__ == '__main__':
    manager = multiprocessing.Manager()
    hash_to_metadata = manager.dict()
    requests = manager.dict()
    responses = manager.dict()

    proxy = NodesProxy(
        partial(Handler,
                hash_to_metadata=hash_to_metadata,
                requests=requests,
                responses=responses))

    timeout = Timeout(TIMEOUT)

    logging.basicConfig(format='%(asctime)s %(message)s', level=logging.INFO)

    config = load_config()
    near_root, node_dirs = init_cluster(
        2,
        3,
        2,
        config,
        [
            ["min_gas_price", 0],
            ["max_inflation_rate", [0, 1]],
            ["epoch_length", EPOCH_LENGTH],
            ['num_block_producer_seats', 4],
            ["block_producer_kickout_threshold", 20],
            ["chunk_producer_kickout_threshold", 20],
            ["validators", 0, "amount", "110000000000000000000000000000000"],
            ["validators", 1, "amount", "110000000000000000000000000000000"],
            [
                "records", 0, "Account", "account", "locked",
                "110000000000000000000000000000000"
            ],
            # each validator account is two records, thus the index of a record for the second is 2, not 1
            [
                "records", 2, "Account", "account", "locked",
                "110000000000000000000000000000000"
            ],
            ["total_supply", "6120000000000000000000000000000000"]
        ],
        {
            4: {
                "tracked_shards": [0, 1],
                "archive": True
            },
            3: {
                "archive": True,
                "tracked_shards": [1],
                "network": {
                    "ttl_account_id_router": {
                        "secs": 1,
                        "nanos": 0
                    }
                }
            },
            2: {
                "archive": True,
                "tracked_shards": [0],
                "network": {
                    "ttl_account_id_router": {
                        "secs": 1,
                        "nanos": 0
                    }
                }
            }
        })

    boot_node = spin_up_node(config, near_root, node_dirs[0], 0, proxy=proxy)
    node1 = spin_up_node(config,
                         near_root,
                         node_dirs[1],
                         1,
                         boot_node=boot_node,
                         proxy=proxy)

    def get_validators(node):
        return set([x['account_id'] for x in node.get_status()['validators']])

    logging.info(f'Getting to height {HEIGHTS_BEFORE_ROTATE}')
    utils.wait_for_blocks(boot_node,
                          target=HEIGHTS_BEFORE_ROTATE,
                          timeout=timeout.left_seconds())

    node2 = spin_up_node(config,
                         near_root,
                         node_dirs[2],
                         2,
                         boot_node=boot_node,
                         proxy=proxy)
    node3 = spin_up_node(config,
                         near_root,
                         node_dirs[3],
                         3,
                         boot_node=boot_node,
                         proxy=proxy)

    hash_ = boot_node.get_latest_block().hash_bytes

    logging.info("Waiting for the new nodes to sync")
    while True:
        if (not node2.get_status()['sync_info']['syncing'] and
                not node3.get_status()['sync_info']['syncing']):
            break
        time.sleep(1)

    for stake, nodes, expected_vals in [
        (100000000000000000000000000000000, [node2, node3],
         ["test0", "test1", "test2", "test3"]),
        (0, [boot_node, node1], ["test2", "test3"]),
    ]:
        logging.info("Rotating validators")
        for ord_, node in enumerate(reversed(nodes)):
            tx = sign_staking_tx(node.signer_key, node.validator_key, stake, 10,
                                 hash_)
            boot_node.send_tx(tx)

        logging.info("Waiting for rotation to occur")
        while True:
            assert timeout.check(), get_validators(boot_node)
            if set(get_validators(boot_node)) == set(expected_vals):
                break
            else:
                time.sleep(1)

    start_height = boot_node.get_latest_block().height

    logging.info("Killing old nodes")
    boot_node.kill()
    node1.kill()

    target = start_height + HEIGHTS_BEFORE_CHECK
    logging.info(f'Getting to height {target}')
    height_to_sync_to, _ = utils.wait_for_blocks(node2,
                                                 target=target,
                                                 timeout=timeout.left_seconds())

    logging.info("Spinning up one more node")
    node4 = spin_up_node(config, near_root, node_dirs[4], 4, boot_node=node2)

    logging.info('Waiting for the new node to sync.  '
                 f'We are {timeout.elapsed_seconds()} seconds in')
    while True:
        assert timeout.check()
        sync_info = node4.get_status()['sync_info']
        if not sync_info['syncing']:
            new_height = sync_info['latest_block_height']
            assert new_height > height_to_sync_to, (
                f'new height {new_height} height to sync to {height_to_sync_to}'
            )
            break
        time.sleep(1)

    logging.info("Checking the messages sent and received")

    # The first two blocks are certainly more than two epochs in the past
    # compared to head, and thus should be requested from archival nodes. Check
    # that it's the case.  Start from 10 to account for possibly skipped blocks
    # while the nodes were starting.
    for height in range(12, HEIGHTS_BEFORE_ROTATE):
        for shard in (0, 1):
            for node in (2, 3):
                other = 5 - node
                if (height, shard, node) in requests:
                    assert (height, shard, node) in responses, (height, shard,
                                                                node)
                    assert (height, shard,
                            other) not in requests, (height, shard, other)
                    assert (height, shard,
                            other) not in responses, (height, shard, other)
                    break
            else:
                assert False, f'Missing request for shard {shard} in block {height}'

    # The last 5 blocks with epoch_length=10 will certainly be in the same epoch
    # as head, or in the previous epoch, and thus should be requested from the
    # block producers.  Note that in our case block producers are also archival
    # nodes so we’re again checking nodes 2 and 3.
    for height in range(new_height - 5, new_height - 1):
        for shard in (0, 1):
            found = False
            for node in (2, 3):
                if (height, shard, node) in requests:
                    assert (height, shard, node) in responses, (height, shard,
                                                                node)
                    found = True
            assert found, f'Missing request for shard {shard} in block {height}'

    logging.info(f'Done.  Took {timeout.elapsed_seconds()} seconds')

'''
'''--- pytest/tests/sanity/transactions.py ---
#!/usr/bin/env python3
# Consists of a small sanity test that verifies that a single transaction
# gets properly processed (to simplify debugging when the code is completely
# broken). If one transaction goes through, sends batches of transactions
# and ensures the balances get to the expected state in a timely manner.
# Sets epoch length to 10

import sys, time, base58, random
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import start_cluster
from configured_logger import logger
from transaction import sign_payment_tx
import utils

TIMEOUT = 240

nodes = start_cluster(
    num_nodes=4,
    num_observers=1,
    num_shards=4,
    config=None,
    genesis_config_changes=[["min_gas_price",
                             0], ["max_inflation_rate", [0, 1]],
                            ["epoch_length", 10],
                            ["block_producer_kickout_threshold", 70]],
    client_config_changes={
        0: {
            "consensus": {
                "state_sync_timeout": {
                    "secs": 2,
                    "nanos": 0
                }
            }
        },
        1: {
            "consensus": {
                "state_sync_timeout": {
                    "secs": 2,
                    "nanos": 0
                }
            }
        },
        2: {
            "consensus": {
                "state_sync_timeout": {
                    "secs": 2,
                    "nanos": 0
                }
            }
        },
        3: {
            "consensus": {
                "state_sync_timeout": {
                    "secs": 2,
                    "nanos": 0
                }
            }
        },
        4: {
            "consensus": {
                "state_sync_timeout": {
                    "secs": 2,
                    "nanos": 0
                }
            },
            "tracked_shards": [0, 1, 2, 3]
        }
    })

started = time.time()

act_to_val = [4, 4, 4, 4, 4]

ctx = utils.TxContext(act_to_val, nodes)

last_balances = [x for x in ctx.expected_balances]

step = 0
sent_height = -1

height, hash_ = utils.wait_for_blocks(nodes[4], target=1, check_storage=False)
tx = sign_payment_tx(nodes[0].signer_key, 'test1', 100, 1,
                     base58.b58decode(hash_.encode('utf8')))
nodes[4].send_tx(tx)
ctx.expected_balances[0] -= 100
ctx.expected_balances[1] += 100
logger.info('Sent tx at height %s' % height)
sent_height = height

height, hash_ = utils.wait_for_blocks(nodes[4],
                                      target=sent_height + 6,
                                      check_storage=False)
cur_balances = ctx.get_balances()
assert cur_balances == ctx.expected_balances, "%s != %s" % (
    cur_balances, ctx.expected_balances)

# we are done with the sanity test, now let's stress it
for height, _ in utils.poll_blocks(nodes[4], timeout=TIMEOUT):
    if ctx.get_balances() == ctx.expected_balances:
        count = height - sent_height
        logger.info(f'Balances caught up, took {count} blocks, moving on')
        last_balances = [x for x in ctx.expected_balances]
        ctx.send_moar_txs(hash_, 10, use_routing=True)
        sent_height = height
    else:
        assert height <= sent_height + 10, ('Balances before: {before}\n'
                                            'Expected balances: {expected}\n'
                                            'Current balances: {current}\n'
                                            'Sent at height: {sent_at}\n'
                                            'Current height: {height}').format(
                                                before=last_balances,
                                                expected=ctx.expected_balances,
                                                current=ctx.get_balances(),
                                                sent_at=sent_height,
                                                height=height)
    if height >= 100:
        break

'''
'''--- pytest/tests/sanity/upgradable.py ---
#!/usr/bin/env python3
"""Test if the node is backwards compatible with the latest release."""

import base58
import json
import os
import pathlib
import re
import subprocess
import sys
import time
import typing
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

import branches
import cluster
from configured_logger import logger
from transaction import sign_deploy_contract_tx, sign_function_call_tx, sign_payment_tx
import utils

_EXECUTABLES = None

def get_executables() -> branches.ABExecutables:
    global _EXECUTABLES
    if _EXECUTABLES is None:
        _EXECUTABLES = branches.prepare_ab_test()
        logger.info(f"Latest mainnet release is {_EXECUTABLES.release}")
    return _EXECUTABLES

def test_protocol_versions() -> None:
    """Verify that mainnet, testnet and current protocol versions differ by ≤ 1.

    Checks whether the protocol versions used by the latest mainnet, the latest
    testnet and current binary do not differed by more than one.  Some protocol
    features implementations rely on the fact that no protocol version is
    skipped.  See <https://github.com/near/nearcore/issues/4956>.

    This test downloads the latest official mainnet and testnet binaries.  If
    that fails for whatever reason, builds each of those executables.
    """
    executables = get_executables()
    testnet = branches.get_executables_for('testnet')

    def get_proto_version(exe: pathlib.Path) -> int:
        line = subprocess.check_output((exe, '--version'), text=True)
        m = re.search(r'\(release (.*?)\) .* \(protocol ([0-9]+)\)', line)
        assert m, (f'Unable to extract protocol version number from {exe};\n'
                   f'Got {line.rstrip()} on standard output')
        return m.group(1), int(m.group(2))

    main_release, main_proto = get_proto_version(executables.stable.neard)
    test_release, test_proto = get_proto_version(testnet.neard)
    _, head_proto = get_proto_version(executables.current.neard)

    logger.info(f'Got protocol {main_proto} in mainnet release {main_release}.')
    logger.info(f'Got protocol {test_proto} in testnet release {test_release}.')
    logger.info(f'Got protocol {head_proto} on master branch.')

    ok = (head_proto in (test_proto, test_proto + 1) and
          test_proto in (main_proto, main_proto + 1))
    assert ok, ('If changed, protocol version of a new release can increase by '
                'at most one.')

def test_upgrade() -> None:
    """Test that upgrade from ‘stable’ to ‘current’ binary is possible.

    1. Start a network with 3 `stable` nodes and 1 `new` node.
    2. Start switching `stable` nodes one by one with `new` nodes.
    3. Run for three epochs and observe that current protocol version of the
       network matches `new` nodes.
    """
    executables = get_executables()
    node_root = utils.get_near_tempdir('upgradable', clean=True)

    # Setup local network.
    cmd = (executables.stable.neard, f'--home={node_root}', 'localnet', '-v',
           '4', '--prefix', 'test')
    logger.info(' '.join(str(arg) for arg in cmd))
    subprocess.check_call(cmd)
    genesis_config_changes = [("epoch_length", 20),
                              ("num_block_producer_seats", 10),
                              ("num_block_producer_seats_per_shard", [10]),
                              ("block_producer_kickout_threshold", 80),
                              ("chunk_producer_kickout_threshold", 80)]
    node_dirs = [os.path.join(node_root, 'test%d' % i) for i in range(4)]
    for i, node_dir in enumerate(node_dirs):
        cluster.apply_genesis_changes(node_dir, genesis_config_changes)
        cluster.apply_config_changes(node_dir, {'tracked_shards': [0]})

        # Adjust changes required since #7486.  This is needed because current
        # stable release populates the deprecated migration configuration options.
        # TODO(mina86): Remove this once we get stable release which doesn’t
        # populate those fields by default.
        config_path = pathlib.Path(node_dir) / 'config.json'
        data = json.loads(config_path.read_text(encoding='utf-8'))
        data.pop('db_migration_snapshot_path', None)
        data.pop('use_db_migration_snapshot', None)
        config_path.write_text(json.dumps(data), encoding='utf-8')

    # Start 3 stable nodes and one current node.
    config = executables.stable.node_config()
    nodes = [
        cluster.spin_up_node(config, executables.stable.root, node_dirs[0], 0)
    ]
    for i in range(1, 3):
        nodes.append(
            cluster.spin_up_node(config,
                                 executables.stable.root,
                                 node_dirs[i],
                                 i,
                                 boot_node=nodes[0]))
    config = executables.current.node_config()
    nodes.append(
        cluster.spin_up_node(config,
                             executables.current.root,
                             node_dirs[3],
                             3,
                             boot_node=nodes[0]))

    time.sleep(2)

    # deploy a contract
    hash = nodes[0].get_latest_block().hash_bytes
    tx = sign_deploy_contract_tx(nodes[0].signer_key,
                                 utils.load_test_contract(), 1, hash)
    res = nodes[0].send_tx_and_wait(tx, timeout=20)
    assert 'error' not in res, res

    # write some random value
    tx = sign_function_call_tx(nodes[0].signer_key,
                               nodes[0].signer_key.account_id,
                               'write_random_value', [], 10**13, 0, 2, hash)
    res = nodes[0].send_tx_and_wait(tx, timeout=20)
    assert 'error' not in res, res
    assert 'Failure' not in res['result']['status'], res

    metrics_tracker = utils.MetricsTracker(nodes[3])

    count = 0
    for height, _ in utils.poll_blocks(nodes[0]):
        votes = metrics_tracker.get_metric_all_values(
            "near_protocol_version_votes")
        next = metrics_tracker.get_int_metric_value(
            "near_protocol_version_next")

        print(f"#{height}: {votes} -> {next}")

        count += 1
        if count > 20:
            break

    # Restart stable nodes into new version.
    for i in range(3):
        nodes[i].kill()
        nodes[i].near_root = executables.current.root
        nodes[i].binary_name = executables.current.neard
        nodes[i].start(
            boot_node=nodes[0],
            extra_env={"NEAR_TESTS_IMMEDIATE_PROTOCOL_UPGRADE": "1"},
        )

    count = 0
    for height, _ in utils.poll_blocks(nodes[3]):
        votes = metrics_tracker.get_metric_all_values(
            "near_protocol_version_votes")
        next = metrics_tracker.get_int_metric_value(
            "near_protocol_version_next")

        print(f"#{height}: {votes} -> {next}")

        count += 1
        if count > 60:
            break

    status0 = nodes[0].get_status()
    status3 = nodes[3].get_status()
    protocol_version = status0['protocol_version']
    latest_protocol_version = status3["latest_protocol_version"]
    assert protocol_version == latest_protocol_version, \
        "Latest protocol version %d should match active protocol version %d" % (
        latest_protocol_version, protocol_version)

    hash = base58.b58decode(
        status0['sync_info']['latest_block_hash'].encode('ascii'))

    # write some random value again
    tx = sign_function_call_tx(nodes[0].signer_key,
                               nodes[0].signer_key.account_id,
                               'write_random_value', [], 10**13, 0, 4, hash)
    res = nodes[0].send_tx_and_wait(tx, timeout=20)
    assert 'error' not in res, res
    assert 'Failure' not in res['result']['status'], res

    # hex_account_id = (b"I'm hex!" * 4).hex()
    hex_account_id = '49276d206865782149276d206865782149276d206865782149276d2068657821'
    tx = sign_payment_tx(key=nodes[0].signer_key,
                         to=hex_account_id,
                         amount=10**25,
                         nonce=5,
                         blockHash=hash)
    res = nodes[0].send_tx_and_wait(tx, timeout=20)
    # Successfully created a new account on transfer to hex
    assert 'error' not in res, res
    assert 'Failure' not in res['result']['status'], res

    hex_account_balance = int(
        nodes[0].get_account(hex_account_id)['result']['amount'])
    assert hex_account_balance == 10**25

def main():
    test_protocol_versions()
    test_upgrade()

if __name__ == "__main__":
    main()

'''
'''--- pytest/tests/sanity/validator_switch.py ---
#!/usr/bin/env python3
# Starts three validating nodes and one non-validating node
# Make the validating nodes unstake and the non-validating node stake
# so that the next epoch block producers set is completely different
# Make sure all nodes can still sync.

import sys, time, base58
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import start_cluster
from configured_logger import logger
from transaction import sign_staking_tx
import utils

EPOCH_LENGTH = 20
tracked_shards = {
    "tracked_shards": [0],  # Track all shards
    "state_sync_enabled": True,
    "store.state_snapshot_enabled": True
}

nodes = start_cluster(
    3, 1, 4, None,
    [["epoch_length", EPOCH_LENGTH], ["block_producer_kickout_threshold", 10],
     ["chunk_producer_kickout_threshold", 10]], {
         0: tracked_shards,
         1: tracked_shards
     })

time.sleep(3)

hash_ = nodes[0].get_latest_block().hash_bytes

for i in range(4):
    stake = 50000000000000000000000000000000 if i == 3 else 0
    tx = sign_staking_tx(nodes[i].signer_key, nodes[i].validator_key, stake, 1,
                         hash_)
    nodes[0].send_tx(tx)
    logger.info("test%s stakes %d" % (i, stake))

for cur_height, _ in utils.poll_blocks(nodes[0], poll_interval=1):
    if cur_height >= EPOCH_LENGTH * 2:
        break
    if cur_height > EPOCH_LENGTH + 1:
        info = nodes[0].json_rpc('validators', 'latest')
        count = len(info['result']['next_validators'])
        assert count == 1, 'Number of validators do not match'
        validator = info['result']['next_validators'][0]['account_id']
        assert validator == 'test3'

while cur_height <= EPOCH_LENGTH * 3:
    statuses = sorted((enumerate(node.get_latest_block() for node in nodes)),
                      key=lambda element: element[1].height)
    last = statuses.pop()
    cur_height = last[1].height
    node = nodes[last[0]]
    succeed = True
    for _, block in statuses:
        try:
            node.get_block(block.hash)
        except Exception:
            succeed = False
            break
    if statuses[0][1].height > EPOCH_LENGTH * 2 + 5 and succeed:
        sys.exit(0)

assert False, 'Nodes are not synced'

'''
'''--- pytest/tests/sanity/validator_switch_key.py ---
#!/usr/bin/env python3
# Starts two validating nodes and one non-validating node
# Set a new validator key that has the same account id as one of
# the validating nodes. Stake that account with the new key
# and make sure that the network doesn't stall even after
# the non-validating node becomes a validator.

import sys, time, base58
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import start_cluster
from key import Key
from transaction import sign_staking_tx

EPOCH_LENGTH = 30
TIMEOUT = 200

client_config = {
    "network": {
        "ttl_account_id_router": {
            "secs": 0,
            "nanos": 100000000
        }
    }
}
nodes = start_cluster(
    2, 1, 1, None,
    [["epoch_length", EPOCH_LENGTH], ["block_producer_kickout_threshold", 10],
     ["chunk_producer_kickout_threshold", 10]], {
         1: client_config,
         2: client_config
     })
time.sleep(2)

nodes[2].kill()

validator_key = Key(nodes[1].validator_key.account_id, nodes[2].signer_key.pk,
                    nodes[2].signer_key.sk)
nodes[2].reset_validator_key(validator_key)
nodes[2].reset_data()
nodes[2].start(boot_node=nodes[0])
time.sleep(3)

block = nodes[0].get_latest_block()
block_height = block.height
block_hash = block.hash_bytes

tx = sign_staking_tx(nodes[1].signer_key, validator_key,
                     50000000000000000000000000000000, 1, block_hash)
res = nodes[0].send_tx_and_wait(tx, timeout=15)
assert 'error' not in res

start_time = time.time()
while True:
    assert time.time() - start_time < TIMEOUT, 'Validators got stuck'
    node1_height = nodes[1].get_latest_block().height
    node2_height = nodes[2].get_latest_block().height
    if (node1_height > block_height + 4 * EPOCH_LENGTH and
            node2_height > block_height + 4 * EPOCH_LENGTH):
        break
    time.sleep(2)

'''
'''--- pytest/tests/shardnet/__init__.py ---

'''
'''--- pytest/tests/shardnet/collect_ips.py ---
import argparse
import requests
import sys
from rc import pmap
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))
from configured_logger import logger

validators_found = {}
visited_nodes = set()
known_nodes = set()
next_to_visit = []
active_validators = set()
active_validators_height = 0

def learn_about_node(node_ip):
    if node_ip not in known_nodes:
        known_nodes.add(node_ip)
        next_to_visit.append(node_ip)

def visit_node(node_ip, timeout):
    # logger.info(f'Visiting node {node_ip}')

    visited_nodes.add(node_ip)
    assert node_ip in known_nodes
    global active_validators
    global active_validators_height

    account_id = None
    try:
        data = requests.get(f'http://{node_ip}:3030/status',
                            timeout=timeout).json()
        validators = set([x['account_id'] for x in data['validators']])
        height = int(data['sync_info']['latest_block_height'])
        if not active_validators or height > active_validators_height + 1000:
            active_validators = validators
        if 'validator_account_id' in data:
            account_id = data['validator_account_id']
            if account_id in active_validators and not node_ip in validators_found:
                validators_found[node_ip] = account_id
                logger.info(f'Scraped validator {account_id}')

        data = requests.get(f'http://{node_ip}:3030/network_info',
                            timeout=timeout).json()
        peer_id_to_account_id = {}
        for known_producer in data['known_producers']:
            peer_id_to_account_id[
                known_producer['peer_id']] = known_producer['account_id']
        for peer in data['active_peers']:
            ip = peer['addr'].split(':')[0]
            learn_about_node(ip)
            peer_id = peer['id']
            if peer_id in peer_id_to_account_id:
                account_id = peer_id_to_account_id[peer_id]
                if account_id in validators and not ip in validators_found:
                    validators_found[ip] = account_id
                    logger.info(f'Found a validator {account_id} in peers')
            # Note that peer['account_id'] is always 'null'

    except Exception as e:
        logger.exception(f'Error scraping {node_ip}')

def discover_ips(node_ip, timeout):
    learn_about_node(node_ip)

    while next_to_visit:
        to_visit = next_to_visit[:]
        logger.info(f'Will visit {len(to_visit)} nodes. '
                    f'Validators found: {len(validators_found)}. '
                    f'Visited nodes: {len(visited_nodes)}. '
                    f'Known nodes: {len(known_nodes)}.')
        next_to_visit.clear()
        pmap(lambda ip: visit_node(ip, timeout), to_visit)

    logger.info(f'Validators found: {len(validators_found)}.')
    logger.info(f'Visited nodes: {len(visited_nodes)}.')
    with open('validators.csv', 'w') as f:
        for ip in validators_found:
            f.write('%s,%s\n' % (validators_found[ip], ip))

if __name__ == '__main__':
    logger.info('Starting IP collector')
    parser = argparse.ArgumentParser(description='Run IP collector')
    parser.add_argument('--timeout', type=float, required=True)
    parser.add_argument('--node_ip', required=True)
    args = parser.parse_args()

    timeout = args.timeout
    assert timeout
    node_ip = args.node_ip
    assert node_ip

    discover_ips(node_ip, timeout)

'''
'''--- pytest/tests/shardnet/restake.py ---
import argparse
import shlex
import random
import sys
from retrying import retry, RetryError
from rc import pmap
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))
import mocknet
from configured_logger import logger

@retry(retry_on_result=bool, wait_fixed=2000, stop_max_attempt_number=3)
def create_account(node, near_pk, near_sk):
    node.machine.upload('tests/shardnet/scripts/create_account.sh',
                        '/home/ubuntu',
                        switch_user='ubuntu')
    s = '''
        bash /home/ubuntu/create_account.sh {near_pk} {near_sk} 1>/home/ubuntu/create_account.out 2>/home/ubuntu/create_account.err
    '''.format(near_pk=shlex.quote(near_pk), near_sk=shlex.quote(near_sk))
    logger.info(f'Creating an account on {node.instance_name}: {s}')
    result = node.machine.run('bash', input=s)
    if result.returncode != 0:
        logger.error(f'error running create_account.sh on {node.instance_name}')
    return result.returncode

def restart_restaked(node, delay_sec, near_pk, near_sk, need_create_accounts):
    if need_create_accounts and not node.instance_name.startswith(
            'shardnet-boot'):
        try:
            create_account(node, near_pk, near_sk)
        except RetryError:
            logger.error(
                f'Skipping stake step after errors running create_account.sh on {node.instance_name}'
            )
            return

    node.machine.upload('tests/shardnet/scripts/restaked.sh',
                        '/home/ubuntu',
                        switch_user='ubuntu')
    s = '''
        nohup bash ./restaked.sh {delay_sec} {stake_amount} 1>>/home/ubuntu/restaked.out 2>>/home/ubuntu/restaked.err </dev/null &
    '''.format(stake_amount=shlex.quote(str(random.randint(10**3, 10**5))),
               delay_sec=shlex.quote(str(delay_sec)))
    logger.info(f'Starting restaked on {node.instance_name}: {s}')
    node.machine.run('bash', input=s)

if __name__ == '__main__':
    logger.info('Starting restaker.')
    parser = argparse.ArgumentParser(description='Run restaker')
    parser.add_argument('--delay-sec', type=int, required=True)
    parser.add_argument('--near-pk', required=True)
    parser.add_argument('--near-sk', required=True)
    parser.add_argument('--create-accounts', default=False, action='store_true')
    args = parser.parse_args()

    delay_sec = args.delay_sec
    assert delay_sec
    near_pk = args.near_pk
    near_sk = args.near_sk
    need_create_accounts = args.create_accounts

    all_machines = mocknet.get_nodes(pattern='shardnet-')
    random.shuffle(all_machines)

    pmap(
        lambda machine: restart_restaked(machine, delay_sec, near_pk, near_sk,
                                         need_create_accounts), all_machines)

'''
'''--- pytest/tests/spec/network/peers_request.py ---
#!/usr/bin/env python3
"""
PeersRequest

Start one real node. Create a connection (conn0) to real node, send PeersRequest and wait for the response.
Create a new connection (conn1) to real node, send PeersRequest and wait for the response. In the latter
response there must exist an entry with information from the first connection that was established.
"""
import asyncio
import socket
import sys
import time
import pathlib

import nacl.signing

sys.path.append(str(pathlib.Path(__file__).resolve().parents[3] / 'lib'))
from cluster import start_cluster
from messages import schema
from peer import ED_PREFIX, connect, run_handshake, create_peer_request
from utils import obj_to_string

nodes = start_cluster(1, 0, 4, None, [], {})

async def main():
    key_pair_0 = nacl.signing.SigningKey.generate()
    conn0 = await connect(nodes[0].addr())
    await run_handshake(conn0,
                        nodes[0].node_key.pk,
                        key_pair_0,
                        listen_port=12345)
    peer_request = create_peer_request()
    await conn0.send(peer_request)
    response = await conn0.recv('PeersResponse')
    assert response.enum == 'PeersResponse', obj_to_string(response)

    key_pair_1 = nacl.signing.SigningKey.generate()
    conn1 = await connect(nodes[0].addr())
    await run_handshake(conn1,
                        nodes[0].node_key.pk,
                        key_pair_1,
                        listen_port=12346)
    peer_request = create_peer_request()
    await conn1.send(peer_request)
    response = await conn1.recv('PeersResponse')
    assert response.enum == 'PeersResponse', obj_to_string(response)
    assert any(peer_info.addr.V4[1] == 12345
               for peer_info in response.PeersResponse), obj_to_string(response)

asyncio.run(main())

'''
'''--- pytest/tools/mirror/mirror_utils.py ---
import sys
import time
import base58
import atexit
import json
import os
import pathlib
import shutil
import signal
import subprocess

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import init_cluster, spin_up_node, load_config
from configured_logger import logger
import transaction
import utils
import key

MIRROR_DIR = 'test-mirror'
TIMEOUT = 240
TARGET_VALIDATORS = ['foo0', 'foo1', 'foo2']

CONTRACT_PATH = pathlib.Path(__file__).resolve().parents[
    0] / 'contract/target/wasm32-unknown-unknown/release/addkey_contract.wasm'

def mkdir_clean(dirname):
    try:
        dirname.mkdir()
    except FileExistsError:
        shutil.rmtree(dirname)
        dirname.mkdir()

def dot_near():
    return pathlib.Path.home() / '.near'

def ordinal_to_addr(port, ordinal):
    return f'0.0.0.0:{port + 10 + ordinal}'

def copy_genesis(home):
    shutil.copy(dot_near() / 'test0/forked/genesis.json', home / 'genesis.json')
    shutil.copy(dot_near() / 'test0/forked/records.json', home / 'records.json')

def init_target_dir(neard,
                    home,
                    ordinal,
                    boot_node_home,
                    validator_account=None):
    mkdir_clean(home)

    try:
        args = [neard, '--home', home, 'init']
        if validator_account is not None:
            args.extend(['--account-id', validator_account])
        subprocess.check_output(args, stderr=subprocess.STDOUT)
    except subprocess.CalledProcessError as e:
        sys.exit(f'"neard init" command failed: output: {e.stdout}')
    shutil.copy(dot_near() / 'test0/config.json', home / 'config.json')

    with open(home / 'config.json', 'r') as f:
        config = json.load(f)
        config['genesis_records_file'] = 'records.json'
        config['network']['addr'] = ordinal_to_addr(24567, ordinal)
        if boot_node_home is not None:
            config['network']['boot_nodes'] = read_addr_pk(boot_node_home)
        config['rpc']['addr'] = ordinal_to_addr(3030, ordinal)

    with open(home / 'config.json', 'w') as f:
        json.dump(config, f)

    if validator_account is None:
        os.remove(home / 'validator_key.json')

def init_target_dirs(neard, last_ordinal, target_validators):
    ordinal = last_ordinal + 1
    dirs = []

    for i in range(len(target_validators)):
        account_id = target_validators[i]
        if i > 0:
            boot_node_home = dirs[0]
        else:
            boot_node_home = None
        home = dot_near() / f'test_target_{account_id}'
        dirs.append(home)
        init_target_dir(neard,
                        home,
                        ordinal,
                        boot_node_home,
                        validator_account=account_id)
        ordinal += 1

    return dirs

def create_forked_chain(config, near_root, source_node_homes,
                        target_validators):
    mkdir_clean(dot_near() / MIRROR_DIR)
    binary_name = config.get('binary_name', 'neard')
    neard = os.path.join(near_root, binary_name)
    try:
        subprocess.check_output([
            neard, "--home",
            dot_near() / 'test0', "view-state", "dump-state", "--stream"
        ],
                                stderr=subprocess.STDOUT)
    except subprocess.CalledProcessError as e:
        sys.exit(f'"dump-state" command failed: output: {e.stdout}')
    try:
        subprocess.check_output([
            neard,
            'mirror',
            'prepare',
            '--records-file-in',
            dot_near() / 'test0/output/records.json',
            '--records-file-out',
            dot_near() / 'test0/output/mirror-records.json',
            '--secret-file-out',
            dot_near() / 'test0/output/mirror-secret.json',
        ],
                                stderr=subprocess.STDOUT)
    except subprocess.CalledProcessError as e:
        sys.exit(f'"mirror prepare" command failed: output: {e.stdout}')

    os.rename(source_node_homes[-1], dot_near() / f'{MIRROR_DIR}/source')
    ordinal = len(source_node_homes) - 1
    with open(dot_near() / f'{MIRROR_DIR}/source/config.json', 'r') as f:
        config = json.load(f)
        config['network']['boot_nodes'] = read_addr_pk(source_node_homes[0])
        config['network']['addr'] = ordinal_to_addr(24567, ordinal)
        config['rpc']['addr'] = ordinal_to_addr(3030, ordinal)
    with open(dot_near() / f'{MIRROR_DIR}/source/config.json', 'w') as f:
        json.dump(config, f)

    dirs = init_target_dirs(neard, ordinal, target_validators)

    target_dir = dot_near() / f'{MIRROR_DIR}/target'
    init_target_dir(neard,
                    target_dir,
                    len(source_node_homes) + len(dirs),
                    dirs[0],
                    validator_account=None)
    shutil.copy(dot_near() / 'test0/output/mirror-secret.json',
                target_dir / 'mirror-secret.json')

    os.mkdir(dot_near() / 'test0/forked')
    genesis_file_in = dot_near() / 'test0/output/genesis.json'
    genesis_file_out = dot_near() / 'test0/forked/genesis.json'
    records_file_in = dot_near() / 'test0/output/mirror-records.json'
    records_file_out = dot_near() / 'test0/forked/records.json'

    validators = []
    for d in dirs:
        with open(d / 'validator_key.json') as f:
            key = json.load(f)
        validators.append({
            'account_id': key['account_id'],
            'public_key': key['public_key'],
            'amount': '700000000000000'
        })

    validators_file = dot_near() / 'test0/forked/validators.json'
    with open(validators_file, 'w') as f:
        json.dump(validators, f)

    try:
        # we want to set transaction-validity-period to a bigger number
        # because the mirror code sets the block hash on transactions up to a couple minutes
        # before sending them, and that can cause problems for the default localnet
        # setting of transaction_validity_period. Not really worth changing the code since
        # transaction_validity_period is large on mainnet and testnet anyway
        subprocess.check_output([
            neard, 'amend-genesis', '--genesis-file-in', genesis_file_in,
            '--records-file-in', records_file_in, '--genesis-file-out',
            genesis_file_out, '--records-file-out', records_file_out,
            '--validators', validators_file, '--chain-id', 'foonet',
            '--transaction-validity-period', '10000'
        ],
                                stderr=subprocess.STDOUT)
    except subprocess.CalledProcessError as e:
        sys.exit(f'"amend-genesis" command failed: output: {e.stdout}')

    for d in dirs:
        copy_genesis(d)
    copy_genesis(target_dir)

    return [str(d) for d in dirs]

def read_addr_pk(home):
    with open(os.path.join(home, 'config.json'), 'r') as f:
        config = json.load(f)
        addr = config['network']['addr']

    with open(os.path.join(home, 'node_key.json'), 'r') as f:
        k = json.load(f)
        public_key = k['public_key']

    return f'{public_key}@{addr}'

def mirror_cleanup(process):
    process.send_signal(signal.SIGINT)
    try:
        process.wait(5)
    except:
        process.kill()
        logger.error('can\'t kill mirror process')

# helper class so we can pass restart_once() as a callback to send_traffic()
class MirrorProcess:

    def __init__(self, near_root, source_home, online_source):
        self.online_source = online_source
        self.source_home = source_home
        self.neard = os.path.join(near_root, 'neard')
        self.start()
        self.start_time = time.time()
        self.restarted = False

    def start(self):
        env = os.environ.copy()
        env["RUST_LOG"] = "actix_web=warn,mio=warn,tokio_util=warn,actix_server=warn,actix_http=warn,indexer=info," + env.get(
            "RUST_LOG", "debug")
        config_path = dot_near() / f'{MIRROR_DIR}/config.json'
        with open(dot_near() / f'{MIRROR_DIR}/stdout', 'ab') as stdout, \
            open(dot_near() / f'{MIRROR_DIR}/stderr', 'ab') as stderr, \
            open(config_path, 'w') as mirror_config:
            json.dump({'tx_batch_interval': {
                'secs': 0,
                'nanos': 600000000
            }}, mirror_config)
            args = [
                self.neard,
                'mirror',
                'run',
                "--source-home",
                self.source_home,
                "--target-home",
                dot_near() / f'{MIRROR_DIR}/target/',
                '--secret-file',
                dot_near() / f'{MIRROR_DIR}/target/mirror-secret.json',
                '--config-path',
                config_path,
            ]
            if self.online_source:
                args.append('--online-source')
            self.process = subprocess.Popen(args,
                                            stdin=subprocess.DEVNULL,
                                            stdout=stdout,
                                            stderr=stderr,
                                            env=env)
        logger.info("Started mirror process")
        atexit.register(mirror_cleanup, self.process)

    def restart(self):
        logger.info('stopping mirror process')
        self.process.terminate()
        self.process.wait()
        with open(dot_near() / f'{MIRROR_DIR}/stderr', 'ab') as stderr:
            stderr.write(
                b'<><><><><><><><><><><><> restarting <><><><><><><><><><><><><><><><><><><><>\n'
            )
            stderr.write(
                b'<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n'
            )
            stderr.write(
                b'<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n'
            )
        self.start()

    # meant to be used in the callback to send_traffic(). restarts the process once after 30 seconds
    def restart_once(self):
        code = self.process.poll()
        if code is not None:
            assert code == 0
            return False

        if not self.restarted and time.time() - self.start_time > 30:
            self.restart()
            self.restarted = True
        return True

def check_target_validators(target_node):
    try:
        validators = target_node.get_validators()['result']
    except KeyError:
        return

    for v in validators['current_validators']:
        assert v['account_id'] in TARGET_VALIDATORS, v['account_id']
    for v in validators['next_validators']:
        assert v['account_id'] in TARGET_VALIDATORS, v['account_id']

# we'll test out adding an access key and then sending txs signed with it
# since that hits some codepaths we want to test
def send_add_access_key(node, key, target_key, nonce, block_hash):
    action = transaction.create_full_access_key_action(target_key.decoded_pk())
    tx = transaction.sign_and_serialize_transaction(target_key.account_id,
                                                    nonce, [action], block_hash,
                                                    key.account_id,
                                                    key.decoded_pk(),
                                                    key.decoded_sk())
    res = node.send_tx(tx)
    logger.info(
        f'sent add key tx for {target_key.account_id} {target_key.pk}: {res}')

def send_delete_access_key(node, key, target_key, nonce, block_hash):
    action = transaction.create_delete_access_key_action(
        target_key.decoded_pk())
    tx = transaction.sign_and_serialize_transaction(target_key.account_id,
                                                    nonce, [action], block_hash,
                                                    target_key.account_id,
                                                    key.decoded_pk(),
                                                    key.decoded_sk())
    res = node.send_tx(tx)
    logger.info(
        f'sent delete key tx for {target_key.account_id} {target_key.pk}: {res}'
    )

def create_subaccount(node, signer_key, nonce, block_hash):
    k = key.Key.from_random('foo.' + signer_key.account_id)
    actions = []
    actions.append(transaction.create_create_account_action())
    actions.append(transaction.create_full_access_key_action(k.decoded_pk()))
    actions.append(transaction.create_payment_action(10**29))
    # add an extra one just to exercise some more corner cases
    actions.append(
        transaction.create_full_access_key_action(
            key.Key.from_random(k.account_id).decoded_pk()))

    tx = transaction.sign_and_serialize_transaction(k.account_id, nonce,
                                                    actions, block_hash,
                                                    signer_key.account_id,
                                                    signer_key.decoded_pk(),
                                                    signer_key.decoded_sk())
    res = node.send_tx(tx)
    logger.info(f'sent create account tx for {k.account_id} {k.pk}: {res}')
    return k

def deploy_addkey_contract(node, signer_key, contract_path, nonce, block_hash):
    code = utils.load_binary_file(contract_path)
    tx = transaction.sign_deploy_contract_tx(signer_key, code, nonce,
                                             block_hash)
    res = node.send_tx(tx)
    logger.info(f'sent deploy contract tx for {signer_key.account_id}: {res}')

def call_addkey(node, signer_key, new_key, nonce, block_hash, extra_actions=[]):
    args = bytearray(json.dumps({'public_key': new_key.pk}), encoding='utf-8')

    # add a transfer action and the extra_actions to exercise some more code paths
    actions = [
        transaction.create_function_call_action('add_key', args, 10**14, 0),
        transaction.create_payment_action(123)
    ]
    actions.extend(extra_actions)
    tx = transaction.sign_and_serialize_transaction('test0', nonce, actions,
                                                    block_hash,
                                                    signer_key.account_id,
                                                    signer_key.decoded_pk(),
                                                    signer_key.decoded_sk())
    res = node.send_tx(tx)
    logger.info(f'called add_key for {new_key.account_id} {new_key.pk}: {res}')

def call_create_account(node, signer_key, account_id, public_key, nonce,
                        block_hash):
    args = json.dumps({'account_id': account_id, 'public_key': public_key})
    args = bytearray(args, encoding='utf-8')

    actions = [
        transaction.create_function_call_action('create_account', args, 10**14,
                                                10**24),
        transaction.create_payment_action(123)
    ]
    tx = transaction.sign_and_serialize_transaction('test0', nonce, actions,
                                                    block_hash,
                                                    signer_key.account_id,
                                                    signer_key.decoded_pk(),
                                                    signer_key.decoded_sk())
    res = node.send_tx(tx)
    logger.info(
        f'called create account contract for {account_id}, public key: {public_key}: {res}'
    )

def call_stake(node, signer_key, amount, public_key, nonce, block_hash):
    args = json.dumps({'amount': amount, 'public_key': public_key})
    args = bytearray(args, encoding='utf-8')

    actions = [
        transaction.create_function_call_action('stake', args, 10**13, 0),
    ]
    tx = transaction.sign_and_serialize_transaction(signer_key.account_id,
                                                    nonce, actions, block_hash,
                                                    signer_key.account_id,
                                                    signer_key.decoded_pk(),
                                                    signer_key.decoded_sk())
    res = node.send_tx(tx)
    logger.info(
        f'called stake contract for {signer_key.account_id} {amount} {public_key}: {res}'
    )

def contract_deployed(node, account_id):
    return 'error' not in node.json_rpc('query', {
        "request_type": "view_code",
        "account_id": account_id,
        "finality": "final"
    })

# a key that we added with an AddKey tx or implicit account transfer.
# just for nonce handling convenience
class AddedKey:

    def __init__(self, key):
        self.nonce = None
        self.key = key

    def send_if_inited(self, node, transfers, block_hash):
        if self.nonce is None:
            self.nonce = node.get_nonce_for_pk(self.key.account_id,
                                               self.key.pk,
                                               finality='final')
            if self.nonce is not None:
                logger.info(
                    f'added key {self.key.account_id} {self.key.pk} inited @ {self.nonce}'
                )

        if self.nonce is not None:
            for (receiver_id, amount) in transfers:
                self.nonce += 1
                tx = transaction.sign_payment_tx(self.key, receiver_id, amount,
                                                 self.nonce, block_hash)
                node.send_tx(tx)

    def account_id(self):
        return self.key.account_id

    def inited(self):
        return self.nonce is not None

class ImplicitAccount:

    def __init__(self):
        self.key = AddedKey(key.Key.implicit_account())

    def account_id(self):
        return self.key.account_id()

    def transfer(self, node, sender_key, amount, block_hash, nonce):
        tx = transaction.sign_payment_tx(sender_key, self.account_id(), amount,
                                         nonce, block_hash)
        node.send_tx(tx)
        logger.info(
            f'sent {amount} to initialize implicit account {self.account_id()}')

    def send_if_inited(self, node, transfers, block_hash):
        self.key.send_if_inited(node, transfers, block_hash)

    def inited(self):
        return self.key.inited()

def count_total_txs(node, min_height=0):
    total = 0
    h = node.get_latest_block().hash
    while True:
        block = node.get_block(h)['result']
        height = int(block['header']['height'])
        if height < min_height:
            return total

        for c in block['chunks']:
            if int(c['height_included']) == height:
                chunk = node.get_chunk(c['chunk_hash'])['result']
                total += len(chunk['transactions'])

        h = block['header']['prev_hash']
        if h == '11111111111111111111111111111111':
            return total

def allowed_run_time(target_node_dir, start_time, end_source_height):
    with open(os.path.join(target_node_dir, 'genesis.json'), 'r') as f:
        genesis_height = json.load(f)['genesis_height']
    with open(os.path.join(target_node_dir, 'config.json'), 'r') as f:
        delay = json.load(f)['consensus']['min_block_production_delay']
        block_delay = 10**9 * int(delay['secs']) + int(delay['nanos'])
        block_delay = block_delay / 10**9

    # start_time is the time the mirror binary was started. Give it 20 seconds to
    # sync and then 50% more than min_block_production_delay for each block between
    # the start and end points of the source chain. Not ideal to be basing a test on time
    # like this but there's no real strong guarantee on when the transactions should
    # make it on chain, so this is some kind of reasonable timeout

    return 20 + (end_source_height - genesis_height) * block_delay * 1.5

def check_num_txs(source_node, target_node):
    with open(os.path.join(target_node.node_dir, 'genesis.json'), 'r') as f:
        genesis_height = json.load(f)['genesis_height']

    total_source_txs = count_total_txs(source_node, min_height=genesis_height)
    total_target_txs = count_total_txs(target_node)
    assert total_source_txs <= total_target_txs, (total_source_txs,
                                                  total_target_txs)
    logger.info(
        f'passed. num source txs: {total_source_txs} num target txs: {total_target_txs}'
    )

# keeps info initialized during start_source_chain() for use in send_traffic()
class TrafficData:

    def __init__(self, num_accounts):
        self.nonces = [2] * num_accounts
        self.implicit_account = None
        self.keyless_account0 = 'keyless0.test0'
        self.keyless_account1 = 'keyless1.test0'

    def send_transfers(self, nodes, block_hash, skip_senders=None):
        for sender in range(len(self.nonces)):
            if skip_senders is not None and sender in skip_senders:
                continue
            receiver = (sender + 1) % len(self.nonces)
            receiver_id = nodes[receiver].signer_key.account_id

            tx = transaction.sign_payment_tx(nodes[sender].signer_key,
                                             receiver_id, 300,
                                             self.nonces[sender], block_hash)
            nodes[sender].send_tx(tx)
            self.nonces[sender] += 1

    def check_ok(self, source_node, target_node):
        keys = target_node.get_access_key_list(self.keyless_account0)
        assert len(keys['result']['keys']) > 0, keys
        keys = target_node.get_access_key_list(self.keyless_account1)
        assert len(keys['result']['keys']) > 0, keys
        check_num_txs(source_node, target_node)

def added_keys_send_transfers(nodes, added_keys, receivers, amount, block_hash):
    node_idx = 0
    for key in added_keys:
        key.send_if_inited(nodes[node_idx],
                           [(receiver, amount) for receiver in receivers],
                           block_hash)
        node_idx += 1
        node_idx %= len(nodes)

def start_source_chain(config, num_source_validators=3):
    # for now we need at least 2 because we're sending traffic for source_nodes[1].signer_key
    # Could fix that but for now this assert is fine
    assert num_source_validators >= 2

    if not os.path.exists(CONTRACT_PATH):
        sys.exit(
            'please build the addkey contract by running cargo build --target wasm32-unknown-unknown --release from the ./contract/ dir'
        )

    config_changes = {}
    for i in range(num_source_validators + 1):
        config_changes[i] = {"tracked_shards": [0, 1, 2, 3], "archive": True}

    config = load_config()
    near_root, source_node_dirs = init_cluster(
        num_nodes=num_source_validators,
        num_observers=1,
        num_shards=4,
        config=config,
        # set epoch length to a larger number because otherwise there
        # are often problems with validators getting kicked for missing
        # only one block or chunk
        genesis_config_changes=[
            ["epoch_length", 100],
        ],
        client_config_changes=config_changes)

    source_nodes = [spin_up_node(config, near_root, source_node_dirs[0], 0)]

    for i in range(1, num_source_validators):
        source_nodes.append(
            spin_up_node(config,
                         near_root,
                         source_node_dirs[i],
                         i,
                         boot_node=source_nodes[0]))
    traffic_data = TrafficData(len(source_nodes))

    traffic_data.implicit_account = ImplicitAccount()
    for height, block_hash in utils.poll_blocks(source_nodes[0],
                                                timeout=TIMEOUT):
        block_hash_bytes = base58.b58decode(block_hash.encode('utf8'))
        traffic_data.implicit_account.transfer(source_nodes[0],
                                               source_nodes[0].signer_key,
                                               10**24, block_hash_bytes,
                                               traffic_data.nonces[0])
        traffic_data.nonces[0] += 1

        deploy_addkey_contract(source_nodes[0], source_nodes[0].signer_key,
                               CONTRACT_PATH, traffic_data.nonces[0],
                               block_hash_bytes)
        traffic_data.nonces[0] += 1
        deploy_addkey_contract(source_nodes[0], source_nodes[1].signer_key,
                               CONTRACT_PATH, traffic_data.nonces[1],
                               block_hash_bytes)
        traffic_data.nonces[1] += 1
        break

    for height, block_hash in utils.poll_blocks(source_nodes[0],
                                                timeout=TIMEOUT):
        block_hash_bytes = base58.b58decode(block_hash.encode('utf8'))

        traffic_data.implicit_account.send_if_inited(source_nodes[0],
                                                     [('test2', height),
                                                      ('test3', height)],
                                                     block_hash_bytes)
        traffic_data.send_transfers(source_nodes, block_hash_bytes)

        if height > 12:
            break

    source_nodes[0].kill()
    target_node_dirs = create_forked_chain(config, near_root, source_node_dirs,
                                           TARGET_VALIDATORS)
    source_nodes[0].start(boot_node=source_nodes[1])
    return near_root, source_nodes, target_node_dirs, traffic_data

# callback will be called once for every iteration of the utils.poll_blocks()
# loop, and we break if it returns False
def send_traffic(near_root, source_nodes, traffic_data, callback):
    tip = source_nodes[1].get_latest_block()
    block_hash_bytes = base58.b58decode(tip.hash.encode('utf8'))
    start_source_height = tip.height

    subaccount_key = AddedKey(
        create_subaccount(source_nodes[1], source_nodes[0].signer_key,
                          traffic_data.nonces[0], block_hash_bytes))
    traffic_data.nonces[0] += 1

    k = key.Key.from_random('test0')
    new_key = AddedKey(k)
    send_add_access_key(source_nodes[1], source_nodes[0].signer_key, k,
                        traffic_data.nonces[0], block_hash_bytes)
    traffic_data.nonces[0] += 1

    test0_contract_key = key.Key.from_random('test0')
    test0_contract_extra_key = key.Key.from_random('test0')

    # here we are assuming that the deployed contract has landed since we called start_source_chain()
    # we will add an extra AddKey action to hit some more code paths
    call_addkey(source_nodes[1],
                source_nodes[0].signer_key,
                test0_contract_key,
                traffic_data.nonces[0],
                block_hash_bytes,
                extra_actions=[
                    transaction.create_full_access_key_action(
                        test0_contract_extra_key.decoded_pk())
                ])
    traffic_data.nonces[0] += 1

    test0_contract_key = AddedKey(test0_contract_key)
    test0_contract_extra_key = AddedKey(test0_contract_extra_key)

    test1_contract_key = key.Key.from_random('test1')

    call_addkey(source_nodes[1], source_nodes[1].signer_key, test1_contract_key,
                traffic_data.nonces[1], block_hash_bytes)
    traffic_data.nonces[1] += 1
    test1_contract_key = AddedKey(test1_contract_key)

    test0_subaccount_contract_key = AddedKey(key.Key.from_random('test0.test0'))
    call_create_account(source_nodes[1], source_nodes[0].signer_key,
                        test0_subaccount_contract_key.key.account_id,
                        test0_subaccount_contract_key.key.pk,
                        traffic_data.nonces[0], block_hash_bytes)
    traffic_data.nonces[0] += 1
    test1_subaccount_contract_key = AddedKey(key.Key.from_random('test1.test0'))
    call_create_account(source_nodes[1], source_nodes[1].signer_key,
                        test1_subaccount_contract_key.key.account_id,
                        test1_subaccount_contract_key.key.pk,
                        traffic_data.nonces[1], block_hash_bytes)
    traffic_data.nonces[1] += 1

    # here we create an account from a contract without adding an access key,
    # and then check that the mirror binary adds a full access key so we can control the account
    call_create_account(source_nodes[1], source_nodes[0].signer_key,
                        traffic_data.keyless_account0, None,
                        traffic_data.nonces[0], block_hash_bytes)
    traffic_data.nonces[0] += 1
    # now do the same thing but with signer different from the contract account
    # to exercise different code paths
    call_create_account(source_nodes[1], source_nodes[1].signer_key,
                        traffic_data.keyless_account1, None,
                        traffic_data.nonces[1], block_hash_bytes)
    traffic_data.nonces[1] += 1

    test0_deleted_height = None
    test0_readded_key = None
    implicit_added = None
    implicit_deleted = None
    implicit_account2 = ImplicitAccount()
    subaccount_contract_deployed = False
    subaccount_staked = False

    # here we are gonna send a tiny amount (1 yoctoNEAR) to the implicit account and
    # then wait a bit before properly initializing it. This hits a corner case where the
    # mirror binary needs to properly look for the second tx's outcome to find the starting
    # nonce because the first one failed
    implicit_account2.transfer(source_nodes[1], source_nodes[0].signer_key, 1,
                               block_hash_bytes, traffic_data.nonces[0])
    traffic_data.nonces[0] += 1
    time.sleep(2)
    implicit_account2.transfer(source_nodes[1], source_nodes[0].signer_key,
                               10**24, block_hash_bytes, traffic_data.nonces[0])
    traffic_data.nonces[0] += 1

    for height, block_hash in utils.poll_blocks(source_nodes[1],
                                                timeout=TIMEOUT):
        if not callback():
            break
        block_hash_bytes = base58.b58decode(block_hash.encode('utf8'))

        if test0_deleted_height is None:
            traffic_data.send_transfers(source_nodes, block_hash_bytes)
        else:
            traffic_data.send_transfers(source_nodes,
                                        block_hash_bytes,
                                        skip_senders=set([0]))

        traffic_data.implicit_account.send_if_inited(
            source_nodes[1], [('test2', height), ('test1', height),
                              (implicit_account2.account_id(), height)],
            block_hash_bytes)
        if not implicit_deleted:
            implicit_account2.send_if_inited(
                source_nodes[1],
                [('test2', height), ('test0', height),
                 (traffic_data.implicit_account.account_id(), height)],
                block_hash_bytes)
        keys = [
            new_key,
            subaccount_key,
            test0_contract_key,
            test0_contract_extra_key,
            test1_contract_key,
            test0_subaccount_contract_key,
            test1_subaccount_contract_key,
        ]
        added_keys_send_transfers(source_nodes, keys, [
            traffic_data.implicit_account.account_id(),
            implicit_account2.account_id(), 'test2', 'test3'
        ], height, block_hash_bytes)

        if implicit_added is None:
            # wait for 15 blocks after we started to get some "normal" traffic
            # from this implicit account that's closer to what we usually see from
            # these (most people aren't adding access keys to implicit accounts much).
            # then after that we add an access key and delete the original one to test
            # some more code paths
            if implicit_account2.inited(
            ) and height - start_source_height >= 15:
                k = key.Key.from_random(implicit_account2.account_id())
                implicit_added = AddedKey(k)
                send_add_access_key(source_nodes[1], implicit_account2.key.key,
                                    k, implicit_account2.key.nonce,
                                    block_hash_bytes)
                implicit_account2.key.nonce += 1
        else:
            implicit_added.send_if_inited(source_nodes[1], [('test0', height)],
                                          block_hash_bytes)
            if implicit_added.inited() and not implicit_deleted:
                send_delete_access_key(source_nodes[1], implicit_added.key,
                                       implicit_account2.key.key,
                                       implicit_added.nonce, block_hash_bytes)
                implicit_added.nonce += 1
                implicit_deleted = True

        if test0_deleted_height is None and new_key.inited(
        ) and height - start_source_height >= 15:
            send_delete_access_key(source_nodes[1], new_key.key,
                                   source_nodes[0].signer_key,
                                   new_key.nonce + 1, block_hash_bytes)
            new_key.nonce += 1
            test0_deleted_height = height

        if test0_readded_key is None and test0_deleted_height is not None and height - test0_deleted_height >= 5:
            send_add_access_key(source_nodes[1], new_key.key,
                                source_nodes[0].signer_key, new_key.nonce + 1,
                                block_hash_bytes)
            test0_readded_key = AddedKey(source_nodes[0].signer_key)
            new_key.nonce += 1

        if test0_readded_key is not None:
            test0_readded_key.send_if_inited(
                source_nodes[1], [('test3', height),
                                  (implicit_account2.account_id(), height)],
                block_hash_bytes)

        if subaccount_key.inited():
            if not subaccount_contract_deployed:
                subaccount_key.nonce += 1
                deploy_addkey_contract(source_nodes[0], subaccount_key.key,
                                       CONTRACT_PATH, subaccount_key.nonce,
                                       block_hash_bytes)
                subaccount_contract_deployed = True
            elif not subaccount_staked:
                if contract_deployed(source_nodes[0],
                                     subaccount_key.account_id()):
                    subaccount_key.nonce += 1
                    call_stake(source_nodes[0], subaccount_key.key, 10**28,
                               subaccount_key.key.pk, subaccount_key.nonce,
                               block_hash_bytes)
                    subaccount_staked = True

        if height - start_source_height >= 100:
            break

'''
'''--- pytest/tools/mirror/offline_test.py ---
#!/usr/bin/env python3

import sys
import time
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import spin_up_node, load_config
from configured_logger import logger

import mirror_utils

# This sets up an environment to test the tools/mirror process. It starts a localnet with a few validators
# and waits for some blocks to be produced. Then we fork the state and start a new chain from that, and
# send some traffic. After a while we stop the source chain nodes and start the target chain nodes,
# and run the mirror binary that should send the source chain traffic to the target chain

def main():
    config = load_config()

    near_root, source_nodes, target_node_dirs, traffic_data = mirror_utils.start_source_chain(
        config)

    # sleep for a bit to allow test0 to catch up after restarting before we send traffic
    time.sleep(5)
    mirror_utils.send_traffic(near_root, source_nodes, traffic_data,
                              lambda: True)

    target_nodes = [
        spin_up_node(config, near_root, target_node_dirs[i],
                     len(source_nodes) + 1 + i)
        for i in range(len(target_node_dirs))
    ]

    end_source_height = source_nodes[0].get_latest_block().height
    time.sleep(5)
    # we don't need these anymore
    for node in source_nodes[1:]:
        node.kill()

    mirror = mirror_utils.MirrorProcess(near_root,
                                        source_nodes[1].node_dir,
                                        online_source=False)

    total_time_allowed = mirror_utils.allowed_run_time(target_node_dirs[0],
                                                       mirror.start_time,
                                                       end_source_height)
    while True:
        time.sleep(5)

        mirror_utils.check_target_validators(target_nodes[0])

        # this will restart the binary one time during this test, and it will return false
        # when it exits on its own, which should happen once it finishes sending all the
        # transactions in its source chain (~/.near/test1/)
        if not mirror.restart_once():
            break
        elapsed = time.time() - mirror.start_time
        if elapsed > total_time_allowed:
            logger.warn(
                f'mirror process has not exited after {int(elapsed)} seconds. stopping the test now'
            )
            break

    traffic_data.check_ok(source_nodes[0], target_nodes[0])

if __name__ == '__main__':
    main()

'''
'''--- pytest/tools/mirror/online_test.py ---
#!/usr/bin/env python3

import sys
import time
import pathlib

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

from cluster import spin_up_node, load_config
from configured_logger import logger

import mirror_utils

# This sets up an environment to test the tools/mirror process. It starts a localnet with a few validators
# and waits for some blocks to be produced. Then we fork the state and start a new chain from that, and
# start the mirror process while the sending more traffic to the source chain

def main():
    config = load_config()

    near_root, source_nodes, target_node_dirs, traffic_data = mirror_utils.start_source_chain(
        config)

    target_nodes = [
        spin_up_node(config, near_root, target_node_dirs[i],
                     len(source_nodes) + 1 + i)
        for i in range(len(target_node_dirs))
    ]

    mirror = mirror_utils.MirrorProcess(near_root,
                                        mirror_utils.dot_near() /
                                        f'{mirror_utils.MIRROR_DIR}/source',
                                        online_source=True)
    mirror_utils.send_traffic(near_root, source_nodes, traffic_data,
                              mirror.restart_once)

    end_source_height = source_nodes[0].get_latest_block().height
    time.sleep(5)
    # we don't need these anymore
    for node in source_nodes[1:]:
        node.kill()

    total_time_allowed = mirror_utils.allowed_run_time(target_node_dirs[0],
                                                       mirror.start_time,
                                                       end_source_height)
    time_elapsed = time.time() - mirror.start_time
    if time_elapsed < total_time_allowed:
        time_left = total_time_allowed - time_elapsed
        logger.info(
            f'waiting for {int(time_left)} seconds to allow transactions to make it to the target chain'
        )
        time.sleep(time_left)
    traffic_data.check_ok(source_nodes[0], target_nodes[0])

if __name__ == '__main__':
    main()

'''
'''--- pytest/tools/prober/prober.py ---
#!/usr/bin/env python3
"""
Prober that is compatible with cloudprober.
Prober verifies that a random block since the genesis can be retrieved from a node.
Makes 3 separate RPC requests: Get genesis height, get head, get a block.
If the block contains chunks, then make one more RPC request to get a chunk.

Run like this:
./prober --url http://my.test.net.node:3030

"""

import argparse
import datetime
import pathlib
import random
import sys

from prober_util import *

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

def main():
    # log an empty line for cloudprober nice formatting
    logger.info('')
    logger.info('Running Prober')
    parser = argparse.ArgumentParser(description='Run a prober')
    parser.add_argument('--url', required=True)
    args = parser.parse_args()

    url = args.url

    # Determine the genesis height and the head height.
    genesis_height = get_genesis_height(url)
    head = get_head(url)
    if head < genesis_height:
        logger.error(
            f'head must be higher than genesis. Got {head} and {genesis_height}'
        )
        sys.exit(1)

    # Pick a random number and then try to lookup a block at that height.
    random_height = random.randint(genesis_height, head)
    attempt = 0
    while True:
        block = get_block(random_height, url)
        if block:
            break

        # Some blocks are really missing and there is no way to know if they are
        # missing because the node doesn't have history, or because the block
        # was skipped.
        if random_height == genesis_height:
            logger.error(
                f'Genesis block is missing. This is impossible {random_height}')
            sys.exit(1)

        random_height -= 1
        attempt += 1
        # Limit the number of attempts.
        if attempt > 10:
            logger.error(
                f'{attempt} consecutive blocks are missing. This is improbable. From {random_height + 1} to {random_height + attempt}'
            )
            sys.exit(1)

    # Lookup a chunk to make sure the node contains it.
    num_chunks = len(block['chunks'])
    logger.info(f'Block {random_height} contains {num_chunks} chunks')
    timestamp = block["header"]["timestamp"] // 10**9
    timestamp = datetime.datetime.fromtimestamp(timestamp)
    logger.info(f'Block {random_height} timestamp {timestamp}')
    if num_chunks > 0:
        chunk = random.choice(block['chunks'])
        get_chunk(chunk, url)

    logger.info('Success.')

if __name__ == '__main__':
    main()

'''
'''--- pytest/tools/prober/prober_split.py ---
#!/usr/bin/env python3
"""
Prober that is compatible with cloudprober.

The ProberSplit queries two nodes for blocks and chunks at random heights and
compares the results. The expectation is that the block and chunks at each
height will be identical even when fetched from two different nodes. It also
executes a contract view call on both nodes and compares the results.

The prober runs continuously for the duration specified in the command line
arguments. It runs at least one block and chunk check at a random height.

The intended goal of this prober is ensure that a legacy archival node and a
split storage archival node contain the same data.

Run like this:
./prober_split.py --chain-id testnet --split-url http://split.archival.node:3030 --duration-ms 20000

"""

import argparse
import datetime
import random
import sys
import subprocess
from datetime import datetime, timedelta

from prober_util import *

def check_genesis(legacy_url: str, split_url: str) -> int:
    legacy_genesis_height = get_genesis_height(legacy_url)
    split_genesis_height = get_genesis_height(split_url)

    if legacy_genesis_height != split_genesis_height:
        logger.error(
            "The genesis height is different. legacy: {}, split {}",
            legacy_genesis_height,
            split_genesis_height,
        )
        sys.exit(1)

    return legacy_genesis_height

def check_head(legacy_url: str, split_url: str, genesis_height: int) -> int:
    legacy_head_height = get_head(legacy_url)
    split_head_height = get_head(split_url)

    if legacy_head_height <= genesis_height:
        logger.error(
            '{} head must be higher than genesis. Got {} and {}',
            legacy_url,
            legacy_head_height,
            genesis_height,
        )
        sys.exit(1)

    if split_head_height <= genesis_height:
        logger.error(
            '{} head must be higher than genesis. Got {} and {}',
            split_url,
            split_head_height,
            genesis_height,
        )
        sys.exit(1)

    return min(legacy_head_height, split_head_height)

def check_blocks(legacy_url: str, split_url: str, height: int):
    logger.info(f"Checking blocks at height {height}.")

    legacy_block = get_block(height, legacy_url)
    split_block = get_block(height, split_url)

    if legacy_block != split_block:
        logger.error(
            f"Block check failed, the legacy block and the split block are different",
            f"\nlegacy block\n{pretty_print(legacy_block)}"
            f"\nsplit block\n{pretty_print(split_block)}")
        sys.exit(1)

    return legacy_block

def check_chunks(legacy_url: str, split_url: str, block):
    if block is None:
        return

    logger.info(f"Checking chunks.")
    for chunk in block['chunks']:
        legacy_chunk = get_chunk(chunk, legacy_url)
        split_chunk = get_chunk(chunk, split_url)

        if legacy_chunk != split_chunk:
            logger.error(
                f"Chunk check failed, the legacy chunk and the split chunk are different"
                f"\nlegacy chunk\n{pretty_print(legacy_chunk)}"
                f"\nsplit chunk\n{pretty_print(split_chunk)}")
            sys.exit(1)

def check_view_call(legacy_url, split_url):
    logger.info(f"Checking view call.")

    # This is the example contract function call from
    # https://docs.near.org/api/rpc/contracts#call-a-contract-function
    params = {
        "request_type": "call_function",
        "finality": "final",
        "account_id": "dev-1588039999690",
        "method_name": "get_num",
        "args_base64": "e30="
    }
    legacy_resp = json_rpc('query', params, legacy_url)
    split_resp = json_rpc('query', params, split_url)

    if legacy_resp['result']['result'] != split_resp['result']['result']:
        logger.error(
            f'View call check failed, the legacy response and the split response are different'
            f'\nlegacy response\n{legacy_resp}'
            f'\nsplit response\n{split_resp}')
        sys.exit(1)

# Query gcp for the archive nodes, pick a random one and return its url.
def get_random_legacy_url(chain_id):
    cmd = [
        'gcloud',
        'compute',
        'instances',
        'list',
    ]
    logger.info(" ".join(cmd))
    result = subprocess.run(cmd, text=True, capture_output=True)
    stdout = result.stdout
    lines = stdout.split('\n')
    pattern = f'{chain_id}-rpc-archive-public'
    lines = list(filter(lambda line: pattern in line, lines))
    line = random.choice(lines)
    tokens = line.split()
    external_ip = tokens[4]

    logger.info(f'Selected random legacy node - {external_ip}')
    return f'http://{external_ip}:3030'

def main():
    start_time = datetime.now()

    parser = argparse.ArgumentParser(
        description='Run a prober for split archival nodes')
    parser.add_argument('--chain-id', required=True, type=str)
    parser.add_argument('--split-url', required=True, type=str)
    parser.add_argument('--duration-ms', default=2000, type=int)
    parser.add_argument('--log-level', default="INFO")
    args = parser.parse_args()

    logger.setLevel(args.log_level)
    # log an empty line for cloudprober nice formatting
    logger.info('')
    logger.info('Running Prober Split')

    legacy_url = get_random_legacy_url(args.chain_id)
    split_url = args.split_url
    duration = timedelta(milliseconds=args.duration_ms)

    genesis_height = check_genesis(legacy_url, split_url)
    head = check_head(legacy_url, split_url, genesis_height)
    logger.info(f'The genesis height is {genesis_height}.')
    logger.info(f'The head height is {head}')

    check_view_call(legacy_url, split_url)

    # Verify multiple heights - optimization to allow the prober to verify
    # multiple heights in a single run.
    count = 0
    none_count = 0
    while True:
        # Pick a random number and then check the block and chunks at that height.
        height = random.randint(genesis_height, head)
        block = check_blocks(legacy_url, split_url, height)
        check_chunks(legacy_url, split_url, block)

        count += 1
        none_count += block is None

        current_time = datetime.now()
        current_duration = current_time - start_time
        if current_duration >= duration:
            break

        time.sleep(0.200)

    logger.info(
        f"Success. Validated {count} blocks. There were {none_count} missing blocks."
    )

if __name__ == '__main__':
    main()

'''
'''--- pytest/tools/prober/prober_util.py ---
#!/usr/bin/env python3

import sys
import json
import time
import pathlib
import requests
import json

sys.path.append(str(pathlib.Path(__file__).resolve().parents[2] / 'lib'))

import configured_logger

logger = configured_logger.new_logger("stderr", stderr=True)

def pretty_print(value) -> str:
    return json.dumps(value, indent=2)

def json_rpc(method, params, url):
    try:
        j = {
            'method': method,
            'params': params,
            'id': 'dontcare',
            'jsonrpc': '2.0'
        }
        start_time = time.time()
        r = requests.post(url, json=j, timeout=5)
        latency_ms = (time.time() - start_time)
        logger.debug(
            f'prober_request_latency_ms{{method="{method}",url="{url}"}} {latency_ms:.2f}'
        )
        result = json.loads(r.content)
        return result
    except Exception as e:
        logger.error(f'json_rpc({method}, {url}) query failed: {e}')
        sys.exit(1)

def get_genesis_height(url):
    try:
        genesis_config = json_rpc('EXPERIMENTAL_genesis_config', None, url)
        genesis_height = genesis_config['result']['genesis_height']
        logger.debug(f'Got genesis_height {genesis_height}')
        return genesis_height
    except Exception as e:
        logger.error(f'get_genesis_height({url}) failed: {e}')
        sys.exit(1)

def get_head(url):
    try:
        status = json_rpc('status', None, url)
        head = status['result']['sync_info']['latest_block_height']
        logger.debug(f'Got latest_block_height {head}')
        return head
    except Exception as e:
        logger.error(f'get_head({url}) failed: {e}')
        sys.exit(1)

def get_block(height, url):
    try:
        block = json_rpc('block', {'block_id': height}, url)
        if 'error' in block:
            raise Exception(block['error'])
        block = block['result']
        logger.debug(f'Got block at height {height}')
        return block
    except Exception as e:
        # This is typically fine as there may be gaps in the chain.
        logger.error(f'get_block({height}, {url}) failed: {e}')
        return None

def get_chunk(chunk, url):
    try:
        shard_id = chunk['shard_id']
        chunk_hash = chunk['chunk_hash']
        chunk = json_rpc('chunk', {'chunk_id': chunk_hash}, url)
        chunk = chunk['result']

        logger.debug(f'Got chunk {chunk_hash} for shard {shard_id}')
        return chunk
    except Exception as e:
        logger.error(f'get_chunk({chunk_hash}, {url}) failed: {e}')
        sys.exit(1)

'''
'''--- runtime/near-vm/tests/ignores.txt ---
# Compilers
singlepass spec::multi_value # Singlepass has not implemented multivalue (functions that returns "structs"/"tuples")
singlepass spec::simd # Singlepass doesn't support yet SIMD (no one asked for this feature)

# Traps
## Traps. Tracing doesn't work properly in Singlepass
## Unwinding is not properly implemented in Singlepass
# Needs investigation
singlepass traps::test_trap_trace
aarch64    traps::test_trap_trace
singlepass traps::test_trap_stack_overflow # Need to investigate
aarch64    traps::test_trap_stack_overflow # Need to investigate
singlepass traps::trap_display_pretty
aarch64    traps::trap_display_pretty
singlepass traps::trap_display_multi_module
aarch64    traps::trap_display_multi_module
singlepass traps::call_signature_mismatch
macos+aarch64    traps::call_signature_mismatch
singlepass traps::start_trap_pretty
aarch64    traps::start_trap_pretty

singlepass multi_value_imports::dylib # Singlepass doesn't support multivalue
singlepass multi_value_imports::dynamic # Singlepass doesn't support multivalue

# TODO: We need to fix this in ARM. The issue is caused by libunwind overflowing
# the stack while creating the stacktrace.
# https://github.com/rust-lang/backtrace-rs/issues/356
singlepass+windows spec::skip_stack_guard_page # Needs investigation.

# Windows doesn't overcommit and fails to allocate 4GB of memory
windows wasmer::max_size_of_memory

# Frontends

## WASI

### These tests don't pass due to race conditions in the new way we run tests.
### It's not built to be run in parallel with itself, so we disable it for now.

wasitests::snapshot1::host_fs::writing
wasitests::unstable::host_fs::writing
wasitests::snapshot1::mem_fs::writing
wasitests::unstable::mem_fs::writing

### due to hard-coded direct calls into WASI for wasi unstable

wasitests::snapshot1::host_fs::fd_read
wasitests::snapshot1::host_fs::poll_oneoff
wasitests::snapshot1::host_fs::fd_pread
wasitests::snapshot1::host_fs::fd_close
wasitests::snapshot1::host_fs::fd_allocate
wasitests::snapshot1::host_fs::close_preopen_fd
wasitests::snapshot1::host_fs::envvar
wasitests::snapshot1::mem_fs::fd_read
wasitests::snapshot1::mem_fs::poll_oneoff
wasitests::snapshot1::mem_fs::fd_pread
wasitests::snapshot1::mem_fs::fd_close
wasitests::snapshot1::mem_fs::fd_allocate
wasitests::snapshot1::mem_fs::close_preopen_fd
wasitests::snapshot1::mem_fs::envvar

### TODO: resolve the disabled tests below. These are newly disabled tests from the migration:

### due to git clone not preserving symlinks:
wasitests::snapshot1::host_fs::readlink
wasitests::unstable::host_fs::readlink
wasitests::snapshot1::mem_fs::readlink
wasitests::unstable::mem_fs::readlink

### failing due to `remove_dir_all`. this test is also bad for parallelism
wasitests::snapshot1::host_fs::create_dir
wasitests::unstable::host_fs::create_dir
wasitests::snapshot1::mem_fs::create_dir
wasitests::unstable::mem_fs::create_dir

### failing because it closes `stdout` which breaks our testing system
wasitests::unstable::host_fs::fd_close
wasitests::unstable::mem_fs::fd_close

### failing because we're operating on stdout which is now overridden.
### TODO: check WasiFile implementation
### Alterative: split test into 2 parts, one printing to stderr, the other printing to stdout to test the real versions
wasitests::unstable::host_fs::poll_oneoff
wasitests::unstable::mem_fs::poll_oneoff

### randomly failed, mainly on windows but also on macos, due to a race condition when concurently testing multiple compiler / engines
wasitests::snapshot1::host_fs::fd_rename_path

# This tests are disabled for now
wasitests::unstable::host_fs::unix_open_special_files
wasitests::snapshot1::host_fs::unix_open_special_files
wasitests::unstable::mem_fs::unix_open_special_files
wasitests::snapshot1::mem_fs::unix_open_special_files

'''
'''--- runtime/runtime-params-estimator/costs.txt ---
ActionReceiptCreation                         216_119_000_000
DataReceiptCreationBase                        72_973_464_625
DataReceiptCreationPerByte                         34_424_023
ActionCreateAccount                           199_214_750_000
ActionDeployContractBase                      369_531_500_000
ActionDeployContractPerByte                        13_625_998
ActionFunctionCallBase                      4_639_723_000_000
ActionFunctionCallPerByte                           4_471_868
ActionTransfer                                230_246_125_000
ActionStake                                   204_435_250_000
ActionAddFullAccessKey                        203_530_250_000
ActionAddFunctionAccessKeyBase                204_435_250_000
ActionAddFunctionAccessKeyPerByte                   3_850_662
ActionDeleteKey                               189_893_250_000
ActionDeleteAccount                           294_978_000_000
HostFunctionCall                                   88_256_037
WasmInstruction                                     1_285_457
ReadMemoryBase                                    869_954_400
ReadMemoryByte                                      1_267_111
WriteMemoryBase                                   934_598_287
WriteMemoryByte                                       907_924
ReadRegisterBase                                  839_055_062
ReadRegisterByte                                       32_854
WriteRegisterBase                                 955_174_162
WriteRegisterByte                                   1_267_188
Utf8DecodingBase                                1_037_259_687
Utf8DecodingByte                                   97_193_493
Utf16DecodingBase                               1_181_104_350
Utf16DecodingByte                                  54_525_831
Sha256Base                                      1_513_656_750
Sha256Byte                                          8_039_117
Keccak256Base                                   1_959_830_425
Keccak256Byte                                       7_157_035
Keccak512Base                                   1_937_129_412
Keccak512Byte                                      12_216_567
Ripemd160Base                                     284_558_362
Ripemd160Block                                    226_702_528
EcrecoverBase                                  92_940_662_819
Ed25519VerifyBase                              40_311_888_867
Ed25519VerifyByte                                 423_978_605
LogBase                                         1_181_104_350
LogByte                                             4_399_597
StorageWriteBase                               21_398_912_000
StorageWriteKeyByte                                23_494_289
StorageWriteValueByte                              10_339_513
StorageWriteEvictedByte                            10_705_769
StorageReadBase                                18_785_615_250
StorageReadKeyByte                                 10_317_511
StorageReadValueByte                                1_870_335
StorageRemoveBase                              17_824_343_500
StorageRemoveKeyByte                               12_740_128
StorageRemoveRetValueByte                           3_843_852
StorageHasKeyBase                              18_013_298_875
StorageHasKeyByte                                  10_263_615
TouchingTrieNode                                5_367_318_642
PromiseAndBase                                    488_337_800
PromiseAndPerPromise                                1_817_392
PromiseReturn                                     186_717_462
AltBn128G1MultiexpBase                        237_668_976_500
AltBn128G1MultiexpByte                          1_111_697_487
AltBn128G1MultiexpSublinear                         1_441_698
AltBn128PairingCheckBase                    3_228_502_967_000
AltBn128PairingCheckByte                        8_858_396_182
AltBn128G1SumBase                               1_058_438_125
AltBn128G1SumByte                                  25_406_181

'''
'''--- runtime/runtime-params-estimator/emu-cost/data_builder.py ---
import re
import sys

re1 = re.compile('Using (\d+) accounts')
re2 = re.compile('executed (\d+) instructions; (\d+) bytes read; (\d+) bytes written')

accounts = 0
interesting = [ 1000000 ]

for line in sys.stdin:
    m = re1.match(line)
    if m:
        accounts = int(m.group(1))
    m = re2.match(line)
    if m and accounts in interesting:
        insn = m.group(1)
        read = m.group(2)
        written = m.group(3)
        print insn, "r"+str(accounts), read
        print insn, "w"+str(accounts), written

'''
'''--- scripts/__init__.py ---

'''
'''--- scripts/check_nightly.py ---
#!/usr/bin/env python3
"""Checks whether all expensive tests are mentioned in NayDuck tests list

Scans all Rust source files looking for expensive tests and than makes sure that
they are all referenced in NayDuck test list files (the nightly/*.txt files).
Returns with success if that's the case; with failure otherwise.

An expensive test is one which is marked with expensive_tests feature as
follows:

    #[test]
    #[cfg_attr(not(feature = "expensive_tests"), ignore)]
    fn test_gc_random_large() {
        test_gc_random_common(25);
    }

The `test` and `cfg_attr` annotations can be specified in whatever order but
note that the script isn’t too smart about parsing Rust files and using
something more complicated in the `cfg_attr` will confuse it.

Expensive tests are not executed when running `cargo test` nor are they run in
CI and it’s the purpose of this script to make sure that they are listed for
NayDuck to run.
"""

import os
import pathlib
import re
import sys
import typing

import nayduck

IGNORED_SUBDIRS = ('target', 'target_expensive', 'sandbox')

EXPENSIVE_DIRECTIVE = '#[cfg_attr(not(feature = "expensive_tests"), ignore)]'
TEST_DIRECTIVE = '#[test]'

def expensive_tests_in_file(path: pathlib.Path) -> typing.Iterable[str]:
    """Yields names of expensive tests found in given Rust file.

    An expensive test is a function annotated with `test` and a conditional
    `ignore` attributes, specifically:

        #[test]
        #[cfg_attr(not(feature = "expensive_tests"), ignore)]
        fn test_slow() {
            // ...
        }

    Note that anything more complex in the `cfg_attr` will cause the function
    not to recognise the test.

    Args:
        path: Path to the Rust source file.
    Yields:
        Names of functions defining expensive tests (e.g. `test_slow` in example
        above).
    """
    with open(path) as rd:
        is_expensive = False
        is_test = False
        for line in rd:
            line = line.strip()
            if not line:
                pass
            elif line.startswith('#'):
                is_expensive = is_expensive or line == EXPENSIVE_DIRECTIVE
                is_test = is_test or line == TEST_DIRECTIVE
            elif is_expensive or is_test:
                if is_expensive and is_test:
                    match = re.search(r'\bfn\s+([A-Za-z_][A-Za-z_0-9]*)\b',
                                      line)
                    if match:
                        yield match.group(1)
                is_expensive = False
                is_test = False

def nightly_tests(repo_dir: pathlib.Path) -> typing.Iterable[str]:
    """Yields expensive tests mentioned in the nightly configuration file."""
    for test in nayduck.read_tests_from_file(repo_dir /
                                             nayduck.DEFAULT_TEST_FILE,
                                             include_comments=True):
        t = test.split()
        try:
            # It's okay to comment out a test intentionally.
            if t[t[0] == '#'] in ('expensive', '#expensive'):
                yield t[-1].split('::')[-1]
        except IndexError:
            pass

def main() -> typing.Optional[str]:
    repo_dir = pathlib.Path(__file__).parent.parent
    nightly_txt_tests = set(nightly_tests(repo_dir))
    for root, dirs, files in os.walk(repo_dir):
        dirs[:] = [
            dirname for dirname in dirs if dirname not in IGNORED_SUBDIRS
        ]
        path = pathlib.Path(root)
        for filename in files:
            if filename.endswith('.rs'):
                filepath = path / filename
                print(f'checking file {filepath}')
                for test in expensive_tests_in_file(filepath):
                    print(f'  expensive test {test}')
                    if test not in nightly_txt_tests:
                        return f'error: file {filepath} test {test} not in nightly.txt'
    print('all tests in nightly')
    return None

if __name__ == '__main__':
    sys.exit(main())

'''
'''--- scripts/check_pytests.py ---
#!/usr/bin/env python3
"""Checks whether all pytest tests are mentioned in NayDuck or Buildkite

Lists all Python scripts inside of pytest/tests directory and checks whether
they are referenced in NayDuck test list files (the nightly/*.txt files) or
Buildkite pipeline configuration (the .buildkite/pipeline.yml file).  Returns
with success if that's the case; with failure otherwise.
"""
import fnmatch
import os
import pathlib
import random
import re
import sys
import typing

import yaml

import nayduck

# List of globs of Python scripts in the pytest/tests directory which are not
# test but rather helper scripts and libraries.  The entire mocknet/ directory
# is covered here as well since those tests are not run on NayDuck any more.
HELPER_SCRIPTS = [
    'delete_remote_nodes.py',
    'loadtest/*',
    'mocknet/*',
    'shardnet/*',
    'stress/hundred_nodes/*',
    'loadtest/*',
    'replay/*',
]

PYTEST_TESTS_DIRECTORY = pathlib.Path('pytest/tests')
NIGHTLY_TESTS_FILE = pathlib.Path(nayduck.DEFAULT_TEST_FILE)

# TODO: this should read ci.yml and fetch the list of tests from there
# Currently, the list of tests is hardcoded, so if a test is removed/added to ci.yml then the list
# here should be updated too
GHA_TESTS = [
    "sanity/backward_compatible.py",
    "sanity/db_migration.py",
    "sanity/spin_up_cluster.py",
    "sanity/upgradable.py",
]

StrGenerator = typing.Generator[str, None, None]

def list_test_files(topdir: pathlib.Path) -> StrGenerator:
    """Yields all *.py files in a given directory traversing it recursively.

    Args:
        topdir: Path to the directory to traverse.  Directory is traversed
            recursively.
    Yields:
        Paths (as str objects) to all the Python source files in the directory
        relative to the top directory.  __init__.py files (and in fact any files
        starting with __) are ignored.
    """
    for dirname, _, filenames in os.walk(topdir):
        dirpath = pathlib.Path(dirname).relative_to(topdir)
        for filename in filenames:
            if not filename.startswith('__') and filename.endswith('.py'):
                yield str(dirpath / filename)

def read_nayduck_tests(path: pathlib.Path) -> StrGenerator:
    """Reads NayDuck test file and yields all tests mentioned there.

    The NayDuck test list files are ones with .txt extension.  Only pytest and
    mocknet tests are taken into account and returned.  Enabled tests as well as
    those commented out with a corresponding TODO comment are considered.

    Args:
        path: Path to the test set.
    Yields:
        pytest and mocknet tests mentioned in the file.  May include duplicates.
    """

    def extract_name(line: str) -> StrGenerator:
        tokens = line.split()
        idx = 1 + (tokens[0] == '#')
        while idx < len(tokens) and tokens[idx].startswith('--'):
            idx += 1
        if idx < len(tokens):
            yield tokens[idx]

    found_todo = False
    for line in nayduck.read_tests_from_file(path, include_comments=True):
        line = re.sub(r'\s+', ' ', line.strip())
        if re.search(r'^(?:pytest|mocknet) ', line):
            found_todo = False
            yield from extract_name(line)
        elif found_todo and re.search(r'^# ?(?:pytest|mocknet) ', line):
            yield from extract_name(line)
        elif re.search('^# ?TODO.*#[0-9]{4,}', line):
            found_todo = True
        elif not line.strip().startswith('#'):
            found_todo = False

def print_error(missing: typing.Collection[str]) -> None:
    """Formats and outputs an error message listing missing tests."""
    this_file = os.path.relpath(__file__)
    example = random.sample(tuple(missing), 1)[0]
    if example.startswith('mocknet/'):
        example = 'mocknet ' + example
    else:
        example = 'pytest ' + example
    msg = '''\
Found {count} pytest file{s} (i.e. Python file{s} in pytest/tests directory)
which are not included in any of the nightly/*.txt files:

{missing}

Add the test{s} to one of the lists in nightly/*.txt files.  For example as:

    {example}

If the test is temporarily disabled, add it to one of the files with an
appropriate {todo} comment.  For example:

    # {todo}(#1234): Enable the test again once <some condition>
    # {example}

Note that the {todo} comment must reference a GitHub issue (i.e. must
contain a #<number> string).

If the file is not a test but a helper library, consider moving it out
of the pytest/tests directory to pytest/lib or add it to HELPER_SCRIPTS
list at the top of {this_file} file.'''.format(
        count=len(missing),
        s='' if len(missing) == 1 else 's',
        missing='\n'.join(
            ' -  pytest/tests/' + name for name in sorted(missing)),
        example=example,
        this_file=this_file,
        todo='TO'
        'DO',
    )
    print(msg, file=sys.stderr)

def main() -> int:
    """Main function of the script; returns integer exit code."""
    missing = set(list_test_files(PYTEST_TESTS_DIRECTORY))
    count = len(missing)
    missing.difference_update(
        read_nayduck_tests(pathlib.Path(nayduck.DEFAULT_TEST_FILE)))
    missing.difference_update(GHA_TESTS)
    missing = set(filename for filename in missing if not any(
        fnmatch.fnmatch(filename, pattern) for pattern in HELPER_SCRIPTS))
    if missing:
        print_error(missing)
        return 1
    print(f'All {count} tests included')
    return 0

if __name__ == '__main__':
    sys.exit(main())

'''
'''--- scripts/fix_nightly_feature_flags.py ---
#!/bin/python3

# This script checks that nightly feature flags are declared consistently so
# we avoid nasty subtle bugs. See the individual checks below for details.

import toml
import re
import difflib
import sys

class Crate:

    def __init__(self, dir):
        self.dir = dir
        self.height = None
        self.load_toml()

    def load_toml(self):
        self.toml = toml.load(self.dir + "/Cargo.toml")
        self.name = self.toml["package"]["name"]
        self.features = self.toml.get("features", {})

        self.has_nightly_feature = "nightly" in self.features
        self.has_nightly_protocol_feature = "nightly_protocol" in self.features
        self.local_nightly_features = self.get_local_nightly_features()
        self.local_nightly_protocol_features = self.get_local_nightly_protocol_features(
        )
        self.dependency_nightly_features = self.get_dependency_nightly_features(
        )
        self.dependency_nightly_protocol_features = self.get_dependency_nightly_protocol_features(
        )

    def get_local_nightly_features(self):
        if not self.has_nightly_feature:
            return []
        return [
            feature for feature in self.features["nightly"]
            if '/' not in feature
        ]

    def get_local_nightly_protocol_features(self):
        if not self.has_nightly_protocol_feature:
            return []
        return [
            feature for feature in self.features["nightly_protocol"]
            if '/' not in feature
        ]

    def get_dependency_nightly_features(self):
        if not self.has_nightly_feature:
            return []
        return [
            feature for feature in self.features["nightly"] if '/' in feature
        ]

    def get_dependency_nightly_protocol_features(self):
        if not self.has_nightly_protocol_feature:
            return []
        return [
            feature for feature in self.features["nightly_protocol"]
            if '/' in feature
        ]

    def build_deps(self, crates_by_name):
        self.deps = [
            crates_by_name[dep]
            for dep in self.toml.get('dependencies', {}).keys()
            if dep in crates_by_name
        ] + [
            crates_by_name[dep]
            for dep in self.toml.get('dev-dependencies', {}).keys()
            if dep in crates_by_name
        ]

    def build_transitive_deps(self):
        visited = set()
        result = []

        def recursion_helper(crate):
            result.append(crate)
            for dep in crate.deps:
                if dep.name not in visited:
                    visited.add(dep.name)
                    recursion_helper(dep)

        for dep in self.deps:
            if dep.name not in visited:
                visited.add(dep.name)
                recursion_helper(dep)
        self.transitive_deps = result

    def write_toml(self, apply_fix):
        toml_file_name = self.dir + "/Cargo.toml"
        existing_toml_text = open(toml_file_name, "r").read()
        new_toml_text = existing_toml_text

        def ensure_features():
            nonlocal new_toml_text
            if "[features]" not in new_toml_text:
                new_toml_text += "\n[features]\nnightly = []\nnightly_protocol = []\n"
            if "nightly =" not in new_toml_text:
                new_toml_text = re.sub(r"\[features\]",
                                       "[features]\nnightly = []",
                                       new_toml_text)
            if "nightly_protocol =" not in new_toml_text:
                new_toml_text = re.sub(r"\[features\]",
                                       "[features]\nnightly_protocol = []",
                                       new_toml_text)

        def replace_nightly(deps):
            nonlocal new_toml_text
            desired_text = 'nightly = [\n'
            for dep in deps:
                desired_text += '  "{}",\n'.format(dep)
            desired_text += ']'
            regex = re.compile(r"nightly\s*=\s*\[.*?\]", re.DOTALL)
            (new_toml_text, n) = re.subn(regex, desired_text, new_toml_text)
            assert n == 1, "Substitution failed for crate {}".format(self.name)

        def replace_nightly_protocol(deps):
            nonlocal new_toml_text
            desired_text = 'nightly_protocol = [\n'
            for dep in deps:
                desired_text += '  "{}",\n'.format(dep)
            desired_text += ']'
            regex = re.compile(r"nightly_protocol\s*=\s*\[.*?\]", re.DOTALL)
            (new_toml_text, n) = re.subn(regex, desired_text, new_toml_text)
            assert n == 1, "Substitution failed for crate {}".format(self.name)

        if self.has_nightly_feature:
            ensure_features()
            replace_nightly(
                sorted(
                    set(self.local_nightly_features) |
                    set(self.dependency_nightly_features)))
        if self.has_nightly_protocol_feature:
            ensure_features()
            replace_nightly_protocol(
                sorted(
                    set(self.local_nightly_protocol_features) |
                    set(self.dependency_nightly_protocol_features)))

        if apply_fix:
            open(toml_file_name, "w").write(new_toml_text)
        else:
            if new_toml_text != existing_toml_text:
                print("Nightly feature flags need updating:")
                sys.stdout.writelines(
                    difflib.unified_diff(
                        existing_toml_text.splitlines(keepends=True),
                        new_toml_text.splitlines(keepends=True),
                        fromfile=toml_file_name,
                        tofile=toml_file_name))
                return False
            return True

workspace_toml = toml.load("Cargo.toml")
crate_dirs = workspace_toml["workspace"]["members"]

crates = [Crate(dir) for dir in crate_dirs]
crate_by_name = {crate.name: crate for crate in crates}
for crate in crates:
    crate.build_deps(crate_by_name)
for crate in crates:
    crate.build_transitive_deps()

for crate in crates:
    # check 1: we should either have neither feature, or both.
    if crate.has_nightly_feature != crate.has_nightly_protocol_feature:
        if crate.has_nightly_feature:
            print(
                "crate {} has nightly feature but not nightly_protocol. Adding nightly_protocol"
                .format(crate.name))
            crate.has_nightly_protocol_feature = True
            crate.local_nightly_features.append('nightly_protocol')
        else:
            print(
                "crate {} has nightly_protocol feature but not nightly. Adding nightly"
                .format(crate.name))
            crate.has_nightly_feature = True
            crate.local_nightly_features.append('nightly_protocol')

    # check 2: nightly_protocol should not activate on local features.
    if crate.local_nightly_protocol_features != []:
        print(
            "crate {} has local nightly_protocol features: {}, moving to nightly"
            .format(crate.name, crate.local_nightly_protocol_features))
        crate.local_nightly_features += crate.local_nightly_protocol_features
        crate.local_nightly_protocol_features = []

    # check 3: nightly should activate local nightly_protocol.
    if crate.has_nightly_feature:
        if 'nightly_protocol' not in crate.local_nightly_features:
            print(
                "crate {} nightly feature does not include nightly_protocol. Adding nightly_protocol"
                .format(crate.name))
            crate.local_nightly_features.append('nightly_protocol')

while True:
    fixed = False
    for crate in crates:
        # check 4: nightly should be present if there's a transitive dependency that has it and there's also a transitive dependent that
        # has it. Without this, suppose A depends on B and B depends on C, but B does not declare a nightly feature. There would be no way
        # for A's nightly feature to activate C's nightly feature, since C is not a direct dependency of A. So to avoid that kind of headache
        # we require B to also declare nightly.
        transitive_dependencies_with_nightly = [
            dep for dep in crate.transitive_deps if dep.has_nightly_feature
        ]
        dependents_with_nightly = [
            other for other in crates
            if crate in other.deps and other.has_nightly_feature
        ]
        if transitive_dependencies_with_nightly != [] and dependents_with_nightly != [] and crate.has_nightly_feature == False:
            print(
                "crate {} has both dependencies (e.g. {}) with nightly and dependents (e.g. {}) with nightly but does not have nightly feature itself. Adding nightly feature"
                .format(crate.name,
                        transitive_dependencies_with_nightly[0].name,
                        dependents_with_nightly[0].name))
            crate.has_nightly_feature = True
            crate.has_nightly_protocol_feature = True
            crate.local_nightly_features.append('nightly_protocol')
            fixed = True
    if not fixed:
        break

for crate in crates:
    if not crate.has_nightly_feature:
        continue
    # check 5: nightly should activate precisely the nightly flags of all direct dependencies.
    # Same with nightly_protocol.
    dependencies_with_nightly = [
        dep for dep in crate.deps if dep.has_nightly_feature
    ]
    desired_dependency_features = [
        dep.name + '/nightly' for dep in dependencies_with_nightly
    ]
    if set(crate.dependency_nightly_features) != set(
            desired_dependency_features):
        print("crate {} has nightly dependencies {} but should be {}, fixing".
              format(crate.name, crate.dependency_nightly_features,
                     desired_dependency_features))
        crate.dependency_nightly_features = desired_dependency_features

    dependencies_with_nightly_protocol = [
        dep for dep in crate.deps if dep.has_nightly_protocol_feature
    ]
    desired_dependency_features = [
        dep.name + '/nightly_protocol'
        for dep in dependencies_with_nightly_protocol
    ]
    if set(crate.dependency_nightly_protocol_features) != set(
            desired_dependency_features):
        print(
            "crate {} has nightly_protocol dependencies {} but should be {}, fixing"
            .format(crate.name, crate.dependency_nightly_protocol_features,
                    desired_dependency_features))
        crate.dependency_nightly_protocol_features = desired_dependency_features

    # check 6: if any feature declared locally has a name that coincides with a nightly
    # feature of a transitive dependency, that feature should also be included in the
    # local nightly list. Otherwise, this is likely a mistake, because nightly would
    # activate a feature in a dependency crate but not a dependent crate.
    all_dependency_local_nightly_features = []
    for dep in crate.transitive_deps:
        all_dependency_local_nightly_features += dep.local_nightly_features
    for feature in crate.features.keys():
        if feature == 'nightly' or feature == 'nightly_protocol':
            continue
        if feature in all_dependency_local_nightly_features:
            if feature not in crate.local_nightly_features:
                print(
                    "crate {} has local feature {} that coincides with a dependency nightly feature. Adding to local nightly list"
                    .format(crate.name, feature))
                crate.local_nightly_features.append(feature)

    # TODO: there's a similar check we should do that checks if A depends on B and they
    # both have the same nightly feature name, then that feature of A should activate
    # that feature of B.

apply_fix = len(sys.argv) == 2 and sys.argv[1] == 'fix'
if apply_fix:
    for crate in crates:
        crate.write_toml(True)
else:
    all_correct = True
    for crate in crates:
        all_correct = all_correct and crate.write_toml(False)
    if not all_correct:
        print("Run the following to apply nightly feature flag fixes:")
        print("  python3 scripts/fix_nightly_feature_flags.py fix")
        sys.exit(1)

'''
'''--- scripts/flaky_test_check.py ---
#!/usr/bin/env python3

import os
from testlib import clean_binary_tests, build_tests, test_binaries, run_test

TIMES = 10

if __name__ == "__main__":
    clean_binary_tests()
    build_tests()
    binaries = test_binaries(exclude=[r'test_regression-.*'])
    print(f'========= collected {len(binaries)} test binaries:')
    print('\n'.join(binaries))

    print(f"Run all tests sequentially for {TIMES} times: ")
    test_with_fails = {}
    for binary in binaries:
        fails = []
        for i in range(TIMES):
            exitcode, stdout, stderr = run_test(binary, isolate=False)
            print(f'Run {binary} {i+1} of {TIMES}, exit code {exitcode}')
            if exitcode != 0:
                fails.append(exitcode)
        if fails:
            test_with_fails[binary] = fails

    if test_with_fails:
        print("Some tests failed: ")
        for t, f in test_with_fails.items():
            print(f'{t} failed {len(f)} times')

'''
'''--- scripts/nayduck.py ---
#!/usr/bin/env python3
"""Runs integration tests in the cloud on NayDuck.

To request a new run, use the following command:

   python3 scripts/nayduck.py      \
       --branch    <your_branch>   \
       --test-file <test_file>.txt

Scheduled runs can be seen at <https://nayduck.nearone.org/>.

See README.md in nightly directory for documentation of the test suite file
format.  Note that you must be a member of the Near or Near Protocol
organisation on GitHub to authenticate (<https://github.com/orgs/near/people>).

The source code for NayDuck itself is at <https://github.com/Near-One/nayduck>.
"""

import getpass
import json
import os
import pathlib
import shlex
import subprocess
import sys
import typing

REPO_DIR = pathlib.Path(__file__).resolve().parents[1]

DEFAULT_TEST_FILE = 'nightly/nightly.txt'
NAYDUCK_BASE_HREF = 'https://nayduck.nearone.org'

def _parse_args():
    import argparse

    default_test_path = (pathlib.Path(__file__).parent.parent /
                         DEFAULT_TEST_FILE)

    parser = argparse.ArgumentParser(description='Run tests.')
    parser.add_argument(
        '--cancel',
        '-c',
        help='Cancel scheduled run. In progress tests cannot be stopped.')
    parser.add_argument('--branch',
                        '-b',
                        help='Branch to test. By default gets current one.')
    parser.add_argument(
        '--sha',
        '-s',
        help=
        'Commit sha to test. By default gets current one. This is ignored if branch name is provided'
    )
    group = parser.add_mutually_exclusive_group()
    group.add_argument('--test-file',
                       '-t',
                       default=default_test_path,
                       help=f'Test set file; {DEFAULT_TEST_FILE} by default.')
    group.add_argument('--stdin',
                       '-i',
                       action='store_true',
                       help='Read test set from standard input.')
    parser.add_argument('--run-locally',
                        '-l',
                        action='store_true',
                        help='Run tests locally.')
    parser.add_argument(
        '--dry-run',
        '-n',
        action='store_true',
        help='Prints list of tests to execute, without doing anything')
    args = parser.parse_args()

    return args

def get_sha(branch: str):
    try:
        sha = subprocess.check_output(['git', 'rev-parse', branch], text=True)

    except subprocess.CalledProcessError as e:
        remote_branch = 'remotes/origin/' + branch
        print(
            f"Couldn't find a local branch \'{branch}\'. Trying remote: {remote_branch}"
        )
        sha = subprocess.check_output(
            ['git', 'rev-parse', remote_branch],
            text=True,
        )

    return sha

def get_branch(sha: str = ""):
    if sha:
        return subprocess.check_output(['git', 'name-rev', sha],
                                       text=True).strip().split(' ')[-1]
    else:
        return subprocess.check_output(
            ['git', 'rev-parse', '--abbrev-ref', "HEAD"], text=True).strip()

FileReader = typing.Callable[[pathlib.Path], str]

def read_tests_from_file(
    path: pathlib.Path,
    *,
    include_comments: bool = False,
    reader: FileReader = lambda path: path.read_text()
) -> typing.Iterable[str]:
    """Reads lines from file handling `./<path>` includes.

    Returns an iterable over lines in given file but also handles `./<path>`
    syntax for including other files in the output and optionally filters
    commented out lines.

    A `./<path>` syntax acts like C's #include directive, Rust's `include!`
    macro or shell's `source` command.  All lines are read from file at <path>
    as if they were directly in the source file.  The `./<path>` directives are
    handled recursively up to three levels deep.

    If include_comments is True, `#./<path>` lines are handled as well with all
    included line commented out.  This is useful to comment out a include a with
    TODO comment included and have check_nightly.py and check_pytest.py scripts
    still recognise those includes.  Note that the line must start with `#./`,
    i.e. there must be no space between hash and the dot.

    Args:
        path: Path to the file to read.
        include_comments: By default empty lines and lines whose first non-space
            character is hash are ignored and not included in the output.  With
            this set to `True` such lines are included as well.
        reader: A callback which reads text content of a given file.  This is
            used by NayDuck.
    Returns:
        An iterable over lines in the given file.  All lines are stripped of
        leading and trailing white space.
    """
    return __read_tests(reader(path).splitlines(),
                        filename=path,
                        dirpath=path.parent,
                        include_comments=include_comments,
                        reader=reader)

def read_tests_from_stdin(
    *,
    include_comments: bool = False,
    reader: FileReader = lambda path: path.read_text()
) -> typing.Iterable[str]:
    """Reads lines from standard input handling `./<path>` includes.

    Behaves like `read_tests_from_file` but rather than reading contents of
    given file reads lines from standard input.  `./<path>` includes are
    resolved relative to current working directory.

    Returns:
        An iterable over lines in the given file.  All lines are stripped of
        leading and trailing white space.
    """
    return __read_tests(sys.stdin,
                        filename='<stdin>',
                        dirpath=pathlib.Path.cwd(),
                        include_comments=include_comments,
                        reader=reader)

def __read_tests(
    lines: typing.Iterable[str],
    *,
    filename: typing.Union[str, pathlib.Path],
    dirpath: pathlib.Path,
    include_comments: bool = False,
    reader: FileReader = lambda path: path.read_text()
) -> typing.Iterable[str]:

    def impl(lines: typing.Iterable[str],
             filename: typing.Union[str, pathlib.Path],
             dirpath: pathlib.Path,
             depth: int = 1,
             comment: bool = False) -> typing.Iterable[str]:
        for lineno, line in enumerate(lines, start=1):
            line = line.rstrip()
            if line.startswith('./') or (include_comments and
                                         line.startswith('#./')):
                if depth == 3:
                    print(f'{filename}:{lineno}: ignoring {line}; '
                          f'would exceed depth limit of {depth}')
                else:
                    incpath = dirpath / line.lstrip('#')
                    yield from impl(
                        reader(incpath).splitlines(), incpath, incpath.parent,
                        depth + 1, comment or line.startswith('#'))
            elif include_comments or (line and line[0] != '#'):
                if comment and not line.startswith('#'):
                    line = '#' + line
                yield line

    return impl(lines, filename, dirpath)

def github_auth(code_path: pathlib.Path):
    print('Go to the following link in your browser:\n\n{}/login/cli\n'.format(
        NAYDUCK_BASE_HREF))
    code = getpass.getpass('Enter authorisation code: ')
    code_path.parent.mkdir(parents=True, exist_ok=True)
    code_path.write_text(code)
    return code

def _parse_timeout(timeout: typing.Optional[str]) -> typing.Optional[int]:
    """Parses timeout interval and converts it into number of seconds.

    Args:
        timeout: An integer with an optional ‘h’, ‘m’ or ‘s’ suffix which
            multiply the integer by 3600, 60 and 1 respectively.
    Returns:
        Interval in seconds.
    """
    if not timeout:
        return None
    mul_ary = {'h': 3600, 'm': 60, 's': 1}
    mul = mul_ary.get(timeout[-1])
    if mul:
        timeout = timeout[:-1]
    else:
        mul = 1
    return int(timeout) * mul

def run_locally(args, tests):
    for test in tests:
        # See nayduck specs at https://github.com/Near-One/nayduck/blob/master/lib/testspec.py
        fields = test.split()

        timeout = None
        index = 1
        ignored = []
        while len(fields) > index and fields[index].startswith('--'):
            if fields[index].startswith("--timeout="):
                timeout = fields[index][10:]
            elif fields[index] != '--skip-build':
                ignored.append(fields[index])
            index += 1

        del fields[1:index]
        message = f'Running ‘{"".join(fields)}’'
        if ignored:
            message = f'{message} (ignoring flags ‘{" ".join(ignored)}`)'
        if not args.dry_run:
            print(message)

        if fields[0] == 'expensive':
            # TODO --test doesn't work
            cmd = [
                'cargo',
                'test',
                '-p',
                fields[1],  # '--test', fields[2],
                '--features',
                'expensive_tests',
                '--',
                '--exact',
                fields[3]
            ]
            cwd = REPO_DIR
        elif fields[0] in ('pytest', 'mocknet'):
            fields[0] = sys.executable
            fields[1] = os.path.join('tests', fields[1])
            cmd = fields
            cwd = REPO_DIR / 'pytest'
        else:
            print(f'Unrecognised test category ‘{fields[0]}’', file=sys.stderr)
            continue
        if args.dry_run:
            print('( cd {} && {} )'.format(
                shlex.quote(str(cwd)),
                ' '.join(shlex.quote(str(arg)) for arg in cmd)))
            continue
        print(f"RUNNING COMMAND cwd = {cwd} cmd = {cmd}")
        subprocess.check_call(cmd, cwd=cwd, timeout=_parse_timeout(timeout))

def run_command(args, tests):
    import requests

    try:
        import colorama
        styles = (colorama.Fore.RED, colorama.Fore.GREEN,
                  colorama.Style.RESET_ALL)
    except ImportError:
        styles = ('', '', '')

    code_path = pathlib.Path(
        os.environ.get('XDG_CONFIG_HOME') or
        pathlib.Path.home() / '.config') / 'nayduck-code'
    if code_path.is_file():
        code = code_path.read_text().strip()
    else:
        code = github_auth(code_path)

    if args.dry_run:
        for test in tests:
            print(test)
        return

    if args.branch:
        test_branch = args.branch
        test_sha = get_sha(test_branch).strip()

    else:
        test_sha = args.sha or get_sha("HEAD").strip()
        test_branch = get_branch(test_sha)

    post = {'branch': test_branch, 'sha': test_sha, 'tests': list(tests)}

    print("Scheduling tests for: \n"
          f"branch - {post['branch']} \n"
          f"commit hash - {post['sha']}")

    while True:
        print('Sending request ...')
        if args.cancel:
            res = requests.post(NAYDUCK_BASE_HREF +
                                f'/api/run/{args.cancel}/cancel',
                                cookies={'nay-code': code})
        else:
            res = requests.post(NAYDUCK_BASE_HREF + '/api/run/new',
                                json=post,
                                cookies={'nay-code': code})
        if res.status_code != 401:
            break
        print(f'{styles[0]}Unauthorised.{styles[2]}\n')
        code = github_auth(code_path)

    if res.status_code == 200:
        json_res = json.loads(res.text)
        if args.cancel:
            print(styles[1] + 'Cancelled ' + str(json_res) + ' test(s)' +
                  styles[2])
        else:
            print(styles[json_res['code'] == 0] + json_res['response'] +
                  styles[2])
    else:
        print(f'{styles[0]}Got status code {res.status_code}:{styles[2]}\n')
        print(res.text)
    code = res.cookies.get('nay-code')
    if code:
        code_path.write_text(code)

def main():
    args = _parse_args()

    if args.stdin:
        tests = list(read_tests_from_stdin())
    else:
        tests = list(read_tests_from_file(pathlib.Path(args.test_file)))

    if args.run_locally:
        run_locally(args, tests)
    else:
        run_command(args, tests)

if __name__ == "__main__":
    main()

'''
'''--- scripts/nodelib.py ---
#!/usr/bin/env python3

import json
import os
import subprocess
import urllib

try:
    input = raw_input
except NameError:
    pass

USER = str(os.getuid()) + ':' + str(os.getgid())
"""Installs cargo/Rust."""

def install_cargo():
    try:
        subprocess.call([os.path.expanduser('~/.cargo/bin/cargo'), '--version'])
    except OSError:
        print("Installing Rust...")
        subprocess.check_output('curl https://sh.rustup.rs -sSf | sh -s -- -y',
                                shell=True)

"""Inits the node configuration using docker."""

def docker_init(image, home_dir, init_flags):
    subprocess.check_output(['mkdir', '-p', home_dir])
    subprocess.check_output([
        'docker', 'run', '-u', USER, '-v',
        '%s:/srv/near' % home_dir, '-v',
        os.path.abspath('near/res') +
        ':/near/res', image, 'near', '--home=/srv/near', 'init'
    ] + init_flags)

"""Inits the node configuration using local build."""

def nodocker_init(home_dir, is_release, init_flags):
    target = './target/%s/neard' % ('release' if is_release else 'debug')
    cmd = [target]
    if home_dir:
        cmd.extend(['--home', home_dir])
    cmd.extend(['init'])
    subprocess.call(cmd + init_flags)

def get_chain_id_from_flags(flags):
    """Retrieve requested chain id from the flags."""
    chain_id = None
    flags_iter = iter(flags)
    for flag in flags_iter:
        if flag.startswith('--chain-id='):
            chain_id = flag[len('--chain-id='):]
        elif flag == '--chain-id':
            chain_id = next(flags_iter, chain_id)
    return chain_id

"""Compile given package using cargo"""

def compile_package(package_name, is_release):
    flags = ['-p', package_name]
    if is_release:
        flags = ['--release'] + flags
    code = subprocess.call([os.path.expanduser('cargo'), 'build'] + flags)
    if code != 0:
        print("Compilation failed, aborting")
        exit(code)

"""Checks if there is already everything setup on this machine, otherwise sets up NEAR node."""

def check_and_setup(nodocker,
                    is_release,
                    image,
                    home_dir,
                    init_flags,
                    no_gas_price=False):
    if nodocker:
        compile_package('neard', is_release)

    chain_id = get_chain_id_from_flags(init_flags)
    if os.path.exists(os.path.join(home_dir, 'config.json')):
        with open(os.path.join(os.path.join(home_dir, 'genesis.json'))) as fd:
            genesis_config = json.load(fd)
        if chain_id and genesis_config['chain_id'] != chain_id:
            if chain_id == 'testnet':
                print(
                    "Folder %s already has network configuration for %s, which is not the official testnet.\n"
                    "Use ./scripts/start_localnet.py instead to keep running with existing configuration.\n"
                    "If you want to run a different network, either specify different --home or remove %s to start from scratch."
                    % (home_dir, genesis_config['chain_id'], home_dir))
            elif genesis_config['chain_id'] == 'testnet':
                print(
                    "Folder %s already has network configuration for the official testnet.\n"
                    "Use ./scripts/start_testnet.py instead to keep running it.\n"
                    "If you want to run a different network, either specify different --home or remove %s to start from scratch"
                    % (home_dir, home_dir))
            elif chain_id != '':
                print(
                    "Folder %s already has network configuration for %s. Use ./scripts/start_localnet.py to continue running it."
                    % (home_dir, genesis_config['chain_id']))
            exit(1)
        print("Using existing node configuration from %s for %s" %
              (home_dir, genesis_config['chain_id']))
        return

    print("Setting up network configuration.")
    if len([x for x in init_flags if x.startswith('--account-id')]) == 0:
        prompt = "Enter your account ID"
        if chain_id:
            prompt += " (leave empty if not going to be a validator): "
        else:
            prompt += ": "
        account_id = input(prompt)
        if account_id:
            init_flags.append('--account-id=%s' % account_id)

    if chain_id == 'testnet':
        testnet_genesis_hash = open('near/res/testnet_genesis_hash').read()
        testnet_genesis_records = 'near/res/testnet_genesis_records_%s.json' % testnet_genesis_hash
        if not os.path.exists(testnet_genesis_records):
            print('Downloading testnet genesis records')
            url = 'https://s3-us-west-1.amazonaws.com/testnet.nearprotocol.com/testnet_genesis_records_%s.json' % testnet_genesis_hash
            urllib.urlretrieve(url, testnet_genesis_records)
        init_flags.extend([
            '--genesis-config', 'near/res/testnet_genesis_config.json',
            '--genesis-records', testnet_genesis_records, '--genesis-hash',
            testnet_genesis_hash
        ])

    if nodocker:
        nodocker_init(home_dir, is_release, init_flags)
    else:
        docker_init(image, home_dir, init_flags)
    if no_gas_price:
        filename = os.path.join(home_dir, 'genesis.json')
        with open(filename) as fd:
            genesis_config = json.load(fd)
        genesis_config['gas_price'] = 0
        genesis_config['min_gas_price'] = 0
        json.dump(genesis_config, open(filename, 'w'))

def print_staking_key(home_dir):
    key_path = os.path.join(home_dir, 'validator_key.json')
    if not os.path.exists(key_path):
        return

    with open(key_path) as fd:
        key_file = json.load(fd)
    if not key_file['account_id']:
        print("Node is not staking. Re-run init to specify staking account.")
        return
    print("Stake for user '%s' with '%s'" %
          (key_file['account_id'], key_file['public_key']))

"""Stops and removes given docker container."""

def docker_stop_if_exists(name):
    try:
        subprocess.Popen(['docker', 'stop', name],
                         stdout=subprocess.PIPE,
                         stderr=subprocess.PIPE).communicate()
    except subprocess.CalledProcessError:
        pass
    try:
        subprocess.Popen(['docker', 'rm', name],
                         stdout=subprocess.PIPE,
                         stderr=subprocess.PIPE).communicate()
    except subprocess.CalledProcessError:
        pass

"""Checks the ports saved in config.json"""

def get_port(home_dir, net):
    config = json.load(open(os.path.join(home_dir, 'config.json')))
    p = config[net]['addr'][config[net]['addr'].find(':') + 1:]
    return p + ":" + p

"""Runs NEAR core inside the docker container for isolation and easy update with Watchtower."""

def run_docker(image, home_dir, boot_nodes, telemetry_url, verbose):
    print("Starting NEAR client and Watchtower dockers...")
    docker_stop_if_exists('watchtower')
    docker_stop_if_exists('nearcore')
    # Start nearcore container, mapping home folder and ports.
    envs = [
        '-e',
        'BOOT_NODES=%s' % boot_nodes, '-e',
        'TELEMETRY_URL=%s' % telemetry_url
    ]
    rpc_port = get_port(home_dir, 'rpc')
    network_port = get_port(home_dir, 'network')
    if verbose:
        envs.extend(['-e', 'VERBOSE=1'])
    subprocess.check_output(['mkdir', '-p', home_dir])
    subprocess.check_output([
        'docker', 'run', '-u', USER, '-d', '-p', rpc_port, '-p', network_port,
        '-v',
        '%s:/srv/near' % home_dir, '-v', '/tmp:/tmp', '--ulimit', 'core=-1',
        '--name', 'nearcore', '--restart', 'unless-stopped'
    ] + envs + [image])
    # Start Watchtower that will automatically update the nearcore container when new version appears.
    subprocess.check_output([
        'docker', 'run', '-u', USER, '-d', '--restart', 'unless-stopped',
        '--name', 'watchtower', '-v',
        '/var/run/docker.sock:/var/run/docker.sock', 'v2tec/watchtower', image
    ])
    print(
        "Node is running! \nTo check logs call: docker logs --follow nearcore")

"""Runs NEAR core outside of docker."""

def run_nodocker(home_dir, is_release, boot_nodes, telemetry_url, verbose):
    print("Starting NEAR client...")
    print(
        "Autoupdate is not supported at the moment for runs outside of docker")
    cmd = ['./target/%s/neard' % ('release' if is_release else 'debug')]
    cmd.extend(['--home', home_dir])
    if verbose:
        cmd += ['--verbose', '']
    cmd.append('run')
    if telemetry_url:
        cmd.append('--telemetry-url=%s' % telemetry_url)
    if boot_nodes:
        cmd.append('--boot-nodes=%s' % boot_nodes)
    try:
        subprocess.call(cmd)
    except KeyboardInterrupt:
        print("\nStopping NEARCore.")

def setup_and_run(nodocker,
                  is_release,
                  image,
                  home_dir,
                  init_flags,
                  boot_nodes,
                  telemetry_url,
                  verbose=False,
                  no_gas_price=False):
    if nodocker:
        install_cargo()
    else:
        try:
            subprocess.check_output(['docker', 'pull', image])
            subprocess.check_output(['docker', 'pull', 'v2tec/watchtower'])
        except subprocess.CalledProcessError as exc:
            print("Failed to fetch docker containers: %s" % exc)
            exit(1)

    check_and_setup(nodocker, is_release, image, home_dir, init_flags,
                    no_gas_price)

    print_staking_key(home_dir)

    if nodocker:
        run_nodocker(home_dir, is_release, boot_nodes, telemetry_url, verbose)
    else:
        run_docker(image, home_dir, boot_nodes, telemetry_url, verbose)

"""Stops docker for Nearcore and watchtower if they are running."""

def stop_docker():
    docker_stop_if_exists('watchtower')
    docker_stop_if_exists('nearcore')

def generate_node_key(home, is_release, nodocker, image):
    print("Generating node key...")
    if nodocker:
        cmd = [
            './target/%s/keypair-generator' %
            ('release' if is_release else 'debug')
        ]
        cmd.extend(['--home', home])
        cmd.extend(['--generate-config'])
        cmd.extend(['node-key'])
        try:
            subprocess.call(cmd)
        except KeyboardInterrupt:
            print("\nStopping NEARCore.")
    else:
        subprocess.check_output(['mkdir', '-p', home])
        subprocess.check_output([
            'docker', 'run', '-u', USER, '-v',
            '%s:/srv/keypair-generator' % home, image, 'keypair-generator',
            '--home=/srv/keypair-generator', '--generate-config', 'node-key'
        ])
    print("Node key generated")

def generate_validator_key(home, is_release, nodocker, image, account_id):
    print("Generating validator key...")
    if nodocker:
        cmd = [
            './target/%s/keypair-generator' %
            ('release' if is_release else 'debug')
        ]
        cmd.extend(['--home', home])
        cmd.extend(['--generate-config'])
        cmd.extend(['--account-id', account_id])
        cmd.extend(['validator-key'])
        try:
            subprocess.call(cmd)
        except KeyboardInterrupt:
            print("\nStopping NEARCore.")
    else:
        subprocess.check_output(['mkdir', '-p', home])
        subprocess.check_output([
            'docker', 'run', '-u', USER, '-v',
            '%s:/srv/keypair-generator' % home, image, 'keypair-generator',
            '--home=/srv/keypair-generator', '--generate-config',
            '--account-id=%s' % account_id, 'validator-key'
        ])
    print("Validator key generated")

def generate_signer_key(home, is_release, nodocker, image, account_id):
    print("Generating signer keys...")
    if nodocker:
        cmd = [
            './target/%s/keypair-generator' %
            ('release' if is_release else 'debug')
        ]
        cmd.extend(['--home', home])
        cmd.extend(['--generate-config'])
        cmd.extend(['--account-id', account_id])
        cmd.extend(['signer-keys'])
        try:
            subprocess.call(cmd)
        except KeyboardInterrupt:
            print("\nStopping NEARCore.")
    else:
        subprocess.check_output(['mkdir', '-p', home])
        subprocess.check_output([
            'docker', 'run', '-u', USER, '-v',
            '%s:/srv/keypair-generator' % home, image, 'keypair-generator',
            '--home=/srv/keypair-generator', '--generate-config',
            '--account-id=%s' % account_id, 'signer-keys'
        ])
    print("Signer keys generated")

def initialize_keys(home, is_release, nodocker, image, account_id,
                    generate_signer_keys):
    if nodocker:
        install_cargo()
        compile_package('keypair-generator', is_release)
    else:
        try:
            subprocess.check_output(['docker', 'pull', image])
        except subprocess.CalledProcessError as exc:
            print("Failed to fetch docker containers: %s" % exc)
            exit(1)
    if generate_signer_keys:
        generate_signer_key(home, is_release, nodocker, image, account_id)
    generate_node_key(home, is_release, nodocker, image)
    if account_id:
        generate_validator_key(home, is_release, nodocker, image, account_id)

def create_genesis(home, is_release, nodocker, image, chain_id, tracked_shards):
    if os.path.exists(os.path.join(home, 'genesis.json')):
        print("Genesis already exists")
        return
    print("Creating genesis...")
    if not os.path.exists(os.path.join(home, 'accounts.csv')):
        raise Exception(
            "Failed to generate genesis: accounts.csv does not exist")
    if nodocker:
        cmd = [
            './target/%s/genesis-csv-to-json' %
            ('release' if is_release else 'debug')
        ]
        if home:
            cmd.extend(['--home', home])
        if chain_id:
            cmd.extend(['--chain-id', chain_id])
        if len(tracked_shards) > 0:
            cmd.extend(['--tracked-shards', tracked_shards])
        try:
            subprocess.call(cmd)
        except KeyboardInterrupt:
            print("\nStopping NEARCore.")
    else:
        subprocess.check_output(['mkdir', '-p', home])
        cmd = [
            'docker',
            'run',
            '-u',
            USER,
            '-v',
            '%s:/srv/genesis-csv-to-json' % home,
            image,
            'genesis-csv-to-json',
            '--home=/srv/genesis-csv-to-json',
        ]
        if chain_id:
            cmd.extend(['--chain-id', chain_id])
        if tracked_shards:
            cmd.extend(['--tracked-shards', tracked_shards])
        subprocess.check_output(cmd)
    print("Genesis created")

def start_stakewars(home, is_release, nodocker, image, telemetry_url, verbose,
                    tracked_shards):
    if nodocker:
        install_cargo()
        compile_package('genesis-csv-to-json', is_release)
        compile_package('neard', is_release)
    else:
        try:
            subprocess.check_output(['docker', 'pull', image])
        except subprocess.CalledProcessError as exc:
            print("Failed to fetch docker containers: %s" % exc)
            exit(1)
    create_genesis(home, is_release, nodocker, image, 'stakewars',
                   tracked_shards)
    if nodocker:
        run_nodocker(home,
                     is_release,
                     boot_nodes='',
                     telemetry_url=telemetry_url,
                     verbose=verbose)
    else:
        run_docker(image,
                   home,
                   boot_nodes='',
                   telemetry_url=telemetry_url,
                   verbose=verbose)

'''
'''--- scripts/parallel_coverage.py ---
#!/usr/bin/env python3
import os
from testlib import clean_binary_tests, build_tests, test_binaries, current_path
from concurrent.futures import ThreadPoolExecutor, as_completed
import subprocess
import glob
from itertools import zip_longest
from multiprocessing import cpu_count

def grouper(iterable, n, fillvalue=None):
    args = [iter(iterable)] * n
    return zip_longest(*args, fillvalue=fillvalue)

def coverage(test_binary):
    """ Run a single test coverage by copying to docker, save exitcode, stdout and stderr """
    src_dir = os.path.abspath('.')
    test_binary_basename = os.path.basename(test_binary)
    coverage_output = f'target/cov0/{test_binary_basename}'
    subprocess.check_output(f'mkdir -p {coverage_output}', shell=True)
    coverage_output = os.path.abspath(coverage_output)

    if not os.path.isfile(test_binary):
        return -1, '', f'{test_binary} does not exist'

    p = subprocess.Popen([
        'docker', 'run', '--rm', '--security-opt', 'seccomp=unconfined', '-u',
        f'{os.getuid()}:{os.getgid()}', '-v', f'{test_binary}:{test_binary}',
        '-v', f'{src_dir}:{src_dir}', '-v',
        f'{coverage_output}:{coverage_output}',
        'nearprotocol/near-coverage-runtime', 'bash', '-c',
        f'/usr/local/bin/kcov --include-pattern=nearcore --exclude-pattern=.so --verify {coverage_output} {test_binary}'
    ],
                         stdout=subprocess.PIPE,
                         stderr=subprocess.PIPE,
                         universal_newlines=True)
    stdout, stderr = p.communicate()
    return (p.returncode, stdout, stderr)

def clean_coverage():
    subprocess.check_output(f'rm -rf {current_path}/../target/cov*', shell=True)
    subprocess.check_output(f'rm -rf {current_path}/../target/merged_coverage',
                            shell=True)

def coverage_dir(i):
    return f'{current_path}/../target/cov{i}'

def merge_coverage(i, to_merge, j):
    p = subprocess.Popen([
        'kcov', '--merge',
        os.path.join(coverage_dir(i + 1), str(j)), *to_merge
    ],
                         stdout=subprocess.PIPE,
                         stderr=subprocess.PIPE)
    stdout, stderr = p.communicate()
    return (p.returncode, stdout, stderr)

if __name__ == "__main__":
    clean_coverage()
    clean_binary_tests()
    build_tests()
    binaries = test_binaries(exclude=[
        r'test_regression-.*',
        r'near-.*',
        r'test_cases_runtime-.*',
        r'test_cases_testnet_rpc-.*',
        r'test_catchup-.*',
        r'test_errors-.*',
        r'test_rejoin-.*',
        r'test_simple-.*',
        r'test_tps_regression-.*',
    ])
    errors = False

    # Run coverage
    with ThreadPoolExecutor(max_workers=cpu_count()) as executor:
        future_to_binary = {
            executor.submit(coverage, binary): binary for binary in binaries
        }
        for future in as_completed(future_to_binary):
            binary = future_to_binary[future]
            result = future.result()
            if result[0] != 0:
                print(result[2])
                errors = True
                print(
                    f'========= error: kcov {binary} fail, exit code {result[0]} cause coverage fail'
                )
            else:
                print(f'========= kcov {binary} done')

    # Merge coverage
    i = 0
    j = 0
    with ThreadPoolExecutor(max_workers=cpu_count()) as executor:
        while True:
            covs = glob.glob(f'{coverage_dir(i)}/*')
            if len(covs) == 1:
                break
            subprocess.check_output(f'mkdir -p {coverage_dir(i+1)}', shell=True)

            cov_to_merge = list(grouper(covs, 2))
            if cov_to_merge[-1][-1] is None:
                # ensure the last to merge is not only one cov
                cov_to_merge[-2] += (cov_to_merge[-1][0],)
                del cov_to_merge[-1]

            futures = []
            for cov in cov_to_merge:
                j += 1
                futures.append(executor.submit(merge_coverage, i, cov, j))

            for f in as_completed(futures):
                pass

            i += 1

    merged_coverage = os.path.join(coverage_dir(i), str(j))
    print(f'========= coverage merged to {merged_coverage}')
    subprocess.check_output(
        ['mv', merged_coverage, f'{current_path}/../merged_coverage'])
    subprocess.check_output(f'rm -rf {current_path}/../target/cov*', shell=True)

    if errors:
        print(
            f'========= some errors in running kcov, coverage maybe inaccurate')

'''
'''--- scripts/remote_diff/show_config_hash.py ---
#!/usr/bin/env python3
"""
Shows md5sum of /home/ubuntu/.near/config.json on provided google cloud machines.

Usage: ./show_config_hash.py project host1 host2 host3 ...
Example for testnet canaries:
    ./show_config_hash.py near-core testnet-canary-rpc-01-europe-north1-a-1f3e1e97 \
    testnet-canary-rpc-02-europe-west2-a-031e15e8 testnet-canary-rpc-archive-01-asia-east2-a-b25465d1 \
    testnet-canary-validator-01-us-west1-a-f160e149
"""
import sys
from utils import display_table, run_on_machine

def install_jq(project, host, user='ubuntu'):
    run_on_machine("sudo apt-get install jq -y", user, host, project)

def get_canonical_md5sum(project, host, user='ubuntu'):
    install_jq(project, host, user)
    return run_on_machine("jq --sort-keys . ~/.near/config.json | md5sum", user,
                          host, project)

def display_hashes(names, hashes):
    rows = sorted(zip(names, hashes), key=lambda x: x[1])
    display_table([("name", "hash")] + rows)

if __name__ == '__main__':
    project = sys.argv[1]
    hosts = sys.argv[2:]
    md5sums = [get_canonical_md5sum(project, host) for host in hosts]
    display_hashes(hosts, md5sums)

'''
'''--- scripts/remote_diff/show_neard_version.py ---
#!/usr/bin/env python3
"""
Shows results of '/home/ubuntu/neard -V' on provided google cloud machines.
Usage: ./show_neard_version.py project host1 host2 host3 ...
Example for testnet canaries:
    ./show_neard_version.py near-core testnet-canary-rpc-01-europe-north1-a-1f3e1e97 \
    testnet-canary-rpc-02-europe-west2-a-031e15e8 testnet-canary-rpc-archive-01-asia-east2-a-b25465d1 \
    testnet-canary-validator-01-us-west1-a-f160e149
"""
import sys
from utils import display_table, run_on_machine

def get_neard_info(project, host, user='ubuntu'):
    return run_on_machine("./neard -V", user, host, project)

def display_neard_info(hosts, neard_info, user='ubuntu'):
    display_table([[host] + neard_info.split(' ')[1:]
                   for (host, neard_info) in zip(hosts, neard_infos)])

if __name__ == '__main__':
    project = sys.argv[1]
    hosts = sys.argv[2:]
    neard_infos = [get_neard_info(project, host) for host in hosts]
    display_neard_info(hosts, neard_infos)

'''
'''--- scripts/remote_diff/utils.py ---
import subprocess

def get_zone(project, host):
    zone_call = subprocess.run(
        [
            f"gcloud compute instances list \
        --project={project} | grep ^{host} | awk -F ' ' '{{print $2}}'"
        ],
        stdout=subprocess.PIPE,
        check=True,
        text=True,
        shell=True,
    )
    return zone_call.stdout.replace('\n', '')

def run_on_machine(command, user, host, project):
    zone = get_zone(project, host)
    call = subprocess.run(
        [
            f"gcloud compute ssh {user}@{host} --command='{command}' --project {project} --zone {zone}"
        ],
        stdout=subprocess.PIPE,
        check=True,
        text=True,
        shell=True,
    )
    return call.stdout.replace('\n', '')

def display_table(rows):
    widths = [
        max([len(str(row[i])) for row in rows]) for i in range(len(rows[0]))
    ]
    format_str = " | ".join([f"{{:<{w}}}" for w in widths])
    for row in rows:
        print(format_str.format(*row))

'''
'''--- scripts/state/mega-migrate.py ---
#!/usr/bin/env python3

# Migrates from 0.4 to the latest version.
# When adding changes to the genesis config:
# - add a new version to the change log
# - implement the migration code using the following template:
# ```
# if config_version == 5:
#     # add migration code here
#     pass
#     # increment config version
#     config_version = 6
# ```
#
# Config version change log:
# - #1: Replaces `runtime_config` to use defaults and introduces `config_version`.

import json
import os

filename = os.path.join(os.path.dirname(os.path.realpath(__file__)),
                        '../../near/res/testnet.json')
with open(filename) as fd:
    q = json.load(rd)

config_version = q.get('config_version', 0)

if config_version == 0:
    num_sec_per_year = 31556952
    # The rest of `runtime_config` fields are default
    q['runtime_config'] = {
        'poke_threshold':
            24 * 3600,
        'storage_cost_byte_per_block':
            str(5 * 10**6),
        'account_length_baseline_cost_per_block':
            str(10**24 * 3**8 // num_sec_per_year),
    }
    config_version = 1

# Add future migration code below, without removing the previous migration code.
# Use the following template:
#
# if config_version == 1:
#    ...
#    config_version = 2

# Update the config version in the testnet
q['config_version'] = config_version

# We overwrite the file instead of creating a new one.
with open(filename, 'w') as fd:
    json.dump(q, fd, indent=2, sort_keys=True)

# Dump the config into a separate file for easier reviewing in the git.
# It's not used for the reading genesis.
del q['records']
with open(filename + '.config', 'w') as fd:
    json.dump(q, fd, indent=2, sort_keys=True)

'''
'''--- scripts/state/split-genesis.py ---
#!/usr/bin/env python3

# Splits given genesis.json into 2 files:
# - genesis_config.json - that contains all fields except for the records
# - genesis_records.json - that contains all records from the genesis.json
# It keeps key order in the genesis config file.

import json
import os
import sys
from collections import OrderedDict

filename = sys.argv[1]
with open(filename) as fd:
    q = json.load(fd, object_pairs_hook=OrderedDict)

records = q['records']
q['records'] = []

dirname = os.path.dirname(filename)
with open(os.path.join(dirname, 'genesis_config.json'), 'w') as fd:
    json.dump(q, fd, indent=2)
with open(os.path.join(dirname, '_genesis_records.json'), 'w') as fd:
    json.dump(records, fd, indent=2)

'''
'''--- scripts/state/update_res.py ---
#!/usr/bin/env python3
"""Checks or updates contents of nearcore/res/genesis_config.json file.

* `update_res.py` updates nearcore/res/genesis_config.json to current
  `near init` version without any records.

* `update_res.py check` checks whether nearcore/res/genesis_config.json
  file matches what `near init` generates.
"""

import collections
import json
import os
import pathlib
import subprocess
import sys
import tempfile

def main():
    if len(sys.argv) == 1:
        update_res()
    elif len(sys.argv) == 2 and sys.argv[1] == 'check':
        check_res()
    else:
        sys.exit(__doc__.rstrip())

GENESIS_REPO_PATH = 'chain/jsonrpc/jsonrpc-tests/res/genesis_config.json'
REPO_FULL_PATH = pathlib.Path(__file__).resolve().parent.parent.parent
GENESIS_FULL_PATH = REPO_FULL_PATH / GENESIS_REPO_PATH
SCRIPT_REPO_PATH = pathlib.Path(__file__).resolve().relative_to(REPO_FULL_PATH)

def near_init_genesis():
    with tempfile.TemporaryDirectory() as tempdir:
        args = ['--home', tempdir, 'init', '--chain-id', 'sample']
        prebuilt_neard = os.environ.get("CURRENT_NEARD")
        if prebuilt_neard is not None:
            subprocess.check_call([prebuilt_neard] + args)
        else:
            subprocess.check_call(
                ['cargo', 'run', '-p', 'neard', '--bin', 'neard', '--'] + args)
        with open(os.path.join(tempdir, 'genesis.json')) as rd:
            genesis = json.load(rd, object_pairs_hook=collections.OrderedDict)
    genesis['records'] = []
    # To avoid the genesis config changing each time
    genesis['genesis_time'] = '1970-01-01T00:00:00.000000000Z'
    # Secret key is seeded from test.near
    genesis['validators'][0][
        'public_key'] = 'ed25519:9BmAFNRTa5mRRXpSAm6MxSEeqRASDGNh2FuuwZ4gyxTw'
    return genesis

def update_res():
    genesis = near_init_genesis()
    with open(GENESIS_FULL_PATH, 'w') as wr:
        json.dump(genesis, wr, indent=2)
    print(f'{GENESIS_REPO_PATH} updated')

def check_res():
    want_genesis = near_init_genesis()
    with open(GENESIS_FULL_PATH) as rd:
        got_genesis = json.load(rd)
    if want_genesis != got_genesis:
        sys.exit(
            f'`{GENESIS_REPO_PATH}` does not match `near init` generated one\n'
            f'Please update by running `{SCRIPT_REPO_PATH}` script')

if __name__ == '__main__':
    main()

'''
'''--- scripts/testlib.py ---
#!/usr/bin/env python3

import glob
import os
import subprocess
import fcntl
import re
import filecmp

fcntl.fcntl(1, fcntl.F_SETFL, 0)

current_path = os.path.dirname(os.path.abspath(__file__))
target_debug = os.path.abspath(os.path.join(current_path, '../target/debug'))

def clean_binary_tests():
    if os.environ.get('RFCI_COMMIT'):
        return
    for f in glob.glob(f'{target_debug}/*'):
        if os.path.isfile(f):
            os.remove(f)

def build_tests(nightly=False):
    command = ['cargo', 'test', '--workspace', '--no-run']
    if nightly:
        command += ['--features', 'nightly']
    print("Building tests using command: ", ' '.join(command))
    p = subprocess.run(command)
    if p.returncode != 0:
        os._exit(p.returncode)

def run_doc_tests(nightly=False):
    command = ['cargo', 'test', '--workspace', '--doc']
    if nightly:
        command += ['--features', 'nightly']
    print("Building doc tests using command: ", ' '.join(command))
    p = subprocess.run(command)
    if p.returncode != 0:
        os._exit(p.returncode)

def test_binaries(exclude=None):
    binaries = []
    for f in glob.glob(f'{target_debug}/deps/*'):
        fname = os.path.basename(f)
        ext = os.path.splitext(fname)[1]
        is_near_binary = filecmp.cmp(f, f'{target_debug}/near') or filecmp.cmp(
            f, f'{target_debug}/neard')
        if os.path.isfile(f) and not is_near_binary and ext == '':
            if not exclude:
                binaries.append(f)
            elif not any(map(lambda e: re.match(e, fname), exclude)):
                binaries.append(f)
            else:
                print(f'========= ignore {f}')
    return binaries

def run_test(test_binary, isolate=True):
    """ Run a single test, save exitcode, stdout and stderr """
    if isolate:
        cmd = [
            'docker', 'run', '--rm', '-u', f'{os.getuid()}:{os.getgid()}', '-v',
            f'{test_binary}:{test_binary}', 'nearprotocol/near-test-runtime',
            'bash', '-c', f'RUST_BACKTRACE=1 {test_binary}'
        ]
    else:
        cmd = [test_binary]
    print(f'========= run test {test_binary}')
    if os.path.isfile(test_binary):
        p = subprocess.Popen(cmd,
                             stdout=subprocess.PIPE,
                             stderr=subprocess.PIPE,
                             universal_newlines=True)
        stdout, stderr = p.communicate()
        return (p.returncode, stdout, stderr)
    return -1, '', f'{test_binary} does not exist'

'''
'''--- tools/debug-ui/public/index.html ---
<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="theme-color" content="#000000" />
  <title>React App</title>
</head>

<body>
  <noscript>You need to enable JavaScript to run this app.</noscript>
  <div id="root"></div>
  <!--
      This HTML file is a template.
      If you open it directly in the browser, you will see an empty page.

      You can add webfonts, meta tags, or analytics to this file.
      The build step will place the bundled scripts into the <body> tag.

      To begin the development, run `npm start` or `yarn start`.
      To create a production bundle, use `npm run build` or `yarn build`.
    -->
</body>

</html>
'''