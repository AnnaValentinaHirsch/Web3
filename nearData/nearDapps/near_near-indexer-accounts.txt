*GitHub Repository "near/near-indexer-accounts"*

'''--- Cargo.toml ---
[package]
name = "indexer-accounts"
version = "0.1.0"
edition = "2021"
rust-version = "1.61.0"

[lib]
proc-macro = true

[dependencies]
anyhow = "1.0.51"
avro-rs = "0.13.0"
base64 = "0.11"
bigdecimal = { version = "0.2", features = ["serde"] }
cached = "0.23.0"
clap = { version = "3.0.0-beta.5", features = ["color", "derive", "env"] }
dotenv = "0.15.0"
futures = "0.3.5"
hex = "0.4"
itertools = "0.9.0"
num-traits = "0.2.11"
serde = { version = "1", features = ["derive"] }
serde_json = "1.0.55"
sqlx = { version = "0.5.13", features = ["runtime-tokio-native-tls", "postgres", "bigdecimal", "json"] }
syn = "1.0.90"
tempfile = "3.3.0"
tokio = { version = "1", features = ["full"] }
tokio-stream = { version = "0.1" }
tracing = "0.1.13"
tracing-subscriber = "0.2.4"
quote = "1.0.17"

near-crypto = "0.12.0"
near-indexer-primitives = "0.12.0"
near-lake-framework = "0.3.0"

'''
'''--- README.md ---
# Indexer Accounts

Async Postgres-compatible solution to load the data from NEAR blockchain.
Based on [NEAR Lake Framework](https://github.com/near/near-lake-framework-rs).

See [Indexer Base](https://github.com/near/near-indexer-base#indexer-base) docs for all the explanations, installation guide, etc.

### What else do I need to know?

Indexer Accounts is the only indexer that modifies the existing data.  
While other indexers are append-only, Indexer Accounts updates the existing records with the deletion info.

`accounts` table in [Indexer For Explorer](https://github.com/near/near-indexer-for-explorer) stored only the first creation and last deletion of the account.  
This solution stores all the creations/deletions, so accounts may appear in the table more than once.

'''
'''--- migrations/20220531131424_initial.sql ---
CREATE TABLE accounts
(
    account_id               text           NOT NULL,
    created_by_receipt_id    text,
    deleted_by_receipt_id    text,
    created_by_block_height numeric(20, 0) NOT NULL,
    deleted_by_block_height numeric(20, 0),
    PRIMARY KEY (account_id, created_by_block_height)
);
ALTER TABLE accounts
    ADD CONSTRAINT accounts_created_by_receipt_id_fk FOREIGN KEY (created_by_receipt_id) REFERENCES action_receipts(receipt_id);
ALTER TABLE accounts
    ADD CONSTRAINT accounts_deleted_by_receipt_id_fk FOREIGN KEY (deleted_by_receipt_id) REFERENCES action_receipts(receipt_id);
CREATE INDEX CONCURRENTLY accounts_account_id_idx ON accounts (account_id);
CREATE INDEX CONCURRENTLY accounts_created_by_block_height_idx ON accounts (created_by_block_height);
CREATE INDEX CONCURRENTLY accounts_deleted_by_block_height_idx ON accounts (deleted_by_block_height);

CREATE TABLE access_keys
(
    public_key               text           NOT NULL,
    account_id               text           NOT NULL,
    created_by_receipt_id    text,
    deleted_by_receipt_id    text,
    created_by_block_height numeric(20, 0) NOT NULL,
    deleted_by_block_height numeric(20, 0),
    permission_kind          text           NOT NULL,
    PRIMARY KEY (public_key, account_id)
);
ALTER TABLE access_keys
    ADD CONSTRAINT access_keys_created_by_receipt_id_fk FOREIGN KEY (created_by_receipt_id) REFERENCES action_receipts(receipt_id);
ALTER TABLE access_keys
    ADD CONSTRAINT access_keys_deleted_by_receipt_id_fk FOREIGN KEY (deleted_by_receipt_id) REFERENCES action_receipts(receipt_id);
CREATE INDEX CONCURRENTLY access_keys_account_id_idx ON access_keys (account_id);
CREATE INDEX CONCURRENTLY access_keys_public_key_idx ON access_keys (public_key);
CREATE INDEX CONCURRENTLY access_keys_created_by_block_height_idx ON access_keys (created_by_block_height);
CREATE INDEX CONCURRENTLY access_keys_deleted_by_block_height_idx ON access_keys (deleted_by_block_height);

'''
'''--- redshift/migration.sql ---
CREATE TABLE accounts
(
    account_id              varchar(64000) NOT NULL,
    created_by_receipt_id   varchar(64000),
    deleted_by_receipt_id   varchar(64000),
    created_by_block_height numeric(20, 0) NOT NULL,
    deleted_by_block_height numeric(20, 0)
);

CREATE TABLE access_keys
(
    public_key              varchar(64000) NOT NULL,
    account_id              varchar(64000) NOT NULL,
    created_by_receipt_id   varchar(64000),
    deleted_by_receipt_id   varchar(64000),
    created_by_block_height numeric(20, 0) NOT NULL,
    deleted_by_block_height numeric(20, 0),
    permission_kind         varchar(64000) NOT NULL
);

'''
'''--- src/configs.rs ---
use clap::Parser;

/// NEAR Indexer for Explorer
/// Watches for stream of blocks from the chain
#[derive(Parser, Debug)]
#[clap(
    version,
    author,
    about,
    disable_help_subcommand(true),
    propagate_version(true),
    next_line_help(true)
)]
pub(crate) struct Opts {
    /// Enabled Indexer for Explorer debug level of logs
    #[clap(long)]
    pub debug: bool,
    // todo
    // /// Store initial data from genesis like Accounts, AccessKeys
    // #[clap(long)]
    // pub store_genesis: bool,
    /// AWS S3 bucket name to get the stream from
    #[clap(long)]
    pub s3_bucket_name: String,
    /// AWS S3 bucket region
    #[clap(long)]
    pub s3_region_name: String,
    /// Block height to start the stream from. If None, start from interruption
    #[clap(long, short)]
    pub start_block_height: Option<u64>,
}

'''
'''--- src/db_adapters/access_keys.rs ---
use std::convert::TryFrom;

use futures::future::try_join_all;
use futures::try_join;

use crate::models;

pub(crate) async fn store_access_keys(
    pool: &sqlx::Pool<sqlx::Postgres>,
    shards: &[near_indexer_primitives::IndexerShard],
    block_height: near_indexer_primitives::types::BlockHeight,
) -> anyhow::Result<()> {
    let futures = shards.iter().map(|shard| {
        store_access_keys_for_chunk(pool, &shard.receipt_execution_outcomes, block_height)
    });

    try_join_all(futures).await.map(|_| ())
}

async fn store_access_keys_for_chunk(
    pool: &sqlx::Pool<sqlx::Postgres>,
    outcomes: &[near_indexer_primitives::IndexerExecutionOutcomeWithReceipt],
    block_height: near_indexer_primitives::types::BlockHeight,
) -> anyhow::Result<()> {
    if outcomes.is_empty() {
        return Ok(());
    }
    let successful_receipts = outcomes
        .iter()
        .filter(|outcome_with_receipt| {
            matches!(
                outcome_with_receipt.execution_outcome.outcome.status,
                near_indexer_primitives::views::ExecutionStatusView::SuccessValue(_)
                    | near_indexer_primitives::views::ExecutionStatusView::SuccessReceiptId(_)
            )
        })
        .map(|outcome_with_receipt| &outcome_with_receipt.receipt);

    let mut created_access_keys: Vec<models::access_keys::AccessKey> = vec![];
    let mut deleted_access_keys: Vec<models::access_keys::AccessKey> = vec![];
    let mut access_keys_from_deleted_accounts: Vec<models::access_keys::AccessKey> = vec![];

    for receipt in successful_receipts {
        if let near_indexer_primitives::views::ReceiptEnumView::Action { actions, .. } =
            &receipt.receipt
        {
            for action in actions {
                match action {
                    near_indexer_primitives::views::ActionView::DeleteAccount { .. } => {
                        access_keys_from_deleted_accounts.push(
                            models::access_keys::AccessKey::access_key_to_delete(
                                "".to_string(),
                                &receipt.receiver_id,
                                &receipt.receipt_id,
                                block_height,
                            ),
                        );
                    }
                    near_indexer_primitives::views::ActionView::AddKey {
                        public_key,
                        access_key,
                    } => {
                        created_access_keys.push(models::access_keys::AccessKey::from_action_view(
                            public_key,
                            &receipt.receiver_id,
                            access_key,
                            &receipt.receipt_id,
                            block_height,
                        ));
                    }
                    near_indexer_primitives::views::ActionView::DeleteKey { public_key } => {
                        deleted_access_keys.push(
                            models::access_keys::AccessKey::access_key_to_delete(
                                public_key.to_string(),
                                &receipt.receiver_id,
                                &receipt.receipt_id,
                                block_height,
                            ),
                        );
                    }
                    near_indexer_primitives::views::ActionView::Transfer { .. } => {
                        if receipt.receiver_id.len() == 64usize {
                            // we can just insert it, the duplicates will be ignored by the db
                            if let Ok(public_key_bytes) = hex::decode(receipt.receiver_id.as_ref())
                            {
                                if let Ok(public_key) =
                                    near_crypto::ED25519PublicKey::try_from(&public_key_bytes[..])
                                {
                                    created_access_keys.push(
                                        models::access_keys::AccessKey::from_action_view(
                                            &near_crypto::PublicKey::from(public_key.clone()),
                                            &receipt.receiver_id,
                                            &near_indexer_primitives::views::AccessKeyView {
                                                nonce: 0,
                                                permission: near_indexer_primitives::views::AccessKeyPermissionView::FullAccess
                                            },
                                            &receipt.receipt_id,
                                            block_height
                                        ),
                                    );
                                }
                            }
                        }
                    }
                    _ => continue,
                }
            }
        }
    }

    let update_access_keys_for_deleted_accounts_future = async {
        let query = r"UPDATE access_keys
                            SET deleted_by_receipt_id = $4, deleted_by_block_height = $6
                            WHERE account_id = $2
                                AND created_by_block_height < $6
                                AND deleted_by_block_height IS NULL";
        models::update_retry_or_panic(pool, query, &access_keys_from_deleted_accounts, 10).await
    };

    let update_access_keys_future = async {
        let query = r"UPDATE access_keys
                            SET deleted_by_receipt_id = $4, deleted_by_block_height = $6
                            WHERE account_id = $2 AND public_key = $1
                                AND created_by_block_height < $6
                                AND deleted_by_block_height IS NULL";
        models::update_retry_or_panic(pool, query, &deleted_access_keys, 10).await
    };

    let add_access_keys_future =
        async { models::chunked_insert(pool, &created_access_keys, 10).await };

    try_join!(
        update_access_keys_for_deleted_accounts_future,
        update_access_keys_future,
        add_access_keys_future
    )?;

    Ok(())
}

'''
'''--- src/db_adapters/accounts.rs ---
use crate::models;
use bigdecimal::BigDecimal;
use futures::future::try_join_all;
use futures::try_join;

pub(crate) async fn store_accounts(
    pool: &sqlx::Pool<sqlx::Postgres>,
    shards: &[near_indexer_primitives::IndexerShard],
    block_height: near_indexer_primitives::types::BlockHeight,
) -> anyhow::Result<()> {
    let futures = shards.iter().map(|shard| {
        store_accounts_for_chunk(pool, &shard.receipt_execution_outcomes, block_height)
    });

    try_join_all(futures).await.map(|_| ())
}

async fn store_accounts_for_chunk(
    pool: &sqlx::Pool<sqlx::Postgres>,
    outcomes: &[near_indexer_primitives::IndexerExecutionOutcomeWithReceipt],
    block_height: near_indexer_primitives::types::BlockHeight,
) -> anyhow::Result<()> {
    if outcomes.is_empty() {
        return Ok(());
    }
    let successful_receipts = outcomes
        .iter()
        .filter(|outcome_with_receipt| {
            matches!(
                outcome_with_receipt.execution_outcome.outcome.status,
                near_indexer_primitives::views::ExecutionStatusView::SuccessValue(_)
                    | near_indexer_primitives::views::ExecutionStatusView::SuccessReceiptId(_)
            )
        })
        .map(|outcome_with_receipt| &outcome_with_receipt.receipt);

    let mut accounts_to_create: Vec<models::accounts::Account> = vec![];
    let mut accounts_to_update: Vec<models::accounts::Account> = vec![];

    for receipt in successful_receipts {
        if let near_indexer_primitives::views::ReceiptEnumView::Action { actions, .. } =
            &receipt.receipt
        {
            for action in actions {
                match action {
                    near_indexer_primitives::views::ActionView::CreateAccount => {
                        accounts_to_create.push(models::accounts::Account::new_from_receipt(
                            &receipt.receiver_id,
                            &receipt.receipt_id,
                            block_height,
                        ));
                    }
                    near_indexer_primitives::views::ActionView::Transfer { .. } => {
                        if receipt.receiver_id.len() == 64usize {
                            let query = r"SELECT * FROM accounts
                                                WHERE account_id = $1
                                                    AND created_by_block_height < $2::numeric(20, 0)
                                                    AND (deleted_by_block_height IS NULL OR deleted_by_block_height > $2::numeric(20, 0))";
                            let previously_created = models::select_retry_or_panic(
                                pool,
                                query,
                                &[receipt.receiver_id.to_string(), block_height.to_string()],
                                10,
                            )
                            .await?;
                            if previously_created.is_empty() {
                                accounts_to_create.push(
                                    models::accounts::Account::new_from_receipt(
                                        &receipt.receiver_id,
                                        &receipt.receipt_id,
                                        block_height,
                                    ),
                                );
                            }
                        }
                    }
                    near_indexer_primitives::views::ActionView::DeleteAccount { .. } => {
                        accounts_to_update.push(models::accounts::Account {
                            account_id: receipt.receiver_id.to_string(),
                            created_by_receipt_id: None,
                            deleted_by_receipt_id: Some(receipt.receipt_id.to_string()),
                            created_by_block_height: Default::default(),
                            deleted_by_block_height: Some(BigDecimal::from(block_height)),
                        });
                    }
                    _ => {}
                }
            }
        }
    }

    let create_accounts_future =
        async { models::chunked_insert(pool, &accounts_to_create, 10).await };

    let update_accounts_future = async {
        let query = r"UPDATE accounts
                            SET deleted_by_receipt_id = $3, deleted_by_block_height = $5
                            WHERE account_id = $1
                                AND created_by_block_height < $5
                                AND deleted_by_block_height IS NULL";
        models::update_retry_or_panic(pool, query, &accounts_to_update, 10).await
    };

    try_join!(create_accounts_future, update_accounts_future)?;
    Ok(())
}

'''
'''--- src/db_adapters/genesis.rs ---
// TODO how to store genesis? How it's stored in S3?
use actix_diesel::Database;
use diesel::PgConnection;

use crate::db_adapters::access_keys::store_access_keys_from_genesis;
use crate::db_adapters::accounts::store_accounts_from_genesis;

/// This is an ugly hack that allows to execute an async body on a specified actix runtime.
/// You should only call it from a separate thread!
///
/// ```ignore
/// async fn some_async_function() {
///     let current_actix_system = actix::System::current();
///     tokio::tasks::spawn_blocking(move || {
///         let x = vec![0, 1, 2];
///         x.map(|i| {
///             block_on(current_actix_system, async move {
///                 reqwest::get(...).await.text().await
///             })
///         });
///     }
/// }
fn block_on<Fut, T>(
    actix_arbiter: &actix_rt::ArbiterHandle,
    f: Fut,
) -> Result<T, std::sync::mpsc::RecvError>
where
    T: Send + 'static,
    Fut: std::future::Future<Output = T> + Send + 'static,
{
    let (tx, rx) = std::sync::mpsc::channel();
    actix_arbiter.spawn(async move {
        let result = f.await;
        let _ = tx.send(result);
    });
    rx.recv()
}

/// Iterates over GenesisRecords and stores selected ones (Accounts, AccessKeys)
/// to database.
/// Separately stores records divided in portions by 5000 to optimize
/// memory usage and minimize database queries
pub(crate) async fn store_genesis_records(
    pool: Database<PgConnection>,
    near_config: near_indexer::NearConfig,
) -> anyhow::Result<()> {
    tracing::info!(
        target: crate::INDEXER_FOR_EXPLORER,
        "Storing genesis records to database...",
    );
    let genesis_height = near_config.genesis.config.genesis_height;

    // Remember the current actix runtime thread in order to be able to
    // schedule async function on it from the thread that processes genesis in
    // a blocking way.
    let actix_system = actix::System::current();
    // Spawn the blocking genesis processing on a separate thread
    tokio::task::spawn_blocking(move || {
        let actix_arbiter = actix_system.arbiter();

        let mut accounts_to_store: Vec<crate::models::accounts::Account> = vec![];
        let mut access_keys_to_store: Vec<crate::models::access_keys::AccessKey> = vec![];

        near_config.genesis.for_each_record(|record| {
            if accounts_to_store.len() == 5_000 {
                let mut accounts_to_store_chunk = vec![];
                std::mem::swap(&mut accounts_to_store, &mut accounts_to_store_chunk);
                let pool = pool.clone();
                block_on(
                    actix_arbiter,
                    store_accounts_from_genesis(pool, accounts_to_store_chunk),
                )
                .expect("storing accounts from genesis failed")
                .expect("storing accounts from genesis failed");
            }
            if access_keys_to_store.len() == 5_000 {
                let mut access_keys_to_store_chunk = vec![];
                std::mem::swap(&mut access_keys_to_store, &mut access_keys_to_store_chunk);
                let pool = pool.clone();
                block_on(
                    actix_arbiter,
                    store_access_keys_from_genesis(pool, access_keys_to_store_chunk),
                )
                .expect("storing access keys from genesis failed")
                .expect("storing access keys from genesis failed");
            }

            match record {
                near_indexer_primitives::state_record::StateRecord::Account {
                    account_id,
                    ..
                } => {
                    accounts_to_store.push(crate::models::accounts::Account::new_from_genesis(
                        account_id,
                        genesis_height,
                    ));
                }
                near_indexer_primitives::state_record::StateRecord::AccessKey {
                    account_id,
                    public_key,
                    access_key,
                } => {
                    access_keys_to_store.push(crate::models::access_keys::AccessKey::from_genesis(
                        public_key,
                        account_id,
                        access_key,
                        genesis_height,
                    ));
                }
                _ => {}
            };
        });

        let fut = || async move {
            store_accounts_from_genesis(pool.clone(), accounts_to_store).await?;
            store_access_keys_from_genesis(pool, access_keys_to_store).await?;
            anyhow::Result::<()>::Ok(())
        };
        block_on(actix_arbiter, fut())
            .expect("storing leftover accounts and access keys from genesis failed")
            .expect("storing leftover accounts and access keys from genesis failed");
    })
    .await?;

    tracing::info!(
        target: crate::INDEXER_FOR_EXPLORER,
        "Genesis records has been stored.",
    );
    Ok(())
}

'''
'''--- src/db_adapters/mod.rs ---
pub(crate) mod access_keys;
pub(crate) mod accounts;
// pub(crate) mod genesis;

pub(crate) const CHUNK_SIZE_FOR_BATCH_INSERT: usize = 100;

'''
'''--- src/lib.rs ---
use proc_macro::TokenStream;
use quote::quote;
use syn::{parse_macro_input, ItemStruct};

#[proc_macro_derive(FieldCount)]
pub fn derive_field_count(input: TokenStream) -> TokenStream {
    let input = parse_macro_input!(input as ItemStruct);

    let name = &input.ident;
    let (impl_generics, ty_generics, where_clause) = input.generics.split_for_impl();
    let field_count = input.fields.iter().count();

    let output = quote! {
        impl #impl_generics FieldCount for #name #ty_generics #where_clause {
            fn field_count() -> usize {
                #field_count
            }
        }
    };

    TokenStream::from(output)
}

'''
'''--- src/main.rs ---
// TODO cleanup imports in all the files in the end
use clap::Parser;
use dotenv::dotenv;
use futures::{try_join, StreamExt};
use std::env;
use tracing_subscriber::EnvFilter;

use crate::configs::Opts;

mod configs;
mod db_adapters;
mod models;

// Categories for logging
// TODO naming
pub(crate) const INDEXER: &str = "indexer";

const INTERVAL: std::time::Duration = std::time::Duration::from_millis(100);
const MAX_DELAY_TIME: std::time::Duration = std::time::Duration::from_secs(120);

#[tokio::main]
async fn main() -> anyhow::Result<()> {
    dotenv().ok();

    let opts: Opts = Opts::parse();

    // let options = sqlx::postgres::PgConnectOptions::new()
    //     .host(&env::var("DB_HOST")?)
    //     .port(env::var("DB_PORT")?.parse()?)
    //     .username(&env::var("DB_USER")?)
    //     .password(&env::var("DB_PASSWORD")?)
    //     .database(&env::var("DB_NAME")?)
    //     .extra_float_digits(2);

    // let pool = sqlx::PgPool::connect_with(options).await?;
    let pool = sqlx::PgPool::connect(&env::var("DATABASE_URL")?).await?;
    // TODO Error: while executing migrations: error returned from database: 1128 (HY000): Function 'near_indexer.GET_LOCK' is not defined
    // sqlx::migrate!().run(&pool).await?;

    // let start_block_height = match opts.start_block_height {
    //     Some(x) => x,
    //     None => models::start_after_interruption(&pool).await?,
    // };
    let config = near_lake_framework::LakeConfig {
        s3_config: None,
        s3_bucket_name: opts.s3_bucket_name.clone(),
        s3_region_name: opts.s3_region_name.clone(),
        start_block_height: opts.start_block_height.unwrap(),
    };
    init_tracing();

    let stream = near_lake_framework::streamer(config);

    let mut handlers = tokio_stream::wrappers::ReceiverStream::new(stream)
        .map(|streamer_message| handle_streamer_message(streamer_message, &pool))
        .buffer_unordered(1usize);

    // let mut time_now = std::time::Instant::now();
    while let Some(handle_message) = handlers.next().await {
        match handle_message {
            Ok(block_height) => {
                // let elapsed = time_now.elapsed();
                // println!(
                //     "Elapsed time spent on block {}: {:.3?}",
                //     block_height, elapsed
                // );
                // time_now = std::time::Instant::now();
            }
            Err(e) => {
                return Err(anyhow::anyhow!(e));
            }
        }
    }

    Ok(())
}

async fn handle_streamer_message(
    streamer_message: near_indexer_primitives::StreamerMessage,
    pool: &sqlx::Pool<sqlx::Postgres>,
) -> anyhow::Result<u64> {
    if streamer_message.block.header.height % 100 == 0 {
        eprintln!(
            "{} / shards {}",
            streamer_message.block.header.height,
            streamer_message.shards.len()
        );
    }

    let accounts_future = db_adapters::accounts::store_accounts(
        pool,
        &streamer_message.shards,
        streamer_message.block.header.height,
    );

    let access_keys_future = db_adapters::access_keys::store_access_keys(
        pool,
        &streamer_message.shards,
        streamer_message.block.header.height,
    );

    try_join!(accounts_future, access_keys_future)?;
    Ok(streamer_message.block.header.height)
}

fn init_tracing() {
    let mut env_filter = EnvFilter::new("near_lake_framework=info");

    if let Ok(rust_log) = std::env::var("RUST_LOG") {
        if !rust_log.is_empty() {
            for directive in rust_log.split(',').filter_map(|s| match s.parse() {
                Ok(directive) => Some(directive),
                Err(err) => {
                    eprintln!("Ignoring directive `{}`: {}", s, err);
                    None
                }
            }) {
                env_filter = env_filter.add_directive(directive);
            }
        }
    }

    tracing_subscriber::fmt::Subscriber::builder()
        .with_env_filter(env_filter)
        .with_writer(std::io::stderr)
        .init();
}

'''
'''--- src/models/access_keys.rs ---
use bigdecimal::BigDecimal;
use sqlx::Arguments;

use crate::models::{FieldCount, PrintEnum};

#[derive(Debug, sqlx::FromRow, FieldCount)]
pub struct AccessKey {
    pub public_key: String,
    pub account_id: String,
    pub created_by_receipt_id: Option<String>,
    pub deleted_by_receipt_id: Option<String>,
    pub created_by_block_height: BigDecimal,
    pub deleted_by_block_height: Option<BigDecimal>,
    pub permission_kind: String,
}

impl AccessKey {
    pub fn access_key_to_delete(
        public_key: String,
        account_id: &near_indexer_primitives::types::AccountId,
        deleted_by_receipt_id: &near_indexer_primitives::CryptoHash,
        deleted_by_block_height: near_indexer_primitives::types::BlockHeight,
    ) -> Self {
        Self {
            public_key,
            account_id: account_id.to_string(),
            created_by_receipt_id: None,
            deleted_by_receipt_id: Some(deleted_by_receipt_id.to_string()),
            created_by_block_height: Default::default(),
            deleted_by_block_height: Some(BigDecimal::from(deleted_by_block_height)),
            permission_kind: "".to_string(),
        }
    }

    pub fn from_action_view(
        public_key: &near_crypto::PublicKey,
        account_id: &near_indexer_primitives::types::AccountId,
        access_key: &near_indexer_primitives::views::AccessKeyView,
        created_by_receipt_id: &near_indexer_primitives::CryptoHash,
        created_by_block_height: near_indexer_primitives::types::BlockHeight,
    ) -> Self {
        Self {
            public_key: public_key.to_string(),
            account_id: account_id.to_string(),
            created_by_receipt_id: Some(created_by_receipt_id.to_string()),
            deleted_by_receipt_id: None,
            created_by_block_height: BigDecimal::from(created_by_block_height),
            deleted_by_block_height: None,
            permission_kind: access_key.permission.print().to_string(),
        }
    }
}

impl crate::models::MySqlMethods for AccessKey {
    fn add_to_args(&self, args: &mut sqlx::postgres::PgArguments) {
        args.add(&self.public_key);
        args.add(&self.account_id);
        args.add(&self.created_by_receipt_id);
        args.add(&self.deleted_by_receipt_id);
        args.add(&self.created_by_block_height);
        args.add(&self.deleted_by_block_height);
        args.add(&self.permission_kind);
    }

    fn insert_query(items_count: usize) -> anyhow::Result<String> {
        Ok("INSERT INTO access_keys VALUES ".to_owned()
            + &crate::models::create_placeholders(items_count, AccessKey::field_count())?
            + " ON CONFLICT DO NOTHING")
    }

    fn name() -> String {
        "access_keys".to_string()
    }
}

'''
'''--- src/models/accounts.rs ---
use bigdecimal::BigDecimal;
use sqlx::Arguments;

use crate::models::FieldCount;

#[derive(Debug, Clone, sqlx::FromRow, FieldCount)]
pub struct Account {
    pub account_id: String,
    pub created_by_receipt_id: Option<String>,
    pub deleted_by_receipt_id: Option<String>,
    pub created_by_block_height: BigDecimal,
    pub deleted_by_block_height: Option<BigDecimal>,
}

impl Account {
    pub fn new_from_receipt(
        account_id: &near_indexer_primitives::types::AccountId,
        created_by_receipt_id: &near_indexer_primitives::CryptoHash,
        created_by_block_height: near_indexer_primitives::types::BlockHeight,
    ) -> Self {
        Self {
            account_id: account_id.to_string(),
            created_by_receipt_id: Some(created_by_receipt_id.to_string()),
            deleted_by_receipt_id: None,
            created_by_block_height: BigDecimal::from(created_by_block_height),
            deleted_by_block_height: None,
        }
    }
}

impl crate::models::MySqlMethods for Account {
    fn add_to_args(&self, args: &mut sqlx::postgres::PgArguments) {
        args.add(&self.account_id);
        args.add(&self.created_by_receipt_id);
        args.add(&self.deleted_by_receipt_id);
        args.add(&self.created_by_block_height);
        args.add(&self.deleted_by_block_height);
    }

    fn insert_query(items_count: usize) -> anyhow::Result<String> {
        Ok("INSERT INTO accounts VALUES ".to_owned()
            + &crate::models::create_placeholders(items_count, Account::field_count())?
            + " ON CONFLICT DO NOTHING")
    }

    fn name() -> String {
        "accounts".to_string()
    }
}

'''
'''--- src/models/mod.rs ---
use futures::future::try_join_all;
use near_indexer_primitives::views::AccessKeyPermissionView;
use sqlx::Arguments;
use std::fmt::Write;

pub use indexer_accounts::FieldCount;

pub(crate) mod access_keys;
pub(crate) mod accounts;

pub trait FieldCount {
    /// Get the number of fields on a struct.
    fn field_count() -> usize;
}

pub trait MySqlMethods {
    fn add_to_args(&self, args: &mut sqlx::postgres::PgArguments);
    fn insert_query(count: usize) -> anyhow::Result<String>;
    fn name() -> String;
}

pub async fn chunked_insert<T: MySqlMethods + std::fmt::Debug>(
    pool: &sqlx::Pool<sqlx::Postgres>,
    items: &[T],
    retry_count: usize,
) -> anyhow::Result<()> {
    let futures = items
        .chunks(crate::db_adapters::CHUNK_SIZE_FOR_BATCH_INSERT)
        .map(|items_part| insert_retry_or_panic(pool, items_part, retry_count));
    try_join_all(futures).await.map(|_| ())
}

async fn insert_retry_or_panic<T: MySqlMethods + std::fmt::Debug>(
    pool: &sqlx::Pool<sqlx::Postgres>,
    items: &[T],
    retry_count: usize,
) -> anyhow::Result<()> {
    let mut interval = crate::INTERVAL;
    let mut retry_attempt = 0usize;
    let query = T::insert_query(items.len())?;

    loop {
        if retry_attempt == retry_count {
            return Err(anyhow::anyhow!(
                "Failed to perform query to database after {} attempts. Stop trying.",
                retry_count
            ));
        }
        retry_attempt += 1;

        let mut args = sqlx::postgres::PgArguments::default();
        for item in items {
            item.add_to_args(&mut args);
        }

        match sqlx::query_with(&query, args).execute(pool).await {
            Ok(_) => break,
            Err(async_error) => {
                eprintln!(
                    "Error occurred during {}:\n{} were not stored. \n{:#?} \n Retrying in {} milliseconds...",
                    async_error,
                    &T::name(),
                    &items,
                    interval.as_millis(),
                );
                tokio::time::sleep(interval).await;
                if interval < crate::MAX_DELAY_TIME {
                    interval *= 2;
                }
            }
        }
    }
    Ok(())
}

// todo it would be great to control how many lines we've updated
pub(crate) async fn update_retry_or_panic<T: MySqlMethods + std::fmt::Debug>(
    pool: &sqlx::Pool<sqlx::Postgres>,
    query: &str,
    items: &[T],
    retry_count: usize,
) -> anyhow::Result<()> {
    for item in items {
        let mut interval = crate::INTERVAL;
        let mut retry_attempt = 0usize;

        loop {
            if retry_attempt == retry_count {
                return Err(anyhow::anyhow!(
                    "Failed to perform query to database after {} attempts. Stop trying.",
                    retry_count
                ));
            }
            retry_attempt += 1;

            let mut args = sqlx::postgres::PgArguments::default();
            item.add_to_args(&mut args);

            match sqlx::query_with(query, args).execute(pool).await {
                Ok(_) => break,
                Err(async_error) => {
                    eprintln!(
                        "Error occurred during {}:\n{} were not updated. \n{:#?} \n Retrying in {} milliseconds...",
                        async_error,
                        &T::name(),
                        &items,
                        interval.as_millis(),
                    );
                    tokio::time::sleep(interval).await;
                    if interval < crate::MAX_DELAY_TIME {
                        interval *= 2;
                    }
                }
            }
        }
    }
    Ok(())
}

pub(crate) async fn select_retry_or_panic(
    pool: &sqlx::Pool<sqlx::Postgres>,
    query: &str,
    substitution_items: &[String],
    retry_count: usize,
) -> anyhow::Result<Vec<sqlx::postgres::PgRow>> {
    let mut interval = crate::INTERVAL;
    let mut retry_attempt = 0usize;

    loop {
        if retry_attempt == retry_count {
            return Err(anyhow::anyhow!(
                "Failed to perform query to database after {} attempts. Stop trying.",
                retry_count
            ));
        }
        retry_attempt += 1;

        let mut args = sqlx::postgres::PgArguments::default();
        for item in substitution_items {
            args.add(item);
        }

        match sqlx::query_with(query, args).fetch_all(pool).await {
            Ok(res) => return Ok(res),
            Err(async_error) => {
                // todo we print here select with non-filled placeholders. It would be better to get the final select statement here
                tracing::error!(
                         target: crate::INDEXER,
                         "Error occurred during {}:\nFailed SELECT:\n{}\n Retrying in {} milliseconds...",
                         async_error,
                    query,
                         interval.as_millis(),
                     );
                tokio::time::sleep(interval).await;
                if interval < crate::MAX_DELAY_TIME {
                    interval *= 2;
                }
            }
        }
    }
}

// Generates `($1, $2), ($3, $4)`
pub(crate) fn create_placeholders(
    mut items_count: usize,
    fields_count: usize,
) -> anyhow::Result<String> {
    if items_count < 1 {
        return Err(anyhow::anyhow!("At least 1 item expected"));
    }

    let mut start_num: usize = 1;
    let mut res = create_placeholder(&mut start_num, fields_count)?;
    items_count -= 1;
    while items_count > 0 {
        write!(
            res,
            ", {}",
            create_placeholder(&mut start_num, fields_count)?
        )?;
        items_count -= 1;
    }

    Ok(res)
}

// Generates `($1, $2, $3)`
pub(crate) fn create_placeholder(
    start_num: &mut usize,
    mut fields_count: usize,
) -> anyhow::Result<String> {
    if fields_count < 1 {
        return Err(anyhow::anyhow!("At least 1 field expected"));
    }
    let mut item = format!("(${}", start_num);
    *start_num += 1;
    fields_count -= 1;
    while fields_count > 0 {
        write!(item, ", ${}", start_num)?;
        *start_num += 1;
        fields_count -= 1;
    }
    item += ")";
    Ok(item)
}

pub(crate) trait PrintEnum {
    fn print(&self) -> &str;
}

impl PrintEnum for AccessKeyPermissionView {
    fn print(&self) -> &str {
        match self {
            AccessKeyPermissionView::FunctionCall { .. } => "FUNCTION_CALL",
            AccessKeyPermissionView::FullAccess => "FULL_ACCESS",
        }
    }
}

'''