*GitHub Repository "near/wasmer"*

'''--- .cargo/config.toml ---
[target.'cfg(all(target_os = "linux", target_env = "gnu"))']
rustflags = [
    # Put the VM functions in the dynamic symbol table.
    "-C", "link-arg=-Wl,-E",
]

'''
'''--- .github/ISSUE_TEMPLATE/---bug-report.md ---
---
name: "\U0001F41E Bug report"
about: Create a report to help us improve
title: ''
labels: "\U0001F41E bug"
assignees: ''

---

<!-- Thanks for the bug report! -->

### Describe the bug

<!--
A clear and concise description of what the bug is.

Copy and paste the result of executing the following in your shell, so we can know the version of wasmer, Rust (if available) and architecture of your environment.
-->

```sh
echo "`wasmer -V` | `rustc -V` | `uname -m`"
```

### Steps to reproduce
<!--
Include steps that will help us recreate the issue.

For example,
1. Go to '‚Ä¶'
2. Compile with '‚Ä¶'
3. Run '‚Ä¶'
4. See error

If applicable, add a link to a test case (as a zip file or link to a repository we can clone).
-->

### Expected behavior
<!-- A clear and concise description of what you expected to happen. -->

### Actual behavior

<!--
A clear and concise description of what actually happened.

If applicable, add screenshots to help explain your problem.
-->

### Additional context
<!-- Add any other context about the problem here. -->

'''
'''--- .github/ISSUE_TEMPLATE/---feature-request.md ---
---
name: "\U0001F389 Feature request"
about: Suggest an idea for this project
title: ''
labels: "\U0001F389 enhancement"
assignees: ''

---

Thanks for proposing a new feature!

### Motivation

A clear and concise description of what the motivation for the new feature is, and what problem it is solving.

### Proposed solution

A clear and concise description of the feature you would like to add, and how it solves the motivating problem.

### Alternatives

A clear and concise description of any alternative solutions or features you've considered, and why you're proposed solution is better.

### Additional context

Add any other context or screenshots about the feature request here.

'''
'''--- .github/ISSUE_TEMPLATE/--question.md ---
---
name: "‚ùì Question"
about: Ask a question about this project
title: ''
labels: "‚ùì question"
assignees: ''

---

### Summary

A clear and concise summary of your question.

### Additional details

Provide any additional details here.

'''
'''--- .github/ISSUE_TEMPLATE/BOUNTY.yml ---
name: "Simple Bounty"
description: "Use this template to create a HEROES Simple Bounty via Github bot"
title: "Bounty: "
labels: ["bounty"]
assignees: heroes-bot-test
body:
  - type: markdown
    attributes:
      value: |
        Hi! Let's set up your bounty! Please don't change the template - @heroes-bot-test won't be able to help you.

  - type: dropdown
    id: type
    attributes:
      label: What talent are you looking for?
      options:
        - Marketing
        - Development
        - Design
        - Other
        - Content
        - Research
        - Audit

  - type: textarea
    id: description
    attributes:
      label: What you need to be done?

  - type: dropdown
    id: tags
    attributes:
      label: Tags
      description: Add tags that match the topic of the work
      multiple: true
      options:
        - API
        - Blockchain
        - Community
        - CSS
        - DAO
        - dApp
        - DeFi
        - Design
        - Documentation
        - HTML
        - Javascript
        - NFT
        - React
        - Rust
        - Smart contract
        - Typescript
        - UI/UX
        - web3
        - Translation
        - Illustration
        - Branding
        - Copywriting
        - Blogging
        - Editing
        - Video Creation
        - Social Media
        - Graphic Design
        - Transcription
        - Product Design
        - Artificial Intelligence
        - Quality Assurance
        - Risk Assessment
        - Security Audit
        - Bug Bounty
        - Code Review
        - Blockchain Security
        - Smart Contract Testing
        - Penetration Testing
        - Vulnerability Assessment
        - BOS
        - News
        - Hackathon
        - NEARCON2023
        - NEARWEEK

  - type: input
    id: deadline
    attributes:
      label: Deadline
      description: "Set a deadline for your bounty. Please enter the date in format: DD.MM.YYYY"
      placeholder: "19.05.2027"

  - type: dropdown
    id: currencyType
    attributes:
      label: Currency
      description: What is the currency you want to pay?
      options:
        - USDC.e
        - USDT.e
        - DAI
        - wNEAR
        - USDt
        - XP
        - marmaj
        - NEKO
        - JUMP
        - USDC
        - NEARVIDIA
      default: 0
    validations:
      required: true

  - type: input
    id: currencyAmount
    attributes:
      label: Amount
      description: How much it will be cost?

  - type: markdown
    attributes:
      value: "## Advanced settings"

  - type: checkboxes
    id: kyc
    attributes:
      label: KYC
      description: "Use HEROES' KYC Verification, only applicants who passed HEROES' KYC can apply and work on this bounty!"
      options:
        - label: Use KYC Verification

  - type: markdown
    attributes:
      value: |
        ### This cannot be changed once the bounty is live!

'''
'''--- .github/codecov.yml ---
coverage:
  status:
    project:
      default:
        target: auto
        threshold: 1%
    patch:
      default:
        target: auto
        threshold: 5%
        base: auto

'''
'''--- .github/pull_request_template.md ---
<!-- 
Prior to submitting a PR, review the CONTRIBUTING.md document for recommendations on how to test:
https://github.com/wasmerio/wasmer/blob/master/CONTRIBUTING.md#pull-requests

-->

# Description
<!-- 
Provide details regarding the change including motivation,
links to related issues, and the context of the PR.
-->

# Review

- [ ] Add a short description of the change to the CHANGELOG.md file

'''
'''--- .github/stale.yml ---
# Number of days of inactivity before an issue becomes stale
daysUntilStale: 365
# Number of days of inactivity before a stale issue is closed
daysUntilClose: 30
# Issues with these labels will never be considered stale
exemptLabels:
  - "üêû bug"
# Label to use when marking an issue as stale
staleLabel: "üèö stale"
# Comment to post when marking an issue as stale. Set to `false` to disable
markComment: >
  This issue has been automatically marked as stale because it has not had
  recent activity. It will be closed if no further activity occurs. Thank you
  for your contributions.
# Comment to post when closing a stale issue. Set to `false` to disable
closeComment: >
  Feel free to reopen the issue if it has been closed by mistake.

'''
'''--- .tarpaulin.toml ---
[cranelift_coverage]
features = "cranelift,singlepass,llvm,coverage,test-cranelift,test-jit"
examples = ["early-exit", "engine-jit", "engine-native", "engine-headless", "cross-compilation", "compiler-cranelift", "exported-function", "wasi"]
release = true

[llvm_coverage]
features = "cranelift,singlepass,llvm,coverage,test-llvm,test-jit"
examples = ["compiler-llvm"]
release = true

[singlepass_coverage]
features = "cranelift,singlepass,llvm,coverage,test-singlepass,test-jit"
examples = ["compiler-singlepass"]
release = true

[report]
out = ["Xml"]

'''
'''--- ATTRIBUTIONS.md ---
# Wasmer Attributions

Wasmer is a community effort and makes use of code from various other
projects ‚ù§Ô∏è.
Listed below are notable sections of code that are licensed
from other projects and the relevant license of those projects.

These are the projects that were used as inspiration and/or that we are using code from.
Each of the subcrates we have have an `Aknowledgements` section with more details.

Projects:

- [Emscripten](https://github.com/kripken/emscripten): for emtests test sources to ensure compatibility - [LICENSE](#emscripten)
- [Nebulet](https://github.com/nebulet/nebulet): as the base for creating a great Rust WebAssembly runtime - [LICENSE](#nebulet)
- [WAVM](https://github.com/wavm/wavm): for their great integration and testing framework - [LICENSE](#wavm)
- [wasmtime](https://github.com/CraneStation/wasmtime): for their API and internal documentation, as well as some internal implementations - [LICENSE](#wasmtime)
- [WebAssembly spec](https://github.com/WebAssembly/spec/tree/master/test): for the spectests implementation

üôè Please let us know if you believe there is an error or omission in
this list and we will correct.

## Licenses

### Nebulet

```text
MIT License

Copyright (c) 2018

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

### WAVM

```text
Copyright (c) 2018, Andrew Scheidecker
All rights reserved.

Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:
* Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.
* Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.
* Neither the name of WAVM nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL <COPYRIGHT HOLDER> BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

The contents of [Test/spec](Test/spec) is covered by the license in [Test/spec/LICENSE](Test/spec/LICENSE).

[Source/ThirdParty/dtoa/dtoa.c](Source/ThirdParty/dtoa/dtoa.c) is covered by the license in that file.

[Source/ThirdParty/libunwind](Source/ThirdParty/libunwind) is covered by the license in [Source/ThirdParty/libunwind/LICENSE.TXT](Source/ThirdParty/libunwind/LICENSE.TXT).

[Source/ThirdParty/xxhash](Source/ThirdParty/xxhash) is covered by the license in [Source/ThirdParty/xxhash/LICENSE](Source/ThirdParty/xxhash/LICENSE).
```

### Greenwasm

```text
                              Apache License
                        Version 2.0, January 2004
                     http://www.apache.org/licenses/

TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

1. Definitions.

   "License" shall mean the terms and conditions for use, reproduction,
   and distribution as defined by Sections 1 through 9 of this document.

   "Licensor" shall mean the copyright owner or entity authorized by
   the copyright owner that is granting the License.

   "Legal Entity" shall mean the union of the acting entity and all
   other entities that control, are controlled by, or are under common
   control with that entity. For the purposes of this definition,
   "control" means (i) the power, direct or indirect, to cause the
   direction or management of such entity, whether by contract or
   otherwise, or (ii) ownership of fifty percent (50%) or more of the
   outstanding shares, or (iii) beneficial ownership of such entity.

   "You" (or "Your") shall mean an individual or Legal Entity
   exercising permissions granted by this License.

   "Source" form shall mean the preferred form for making modifications,
   including but not limited to software source code, documentation
   source, and configuration files.

   "Object" form shall mean any form resulting from mechanical
   transformation or translation of a Source form, including but
   not limited to compiled object code, generated documentation,
   and conversions to other media types.

   "Work" shall mean the work of authorship, whether in Source or
   Object form, made available under the License, as indicated by a
   copyright notice that is included in or attached to the work
   (an example is provided in the Appendix below).

   "Derivative Works" shall mean any work, whether in Source or Object
   form, that is based on (or derived from) the Work and for which the
   editorial revisions, annotations, elaborations, or other modifications
   represent, as a whole, an original work of authorship. For the purposes
   of this License, Derivative Works shall not include works that remain
   separable from, or merely link (or bind by name) to the interfaces of,
   the Work and Derivative Works thereof.

   "Contribution" shall mean any work of authorship, including
   the original version of the Work and any modifications or additions
   to that Work or Derivative Works thereof, that is intentionally
   submitted to Licensor for inclusion in the Work by the copyright owner
   or by an individual or Legal Entity authorized to submit on behalf of
   the copyright owner. For the purposes of this definition, "submitted"
   means any form of electronic, verbal, or written communication sent
   to the Licensor or its representatives, including but not limited to
   communication on electronic mailing lists, source code control systems,
   and issue tracking systems that are managed by, or on behalf of, the
   Licensor for the purpose of discussing and improving the Work, but
   excluding communication that is conspicuously marked or otherwise
   designated in writing by the copyright owner as "Not a Contribution."

   "Contributor" shall mean Licensor and any individual or Legal Entity
   on behalf of whom a Contribution has been received by Licensor and
   subsequently incorporated within the Work.

2. Grant of Copyright License. Subject to the terms and conditions of
   this License, each Contributor hereby grants to You a perpetual,
   worldwide, non-exclusive, no-charge, royalty-free, irrevocable
   copyright license to reproduce, prepare Derivative Works of,
   publicly display, publicly perform, sublicense, and distribute the
   Work and such Derivative Works in Source or Object form.

3. Grant of Patent License. Subject to the terms and conditions of
   this License, each Contributor hereby grants to You a perpetual,
   worldwide, non-exclusive, no-charge, royalty-free, irrevocable
   (except as stated in this section) patent license to make, have made,
   use, offer to sell, sell, import, and otherwise transfer the Work,
   where such license applies only to those patent claims licensable
   by such Contributor that are necessarily infringed by their
   Contribution(s) alone or by combination of their Contribution(s)
   with the Work to which such Contribution(s) was submitted. If You
   institute patent litigation against any entity (including a
   cross-claim or counterclaim in a lawsuit) alleging that the Work
   or a Contribution incorporated within the Work constitutes direct
   or contributory patent infringement, then any patent licenses
   granted to You under this License for that Work shall terminate
   as of the date such litigation is filed.

4. Redistribution. You may reproduce and distribute copies of the
   Work or Derivative Works thereof in any medium, with or without
   modifications, and in Source or Object form, provided that You
   meet the following conditions:

   (a) You must give any other recipients of the Work or
       Derivative Works a copy of this License; and

   (b) You must cause any modified files to carry prominent notices
       stating that You changed the files; and

   (c) You must retain, in the Source form of any Derivative Works
       that You distribute, all copyright, patent, trademark, and
       attribution notices from the Source form of the Work,
       excluding those notices that do not pertain to any part of
       the Derivative Works; and

   (d) If the Work includes a "NOTICE" text file as part of its
       distribution, then any Derivative Works that You distribute must
       include a readable copy of the attribution notices contained
       within such NOTICE file, excluding those notices that do not
       pertain to any part of the Derivative Works, in at least one
       of the following places: within a NOTICE text file distributed
       as part of the Derivative Works; within the Source form or
       documentation, if provided along with the Derivative Works; or,
       within a display generated by the Derivative Works, if and
       wherever such third-party notices normally appear. The contents
       of the NOTICE file are for informational purposes only and
       do not modify the License. You may add Your own attribution
       notices within Derivative Works that You distribute, alongside
       or as an addendum to the NOTICE text from the Work, provided
       that such additional attribution notices cannot be construed
       as modifying the License.

   You may add Your own copyright statement to Your modifications and
   may provide additional or different license terms and conditions
   for use, reproduction, or distribution of Your modifications, or
   for any such Derivative Works as a whole, provided Your use,
   reproduction, and distribution of the Work otherwise complies with
   the conditions stated in this License.

5. Submission of Contributions. Unless You explicitly state otherwise,
   any Contribution intentionally submitted for inclusion in the Work
   by You to the Licensor shall be under the terms and conditions of
   this License, without any additional terms or conditions.
   Notwithstanding the above, nothing herein shall supersede or modify
   the terms of any separate license agreement you may have executed
   with Licensor regarding such Contributions.

6. Trademarks. This License does not grant permission to use the trade
   names, trademarks, service marks, or product names of the Licensor,
   except as required for reasonable and customary use in describing the
   origin of the Work and reproducing the content of the NOTICE file.

7. Disclaimer of Warranty. Unless required by applicable law or
   agreed to in writing, Licensor provides the Work (and each
   Contributor provides its Contributions) on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
   implied, including, without limitation, any warranties or conditions
   of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
   PARTICULAR PURPOSE. You are solely responsible for determining the
   appropriateness of using or redistributing the Work and assume any
   risks associated with Your exercise of permissions under this License.

8. Limitation of Liability. In no event and under no legal theory,
   whether in tort (including negligence), contract, or otherwise,
   unless required by applicable law (such as deliberate and grossly
   negligent acts) or agreed to in writing, shall any Contributor be
   liable to You for damages, including any direct, indirect, special,
   incidental, or consequential damages of any character arising as a
   result of this License or out of the use or inability to use the
   Work (including but not limited to damages for loss of goodwill,
   work stoppage, computer failure or malfunction, or any and all
   other commercial damages or losses), even if such Contributor
   has been advised of the possibility of such damages.

9. Accepting Warranty or Additional Liability. While redistributing
   the Work or Derivative Works thereof, You may choose to offer,
   and charge a fee for, acceptance of support, warranty, indemnity,
   or other liability obligations and/or rights consistent with this
   License. However, in accepting such obligations, You may act only
   on Your own behalf and on Your sole responsibility, not on behalf
   of any other Contributor, and only if You agree to indemnify,
   defend, and hold each Contributor harmless for any liability
   incurred by, or claims asserted against, such Contributor by reason
   of your accepting any such warranty or additional liability.

END OF TERMS AND CONDITIONS

APPENDIX: How to apply the Apache License to your work.

   To apply the Apache License to your work, attach the following
   boilerplate notice, with the fields enclosed by brackets "[]"
   replaced with your own identifying information. (Don't include
   the brackets!)  The text should be enclosed in the appropriate
   comment syntax for the file format. We also recommend that a
   file or class name and description of purpose be included on the
   same "printed page" as the copyright notice for easier
   identification within third-party archives.

Copyright [yyyy] [name of copyright owner]

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

	http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
```

### Wasmtime

```text
                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright [yyyy] [name of copyright owner]

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.

--- LLVM Exceptions to the Apache 2.0 License ----

As an exception, if, as a result of your compiling your source code, portions
of this Software are embedded into an Object form of such source code, you
may redistribute such embedded portions in such Object form without complying
with the conditions of Sections 4(a), 4(b) and 4(d) of the License.

In addition, if you combine or link compiled forms of this Software with
software that is licensed under the GPLv2 ("Combined Software") and if a
court of competent jurisdiction determines that the patent provision (Section
3), the indemnity provision (Section 9) or other Section of the License
conflicts with the conditions of the GPLv2, you may retroactively and
prospectively choose to deem waived or otherwise exclude such Section(s) of
the License, but only in their entirety and only with respect to the Combined
Software.
```

### Emscripten
```text
Emscripten is available under 2 licenses, the MIT license and the
University of Illinois/NCSA Open Source License.

Both are permissive open source licenses, with little if any
practical difference between them.

The reason for offering both is that (1) the MIT license is
well-known, while (2) the University of Illinois/NCSA Open Source
License allows Emscripten's code to be integrated upstream into
LLVM, which uses that license, should the opportunity arise.

The full text of both licenses follows.

==============================================================================

Copyright (c) 2010-2014 Emscripten authors, see AUTHORS file.

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.

==============================================================================

Copyright (c) 2010-2014 Emscripten authors, see AUTHORS file.
All rights reserved.

Permission is hereby granted, free of charge, to any person obtaining a
copy of this software and associated documentation files (the
"Software"), to deal with the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:

    Redistributions of source code must retain the above copyright
    notice, this list of conditions and the following disclaimers.

    Redistributions in binary form must reproduce the above
    copyright notice, this list of conditions and the following disclaimers
    in the documentation and/or other materials provided with the
    distribution.

    Neither the names of Mozilla,
    nor the names of its contributors may be used to endorse
    or promote products derived from this Software without specific prior
    written permission.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
IN NO EVENT SHALL THE CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR
ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE SOFTWARE.

==============================================================================

This program uses portions of Node.js source code located in src/library_path.js,
in accordance with the terms of the MIT license. Node's license follows:

    """
        Copyright Joyent, Inc. and other Node contributors. All rights reserved.
        Permission is hereby granted, free of charge, to any person obtaining a copy
        of this software and associated documentation files (the "Software"), to
        deal in the Software without restriction, including without limitation the
        rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
        sell copies of the Software, and to permit persons to whom the Software is
        furnished to do so, subject to the following conditions:

        The above copyright notice and this permission notice shall be included in
        all copies or substantial portions of the Software.

        THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
        IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
        FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
        AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
        LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
        FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
        IN THE SOFTWARE.
    """

The musl libc project is bundled in this repo, and it has the MIT license, see
system/lib/libc/musl/COPYRIGHT

The third_party/ subdirectory contains code with other licenses. None of it is
used by default, but certain options use it (e.g., the optional closure compiler
flag will run closure compiler from third_party/).

```

'''
'''--- CHANGELOG.md ---
# Changelog

*The format is based on [Keep a Changelog].*

[Keep a Changelog]: http://keepachangelog.com/en/1.0.0/

Looking for changes that affect our C API? See the [C API Changelog](lib/c-api/CHANGELOG.md).

## **[Unreleased]**

## 2.1.0 - 2021/11/30

### Added
- [#2574](https://github.com/wasmerio/wasmer/pull/2574) Added Windows support to Singlepass.
- [#2535](https://github.com/wasmerio/wasmer/pull/2435) Added iOS support for Wasmer. This relies on the `dylib-engine`.
- [#2460](https://github.com/wasmerio/wasmer/pull/2460) Wasmer can now compile to Javascript via `wasm-bindgen`. Use the `js-default` (and no default features) feature to try it!.
- [#2491](https://github.com/wasmerio/wasmer/pull/2491) Added support for WASI to Wasmer-js.
- [#2436](https://github.com/wasmerio/wasmer/pull/2436) Added the x86-32 bit variant support to LLVM compiler.
- [#2499](https://github.com/wasmerio/wasmer/pull/2499) Added a subcommand to linux wasmer-cli to register wasmer with binfmt_misc
- [#2511](https://github.com/wasmerio/wasmer/pull/2511) Added support for calling dynamic functions defined on the host
- [#2491](https://github.com/wasmerio/wasmer/pull/2491) Added support for WASI in Wasmer-js
- [#2592](https://github.com/wasmerio/wasmer/pull/2592) Added `ImportObject::get_namespace_exports` to allow modifying the contents of an existing namespace in an `ImportObject`.
- [#2694](https://github.com/wasmerio/wasmer/pull/2694) wasmer-js: Allow an `ImportObject` to be extended with a JS object.
- [#2698](https://github.com/wasmerio/wasmer/pull/2698) Provide WASI imports when invoking an explicit export from the CLI.
- [#2701](https://github.com/wasmerio/wasmer/pull/2701) Improved VFS API for usage from JS

### Changed
- [#2460](https://github.com/wasmerio/wasmer/pull/2460) **breaking change** `wasmer` API usage with `no-default-features` requires now the `sys` feature to preserve old behavior.
- [#2476](https://github.com/wasmerio/wasmer/pull/2476) Removed unncessary abstraction `ModuleInfoTranslate` from `wasmer-compiler`.
- [#2442](https://github.com/wasmerio/wasmer/pull/2442) Improved `WasmPtr`, added `WasmCell` for host/guest interaction.
- [#2427](https://github.com/wasmerio/wasmer/pull/2427) Update `loupe` to 0.1.3.
- [#2685](https://github.com/wasmerio/wasmer/pull/2685) The minimum LLVM version for the LLVM compiler is now 12. LLVM 13 is used by default.
- [#2569](https://github.com/wasmerio/wasmer/pull/2569) Add `Send` and `Sync` to uses of the `LikeNamespace` trait object.
- [#2692](https://github.com/wasmerio/wasmer/pull/2692) Made module serialization deterministic.
- [#2693](https://github.com/wasmerio/wasmer/pull/2693) Validate CPU features when loading a deserialized module.

### Fixed
- [#2599](https://github.com/wasmerio/wasmer/pull/2599) Fixed Universal engine for Linux/Aarch64 target.
- [#2587](https://github.com/wasmerio/wasmer/pull/2587) Fixed deriving `WasmerEnv` when aliasing `Result`.
- [#2518](https://github.com/wasmerio/wasmer/pull/2518) Remove temporary file used to creating an artifact when creating a Dylib engine artifact.
- [#2494](https://github.com/wasmerio/wasmer/pull/2494) Fixed `WasmerEnv` access when using `call_indirect` with the Singlepass compiler.
- [#2479](https://github.com/wasmerio/wasmer/pull/2479) Improved `wasmer validate` error message on non-wasm inputs.
- [#2454](https://github.com/wasmerio/wasmer/issues/2454) Won't set `WASMER_CACHE_DIR` for Windows.
- [#2426](https://github.com/wasmerio/wasmer/pull/2426) Fix the `wax` script generation.
- [#2635](https://github.com/wasmerio/wasmer/pull/2635) Fix cross-compilation for singlepass.
- [#2672](https://github.com/wasmerio/wasmer/pull/2672) Use `ENOENT` instead of `EINVAL` in some WASI syscalls for a non-existent file
- [#2547](https://github.com/wasmerio/wasmer/pull/2547) Delete temporary files created by the dylib engine.
- [#2548](https://github.com/wasmerio/wasmer/pull/2548) Fix stack probing on x86_64 linux with the cranelift compiler.
- [#2557](https://github.com/wasmerio/wasmer/pull/2557) [#2559](https://github.com/wasmerio/wasmer/pull/2559) Fix WASI dir path renaming.
- [#2560](https://github.com/wasmerio/wasmer/pull/2560) Fix signal handling on M1 MacOS.
- [#2474](https://github.com/wasmerio/wasmer/pull/2474) Fix permissions on `WASMER_CACHE_DIR` on Windows.
- [#2528](https://github.com/wasmerio/wasmer/pull/2528) [#2525](https://github.com/wasmerio/wasmer/pull/2525) [#2523](https://github.com/wasmerio/wasmer/pull/2523) [#2522](https://github.com/wasmerio/wasmer/pull/2522) [#2545](https://github.com/wasmerio/wasmer/pull/2545) [#2550](https://github.com/wasmerio/wasmer/pull/2550) [#2551](https://github.com/wasmerio/wasmer/pull/2551)  Fix various bugs in the new VFS implementation.
- [#2552](https://github.com/wasmerio/wasmer/pull/2552) Fix stack guard handling on Windows.
- [#2585](https://github.com/wasmerio/wasmer/pull/2585) Fix build with 64-bit MinGW toolchain.
- [#2587](https://github.com/wasmerio/wasmer/pull/2587) Fix absolute import of `Result` in derive.
- [#2599](https://github.com/wasmerio/wasmer/pull/2599) Fix AArch64 support in the LLVM compiler.
- [#2655](https://github.com/wasmerio/wasmer/pull/2655) Fix argument parsing of `--dir` and `--mapdir`.
- [#2666](https://github.com/wasmerio/wasmer/pull/2666) Fix performance on Windows by using static memories by default.
- [#2667](https://github.com/wasmerio/wasmer/pull/2667) Fix error code for path_rename of a non-existant file
- [#2672](https://github.com/wasmerio/wasmer/pull/2672) Fix error code returned by some wasi fs syscalls for a non-existent file
- [#2673](https://github.com/wasmerio/wasmer/pull/2673) Fix BrTable codegen on the LLVM compiler
- [#2674](https://github.com/wasmerio/wasmer/pull/2674) Add missing `__WASI_RIGHT_FD_DATASYNC` for preopened directories
- [#2677](https://github.com/wasmerio/wasmer/pull/2677) Support 32-bit memories with 65536 pages
- [#2681](https://github.com/wasmerio/wasmer/pull/2681) Fix slow compilation in singlepass by using dynasm's `VecAssembler`.
- [#2690](https://github.com/wasmerio/wasmer/pull/2690) Fix memory leak when obtaining the stack bounds of a thread
- [#2699](https://github.com/wasmerio/wasmer/pull/2699) Partially fix unbounded memory leak from the FuncDataRegistry

## 2.0.0 - 2021/06/16

### Added
- [#2411](https://github.com/wasmerio/wasmer/pull/2411) Extract types from `wasi` to a new `wasi-types` crate.
- [#2390](https://github.com/wasmerio/wasmer/pull/2390) Make `wasmer-vm` to compile on Windows 32bits.
- [#2402](https://github.com/wasmerio/wasmer/pull/2402) Add more examples and more doctests for `wasmer-middlewares`.

### Changed
- [#2399](https://github.com/wasmerio/wasmer/pull/2399) Add the Dart integration in the `README.md`.

### Fixed
- [#2386](https://github.com/wasmerio/wasmer/pull/2386) Handle properly when a module has no exported functions in the CLI.

## 2.0.0-rc2 - 2021/06/03

### Fixed
- [#2383](https://github.com/wasmerio/wasmer/pull/2383) Fix bugs in the Wasmer CLI tool with the way `--version` and the name of the CLI tool itself were printed.

## 2.0.0-rc1 - 2021/06/02

### Added
- [#2348](https://github.com/wasmerio/wasmer/pull/2348) Make Wasmer available on `aarch64-linux-android`.
- [#2315](https://github.com/wasmerio/wasmer/pull/2315) Make the Cranelift compiler working with the Native engine.
- [#2306](https://github.com/wasmerio/wasmer/pull/2306) Add support for the latest version of the Wasm SIMD proposal to compiler LLVM.
- [#2296](https://github.com/wasmerio/wasmer/pull/2296) Add support for the bulk memory proposal in compiler Singlepass and compiler LLVM.
- [#2291](https://github.com/wasmerio/wasmer/pull/2291) Type check tables when importing.
- [#2262](https://github.com/wasmerio/wasmer/pull/2262) Make parallelism optional for the Singlepass compiler.
- [#2249](https://github.com/wasmerio/wasmer/pull/2249) Make Cranelift unwind feature optional.
- [#2208](https://github.com/wasmerio/wasmer/pull/2208) Add a new CHANGELOG.md specific to our C API to make it easier for users primarily consuming our C API to keep up to date with changes that affect them.
- [#2154](https://github.com/wasmerio/wasmer/pull/2154) Implement Reference Types in the LLVM compiler.
- [#2003](https://github.com/wasmerio/wasmer/pull/2003) Wasmer works with musl, and is built, tested and packaged for musl.
- [#2250](https://github.com/wasmerio/wasmer/pull/2250) Use `rkyv` for the JIT/Universal engine.
- [#2190](https://github.com/wasmerio/wasmer/pull/2190) Use `rkyv` to read native `Module` artifact.
- [#2186](https://github.com/wasmerio/wasmer/pull/2186) Update and improve the Fuzz Testing infrastructure.
- [#2161](https://github.com/wasmerio/wasmer/pull/2161) Make NaN canonicalization configurable.
- [#2116](https://github.com/wasmerio/wasmer/pull/2116) Add a package for Windows that is not an installer, but all the `lib` and `include` files as for macOS and Linux.
- [#2123](https://github.com/wasmerio/wasmer/pull/2123) Use `ENABLE_{{compiler_name}}=(0|1)` to resp. force to disable or enable a compiler when running the `Makefile`, e.g. `ENABLE_LLVM=1 make build-wasmer`.
- [#2123](https://github.com/wasmerio/wasmer/pull/2123) `libwasmer` comes with all available compilers per target instead of Cranelift only.
- [#2135](https://github.com/wasmerio/wasmer/pull/2135) [Documentation](./PACKAGING.md) for Linux distribution maintainers
- [#2104](https://github.com/wasmerio/wasmer/pull/2104) Update WAsm core spectests and wasmparser.

### Changed
- [#2369](https://github.com/wasmerio/wasmer/pull/2369) Remove the deprecated `--backend` option in the CLI.
- [#2368](https://github.com/wasmerio/wasmer/pull/2368) Remove the deprecated code in the `wasmer-wasi` crate.
- [#2367](https://github.com/wasmerio/wasmer/pull/2367) Remove the `deprecated` features and associated code in the `wasmer` crate.
- [#2366](https://github.com/wasmerio/wasmer/pull/2366) Remove the deprecated crates.
- [#2364](https://github.com/wasmerio/wasmer/pull/2364) Rename `wasmer-engine-object-file` to `wasmer-engine-staticlib`.
- [#2356](https://github.com/wasmerio/wasmer/pull/2356) Rename `wasmer-engine-native` to `wasmer-engine-dylib`.
- [#2340](https://github.com/wasmerio/wasmer/pull/2340) Rename `wasmer-engine-jit` to `wasmer-engine-universal`.
- [#2307](https://github.com/wasmerio/wasmer/pull/2307) Update Cranelift, implement low hanging fruit SIMD opcodes.
- [#2305](https://github.com/wasmerio/wasmer/pull/2305) Clean up and improve the trap API, more deterministic errors etc.
- [#2299](https://github.com/wasmerio/wasmer/pull/2299) Unused trap codes (due to Wasm spec changes), `HeapSetterOutOfBounds` and `TableSetterOutOfBounds` were removed from `wasmer_vm::TrapCode` and the numbering of the remaining variants has been adjusted.
- [#2293](https://github.com/wasmerio/wasmer/pull/2293) The `Memory::ty` trait method now returns `MemoryType` by value. `wasmer_vm::LinearMemory` now recomputes `MemoryType`'s `minimum` field when accessing its type. This behavior is what's expected by the latest spectests. `wasmer::Memory::ty` has also been updated to follow suit, it now returns `MemoryType` by value.
- [#2286](https://github.com/wasmerio/wasmer/pull/2286) Replace the `goblin` crate by the `object` crate.
- [#2281](https://github.com/wasmerio/wasmer/pull/2281) Refactor the `wasmer_vm` crate to remove unnecessary structs, reuse data when available etc.
- [#2251](https://github.com/wasmerio/wasmer/pull/2251) Wasmer CLI will now execute WASI modules with multiple WASI namespaces in them by default. Use `--allow-multiple-wasi-versions` to suppress the warning and use `--deny-multiple-wasi-versions` to make it an error.
- [#2201](https://github.com/wasmerio/wasmer/pull/2201) Implement `loupe::MemoryUsage` for `wasmer::Instance`.
- [#2200](https://github.com/wasmerio/wasmer/pull/2200) Implement `loupe::MemoryUsage` for `wasmer::Module`.
- [#2199](https://github.com/wasmerio/wasmer/pull/2199) Implement `loupe::MemoryUsage` for `wasmer::Store`.
- [#2195](https://github.com/wasmerio/wasmer/pull/2195) Remove dependency to `cranelift-entity`.
- [#2140](https://github.com/wasmerio/wasmer/pull/2140) Reduce the number of dependencies in the `wasmer.dll` shared library by statically compiling CRT.
- [#2113](https://github.com/wasmerio/wasmer/pull/2113) Bump minimum supported Rust version to 1.49
- [#2144](https://github.com/wasmerio/wasmer/pull/2144) Bump cranelift version to 0.70
- [#2149](https://github.com/wasmerio/wasmer/pull/2144) `wasmer-engine-native` looks for clang-11 instead of clang-10.
- [#2157](https://github.com/wasmerio/wasmer/pull/2157) Simplify the code behind `WasmPtr`

### Fixed
- [#2397](https://github.com/wasmerio/wasmer/pull/2397) Fix WASI rename temporary file issue.
- [#2391](https://github.com/wasmerio/wasmer/pull/2391) Fix Singlepass emit bug, [#2347](https://github.com/wasmerio/wasmer/issues/2347) and [#2159](https://github.com/wasmerio/wasmer/issues/2159)
- [#2327](https://github.com/wasmerio/wasmer/pull/2327) Fix memory leak preventing internal instance memory from being freed when a WasmerEnv contained an exported extern (e.g. Memory, etc.).
- [#2247](https://github.com/wasmerio/wasmer/pull/2247) Internal WasiFS logic updated to be closer to what WASI libc does when finding a preopened fd for a path.
- [#2241](https://github.com/wasmerio/wasmer/pull/2241) Fix Undefined Behavior in setting memory in emscripten `EmEnv`.
- [#2224](https://github.com/wasmerio/wasmer/pull/2224) Enable SIMD based on actual Wasm features in the Cranelift compiler.
- [#2217](https://github.com/wasmerio/wasmer/pull/2217) Fix bug in `i64.rotr X 0` in the LLVM compiler.
- [#2290](https://github.com/wasmerio/wasmer/pull/2290) Handle Wasm modules with no imports in the CLI.
- [#2108](https://github.com/wasmerio/wasmer/pull/2108) The Object Native Engine generates code that now compiles correctly with C++.
- [#2125](https://github.com/wasmerio/wasmer/pull/2125) Fix RUSTSEC-2021-0023.
- [#2155](https://github.com/wasmerio/wasmer/pull/2155) Fix the implementation of shift and rotate in the LLVM compiler.
- [#2101](https://github.com/wasmerio/wasmer/pull/2101) cflags emitted by `wasmer config --pkg-config` are now correct.

## 1.0.2 - 2021-02-04

### Added
- [#2053](https://github.com/wasmerio/wasmer/pull/2053) Implement the non-standard `wasi_get_unordered_imports` function in the C API.
- [#2072](https://github.com/wasmerio/wasmer/pull/2072) Add `wasm_config_set_target`, along with `wasm_target_t`, `wasm_triple_t` and `wasm_cpu_features_t` in the unstable C API.
- [#2059](https://github.com/wasmerio/wasmer/pull/2059) Ability to capture `stdout` and `stderr` with WASI in the C API.
- [#2040](https://github.com/wasmerio/wasmer/pull/2040) Add `InstanceHandle::vmoffsets` to expose the offsets of the `vmctx` region.
- [#2026](https://github.com/wasmerio/wasmer/pull/2026) Expose trap code of a `RuntimeError`, if it's a `Trap`.
- [#2054](https://github.com/wasmerio/wasmer/pull/2054) Add `wasm_config_delete` to the Wasm C API.
- [#2072](https://github.com/wasmerio/wasmer/pull/2072) Added cross-compilation to Wasm C API.

### Changed
- [#2085](https://github.com/wasmerio/wasmer/pull/2085) Update to latest inkwell and LLVM 11.
- [#2037](https://github.com/wasmerio/wasmer/pull/2037) Improved parallelism of LLVM with the Native/Object engine
- [#2012](https://github.com/wasmerio/wasmer/pull/2012) Refactor Singlepass init stack assembly (more performant now)
- [#2036](https://github.com/wasmerio/wasmer/pull/2036) Optimize memory allocated for Function type definitions
- [#2083](https://github.com/wasmerio/wasmer/pull/2083) Mark `wasi_env_set_instance` and `wasi_env_set_memory` as deprecated. You may simply remove the calls with no side-effect.
- [#2056](https://github.com/wasmerio/wasmer/pull/2056) Change back to depend on the `enumset` crate instead of `wasmer_enumset`

### Fixed
- [#2066](https://github.com/wasmerio/wasmer/pull/2066) Include 'extern "C"' in our C headers when included by C++ code.
- [#2090](https://github.com/wasmerio/wasmer/pull/2090) `wasi_env_t` needs to be freed with `wasi_env_delete` in the C API.
- [#2084](https://github.com/wasmerio/wasmer/pull/2084) Avoid calling the function environment finalizer more than once when the environment has been cloned in the C API.
- [#2069](https://github.com/wasmerio/wasmer/pull/2069) Use the new documentation for `include/README.md` in the Wasmer package.
- [#2042](https://github.com/wasmerio/wasmer/pull/2042) Parse more exotic environment variables in `wasmer run`.
- [#2041](https://github.com/wasmerio/wasmer/pull/2041) Documentation diagrams now have a solid white background rather than a transparent background.
- [#2070](https://github.com/wasmerio/wasmer/pull/2070) Do not drain the entire captured stream at first read with `wasi_env_read_stdout` or `_stderr` in the C API.
- [#2058](https://github.com/wasmerio/wasmer/pull/2058) Expose WASI versions to C correctly.
- [#2044](https://github.com/wasmerio/wasmer/pull/2044) Do not build C headers on docs.rs.

## 1.0.1 - 2021-01-12

This release includes a breaking change in the API (changing the trait `enumset::EnumsetType` to `wasmer_enumset::EnumSetType` and changing `enumset::EnumSet` in signatures to `wasmer_enumset::EnumSet` to work around a breaking change introduced by `syn`) but is being released as a minor version because `1.0.0` is also in a broken state due to a breaking change introduced by `syn` which affects `enumset` and thus `wasmer`.

This change is unlikely to affect any users of `wasmer`, but if it does please change uses of the `enumset` crate to the `wasmer_enumset` crate where possible.

### Added
- [#2010](https://github.com/wasmerio/wasmer/pull/2010) A new, experimental, minified build of `wasmer` called `wasmer-headless` will now be included with releases. `wasmer-headless` is the `wasmer` VM without any compilers attached, so it can only run precompiled Wasm modules.
- [#2005](https://github.com/wasmerio/wasmer/pull/2005) Added the arguments `alias` and `optional` to `WasmerEnv` derive's `export` attribute.

### Changed
- [#2006](https://github.com/wasmerio/wasmer/pull/2006) Use `wasmer_enumset`, a fork of the `enumset` crate to work around a breaking change in `syn`
- [#1985](https://github.com/wasmerio/wasmer/pull/1985) Bump minimum supported Rust version to 1.48

### Fixed
- [#2007](https://github.com/wasmerio/wasmer/pull/2007) Fix packaging of wapm on Windows
- [#2005](https://github.com/wasmerio/wasmer/pull/2005) Emscripten is now working again.

## 1.0.0 - 2021-01-05

### Added

- [#1969](https://github.com/wasmerio/wasmer/pull/1969) Added D integration to the README

### Changed
- [#1979](https://github.com/wasmerio/wasmer/pull/1979) `WasmPtr::get_utf8_string` was renamed to `WasmPtr::get_utf8_str` and made `unsafe`.

### Fixed
- [#1979](https://github.com/wasmerio/wasmer/pull/1979) `WasmPtr::get_utf8_string` now returns a `String`, fixing a soundness issue in certain circumstances. The old functionality is available under a new `unsafe` function, `WasmPtr::get_utf8_str`.

## 1.0.0-rc1 - 2020-12-23

### Added

* [#1894](https://github.com/wasmerio/wasmer/pull/1894) Added exports `wasmer::{CraneliftOptLevel, LLVMOptLevel}` to allow using `Cranelift::opt_level` and `LLVM::opt_level` directly via the `wasmer` crate

### Changed

* [#1941](https://github.com/wasmerio/wasmer/pull/1941) Turn `get_remaining_points`/`set_remaining_points` of the `Metering` middleware into free functions to allow using them in an ahead-of-time compilation setup
* [#1955](https://github.com/wasmerio/wasmer/pull/1955) Set `jit` as a default feature of the `wasmer-wasm-c-api` crate
* [#1944](https://github.com/wasmerio/wasmer/pull/1944) Require `WasmerEnv` to be `Send + Sync` even in dynamic functions.
* [#1963](https://github.com/wasmerio/wasmer/pull/1963) Removed `to_wasm_error` in favour of `impl From<BinaryReaderError> for WasmError`
* [#1962](https://github.com/wasmerio/wasmer/pull/1962) Replace `wasmparser::Result<()>` with `Result<(), MiddlewareError>` in middleware, allowing implementors to return errors in `FunctionMiddleware::feed`

### Fixed

- [#1949](https://github.com/wasmerio/wasmer/pull/1949) `wasm_<type>_vec_delete` functions no longer crash when the given vector is uninitialized, in the Wasmer C API
- [#1949](https://github.com/wasmerio/wasmer/pull/1949) The `wasm_frame_vec_t`, `wasm_functype_vec_t`, `wasm_globaltype_vec_t`, `wasm_memorytype_vec_t`, and `wasm_tabletype_vec_t` are now boxed vectors in the Wasmer C API

## 1.0.0-beta2 - 2020-12-16

### Added

* [#1916](https://github.com/wasmerio/wasmer/pull/1916) Add the `WASMER_VERSION*` constants with the `wasmer_version*` functions in the Wasmer C API
* [#1867](https://github.com/wasmerio/wasmer/pull/1867) Added `Metering::get_remaining_points` and `Metering::set_remaining_points`
* [#1881](https://github.com/wasmerio/wasmer/pull/1881) Added `UnsupportedTarget` error to `CompileError`
* [#1908](https://github.com/wasmerio/wasmer/pull/1908) Implemented `TryFrom<Value<T>>` for `i32`/`u32`/`i64`/`u64`/`f32`/`f64`
* [#1927](https://github.com/wasmerio/wasmer/pull/1927) Added mmap support in `Engine::deserialize_from_file` to speed up artifact loading
* [#1911](https://github.com/wasmerio/wasmer/pull/1911) Generalized signature type in `Function::new` and `Function::new_with_env` to accept owned and reference `FunctionType` as well as array pairs. This allows users to define signatures as constants. Implemented `From<([Type; $N], [Type; $M])>` for `FunctionType` to support this.

### Changed

- [#1865](https://github.com/wasmerio/wasmer/pull/1865) Require that implementors of `WasmerEnv` also implement `Send`, `Sync`, and `Clone`.
- [#1851](https://github.com/wasmerio/wasmer/pull/1851) Improve test suite and documentation of the Wasmer C API
- [#1874](https://github.com/wasmerio/wasmer/pull/1874) Set `CompilerConfig` to be owned (following wasm-c-api)
- [#1880](https://github.com/wasmerio/wasmer/pull/1880) Remove cmake dependency for tests
- [#1924](https://github.com/wasmerio/wasmer/pull/1924) Rename reference implementation `wasmer::Tunables` to `wasmer::BaseTunables`. Export trait `wasmer_engine::Tunables` as `wasmer::Tunables`.

### Fixed

- [#1865](https://github.com/wasmerio/wasmer/pull/1865) Fix memory leaks with host function environments.
- [#1870](https://github.com/wasmerio/wasmer/pull/1870) Fixed Trap instruction address maps in Singlepass
* [#1914](https://github.com/wasmerio/wasmer/pull/1914) Implemented `TryFrom<Bytes> for Pages` instead of `From<Bytes> for Pages` to properly handle overflow errors

## 1.0.0-beta1 - 2020-12-01

### Added

- [#1839](https://github.com/wasmerio/wasmer/pull/1839) Added support for Metering Middleware
- [#1837](https://github.com/wasmerio/wasmer/pull/1837) It is now possible to use exports of an `Instance` even after the `Instance` has been freed
- [#1831](https://github.com/wasmerio/wasmer/pull/1831) Added support for Apple Silicon chips (`arm64-apple-darwin`)
- [#1739](https://github.com/wasmerio/wasmer/pull/1739) Improved function environment setup via `WasmerEnv` proc macro.
- [#1649](https://github.com/wasmerio/wasmer/pull/1649) Add outline of migration to 1.0.0 docs.

### Changed

- [#1739](https://github.com/wasmerio/wasmer/pull/1739) Environments passed to host function- must now implement the `WasmerEnv` trait. You can implement it on your existing type with `#[derive(WasmerEnv)]`.
- [#1838](https://github.com/wasmerio/wasmer/pull/1838) Deprecate `WasiEnv::state_mut`: prefer `WasiEnv::state` instead.
- [#1663](https://github.com/wasmerio/wasmer/pull/1663) Function environments passed to host functions now must be passed by `&` instead of `&mut`. This is a breaking change. This change fixes a race condition when a host function is called from multiple threads. If you need mutability in your environment, consider using `std::sync::Mutex` or other synchronization primitives.
- [#1830](https://github.com/wasmerio/wasmer/pull/1830) Minimum supported Rust version bumped to 1.47.0
- [#1810](https://github.com/wasmerio/wasmer/pull/1810) Make the `state` field of `WasiEnv` public

### Fixed

- [#1857](https://github.com/wasmerio/wasmer/pull/1857) Fix dynamic function with new Environment API
- [#1855](https://github.com/wasmerio/wasmer/pull/1855) Fix memory leak when using `wat2wasm` in the C API, the function now takes its output parameter by pointer rather than returning an allocated `wasm_byte_vec_t`.
- [#1841](https://github.com/wasmerio/wasmer/pull/1841) We will now panic when attempting to use a native function with a captured env as a host function. Previously this would silently do the wrong thing. See [#1840](https://github.com/wasmerio/wasmer/pull/1840) for info about Wasmer's support of closures as host functions.
- [#1764](https://github.com/wasmerio/wasmer/pull/1764) Fix bug in WASI `path_rename` allowing renamed files to be 1 directory below a preopened directory.

## 1.0.0-alpha5 - 2020-11-06

### Added

- [#1761](https://github.com/wasmerio/wasmer/pull/1761) Implement the `wasm_trap_t**` argument of `wasm_instance_new` in the Wasm C API.
- [#1687](https://github.com/wasmerio/wasmer/pull/1687) Add basic table example; fix ownership of local memory and local table metadata in the VM.
- [#1751](https://github.com/wasmerio/wasmer/pull/1751) Implement `wasm_trap_t` inside a function declared with `wasm_func_new_with_env` in the Wasm C API.
- [#1741](https://github.com/wasmerio/wasmer/pull/1741) Implement `wasm_memory_type` in the Wasm C API.
- [#1736](https://github.com/wasmerio/wasmer/pull/1736) Implement `wasm_global_type` in the Wasm C API.
- [#1699](https://github.com/wasmerio/wasmer/pull/1699) Update `wasm.h` to its latest version.
- [#1685](https://github.com/wasmerio/wasmer/pull/1685) Implement `wasm_exporttype_delete` in the Wasm C API.
- [#1725](https://github.com/wasmerio/wasmer/pull/1725) Implement `wasm_func_type` in the Wasm C API.
- [#1715](https://github.com/wasmerio/wasmer/pull/1715) Register errors from `wasm_module_serialize` in the Wasm C API.
- [#1709](https://github.com/wasmerio/wasmer/pull/1709) Implement `wasm_module_name` and `wasm_module_set_name` in the Wasm(er) C API.
- [#1700](https://github.com/wasmerio/wasmer/pull/1700) Implement `wasm_externtype_copy` in the Wasm C API.
- [#1785](https://github.com/wasmerio/wasmer/pull/1785) Add more examples on the Rust API.
- [#1783](https://github.com/wasmerio/wasmer/pull/1783) Handle initialized but empty results in `wasm_func_call` in the Wasm C API.
- [#1780](https://github.com/wasmerio/wasmer/pull/1780) Implement new SIMD zero-extend loads in compiler-llvm.
- [#1754](https://github.com/wasmerio/wasmer/pull/1754) Implement aarch64 ABI for compiler-llvm.
- [#1693](https://github.com/wasmerio/wasmer/pull/1693) Add `wasmer create-exe` subcommand.

### Changed

- [#1772](https://github.com/wasmerio/wasmer/pull/1772) Remove lifetime parameter from `NativeFunc`.
- [#1762](https://github.com/wasmerio/wasmer/pull/1762) Allow the `=` sign in a WASI environment variable value.
- [#1710](https://github.com/wasmerio/wasmer/pull/1710) Memory for function call trampolines is now owned by the Artifact.
- [#1781](https://github.com/wasmerio/wasmer/pull/1781) Cranelift upgrade to 0.67.
- [#1777](https://github.com/wasmerio/wasmer/pull/1777) Wasmparser update to 0.65.
- [#1775](https://github.com/wasmerio/wasmer/pull/1775) Improve LimitingTunables implementation.
- [#1720](https://github.com/wasmerio/wasmer/pull/1720) Autodetect llvm regardless of architecture.

### Fixed

- [#1718](https://github.com/wasmerio/wasmer/pull/1718) Fix panic in the API in some situations when the memory's min bound was greater than the memory's max bound.
- [#1731](https://github.com/wasmerio/wasmer/pull/1731) In compiler-llvm always load before store, to trigger any traps before any bytes are written.

## 1.0.0-alpha4 - 2020-10-08

### Added
- [#1635](https://github.com/wasmerio/wasmer/pull/1635) Implement `wat2wasm` in the Wasm C API.
- [#1636](https://github.com/wasmerio/wasmer/pull/1636) Implement `wasm_module_validate` in the Wasm C API.
- [#1657](https://github.com/wasmerio/wasmer/pull/1657) Implement `wasm_trap_t` and `wasm_frame_t` for Wasm C API; add examples in Rust and C of exiting early with a host function.

### Fixed
- [#1690](https://github.com/wasmerio/wasmer/pull/1690) Fix `wasm_memorytype_limits` where `min` and `max` represents pages, not bytes. Additionally, fixes the max limit sentinel value.
- [#1671](https://github.com/wasmerio/wasmer/pull/1671) Fix probestack firing inappropriately, and sometimes over/under allocating stack.
- [#1660](https://github.com/wasmerio/wasmer/pull/1660) Fix issue preventing map-dir aliases starting with `/` from working properly.
- [#1624](https://github.com/wasmerio/wasmer/pull/1624) Add Value::I32/Value::I64 converters from unsigned ints.

### Changed
- [#1682](https://github.com/wasmerio/wasmer/pull/1682) Improve error reporting when making a memory with invalid settings.
- [#1691](https://github.com/wasmerio/wasmer/pull/1691) Bump minimum supported Rust version to 1.46.0
- [#1645](https://github.com/wasmerio/wasmer/pull/1645) Move the install script to https://github.com/wasmerio/wasmer-install

## 1.0.0-alpha3 - 2020-09-14

### Fixed

- [#1620](https://github.com/wasmerio/wasmer/pull/1620) Fix bug causing the Wapm binary to not be packaged with the release
- [#1619](https://github.com/wasmerio/wasmer/pull/1619) Improve error message in engine-native when C compiler is missing

## 1.0.0-alpha02.0 - 2020-09-11

### Added

- [#1566](https://github.com/wasmerio/wasmer/pull/1566) Add support for opening special Unix files to the WASI FS

### Fixed

- [#1602](https://github.com/wasmerio/wasmer/pull/1602) Fix panic when calling host functions with negative numbers in certain situations
- [#1590](https://github.com/wasmerio/wasmer/pull/1590) Fix soundness issue in API of vm::Global

## TODO: 1.0.0-alpha01.0

- Wasmer refactor lands

## 0.17.1 - 2020-06-24

### Changed
- [#1439](https://github.com/wasmerio/wasmer/pull/1439) Move `wasmer-interface-types` into its own repository

### Fixed

- [#1554](https://github.com/wasmerio/wasmer/pull/1554) Update supported stable Rust version to 1.45.2.
- [#1552](https://github.com/wasmerio/wasmer/pull/1552) Disable `sigint` handler by default.

## 0.17.0 - 2020-05-11

### Added
- [#1331](https://github.com/wasmerio/wasmer/pull/1331) Implement the `record` type and instrutions for WIT
- [#1345](https://github.com/wasmerio/wasmer/pull/1345) Adding ARM testing in Azure Pipelines
- [#1329](https://github.com/wasmerio/wasmer/pull/1329) New numbers and strings instructions for WIT
- [#1285](https://github.com/wasmerio/wasmer/pull/1285) Greatly improve errors in `wasmer-interface-types`
- [#1303](https://github.com/wasmerio/wasmer/pull/1303) NaN canonicalization for singlepass backend.
- [#1313](https://github.com/wasmerio/wasmer/pull/1313) Add new high-level public API through `wasmer` crate. Includes many updates including:
  - Minor improvement: `imports!` macro now handles no trailing comma as well as a trailing comma in namespaces and between namespaces.
  - New methods on `Module`: `exports`, `imports`, and `custom_sections`.
  - New way to get exports from an instance with `let func_name: Func<i32, i64> = instance.exports.get("func_name");`.
  - Improved `Table` APIs including `set` which now allows setting functions directly.  TODO: update this more if `Table::get` gets made public in this PR
  - TODO: finish the list of changes here
- [#1305](https://github.com/wasmerio/wasmer/pull/1305) Handle panics from DynamicFunc.
- [#1300](https://github.com/wasmerio/wasmer/pull/1300) Add support for multiple versions of WASI tests: wasitests now test all versions of WASI.
- [#1292](https://github.com/wasmerio/wasmer/pull/1292) Experimental Support for Android (x86_64 and AArch64)

### Fixed
- [#1283](https://github.com/wasmerio/wasmer/pull/1283) Workaround for floating point arguments and return values in `DynamicFunc`s.

### Changed
- [#1401](https://github.com/wasmerio/wasmer/pull/1401) Make breaking change to `RuntimeError`: `RuntimeError` is now more explicit about its possible error values allowing for better insight into why a call into Wasm failed.
- [#1382](https://github.com/wasmerio/wasmer/pull/1382) Refactored test infranstructure (part 2)
- [#1380](https://github.com/wasmerio/wasmer/pull/1380) Refactored test infranstructure (part 1)
- [#1357](https://github.com/wasmerio/wasmer/pull/1357) Refactored bin commands into separate files
- [#1335](https://github.com/wasmerio/wasmer/pull/1335) Change mutability of `memory` to `const` in `wasmer_memory_data_length` in the C API
- [#1332](https://github.com/wasmerio/wasmer/pull/1332) Add option to `CompilerConfig` to force compiler IR verification off even when `debug_assertions` are enabled. This can be used to make debug builds faster, which may be important if you're creating a library that wraps Wasmer and depend on the speed of debug builds.
- [#1320](https://github.com/wasmerio/wasmer/pull/1320) Change `custom_sections` field in `ModuleInfo` to be more standards compliant by allowing multiple custom sections with the same name. To get the old behavior with the new API, you can add `.last().unwrap()` to accesses. For example, `module_info.custom_sections["custom_section_name"].last().unwrap()`.
- [#1301](https://github.com/wasmerio/wasmer/pull/1301) Update supported stable Rust version to 1.41.1.

## 0.16.2 - 2020-03-11

### Fixed

- [#1294](https://github.com/wasmerio/wasmer/pull/1294) Fix bug related to system calls in WASI that rely on reading from WasmPtrs as arrays of length 0. `WasmPtr` will now succeed on length 0 arrays again.

## 0.16.1 - 2020-03-11

### Fixed

- [#1291](https://github.com/wasmerio/wasmer/pull/1291) Fix installation packaging script to package the `wax` command.

## 0.16.0 - 2020-03-11

### Added
- [#1286](https://github.com/wasmerio/wasmer/pull/1286) Updated Windows Wasmer icons. Add wax
- [#1284](https://github.com/wasmerio/wasmer/pull/1284) Implement string and memory instructions in `wasmer-interface-types`

### Fixed
- [#1272](https://github.com/wasmerio/wasmer/pull/1272) Fix off-by-one error bug when accessing memory with a `WasmPtr` that contains the last valid byte of memory. Also changes the behavior of `WasmPtr<T, Array>` with a length of 0 and `WasmPtr<T>` where `std::mem::size_of::<T>()` is 0 to always return `None`

## 0.15.0 - 2020-03-04

- [#1263](https://github.com/wasmerio/wasmer/pull/1263) Changed the behavior of some WASI syscalls to now handle preopened directories more properly. Changed default `--debug` logging to only show Wasmer-related messages.
- [#1217](https://github.com/wasmerio/wasmer/pull/1217) Polymorphic host functions based on dynamic trampoline generation.
- [#1252](https://github.com/wasmerio/wasmer/pull/1252) Allow `/` in wasi `--mapdir` wasm path.
- [#1212](https://github.com/wasmerio/wasmer/pull/1212) Add support for GDB JIT debugging:
  - Add `--generate-debug-info` and `-g` flags to `wasmer run` to generate debug information during compilation. The debug info is passed via the GDB JIT interface to a debugger to allow source-level debugging of Wasm files. Currently only available on clif-backend.
  - Break public middleware APIs: there is now a `source_loc` parameter that should be passed through if applicable.
  - Break compiler trait methods such as `feed_local`, `feed_event` as well as `ModuleCodeGenerator::finalize`.

## 0.14.1 - 2020-02-24

- [#1245](https://github.com/wasmerio/wasmer/pull/1245) Use Ubuntu 16.04 in CI so that we use an earlier version of GLIBC.
- [#1234](https://github.com/wasmerio/wasmer/pull/1234) Check for unused excluded spectest failures.
- [#1232](https://github.com/wasmerio/wasmer/pull/1232) `wasmer-interface-types` has a WAT decoder.

## 0.14.0 - 2020-02-20

- [#1233](https://github.com/wasmerio/wasmer/pull/1233) Improved Wasmer C API release artifacts.
- [#1216](https://github.com/wasmerio/wasmer/pull/1216) `wasmer-interface-types` receives a binary encoder.
- [#1228](https://github.com/wasmerio/wasmer/pull/1228) Singlepass cleanup: Resolve several FIXMEs and remove protect_unix.
- [#1218](https://github.com/wasmerio/wasmer/pull/1218) Enable Cranelift verifier in debug mode. Fix bug with table indices being the wrong type.
- [#787](https://github.com/wasmerio/wasmer/pull/787) New crate `wasmer-interface-types` to implement WebAssembly Interface Types.
- [#1213](https://github.com/wasmerio/wasmer/pull/1213) Fixed WASI `fdstat` to detect `isatty` properly.
- [#1192](https://github.com/wasmerio/wasmer/pull/1192) Use `ExceptionCode` for error representation.
- [#1191](https://github.com/wasmerio/wasmer/pull/1191) Fix singlepass miscompilation on `Operator::CallIndirect`.
- [#1180](https://github.com/wasmerio/wasmer/pull/1180) Fix compilation for target `x86_64-unknown-linux-musl`.
- [#1170](https://github.com/wasmerio/wasmer/pull/1170) Improve the WasiFs builder API with convenience methods for overriding stdin, stdout, and stderr as well as a new sub-builder for controlling the permissions and properties of preopened directories.  Also breaks that implementations of `WasiFile` must be `Send` -- please file an issue if this change causes you any issues.
- [#1161](https://github.com/wasmerio/wasmer/pull/1161) Require imported functions to be `Send`. This is a breaking change that fixes a soundness issue in the API.
- [#1140](https://github.com/wasmerio/wasmer/pull/1140) Use [`blake3`](https://github.com/BLAKE3-team/BLAKE3) as default hashing algorithm for caching.
- [#1129](https://github.com/wasmerio/wasmer/pull/1129) Standard exception types for singlepass backend.

## 0.13.1 - 2020-01-16
- Fix bug in wapm related to the `package.wasmer_extra_flags` entry in the manifest

## 0.13.0 - 2020-01-15

Special thanks to [@repi](https://github.com/repi) and [@srenatus](https://github.com/srenatus) for their contributions!

- [#1153](https://github.com/wasmerio/wasmer/pull/1153) Added Wasmex, an Elixir language integration, to the README
- [#1133](https://github.com/wasmerio/wasmer/pull/1133) New `wasmer_trap` function in the C API, to properly error from within a host function
- [#1147](https://github.com/wasmerio/wasmer/pull/1147) Remove `log` and `trace` macros from `wasmer-runtime-core`, remove `debug` and `trace` features from `wasmer-*` crates, use the `log` crate for logging and use `fern` in the Wasmer CLI binary to output log messages.  Colorized output will be enabled automatically if printing to a terminal, to force colorization on or off, set the `WASMER_COLOR` environment variable to `true` or `false`.
- [#1128](https://github.com/wasmerio/wasmer/pull/1128) Fix a crash when a host function is missing and the `allow_missing_functions` flag is enabled
- [#1099](https://github.com/wasmerio/wasmer/pull/1099) Remove `backend::Backend` from `wasmer_runtime_core`
- [#1097](https://github.com/wasmerio/wasmer/pull/1097) Move inline breakpoint outside of runtime backend
- [#1095](https://github.com/wasmerio/wasmer/pull/1095) Update to cranelift 0.52.
- [#1092](https://github.com/wasmerio/wasmer/pull/1092) Add `get_utf8_string_with_nul` to `WasmPtr` to read nul-terminated strings from memory.
- [#1071](https://github.com/wasmerio/wasmer/pull/1071) Add support for non-trapping float-to-int conversions, enabled by default.

## 0.12.0 - 2019-12-18

Special thanks to [@ethanfrey](https://github.com/ethanfrey), [@AdamSLevy](https://github.com/AdamSLevy), [@Jasper-Bekkers](https://github.com/Jasper-Bekkers), [@srenatus](https://github.com/srenatus) for their contributions!

- [#1078](https://github.com/wasmerio/wasmer/pull/1078) Increase the maximum number of parameters `Func` can take
- [#1062](https://github.com/wasmerio/wasmer/pull/1062) Expose some opt-in Emscripten functions to the C API
- [#1032](https://github.com/wasmerio/wasmer/pull/1032) Change the signature of the Emscripten `abort` function to work with Emscripten 1.38.30
- [#1060](https://github.com/wasmerio/wasmer/pull/1060) Test the capi with all the backends
- [#1069](https://github.com/wasmerio/wasmer/pull/1069) Add function `get_memory_and_data` to `Ctx` to help prevent undefined behavior and mutable aliasing. It allows accessing memory while borrowing data mutably for the `Ctx` lifetime. This new function is now being used in `wasmer-wasi`.
- [#1058](https://github.com/wasmerio/wasmer/pull/1058) Fix minor panic issue when `wasmer::compile_with` called with llvm backend.
- [#858](https://github.com/wasmerio/wasmer/pull/858) Minor panic fix when wasmer binary with `loader` option run a module without exported `_start` function.
- [#1056](https://github.com/wasmerio/wasmer/pull/1056) Improved `--invoke` args parsing (supporting `i32`, `i64`, `f32` and `f32`) in Wasmer CLI
- [#1054](https://github.com/wasmerio/wasmer/pull/1054) Improve `--invoke` output in Wasmer CLI
- [#1053](https://github.com/wasmerio/wasmer/pull/1053) For RuntimeError and breakpoints, use Box<Any + Send> instead of Box<Any>.
- [#1052](https://github.com/wasmerio/wasmer/pull/1052) Fix minor panic and improve Error handling in singlepass backend.
- [#1050](https://github.com/wasmerio/wasmer/pull/1050) Attach C & C++ headers to releases.
- [#1033](https://github.com/wasmerio/wasmer/pull/1033) Set cranelift backend as default compiler backend again, require at least one backend to be enabled for Wasmer CLI
- [#1044](https://github.com/wasmerio/wasmer/pull/1044) Enable AArch64 support in the LLVM backend.
- [#1030](https://github.com/wasmerio/wasmer/pull/1030) Ability to generate `ImportObject` for a specific version WASI version with the C API.
- [#1028](https://github.com/wasmerio/wasmer/pull/1028) Introduce strict/non-strict modes for `get_wasi_version`
- [#1029](https://github.com/wasmerio/wasmer/pull/1029) Add the ‚Äúfloating‚Äù `WasiVersion::Latest` version.
- [#1006](https://github.com/wasmerio/wasmer/pull/1006) Fix minor panic issue when `wasmer::compile_with` called with llvm backend
- [#1009](https://github.com/wasmerio/wasmer/pull/1009) Enable LLVM verifier for all tests, add new llvm-backend-tests crate.
- [#1022](https://github.com/wasmerio/wasmer/pull/1022) Add caching support for Singlepass backend.
- [#1004](https://github.com/wasmerio/wasmer/pull/1004) Add the Auto backend to enable to adapt backend usage depending on wasm file executed.
- [#1068](https://github.com/wasmerio/wasmer/pull/1068) Various cleanups for the singlepass backend on AArch64.

## 0.11.0 - 2019-11-22

- [#713](https://github.com/wasmerio/wasmer/pull/713) Add AArch64 support for singlepass.
- [#995](https://github.com/wasmerio/wasmer/pull/995) Detect when a global is read without being initialized (emit a proper error instead of panicking)
- [#996](https://github.com/wasmerio/wasmer/pull/997) Refactored spectests, emtests and wasitests to use default compiler logic
- [#992](https://github.com/wasmerio/wasmer/pull/992) Updates WAPM version to 0.4.1, fix arguments issue introduced in #990
- [#990](https://github.com/wasmerio/wasmer/pull/990) Default wasmer CLI to `run`.  Wasmer will now attempt to parse unrecognized command line options as if they were applied to the run command: `wasmer mywasm.wasm --dir=.` now works!
- [#987](https://github.com/wasmerio/wasmer/pull/987) Fix `runtime-c-api` header files when compiled by gnuc.
- [#957](https://github.com/wasmerio/wasmer/pull/957) Change the meaning of `wasmer_wasi::is_wasi_module` to detect any type of WASI module, add support for new wasi snapshot_preview1
- [#934](https://github.com/wasmerio/wasmer/pull/934) Simplify float expressions in the LLVM backend.

## 0.10.2 - 2019-11-18

- [#968](https://github.com/wasmerio/wasmer/pull/968) Added `--invoke` option to the command
- [#964](https://github.com/wasmerio/wasmer/pull/964) Enable cross-compilation for specific target
- [#971](https://github.com/wasmerio/wasmer/pull/971) In LLVM backend, use unaligned loads and stores for non-atomic accesses to wasmer memory.
- [#960](https://github.com/wasmerio/wasmer/pull/960) Fix `runtime-c-api` header files when compiled by clang.
- [#925](https://github.com/wasmerio/wasmer/pull/925) Host functions can be closures with a captured environment.
- [#917](https://github.com/wasmerio/wasmer/pull/917) Host functions (aka imported functions) may not have `&mut vm::Ctx` as first argument, i.e. the presence of the `&mut vm::Ctx` argument is optional.
- [#915](https://github.com/wasmerio/wasmer/pull/915) All backends share the same definition of `Trampoline` (defined in `wasmer-runtime-core`).

## 0.10.1 - 2019-11-11

- [#952](https://github.com/wasmerio/wasmer/pull/952) Use C preprocessor to properly hide trampoline functions on Windows and non-x86_64 targets.

## 0.10.0 - 2019-11-11

Special thanks to [@newpavlov](https://github.com/newpavlov) and [@Maxgy](https://github.com/Maxgy) for their contributions!

- [#942](https://github.com/wasmerio/wasmer/pull/942) Deny missing docs in runtime core and add missing docs
- [#939](https://github.com/wasmerio/wasmer/pull/939) Fix bug causing attempts to append to files with WASI to delete the contents of the file
- [#940](https://github.com/wasmerio/wasmer/pull/940) Update supported Rust version to 1.38+
- [#923](https://github.com/wasmerio/wasmer/pull/923) Fix memory leak in the C API caused by an incorrect cast in `wasmer_trampoline_buffer_destroy`
- [#921](https://github.com/wasmerio/wasmer/pull/921) In LLVM backend, annotate all memory accesses with TBAA metadata.
- [#883](https://github.com/wasmerio/wasmer/pull/883) Allow floating point operations to have arbitrary inputs, even including SNaNs.
- [#856](https://github.com/wasmerio/wasmer/pull/856) Expose methods in the runtime C API to get a WASI import object

## 0.9.0 - 2019-10-23

Special thanks to @alocquet for their contributions!

- [#898](https://github.com/wasmerio/wasmer/pull/898) State tracking is now disabled by default in the LLVM backend. It can be enabled with `--track-state`.
- [#861](https://github.com/wasmerio/wasmer/pull/861) Add descriptions to `unimplemented!` macro in various places
- [#897](https://github.com/wasmerio/wasmer/pull/897) Removes special casing of stdin, stdout, and stderr in WASI.  Closing these files now works.  Removes `stdin`, `stdout`, and `stderr` from `WasiFS`, replaced by the methods `stdout`, `stdout_mut`, and so on.
- [#863](https://github.com/wasmerio/wasmer/pull/863) Fix min and max for cases involving NaN and negative zero when using the LLVM backend.

## 0.8.0 - 2019-10-02

Special thanks to @jdanford for their contributions!

- [#850](https://github.com/wasmerio/wasmer/pull/850) New `WasiStateBuilder` API. small, add misc. breaking changes to existing API (for example, changing the preopen dirs arg on `wasi::generate_import_object` from `Vec<String>` to `Vec<Pathbuf>`)
- [#852](https://github.com/wasmerio/wasmer/pull/852) Make minor grammar/capitalization fixes to README.md
- [#841](https://github.com/wasmerio/wasmer/pull/841) Slightly improve rustdoc documentation and small updates to outdated info in readme files
- [#836](https://github.com/wasmerio/wasmer/pull/836) Update Cranelift fork version to `0.44.0`
- [#839](https://github.com/wasmerio/wasmer/pull/839) Change supported version to stable Rust 1.37+
- [#834](https://github.com/wasmerio/wasmer/pull/834) Fix panic when unwraping `wasmer` arguments
- [#835](https://github.com/wasmerio/wasmer/pull/835) Add parallel execution example (independent instances created from the same `ImportObject` and `Module` run with rayon)
- [#834](https://github.com/wasmerio/wasmer/pull/834) Fix panic when parsing numerical arguments for no-ABI targets run with the wasmer binary
- [#833](https://github.com/wasmerio/wasmer/pull/833) Add doc example of using ImportObject's new `maybe_with_namespace` method
- [#832](https://github.com/wasmerio/wasmer/pull/832) Delete unused runtime ABI
- [#809](https://github.com/wasmerio/wasmer/pull/809) Fix bugs leading to panics in `LocalBacking`.
- [#831](https://github.com/wasmerio/wasmer/pull/831) Add support for atomic operations, excluding wait and notify, to singlepass.
- [#822](https://github.com/wasmerio/wasmer/pull/822) Update Cranelift fork version to `0.43.1`
- [#829](https://github.com/wasmerio/wasmer/pull/829) Fix deps on `make bench-*` commands; benchmarks don't compile other backends now
- [#807](https://github.com/wasmerio/wasmer/pull/807) Implement Send for `Instance`, breaking change on `ImportObject`, remove method `get_namespace` replaced with `with_namespace` and `maybe_with_namespace`
- [#817](https://github.com/wasmerio/wasmer/pull/817) Add document for tracking features across backends and language integrations, [docs/feature_matrix.md]
- [#823](https://github.com/wasmerio/wasmer/issues/823) Improved Emscripten / WASI integration
- [#821](https://github.com/wasmerio/wasmer/issues/821) Remove patch version on most deps Cargo manifests.  This gives Wasmer library users more control over which versions of the deps they use.
- [#820](https://github.com/wasmerio/wasmer/issues/820) Remove null-pointer checks in `WasmPtr` from runtime-core, re-add them in Emscripten
- [#803](https://github.com/wasmerio/wasmer/issues/803) Add method to `Ctx` to invoke functions by their `TableIndex`
- [#790](https://github.com/wasmerio/wasmer/pull/790) Fix flaky test failure with LLVM, switch to large code model.
- [#788](https://github.com/wasmerio/wasmer/pull/788) Use union merge on the changelog file.
- [#785](https://github.com/wasmerio/wasmer/pull/785) Include Apache license file for spectests.
- [#786](https://github.com/wasmerio/wasmer/pull/786) In the LLVM backend, lower atomic wasm operations to atomic machine instructions.
- [#784](https://github.com/wasmerio/wasmer/pull/784) Fix help string for wasmer run.

## 0.7.0 - 2019-09-12

Special thanks to @YaronWittenstein @penberg for their contributions.

- [#776](https://github.com/wasmerio/wasmer/issues/776) Allow WASI preopened fds to be closed
- [#774](https://github.com/wasmerio/wasmer/issues/774) Add more methods to the `WasiFile` trait
- [#772](https://github.com/wasmerio/wasmer/issues/772) [#770](https://github.com/wasmerio/wasmer/issues/770) Handle more internal failures by passing back errors
- [#756](https://github.com/wasmerio/wasmer/issues/756) Allow NULL parameter and 0 arity in `wasmer_export_func_call` C API
- [#747](https://github.com/wasmerio/wasmer/issues/747) Return error instead of panicking on traps when using the Wasmer binary
- [#741](https://github.com/wasmerio/wasmer/issues/741) Add validate Wasm fuzz target
- [#733](https://github.com/wasmerio/wasmer/issues/733) Remove dependency on compiler backends for `middleware-common`
- [#732](https://github.com/wasmerio/wasmer/issues/732) [#731](https://github.com/wasmerio/wasmer/issues/731) WASI bug fixes and improvements
- [#726](https://github.com/wasmerio/wasmer/issues/726) Add serialization and deserialization for Wasi State
- [#716](https://github.com/wasmerio/wasmer/issues/716) Improve portability of install script
- [#714](https://github.com/wasmerio/wasmer/issues/714) Add Code of Conduct
- [#708](https://github.com/wasmerio/wasmer/issues/708) Remove unconditional dependency on Cranelift in the C API
- [#703](https://github.com/wasmerio/wasmer/issues/703) Fix compilation on AArch64 Linux
- [#702](https://github.com/wasmerio/wasmer/issues/702) Add SharedMemory to Wasmer. Add `--enable-threads` flag, add partial implementation of atomics to LLVM backend.
- [#698](https://github.com/wasmerio/wasmer/issues/698) [#690](https://github.com/wasmerio/wasmer/issues/690) [#687](https://github.com/wasmerio/wasmer/issues/690) Fix panics in Emscripten
- [#689](https://github.com/wasmerio/wasmer/issues/689) Replace `wasmer_runtime_code::memory::Atomic` with `std::sync::atomic` atomics, changing its interface
- [#680](https://github.com/wasmerio/wasmer/issues/680) [#673](https://github.com/wasmerio/wasmer/issues/673) [#669](https://github.com/wasmerio/wasmer/issues/669) [#660](https://github.com/wasmerio/wasmer/issues/660) [#659](https://github.com/wasmerio/wasmer/issues/659) Misc. runtime and singlepass fixes
- [#677](https://github.com/wasmerio/wasmer/issues/677) [#675](https://github.com/wasmerio/wasmer/issues/675) [#674](https://github.com/wasmerio/wasmer/issues/674) LLVM backend fixes and improvements
- [#671](https://github.com/wasmerio/wasmer/issues/671) Implement fs polling in `wasi::poll_oneoff` for Unix-like platforms
- [#656](https://github.com/wasmerio/wasmer/issues/656) Move CI to Azure Pipelines
- [#650](https://github.com/wasmerio/wasmer/issues/650) Implement `wasi::path_rename`, improve WASI FS public api, and allow open files to exist even when the underlying file is deleted
- [#643](https://github.com/wasmerio/wasmer/issues/643) Implement `wasi::path_symlink` and improve WASI FS public api IO error reporting
- [#608](https://github.com/wasmerio/wasmer/issues/608) Implement wasi syscalls `fd_allocate`, `fd_sync`, `fd_pread`, `path_link`, `path_filestat_set_times`; update WASI fs API in a WIP way; reduce coupling of WASI code to host filesystem; make debug messages from WASI more readable; improve rights-checking when calling syscalls; implement reference counting on inodes; misc bug fixes and improvements
- [#616](https://github.com/wasmerio/wasmer/issues/616) Create the import object separately from instance instantiation in `runtime-c-api`
- [#620](https://github.com/wasmerio/wasmer/issues/620) Replace one `throw()` with `noexcept` in llvm backend
- [#618](https://github.com/wasmerio/wasmer/issues/618) Implement `InternalEvent::Breakpoint` in the llvm backend to allow metering in llvm
- [#615](https://github.com/wasmerio/wasmer/issues/615) Eliminate `FunctionEnvironment` construction in `feed_event()` speeding up to 70% of compilation in clif
- [#609](https://github.com/wasmerio/wasmer/issues/609) Update dependencies
- [#602](https://github.com/wasmerio/wasmer/issues/602) C api extract instance context from instance
- [#590](https://github.com/wasmerio/wasmer/issues/590) Error visibility changes in wasmer-c-api
- [#589](https://github.com/wasmerio/wasmer/issues/589) Make `wasmer_byte_array` fields `public` in wasmer-c-api

## 0.6.0 - 2019-07-31
- [#603](https://github.com/wasmerio/wasmer/pull/603) Update Wapm-cli, bump version numbers
- [#595](https://github.com/wasmerio/wasmer/pull/595) Add unstable public API for interfacing with the WASI file system in plugin-like usecases
- [#598](https://github.com/wasmerio/wasmer/pull/598) LLVM Backend is now supported in Windows
- [#599](https://github.com/wasmerio/wasmer/pull/599) Fix llvm backend failures in fat spec tests and simd_binaryen spec test.
- [#579](https://github.com/wasmerio/wasmer/pull/579) Fix bug in caching with LLVM and Singlepass backends.
  Add `default-backend-singlepass`, `default-backend-llvm`, and `default-backend-cranelift` features to `wasmer-runtime`
  to control the `default_compiler()` function (this is a breaking change).  Add `compiler_for_backend` function in `wasmer-runtime`
- [#561](https://github.com/wasmerio/wasmer/pull/561) Call the `data_finalizer` field on the `Ctx`
- [#576](https://github.com/wasmerio/wasmer/pull/576) fix `Drop` of uninit `Ctx`
- [#542](https://github.com/wasmerio/wasmer/pull/542) Add SIMD support to Wasmer (LLVM backend only)
  - Updates LLVM to version 8.0

## 0.5.7 - 2019-07-23
- [#575](https://github.com/wasmerio/wasmer/pull/575) Prepare for release; update wapm to 0.3.6
- [#555](https://github.com/wasmerio/wasmer/pull/555) WASI filesystem rewrite.  Major improvements
  - adds virtual root showing all preopened directories
  - improved sandboxing and code-reuse
  - symlinks work in a lot more situations
  - many misc. improvements to most syscalls touching the filesystem

## 0.5.6 - 2019-07-16
- [#565](https://github.com/wasmerio/wasmer/pull/565) Update wapm and bump version to 0.5.6
- [#563](https://github.com/wasmerio/wasmer/pull/563) Improve wasi testing infrastructure
  - fixes arg parsing from comments & fixes the mapdir test to have the native code doing the same thing as the WASI code
  - makes wasitests-generate output stdout/stderr by default & adds function to print stdout and stderr for a command if it fails
  - compiles wasm with size optimizations & strips generated wasm with wasm-strip
- [#554](https://github.com/wasmerio/wasmer/pull/554) Finish implementation of `wasi::fd_seek`, fix bug in filestat
- [#550](https://github.com/wasmerio/wasmer/pull/550) Fix singlepass compilation error with `imul` instruction

## 0.5.5 - 2019-07-10
- [#541](https://github.com/wasmerio/wasmer/pull/541) Fix dependency graph by making separate test crates; ABI implementations should not depend on compilers. Add Cranelift fork as git submodule of clif-backend
- [#537](https://github.com/wasmerio/wasmer/pull/537) Add hidden flag (`--cache-key`) to use prehashed key into the compiled wasm cache and change compiler backend-specific caching to use directories
- [#536](https://github.com/wasmerio/wasmer/pull/536) ~Update cache to use compiler backend name in cache key~

## 0.5.4 - 2019-07-06
- [#529](https://github.com/wasmerio/wasmer/pull/529) Updates the Wasm Interface library, which is used by wapm, with bug fixes and error message improvements

## 0.5.3 - 2019-07-03
- [#523](https://github.com/wasmerio/wasmer/pull/523) Update wapm version to fix bug related to signed packages in the global namespace and locally-stored public keys

## 0.5.2 - 2019-07-02
- [#516](https://github.com/wasmerio/wasmer/pull/516) Add workaround for singlepass miscompilation on GetLocal
- [#521](https://github.com/wasmerio/wasmer/pull/521) Update Wapm-cli, bump version numbers
- [#518](https://github.com/wasmerio/wasmer/pull/518) Update Cranelift and WasmParser
- [#514](https://github.com/wasmerio/wasmer/pull/514) [#519](https://github.com/wasmerio/wasmer/pull/519) Improved Emscripten network related calls, added a null check to `WasmPtr`
- [#515](https://github.com/wasmerio/wasmer/pull/515) Improved Emscripten dyncalls
- [#513](https://github.com/wasmerio/wasmer/pull/513) Fix emscripten lseek implementation.
- [#510](https://github.com/wasmerio/wasmer/pull/510) Simplify construction of floating point constants in LLVM backend. Fix LLVM assertion failure due to definition of %ctx.

## 0.5.1 - 2019-06-24
- [#508](https://github.com/wasmerio/wasmer/pull/508) Update wapm version, includes bug fixes

## 0.5.0 - 2019-06-17

- [#471](https://github.com/wasmerio/wasmer/pull/471) Added missing functions to run Python. Improved Emscripten bindings
- [#494](https://github.com/wasmerio/wasmer/pull/494) Remove deprecated type aliases from libc in the runtime C API
- [#493](https://github.com/wasmerio/wasmer/pull/493) `wasmer_module_instantiate` has better error messages in the runtime C API
- [#474](https://github.com/wasmerio/wasmer/pull/474) Set the install name of the dylib to `@rpath`
- [#490](https://github.com/wasmerio/wasmer/pull/490) Add MiddlewareChain and StreamingCompiler to runtime
- [#487](https://github.com/wasmerio/wasmer/pull/487) Fix stack offset check in singlepass backend
- [#450](https://github.com/wasmerio/wasmer/pull/450) Added Metering
- [#481](https://github.com/wasmerio/wasmer/pull/481) Added context trampoline into runtime
- [#484](https://github.com/wasmerio/wasmer/pull/484) Fix bugs in emscripten socket syscalls
- [#476](https://github.com/wasmerio/wasmer/pull/476) Fix bug with wasi::environ_get, fix off by one error in wasi::environ_sizes_get
- [#470](https://github.com/wasmerio/wasmer/pull/470) Add mapdir support to Emscripten, implement getdents for Unix
- [#467](https://github.com/wasmerio/wasmer/pull/467) `wasmer_instantiate` returns better error messages in the runtime C API
- [#463](https://github.com/wasmerio/wasmer/pull/463) Fix bug in WASI path_open allowing one level above preopened dir to be accessed
- [#461](https://github.com/wasmerio/wasmer/pull/461) Prevent passing negative lengths in various places in the runtime C API
- [#459](https://github.com/wasmerio/wasmer/pull/459) Add monotonic and real time clocks for wasi on windows
- [#447](https://github.com/wasmerio/wasmer/pull/447) Add trace macro (`--features trace`) for more verbose debug statements
- [#451](https://github.com/wasmerio/wasmer/pull/451) Add `--mapdir=src:dest` flag to rename host directories in the guest context
- [#457](https://github.com/wasmerio/wasmer/pull/457) Implement file metadata for WASI, fix bugs in WASI clock code for Unix platforms

## 0.4.2 - 2019-05-16

- [#416](https://github.com/wasmerio/wasmer/pull/416) Remote code loading framework
- [#449](https://github.com/wasmerio/wasmer/pull/449) Fix bugs: opening host files in filestat and opening with write permissions unconditionally in path_open
- [#442](https://github.com/wasmerio/wasmer/pull/442) Misc. WASI FS fixes and implement readdir
- [#440](https://github.com/wasmerio/wasmer/pull/440) Fix type mismatch between `wasmer_instance_call` and `wasmer_export_func_*_arity` functions in the runtime C API.
- [#269](https://github.com/wasmerio/wasmer/pull/269) Add better runtime docs
- [#432](https://github.com/wasmerio/wasmer/pull/432) Fix returned value of `wasmer_last_error_message` in the runtime C API
- [#429](https://github.com/wasmerio/wasmer/pull/429) Get wasi::path_filestat_get working for some programs; misc. minor WASI FS improvements
- [#413](https://github.com/wasmerio/wasmer/pull/413) Update LLVM backend to use new parser codegen traits

## 0.4.1 - 2019-05-06

- [#426](https://github.com/wasmerio/wasmer/pull/426) Update wapm-cli submodule, bump version to 0.4.1
- [#422](https://github.com/wasmerio/wasmer/pull/422) Improved Emscripten functions to run optipng and pngquant compiled to wasm
- [#409](https://github.com/wasmerio/wasmer/pull/409) Improved Emscripten functions to run JavascriptCore compiled to wasm
- [#399](https://github.com/wasmerio/wasmer/pull/399) Add example of using a plugin extended from WASI
- [#397](https://github.com/wasmerio/wasmer/pull/397) Fix WASI fs abstraction to work on Windows
- [#390](https://github.com/wasmerio/wasmer/pull/390) Pin released wapm version and add it as a git submodule
- [#408](https://github.com/wasmerio/wasmer/pull/408) Add images to windows installer and update installer to add wapm bin directory to path

## 0.4.0 - 2019-04-23

- [#383](https://github.com/wasmerio/wasmer/pull/383) Hook up wasi exit code to wasmer cli.
- [#382](https://github.com/wasmerio/wasmer/pull/382) Improve error message on `--backend` flag to only suggest currently enabled backends
- [#381](https://github.com/wasmerio/wasmer/pull/381) Allow retrieving propagated user errors.
- [#379](https://github.com/wasmerio/wasmer/pull/379) Fix small return types from imported functions.
- [#371](https://github.com/wasmerio/wasmer/pull/371) Add more Debug impl for WASI types
- [#368](https://github.com/wasmerio/wasmer/pull/368) Fix issue with write buffering
- [#343](https://github.com/wasmerio/wasmer/pull/343) Implement preopened files for WASI and fix aligment issue when accessing WASI memory
- [#367](https://github.com/wasmerio/wasmer/pull/367) Add caching support to the LLVM backend.
- [#366](https://github.com/wasmerio/wasmer/pull/366) Remove `UserTrapper` trait to fix [#365](https://github.com/wasmerio/wasmer/issues/365).
- [#348](https://github.com/wasmerio/wasmer/pull/348) Refactor internal runtime ‚ÜîÔ∏è backend abstraction.
- [#355](https://github.com/wasmerio/wasmer/pull/355) Misc changes to `Cargo.toml`s for publishing
- [#352](https://github.com/wasmerio/wasmer/pull/352) Bump version numbers to 0.3.0
- [#351](https://github.com/wasmerio/wasmer/pull/351) Add hidden option to specify wasm program name (can be used to improve error messages)
- [#350](https://github.com/wasmerio/wasmer/pull/350) Enforce that CHANGELOG.md is updated through CI.
- [#349](https://github.com/wasmerio/wasmer/pull/349) Add [CHANGELOG.md](https://github.com/wasmerio/wasmer/blob/master/CHANGELOG.md).

## 0.3.0 - 2019-04-12

- [#276](https://github.com/wasmerio/wasmer/pull/276) [#288](https://github.com/wasmerio/wasmer/pull/288) [#344](https://github.com/wasmerio/wasmer/pull/344) Use new singlepass backend (with the `--backend=singlepass` when running Wasmer)
- [#338](https://github.com/wasmerio/wasmer/pull/338) Actually catch traps/panics/etc when using a typed func.
- [#325](https://github.com/wasmerio/wasmer/pull/325) Fixed func_index in debug mode
- [#323](https://github.com/wasmerio/wasmer/pull/323) Add validate subcommand to validate Wasm files
- [#321](https://github.com/wasmerio/wasmer/pull/321) Upgrade to Cranelift 0.3.0
- [#319](https://github.com/wasmerio/wasmer/pull/319) Add Export and GlobalDescriptor to Runtime API
- [#310](https://github.com/wasmerio/wasmer/pull/310) Cleanup warnings
- [#299](https://github.com/wasmerio/wasmer/pull/299) [#300](https://github.com/wasmerio/wasmer/pull/300) [#301](https://github.com/wasmerio/wasmer/pull/301) [#303](https://github.com/wasmerio/wasmer/pull/303) [#304](https://github.com/wasmerio/wasmer/pull/304) [#305](https://github.com/wasmerio/wasmer/pull/305) [#306](https://github.com/wasmerio/wasmer/pull/306) [#307](https://github.com/wasmerio/wasmer/pull/307) Add support for WASI üéâ
- [#286](https://github.com/wasmerio/wasmer/pull/286) Add extend to imports
- [#278](https://github.com/wasmerio/wasmer/pull/278) Add versioning to cache
- [#250](https://github.com/wasmerio/wasmer/pull/250) Setup bors

'''
'''--- CODE_OF_CONDUCT.md ---
# Contributor Covenant Code of Conduct

## Our Pledge

In the interest of fostering an open and welcoming environment, we as
contributors and maintainers pledge to making participation in our project and
our community a harassment-free experience for everyone, regardless of age, body
size, disability, ethnicity, sex characteristics, gender identity and expression,
level of experience, education, socio-economic status, nationality, personal
appearance, race, religion, or sexual identity and orientation.

## Our Standards

Examples of behavior that contributes to creating a positive environment
include:

* Using welcoming and inclusive language
* Being respectful of differing viewpoints and experiences
* Gracefully accepting constructive criticism
* Focusing on what is best for the community
* Showing empathy towards other community members

Examples of unacceptable behavior by participants include:

* The use of sexualized language or imagery and unwelcome sexual attention or
 advances
* Trolling, insulting/derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or electronic
 address, without explicit permission
* Other conduct which could reasonably be considered inappropriate in a
 professional setting

## Our Responsibilities

Project maintainers are responsible for clarifying the standards of acceptable
behavior and are expected to take appropriate and fair corrective action in
response to any instances of unacceptable behavior.

Project maintainers have the right and responsibility to remove, edit, or
reject comments, commits, code, wiki edits, issues, and other contributions
that are not aligned to this Code of Conduct, or to ban temporarily or
permanently any contributor for other behaviors that they deem inappropriate,
threatening, offensive, or harmful.

## Scope

This Code of Conduct applies both within project spaces and in public spaces
when an individual is representing the project or its community. Examples of
representing a project or community include using an official project e-mail
address, posting via an official social media account, or acting as an appointed
representative at an online or offline event. Representation of a project may be
further defined and clarified by project maintainers.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported by contacting the project team at contact@wasmer.io. All
complaints will be reviewed and investigated and will result in a response that
is deemed necessary and appropriate to the circumstances. The project team is
obligated to maintain confidentiality with regard to the reporter of an incident.
Further details of specific enforcement policies may be posted separately.

Project maintainers who do not follow or enforce the Code of Conduct in good
faith may face temporary or permanent repercussions as determined by other
members of the project's leadership.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,
available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html

[homepage]: https://www.contributor-covenant.org

For answers to common questions about this code of conduct, see
https://www.contributor-covenant.org/faq

'''
'''--- CONTRIBUTING.md ---
# How to Contribute to Wasmer

Thank you for your interest in contributing to Wasmer. This document outlines some recommendations on how to contribute.

## Issues & Feature Requests

Please use the issue template and provide a failing example if possible to help us recreate the issue.

## Pull Requests

For large changes, please try reaching communicating with the Wasmer maintainers via GitHub Issues or Spectrum Chat to ensure we can accept the change once it is ready.

We recommend trying the following commands before sending a pull request to ensure code quality:

- `cargo fmt --all` Ensures all code is correctly formatted.
- Run `cargo test` in the crates that you are modifying.
- Run `cargo build --all`.

A comprehensive CI test suite will be run by a Wasmer team member after the PR has been created.

### Common Build Issues

#### LLVM Dependency

`Didn't find usable system-wide LLVM`

Building Wasmer with the LLVM backend requires LLVM to be installed

'''
'''--- Cargo.toml ---
[package]
name = "wasmer-workspace"
version = "2.4.1"
description = "Wasmer workspace"
authors = ["Wasmer Engineering Team <engineering@wasmer.io>"]
repository = "https://github.com/wasmerio/wasmer"
license = "MIT"
edition = "2018"
publish = false
autoexamples = false

[dependencies]
wasmer = { version = "=2.4.1", path = "lib/api", package = "wasmer-near" }
wasmer-compiler = { version = "=2.4.1", path = "lib/compiler", package = "wasmer-compiler-near" }
wasmer-compiler-cranelift = { version = "2.0.0", path = "lib/compiler-cranelift", optional = true }
wasmer-compiler-singlepass = { version = "=2.4.1", path = "lib/compiler-singlepass", optional = true, package = "wasmer-compiler-singlepass-near" }
wasmer-compiler-llvm = { version = "2.0.0", path = "lib/compiler-llvm", optional = true }
wasmer-engine = { version = "=2.4.1", path = "lib/engine", package = "wasmer-engine-near"  }
wasmer-engine-universal = { version = "=2.4.1", path = "lib/engine-universal", optional = true, package = "wasmer-engine-universal-near" }
wasmer-wast = { version = "2.0.0", path = "tests/lib/wast", optional = true }
wasmer-types = { version = "=2.4.1", path = "lib/types", package = "wasmer-types-near" }
wasmer-vm = { version = "=2.4.1", path = "lib/vm", package = "wasmer-vm-near" }

cfg-if = "1.0"

[workspace]
members = [
    "lib/api",
    "lib/compiler",
    "lib/compiler-cranelift",
    "lib/compiler-singlepass",
    "lib/compiler-llvm",
    "lib/derive",
    "lib/engine",
    "lib/engine-universal",
    "lib/vm",
    "lib/types",
    "tests/lib/wast",
    "tests/lib/compiler-test-derive",
    "tests/integration/ios",
    "fuzz",
]
resolver = "2"

[build-dependencies]
test-generator = { path = "tests/lib/test-generator" }
build-deps = "0.1.4"
anyhow = "1.0"
glob = "0.3"
rustc_version = "0.4"

[dev-dependencies]
anyhow = "1.0"
criterion = "0.3"
lazy_static = "1.4"
serial_test = "0.5"
compiler-test-derive = { path = "tests/lib/compiler-test-derive" }
tempfile = "3.1"
# For logging tests using the `RUST_LOG=debug` when testing
test-log = { version = "0.2", default-features = false, features = ["trace"] }
tracing = { version = "0.1", default-features = false, features = ["log"] }
tracing-subscriber = { version = "0.3", default-features = false, features = ["env-filter", "fmt"] }
wat = "1.0"

[features]
# Don't add the compiler features in default, please add them on the Makefile
# since we might want to autoconfigure them depending on the availability on the host.
default = [
    "wasmer/wat",
    "wast",
    "universal",
    "singlepass",
]
engine = []
universal = [
    "wasmer-engine-universal",
    "engine",
]
wast = ["wasmer-wast"]
compiler = [
    "wasmer/compiler",
    "wasmer-compiler/translator",
    "wasmer-engine-universal/compiler",
]
singlepass = [
    "wasmer-compiler-singlepass",
    "compiler",
]
cranelift = [
    "wasmer-compiler-cranelift",
    "compiler",
]
llvm = [
    "wasmer-compiler-llvm",
    "compiler",
]

# Testing features
test-singlepass = [
    "singlepass",
]
test-cranelift = [
    "cranelift",
]
test-llvm = [
    "llvm",
]
test-universal = [
    "universal",
    "test-generator/test-universal",
]

# Specifies that we're running in coverage testing mode. This disables tests
# that raise signals because that interferes with tarpaulin.
coverage = []

[profile.dev]
split-debuginfo = "unpacked"

[[bench]]
name = "static_and_dynamic_functions"
harness = false

[[bench]]
name = "many_functions"
harness = false

[[example]]
name = "early-exit"
path = "examples/early_exit.rs"
required-features = ["cranelift"]

[[example]]
name = "engine-universal"
path = "examples/engine_universal.rs"
required-features = ["cranelift"]

[[example]]
name = "engine-headless"
path = "examples/engine_headless.rs"
required-features = ["cranelift"]

[[example]]
name = "platform-headless-ios"
path = "examples/platform_ios_headless.rs"
required-features = ["cranelift"]

[[example]]
name = "cross-compilation"
path = "examples/engine_cross_compilation.rs"
required-features = ["cranelift"]

[[example]]
name = "compiler-singlepass"
path = "examples/compiler_singlepass.rs"
required-features = ["singlepass"]

[[example]]
name = "compiler-cranelift"
path = "examples/compiler_cranelift.rs"
required-features = ["cranelift"]

[[example]]
name = "compiler-llvm"
path = "examples/compiler_llvm.rs"
required-features = ["llvm"]

[[example]]
name = "exported-function"
path = "examples/exports_function.rs"
required-features = ["cranelift"]

[[example]]
name = "exported-global"
path = "examples/exports_global.rs"
required-features = ["cranelift"]

[[example]]
name = "exported-memory"
path = "examples/exports_memory.rs"
required-features = ["cranelift"]

[[example]]
name = "imported-function"
path = "examples/imports_function.rs"
required-features = ["cranelift"]

[[example]]
name = "imported-global"
path = "examples/imports_global.rs"
required-features = ["cranelift"]

[[example]]
name = "tunables-limit-memory"
path = "examples/tunables_limit_memory.rs"
required-features = ["cranelift"]

[[example]]
name = "table"
path = "examples/table.rs"
required-features = ["cranelift"]

[[example]]
name = "memory"
path = "examples/memory.rs"
required-features = ["cranelift"]

[[example]]
name = "instance"
path = "examples/instance.rs"
required-features = ["cranelift"]

[[example]]
name = "errors"
path = "examples/errors.rs"
required-features = ["cranelift"]

[[example]]
name = "imported-function-env"
path = "examples/imports_function_env.rs"
required-features = ["cranelift"]

[[example]]
name = "hello-world"
path = "examples/hello_world.rs"
required-features = ["cranelift"]

[[example]]
name = "metering"
path = "examples/metering.rs"
required-features = ["cranelift"]

[[example]]
name = "imports-exports"
path = "examples/imports_exports.rs"
required-features = ["cranelift"]

[[example]]
name = "features"
path = "examples/features.rs"
required-features = ["cranelift"]

'''
'''--- PACKAGING.md ---
# Wasmer OS distro packaging notes

* Wasmer is written in Rust. To build Wasmer, where possible, do not
  directly invoke `cargo`, but use the supplied `Makefile`

* Wasmer provides several compilers and the `Makefile` autodetects
  when compilers can be compiled and/or installed. Set the environment
  variables `ENABLE_{CRANELIFT,LLVM,SINGLEPASS}=1` to force compiler
  to be build or to fail trying, e.g:

  ```sh
  $ ENABLE_LLVM=1 make build-wasmer
  ```

* `make install` respects `DESTDIR`, but `prefix` must be configured
  with `WASMER_INSTALL_PREFIX`, e.g.:

  ```sh
  export WASMER_INSTALL_PREFIX=/usr
  make
  DESTDIR=.../usr make install
  ```

* In case you must build/install directly with `cargo`, make sure to
  enable at least one compiler feature, like e.g. `--features
  cranelift`,

  * Beware that compiling with `cargo build --workspace --features ‚Ä¶`
    will not enable features on the subcrates in the workspace and
    result in a headless Wasmer binary that can not compile Wasm files
    directly.

* If you split the package into several subpackages, beware that the
  `create-exe` command of the `wasmer` CLI requires `libwasmer.a` to
  be installed at `$WASMER_INSTALL_PREFIX/lib/libwasmer.a`. Suggestions for splitting:

  * The `wasmer-headless` CLI contains a subset of the `wasmer`'s functionalities
    and should only be packaged when splitting ‚Äî it must be built
    explicitly with:
    
    ```sh
    $ make build-wasmer-headless-minimal install-wasmer-headless-minimal
    ```
  * `libwasmer`, containing `libwasmer.so*`,
  * `libwasmer-dev`, containing the header files and a `.pc` file,
  * `libwasmer-static`, containing `libwasmer.a`.

The Wasmer distro packaging story is still in its infancy, so feedback is very welcome.

## Miscellaneous: binfmt_misc

Wasmer can be registered as a binfmt interpreter for wasm binaries.
An example systemd [.service](./scripts/wasmer-binfmt.service.example) is included here.
Please consider statically linking the wasmer binary so that this capability is also available in mount namespaces.

'''
'''--- README.md ---
<div align="center">
  <a href="https://wasmer.io" target="_blank" rel="noopener noreferrer">
    <img width="300" src="https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/logo.png" alt="Wasmer logo">
  </a>

  <p>
    <a href="https://github.com/wasmerio/wasmer/actions?query=workflow%3Abuild">
      <img src="https://github.com/wasmerio/wasmer/workflows/build/badge.svg?style=flat-square" alt="Build Status">
    </a>
    <a href="https://github.com/wasmerio/wasmer/blob/master/LICENSE">
      <img src="https://img.shields.io/github/license/wasmerio/wasmer.svg" alt="License">
    </a>
    <a href="https://docs.wasmer.io">
      <img src="https://img.shields.io/static/v1?label=Docs&message=docs.wasmer.io&color=blue" alt="Wasmer Docs">
    </a>
    <a href="https://slack.wasmer.io">
      <img src="https://img.shields.io/static/v1?label=Slack&message=join%20us!&color=brighgreen" alt="Slack channel">
    </a>
  </p>
</div>

<br />

Wasmer is a _fast_ and _secure_ [**WebAssembly**](https://webassembly.org) runtime that enables super
_lightweight containers_ to run anywhere: from *Desktop* to the *Cloud*, *Edge* and *IoT* devices.

> _This document is also available in:
[üá®üá≥ ‰∏≠ Êñá -Chinese](https://github.com/wasmerio/wasmer/blob/master/docs/cn/README.md) ‚Ä¢ 
[üá©üá™ Deutsch-German](https://github.com/wasmerio/wasmer/blob/master/docs/de/README.md) ‚Ä¢ 
[üá™üá∏ Espa√±ol-Spanish](https://github.com/wasmerio/wasmer/blob/master/docs/es/README.md) ‚Ä¢ 
[üá´üá∑ Fran√ßais-French](https://github.com/wasmerio/wasmer/blob/master/docs/fr/README.md) ‚Ä¢ 
[üáØüáµ Êó•Êú¨ Ë™û -Japanese](https://github.com/wasmerio/wasmer/blob/master/docs/ja/README.md) ‚Ä¢ 
[üá∞üá∑ ÌïúÍµ≠Ïù∏ -Korean](https://github.com/wasmerio/wasmer/blob/master/docs/ko/README.md)_.

### Features

* Secure by default. No file, network, or environment access, unless explicitly enabled.
* Supports [WASI](https://github.com/WebAssembly/WASI) and [Emscripten](https://emscripten.org/) out of the box.
* Fast. Run WebAssembly at near-native speeds.
* Embeddable in [multiple programming languages](https://github.com/wasmerio/wasmer/#-language-integrations)
* Compliant with latest WebAssembly Proposals (SIMD, Reference Types, Threads, ...)

### Install

Wasmer CLI ships as a single executable with no dependencies.

```sh
curl https://get.wasmer.io -sSfL | sh
```

<details>
  <summary>Other installation options (Powershell, Brew, Cargo, ...)</summary>
  
  _Wasmer can be installed from various package managers. Choose the one that fits best for your environment:_
  
  * Powershell (Windows)
    ```powershell
    iwr https://win.wasmer.io -useb | iex
    ```

  * <a href="https://formulae.brew.sh/formula/wasmer">Homebrew</a> (macOS, Linux)

    ```sh
    brew install wasmer
    ```

  * <a href="https://github.com/ScoopInstaller/Main/blob/master/bucket/wasmer.json">Scoop</a> (Windows)

    ```sh
    scoop install wasmer
    ```

  * <a href="https://chocolatey.org/packages/wasmer">Chocolatey</a> (Windows)

    ```sh
    choco install wasmer
    ```
  
  * <a href="https://crates.io/crates/wasmer-cli/">Cargo</a>

    _Note: All the available
    features are described in the [`wasmer-cli`
    crate docs](https://github.com/wasmerio/wasmer/tree/master/lib/cli/README.md)_

    ```sh
    cargo install wasmer-cli
    ```

  > Looking for more installation options? See [the `wasmer-install`
  repository](https://github.com/wasmerio/wasmer-install) to learn
  more!
</details>

### Quickstart

You can start by running
[QuickJS](https://github.com/bellard/quickjs/), a small and
embeddable Javascript engine compiled as a WebAssembly module ([`qjs.wasm`](https://registry-cdn.wapm.io/contents/_/quickjs/0.0.3/build/qjs.wasm)):

```bash
$ wasmer qjs.wasm
QuickJS - Type "\h" for help
qjs > const i = 1 + 2;
qjs > console.log("hello " + i);
hello 3
```

#### Here is what you can do next:

- [Use Wasmer from your Rust application](https://docs.wasmer.io/integrations/rust)
- [Publish a Wasm package on WAPM](https://docs.wasmer.io/ecosystem/wapm/publishing-your-package)
- [Read more about Wasmer](https://medium.com/wasmer/)

## üì¶ Language Integrations

The Wasmer runtime can be used as a library **embedded in different
languages**, so you can use WebAssembly _anywhere_.

| | Language | Package | Documentation |
|-|-|-|-|
| ![Rust logo] | [**Rust**][Rust integration] | [`wasmer` Rust crate] | [Learn][rust docs]
| ![C logo] | [**C/C++**][C integration] | [`wasmer.h` header] | [Learn][c docs] |
| ![C# logo] | [**C#**][C# integration] | [`WasmerSharp` NuGet package] | [Learn][c# docs] |
| ![D logo] | [**D**][D integration] | [`wasmer` Dub package] | [Learn][d docs] |
| ![Python logo] | [**Python**][Python integration] | [`wasmer` PyPI package] | [Learn][python docs] |
| ![JS logo] | [**Javascript**][JS integration] | [`@wasmerio` NPM packages] | [Learn][js docs] |
| ![Go logo] | [**Go**][Go integration] | [`wasmer` Go package] | [Learn][go docs] |
| ![PHP logo] | [**PHP**][PHP integration] | [`wasm` PECL package] | [Learn][php docs] |
| ![Ruby logo] | [**Ruby**][Ruby integration] | [`wasmer` Ruby Gem] | [Learn][ruby docs] |
| ![Java logo] | [**Java**][Java integration] | [`wasmer/wasmer-jni` Bintray package] | [Learn][java docs] |
| ![Elixir logo] | [**Elixir**][Elixir integration] | [`wasmex` hex package] | [Learn][elixir docs] |
| ![R logo] | [**R**][R integration] | *no published package* | [Learn][r docs] |
| ![Postgres logo] | [**Postgres**][Postgres integration] | *no published package* | [Learn][postgres docs] |
| ![Swift logo] | [**Swift**][Swift integration] | *no published package* | |
| ![Zig logo] | [**Zig**][Zig integration] | *no published package* | |
| ![Dart logo] | [**Dart**][Dart integration] | [`wasm` pub package] | |
| ![Crystal logo] | [**Crystal**][Crystal integration] | *no published package* | [Learn][crystal docs] |
| ![Lisp logo] | [**Lisp**][Lisp integration] | *under heavy development - no published package* | |

[üëã&nbsp;&nbsp;Missing a language?](https://github.com/wasmerio/wasmer/issues/new?assignees=&labels=%F0%9F%8E%89+enhancement&template=---feature-request.md&title=)

[rust logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/rust.svg
[rust integration]: https://github.com/wasmerio/wasmer/tree/master/lib/api
[`wasmer` rust crate]: https://crates.io/crates/wasmer/
[rust docs]: https://docs.rs/wasmer/

[c logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/c.svg
[c integration]: https://github.com/wasmerio/wasmer/tree/master/lib/c-api
[`wasmer.h` header]: https://github.com/wasmerio/wasmer/blob/master/lib/c-api/wasmer.h
[c docs]: https://docs.rs/wasmer-c-api/*/wasmer_c_api/wasm_c_api/index.html

[c# logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/csharp.svg
[c# integration]: https://github.com/migueldeicaza/WasmerSharp
[`wasmersharp` nuget package]: https://www.nuget.org/packages/WasmerSharp/
[c# docs]: https://migueldeicaza.github.io/WasmerSharp/

[d logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/d.svg
[d integration]: https://github.com/chances/wasmer-d
[`wasmer` Dub package]: https://code.dlang.org/packages/wasmer
[d docs]: https://chances.github.io/wasmer-d

[python logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/python.svg
[python integration]: https://github.com/wasmerio/wasmer-python
[`wasmer` pypi package]: https://pypi.org/project/wasmer/
[python docs]: https://wasmerio.github.io/wasmer-python/api/wasmer/

[go logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/go.svg
[go integration]: https://github.com/wasmerio/wasmer-go
[`wasmer` go package]: https://pkg.go.dev/github.com/wasmerio/wasmer-go/wasmer
[go docs]: https://pkg.go.dev/github.com/wasmerio/wasmer-go/wasmer?tab=doc

[php logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/php.svg
[php integration]: https://github.com/wasmerio/wasmer-php
[`wasm` pecl package]: https://pecl.php.net/package/wasm
[php docs]: https://wasmerio.github.io/wasmer-php/

[js logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/js.svg
[js integration]: https://github.com/wasmerio/wasmer-js
[`@wasmerio` npm packages]: https://www.npmjs.com/org/wasmer
[js docs]: https://docs.wasmer.io/integrations/js/reference-api

[ruby logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/ruby.svg
[ruby integration]: https://github.com/wasmerio/wasmer-ruby
[`wasmer` ruby gem]: https://rubygems.org/gems/wasmer
[ruby docs]: https://wasmerio.github.io/wasmer-ruby/wasmer_ruby/index.html

[java logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/java.svg
[java integration]: https://github.com/wasmerio/wasmer-java
[`wasmer/wasmer-jni` bintray package]: https://bintray.com/wasmer/wasmer-jni/wasmer-jni
[java docs]: https://github.com/wasmerio/wasmer-java/#api-of-the-wasmer-library

[elixir logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/elixir.svg
[elixir integration]: https://github.com/tessi/wasmex
[elixir docs]: https://hexdocs.pm/wasmex/api-reference.html
[`wasmex` hex package]: https://hex.pm/packages/wasmex

[r logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/r.svg
[r integration]: https://github.com/dirkschumacher/wasmr
[r docs]: https://github.com/dirkschumacher/wasmr#example

[postgres logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/postgres.svg
[postgres integration]: https://github.com/wasmerio/wasmer-postgres
[postgres docs]: https://github.com/wasmerio/wasmer-postgres#usage--documentation

[swift logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/swift.svg
[swift integration]: https://github.com/AlwaysRightInstitute/SwiftyWasmer

[zig logo]: https://raw.githubusercontent.com/ziglang/logo/master/zig-favicon.png
[zig integration]: https://github.com/zigwasm/wasmer-zig

[dart logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/dart.svg
[dart integration]: https://github.com/dart-lang/wasm
[`wasm` pub package]: https://pub.dev/packages/wasm

[lisp logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/lisp.svg
[lisp integration]: https://github.com/helmutkian/cl-wasm-runtime

[crystal logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/crystal.svg
[crystal integration]: https://github.com/naqvis/wasmer-crystal
[crystal docs]: https://naqvis.github.io/wasmer-crystal/

## Contribute

We appreciate your help! üíú

Check our docs on how to [build Wasmer from
source](https://docs.wasmer.io/ecosystem/wasmer/building-from-source) or [test your changes](https://docs.wasmer.io/ecosystem/wasmer/building-from-source/testing).

## Community

Wasmer has an amazing community of developers and contributors. Welcome, please join us! üëã

- [Wasmer Community Slack](https://slack.wasmer.io/)
- [Wasmer on Twitter](https://twitter.com/wasmerio)
- [Wasmer on Facebook](https://www.facebook.com/wasmerio)
- [Email](mailto:hello@wasmer.io)

'''
'''--- SECURITY.md ---
# Security Policy

## Supported Versions

The table below summarizes which versions are still supported, and which aren't.

| Version | Supported |
|-|-|
| 0.x | ‚ùå |
| 1.x | ‚úÖ |

## Reporting a Vulnerability

The Wasmer team and community take security bugs in Wasmer seriously.
We appreciate your efforts to responsibly disclose your findings, and will make every effort to acknowledge your contributions.

To report a security issue, email security@wasmer.io or
hello@wasmer.io and include the word "SECURITY" in the subject line.

The Wasmer team will send a response indicating the next steps in handling your report.
After the initial reply to your report, the security team will keep you informed of the progress towards a fix and full announcement, and may ask for additional information or guidance.

'''
'''--- Xargo.toml ---
[dependencies]
std = {}

'''
'''--- assets/README.md ---
# Wasmer Documentation Assets

This directory contains documentation assets.

'''
'''--- assets/diagrams/Diagram_module_compilation.svg ---
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="1004" height="352" content="&lt;mxfile host=&quot;app.diagrams.net&quot; modified=&quot;2020-07-20T09:19:59.698Z&quot; agent=&quot;5.0 (Macintosh)&quot; version=&quot;13.4.8&quot; etag=&quot;7basM_j3VExUdDWLz-Ua&quot; type=&quot;device&quot;&gt;&lt;diagram id=&quot;lbpiYurgNz5VTBNmSIvc&quot;&gt;7Vptb9sgEP41+bjI4NhOPi5psk7atEmZ1PZTxWJqs2LwMHnbrx/Y4JfE2dIubbwqaqVwB5w57rnnwEnPnSSbDwKl8WceYtqDTrjpuVc9CGEQOOpDa7aFBoAAFppIkNDoKsWc/MJGaSZGSxLirDFQck4lSZvKBWcML2RDh4Tg6+awB06bT01RhPcU8wWi+9obEsq40A5hUOmvMYli+2Tgj4qeBNnBxpMsRiFfN1R4I2ecSbPELylmSjlHLOt501hK7eL7Hpyp/wc9rB9xHlGMUpL1FzxR6kWmhsweUEKo3uDCwji3EEy+IinI4lGZvEYsfKbJ0sg4N6KW7U577kRwLotWsplgqoNuw1n4NjvQW+6nwEweM8HAZYXo0oSkB32qpo61A3pf5dYEy/+55LbjXZZDSfnqgGG6qTo1CNCiOWFnp2pj/Uh/rlGWYKGtue8VxpcUF22G13YxZkPtDBPycmlQ8CULsfbJ0QZjIvE8LdaxVqmjdLFMqJKAamaPWC5iK0jBH/GEUy5yU+7VyBl4sOyxsMw1FmP6IQ+E0rlZAVpKblQ1SzNH/UGzYzMb8o8sJFitfkb5Wns9nn3bptpIVDq2wkLizcGIghInihgwT7AUWzXETHgHHOD2vWKa4YWBY1JiXSUZsOQR1xLMNzpkkiYqzVcYUg0Do3ZIuXvRwaFKeiNyIWMecYbotNKOm/GrxnziPDWB+oGl3BoGM/tdiyneEHlba99pU2oXCulqYyznwtYKTHl2Wxdqs7RYTcslO+9E+HkiKPQmNiCR8aUwmQYHhruRiLBsZPYRwBGYIklWTev/goBBB0jlRpGK6vy+laq+HckiygtV+PBrMshf6WLHzxegCzgMGmThjlrIAraQxfAEZOF1ACqH6o+q2ClRQrdrUBcKji0bBkEebEHQCNiidOqC4/9/BacflEXmzhr5Q8UJURbniz0hdE5ZfoJOlZ+gM5xyj1lEmKGTaa19oZbjqGXogp2jbBu3QM99GWYZtiDp9ammgwnv7Sd8cHRwT57wo0uY2sNkX/PU4+SdL052OR1g5v3TnloUCZG8cPLfLwzB8AhSBtWoU9MyAHvReMUTn9OH9ph3V+tqP/J1/eQG9wnCZulZGOLw+8iQrJ5DEDZ7z3QANAe9Bt9M2YoIzpJ8d7VaCsQy+ifuKdX5LlwYafcCOmzykT/Y5yPXC16Ijc76wvNJbFS7gDbun9WN883cPy2TnIXFBudFRB0QzrGAeFtvJEC3bijg8HvOt1DXJg3J9N0nxen6UtOeU9P8Y0qa/0Jf4QH/cqM+wCttB+bXu1IrsfptQN5X+2WIO/0N&lt;/diagram&gt;&lt;/mxfile&gt;" version="1.1" viewBox="-.5 -.5 1004 352"><defs/><defs><style type="text/css">@import url(https://fonts.googleapis.com/css?family=Open+Sans);@import url(https://fonts.googleapis.com/css?family=Patrick+Hand);</style></defs><path fill="none" stroke="none" d="M294.5 31h170v60h-170z" pointer-events="all"/><path fill="none" stroke="#f0f0f2" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M294.52 30.97s0 0 0 0m0 0s0 0 0 0m.72 10.93c2.07-2.93 6.6-9.03 9.2-11.05m-9.64 12.33c2.69-3.38 5.99-7.05 11-11.44m-11.17 25.24c7.09-9.4 10.8-16.49 23-24.5m-22.39 22.35c6.41-8.52 12.73-16.14 20-23.05m-22.27 34.33c9.83-8.7 18.57-17.87 31.47-33.64m-30.09 35.9c8.42-10.71 16.88-19.26 31.34-36.55m-30.7 47.97c10.64-13.44 23.26-28.26 41.33-48.7M293.8 80.53c12.16-14.44 23.75-28.06 42.29-50.65m-40.27 63.17c12.81-16.67 23.03-31.15 51.88-61.63M294.95 91.4c12.38-13.8 25.41-28.45 52.37-60.28M305.1 90.63c16.27-16.95 29.74-32.93 54.14-59.73m-53.99 60.17c19.21-21.25 38.39-44.1 53.9-59.32m-40.99 59.27c15.79-19.88 36.92-44.34 49.13-60.15m-50.56 60.96c17.76-20.37 35.78-40.69 51.29-62.07M326.37 90.8c21.26-22.39 37.67-41.71 52.93-58.63m-52.57 59.52c10.98-14.57 24.64-26.94 53.13-61.05M338.05 92.3c8.68-14.35 21.88-28.63 53.24-63.64M336.1 92.13c13.38-15.59 25.63-29.58 53.71-62.04M346.53 90.1c13.94-15.49 30.01-31.04 52.73-59.28m-51.58 61.04c11.5-12.93 22.08-27.11 53.35-62.12m-43.36 61.44c19.81-19.1 35.58-37.95 54.32-59.14m-52.9 58.99c18.89-21.75 37.38-42.35 50.76-60.41m-39.92 62.77c13.92-18.73 26.52-35.38 51.16-60.22m-51.91 57.75c18.07-19.48 37.03-41.85 51.94-59.03m-40 57.96c15.97-18.53 33.16-40.29 49.37-57.22m-50.37 59.81c14.59-18.24 30.13-35.19 52-60.41m-42.27 58.42c12.2-15.36 29.12-33.96 52.69-59.51m-51.35 59.92c17.95-22.32 38.59-44.32 50.71-59.25m-42.93 61.82c14.59-15.68 27.63-31.65 55.78-60.41m-53.18 58.44c14.54-16.63 29.96-34.98 50.88-60.77m-39.43 60.26c13.54-17.28 28.81-34.9 50.23-59.18m-51.13 60.58c13.91-16.33 27-31.93 51.36-61.75m-41.84 62.06c14.83-19.61 29.53-36.69 50-57.74m-49.49 57.17c11.38-14.58 24.61-28.46 47.42-56.18m-36.54 55.32c8.3-10.27 18.82-19.04 37.25-43.68m-37.05 42.94c12.93-14.39 28.65-31.82 37.64-41.4M441.4 89.96c7.99-6.19 16.64-12.25 28.28-28.9m-25.77 31.21c9.67-12.03 18.52-21.1 26.9-31.55m-18.47 31.57c5.99-6.51 10.56-11.88 18.45-20.38m-17.2 19.89c5.05-5.16 9.62-9.58 16.12-19.5" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M294.69 31.94c46.87.22 96.52-1.44 169.1-1.92m-168.97.12c67.24.39 132.72-.28 169.8 1.04m-1.09 1.33c2.06 22.08-.4 46.45.67 59.5m1.3-60.23c-.38 23.33-1.16 46.03-1.99 58.99m1.9.09c-59.57-.02-114.34 3.32-169.75 1.22m168.29-1.63c-51.37.88-103.16-.52-169.82.94m1.71-.31c-2.74-18.98-1.78-35.46-2.49-59.09m1.44 58.82c-.31-15.53-.59-30.46.6-59.85" pointer-events="all"/><switch transform="translate(-.5 -.5)"><foreignObject width="100%" height="100%" pointer-events="none" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow:visible;text-align:left"><div xmlns="http://www.w3.org/1999/xhtml" style="display:flex;align-items:unsafe center;justify-content:unsafe center;width:168px;height:1px;padding-top:61px;margin-left:296px"><div style="box-sizing:border-box;font-size:0;text-align:center"><div style="display:inline-block;font-size:12px;font-family:Indie Flower;color:#000;line-height:1.2;pointer-events:all;white-space:normal;word-wrap:normal"><font face="Patrick Hand" style="font-size:18px">wasmer::Module::new</font></div></div></div></foreignObject><text x="380" y="65" fill="#000" font-family="Indie Flower" font-size="12" text-anchor="middle">wasmer::Module::new</text></switch><path fill="none" stroke="none" d="M141 61h145.26" pointer-events="stroke"/><path fill="none" stroke="none" d="M292.26 61l-8 4 2-4-2-4z" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M140.14 59.89c57.25.5 117.15.29 145.73 2.34M140 60.26c36.81 1.09 73.28 1.73 146.77.56" pointer-events="stroke"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M284.51 56.72s0 0 0 0m0 0s0 0 0 0m2.58 9.25c2-1.51 3.09-4.2 4.2-5.25m-4.93 5.76c1.22-1.81 2.82-3.15 5.7-6.18" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M292.67999999999995 60.63c-1.8 1.02-4 1.78-7.86 4.96m7.28-4.31c-1.1.54-3.23 1.62-8.26 3.77m.4.02c1-1.25 1.74-2.67 1.97-4.18m-2.16 4.32c.85-1.26 1.12-2.19 2.39-4.07m.25-.11c-1.24-1.14-2.13-2.81-2.81-4.04m2.2 4c-.22-.8-.6-1.57-1.72-4.15m-.86-.23c2.66 1.83 5.49 2.24 9.52 3.88M284 57.18c2.98 1.13 6.08 2.4 8.19 4.18" pointer-events="all"/><ellipse cx="81" cy="61" fill="transparent" stroke="none" pointer-events="all" rx="60" ry="40"/><path fill="none" stroke="#f0f0f2" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M29.56 39.14s0 0 0 0m0 0s0 0 0 0m-7.14 20.71C29.51 53 36.47 39.8 52.16 26.32m-31.34 35.8c8.91-9.65 17.39-20.63 31.38-35.96M24.13 68.91c10.67-13.68 23.25-25.32 41.7-45.3M24.46 68.5c16.17-16.85 31.75-36 41.71-46.21M28.45 78.61C39.9 62.64 51.97 50.35 75.24 22.08M28.55 76.16C42.3 60.61 56.61 45.92 77.3 22.59M33.77 84.75c14.7-17.42 31.25-36.25 51.91-61.19M31.92 84.47C47.83 67.1 61.12 50 86.87 22.55m-49.01 66c11.9-12.32 26.74-26.79 61.29-68.83M39.23 89.69c17.71-21.3 36.28-42.02 59.73-67.84M46.01 94.61c13.92-16.13 24.12-30.01 59.73-68.53M46.62 94.1c12.22-15.28 24.23-31.14 59.71-69.6M53.71 96.04c18.7-23.73 39.6-46.36 59.81-68.98M52.67 97.21c13.65-15.66 28.37-32.9 61.01-68.28M60.7 102.18c17.99-23.31 43.7-48.14 61.41-68.91m-62.13 69.3c21-26.04 42.2-50.44 59.64-71.12m-49.28 71.57c13.96-15.36 26.72-32.03 55.71-68.36m-57.21 69.58c14.88-18.18 31.09-37.79 58.6-67.82m-48.33 70.26c11.11-18.66 24.7-32.24 54.36-63.92m-55.23 62.66c18.62-23.69 37.17-44.77 54-62.79M87.9 103.28c11.18-10.62 21.14-21.27 47.6-53.26m-46.05 54.4c18.97-20.44 36.03-41.51 48.23-54.72m-32.91 50.22c12.1-16.18 25.66-28.09 38.93-46.34m-39.42 45.1c13.39-14.28 25.9-27.5 37.55-42.69m-21.37 39.17c3.89-9.06 9.29-14.33 20.41-24.45m-21.91 24.66c5.92-7.35 11.59-14.8 22.46-26.28" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M64.58 20.63c9.47-2.48 23.16-2.26 33.03-.18 9.87 2.09 19.33 7.48 26.19 12.69 6.85 5.21 12.85 11.6 14.95 18.59 2.11 6.98 1.13 16.37-2.32 23.32-3.46 6.95-10.4 13.63-18.42 18.39-8.03 4.76-19.32 9.38-29.72 10.19-10.41.81-23.28-2.01-32.71-5.34-9.44-3.32-18.08-8.43-23.9-14.59s-10.62-15-11.01-22.39c-.38-7.38 3.47-15.86 8.68-21.92 5.21-6.06 14.81-11.01 22.58-14.44 7.77-3.44 17.8-5.26 24.04-6.16 6.23-.89 13.07.09 13.36.78m-32.07 3.27c8.48-2.97 21.38-3.43 31.83-2.67 10.46.77 22.63 2.82 30.89 7.26 8.27 4.43 15.62 12.1 18.7 19.37 3.08 7.28 2.13 17.22-.2 24.28-2.32 7.05-6.55 12.83-13.74 18.04-7.2 5.22-19.16 11.36-29.42 13.23-10.27 1.87-22.47.34-32.19-2.03-9.72-2.36-19.22-6.5-26.13-12.16-6.9-5.66-13.45-14.45-15.32-21.79-1.86-7.34.16-15.57 4.16-22.25 4.01-6.69 14.76-14.36 19.88-17.84 5.12-3.48 8.94-2.64 10.84-3.03 1.9-.4-.22.16.55.67" pointer-events="all"/><switch transform="translate(-.5 -.5)"><foreignObject width="100%" height="100%" pointer-events="none" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow:visible;text-align:left"><div xmlns="http://www.w3.org/1999/xhtml" style="display:flex;align-items:unsafe center;justify-content:unsafe center;width:118px;height:1px;padding-top:61px;margin-left:22px"><div style="box-sizing:border-box;font-size:0;text-align:center"><div style="display:inline-block;font-size:12px;font-family:Patrick Hand;color:#000;line-height:1.2;pointer-events:all;white-space:normal;word-wrap:normal"><font face="Patrick Hand" style="font-size:18px">Wasm bytes</font></div></div></div></foreignObject><text x="81" y="65" fill="#000" font-family="Patrick Hand" font-size="12" text-anchor="middle">Wasm bytes</text></switch><path fill="none" stroke="none" d="M248 151h191.5v60H248z" pointer-events="all"/><path fill="none" stroke="#f0f0f2" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M248.07 150.92s0 0 0 0m0 0s0 0 0 0m-.22 13.36c4.67-4.78 8.42-10.69 12.21-11.89m-12.39 9.92c4.67-4.36 8.47-9.04 10.39-11.75m-8.6 24.29c4.42-8.24 10.06-13.52 18.91-22.44m-20.61 23.25c4.17-6.82 9.44-11.68 20.86-25.48m-19.43 36.57c8.74-11.09 19.77-23.18 31.45-34.04m-33.44 35.41c8.24-8.6 15.02-15.14 33.05-38.21m-31.9 51.4c12.42-14.35 24.95-30.54 44.03-51.99m-44.98 50.26c15.67-16.53 30.48-33.61 42.95-49.71m-42.94 61.4c15.84-17.11 28.02-32.19 52.39-61.55m-51.83 61.08c10.95-12.21 22.88-25.42 52.61-60.65m-43.15 60.27c23.6-22.69 39.42-43.18 55.17-59.54m-53.9 60.28c15.93-19.26 30.84-35.79 52.83-60.18m-42.6 59.89c18-17.49 32.47-33.79 53.38-62m-51.56 61.65c11.03-13.13 22.81-26.7 50.79-59.64m-41.11 61.41c14.13-16.67 28.26-32.55 54.26-59.82m-54.97 58.51c21.26-23.62 41.27-46.58 53.75-60.15m-41.8 60.98c15.06-17.33 28.44-34.02 49.75-62.23m-50.2 61.77c19.33-23.73 41.13-46.14 51.7-60.09m-42.95 58.89c17.44-17.67 33.39-33 55.14-60.35m-53.36 62.09c12.08-13.47 22.63-28.74 51.84-60.7m-42.29 59.09c16.7-15.53 29.89-33.95 54.45-59.71m-53.6 60.87c16.06-17.93 31.33-34.61 52.1-60.5m-40.3 60.79c12-15.46 26.73-31.86 51.75-59.88m-53.96 60.56c15.09-17.83 28.97-33.6 52.33-61.12m-41.12 61c17.05-20.06 36.09-44.43 54.46-61.9m-54.28 60.52c12.7-13.54 25.63-28.45 51.75-60.42m-42.48 63.07c15.12-19.82 30.9-34.44 53.4-64.11M343.05 211c12.32-13.39 25.42-28.03 52.84-60.65m-42.03 60.3c13.31-16.96 27.75-30.96 54.75-58.28m-53.45 58.46c17.08-18.9 33.68-39.78 51.43-60.33m-39.84 59.25c15.71-14.2 27.93-34.57 52.06-57.53m-53.96 58.63c18.53-19.64 35.82-39.51 52.63-60.31m-42.79 59.22c14.94-13.33 23.67-24.97 54.27-60.33m-52.9 62.58c10.49-12.96 21.06-26.47 52.89-61.76m-43.05 61.69c18.87-24.62 38.14-47.37 53.69-62.57m-53.51 62.52c14.51-16.97 27.38-31.49 51.97-60.13m-42.5 57.78c19.5-19.77 38.12-40.29 50.07-52.97m-49.23 54.4c17.74-19.02 33.06-38.79 48.98-54.7m-39.21 53.11c12.07-10.2 21.58-19.33 39.19-42.38m-37.9 43.74c12.58-13.29 25.98-28.7 37.03-41.76m-24.51 40.53c5.77-5.4 10.9-16.6 23.34-27.84m-25.78 30.28c9.07-9.74 17.24-20.25 27.67-30.81m-17.9 31.69c5.31-7.57 10.29-13.82 19.69-21.96m-17.51 19.87c5.25-7.61 11.16-15.15 15.26-17.77" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M248.76 152.83c53.36-2.4 108.32-.74 189.65-3.63m-191.04 2.74c48.93-2.68 99.77-3.06 191.51-1.5m2.26-1.18c-2.28 18.17-2.67 33.08.23 62.14m-.98-61.02c-.58 23.46-.29 47.13-1.66 60.44m-1.16 1.4c-71.92.19-147.12 1.89-188.9.65m190.45-1.85c-43.53 2.59-89.4 2.38-191.67-.5m1.73 1.71c-.31-22.34-.54-41.32-2.56-61.86m1.32 60.43c.2-19.96-.55-41.11-.72-59.87" pointer-events="all"/><switch transform="translate(-.5 -.5)"><foreignObject width="100%" height="100%" pointer-events="none" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow:visible;text-align:left"><div xmlns="http://www.w3.org/1999/xhtml" style="display:flex;align-items:unsafe center;justify-content:unsafe center;width:190px;height:1px;padding-top:181px;margin-left:249px"><div style="box-sizing:border-box;font-size:0;text-align:center"><div style="display:inline-block;font-size:12px;font-family:Indie Flower;color:#000;line-height:1.2;pointer-events:all;white-space:normal;word-wrap:normal"><font face="Patrick Hand" style="font-size:18px">wasmer::Module::compile</font></div></div></div></foreignObject><text x="344" y="185" fill="#000" font-family="Indie Flower" font-size="12" text-anchor="middle">wasmer::Module::compile</text></switch><path fill="none" stroke="none" d="M729.5 181h20v-60H422V99.24" pointer-events="stroke"/><path fill="none" stroke="none" d="M422 93.24l4 8-4-2-4 2z" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-dasharray="6 6" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M728.23 181.92c8.65-.92 15.57-1.87 21.4-.58m-20.3-.84c7.95-.42 14.8.24 21.05.71m-1.74-1.54c.48-13.42 1.37-27.97.73-59.05m.91 60.07c-1.64-20.19-1.65-40.26-1.08-60.29m-.89.46c-84.33-2.77-168.47-1.68-326.49 1.1m327.32-1.57c-89.8-.75-180.73-.85-327.62.06m1.06-1.06c.3-1.7-2.13-5.85-.96-18.33m1.13 19.85c-.4-4.98-.45-9.94-.93-22.13" pointer-events="stroke"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M422.24 92.97s0 0 0 0m0 0s0 0 0 0m2.69 9.06c.27-.4.53-.67.73-.87m-.67.88c.24-.29.35-.53.62-.84" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M421.21999999999997 93.15c1.44 1.35 2.95 4.13 4.67 8.72m-4.13-9.03c1.43 2.31 2.09 4.45 3.93 8.04m.44 0c-.77.21-2.16-.11-4.22-1.23m4.26 1.57c-1.06-.42-2.04-.83-4.21-2.08m.28-.07c-1.5.59-3.38 2.12-4.15 1.74m3.86-1.78c-.97.78-1.99 1.21-3.96 2.3m.56-.68c1.01-1.75 2.08-3.66 2.61-7.64m-3.32 8.39c.53-1.71 1.93-3.71 3.92-7.75" pointer-events="all"/><path fill="none" stroke="none" d="M476.5 151h253v60h-253z" pointer-events="all"/><path fill="none" stroke="#f0f0f2" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M476.63 150.85s0 0 0 0m0 0s0 0 0 0m1.6 12.04c1.72-4.38 5.57-7.29 8.95-11.11m-10.65 10.73c2.33-2.68 4.73-4.74 10.73-11.32m-12.59 23.99c7.47-6.03 16.36-14.64 21.32-24.17m-19.8 24.62c7.91-9.64 15.64-18.14 22.27-25.36m-21.16 39.3c6.73-12.29 17.72-24.23 32.79-36.82m-33.89 35.9c11.55-12.61 21.38-25.4 32.28-38.09m-32.97 49.87c15.21-19.53 30.55-34.12 44.93-50.42m-44.47 49.2c16.51-18.37 32.4-35.28 43.17-48.73m-42.5 58.82c19.86-23.35 42.35-46.68 52.57-60.2m-52.19 62.11c18.27-20.42 35.51-42.54 53.35-60.1m-42.78 62.61c16.11-22.87 36.1-45.09 50.84-61.57m-51.86 60.4c15.68-16.75 28.69-33.38 53.8-61.88m-41.8 59.52c14.74-14.03 28.33-28.95 53.47-59.95m-53.23 61.88c11.22-15.73 25.08-29.74 51.28-61.5m-43.38 59.2c15.52-11.49 25.99-24.62 52.5-59.21m-50.23 61.83c16.02-19.88 31.46-38.59 51.08-61.51m-42.48 62c14.67-14.99 24.66-29.84 55.48-63.34m-54.76 62.79c18.5-19.83 35.21-39.05 53.25-60.36m-40.39 58.1c18.69-19.12 36.83-43.8 50.83-58.3m-51.5 59.36c10.18-12.69 23.43-27.04 50.89-59.14m-42.83 60.55c18.77-21.02 38.59-43.32 53.16-61.22m-51.83 59.5c11.62-12.68 21.65-24.69 52.54-59.88m-39.75 61.47c19.29-24.1 39.05-49.8 49.28-62.44m-51.32 61.5c16.94-17.94 33.29-36.19 53.14-60.47m-42.83 62c12.67-15.89 22.67-27.73 52.16-63.46m-52.26 62.09c17.04-18.67 33.28-36.92 53-59.75m-41.88 61.29c11.27-12.65 20.49-24.59 50.97-62.99m-51.21 61.43c14.2-15.14 27.61-30.68 52.88-60.44m-40.4 58.46c17.32-18.41 35.72-42.44 53.07-58.91m-53.53 61.13c17.26-21.63 34.92-40.68 50.94-60.18m-41.97 61.58c13.46-16.16 22.5-28.09 53.99-62.31m-52.89 61.32c19.27-22.44 38.63-44.4 51.72-60.5m-39.78 59.6c17.92-24.44 38.67-47.11 51.5-59.44m-53.85 58.62c13.03-12.65 22.32-23.59 53.19-59.85m-41.03 61.39c13.2-19.22 29.81-35.83 51.48-61.75m-51.3 61.26c17.18-20.68 36.87-42.11 51.71-60.59m-40.47 59.63c12.73-16.53 28.96-33.77 50.05-59.89m-51.1 61.03c18.06-22.54 37.11-42.62 51.06-59.97m-40.84 57.84c20.84-20.55 37.86-42.08 51.52-57.68m-51.55 58.33c15.41-17.28 31.1-33.69 53.42-59.16m-42.32 58.31c19.29-19.7 40.11-40.71 50.36-58.14m-49.91 60.25c15.48-20 32.5-39.69 52.38-60.22m-41.01 61.09c9.12-17.24 24.33-30.02 49.24-61.46m-49.97 59.53c14.74-18.53 29.4-36.34 51.92-59.14m-40.41 57.94c7.64-10.36 21.16-22.88 49.01-59.03m-50.57 60.66c14.62-16.63 28.82-35.26 51.46-60.81m-39.91 61.34c14.57-17.29 27.16-34.33 50.08-58.69m-50.45 58.09c12.7-14.79 24.78-29.44 50.13-59.25m-41.52 57.64c11.15-5.84 15.94-17.42 42.47-45.62m-40.94 47.25c11.99-13.2 24.4-28.51 41.97-47.59m-32.57 49.02c13.31-12.67 21.78-27.48 30.8-35.34m-30.37 34.51c8.85-8.85 15.9-17.67 29.77-36.01m-18 36.39c7.38-11.5 12.8-18.72 18-25.49m-18.39 25.04c4.27-6.29 9.04-11.01 19.89-22.52m-11.03 21.4c4.49-.92 4.49-2.22 9.85-9.89m-8.46 11.34c3.37-4.72 6.41-7.84 9.76-11.49" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M476.19 151.65c69.69-.76 141.43-1.76 253.41-.17m-253.32-.31c71.11-1.04 140.98-.19 253.06-.82m1.33-.84c.41 18.2-3.29 31.29.09 63.02m-1.07-61.95c-1.05 15.51.26 32.16.45 59.6m.27.01c-52.62 3.86-103.59 1.56-253.5.37m252.33 1.15c-52.91-.51-106.07-.9-253.24-.26m-1.04-1.5c1.84-12.79-.13-27.28 2.75-58.79m-2.05 59.64c1.44-19.44 1.19-36.66 1.5-59.89" pointer-events="all"/><switch transform="translate(-.5 -.5)"><foreignObject width="100%" height="100%" pointer-events="none" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow:visible;text-align:left"><div xmlns="http://www.w3.org/1999/xhtml" style="display:flex;align-items:unsafe center;justify-content:unsafe center;width:251px;height:1px;padding-top:181px;margin-left:478px"><div style="box-sizing:border-box;font-size:0;text-align:center"><div style="display:inline-block;font-size:12px;font-family:Indie Flower;color:#000;line-height:1.2;pointer-events:all;white-space:normal;word-wrap:normal"><font face="Patrick Hand" style="font-size:18px">wasmer_engine::Engine::compile</font></div></div></div></foreignObject><text x="603" y="185" fill="#000" font-family="Indie Flower" font-size="12" text-anchor="middle">wasmer_engine::Engine::compile</text></switch><path fill="none" stroke="none" d="M439.5 181h20-3 11.76" pointer-events="stroke"/><path fill="none" stroke="none" d="M474.26 181l-8 4 2-4-2-4z" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M437.83 182.1c9.82-2.56 17.88-2.13 23.66-.47m-21.36-.19c5.4.48 9.27.6 19.38-.92m-.06.26c-.56.42-1.38.34-2.8-.01m2.94.38c-1.1-.11-2.04-.08-2.95-.29m-.66-.59c4.49.51 7.27.46 11.19 1.77m-10.13-.57c2.71-.19 6.63-.42 11.79-.13" pointer-events="stroke"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M465.96 177.35s0 0 0 0m0 0s0 0 0 0m2.55 9.62c1.2-2.51 3.26-3.28 5.08-6.26m-5.75 6.38c2.6-1.92 4.34-4.63 6.17-6.54" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M473.85999999999996 180.44c-1.1 1.59-3.47 2.61-8.43 5.35m9.24-4.44c-3.33 1.02-5.74 2.12-7.98 3.91m-.37-.25c.87-1.73 1.61-2.35 1.77-3.64m-1.91 3.67c.8-1.22 1-2.14 2.26-4.04m-.2.13c-.49-1.02-.73-1.47-2.01-3.94m2.24 3.95c-.7-.8-1.27-1.82-2.39-3.99m.36-.92c1.27 2.27 4.12 2.4 8.63 5.22m-9.21-4.05c3.48 1.04 5.78 1.87 8.63 3.44" pointer-events="all"/><path fill="none" stroke="none" d="M208 181h31.76" pointer-events="stroke"/><path fill="none" stroke="none" d="M245.76 181l-8 4 2-4-2-4z" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M208.65 182.94c6.7-3.29 15.64-.98 32.67-2.86m-34.13 1.54c10.56-1.12 19.91-1.51 32.4.36" pointer-events="stroke"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M238.05 176.66s0 0 0 0m0 0s0 0 0 0m2.69 9.54c1.06-1.75 2.52-2.78 4.57-5.92m-5.06 6.05c1.13-1.25 2.7-3.61 5.43-6.28" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M245.8 181.42c-1.67-.02-3.83.86-8.65 3.69m8.61-4.32c-1.94.78-3.52 2.31-7.96 4.59m.19-.48c.2-.86.51-1.69 1.76-4.25m-2.12 4.31c.59-1.03 1.01-1.8 2.27-4.12m-.22.53c-.43-1.59-1.37-2.11-1.62-4.42m1.87 4.26c-.71-.86-1.11-1.99-2.21-4.4m.5.46c1.96 1.7 5.15 2.92 7.96 3.6m-8.51-4.32c1.98 1.34 4.18 2.8 8.23 4.18" pointer-events="all"/><path fill="none" stroke="none" d="M29.5 151H208v60H29.5z" pointer-events="all"/><path fill="none" stroke="#f0f0f2" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M29.8 150.66s0 0 0 0m0 0s0 0 0 0m-.44 11.46c1.97-.98 5.93-4.1 8.9-10.32m-8.91 11.53c3.45-3.61 6.54-7.07 11.06-12.49m-11.93 24.53c6.3-7.97 14.21-15.64 23.09-25.14m-22.33 25.25c4.65-6.85 10.35-11.84 21.02-24.5m-20.24 37.3c7.84-8.69 14.62-16.3 29.47-38.87M29.8 187.06c7.01-9.06 15.31-16.52 30.87-35.22m-29.39 47.58c5.34-9.9 14.83-20.92 41.18-48.09m-43.3 47.38c8.69-9.1 17-17.87 42.73-46.99m-42.06 60.23c17.37-20.46 35.69-39.96 52.74-58.84m-52.55 59.14c13.9-17.51 27.94-32.46 52.1-61.56m-43.1 61.45c16.16-17.48 31.89-36.93 53.19-62.08M39.7 211.07c13.08-14.9 25.82-29.22 53.46-60.08m-43.69 61.64c15.48-14.56 28.19-28.1 53.57-61.91m-52.91 60.11c14.63-14.31 30.57-32.76 53.79-59.35m-42.31 59.74c11.88-11.55 18.86-26.13 51.39-59.35m-52.15 59.91c13.03-14.43 26.17-29.19 53.07-61.63m-41.81 59.77c20.77-21.77 41.87-46.87 53.2-60.46M72 211.28c12.46-12.87 24.07-26.92 52.79-60.63m-41.83 59.31c10.66-13.54 24.48-27.5 50.51-60.24m-49.64 62.63c11.49-14.97 23.44-30.4 51.94-61.59m-41.36 60.02c11.54-10.45 19.51-24.55 51.88-58.74m-52.07 60.12c15.68-20.44 32.96-39.85 50.99-61.24M106 213.06c16.08-20.67 30.39-39.59 50.81-60.99m-52.67 58.44c17.57-21.1 36.39-41.84 51.93-59.84m-42.53 59.01c16-14.73 31.44-34.14 53.85-57.02m-52.23 59.66c18.18-21.79 35.71-43.39 52.25-61.08m-41.28 61.51c17.91-21.56 35.08-42.89 52.54-61.86m-53.68 61.31c15.3-18.89 31.03-35.34 52.93-62.25m-42.77 62.63c14.82-16.82 26.7-30.81 55.39-63.54m-53.62 61.8c18.66-21.39 37.37-42.62 52.46-60.03m-42.11 61.91c18.92-22.19 35.27-45.76 51.94-63.09m-51.84 62c20.82-23.22 40.25-47.27 52.82-60.69m-42.77 61.99c18.16-23.2 36.2-44.54 52.07-61.79m-52.13 59.92c20.57-23.09 40.83-47.24 52.83-59.2m-40.35 60.33c5.04-12.09 16.64-19.84 37.45-47.34m-39.28 46.41c10.34-11.99 21.42-22.87 41.65-47.46m-30.65 47.9c6.81-7.42 10.56-14.36 27.8-36.62m-28.73 35.46c10.13-11.45 21.82-23.38 30.9-35.06m-18.24 34.39c2.88-4.74 11.76-11.45 19.96-21.89m-21.31 23.83c3.97-6.43 10.76-12.69 19.93-23.46m-9.99 21.51c3.37-1.6 6.71-3.63 8.69-9.61m-7.75 10.86c2.86-3.84 5.02-7.31 8.49-10.45" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M27.97 150.76c51.37 1.2 102.21 1.08 181.36-.78m-179.75.49c53.37.57 107.79 1.05 178.64 1.48m.49-2.18c.29 18.22-2.72 33.49-.06 59.89m-1.17-58.88c.49 19.55.83 39.2.57 60.75m.12.26c-40.84.07-78.35.69-178.33.27m177.89-1.33c-67.89-1.88-138.06-.63-178.9-.13m.41 1.07c-.02-19.25.28-37.23.07-59.71m.56 58.84c-1.32-16.92-1.06-32.22-.27-59.9" pointer-events="all"/><switch transform="translate(-.5 -.5)"><foreignObject width="100%" height="100%" pointer-events="none" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow:visible;text-align:left"><div xmlns="http://www.w3.org/1999/xhtml" style="display:flex;align-items:unsafe center;justify-content:unsafe center;width:177px;height:1px;padding-top:181px;margin-left:31px"><div style="box-sizing:border-box;font-size:0;text-align:center"><div style="display:inline-block;font-size:12px;font-family:Indie Flower;color:#000;line-height:1.2;pointer-events:all;white-space:normal;word-wrap:normal"><font face="Patrick Hand" style="font-size:18px">wasmer::Module::validate</font></div></div></div></foreignObject><text x="119" y="185" fill="#000" font-family="Indie Flower" font-size="12" text-anchor="middle">wasmer::Module::validate</text></switch><path fill="none" stroke="none" d="M337 91v30H118.8v21.76" pointer-events="stroke"/><path fill="none" stroke="none" d="M118.8 148.76l-4-8 4 2 4-2z" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-dasharray="6 6" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M335.96999999999997 89.78c2.46 9.53-.6 19.88 2.17 32.76m-1.38-30.73c.96 10.61 1.05 20.79-.1 29.63m1.6-.39c-81.81-.31-163.23-2.1-220.95 1m219.76-.43c-54.22-1.35-107.74-1.74-217.75-.13m.53-1.35c-.22 7.93.18 12.98-1.93 21.34m-.03-20.21c.45 6.51.48 12.45.14 20.61" pointer-events="stroke"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M114.65 140.94s0 0 0 0m0 0s0 0 0 0m4.84 7.17c1.17-1.28 2.27-2.66 4.63-6.27m-5.03 6.42c.75-.79 1.55-2.29 4.59-5.94" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M119.27000000000001 148.38c-1.17-1.23-2.05-3.69-4.87-8.19m3.99 8.69c-.96-2.34-2.13-4.93-3.93-8.51m.73.19c.62.47 2.51 1.66 3.76 2.02m-4.33-1.93c.86.42 1.93 1.09 4.27 2.24m-.22-.41c1.53-.59 2.7-1.45 3.87-2m-3.61 2.11c.51-.49 1.55-.86 3.98-1.92m.63-.39c-2.49 3.22-2.42 5.94-4.7 7.77m4.18-7.69c-1.17 2.46-1.61 3.62-4.16 8.02" pointer-events="all"/><path fill="none" stroke="none" d="M223 271h357v60H223z" pointer-events="all"/><path fill="none" stroke="#f0f0f2" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M223.05 270.94s0 0 0 0m0 0s0 0 0 0m-1.16 12.59c4.07-6.41 10.24-8.94 12.03-12.73m-10.35 12.33c2.95-4.54 6.74-7.81 9.79-12.06m-10.32 24.22c9.33-10.16 16.48-17.15 21.5-22.87m-21.52 22.64c8.31-9.59 14.4-18.14 20.42-24.39m-20.61 36.79c12.4-13.47 25.72-28.48 31.45-38.42m-30.8 38.91c10.49-13.89 22.01-27.8 30.76-37.86m-31.1 48.81c13.22-11.25 21.67-24.81 42.72-47.27m-43.44 49.07c10.65-11.94 20.16-22.69 43.61-50.77m-41.61 59.65c14.88-14.62 29.02-30.3 52.91-57.49m-52.91 58.18c19.22-22.68 40.56-46.53 50.85-59.02m-40.08 59.03c12.43-16.35 28.11-29.27 52.2-59.34m-53.82 60.38c12.66-14.21 25.31-26.41 53.78-61.03m-42.23 60.51c12.22-11.1 24.04-23.25 53.65-59.86m-54.58 60.34c16.82-17.96 32.84-36.85 52.29-61.21m-39.25 62.28c15.95-22.63 37.94-46.74 50.29-63.5m-52.06 61.99c18.54-22.01 37.87-42.89 53.05-59.74m-41.79 60.29c16.59-19.67 34.52-39.19 51.57-62.32m-52.52 63.59c19.16-23.3 36.4-44.51 53-61.65m-43.93 59.27c17.12-18.99 33.13-34.3 54.2-58.18M275.26 332c14.79-14.19 25.45-28.7 52.98-60.03m-41.89 61.49c13.61-19.96 28.21-33.94 54.59-60.27m-54.3 58.75c12.44-15.18 24.49-29.92 53.44-60.61m-40.68 58.82c10.09-13.24 22.94-29.39 52.35-59.55m-54.03 61.38c19.41-23.21 38.71-46.44 52.73-61.2m-44.24 60.02c22.13-21.58 39.02-44.55 55.56-58.58m-54.21 59.23c12.03-12.84 22.36-25.71 53.51-60.42m-40.69 61.73c14.05-17.33 24.67-31.98 50.11-60.73m-52.15 59.19c17.19-17.59 31.24-34.94 52.98-60.05m-41.14 60.79c16.03-18.26 31.87-38.28 49.71-60.16m-49.7 60.47c18.83-22.12 38.61-45.53 51.97-61.88m-40.24 62.23c12.66-19.1 25.84-34.23 49.87-59.86m-52.77 58.68c11.86-13.93 22.85-26 53.9-60.88m-43.75 60.38c20.09-18.52 35.88-39.09 53.95-61.65M351.39 332c20.16-23.08 39.4-46.96 51.79-60.05m-42.99 60.78c15.13-21.54 32.94-37.94 55.05-60.1m-54.88 58.74c18.83-20.13 35.66-41.29 53.2-59.5m-42.43 61.33c17.5-20.72 32.48-40.04 54.15-60.67m-52.98 59.22c18.82-22.4 36.62-41.98 51.35-61.47m-42.24 60.26c11.07-10.46 22.66-24.06 54.02-57.58m-52.68 58.8c10.18-11.74 20.27-24.47 52.03-61.23m-39.75 59.16c16.81-21.05 36.51-41.47 49.21-59.88m-50.98 61.33c17.97-18.95 34.45-38.32 52.62-60.2m-42.05 61.03c19.06-24.06 39.1-44.51 52.15-61.44m-52.72 59.84c16.15-19.11 32.5-36.98 53.08-58.86m-43.47 59.3c23.48-22.41 44.13-45.83 55.03-58.92m-53.03 59.43c10.82-14.13 21.84-27.77 53-59.79m-44.17 57.89c19.05-17.01 37.96-39.84 51.87-57.82m-50.83 59.36c16.35-17.86 32.9-35.78 52.53-60.68m-41.57 62.17c16.94-20.41 35.05-41.66 52.49-62.43m-53.26 61.94c17.54-18.55 33.65-37.06 53.44-61.31m-43.03 61.08c16.71-17.59 31.16-34.77 52.55-61.85m-51.16 60.47c21.17-23.48 40.88-46.51 52.07-60.53m-43.23 60.12c11.74-12.61 22.95-22.87 55.49-60.04m-53.55 60.32c15.3-15.32 29.45-32.83 52.36-59.31m-42.79 59.34c18.64-21.67 35.77-42.04 54.58-57.69m-55.11 58.77c18.53-18.93 33.7-36.69 52.85-59.69m-39.79 60.46c18.54-22.24 36.22-46.62 50.9-62.4m-52.61 62.09c10.22-12.59 22.71-26.46 53.69-61.83m-44.31 62.82c11.54-15.38 22.95-26.89 55.35-62.48m-54.41 61.42c20.33-23.84 40.7-47.36 53.58-60.97m-42.53 61.21c13.97-16.45 27.78-33.2 51.28-61.89m-51.58 62.57c11.55-14.52 23.39-27.11 53.51-61.19m-42.79 58.88c15.4-19.84 35.35-38.26 53.26-58.35m-54.1 60.15c13.18-15.33 25.75-29.33 52.43-60.68m-42.25 58.33c18.48-15.52 31.75-37.05 51.56-56.92m-49.81 58.13c19.43-22.47 37.72-44.11 50.8-60.2m-39.54 61.33c14.47-21.62 32.44-40.07 50.38-58.93m-52.16 58.7c16.09-17.82 31.27-37.17 51.76-59.1m-39.59 58.73c15.57-16.27 29.16-36.29 39.78-45.78m-40.54 45.07c14.41-17.25 30.85-35.37 40.13-46.05m-29.65 47.37c7.09-11.02 16.37-19.24 28.73-35.23m-28.86 33.74c9.07-11.53 18.85-23 30.39-34.42m-21.71 35.65c9.47-10.3 16.66-19.31 20.8-21.38m-19.63 21.38c7.26-8.15 13.82-14.79 19.41-23.1m-8.54 23.08c3.5-2.78 6.84-6.85 8.66-10.13m-8.26 10.27c1.9-3.31 3.81-5.62 8.2-10.79" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M223.85 270.19c100.14 2.84 197.97 3.47 355.21 1.69m-355.79-.41c114.06.94 229.14.89 357.14-.02m-.16-1.43c.18 18.25 1.86 35.7-.21 60.78m.74-59.82c-.78 23.59-1.42 48.23-1.33 59.9m.05-.79c-81.84 3.63-165.82 3.67-356.38.05m356.75 1.24c-129.02.7-256.93.05-357.34.03m1.49-2.02c-1.87-9.71.7-23.19-2.62-56.64m1.19 58.05c-.08-14.4.68-27.77-.05-59.91" pointer-events="all"/><switch transform="translate(-.5 -.5)"><foreignObject width="100%" height="100%" pointer-events="none" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow:visible;text-align:left"><div xmlns="http://www.w3.org/1999/xhtml" style="display:flex;align-items:unsafe center;justify-content:unsafe center;width:355px;height:1px;padding-top:301px;margin-left:224px"><div style="box-sizing:border-box;font-size:0;text-align:center"><div style="display:inline-block;font-size:12px;font-family:Indie Flower;color:#000;line-height:1.2;pointer-events:all;white-space:normal;word-wrap:normal"><div style="font-size:18px"><font face="Patrick Hand" style="font-size:18px">wasmer_compiler::ModuleEnvironment::translate</font></div></div></div></div></foreignObject><text x="402" y="305" fill="#000" font-family="Indie Flower" font-size="12" text-anchor="middle">wasmer_compiler::ModuleEnvironment::translate</text></switch><path fill="none" stroke="none" d="M539.75 211l.05 30H401.5v21.76" pointer-events="stroke"/><path fill="none" stroke="none" d="M401.5 268.76l-4-8 4 2 4-2z" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-dasharray="6 6" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M541.44 210.43c-1.97 6.39-1.95 13.67-2.79 30.28m.61-29.65c-.61 9.51.83 20.91.9 30.54m1.48.38c-38.38-2.81-72.68-1.78-140.38-1.25m137.59-.46c-27.6 1.49-55.93.35-138.12.34m1.94-1.24c-3.06 5.66-.57 11.95.02 24.22m-.45-23.27c.04 8.43-1.37 15.56-.04 22.47" pointer-events="stroke"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M397.41 260.86s0 0 0 0m0 0s0 0 0 0m4.1 7.58c2.43-1.68 3.4-4.87 6-6.46m-5.98 6.52c1.09-2.11 3.12-3.43 5.57-6.49" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M402.03 268.03000000000003c-2.44-2.1-2.38-3.93-4-6.9m3.8 7.33c-1.37-2.32-3.36-5.19-4.02-7.69m-.58.21c1.66.19 2.62.98 4.67 1.81m-4.38-1.83c1.4.46 2.74 1.29 3.81 1.67m.25-.22c.91-.12 2.74-.56 4.12-1.89m-4.36 2.24c1.18-.32 2.04-1.14 4.14-2.05m-.37.78c-.93 1.11-2.96 3.73-3.82 7.09m4.08-7.52c-.78 1.59-1.89 3.11-3.47 7.77" pointer-events="all"/><path fill="none" stroke="none" d="M803 271v-30H666.3l-.04-21.76" pointer-events="stroke"/><path fill="none" stroke="none" d="M666.25 213.24l4.02 7.99-4.01-1.99-3.99 2z" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-dasharray="6 6" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M804.0500000000001 272.76c-2.65-9.81.89-16.71-1.35-32.96m.69 30.89c-1.13-6.73-1.05-13.43.32-30.02m-.67-.23c-42.81-.21-83.73 2.36-138.24 1.55m137.74-1.44c-38.41 1.81-79.31 1.32-136.7.59m-.3 1.84c-.35-6.8 1.17-15.2.94-23.86m.38 21.73c-.29-7.47-1.42-15.59-.16-22.13" pointer-events="stroke"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M666.04 213.48s0 0 0 0m0 0s0 0 0 0m3.37 8.3c.29-.34.51-.66.74-.72m-.66.72c.09-.23.3-.35.61-.79" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M665.9100000000001 214.13c.75 1.55 2.48 2.4 4.46 7.04m-3.87-8c1.38 3.09 2.35 5.91 3.97 7.83m.1.66c-1.7-1.23-2.86-1.45-4.23-2.74m4.05 2.21c-1.06-.29-1.62-.62-4.21-1.7m.26-.12c-.72.87-1.7 1.31-3.75 2.16m3.49-2.37c-.98.76-1.82 1.07-3.78 2.33m.7.26c.45-2.51.47-3.49 3.72-9.26m-4.4 9.02c.99-3.24 2.42-5.52 3.5-7.91" pointer-events="all"/><path fill="none" stroke="none" d="M623 271h360v60H623z" pointer-events="all"/><path fill="none" stroke="#f0f0f2" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M622.77 271.26s0 0 0 0m0 0s0 0 0 0m.66 11.27c4.33-3.02 7.39-5.75 8.77-11.95m-8.6 13.13c2.46-5.47 6.96-10.14 10.12-13.23M623.4 296c8.86-8.36 15.25-18.66 19.92-24.59m-20.7 24.01c5.28-7.07 11.57-12.36 21.82-24.27m-21.68 34.77c9.42-10.41 22.48-25.01 32.13-36.46m-32.59 38.65c9.19-10.29 19.15-20.85 33.34-36.98m-33.52 46.53c15.79-16.18 30.82-31.88 42.97-44.95m-42.27 47.25c11.32-12.44 21.74-24.82 43.17-49.03m-41.17 61.06c8.4-11.72 22.35-27.58 53.08-60.13m-55.27 59.22c18.05-18.83 34.19-37.51 53.59-60.48m-43.05 62.62c11.51-16.27 23.14-30.68 55.21-60.62m-55.12 59.75c12.23-12.56 22.8-24.66 54.09-61.98m-40.77 59.38c19.45-19.71 40.88-46.55 49.74-57.81m-52.25 58.57c18.22-18.55 36.32-37.88 52.79-59.07m-42.18 58.82c17.51-18.69 36.01-40.25 53.17-59.64m-51.97 60.07c13.55-16.56 28.58-33.48 51.04-59.86m-42.47 61.32c17.19-18.14 31.51-35.57 52.64-62.69m-50.41 61.85c12.89-16.07 26.8-31.83 51.88-61.16m-40.71 59.73c18.37-21.49 40.56-46.15 49.89-60.13m-51.93 61.18c12.9-13.41 26.25-29.01 54.03-60.48m-44.42 60.96c20.88-21.46 34.12-39.32 53.3-61.79M688.19 331c18.5-20.95 35.8-41.1 51.88-59.99m-44.13 59.5c21.37-21.88 43.26-47.33 53.87-58.1m-51.37 58.31c10.75-12.29 22.03-25.93 51.52-60.55m-39.94 61.02c10.96-13.4 22.81-31.86 49.25-60.14m-50.17 60.81c14.36-15.97 27.99-32.18 52.75-61.76m-44.09 61.95c10.19-14.16 22.27-26.13 55.67-59.6m-54.6 59.06c17.07-19.33 33.43-39.59 53.02-59.84m-43.51 58.95c13.54-12.67 22.85-22.66 52.03-60.79m-49.79 61.15c19.02-23.9 39.85-47.48 51.49-59.74m-42.38 60.57c11.62-13.07 21.62-27.76 51.82-60.65m-51.72 59.36c13.4-13.67 26.47-27.83 53-59.06m-40.74 60.76c16.57-20.5 29.88-36.03 51.17-60.75m-52.66 58.75c19.47-21.73 39.44-43.04 52.08-60.14m-41.62 63.23c9.59-13.36 24.89-27.78 53.49-64.04m-53.32 62.24c11.15-15.01 24.46-29.17 53.6-60.72m-45.07 59.94c13.24-12.78 28.34-27.1 54.2-59.06M771 330.98c18.59-20.32 36.77-40.65 53.5-60.21m-41.94 60.47c16.06-21.5 35.44-39.2 51.26-59.53m-51.24 60.03c18.46-22.04 36.25-42 51.33-61.26m-40.47 59.58c12.93-15.16 26.82-29.25 50.65-60.75m-51.52 61.87c17.21-18.9 34.43-38.11 53.74-60.18m-43.58 61c16.19-16.91 31.39-35.17 53.62-59.43m-53.1 59.98c11.61-14.83 22.55-28.69 53.66-60.71m-42 59.98c17.54-21.88 36.51-44.71 52.04-60.86m-52.27 60.82c19.03-23.45 40.96-47.66 51.77-61.29m-42.29 60.16c21.19-22.77 39.17-42.39 52.44-59.01m-51.7 60.63c12.86-15.8 23.31-29.22 52.41-61.03m-41.28 61.06c10.26-13.81 21.38-26.49 51.93-60.41m-51.84 59.86c14.44-18.34 29.72-37.53 51.59-59.76m-42.67 59.71c13.11-16.38 26.44-28.02 54.26-60.29m-52.39 59.63c18.27-22.58 37.29-44.49 51.05-59.97m-41.67 60.16c15.38-17.73 31.53-38.17 53.82-58.44m-53.72 58.08c16.09-18.66 32.41-36.53 52.19-58.83m-41.47 61.07c14.76-21.49 34.89-40.19 51.62-63.32m-52.33 60.96c20.4-22.78 41.1-45 54.09-60.24m-43.78 61.26c14.62-13.77 26.51-32.61 55.27-59.97m-54.27 59.9c16.5-19.06 34.71-38.59 53.54-60.74m-43.53 59.29c15.04-13.62 24.09-28.85 55.03-58.11m-54.55 58.97c12.58-12.22 22.44-25.19 53-59.59m-41.31 61.02c15.09-22 33.82-39.23 51.79-60.04m-52.65 59.05c18.65-21.76 37.86-42.91 52.63-60.72m-39.94 61.13c16.89-22.92 32.91-41.16 49.04-60.71m-51.9 60.33c21.39-22.91 39.65-43.6 54.26-60.76m-41.11 60.75c9.08-13.88 23.1-26.89 48.91-59.68m-50.91 58.66c14.32-16.44 28.5-31.84 53.44-59.01m-41.89 60.95c21.03-25.28 40.29-48.11 52.17-61.14m-53.07 60.07c18.76-20.67 35.78-41.89 53.37-61.48m-43.26 60.4c10.74-10.2 19.65-19.72 43.03-45.09m-42.77 46.33c11.17-11.63 20.14-21.34 41.81-48.01m-31.59 47.97c8.68-12.72 17.19-20.04 31.31-34.54m-30.97 34.65c7.15-8.38 13.17-15.48 31.14-35.56m-19.26 33.68c7.12-6.22 11.22-11.99 20.97-22.81m-21.24 23.36c5.68-5.28 11.44-12.39 21.02-23.46m-10.06 23.6c1.74-2.11 6.85-8.52 7.93-9.27m-8.69 9.91c2.84-3.07 3.96-4.61 9.51-10.65" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M623.12 272.07c100.23-2.31 201.31-.99 359.71-1.22m-359.34.15c123.41-1.96 246.91-1.12 358.91.29m.38-1.02c.07 18.28 2.44 37.91-.34 61.66m.64-60.75c.75 15.64-.86 31.27.78 59.05m-1.95 1c-88.21 1.07-177.71.77-358.9-.07m359.89-.51c-120.88-1.05-241.91-.61-360.4.15m-1.2.31c4.27-16.16 1.11-33.15 2.69-61.56m-.18 61.25c-.85-11.88.42-23.32.17-59.92" pointer-events="all"/><switch transform="translate(-.5 -.5)"><foreignObject width="100%" height="100%" pointer-events="none" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow:visible;text-align:left"><div xmlns="http://www.w3.org/1999/xhtml" style="display:flex;align-items:unsafe center;justify-content:unsafe center;width:358px;height:1px;padding-top:301px;margin-left:624px"><div style="box-sizing:border-box;font-size:0;text-align:center"><div style="display:inline-block;font-size:12px;font-family:Indie Flower;color:#000;line-height:1.2;pointer-events:all;white-space:normal;word-wrap:normal"><div style="font-size:18px"><font face="Patrick Hand" style="font-size:18px">wasmer_compiler::Compiler::compile_module</font></div></div></div></div></foreignObject><text x="803" y="305" fill="#000" font-family="Indie Flower" font-size="12" text-anchor="middle">wasmer_compiler::Compiler::compile_module</text></switch><path fill="none" stroke="none" d="M580 301h34.76" pointer-events="stroke"/><path fill="none" stroke="none" d="M620.76 301l-8 4 2-4-2-4z" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M579.37 299.59000000000003c13.34 1.19 24.94-.01 34.66 2.67m-33.08-1.38c9.14.92 19.8.56 34.34-.74" pointer-events="stroke"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M612.75 297.01s0 0 0 0m0 0s0 0 0 0m1.95 9.93c2.44-2.25 3.77-5.1 5.94-6.1m-6.02 6.15c2.27-2.57 4.27-4.75 5.61-6.03" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M620.85 301.08c-2.91.74-4.1 2.19-7.77 3.18m7.52-3c-1.61.44-4.2 1.34-8.03 3.63m-.24.43c1.26-1.01 1.58-2.08 2.67-4.46m-2.17 3.97c.9-1.34 1.33-2.73 1.8-3.8m.26.28c-1.14-1.11-1.47-2.14-2.27-4.31m2.02 3.93c-.13-.7-.78-1.86-2.05-4.08m-.51-.15c3.86 1.31 6.92 4.14 8.86 4.7m-8.62-4.14c2.74 1.31 5.51 2.14 8.03 3.92" pointer-events="all"/></svg>

'''
'''--- assets/diagrams/Diagram_module_instantiation.svg ---
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="1462" height="462" content="&lt;mxfile host=&quot;app.diagrams.net&quot; modified=&quot;2020-07-20T09:21:57.702Z&quot; agent=&quot;5.0 (Macintosh)&quot; version=&quot;13.4.8&quot; etag=&quot;3Sb7SC4d8sVG8JZU-d_n&quot; type=&quot;device&quot;&gt;&lt;diagram id=&quot;QskjrULfAQqA6fvHmlu-&quot;&gt;5VptT9swEP41/UgV570fodAxadOQmLTxqTLJNfFw7MxxabtfPyexk6YBLRqUFiqQmnvsu55999zFVkfONFt/EjhPv/IY6Mi24vXIuRzZth0ElvookU2NuL5fA4kgcQ2hFrglf0CDWi9ZkhiKzkTJOZUk74IRZwwi2cGwEHzVnbbgtPutOU6gB9xGmPbRHySWaY2GdtDi10CS1Hwz8if1SIbNZL2SIsUxX3UgWMsZZ1K7+C0HpsBbzIqRd5VKWS7xfGTP1P+inDZOOE8o4JwU44hnCo4KNWW2wBmh5f7WFi4qC8H0BktBogdl8hqz+D9NNkYuKiPKbedq5EwF57J+ytZToGXMTTjrtc2eGW32UwCTQxTsWuER06UOycj2qVK9KBdQ7qvc6GD5v5fcDJwVVSqptVoozNftYJkEOOoq7OzU1lw/KT9XuMhAlNac88+skJhFUEsMVsYdvaVGRwe9cc4WfMliKFdllSZTIuE2rz1ZKe4oLJUZVRJSj8UDyCg1ghT8AaacclGZci4nluvZzYhJzBJZEEq3Zs4s9Wc37jyCkLB+NhKoia/iM/AMpNioKVrhzPd13mouI9cAq5YaKNRYukULMw/rVE8a423k1YMO/tOJ4BxZIqhat6Q6DUiVFJJgCSeUDoHXTQcv7KdDU8hfOx3c3o5CrCq3FrmQKU84w/SqRS+6e97O+cJ5rjf3F0i50W0ILyXvxgHWRP4s1ceelu62Ri7X2nIlbIzA1Mq2lErxbnusVaskoxfjIq18fVEC6EYT73S5gi+Fzn1TYSUWCcgO1wYkgwCKJXnsWn9JXL2jofkcWEKYZvi5kETZkSfM9zDcKf+B9QTfQ3/s7Yfx/mEZb38gyjt9ynuHo3xwnJQXUHD6aNq9luYky1UaFadDe+S6/ni31Yd2n/qOg/ZF/fDdUP/NKOz1KRwMju2rU3hynBT+vmT4nkJRS5EA1bDnGWRcEDglCluBNx7EYGc//DWHgE5+vD2j98nHoM/HyeH4iNA7IqSsoZOhY2C5A7iIwn11U/TUndrHYuOkz0Y0PFCvT0fnsC8w2+8vzSvLv44uQefsgo7l7ILcozq8GHfeRaVNKL/H9IRKrTP07GLv6ZoSPXWf9bFqrSmsnWLrHpCR/off8j3UO616w0lVtAyBPG+HPYGzw4o67FpvJ0qNI8MCF5xWlyxrr3YL2fsmadgn6QFv+Y07x9Y1u9f8C8JIkc7b237C2en0Ts8bcN/vuK/TOJXY/qagrh3tD0qcq78=&lt;/diagram&gt;&lt;/mxfile&gt;" version="1.1" viewBox="-.5 -.5 1462 462"><defs/><defs><style type="text/css">@import url(https://fonts.googleapis.com/css?family=Open+Sans);@import url(https://fonts.googleapis.com/css?family=Patrick+Hand);</style></defs><path fill="none" stroke="none" d="M807 21h180v60H807z" pointer-events="all"/><path fill="none" stroke="#f0f0f2" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M807.19 20.78s0 0 0 0m0 0s0 0 0 0m-.3 11.57c3.33-2.06 6.43-4.89 10.26-11.65m-10.57 12.58c4.07-3.24 6.46-7.14 10.5-12.09m-10.95 25.94c5.16-9.89 13.01-13.91 24.03-27.05m-22.62 25.56c4.62-5.66 7.97-11.12 19.96-24.06m-19.78 34.61c9.67-9.32 17.55-21 32.31-35.49m-33.54 36.01c12.52-11.03 22.41-24.48 31.99-36.69M806.47 70.4c12.6-12.44 19.71-25.31 44.46-50.91m-44.06 50.58c13.77-14.27 26.65-30.2 43.26-49.99m-42.29 59.99c18.31-19.51 39.14-42.79 49.92-57.26m-49.62 57.96c10.48-11.16 20.98-24.01 51.47-59.78m-43.54 62.07c23.44-25.72 41.86-47.39 55.02-60.02m-53.81 58.24c16.54-16.69 31.14-34.33 52.11-59.35m-40.32 58.75c9.55-11.25 23.11-26.99 50.46-57.94m-50.41 58.19c12.68-16.69 28.56-33.61 51.98-59.22m-40.85 59.37c12.03-14.5 24.84-29.83 50.49-57.83m-51.12 57.62c19.94-22.91 40.52-47.99 52.35-59.9m-41.53 61.97c15.45-22.08 31.92-39.63 50.73-61.89M849.2 81.32c15.21-16.81 27.86-31.29 52.83-59.18M862.25 83.1c13.4-16.67 26.38-34.55 52.17-63.66m-53.88 61.33c11.03-12.75 21.94-25.75 51.32-60.22m-41.44 59.64c17.78-21.22 36.83-42.39 53.2-60.09M870.4 81.16c11.68-12.31 24.04-27.08 54.05-59.45m-44.2 60.53c14.19-12.02 24.28-24.19 53.55-59.47m-52.36 58.28c19.76-23.1 41.12-46.6 53.55-60.6m-41.08 61.87c8.17-14.3 20.93-29.16 50.35-61.5m-52.23 60.03c17.33-18.41 35.25-39.75 52.66-60.76m-39.82 62.07c17.65-22.36 34.65-45.29 50.09-61.22m-52.93 61.03c22.05-25.25 42.94-49.77 52.55-60.28m-42.34 58.1c18.2-14.47 31.72-33.17 52.64-57.09m-52.21 59.33c17.15-18.61 31.34-34.5 52.67-60.88m-42.17 59.97c19.8-22.07 41.13-42.42 55.25-58.9m-54.13 59.82c16.73-18.65 32.8-37.21 52.83-61.55M935.2 79.61c14.48-13.94 31.72-34.2 52.54-59.83M934.2 81.57c15.94-19.04 34.3-39.67 53.1-60.6M946.65 79.9c14.97-16.95 31.37-36.82 42.01-48.38m-44.19 50.45c11.09-11.96 20.26-23.56 43.41-49.58m-32.89 48.99c7.75-7.69 16.43-16.72 30.87-35.78m-30.1 35.94c11.71-13.85 21.91-26.78 30.32-34.76m-18.85 35.1c5.4-7.66 11.71-14.3 19.19-22.88m-19.3 21.7c4.98-6.68 11.73-12.79 20.16-21.89m-9.26 22.42c2.84-4.06 6.86-8.55 10.27-11.52m-10.62 11.51c2.18-2.57 5.73-6.95 9.68-11.68" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M808.19 20.23c50.41.83 102.4 1.05 179.09 2.24m-179.55-2.36c57.52-.05 116.66.92 179.46.35m-1.97 1.69c.47 13.1 1.25 23.06 1.28 58.47m-.19-60.5c.19 22.41-.43 44.71 1.23 61.47m-.1-.23c-50.57.48-103.94-1.74-180.52.93m180.43-.82c-47.32 1.47-95.12 1.39-181.25-.28m2.41 1.12c-2.22-21.49-3.63-39.94-.44-62.81m-1.78 61.54c1.37-14.46-.5-29.1.47-59.55" pointer-events="all"/><switch transform="translate(-.5 -.5)"><foreignObject width="100%" height="100%" pointer-events="none" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow:visible;text-align:left"><div xmlns="http://www.w3.org/1999/xhtml" style="display:flex;align-items:unsafe center;justify-content:unsafe center;width:178px;height:1px;padding-top:51px;margin-left:808px"><div style="box-sizing:border-box;font-size:0;text-align:center"><div style="display:inline-block;font-size:12px;font-family:Helvetica;color:#000;line-height:1.2;pointer-events:all;white-space:normal;word-wrap:normal"><font face="Patrick Hand" style="font-size:18px">wasmer::Instance::new</font></div></div></div></foreignObject><text x="897" y="55" fill="#000" font-family="Helvetica" font-size="12" text-anchor="middle">wasmer::Instance::new</text></switch><path fill="none" stroke="none" d="M792 141h210v60H792z" pointer-events="all"/><path fill="none" stroke="#f0f0f2" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M791.8 141.23s0 0 0 0m0 0s0 0 0 0m1.52 13.44c2.48-7.48 7.79-9.73 10.2-14.06m-12.02 13.38c3.82-4.17 6.91-7.87 10.83-13.25m-10.29 26.48c8.02-7.46 10.94-14.6 21.79-28.03m-21.15 26.19c5.65-8.24 13.26-15.12 20.71-25.19m-20.85 38.59c6.75-10.32 18.41-21.64 32.99-37.53m-33.34 35.76c7.23-9.46 15.58-17.58 30.57-35.82m-33.08 48.85c15.39-13.63 29.31-28.9 45.36-49.34m-43.54 49.52c14.61-16.96 28.56-32.72 43.47-49.01m-41.19 60.64c15.83-20.62 32.48-36.07 50.1-59.89m-51.99 58.98c17.81-21.37 35.61-41.12 52.23-59.23m-40.52 59.65c19.71-20.58 38.09-43.73 50.03-61.28m-51.12 61.6c14.12-17.03 28.63-32.57 52.43-60.3m-42.87 61.63c20.79-23.87 43.96-50.3 54.55-63.89m-52.08 62.41c16.08-19.28 32.04-36.63 52.48-61.07m-43.78 61.91c13.59-14.56 22.92-27.34 53.38-61.98m-53.03 61.7c16.94-20.41 33.22-39.53 52.34-62.01m-42.86 59.74c15.05-16.79 32.13-32.5 52.44-59m-50.52 61.29c19.67-23.93 39.72-47.31 52.38-61.46m-41.22 62.18c18.65-23.17 33.82-42.4 51.86-61.6m-52.03 60.6c11.13-13.97 22.74-26.06 52.37-60.67m-43.97 61.11c13.35-14.64 24.54-27.62 55.91-61.61m-53.45 59.68c17.73-18.07 33.35-38.24 52.5-58.81m-43.67 61.2c17.48-20.66 36.61-42.13 55.08-62.03m-53.71 61.22c13.1-15.13 24.44-29.04 52.35-61.96m-40.79 62.88c15.5-22.2 29.71-36.62 52.05-63.06m-52.2 61.6c19.67-23.53 38.89-46.21 51.91-60.09m-43.22 60.29c17.79-19.2 32.24-35.44 51.65-60.1M887.4 200.4c12.65-11.88 23.36-25.23 52.6-60.09m-40.71 62.26c18.2-24.95 39.69-45.69 50.96-61.71m-52.3 60.01c17.34-18.39 32.58-36.45 54.19-60.74m-42.31 60.3c13.56-16.03 31.7-35.94 53.19-59.68m-53.07 60.5c18.27-19.43 34.41-40.09 51.92-59.73m-43.56 60.26c15.13-17.17 30.06-32.59 54.42-59.69m-53.14 60.58c16.68-19.92 33.97-38.21 52.06-61.44m-43.29 59.37c17.21-16.2 30.95-34.53 54.3-59.63m-52.48 61.27c18.24-19.49 34.71-40.85 51.66-60.12m-39.37 60.92c18.13-21.51 34.54-43.49 52.57-63.08m-53.69 60.94c18.53-21.01 38.98-43.74 52.11-60.41m-44.19 62.22c23.26-24.8 42.05-49.55 51.92-61.06m-49.36 59.16c13.33-13.83 26.87-30.14 50.96-58.15m-41.22 59.08c10.88-12.82 21.43-25.12 43.33-48.68m-42.91 48.33c11.82-12.85 23.22-27.66 40.24-48m-29.14 47.74c6.46-7.94 15.01-13.38 30.99-34.17m-32.14 34.73c11.72-13.49 23.5-26.9 30.46-35.41m-17.34 34.75c3.96-6.31 10.54-11.18 16.81-22.85m-19.16 23.55c5.61-5.15 10.55-10.66 20.02-23.67m-8.1 24.53c2.37-3.69 4.3-8.93 7.52-12.61m-7.91 11.05c1.79-1.11 3.83-3.73 8.83-9.62" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M792.05 139.41c57.99 2.38 118.2 3.55 211.26 1.47m-212.12.45c74.73-.31 147.86.04 211.35-1.02m1.22 2.09c-.85 13.13-3.38 25.27-2.87 59.35m1.72-61.43c-1.08 14.46-.67 27.74-.66 60.62m-.4-1.58c-62.55.31-126.42 1.4-209.77.66m210.58.38c-51.17.25-100.97-.06-211.21-.07m-.36-.31c-.08-11.94.78-21.89.87-59.72m.85 60.73c-1.4-11.93-.76-24.64-1.31-59.55" pointer-events="all"/><switch transform="translate(-.5 -.5)"><foreignObject width="100%" height="100%" pointer-events="none" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow:visible;text-align:left"><div xmlns="http://www.w3.org/1999/xhtml" style="display:flex;align-items:unsafe center;justify-content:unsafe center;width:208px;height:1px;padding-top:171px;margin-left:793px"><div style="box-sizing:border-box;font-size:0;text-align:center"><div style="display:inline-block;font-size:12px;font-family:Helvetica;color:#000;line-height:1.2;pointer-events:all;white-space:normal;word-wrap:normal"><font face="Patrick Hand" style="font-size:18px">wasmer::Module::instantiate</font></div></div></div></foreignObject><text x="897" y="175" fill="#000" font-family="Helvetica" font-size="12" text-anchor="middle">wasmer::Module::instantiate</text></switch><path fill="none" stroke="none" d="M897 81v51.76" pointer-events="stroke"/><path fill="none" stroke="none" d="M897 138.76l-4-8 4 2 4-2z" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-dasharray="6 6" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M896.23 81.95c.38 15.04-.39 33.37 1.62 51.89m-1.23-53.36c.46 15.26.54 30.39.32 52.78" pointer-events="stroke"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M892.8 130.98s0 0 0 0m0 0s0 0 0 0m3.55 6.82c2.96-1.14 4.5-4.23 6.29-6.22m-5.94 7c2.15-2.73 4.54-5.26 6.14-7.09" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M896.5 138.4c-1.75-3.12-3.32-5.06-4.25-7.36m4.71 7.63c-.67-1.73-2.01-3.76-3.98-8.08m.25-.07c.92.96 2.24 1.79 4.06 2.16m-4.28-1.99c.86.76 2.13.99 3.77 2.22m.49-.41c1.07-.18 1.99-.97 3.73-1.61m-3.9 1.94c1.04-.78 2.46-1.33 3.99-2.27m-.7-.15c-1.22 2.37-2.67 4.78-3.16 7.95m3.71-8.02c-1.47 3.23-2.22 5.91-4.08 8.71" pointer-events="all"/><path fill="none" stroke="none" d="M587 261h286.5v60H587z" pointer-events="all"/><path fill="none" stroke="#f0f0f2" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M586.84 261.19s0 0 0 0m0 0s0 0 0 0m-.03 13.15c5.44-6.14 8.55-10.51 12.06-12.81m-11.97 11.83c3.48-3.36 5.05-6.67 10.11-11.8m-11.72 22.2c7.5-4.03 13.3-14.47 22.99-22.04m-21.27 22.56c4.38-4.63 8.32-10.08 21.57-22.74m-22.8 36.91c12.27-12.09 19.07-24.9 34.27-39.03m-33.27 38.77c8.46-8.76 16.52-18.42 31.78-37.28M586 308.17c14.73-14.87 30.85-34.12 45.13-46.75m-44.23 48.13c13.42-13.36 25.44-27.82 42.3-49.44m-41.46 59.75c20.19-21.08 38.38-44.91 53.87-58.04m-53.92 59.71c14.77-17.36 28.22-33.93 52.67-61.27m-44.01 62.58c12.13-14.49 24.94-26.39 56.26-61.22m-53.79 59.71c17.44-22.58 37.51-43.99 51.3-61.26m-40.77 60.87c14.04-16.88 26.74-33 52.49-58.84m-52.83 59.48c16.86-18.43 33.15-37.27 51.46-59.96m-40.29 59.82c18.41-18.55 33.03-39.6 53.01-61.16m-53.15 61.44c9.96-13.94 19.55-24.67 52.05-60.59m-42.12 60.77c11.7-16.36 22.71-28.11 54.32-59.53m-54.71 59.74c19.85-24.16 40.25-46.21 52.41-61.37m-42.91 58.8c20.34-21.97 39.59-42.35 52.95-60.34m-51.13 62.54c14.6-17.23 29.65-35.16 51.4-60.43m-39.65 61.37c10.6-18.15 24.85-32.03 51.54-63.06m-52.56 61.47c20.63-24.48 41.94-48.7 52.32-60.91m-43.78 60.05c12.84-10.5 21.42-23.49 52.77-58.22m-51.48 59.86c14.81-16.79 30.55-35.15 53.92-61.91m-42.33 61.08c11.89-15.63 30.03-31.77 53.39-61.38m-53.59 62.15c18.14-22.86 38.93-44.69 51.43-61.48m-40.09 59.11c10.39-13.53 26.26-27.29 50.19-59.23m-51.63 60.24c15.61-17.73 32.31-36.8 53.58-59.67m-42.81 61.81c20.24-26.41 38.5-47.23 54.04-63.46m-54.23 62.77c12.38-13.68 21.94-25.47 53.54-61.48m-43.96 60.3c18.77-19.63 39.14-42.74 52.54-61.61m-52.42 62.29c15.67-16.44 30-33.08 54.6-60.64m-41.24 62.77c18.74-25.76 36.98-46.33 48.71-63.24m-51.18 60.77c13.58-14.67 27.13-30.33 53.91-58.81m-44.12 59.92c20.41-19.74 36.8-39.69 53.58-61.52m-51.82 61.55c9.52-13.16 20.59-25.54 52.01-60.19m-40.87 59.64c13.22-16.34 26.81-29 49.61-59.66m-50.33 59.07c19.59-24.5 40.3-47.8 51.3-58.87m-39.53 60.84c9.45-15.46 24.75-29.84 51.98-60.7m-52.45 59.08c13.21-16.58 27.84-32.91 52.31-59.72m-43.65 61.16c14.95-20.57 30.35-37.6 53.93-60.7m-51.86 59.11c17.07-18.93 33.74-39.87 52.02-60.55m-43.8 61.89c15.19-17.8 29.21-32.86 52.77-62.84m-52.04 60.92c17.69-19.96 35.48-39.08 52.88-59.13m-40.69 58.56c18.52-17.96 33.58-39.9 51.87-59.3m-53.13 59.78c17.57-18.98 33.12-38.5 53.82-60.25m-41.42 62.7c11.14-14.06 25.08-30.31 50.67-61.39m-51.73 60.84c12.01-17.07 25.13-30.05 51.23-60.59m-40.52 58.36c9.77-10.75 20.39-24.98 50.65-57.42m-51.77 58.47c12.91-12.79 23.6-27.08 53.97-59.87m-44.17 61.9c16.6-21.01 33.99-40.93 53.37-62.8m-52.36 60.6c18.82-21.37 35.85-41.18 52.38-59.73m-39.41 60.56c7.48-14.68 20.39-24.06 50.46-62m-52.62 62.05c14.51-16.7 27.92-33.77 53.25-60.81m-44.13 60.15c18.17-18.54 35.54-38.17 49-53.25m-46.95 53.64c15.65-18.49 32.19-37.52 48.27-56.42m-35.82 56.42c7.1-11.06 14.08-20.46 33.42-43.64m-35.7 42.52c11.82-10.99 21.6-23.94 36.84-42.61m-27.48 41.76c6.52-5.15 15.72-13.5 27.43-27.61m-25.01 29.11c4.5-6.65 11.1-12.93 25.11-31.71m-14.78 30.39c2.78-2.27 8.6-7.84 14.91-17.86m-14.49 19.29c3.29-5.19 8.67-9.88 15.71-18.23" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M586.18 260.55c81.62 1.79 162.57.03 287.34.82m-285.91-.22c62.35 1.12 125.06 2.54 285.77-.32m-.31-.91c1.42 13.78-1.9 25.85 1.92 62.7m-2.34-61.88c1.06 19.4-.01 40.49 1.75 59.95m-1.92 1.41c-70.27-3.18-141.08-2.22-285.36-2.6m286.12 1.62c-88.77.55-177.4.3-286.47 0m-.83.71c-.47-17.96 2.27-36.92 1.74-60.07m-1.38 59.23c1.02-12.44.79-24.76 1.6-59.77" pointer-events="all"/><switch transform="translate(-.5 -.5)"><foreignObject width="100%" height="100%" pointer-events="none" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow:visible;text-align:left"><div xmlns="http://www.w3.org/1999/xhtml" style="display:flex;align-items:unsafe center;justify-content:unsafe center;width:285px;height:1px;padding-top:291px;margin-left:588px"><div style="box-sizing:border-box;font-size:0;text-align:center"><div style="display:inline-block;font-size:12px;font-family:Helvetica;color:#000;line-height:1.2;pointer-events:all;white-space:normal;word-wrap:normal"><font face="Patrick Hand" style="font-size:18px">wasmer_engine::Artifact::instantiate</font></div></div></div></foreignObject><text x="730" y="295" fill="#000" font-family="Helvetica" font-size="12" text-anchor="middle">wasmer_engine::Artifact::instantiate</text></switch><path fill="none" stroke="none" d="M844.5 201v30H730.3l-.04 21.76" pointer-events="stroke"/><path fill="none" stroke="none" d="M730.25 258.76l-3.98-8 3.99 2 4.01-1.99z" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-dasharray="6 6" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M842.7600000000001 200.36c1.83 10.27 2.2 17.49 1.2 29.76m.5-29.54c.18 11.12.93 20.38.13 31.08m1.38.75c-32.94-2.74-66.93-2.02-116.26-1.5m115.06.74c-40.09-1.76-81-1.13-114.74-.54m.48-.67c-.85 8.02 1.46 19.42.66 21.38M730.15 231c-.8 5.34-.63 8.89.97 21.41" pointer-events="stroke"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M726.04 251.02s0 0 0 0m0 0s0 0 0 0m4.54 7.94c1.97-1.88 3.95-5.45 5.93-7.73m-6.82 7.2c2.14-1.47 3.68-3.98 5.96-6.79" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M730.35 258.51c-2-3.32-2.52-5.14-3.67-8.17m3.5 8.42c-1.17-1.29-1.91-3.39-3.52-8.16m-.11.29c.25.43 1.46.79 3.99 2m-4.47-2.12c1.35.79 3.02 1.29 4.33 1.77m-.13-.12c1.15.1 2.38-.93 4.19-1.33m-4.04 1.48c.63-.45 1.9-.71 3.99-1.99m.12-.59c-2.04 3.82-2.15 6.7-3.62 8.12m2.99-7.59c-.72 2.82-2.18 4.26-3.86 7.83" pointer-events="all"/><path fill="none" stroke="none" d="M20.25 381h331.5v60H20.25z" pointer-events="all"/><path fill="none" stroke="#f0f0f2" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M20.34 380.9s0 0 0 0m0 0s0 0 0 0m-1.41 11.83c5.45-2.55 8.66-7.12 12.01-12.03m-10.23 12.63c2.99-4.08 5.27-7.2 10.44-12.97m-12.36 24.25c7.23-6.46 16.48-16.45 22.06-24.52m-21.12 24.69c7.76-7.18 14.31-14.43 21.64-23.38m-23.19 36.41c10.45-8.89 17.41-17.77 32.26-38.58m-30.38 38.02c11.94-13.59 23.22-27.71 31.68-35.92M22.1 431.07c8.99-14.25 19.4-26.08 42.04-49.18m-43.72 47.07c14.26-14.06 27.36-30.34 42.52-48.46m-41.02 61.16c16.5-19.24 30.51-39.24 50.05-60.68m-50.28 60.75c9.59-12.46 21.84-25.86 51.41-60.74m-40.99 60.18c18.91-21.45 38.17-46.89 51.27-58.5m-51.09 59.08c17.02-19.88 34.99-41.19 51.61-60.21m-43.97 58.5c14.94-13.6 26.91-28.52 53.24-57.55m-51.15 58.46c17.27-20.34 35.86-41.87 52.61-60.56M50.75 443.5c15.98-18.61 35.11-41.12 55.9-61.31m-55.06 59.53c17.47-19.52 35.25-38.38 54.03-60.71m-41.44 62.54c19.01-27.14 41.12-49.14 52.03-60.64m-53.26 58.75c15.82-17.2 31.12-34.06 51.96-59.63m-40.35 58.01c11.09-12.4 26.04-30.06 52.64-58.29m-53.28 59.81c15.91-16.45 29.67-35.46 52.45-60.87m-42.18 60.84c16.67-19.65 29.55-37.41 54.25-60.58m-54.79 60.09c16.19-18.17 32.26-35.71 52.76-60.28m-43.24 60.73c20.14-18.09 37.75-40.39 54.31-60.77m-52.83 60.79c16.64-19.94 32.85-38.8 52.71-61.26m-42.03 62.09c19.22-22.48 38.81-42.19 51.08-62.95m-51.56 61.72c11.99-11.91 23.58-27 52.68-60.81m-43.49 59.33c21.03-21.39 40.85-44.54 55.75-58.11m-54.09 60.12c18.7-19.48 35.73-39.45 53.63-61.47m-41.18 61.98c10.96-15.78 26.68-30.55 52.36-60.09m-54.32 59.44c12.57-12.4 23.18-26.36 53.05-61.33m-41.43 59.87c12.29-17.33 29.25-31.73 53.83-57.63m-54.71 58.22c17.03-17.02 31.24-35.54 53.03-60.07m-41.57 60.4c19.23-20.8 34.99-44.32 53.94-58.35m-54.12 58.82c14.1-14.37 26.38-29.47 52.2-60.91m-42.09 61.62c14.73-15.39 28.47-29.2 51.37-60.7m-51.61 60.28c16.19-17.92 32.08-37.2 53.75-60.64m-42.85 61.49c18.25-24.47 38.46-44.21 53.66-62.06m-54.27 61.18c22.57-24.43 43.66-48.48 53.45-61.62m-43.23 63.05c19.66-22.49 38.74-44.99 53.22-62.65m-51.02 60.31c11-12.81 22.83-26.3 51.63-59.75m-40.38 61.58c16.37-22.75 36.45-49.53 51.36-61.58m-52.39 59.66c16.31-19.92 35.72-40.71 51.14-60.53m-39.32 61.85c8.3-15.7 21.49-28.55 50.23-60.82m-52.42 61.07c13.15-15.69 25.53-28.8 53.46-61m-43.22 59.25c16.58-17.42 33.96-38.78 52.88-61.25m-52.36 61.17c14.07-16.21 30.03-34.23 52.59-59.74m-43.55 58.96c17.29-15.81 30.3-28.85 55.24-58.57m-52.6 60.11c19.01-22.03 38.53-44.58 51.1-60.94m-40.21 59.25c13.58-16.23 27.7-29.96 50.09-59.41m-50.36 60.4c11.01-12.59 22.88-27.55 52.13-60.31M242 442.7c18.22-19.8 30.49-34.18 55.09-61.23m-53.6 59.74c15.93-18.46 32.27-39.15 53.37-61.16m-43.85 62.6c15.13-19.81 32.97-39.36 52.8-60.41M255 442.07c14.51-20.05 30.11-37.49 51.09-62.34m-42.8 59.87c17.04-16.11 34.97-35.85 54.63-60.15m-52.77 61.11c10.6-12.03 19.68-23.17 52.09-60.24m-43.05 61.25c18.32-17.78 33.61-37.86 53.53-59.33m-53.1 60.44c10.02-13.58 21.63-27.4 52.44-61.63m-42.45 60.34c16.55-18.46 29.77-33.87 52.7-60.66m-52.74 60.08c13.73-14.36 25.79-28.46 54.11-59.77m-41.78 60.4c17.86-17.93 34.25-39.06 52.15-58.27m-52.3 58.03c10.09-12.54 20.29-25.17 51.57-60.02m-42.91 61.48c12.17-14.19 27.46-28.34 50.87-58.36m-50.11 57.57c15.58-17.05 31.31-34.08 49.96-56.53M317.7 441.5c9.2-9.59 17.44-21.41 36.7-45.99m-37.86 46.82c14.05-13.99 24.61-28.86 38.97-44.99m-27.12 44.7c7.34-10.32 18.67-21.1 29.53-33.96m-29.63 34.43c7.55-11.35 16.91-19.56 28.03-33.52m-19.28 31.08c6.33-4.17 10.07-6.72 18.92-17.61m-17.28 19.19c4.81-7.23 11.17-12.26 17.89-21.43m-6.89 22.26c1.61-3.24 4.02-7.47 7.24-9.88m-7.36 8.82c1.21-1.79 2.75-3.51 6.47-7.95" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M21.45 379.89c93.62-.86 187.27-1.69 331.14.35m-332.91.39c80.04-.2 159.43-.67 332.25.05m-1.07-.51c1.3 13.81 2.68 28.06 1.77 59.59m-.43-58.82c-.21 11.44-.26 23.52-.14 59.1m.85.44c-89.31-.35-175.49.14-332.66.25m331.3-.4c-94.08.29-189.39-.12-331.55.12m.47-.9c-2.32-12.42-1.31-22.87-.95-56.99m1.25 58.43c.26-21.92.53-44.31-.18-59.79" pointer-events="all"/><switch transform="translate(-.5 -.5)"><foreignObject width="100%" height="100%" pointer-events="none" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow:visible;text-align:left"><div xmlns="http://www.w3.org/1999/xhtml" style="display:flex;align-items:unsafe center;justify-content:unsafe center;width:330px;height:1px;padding-top:411px;margin-left:21px"><div style="box-sizing:border-box;font-size:0;text-align:center"><div style="display:inline-block;font-size:12px;font-family:Helvetica;color:#000;line-height:1.2;pointer-events:all;white-space:normal;word-wrap:normal"><font face="Patrick Hand" style="font-size:18px">wasmer_engine::resolver::resolve_imports</font></div></div></div></foreignObject><text x="186" y="415" fill="#000" font-family="Helvetica" font-size="12" text-anchor="middle">wasmer_engine::resolver::resolve_imports</text></switch><path fill="none" stroke="none" d="M658.63 321l-.03 30H186.1v21.76" pointer-events="stroke"/><path fill="none" stroke="none" d="M186.1 378.76l-4-8 4 2 4-2z" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-dasharray="6 6" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M658.8100000000001 321.38c-.91 9.99.85 22.82.68 31.48m-1.54-31.16c.26 6.76 1 15.2-.32 28.73m.47-.31c-167.45.28-334.23.32-472.35.94m473.21-.15c-143.95 1.72-287.7 1.13-472.51.33m-2.32.35c3.19 7.43.4 16.97 1.77 23.07m.58-23.25c-.17 7.83-.85 13.94-.73 20.65" pointer-events="stroke"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M182.25 370.59s0 0 0 0m0 0s0 0 0 0m3.33 8.01c.81-2.05 2.1-2.55 6.33-6.5m-6.58 7.01c2.23-2.11 3.64-3.35 6.57-7.19" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M185.22 379.03c-.02-3.75-2.86-6.53-3.21-7.42m4.26 7.34c-1.36-2.55-3.02-5.84-4.32-8.51m.53.71c.94.05 1.76.56 3.97 1.87m-4.41-2.26c1.6.68 2.78 1.33 3.93 2.17m-.17.03c.76-.84 2.38-.96 4.71-2.4m-4.48 1.98c.76-.26 1.62-.73 4.18-1.8m.07.09c-.5.8-1.52 3.94-4.05 7.38m3.57-7.41c-.72 2.65-2.02 5.24-3.52 7.77" pointer-events="all"/><path fill="none" stroke="none" d="M391.5 381h333v60h-333z" pointer-events="all"/><path fill="none" stroke="#f0f0f2" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M391.22 381.32s0 0 0 0m0 0s0 0 0 0m.41 10.51c2.5-2.36 2.62-3.93 8.75-11.24m-8.48 13.43c2.51-5.02 7.07-7.94 9.17-12.54m-10.1 23.19c6.06-8.42 14.34-17.49 19.82-21.49m-19.64 21.32c5.01-4.85 9.54-10.61 20.38-22.52m-18.25 34.4c3.55-5.94 10.33-14.5 28.93-36.62m-30.17 37.77c9.37-10.01 17.09-20.76 30.25-37.06m-30.05 49.46c12.56-14.36 25.58-28.36 42.29-46.86m-42.56 47.26c14.89-18.49 28.84-34.34 42.09-48.73m-40.58 58.58c10.01-12.35 23.84-28.53 50.22-59.33m-52.64 59.78c17.71-18.67 35.26-38.98 54.16-60.18m-43.96 59.76c19.18-21.37 38.39-40.29 54.27-59.77m-52.39 60.45c16.59-18.24 32.48-37.43 51.92-59.16m-41.85 60.61c11.52-13.95 22.45-27.3 52.67-62.75m-52.17 61.95c18.45-22.67 38.9-44.39 52.46-59.67m-42.16 58.9c13.55-17.62 29.19-33.58 50.8-61.46m-50.98 61.61c12.48-14.08 25.95-28.98 52.02-60.83m-42.77 60.32c18.61-20.8 37.33-40.97 53.75-57.76m-51.82 57.77c9.79-11.29 19.99-24.97 51.51-59.9m-41.79 59.22c17.55-17.84 34.68-36.85 52.33-60.23m-51.43 61.08c14.01-16.72 30.47-34.82 51.51-59.31m-42.72 58.31c19.93-21.16 39.45-42.79 52.96-58.11m-51.02 60.72c9.74-14.91 22.58-27.78 51.21-61.64m-42.7 61.39c14.13-14.65 30.29-34.17 55.84-59.32m-54.19 57.73c18.49-20.05 37.17-41.39 51.51-58.62m-41.74 61.09c10.85-13.25 25.39-28.44 52.78-60.5m-51.53 59.29c14.33-17.04 29.22-33.48 51.94-60.15m-42.9 61.55c17.17-22.22 34.44-38.68 53.32-60.97m-52.55 60c18.58-22.19 37.92-43.06 53.67-61.28m-43.55 60.14c18.96-21.21 34.64-42.02 54.68-60.71m-54.4 60.11c12.74-14.17 24.42-28.31 52.56-59.18m-39.57 58.2c7.25-10.24 21.03-24.2 51.77-58.41m-53.65 60.9c16.57-19.81 34.85-40.42 52.13-60.26m-42.56 60.78c19.71-19.83 32.99-42.31 55.16-61.44m-55.05 60.85c13.41-16.07 26.42-28.6 52.49-61m-40.07 61.31c18.34-22.06 35.93-41.8 53.16-59.88m-53.4 59.03c20.86-24.69 41.57-48.86 51.49-61.1m-42.82 59.34c13.99-16.53 30.32-31.27 53.7-60.44m-52.21 61.26c20.35-24.35 41.81-47.16 51.6-60.36m-41.58 60.01c14.35-16.61 30.9-31.35 51.12-57.35m-52.24 58.3c19.52-23.4 41.29-46.42 53.58-60.54m-41.74 62.76c12.48-16.87 30.75-33.31 52.79-62.45m-52.94 60.2c17.56-19.88 35.7-40.5 52.26-58.52m-40.84 59.82c15.93-20.62 34.78-43.35 51.7-62.81m-52.8 63.22c19.11-22.55 36.57-43.72 52.04-60.85m-39.76 59.92c14.66-17.93 30.35-38.71 49.9-59.19m-51.6 58.55c13.78-14.49 26.16-31.01 53.36-59.23m-43.67 59.24c15.43-17.57 31.52-35.4 51.81-59.76M592.4 442.6c15.51-19.96 32.93-38.02 52.98-61.28m-39.91 62.14c17.38-22.75 35.01-43.99 49.52-61.4m-50.94 60.33c11.12-15.33 24.16-28.98 52.29-60.76m-43.49 61.14c15.84-15.63 26.99-28.47 52.81-59.66M614.83 442c15.03-17.55 30.68-35.13 52.36-60.59m-42.31 58.63c13.49-12.85 21.78-26.5 51.14-58.8m-50.99 59.59c18.51-19.4 36.29-39.2 52.92-59.86m-43.47 61.59c17.16-18.88 30.09-32.94 55.68-61.76m-54.01 61.29c13.69-15.89 28.29-33.48 51.34-60.79m-40.38 59.29c17.91-21.16 41.71-47.54 53.24-60.15m-54.11 61.5c16.17-19.89 33.37-39.21 51.64-61.79m-41.03 62.07c16.06-20.71 34.93-39.83 52.39-60.29m-52.88 59.62c14.48-17.87 28.53-33.49 53.53-60.39m-41.21 60.96c19.15-22.62 40.62-48.43 53.31-61.18m-54.02 61.27c16.55-21.95 36.26-41.3 51.34-60.3M678.6 440.4c13.81-12.42 25.34-26.57 51.3-57.47m-52.56 58.49c13.66-14.01 24.98-26.09 50.45-57.36m-39.95 55.34c12.87-15.35 26.91-29.06 38.71-41.51M688.93 442c8.52-9.8 16.5-20.19 40.27-46.57m-30.14 47.08c12.21-12.19 23.21-25.5 30.02-34.66m-29.23 34.28c8.78-12.5 18.73-23.15 28.32-32.37m-17.63 33.23c5.29-6.67 7.92-11.42 17.51-20.17m-18.85 17.7c8.2-5.53 14.02-14.12 18.91-20.62m-7.59 22.17c2.44-3.79 4.33-7.41 8.3-9.09m-8.18 7.81c2.7-2.13 4.67-4.31 7.64-8.17" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M391.92 381.9c93.25-1.64 189.88-1.72 331.52-.03m-332.27-.38c86.89.1 174.87-.76 333.79-.97m-1.82-.1c1.19 13.84 3.26 30.27 1.63 60.47m-.52-59.75c-.68 15.48.29 32.56-.03 60.25m.79.3c-92.21-3.08-185.84-2.95-333.62.13m332.95-.91c-87.26 1.72-173.02 1.13-333.13.25m1.79.11c-.18-18.88-.9-32.83.37-61.92m-2.13 61.64c.29-19.4-.93-39.86.04-59.8" pointer-events="all"/><switch transform="translate(-.5 -.5)"><foreignObject width="100%" height="100%" pointer-events="none" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow:visible;text-align:left"><div xmlns="http://www.w3.org/1999/xhtml" style="display:flex;align-items:unsafe center;justify-content:unsafe center;width:331px;height:1px;padding-top:411px;margin-left:393px"><div style="box-sizing:border-box;font-size:0;text-align:center"><div style="display:inline-block;font-size:12px;font-family:Helvetica;color:#000;line-height:1.2;pointer-events:all;white-space:normal;word-wrap:normal"><font face="Patrick Hand" style="font-size:18px">wasmer_engine::Tunables::create_memories</font></div></div></div></foreignObject><text x="558" y="415" fill="#000" font-family="Helvetica" font-size="12" text-anchor="middle">wasmer_engine::Tunables::create_memories</text></switch><path fill="none" stroke="none" d="M351.75 411h20.05-.3 11.76" pointer-events="stroke"/><path fill="none" stroke="none" d="M389.26 411l-8 4 2-4-2-4z" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M351.2 411.65000000000003c7.45-.61 9.37-.02 21.11.45m-20.98-1c6.76-1.08 13.33.34 20.43-.28m.06.19c-.12.01-.28-.02-.33-.01m.32 0c-.09-.01-.2.01-.31 0m-1.09.11c3.9-.75 6.06 1.16 13.89.86m-12.84-1.11c3.78.13 6.91-.28 12.08-.3" pointer-events="stroke"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M381.2 407.06s0 0 0 0m0 0s0 0 0 0m2.61 9.85c1.46-1.77 3.07-3.07 4.12-6.4m-4.77 6.63c1.92-2.08 3.39-3.97 5.37-6.16" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M388.43 411.09c-.86.2-3.07 2.56-6.38 4.65m7.18-4.85c-2.34 1.41-5.17 2.32-7.76 3.77m-.4.35c1.1-1.23 1.85-2.49 1.88-4.12m-1.86 4.1c.88-1.76 1.78-3.09 2.08-3.96m.06-.01c-.45-.65-.65-1.37-1.72-3.88m1.92 3.87c-.7-1.27-1.41-2.37-2.34-4.2m-.36-.07c2.03 1.65 4.74 2.65 8.18 4.76m-7.88-4.85c2.3 1.12 4.31 2.92 8.65 4.46" pointer-events="all"/><path fill="none" stroke="none" d="M763 381h318.5v60H763z" pointer-events="all"/><path fill="none" stroke="#f0f0f2" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M762.9 381.11s0 0 0 0m0 0s0 0 0 0m-.97 12.39c4.85-4.58 7.17-8.77 11.89-13.66m-11.53 12.63c5.46-4.34 9.12-8.67 11.1-12.1m-10.09 24.48c9.1-6.61 16.62-19 22.24-23.21m-22.84 22.69c8.19-7.16 15.12-14.48 21.79-22.41m-19.98 37.15c11.87-14.42 20.92-30.1 30.26-39.42m-30.62 38.27c6.31-9.32 12.72-15 31.49-36.94m-33.77 48.69c15.54-15.78 31.57-36.4 43.84-50.04m-42.7 50.94c15.95-18.59 31.2-36.52 42.97-50.5m-44.14 62c11.53-14.51 21.17-26.86 54.39-61.97m-53 60.81c14.54-15.76 26.9-30.9 52.91-59.64m-40.94 61.34c15.46-20.23 30.62-40.64 53.28-61.03m-53.7 59.81c14.17-17.53 29.97-34.62 52.24-60.11m-40.4 59.49c18.76-22.56 39.3-46.61 48.76-60.7m-49.83 62.17c20.64-25.26 41.17-49.42 50.95-61.53m-39.09 61.45c11.12-17.68 23.27-31.1 49.69-61.6m-50.89 59.68c17.99-20.7 37.65-43.73 52-58.94m-42.1 60.09c14.22-19.51 29.56-33.84 51.48-58.87m-52.37 58.79c18.24-20.41 33.85-38.99 53.05-60.17m-43.22 58.43c24-24.34 43.32-48.71 56-58.17m-53.56 60.35c15.31-17.94 30.48-37.13 52.55-61.76m-41.25 61.77c11.5-14.58 19.16-28.02 51.68-59.62m-53.26 59.34c17.8-20.67 33.89-40.95 51.65-61.01m-42.15 62.07c21.42-23.29 42.62-48.12 53.36-61.87m-51.54 60.67c20.33-25.21 41.49-49.05 52.31-59.99m-43.44 58.11c18.18-17.15 34.17-35.91 54.48-58.07m-53.5 58.87c17.46-19.22 35.65-41 53.18-59.49m-42.96 62.52c13.48-15.23 24.36-33.19 51.55-64.6m-51.68 62.64c18.72-22.22 38.62-44.28 52.38-59.83m-40.58 59.05c8.47-11.62 21.62-26.39 53.01-57.34M868.78 442c14.14-14.9 26.87-31.21 52.09-61.04m-41.71 62.53c19.49-20.27 36.59-41.87 53.71-63.19m-52.59 61.58c19.32-22.59 37.66-43.29 51.22-60.44m-39.54 59.17c12.18-17.82 26.99-35.26 52.38-60.55m-53.99 60.9c12.73-12.72 24.46-26.69 52.79-59.11m-40.05 61.01c12.66-17.7 27.59-35.3 50.94-63.05m-53.19 61.77c15.83-17.38 30.87-34.38 53.24-59.56m-40.8 59.19c20.22-23.62 39.18-45.43 49.75-58.84m-52.15 59.37c23.33-24.29 43.17-47.85 53.75-61.11M924 440.1c8.67-11.36 18.32-21.92 51.71-58.55m-54.14 60.78c18.26-21.35 36.99-41.23 54.23-61.83m-43.81 62.43c19.1-23.05 42.06-45.24 54.22-63.32m-53.47 62.74c17.59-21.82 36.46-42.28 53.38-60.5m-42.36 59.79c17.04-17.47 31.05-38 53.15-60.8m-53.17 61.36c12.56-16.26 26.62-31.43 52.62-60.7m-42.29 60.6c17.93-17.39 31.94-37.58 50.91-61.13m-50.84 59.94c12.28-11.73 23.07-26.75 52.14-60.73m-41.81 61.51c17.59-19.33 36.74-41.95 52.38-60.94m-52.33 60.63c12.01-13.89 23.33-27.45 52.85-59.62m-43.61 61.02c14.69-12.15 25.33-24.82 52.96-63.38m-51.53 62.25c13.23-14.07 25.45-28.4 52.45-61.2m-43.13 61.76c22.74-23.54 43.26-50.91 54.52-62.09m-52.07 62.04c12.13-16.64 27.1-33.1 51.35-62.02m-40.76 60.66c17.14-19.02 34.37-38.84 53.48-61.2m-55.17 61.36c20.51-20.75 40.47-42.91 52.76-59.38m-42.14 59.3c14.47-14.69 26.41-27.08 52.71-59.37m-51.24 59.48c17.58-19.75 35.69-39.79 52.58-59.34m-44.36 58.09c15.19-16.69 29.21-33.54 53.61-57.73M1018 441.32c11.96-14.28 24.39-29.19 53.5-60.7m-42.25 62.55c20.75-22.96 37.28-45.8 52.06-63.92m-53.01 61.16c16.02-16.33 32.05-35.46 52.94-59.01m-40.63 61.52c9.92-14.88 24.8-31.92 45.88-58.79m-47.15 57.21c13.22-15.86 26.69-30.41 48.52-55.29m-39.93 56.21c12.05-11.67 20.38-21.86 38.62-45.49m-35.9 44.33c11.98-14.19 26.18-31.41 35.82-43.11m-24.35 43.42c5.2-7.97 13.74-18.57 23.59-29.93m-24.24 30.31c6.77-9.99 15.96-20.27 26.44-31.05m-18.03 28.37c4.6-.28 6.78-6.35 18.04-17.03m-16.37 19.81c4.87-7.85 10.8-13.65 16.15-18.9" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M762.64 381.21c91.05.23 180.64.92 318.61-.44m-318.35.22c90.66 1.75 180.03 1.87 317.97-.67m-1.19.35c1.08 17.87 3.83 32.48 1.48 61.35m-.61-60.68c.85 19.53.85 39.59.07 59.41m.75-.59c-93.19-.32-186.93-.57-318.6 0m318.61 1.35c-75.23-1.39-150.28-.79-318.72-1.01m-.88 2.49c1.97-25.33-.49-46.79 1.68-62.84m.5 60.84c-.47-18.88-1.18-35.41-1.74-59.81" pointer-events="all"/><switch transform="translate(-.5 -.5)"><foreignObject width="100%" height="100%" pointer-events="none" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow:visible;text-align:left"><div xmlns="http://www.w3.org/1999/xhtml" style="display:flex;align-items:unsafe center;justify-content:unsafe center;width:317px;height:1px;padding-top:411px;margin-left:764px"><div style="box-sizing:border-box;font-size:0;text-align:center"><div style="display:inline-block;font-size:12px;font-family:Helvetica;color:#000;line-height:1.2;pointer-events:all;white-space:normal;word-wrap:normal"><font face="Patrick Hand" style="font-size:18px">wasmer_engine::Tunables::create_tables</font></div></div></div></foreignObject><text x="922" y="415" fill="#000" font-family="Helvetica" font-size="12" text-anchor="middle">wasmer_engine::Tunables::create_tables</text></switch><path fill="none" stroke="none" d="M724.5 411h20-1.5 11.76" pointer-events="stroke"/><path fill="none" stroke="none" d="M760.76 411l-8 4 2-4-2-4z" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M725.6300000000001 410.81c5.6.2 13.68-1.08 19.81-1.16m-21.93 1.27c7.26.49 12.01.42 21.12.44m-.12-.39c-.41.18-.82.06-1.44-.08m1.43.1c-.61.04-1.13.01-1.47-.05m.66 1.24c2.29-1.18 4.16-1.43 11.83-1.66m-12.03-.09c2.09.11 4.27.89 10.89 1.14" pointer-events="stroke"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M752.88 406.86s0 0 0 0m0 0s0 0 0 0m2.42 9.94c1.35-2.24 3.33-3.19 5.67-6.04m-5.61 6.25c1.5-2.53 2.94-4.49 4.61-6.49" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M761.29 411.89c-2.09.03-4.51.75-7.95 2.74m7.79-4.06c-2.12.83-4.17 2.31-8.66 4.87m-.07-.32c.55-1.3 1.33-1.55 2.78-4.4m-2.54 4.35c.61-1.15.89-1.83 2.08-3.88m.06-.41c-.9-.93-1.7-2.37-2.11-3.41m2.29 3.57c-.68-1.24-1.56-2.7-2.07-3.79m-.95.34c2.95 1.39 5.13 2.1 8.85 2.81m-7.68-3.7c2.01 1.73 5.02 2.89 7.26 4.62" pointer-events="all"/><path fill="none" stroke="none" d="M1280.25 381l.05-30H801.9l-.02-21.76" pointer-events="stroke"/><path fill="none" stroke="none" d="M801.88 323.24l4 7.99-4-1.99-4 2z" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-dasharray="6 6" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M1282.1100000000001 380.55c-.97-7.89-3.42-15.91-.49-30.14m-.61 31.1c-.03-6.62.06-14.49-1.5-30.54m1.65.58c-134.06-.73-266.19-.35-479.01-1.09m478.18.52c-164.88 1.62-330.37 1.19-478.02-.07m.65-1.51c.47-6.36-2.81-13.74-1.76-20.73m-.01 22.01c.88-5.29.34-9.98 1.21-22.42" pointer-events="stroke"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M801.72 323.43s0 0 0 0m0 0s0 0 0 0m3.38 8.25c.27-.09.26-.3.69-.68m-.63.74c.19-.3.44-.54.63-.85" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M802.35 322.53000000000003c1.94 4.16 2.06 7.88 3.23 8.44m-4.02-7.87c1.34 2.07 2.1 4.42 4.55 7.69m-.01.04c-1.79.16-3.08-.82-4.03-1.5m3.8 1.97c-1.61-.8-3.13-1.61-4.08-2.18m-.16.08c-.88.68-2.8 1.62-3.68 2.08m3.9-1.88c-.96.25-1.58.78-4 1.71m-.07.95c1.76-3.46 1.63-4.52 4.62-8.79m-4.23 7.96c1.36-2.66 2.59-5.8 3.97-8.07" pointer-events="all"/><path fill="none" stroke="none" d="M1120.25 381h320v60h-320z" pointer-events="all"/><path fill="none" stroke="#f0f0f2" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M1120.56 380.64s0 0 0 0m0 0s0 0 0 0m.2 11.82c1.9-1.19 4.32-5.58 8.62-9.67m-9.78 10.23c2.87-2.65 5.14-4.37 11.43-11.67m-10.09 24.18c8.15-8.39 14.91-16.1 20.66-24.95m-22.02 24.08c5.65-5.06 10.77-11.13 21.19-24.3m-19.7 37.89c9.15-11.03 18.19-22.59 31.59-38.21m-33.07 36.77c9.55-11.25 20.94-23.1 32.73-34.82m-30.19 46.4c5.65-10.98 15.82-20.01 40.08-47.72m-41.52 49.13c17.51-18.99 33.57-38.45 42.52-48.76m-43.69 59.41c19.54-19.69 35.5-40.3 54.56-60.62m-53.36 61.85c18.66-22.93 38.32-44.97 51.66-61.1m-41.92 60.93c15.74-16.16 30.85-34.04 52.3-62.3m-51.01 63.18c13.74-15.89 27.46-32.87 52.55-61.06m-42.94 58.36c19.5-19.1 39.16-41.76 52.85-58.65m-51.5 60.4c11.54-14.83 23.66-27.35 51.45-59.39m-42.02 57.99c12.68-12.69 21.34-23.56 52.58-57.75m-52.81 59.77c15-19.26 30.36-36.33 53.99-61.07m-41.43 61.87c9.83-18.22 21.77-30.71 49.2-59.99m-50.93 57.82c13.43-14.51 25.93-27.9 52.61-60.44m-40.67 59.64c14.76-13.72 25.76-31.36 51.69-60.12m-51.7 61.62c15.41-17.16 31.28-35.42 51.59-60.2m-41.77 59.24c14.77-16.08 29.07-33.4 50.38-61.14m-51.49 61.97c14.57-15.41 27.42-29.03 54.1-60.38m-43.61 58.75c18.21-19.86 33.96-37.92 54.89-60.43m-52.89 61.6c11.68-13.24 24.81-27.49 51.11-59.35m-43.15 59.12c20.3-20 41.74-42.32 56.18-59.63m-53.48 60.44c19.81-24.34 39.29-47.47 52.44-60.83m-41.71 61.99c19.96-22.89 38.62-46.07 52.47-62.71m-53.49 61.76c21.57-24.64 41.54-47.31 51.76-60.87m-42.29 62.45c16.47-21.06 33.59-37.86 55.33-61.96m-54.58 59.46c14.31-14.67 28.11-31.16 53.6-58.89m-41.84 60.86c14.45-18.24 28.36-35.39 51.65-59.97m-51.53 58.26c18.85-22.33 39.26-45.12 52.31-60.63m-42.53 61.56c13.87-16.86 26.2-33.25 53.61-59.66m-52.94 58.94c10.05-12.42 22.51-25.81 51.09-61.2m-42.02 62.71c21.47-25.43 40.25-48.96 52.72-62.23m-50.98 60.5c18.51-23.08 40.37-44.99 50.99-60.01m-40.78 61.04c14.75-19.67 29.83-36.49 53.8-61.22m-54.09 59.46c10.6-12.14 20.31-24.38 51.89-59.86m-42.32 59.68c18.87-18.4 36.3-37.07 52.95-60.5m-52.71 62.01c16.05-17.58 31.98-34.62 53.54-59.86m-40.53 60.86c11.21-18.21 28.36-34.07 51.65-60.2m-54.01 59.29c18.84-20.83 36.44-41.13 52.5-60.5m-42.54 60.52c14.33-14.51 27.68-33.03 55.27-59.55m-54.2 60.27c18.75-23.39 38.14-44.89 51.86-61.31m-43.48 62.03c15.99-17.9 32.31-33.5 55.92-63.07m-54.07 61.33c20.48-24.09 40.18-47.68 52.91-60.22m-41.94 61.78c19.73-25.09 37.96-44.5 52.95-62.13m-53.2 61.89c19.01-23.9 36.73-45.03 52.73-61.97m-43.31 61.92c14.5-17.62 28.64-33.8 52.39-61.37m-52.11 60.18c13.33-13.87 24.73-28.87 52.61-59.65m-42.79 60.38c20.36-22.33 39.76-44.15 52.25-60.52m-51.32 59.19c13.24-13.73 25.52-27.08 52.35-59.45m-39.22 60.7c11.5-16.08 23.17-30 51.82-59.61m-53.34 59.13c19.3-24.09 41.44-46.62 52.59-60.9m-42.82 59.02c10.59-9.46 21.53-24.17 53.76-56.99m-52.47 59.66c10.16-14.57 21.3-26.99 51.81-61.88m-42.34 62.89c18.98-23.25 37.71-46.63 53.98-63.31m-53.44 61.12c18.23-20.75 36.39-41.31 53.35-59.61m-41.48 59.03c20.26-25.21 42.43-47.76 51.75-59.55m-53.16 60.7c16.78-19.83 34.79-40.49 52.37-61.63m-40.06 60.09c15.15-15.5 31.04-33.11 47.7-54.46m-49.53 55.21c10.47-10.69 19.4-22.4 48.94-56.33m-38.39 55.97c9.67-9.94 22.19-24.09 39.06-44.6m-38.36 45.26c10.87-12.05 21.53-25.33 38.31-43.95m-27.87 42.6c11.49-11.51 20.42-22.39 25.61-29.46m-26 31.61c6.48-7.22 12.26-14.38 27.74-32.62m-15.05 30.75c1.19-5.91 6.79-10.23 15.83-19.24m-17.28 21.17c2.68-5.08 5.89-9.5 15.77-21.27m-5.36 20.59c1.14-2.92 2.68-4.73 7.05-8m-7.69 7.71c1.77-1.39 3.82-3.57 7.17-7.87" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M1119.09 380.48c93.45.48 183.31-.85 321.75-.84m-320.44.84c97.28-1.08 195.05-1.36 319.51 1.07m2.06-.63c-.24 17.9-.79 34.69-2.67 58.23m1.3-57.61c-.42 11.57.6 24.63.18 58.56m-1.33 1.32c-98.66-3.14-196.95-.97-319.55-.14m320.27-.57c-68.68 2.79-135.76 1.8-320.3.52m.44-.52c.11-15.79-.08-32.75-1.01-59.77m1.12 60.05c-1.23-16.36.57-30.97.49-59.82" pointer-events="all"/><switch transform="translate(-.5 -.5)"><foreignObject width="100%" height="100%" pointer-events="none" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow:visible;text-align:left"><div xmlns="http://www.w3.org/1999/xhtml" style="display:flex;align-items:unsafe center;justify-content:unsafe center;width:318px;height:1px;padding-top:411px;margin-left:1121px"><div style="box-sizing:border-box;font-size:0;text-align:center"><div style="display:inline-block;font-size:12px;font-family:Helvetica;color:#000;line-height:1.2;pointer-events:all;white-space:normal;word-wrap:normal"><font face="Patrick Hand" style="font-size:18px">wasmer_engine::Tunables::create_globals</font></div></div></div></foreignObject><text x="1280" y="415" fill="#000" font-family="Helvetica" font-size="12" text-anchor="middle">wasmer_engine::Tunables::create_globals</text></switch><path fill="none" stroke="none" d="M1081.5 411h20-1.2 11.71" pointer-events="stroke"/><path fill="none" stroke="none" d="M1118.01 411l-8 4 2-4-2-4z" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M1080.3200000000002 409.97999999999996c7.76 1.4 14.01 2.24 22.54 1.21m-20.91-.46c3.77.07 8.72.51 19.86 1.17m-.36-.97c-.19-.03-.45.17-1.25.14m1.26-.07c-.48-.06-.87-.06-1.13.03m.09-.14c3.02.74 4.59.69 12.06.52m-12.33-.24c3.91-.15 7.46-.51 11.99.24" pointer-events="stroke"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M1109.89 407.14s0 0 0 0m0 0s0 0 0 0m2.23 10.05c1.23-2.71 3.58-4.92 5.6-5.69m-5.64 5.08c.71-.99 1.87-2.59 5.46-6.04" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M1118.1000000000001 410.91c-1.52 1.64-4.14 2.5-7.73 4.4m7.53-4.18c-2.62 1.23-5.5 2.28-7.79 4.18m.27-.08c.33-1.32.64-3.15 1.89-3.79m-2.33 3.7c.37-1.33 1.33-2.17 2.07-4.24m.07.53c-.96-1.26-1.05-2.62-2.49-4.72m2.21 4.16c-.66-1.22-1.27-2.57-1.8-3.82m.7-.59c1.99 2.74 5.43 3.16 7.73 4.42m-8.38-4.33c3.52 2.33 5.72 3.76 7.65 4.78" pointer-events="all"/><path fill="none" stroke="none" d="M873.5 291h20-1.5 11.76" pointer-events="stroke"/><path fill="none" stroke="none" d="M909.76 291l-8 4 2-4-2-4z" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M873.0400000000001 289.71c6.55 1.7 15.78.1 22.21 2.24m-21.55-.62c6.57-.58 15.22.61 19.18.18m.7-.47c-.48.01-.71-.17-1.43.06m1.3-.09c-.6-.07-1.14.03-1.52-.08m.12.19c3.43.9 8.22-.53 11.03.44m-10.96-.01c4.08-.17 8.11-.86 11.28-.11" pointer-events="stroke"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M901.68 287.1s0 0 0 0m0 0s0 0 0 0m1.65 9.75c2.1-1.17 2.38-2.52 5.51-6.68m-5.12 7.09c1.82-2.79 3.81-5.61 5.12-6.28" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M909.8000000000001 291.09c-4.07 2.42-7.12 2.91-8.56 4.34m8.61-4.01c-3.05 1.48-6.12 2.41-8.36 3.91m.32-.62c.54-.62 1.31-1.28 1.98-3.79m-1.99 4.23c.45-.94.99-1.85 1.92-4.11m-.16-.32c-.14-1.03-.62-2.23-2.06-3.45m2.27 3.82c-.51-1.07-.86-1.77-2.19-3.87m-.21-.43c3.68 1 5.84 2.75 7.5 4.82m-7.14-4.67c1.86.92 4.02 1.91 8.38 4.1" pointer-events="all"/><path fill="none" stroke="none" d="M1087 261v-30H949.5v-21.76" pointer-events="stroke"/><path fill="none" stroke="none" d="M949.5 203.24l4 8-4-2-4 2z" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-dasharray="6 6" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M1088.26 261.35c-1.87-10.09.18-21.9-3.03-32.34m1.33 32.19c-.13-12.08.6-23.48.25-30.41m1.2.46c-49.38-.09-96.24 2.02-138.01-1.79m137.85.75c-51.66-.04-100.83.35-137.58.28m.92 1.57c-.06-5.42-.28-10.63-3.6-23.46m1.84 22.18c-.91-7.6.3-15.8.54-22.21" pointer-events="stroke"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M949.72 202.99s0 0 0 0m0 0s0 0 0 0m2.8 9.11c.19-.29.28-.57.59-.79m-.64.66c.27-.25.4-.45.66-.69" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M950.25 203.70999999999998c1.05 1.71 1.88 3.51 2.4 7.24m-3.18-7.81c1.2 3.52 3.3 6.78 4.24 7.8m-.07.72c-1.82-.96-2.94-1.74-4.43-2.54m4.3 2.33c-1.3-1.11-2.74-1.64-4.01-2.32m-.35.23c-1.22.11-2.21 1.27-3.34 1.9m3.65-2.16c-1.21.6-2.29 1.42-4.03 2.13m-.25.59c1.93-2.59 2.73-5.03 4.59-7.99m-4.71 7.21c1.16-1.65 3.05-4.03 4.12-7.69" pointer-events="all"/><path fill="none" stroke="none" d="M917 261h340v60H917z" pointer-events="all"/><path fill="none" stroke="#f0f0f2" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M917.16 260.81s0 0 0 0m0 0s0 0 0 0m-.75 11.12c3.93-2.94 5.88-2.88 10.07-12.52m-8.71 12.68c2.31-2.33 5.84-6.27 9.84-12m-11.32 24.92c5.89-7.32 14.23-16.96 23.44-23.8m-23.12 24.86c5.62-7.03 11.43-14.38 22.54-25.82m-22.39 38.15c8.3-10.99 14.83-18.61 30.84-36.03m-30.82 36.04c10.68-12.59 21.67-24.44 33.27-38.23m-31.81 48.88c14.51-15.39 29.67-33.72 42.81-46.88m-43.28 47.05c12.92-15.87 28.32-33.27 41.55-49.08m-43.53 61.17c22.54-24.56 44.03-47.22 56.2-62.02m-54.48 62.56c20.77-24.24 41.44-47.42 51.99-62.09m-40.91 61.73c13.89-16.42 32.11-37.54 51.61-59.14m-52.81 58.88c20.59-22.82 41.74-47 53.61-60.69m-40.63 59.72c12.42-12.99 25.91-31.72 51.41-59.27m-53.94 61.08c12.77-14.16 25.31-29.17 53.93-61.26m-41.07 61.67c13.76-18.02 29.8-39.41 52.47-61.94m-54.33 62.06c12.59-15.74 24.62-28.96 53.6-60.46m-41.99 59.32c11.12-14.8 26.26-30.66 51.12-60.32m-51 61.39c12.06-17.41 26.73-31.42 51.47-62.22m-40.43 60.72c10.54-14.62 21.43-26.65 50.58-60.44m-51.8 60.44c9.91-12.04 21.64-23.66 51.96-59.07m-43.18 59.78c17.09-16.9 31.92-38.13 52.99-61.24m-50.22 60.14c15.08-17.02 30.44-36.22 51.04-60.06m-41.94 62.32c17.02-19.77 31.51-38.17 51.3-62.49m-50.13 60.81c18.57-20.73 36.9-42.79 52.25-60.83m-40.61 60.39c11.67-15.25 23.71-27.99 49.39-60.88m-50.66 61.99c19.26-21.4 36.4-42.93 52.7-61.57m-44.38 62.34c14.53-17.08 29.78-31.13 53.97-61.93m-52.98 60.23c13.15-13.43 24.34-28 53.06-59.05m-42.97 59.13c14.66-15.5 32.82-34.4 54.66-60.08m-53.13 60.93c15.05-17.23 28.57-32.68 51.56-60.61m-39.78 58.42c12.3-13.36 22.95-26.52 51.93-58.61m-52.81 60.08c15.56-17.52 29.94-36.65 51.97-60.11m-41.11 58.77c20.72-20.86 37.69-43.37 53.14-57.69m-54.22 60.18c15.29-17.17 29.8-34.27 53.66-61.99m-43.34 61.87c22-23.57 44.36-49.94 52.51-62.36m-50.84 61.97c15.79-20.83 32.51-39.83 51.54-61.28m-42.12 60.72c22.39-22.18 41.5-46.98 51.16-58.55m-50.3 58.28c12.9-15.13 25.51-29.21 52.11-59.56m-41.55 61.77c9.45-15.58 23.61-28.04 53.08-61.03m-53.12 59.82c19.65-23.92 39.9-47.05 51.38-60.93m-41.33 59.15c20.91-22.91 37.54-44.4 53.1-57.5m-53.55 59.03c11.93-11.98 22.25-25.56 52.96-60.12m-42.04 58.87c15.84-18.88 36.8-37.99 52.96-60.47m-52.42 61.83c14.02-17.77 28.82-33.66 52.03-60.37m-40.24 60.95c10.56-15.25 27-33.98 49.26-63.39m-49.89 61.53c11.88-14.83 25.51-29.23 51.31-59.52m-39.72 59.66c13.54-17.61 28.35-31.61 49.5-60.52m-50.33 60.5c14.09-17.3 29.99-34.75 51.1-59.93m-40.63 59.69c17.65-21.12 38.98-46.33 52.03-59.05m-53.07 60.13c14.43-15.81 25.94-30.6 53.24-60.57m-40.7 58.35c18.93-21.4 36.81-42.97 50.06-59.02m-51.22 60.56c12.86-13.67 25.2-28.99 51.14-60.94m-43.09 62.48c19.82-24.06 41.56-44.16 56.36-62.1m-55.2 61.26c15.74-17.38 30.87-36.28 54.46-61.87m-42.95 61.63c11.3-12.78 25.8-27.2 51.96-59.81m-52.22 61.06c12.71-15.98 26.87-30.71 51.75-61.73m-41.86 60.07c11.44-9.31 22.45-21.36 52.67-61.1m-51.65 61.56c14.23-16.23 28.1-31.7 51.1-59.98m-39.05 60.88c12.4-15.54 27.5-33.44 50.56-60.74m-53.27 59.33c18.85-18.72 36.46-41.11 53.51-59.62m-41.3 60.48c9.89-12.61 21.69-26.51 53.24-59.18m-53.41 59.54c14.12-17.56 30.39-35.76 52.2-60.99m-42.74 59.73c18.97-22.69 41.72-48.24 51.2-57.67m-51.06 58.98c14.35-15.01 27.07-29.51 51.97-60.95m-41.37 59.44c16.11-16.54 30.66-30.86 49.84-53.88m-48.76 55.39c14.73-16.42 28.66-32.5 47.25-56.44m-36.4 53.97c9.42-11.59 19.61-22.35 38.79-43.81m-38.65 45.27c7.3-9.07 13.57-18.62 37.3-42.44m-26.51 41.53c6.03-7.31 9.69-14.67 27.04-30.19m-28.54 32.03c10.34-11.44 20.08-23.8 27.54-32.31m-16.44 32.57c6.52-8.06 9.83-10 17.88-19.33m-17.53 18.68c6.06-6.86 11.28-14.14 16.3-18.71" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M917.8 261.81c94.98 1.46 193.99.25 339.09-1.18m-339.62.32c109.6-.09 219.8-.71 339.52-.2m.49-.56c.03 22.76-.46 46.75 1.09 59.75m-1.39-58.58c.05 22.82.75 46.62-.65 59.95m.27-.04c-123.81 1.14-245.98.7-340.43-1.14m340.51 1.15c-106.86-2.48-212.98-2-339.89.08m1.37 1.18c-2.45-22.99-.88-45.45-2.95-62.58m1.29 60.98c-.56-13.23-.09-25.94.94-60.12" pointer-events="all"/><switch transform="translate(-.5 -.5)"><foreignObject width="100%" height="100%" pointer-events="none" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow:visible;text-align:left"><div xmlns="http://www.w3.org/1999/xhtml" style="display:flex;align-items:unsafe center;justify-content:unsafe center;width:338px;height:1px;padding-top:291px;margin-left:918px"><div style="box-sizing:border-box;font-size:0;text-align:center"><div style="display:inline-block;font-size:12px;font-family:Helvetica;color:#000;line-height:1.2;pointer-events:all;white-space:normal;word-wrap:normal"><font face="Patrick Hand" style="font-size:18px">wasmer_engine::Artifact::finish_instantiation</font></div></div></div></foreignObject><text x="1087" y="295" fill="#000" font-family="Helvetica" font-size="12" text-anchor="middle">wasmer_engine::Artifact::finish_instantiation</text></switch></svg>

'''
'''--- assets/diagrams/Diagram_module_serialization.svg ---
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="762" height="222" content="&lt;mxfile host=&quot;app.diagrams.net&quot; modified=&quot;2020-07-20T09:20:34.245Z&quot; agent=&quot;5.0 (Macintosh)&quot; version=&quot;13.4.8&quot; etag=&quot;rDKBPPV5QoUVm1hLuxiG&quot; type=&quot;device&quot;&gt;&lt;diagram id=&quot;2alLs4ajSNHMKkMwH0tN&quot;&gt;7VhLc5swEP41PsYDwmD76GdzSKeZ8aHtqaOADGoEokKO7f76rkDiEXDiSeLYM+4kM9audlfax7cs9JxZvPsicBp95QFhPWQFu54z7yGEhkMLfhRnX3AGnlcwQkGDgmVXjBX9SzRT64UbGpCsISg5Z5KmTabPk4T4ssHDQvBtU2zNWfPUFIekxVj5mLW532kgo4I7QsOKf0toGJmTbW9c7MTYCGtPsggHfNtgkZ1c8kTqK35LSQLMFU6ynruIpFQuTnpoCf9rJdYPOQ8ZwSnN+j6Pge1nILJc45gyFd/CwjS3MJzdYymo/wgmb3ESvNFkaWSaG4FrO4ueMxOcy2IV72aEqZybdBa+LQ/slvEUJJHHKKBC4QmzjU6JDqfcmxyFgm9SLUaEJLuuysAPRtxqX8EuHYNCJjwmUuxBRBu6sdFIK+kqHht6W9WEZ3hRvR4ME+skh6X1ymdYaLe7Q+B0hMBjcMJU5bARC+/PhpuNmyxHE6TbskfprtpUOMB+U+FZsdRkvVD9bnEWE6GsORNA+YaRYp0RQTFTB+kr6coyeq1kQa6SgCjPLGU2opKs0uI2W2ghwItkDJGY27DMHon0I0NIwR/JjDMuclPOfGwNXKS8pYzV+EsL/lCpYXCLyut0VEm9GtAr1dDOPbI6cu99QOoHF5x66MtXl/xBV/LRiZLvvt76SABPK01yISMe8gSzRcWdNkNeydxxnurY/iZS7vWjF28kf1MaDoVb3fDlYINDfCN0UZpmJ7EIiayBoJ0SQRiW9Klp/T3x9g6CLaBPR2MtB84zCL0JtF2GNhlRTubFhgNG4GENIU5CmhzGYMnOvWhyL8axNRSUGohwnLLSlQfxIZc7MiId0LrDDzDPNuAA/S5MYO1DEauuOFX9hMLAONEbMQ2CAnkE7qjHDoW9lFM967nTnjvXjmjc2SNNL1uTV9GWX2hfx0w47svPNKtvjS09uR6NK23uXvlVE+HrdQbQfQ688tSjsDg8R6sjOyp/KPX+0NXkT21Nree7OrE3RAJO1bUU/dNYVESll1NGMcBZlN/2DF121O6yzid12dHFjDS/dOPMp5kJoBjsyKsdawfNVxwbdUw63okmnfF50Y7qaLePRTtqoN26YLR3zFSjT0K7mZf/N/LTpNa227n9rHnZnH3ZrfwaX1Odoft6Mx+fqJnbZ3lNvZ5uPmgj3j7wAfO9kAey+tJbTPHVZ35n8Q8=&lt;/diagram&gt;&lt;/mxfile&gt;" version="1.1" viewBox="-.5 -.5 762 222"><defs/><defs><style type="text/css">@import url(https://fonts.googleapis.com/css?family=Open+Sans);@import url(https://fonts.googleapis.com/css?family=Patrick+Hand);</style></defs><path fill="none" stroke="none" d="M51 21h200v60H51z" pointer-events="all"/><path fill="none" stroke="#f0f0f2" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M51.22 20.74s0 0 0 0m0 0s0 0 0 0m-1.77 13.65c4.15-4.22 9.68-8.41 11.86-14.74M50.74 34.04c4.08-5.06 8-11.2 10.87-13.76M48.96 46.6c9.71-9.97 18.89-21.24 24.07-24.27M50.36 45.5c7.34-7.92 15.49-17.32 21.65-23.92M51.23 55.7c8.33-7.48 20.74-20.75 33.42-33.87M50.04 57.38c8.51-8.11 17.52-17.69 32.56-36.18M51.42 68.63c13.1-12.25 24.19-28.4 41.63-47.11m-42.64 49.1C61.78 58.01 72.02 46 92.98 20.61m-43.4 61.9c16.63-17.66 32.55-34.9 53.57-61.76M50.7 80.71C70.19 62 87.18 41.14 104.19 20.39M61.74 80.95c16.21-17.15 35.73-38.46 52.97-59.6M62.29 82.13c18.87-23.76 38.94-44.5 52.54-60.02M71.26 80.43c13.48-14.18 25.99-24.76 54.51-59.61M73.36 81.97c9.49-13.09 20.44-25.16 51.4-60.29m-39.7 60.61c8.23-12.57 22.73-29.21 49.71-63.01M83.43 82.12c15.73-18.73 30.1-36.61 53.13-60.57M94.04 82.96c10.95-13.61 19.45-25.9 51.35-63.34M93 80.72c21.62-23 41.36-46.76 53.63-59.63m-40.84 61.08c9.67-15.21 20.56-25.61 51.59-60.44m-52.76 58.89c17.13-19.36 33.42-38.92 52.61-58.59m-41.76 59.11c13.21-14.57 27.39-30.83 51.86-58.46m-52.35 58.06c11.93-13.01 23.36-25.93 52.65-59.93M126.6 82.75c12-19.25 25.5-33.5 52.91-59.89m-53.5 57.64c16.32-17.87 31.7-34.51 52.2-58.46m-40.33 59.7c16.92-22.65 33.75-43.12 52.79-59.49m-53.68 58.97c12.16-14.1 25.75-29.96 52.49-60.33m-41.49 60.02c17.47-17.31 33.48-38.37 52.9-58.38m-53.27 57.98c9.68-10.54 20-22.42 52.64-60.04M158.4 81.26c17.07-21.57 35.07-42.63 51.91-61.95m-52.19 62.41c18.08-22.21 37.67-43.67 52.6-61.43m-44.53 61.95c19.77-21.32 39.55-44.08 55.85-61.93M168.31 82.3c17.92-23.32 38.68-45.34 52.19-60.7m-41.61 58.3c16.86-18.04 31.6-36.4 52.01-58.35m-51.14 59.67c13.64-17.07 28.55-34.48 52.02-59.36m-40.53 61.21c12.24-20.59 27.27-37.11 51.01-61.55m-53.44 59.87c13.22-14.83 24.22-28.11 52.57-61.3m-41.34 59.5c18.79-19.85 39.43-42.5 52.17-57.45m-51.49 58.91c15.25-16.51 28.82-34.73 50.53-58.97m-42.29 58.36C219.42 69.7 231.02 57.88 250.48 32m-39.75 48.2c7.94-8.84 17.36-20.28 41.89-47.38m-33.29 49.16c8.81-8.72 13.91-16.08 32.82-34.37m-30.01 34.37c7.14-11.63 16.64-20.2 29.84-36.01m-21.02 36.12c5.96-6.72 14.49-14.33 19.26-24.34m-18.74 23.31c4.16-5.2 9.91-11 19.54-23.58m-9.57 24.54c3.47-4.15 8.47-8.72 9.32-12.64m-8.23 11.73c2.47-3.37 5.54-7.42 9.09-10.14" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M52.08 21.85c57.06.07 113.05.91 197.17-1.6m-197.62.92c53.74-.62 107.91-1.4 199.71.7m-.28-.91c.02 23.81-.27 48.06-.02 58.55m-.64-59.12c.11 24.31.64 46.81 1.17 60.25m-2.15-.6c-59.84.27-123.05.56-197.49-.33m198.62 1.64c-54.32-.39-108.57.7-199.96-1.25m2.35 2.59c-1.35-12.93-.41-25.9-1.02-62.94m-.8 61.12c-.45-20.96-.27-42.2.75-60.47" pointer-events="all"/><switch transform="translate(-.5 -.5)"><foreignObject width="100%" height="100%" pointer-events="none" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow:visible;text-align:left"><div xmlns="http://www.w3.org/1999/xhtml" style="display:flex;align-items:unsafe center;justify-content:unsafe center;width:198px;height:1px;padding-top:51px;margin-left:52px"><div style="box-sizing:border-box;font-size:0;text-align:center"><div style="display:inline-block;font-size:12px;font-family:Helvetica;color:#000;line-height:1.2;pointer-events:all;white-space:normal;word-wrap:normal"><font face="Patrick Hand" style="font-size:18px">wasmer::Module::serialize</font></div></div></div></foreignObject><text x="151" y="55" fill="#000" font-family="Helvetica" font-size="12" text-anchor="middle">wasmer::Module::serialize</text></switch><path fill="none" stroke="none" d="M471 21h220v60H471z" pointer-events="all"/><path fill="none" stroke="#f0f0f2" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M470.87 21.15s0 0 0 0m0 0s0 0 0 0m.77 11.53c.96-3.78 6.27-7.93 11.11-13.09m-11.41 14.34c1.25-3.28 5.15-6.44 10.56-12.52m-11.99 25.23c4.3-7.05 8.27-12.52 21.83-25.24m-21.21 23.8c6.58-5.57 10.7-11.45 22.4-25.06m-21.85 38.11c10.14-12.48 18.33-21.4 30.1-35.91m-30.41 35.3c11.58-13.85 23.41-27.41 31.15-37.32m-32.25 47.91c16.08-17.66 34.18-32.44 43.18-46.29m-42.78 48.79c12.37-12.64 23.85-26.66 43.45-49.79m-42.96 59.32c14.14-10.77 25.88-24.18 53.74-60.4m-52.81 60.99C484.6 64 500.4 48.31 523.73 21.07M480.3 83.22c16.49-21.07 31.96-35.86 55.98-60.87M482.56 80.5C501 61.44 518.98 40.81 535.4 21.52m-42.11 61.2c20.73-26.79 42.85-48.06 50.6-61.56m-52.08 59.38c13.69-12.73 24.72-27.24 53.9-60.15m-42.63 59.15c20.29-23.66 41.81-45.82 52.6-59.15M502.43 81.3c12.74-13.27 24.8-27.2 53.12-60.68M515.69 79.8c17.06-19.34 36.67-41.87 49.08-60.45m-50.94 62.12c15.58-18.04 32.22-36.62 53.17-59.9m-42.28 60.29c16.13-16.66 29.21-36.41 51.27-62.38M525.1 81.64c16.43-21.63 33.43-40.28 51.65-61.04m-42.29 58.58c19.27-16.07 36.09-36.21 54.57-59.98m-54.58 62.69c17.99-21.83 34.67-42.15 53.1-61.3M547.06 79.2c19.28-22.83 37.82-46.39 50.43-58.44m-50.84 60.58c18.16-21.03 36.01-42.16 50.99-59.82m-40.03 60.71c8.55-13.43 20.33-29.38 50.49-61.06m-51.65 60.55c16.49-19.23 31.38-36.43 51.73-61.67m-42.89 60.23c17.61-14.13 31.08-32.5 54.47-61.24M568.03 80.9c10.76-14.29 23.41-27.07 50.68-59.84m-42.89 59.7c15.96-15.11 27.58-26.3 54.89-59.33m-52.94 59.85c18.51-22.27 39.41-44.19 52.78-60.05m-43.32 61.04c14.72-19.28 31.33-33.6 53.79-62.71m-52.67 62.67c19.46-24.11 40.29-48.22 51.28-60.89m-42.59 60.69c17.33-17.07 29.59-34.39 53.23-61.45m-52.08 61.71c14.96-17.72 28.59-32.57 52.31-61.46M608 79.74c21.06-20.31 43.93-46.77 52.8-56.73m-51.24 58.61c15.89-19.59 33.72-39.78 52.32-59.76m-43.32 59.35c17.57-15.14 30.57-33.07 52.87-60.59m-52.08 61.76c15.21-18.66 31.35-35.86 53.33-60.48m-42.64 59.33c17.94-19.68 35.81-42.47 53.91-59.21m-53.48 60.26c18.63-23.96 38.11-45.34 52.35-60.25m-41.15 60.41c16.29-20.46 35.02-39.25 51.89-60.15m-52.19 59.82c14.41-16.84 28.88-34.44 50.61-60.89m-42.2 60.99c10.1-12.15 20.62-25.41 41.72-46.49m-40.11 45.62c13.69-15.39 28.23-31.5 41.12-47.59m-29.59 45.77c10.04-9.42 21.8-22.62 31.39-31.62m-32.06 33.76c9.16-9.99 18.15-21.41 30.82-35.6m-20.11 35.77c5.35-3.05 9.17-9.54 21.25-21.47m-20.73 20.2c6.99-5.75 13-13.73 19.78-22.56M682.6 80.4c4.06-3.84 7.02-6.09 11.44-9.36m-10.32 10.01c2.96-1.94 5.05-5.33 9.53-10.73" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M471.05 20.92c61.74-1.95 126.72-.19 219.31 1.44m-220.18-1.84c64.81.89 128.78 1.38 221.51 1.11m-1.09-.42c-.1 11.84.3 26.27-.17 59.43m1.26-60.05c-.35 16.36-.8 31.84-.71 61.4m1.14-.36c-73.11.36-144.85.16-220.36-.49m218.86-.8c-53.7.08-109.04-.84-220.07.7m-.33-.63c.8-19.39 0-35.86.3-59.87m-.18 60.33c.79-18.44 1.47-37.76.97-60.48" pointer-events="all"/><switch transform="translate(-.5 -.5)"><foreignObject width="100%" height="100%" pointer-events="none" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow:visible;text-align:left"><div xmlns="http://www.w3.org/1999/xhtml" style="display:flex;align-items:unsafe center;justify-content:unsafe center;width:218px;height:1px;padding-top:51px;margin-left:472px"><div style="box-sizing:border-box;font-size:0;text-align:center"><div style="display:inline-block;font-size:12px;font-family:Helvetica;color:#000;line-height:1.2;pointer-events:all;white-space:normal;word-wrap:normal"><font face="Patrick Hand" style="font-size:18px">wasmer::Module::deserialize</font></div></div></div></foreignObject><text x="581" y="55" fill="#000" font-family="Helvetica" font-size="12" text-anchor="middle">wasmer::Module::deserialize</text></switch><path fill="none" stroke="none" d="M251 51h211.76" pointer-events="stroke"/><path fill="none" stroke="none" d="M468.76 51l-8 4 2-4-2-4z" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M251.88 49.39c79.38 3.36 157.97 4.68 209.57.26m-210.73.93c57.02 2.02 112.17 2.64 211.63.18" pointer-events="stroke"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M460.85 46.9s0 0 0 0m0 0s0 0 0 0m2.11 9.16c1.66-1 2.07-1.85 5.32-5.46m-5.67 6.32c1.93-2.4 4.3-4.42 5.25-5.97" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M468.7 51.45c-2.4.58-4.87 2.3-8.43 4.2m8.09-4.54c-1.35 1.02-3.2 1.79-7.72 4.27m.42-.82c.54-.96.87-1.85 1.28-3.78m-1.76 4.39c.92-1.41 1.49-2.82 2.05-4.3m.28.23c-1.28-1.43-1.62-3.2-2.1-4.49m1.93 4.46c-.5-.7-.76-1.8-2.04-3.97m-.69.35c3.38 1.26 4.8 2.08 8.83 3.83m-8.24-4.46c2.71 1.49 6.2 3.19 8.04 4.51" pointer-events="all"/><switch transform="translate(-.5 -.5)"><foreignObject width="100%" height="100%" pointer-events="none" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow:visible;text-align:left"><div xmlns="http://www.w3.org/1999/xhtml" style="display:flex;align-items:unsafe center;justify-content:unsafe center;width:1px;height:1px;padding-top:52px;margin-left:352px"><div style="box-sizing:border-box;font-size:0;text-align:center"><div style="display:inline-block;font-size:18px;font-family:Patrick Hand;color:#000;line-height:1.2;pointer-events:all;background-color:#fff;white-space:nowrap"><div style="font-size:18px"><font style="font-size:18px">use a headless engine</font></div><div style="font-size:18px"><font style="font-size:18px">for example<br style="font-size:18px"/></font></div></div></div></div></foreignObject><text x="352" y="57" fill="#000" font-family="Patrick Hand" font-size="18" text-anchor="middle">use a headless engine...</text></switch><path fill="none" stroke="none" d="M216 141v-30h-15V89.24" pointer-events="stroke"/><path fill="none" stroke="none" d="M201 83.24l4 8-4-2-4 2z" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-dasharray="6 6" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M215.32 140.01000000000002c.81-6.96.8-15.14-1.23-28.09m2.16 28.79c-.64-8.78.77-18.27-.89-30.52m1.92 1.27c-5.26.29-6.69-1.51-16.35-1m15.14.59c-4.68-.29-9.81-.4-15.04.66m.8-1.51c-.62-4.52.27-15.23-1.45-21.03m.81 22.81c-.13-8.43-.37-15.63.68-23.63" pointer-events="stroke"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M200.94 83.31s0 0 0 0m0 0s0 0 0 0m3.44 8.18c.22-.05.22-.2.67-.73m-.71.84c.21-.24.41-.49.66-.8" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M201.37 82.88c.93 3.9 2.66 5.02 3.35 8.33m-3.63-7.53c1.36 1.82 2.51 4.18 4.3 7.16m-.82.75c-1.16-1.02-2.23-2.16-3.83-2.4m4.48 2.14c-1.37-.68-2.22-1.39-4.4-1.93m.28.18c-1.25.06-1.4.22-3.71 1.93m3.56-2.13c-.99.09-1.49.58-3.86 2.07m-.56-.51c2.58-2.25 2.81-4.92 4.16-8m-3.54 8.17c.91-2.03 2.18-5.18 3.64-7.65" pointer-events="all"/><path fill="none" stroke="none" d="M21 141h260v60H21z" pointer-events="all"/><path fill="none" stroke="#f0f0f2" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M20.73 141.31s0 0 0 0m0 0s0 0 0 0m-.6 13.4c4.4-3.59 6.62-7.94 11.04-12.31m-9.66 10.35c2.37-2.61 5.36-5.58 9.29-12.08m-10.38 26.52c3.34-5.25 10.55-14.03 20.25-26.97m-20.4 25.18c7.76-7.89 16.27-17.34 21.81-24.95M20.5 177.31c7.42-9.1 15.59-13.88 31.43-34.71m-30.86 35.79c9.25-11.15 17.76-21.64 32.38-37.19m-30.66 49.62c7.16-12.86 19.4-24.04 39.43-47.96m-41.61 46.97c13.02-15.12 25.37-28.74 43.01-48.05M21.1 202.51c18.15-21 36.21-42.67 53.91-63.05m-53.17 62.03c20.44-24.02 41.54-48.61 52.99-61.23m-42.4 61.73c12.76-15.94 28.19-32.21 50.99-62.14m-51.03 61.52c16.01-19.41 33.91-37.94 51.16-59.93m-42.66 60.08c21.47-23.34 42.7-43.22 54.69-59.52m-53.75 59.61c15.09-15.33 28.2-32.28 52.4-60.01m-39.56 59.7c17.85-23.72 35.88-43.34 51.49-59.31m-53.17 59c18.25-20.86 36.51-40.9 53.11-60.8m-43.19 60.96c16.66-18.05 32.89-38.75 54.8-61.57m-53.5 61.14c12.76-12.14 24.31-27.53 52.72-60.17m-43.72 61.5c22.59-23.16 41.86-44.26 54.97-60.32m-53.05 59.43c16.54-20.86 34.24-40.58 52.7-59.49m-40.81 60.05c18.54-21.57 37.99-45.59 49.27-61.5m-50.81 61.32c12.76-15.52 26.2-29.18 51.55-60.67m-39.95 59.29c17.28-19.4 34.37-36.19 51.97-61m-54.2 61.52c22-21.14 42.32-44.76 53.79-59.19m-41.74 61.72c15.88-20.27 29.11-35.79 52.19-62.61m-53.63 60.11c19.64-20.35 37.82-40.9 52.99-59m-42.3 58.45c24.25-23.04 41.67-46.8 52.04-60.11m-50.2 61.74c12.66-14.99 25.62-30.68 50.73-61.64m-40.6 59.11c15.79-17.42 35.22-38.47 52.56-59.2m-52.37 61.76c20.44-22.71 40.16-46.53 51.64-61.14m-40.8 60.61c8.47-13.24 21.89-27.12 51.72-59.49m-51.6 59.35c10.5-11.77 20.9-25.89 52.36-61.07m-41.57 59.07c9.81-11.06 23.6-27.33 50.45-56.56m-51.01 57.75c12.27-15.41 26.63-31.69 52.6-59.55m-42.48 58.61c15.38-14.91 31.6-35.23 50.59-59.91m-51.03 61.35c12.07-13.23 24.22-26.24 52.06-60.22m-39.94 61.96c20.07-23.57 39.94-48.87 49.56-63.74m-50.67 62.62c14.39-16.82 31.09-34.99 52.14-59.98m-41 60.29c13.65-14.64 24.81-33.46 51.15-61.17m-52.04 61.5c16.41-20.19 33.09-38.73 51.66-60.29m-41.88 60.84c12.63-15.86 25.76-28.56 53.98-61.77m-53.38 61.11c15.84-18.06 31.23-35.71 52.38-61.64m-40.37 61.72c20.67-24.47 39.73-46.91 50.98-62.06m-52.29 61.35c13.16-14.74 26.34-29.51 51.51-61.02m-41.93 60.03c19.78-17.62 33.35-38.85 54.71-59.23m-53.6 60.81c12.31-14.84 26.16-32.18 51.9-60.75m-40.55 61.7c10.03-16.12 23.49-26.97 50.62-60.39m-52.4 59.2c14.66-16.28 29.46-32.29 52.46-60.65m-41.54 58.59c11.67-9.75 21.84-23.29 49.17-54.38m-48.72 57.25c14.63-18.21 29.11-33.6 51.16-58.8m-39.85 59.03c11.28-13.26 21.89-26.55 41.15-46.62m-42.57 44.78c10.57-9.94 20.85-22.81 41.41-45.54m-30.82 46.69c13.18-12.2 19.57-25.78 31.36-35.89m-30.23 35.62c7.49-7.76 12.76-17.18 28.73-35.11m-18.18 36.08c8.1-9.89 13.74-19.3 16.38-24.31m-15.72 22.7c3.88-5.88 8.73-9.98 17.49-20.7m-8.51 20.63c2.92-1.73 5.15-5.78 8.96-8.54m-8.27 9.12c2.52-2.67 5.63-5.83 8.58-9.81" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M20.14 140.1c75.45-.2 148.25 2.31 261.24.87m-260.86.6c81.27.89 163.82.89 259.85-.19m-.23.08c-.22 11.87.87 28.48-.32 60.31m1.17-60.98c-.82 20.4-.24 38.88-.61 60.56m.9-1.54c-90.23 3.71-178.15 3.1-259.71 2.6m259.12-1.33c-58.28.92-114.85 1.46-260.15-.81m.96 1.86c-1.06-25.85.42-49.82-2.39-60.79m2.45 59.53c-1.18-15.92-1.99-33.31-.81-60.49" pointer-events="all"/><switch transform="translate(-.5 -.5)"><foreignObject width="100%" height="100%" pointer-events="none" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow:visible;text-align:left"><div xmlns="http://www.w3.org/1999/xhtml" style="display:flex;align-items:unsafe center;justify-content:unsafe center;width:258px;height:1px;padding-top:171px;margin-left:22px"><div style="box-sizing:border-box;font-size:0;text-align:center"><div style="display:inline-block;font-size:12px;font-family:Helvetica;color:#000;line-height:1.2;pointer-events:all;white-space:normal;word-wrap:normal"><font face="Patrick Hand" style="font-size:18px">wasmer_engine::Artifact::serialize</font></div></div></div></foreignObject><text x="151" y="175" fill="#000" font-family="Helvetica" font-size="12" text-anchor="middle">wasmer_engine::Artifact::serialize</text></switch><path fill="none" stroke="none" d="M101 81v30H86v21.76" pointer-events="stroke"/><path fill="none" stroke="none" d="M86 138.76l-4-8 4 2 4-2z" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-dasharray="6 6" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M99.35000000000001 82.42c2.47 6.43 3.59 9.29 2.34 27.55m-.11-29.16c-1.1 10.6-1.02 22.78-1.07 29.54m1.64 1.81c-6.14-.16-11.45.32-14.79-.09m14.22-.33c-3.57.01-6.53-.97-15.14-1.34m-2.28.05c2.87 6.46 2.32 15.45 3.82 20.67m-1.86-20.94c.56 6.11-.83 12.39-.35 21.73" pointer-events="stroke"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M81.9 130.88s0 0 0 0m0 0s0 0 0 0m3.54 7.79c2.5-1.18 2.93-2.8 5.06-5.56m-4.82 5.28c2.6-1.92 4.49-4.28 5.85-6.16" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M85.18 138.51c-.09-3.43-1.77-5.74-2.3-8.48m3.18 8.36c-.75-1.89-2.42-3.74-4.16-8.01m-.27.2c1.23.92 3.07 1.72 4.1 2.34m-3.72-1.98c1.34.45 2.63 1.1 4.17 1.61m-.34.47c1.35-.47 2.23-1.35 3.84-2.7M86 132.65c1.53-.36 2.89-1.26 4.14-1.67m.27-.96c-1.18 3.5-2.65 7.42-4.28 8.2m3.7-7.42c-.84 1.65-1.83 3.96-4.1 8.37" pointer-events="all"/><path fill="none" stroke="none" d="M653.5 141v-30H636V89.24" pointer-events="stroke"/><path fill="none" stroke="none" d="M636 83.24l4 8-4-2-4 2z" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-dasharray="6 6" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M651.5400000000001 140.66c3.94-8.97 2.99-19.08 1.76-30.56m.21 31.86c-.47-10.24.27-18.87.05-31.62m-1.72 2c-1.81-2.63-9.97-2.02-14.63.24m15.47-1.04c-4.74-.93-9.59.07-16.1-.55m.37-1.56c.51-6.61-.52-11.89.5-22.16m-1.61 23.76c-.21-8.5.12-16.51.5-21.77" pointer-events="stroke"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M635.69 83.6s0 0 0 0m0 0s0 0 0 0m3.34 8.23c.18-.12.57-.49.64-.75m-.62.75c.24-.21.49-.53.67-.67" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M636.4200000000001 82.54c1.44 2.9 2.3 6.38 4.23 7.82m-4.72-7.1c1.19 2.62 2.78 5.43 4.22 7.99m-.34-.13c-.8-.36-2.61-.9-3.83-1.72m3.99 1.79c-.61-.31-1.45-.99-3.97-2.05m.31.37c-1.03.6-2.37.33-4.36 2.04m4.15-2.44c-.86.69-1.63.78-4.14 1.93m.22-.67c.77-2.81 2.6-5.83 3.27-6.9m-3.65 7.45c1.61-2.42 3.33-4.77 4.33-7.91" pointer-events="all"/><path fill="none" stroke="none" d="M436 141h290v60H436z" pointer-events="all"/><path fill="none" stroke="#f0f0f2" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M436.21 140.75s0 0 0 0m0 0s0 0 0 0m-.55 13.13c3.8-4.14 6.38-7.42 10.13-12.65m-9.74 11.89c4.25-4.43 8.04-8.25 10.16-11.61m-10.82 23.64c4.62-8.2 12.62-14.46 21.36-22.92m-20.73 22.44c8.52-7.5 14.6-17.68 21.54-24.92m-21.36 38.16c8.02-13.43 21.8-22.82 33.35-37.35m-32.8 37.06c5.75-8.56 14.47-16.79 31.97-37.06m-34.02 50.2c15.27-14.94 25.58-31.43 41.91-48.66m-40.76 48.4c15.3-17.56 28.93-33.31 42.49-49.54m-41.46 62.08c13.85-17.83 24.91-32.41 50.21-61.12m-50.19 59.85c15.17-18.4 32.83-36.38 51.67-59.64m-40.82 61.43c11.3-15.59 19.52-29.12 50.25-61.9m-51.62 59.74c16.86-19.92 36.05-40 52.97-59.74m-40.18 61.93c19.43-22.02 38.04-47.53 48.9-64.32m-51.51 63.37c19.64-21.31 37.34-43.06 52.88-60.46m-40.87 59.22c19.37-22.84 40.23-47.09 51.8-58.92m-53.03 58.98c21.29-23.15 42.93-47.1 52.77-59.74m-40.42 61.31c17.03-20.76 31.51-40.09 49.82-61.44m-51.35 60.65c19.83-22.7 39.43-45.36 53.4-60.88m-41.72 61.74c12.24-16.24 21.15-30.41 51.11-60.88m-50.58 59.17c10.22-12.75 20.97-25.49 50.87-60.57m-40.09 60.71c20.7-21.51 40.63-45.99 53.04-59.51M499.58 201c12.82-14.12 26.74-29.89 52.7-58.88m-40.44 58.17c20.01-22.03 39.17-42.18 50.25-60.85m-52.03 62.97c18.74-23.14 38.37-42.61 53.75-61.21m-44.38 58.94c18.55-16.27 33.56-32.12 54.61-57.62m-52.92 59.38c16.63-18.03 31.77-37.68 52.25-60.4M532 201.44c18.56-18.89 36.25-40.19 51.27-59.87m-51.83 60.71c21.69-25.04 41.6-49.16 53.86-60.62m-41.82 59.14c20.56-20.16 38.15-43.32 50.61-58.82m-50.57 58.54c17.24-18.28 34.45-40.35 51.53-59.45m-42.19 58.85c14.41-13.49 26.79-29.06 52.24-59m-51.12 60.43c11.54-14.94 24.14-29.11 51.66-59.79m-42.21 60.74c16.24-16.95 30.07-35.44 53.07-62.18m-52.52 60.82c20.23-21.46 39.74-43.72 51.99-59.96m-41.9 61c14.84-16.42 28.25-36 51.23-59.33m-51.73 57.84c20.16-21.18 39.37-43.92 53.65-59.65m-40.73 59.48c10.2-13.32 21.86-26.08 50.09-58.74m-52.48 60.28c18.99-19.86 34.89-40.64 53.12-61.93m-40.78 62.79c13.29-16.96 29.54-33.37 50.29-61.9m-50.18 60.36c19.18-24.46 40.88-48.39 52.28-61.21m-43.81 59.44c18.07-18.92 36.92-38.42 53.71-59.59m-52.78 60.69c18.45-19.39 36.14-41.04 52.37-59.31m-40.44 60.94c12.87-16.44 30.05-36.31 51.49-59.78m-51.66 59.07c16.33-20.32 34.29-39.72 52.01-61.41M628.5 201c16.87-19.01 37.3-41.71 52-59.77m-52.48 59.47c12.5-15.61 26.63-29.75 51.21-60.08m-41.81 60.49c13.74-14.74 22-25.07 52.89-62.04m-52.07 62.76c14.05-18.93 28.91-35.18 53.33-61.24m-44.7 59.57c15.83-15.02 26.23-28.28 53.68-59.12m-51.96 60.49c15.24-16.99 31.87-36.28 51.78-60.95m-41.78 61.54c13.32-14.9 27.67-29.8 52.55-61.73m-51.98 61.88c19.01-24.08 39.23-46.98 52.7-61.64m-42.68 61.91c10.78-16.45 25.7-28.51 52.84-62.41m-51.68 62.31c10.85-12.69 22.11-27.12 51.56-61.64m-41.93 61.47c19.94-21.49 35.13-39.64 49.92-56.9m-50.03 55.59c10.18-13.48 22.87-25.8 49.95-56.59m-38.19 56.59c5.87-7.58 17.47-19.89 38.4-43.91m-39.59 44.61c14.58-16.33 29.39-34.52 38.88-45.28m-29.97 45.43c12.98-15.4 25.57-28.05 31.3-34.49m-28.44 33.59c8.53-8.5 16.07-18.64 27.83-32.93m-17.86 32.36c5.38-3.19 9.03-10.14 18.34-19.18m-19.42 20.08c5.66-4.86 9.18-9.72 19.14-20.36m-6.74 20.42c.87-3.06 3.38-4.62 5.7-8.11m-6.29 8.03c1.23-2.22 3.26-4.25 6.82-8.14" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M434.76 141.73c82.88.44 165.83-.46 290.21-.91m-288.83.43c70.89 1.23 142.95 1.05 289.64-.03m-.22-1.5c.51 16.62-1.11 35.71.03 62.32m.84-60.23c.31 13.47.48 26.73.13 59.34m-.06-.77c-98.5.41-194.32 1.64-290.03 1.88m288.8-1.63c-63.28.82-127.42.73-289.35-.28m.56.74c-.2-23.23-2.04-42.7-1.58-59.9m.11 59.64c.95-22.88.79-44.08.76-60.75" pointer-events="all"/><switch transform="translate(-.5 -.5)"><foreignObject width="100%" height="100%" pointer-events="none" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow:visible;text-align:left"><div xmlns="http://www.w3.org/1999/xhtml" style="display:flex;align-items:unsafe center;justify-content:unsafe center;width:288px;height:1px;padding-top:171px;margin-left:437px"><div style="box-sizing:border-box;font-size:0;text-align:center"><div style="display:inline-block;font-size:12px;font-family:Helvetica;color:#000;line-height:1.2;pointer-events:all;white-space:normal;word-wrap:normal"><font face="Patrick Hand" style="font-size:18px">wasmer_engine::Artifact::deserialize</font></div></div></div></foreignObject><text x="581" y="175" fill="#000" font-family="Helvetica" font-size="12" text-anchor="middle">wasmer_engine::Artifact::deserialize</text></switch><path fill="none" stroke="none" d="M526 81v30h-17.5v21.76" pointer-events="stroke"/><path fill="none" stroke="none" d="M508.5 138.76l-4-8 4 2 4-2z" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-dasharray="6 6" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M526.36 81.5c.37 6.29-.34 10.42-.99 31.05m1.2-32.4c.31 11.74-.28 22.02-.68 31.66m-.76-2.3c-7.27-.13-12.93-.2-16.62.76m17.33 1.15c-4.92 0-8.3-1.03-16.9.22m1.48-.01c-.94 8.28-1.49 13.01.01 21.63m-1-21.49c-1.48 5.03-1.29 10.24-1.45 21.28" pointer-events="stroke"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M504.27 131.02s0 0 0 0m0 0s0 0 0 0m3.67 7.05c1.61-2.3 2.89-4.29 5.85-7.16m-5.57 7.18c1.85-1.35 3.26-3.3 6.01-6.57" pointer-events="all"/><path fill="none" stroke="#d90452" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2" d="M509.36 139.04c-1.72-2.13-3.29-5.87-4-8.05m3.56 8.11c-1.5-2.15-2.41-4.23-4.65-8.21m.2-.36c1.15.46 2.46 1.45 4.18 2.56m-4.23-2.45c1.57.9 2.89 1.41 4.04 2.31m.3-.57c1.04.22 1.86-.88 4.02-1.54m-4.22 1.86c.83-.2 1.53-.65 4.04-2.04m.37.26c-2.68 1.87-2.85 5.27-3.62 7.49m3.26-7.92c-.98 2.1-2.25 4.63-4.06 7.93" pointer-events="all"/></svg>

'''
'''--- assets/languages/c.svg ---
<svg width="20" height="22" data-name="Layer 1" xmlns="http://www.w3.org/2000/svg"><path d="m1 17.22 2.19 1.24 13.6-14.92L11 .26a2 2 0 0 0-2 0L1 4.78a2 2 0 0 0-1 1.74v9a2 2 0 0 0 1 1.7z" style="fill:#7f8b99"/><path d="m19 4.78-2.21-1.24L3.21 18.46 9 21.74a2 2 0 0 0 2 0l8-4.52a2 2 0 0 0 1-1.74v-9a2 2 0 0 0-1-1.7z" style="fill:#a9b9cb"/><path d="M12.07 9.58h3.11a4.84 4.84 0 0 0-5.08-4.33A5.42 5.42 0 0 0 4.48 11a5.31 5.31 0 0 0 5.62 5.66c4 0 4.94-2.86 4.94-4.38h-3a1.73 1.73 0 0 1-2 1.75c-1.91 0-2.22-2.27-2.22-3 0-1.18.42-3 2.22-3a1.86 1.86 0 0 1 2.03 1.55z" style="fill:#fff"/></svg>
'''
'''--- assets/languages/cpp.svg ---
<svg width="20" height="22" id="Layer_1" data-name="Layer 1" xmlns="http://www.w3.org/2000/svg"><defs><style>.cls-4{fill:#fff}</style></defs><path d="M19.72 5.5a2 2 0 0 0-.72-.73L11 .25a2 2 0 0 0-2 0L1 4.77a2 2 0 0 0-1 1.75v9a2 2 0 0 0 .28 1z" style="fill:#659ad2"/><path d="M.28 16.5a2 2 0 0 0 .74.73l8 4.52a2 2 0 0 0 2 0l8-4.52a2 2 0 0 0 .74-.73L10 11z" style="fill:#0e4580"/><path d="M20 6.52a2 2 0 0 0-.28-1L10 11l9.72 5.5a2 2 0 0 0 .28-1z" style="fill:#035a9d"/><path class="cls-4" d="M16.67 10.69h-.74V10h-.75v.73h-.74v.73h.74v.72h.75v-.72h.74zM19.44 10.69h-.74V10H18v.73h-.74v.73H18v.72h.74v-.72h.74z"/><path class="cls-4" d="M12.86 12.62a3.3 3.3 0 1 1 0-3.24l2.8-1.58a6.5 6.5 0 1 0 0 6.4z"/></svg>
'''
'''--- assets/languages/crystal.svg ---
<?xml version="1.0" encoding="utf-8"?>
<!-- Generator: Adobe Illustrator 21.0.0, SVG Export Plug-In . SVG Version: 6.00 Build 0)  -->
<svg version="1.1" id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="30" height="32" x="0px" y="0px"
	 viewBox="0 0 193.2 206.7" style="enable-background:new 0 0 193.2 206.7;" xml:space="preserve">
<style type="text/css">
	.st0{fill:none;}
</style>
<g>
	<path d="M165.4,122l-50,49.9c-0.2,0.2-0.5,0.3-0.7,0.2l-68.3-18.3c-0.3-0.1-0.5-0.3-0.5-0.5L27.5,85.1c-0.1-0.3,0-0.5,0.2-0.7
		l50-49.9c0.2-0.2,0.5-0.3,0.7-0.2l68.3,18.3c0.3,0.1,0.5,0.3,0.5,0.5l18.3,68.2C165.7,121.6,165.6,121.8,165.4,122z M98.4,67.7
		L31.3,85.6c-0.1,0-0.2,0.2-0.1,0.3l49.1,49c0.1,0.1,0.3,0.1,0.3-0.1l18-67C98.7,67.8,98.5,67.6,98.4,67.7z"/>
	<g>
		<rect class="st0" width="193.2" height="206.7"/>
	</g>
</g>
</svg>

'''
'''--- assets/languages/csharp.svg ---
<svg width="20" height="20" id="Layer_1" data-name="Layer 1" xmlns="http://www.w3.org/2000/svg"><defs><style>.cls-4{fill:#fff}</style></defs><path d="M17.07 2.93a10 10 0 0 0-14.14 0l14.14 14.14a10 10 0 0 0 0-14.14z" style="fill:#0c9438"/><path d="M2.93 2.93a10 10 0 0 0 14.14 14.14z" style="fill:#219e38;opacity:.85"/><path d="M1 10.23a.76.76 0 0 1 0-.15v.45z" style="opacity:.1;isolation:isolate"/><path class="cls-4" d="M9.51 13.64a4.6 4.6 0 0 1-2.19.46 3.46 3.46 0 0 1-1.46-.25A3.66 3.66 0 0 1 4.63 13a4.05 4.05 0 0 1-1-2.84 4.34 4.34 0 0 1 .26-1.64 4.2 4.2 0 0 1 .88-1.41 4 4 0 0 1 1.32-.94A3.81 3.81 0 0 1 7.65 6a4.42 4.42 0 0 1 1.84.25v1a3.72 3.72 0 0 0-1.85-.45 2.9 2.9 0 0 0-1.2.2 2.81 2.81 0 0 0-1 .69 3.4 3.4 0 0 0-.85 2.42 3.24 3.24 0 0 0 .79 2.29 2.68 2.68 0 0 0 .94.66 2.9 2.9 0 0 0 1.13.2 4 4 0 0 0 2.05-.53zM16.49 8.3l-.13.58h-1.43l-.35 1.65h1.55l-.15.58h-1.51L14 13.3h-.69l.47-2.19h-1.39l-.46 2.19h-.68l.46-2.19h-1.45l.11-.58h1.46l.33-1.65h-1.52l.12-.58h1.52l.46-2.21h.69L13 8.3h1.38l.47-2.17h.66l-.46 2.17zm-2.24.58h-1.39l-.36 1.65h1.4z"/></svg>
'''
'''--- assets/languages/d.svg ---
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.0" width="26" height="20"><defs><linearGradient id="c"><stop style="stop-color:#000;stop-opacity:.19791667" offset="0"/><stop style="stop-color:#000;stop-opacity:.82291669" offset="1"/></linearGradient><linearGradient id="b"><stop style="stop-color:#fff;stop-opacity:1" offset="0"/><stop style="stop-color:#fff;stop-opacity:.33333334" offset="1"/></linearGradient><linearGradient id="a"><stop style="stop-color:#f2f2f0;stop-opacity:.13541667" offset="0"/><stop style="stop-color:#eeeeec;stop-opacity:.39583334" offset="1"/></linearGradient><linearGradient x1="27.248" y1="33.563" x2="44.496" y2="47.031" id="d" xlink:href="#a" gradientUnits="userSpaceOnUse" gradientTransform="matrix(1 0 0 .99176 -.678 .501)" spreadMethod="reflect"/><linearGradient x1="24.482" y1="30.994" x2="104.024" y2="90.719" id="e" xlink:href="#b" gradientUnits="userSpaceOnUse" gradientTransform="matrix(.99719 0 0 .98872 -.497 .687)"/><linearGradient x1="27.248" y1="33.563" x2="44.496" y2="47.031" id="f" xlink:href="#a" gradientUnits="userSpaceOnUse" gradientTransform="matrix(1 0 0 -.99176 -.678 121.014)" spreadMethod="reflect"/></defs><g transform="matrix(.30906 0 0 .31271 -6.987 -9.286)" style="display:inline"><rect width="80.582" height="60.168" rx="7.694" ry="8.543" x="25.996" y="33.484" style="fill:#2e3436;fill-opacity:.2745098;fill-rule:nonzero;stroke:none"/><rect width="80.582" height="60.168" rx="7.694" ry="8.543" x="23.285" y="30.772" style="fill:#a40000;fill-opacity:1;fill-rule:nonzero;stroke:none"/><rect width="74.011" height="54.138" rx="5.221" ry="5.62" x="26.57" y="33.787" style="fill:url(#d);fill-opacity:1;fill-rule:nonzero;stroke:none"/><path d="M32.333 39.188c-.81.1-1.445.747-1.448 1.53l.051 39.977a1.503 1.503 0 0 0 0 .174 1.3 1.3 0 0 0 0 .27v.04c.01.037.03.077.042.115v.02c.01.038.01.078.021.115v.04c.02.038.04.078.063.115v.02c.028.039.071.079.103.115v.02c.02.038.04.078.063.116v.038c.04.03.082.05.124.077v.04c.03.029.051.049.083.076l.042.039c.03.03.05.05.083.077h.02c.076.064.162.124.25.174h.04c.04.02.083.04.125.058h.02c.04.02.084.04.125.057.06.013.125.013.187.02.1.018.208.037.31.038h.166l15.31-.062c4.376-.007 7.307-.082 9.053-.303h.041c1.67-.232 3.44-.66 5.364-1.284 3.345-1.046 6.311-2.591 8.861-4.655 2.497-2 4.432-4.366 5.792-7.029 1.36-2.663 2.046-5.478 2.04-8.397-.007-4.062-1.236-7.867-3.702-11.289-2.466-3.423-5.832-6.044-9.974-7.78-4.212-1.785-9.703-2.599-16.515-2.586l-16.533.024c-.07 0-.138-.008-.207 0zm8.898 8.226 7.127-.01c3.33-.006 5.7.095 7.044.28 1.363.187 2.855.582 4.435 1.192 1.567.597 2.932 1.328 4.105 2.238v.038c3.228 2.471 4.75 5.441 4.756 9.373.007 4.027-1.463 7.163-4.607 9.793a15.313 15.313 0 0 1-3.23 2.036c-1.12.522-2.584.972-4.431 1.36-1.742.349-4.387.547-7.83.553l-7.334.01-.035-26.863z" style="font-style:normal;font-variant:normal;font-weight:400;font-stretch:normal;font-size:64px;line-height:125%;font-family:'Gill Sans MT';text-align:start;writing-mode:lr-tb;text-anchor:start;fill:#eeeeec;fill-opacity:1;stroke:none"/><path d="M89.368 35.648a5.969 5.472 0 1 1-11.938 0 5.969 5.472 0 1 1 11.938 0z" transform="matrix(1.95002 0 0 1.95002 -82.918 -16.343)" style="fill:#eeeeec;fill-opacity:1;fill-rule:nonzero;stroke:none"/><rect width="78.006" height="57.75" rx="6.57" ry="7.306" x="24.572" y="31.981" style="fill:none;stroke:url(#e);stroke-width:1.34628034;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1"/><rect width="80.582" height="60.168" rx="7.694" ry="8.543" x="23.285" y="30.772" style="fill:none;stroke:#323232;stroke-width:1.3558476;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1"/><path d="M31.791 87.728H95.36c2.892 0 5.22-2.506 5.22-5.62v-9.001c-22.704-8.734-55.576-13.412-74.01-13.559v22.56c0 3.114 2.329 5.62 5.221 5.62z" style="fill:url(#f);fill-opacity:1;fill-rule:nonzero;stroke:none"/><path d="M89.368 35.648a5.969 5.472 0 1 1-11.938 0 5.969 5.472 0 1 1 11.938 0z" transform="matrix(.62657 0 0 .62657 40.72 19.11)" style="display:inline;fill:#eeeeec;fill-opacity:1;fill-rule:nonzero;stroke:none"/></g></svg>
'''
'''--- assets/languages/dart.svg ---
<svg width="30" height="30" viewBox="0 0 256 256" xmlns="http://www.w3.org/2000/svg" preserveAspectRatio="xMidYMid"><path d="M70.534 69.696L53.988 53.15l.07 119.6.198 5.59c.082 2.63.57 5.598 1.384 8.674l131.104 46.23 32.772-14.52.012-.04L70.534 69.696" fill="#00D2B8"/><path d="M55.64 187.014l.008.008c-.008-.054-.036-.114-.036-.17 0 .056.008.108.028.162zm163.876 31.71l-32.772 14.52-131.096-46.222c2.504 9.608 8.048 20.408 14.014 26.314l42.784 42.54 95.13.124 11.952-37.316-.012.04z" fill="#55DDCA"/><path d="M3.034 130.116c-4.236 4.522-2.132 13.85 4.688 20.722L37.14 180.5l18.5 6.514c-.814-3.076-1.302-6.044-1.384-8.674l-.198-5.59-.07-119.6-50.954 76.966z" fill="#0081C6"/><path d="M187.82 54.686c-3.076-.786-6.026-1.272-8.7-1.356l-5.908-.204-119.224.016 165.556 165.542h.014l14.54-32.804L187.82 54.686" fill="#0079B3"/><path d="M187.67 54.654c.064.014.114.032.156.038l-.006-.006c-.036-.018-.086-.018-.15-.032zm26.448 14.078c-6.008-6.058-16.666-11.564-26.292-14.04l46.272 131.188-14.54 32.804h-.014l35.532-11.348.076-97.416-41.034-41.188z" fill="#00A4E4"/><path d="M181.338 36.298L151.684 6.862c-6.858-6.794-16.19-8.908-20.7-4.684L53.988 53.142l119.224-.016 5.908.204c2.674.084 5.624.57 8.7 1.356l-6.482-18.388z" fill="#00D2B8"/></svg>
'''
'''--- assets/languages/elixir.svg ---
<svg width="16" height="25" viewBox="0 0 259 398" xmlns="http://www.w3.org/2000/svg"><defs><linearGradient x1="52.258%" y1="7.389%" x2="50%" y2="100%" id="a"><stop stop-color="#D9D8DC" offset="0%"/><stop stop-color="#FFF" stop-opacity=".385" offset="100%"/></linearGradient><linearGradient x1="71.179%" y1="7.691%" x2="50%" y2="100%" id="b"><stop stop-color="#8D67AF" stop-opacity=".672" offset="0%"/><stop stop-color="#9F8DAF" offset="100%"/></linearGradient><linearGradient x1="54.233%" y1="31.435%" x2="54.233%" y2="98.228%" id="c"><stop stop-color="#26053D" stop-opacity=".762" offset="0%"/><stop stop-color="#B7B4B4" stop-opacity=".278" offset="100%"/></linearGradient><linearGradient x1="10.133%" y1="12.539%" x2="50%" y2="92.768%" id="d"><stop stop-color="#91739F" stop-opacity=".46" offset="0%"/><stop stop-color="#32054F" stop-opacity=".54" offset="100%"/></linearGradient><linearGradient x1="80.023%" y1="96.41%" x2="9.647%" y2="21.227%" id="e"><stop stop-color="#463D49" stop-opacity=".331" offset="0%"/><stop stop-color="#340A50" stop-opacity=".821" offset="100%"/></linearGradient><linearGradient x1="76.347%" y1="7.419%" x2="50%" y2="100%" id="f"><stop stop-color="#715383" stop-opacity=".145" offset="0%"/><stop stop-color="#F4F4F4" stop-opacity=".234" offset="100%"/></linearGradient><linearGradient x1="131.792%" y1="72.665%" x2="11.347%" y2="50%" id="g"><stop stop-color="#A5A1A8" stop-opacity=".356" offset="0%"/><stop stop-color="#370C50" stop-opacity=".582" offset="100%"/></linearGradient></defs><g fill="none" fill-rule="evenodd"><path d="M140.473 1c-28.87 10.233-56.74 40.102-83.608 89.605-40.303 74.255-92.356 179.773-20.678 263.318 33.16 38.65 87.911 61.461 159.764 25.333 57.723-29.023 73.776-112.312 53.053-151.45-42.748-80.737-86.118-100.65-97.732-150.719-7.742-33.38-11.342-58.742-10.8-76.087z" fill="url(#a)"/><path d="M140.473 0c-29.017 10.339-56.886 40.207-83.608 89.605-40.083 74.097-92.356 179.773-20.678 263.318 33.16 38.65 87.206 51.14 117.196 28.217 19.55-14.944 32.847-29.188 40.508-59.201 8.53-33.42 1.985-78.412-2.5-99.08-5.674-26.154-7.532-54.86-5.572-86.118l-1.548-1.89c-15.613-19.106-27.66-35.75-32.999-58.764-7.742-33.38-11.342-58.742-10.8-76.087z" fill="url(#b)"/><path d="M116.447 13C89.42 34.36 69.325 70.188 56.16 120.485 36.417 195.929 34.277 265.5 41.241 305.968c13.5 78.447 83.486 108.224 155.272 71.71 44.178-22.471 62.579-70.705 61.604-123.062-1.01-54.211-105.77-115.65-124.322-160.893-12.368-30.162-18.151-57.07-17.348-80.723z" fill="url(#c)"/><path d="M184.965 154.444c32.414 41.57 39.48 70.507 21.197 86.812-27.424 24.458-94.486 40.415-136.206 10.83-27.813-19.724-38.336-62.036-31.57-126.936-11.481 23.977-21.189 48.267-29.123 72.868-7.933 24.601-10.25 50.346-6.95 77.235 9.943 20.115 34.118 33.55 72.524 40.303 57.61 10.131 112.136 5.125 148.724-13.318 24.391-12.296 35.928-24.536 34.61-36.72.881-17.994-4.667-35.122-16.645-51.385-11.977-16.264-30.83-36.16-56.56-59.689z" fill="url(#d)"/><path d="M58.3 112.936c-.271 30.022 7.448 58.56 23.158 85.617 23.564 40.585 51.07 80.688 92.231 113.93 27.44 22.161 49.367 29.32 65.778 21.48-13.48 24.124-27.812 35.945-42.999 35.463-22.779-.723-50.554-10.678-95.058-64.158-29.669-35.654-49.965-69.38-60.89-101.177 1.733-12.615 3.57-25.18 5.511-37.694 1.942-12.515 6.031-30.335 12.269-53.46z" fill="url(#e)"/><path d="M127.916 154.136c2.517 24.456 12.034 63.578 0 89.625-12.035 26.047-67.621 73.102-52.277 114.553 15.344 41.452 52.737 32.146 76.193 13.043 23.456-19.103 36.012-50.165 38.795-72.01 2.783-21.844-6.659-63.877-9.73-100.437-2.047-24.374-.658-45.271 4.167-62.693l-7.187-9.045-42.305-12.526c-6.783 10.023-9.335 23.186-7.656 39.49z" fill="url(#f)"/><path d="M143.017 33c-13.693 5.786-26.666 17.25-38.919 34.393-18.38 25.714-27.741 41.128-20.812 92.1 4.62 33.98 7.926 65.243 9.92 93.786L152.458 81.12c-2.179-8.772-3.99-16.695-5.436-23.768-1.445-7.072-2.78-15.19-4.006-24.353z" fill="url(#g)"/><path d="M152.345 80.778c-15.042 8.78-26.697 26.767-34.966 53.962-8.269 27.195-15.834 65.212-22.695 114.052 9.15-31.064 15.221-53.564 18.21-67.499 4.485-20.903 6.017-50.462 17.923-70.53 7.938-13.378 15.114-23.373 21.528-29.985z" fill-opacity=".316" fill="#330A4C"/><path d="M82.698 372.158c24.754 3.536 37.697 6.68 38.831 9.432 1.701 4.127-3.133 7.903-16.873 5.365-9.16-1.69-16.48-6.623-21.958-14.797z" fill="#FFF"/><path d="M95.134 33C81.52 49.265 69.721 66.509 59.741 84.73c-9.98 18.223-17.111 31.817-21.394 40.784-1.327 6.687-1.975 16.555-1.943 29.603.032 13.048 1.16 27.556 3.386 43.524 1.948-31.268 7.913-61.62 17.895-91.053C67.666 78.154 80.149 53.29 95.135 33z" fill-opacity=".603" fill="#EDEDED"/></g></svg>
'''
'''--- assets/languages/go.svg ---
<svg width="32" height="12" id="Layer_1" data-name="Layer 1" xmlns="http://www.w3.org/2000/svg"><defs><style>.cls-1{fill:#38b6ac}</style></defs><path class="cls-1" d="M2.4 3.62s-.07 0 0-.09l.33-.42A.25.25 0 0 1 2.86 3h5.57c.05 0 .08 0 0 .09l-.25.4a.24.24 0 0 1-.17.09zM.06 5.06C0 5.06 0 5 0 5l.32-.42a.3.3 0 0 1 .17-.08h7.13a.08.08 0 0 1 .08.1l-.13.4a.15.15 0 0 1-.14.1zM3.84 6.49c-.06 0-.09 0-.06-.09L4 6a.2.2 0 0 1 .16-.09h3.12a.09.09 0 0 1 .09.09v.37a.13.13 0 0 1-.12.11zM31.94 4.55a4.86 4.86 0 0 0-2.2-3.49 5.72 5.72 0 0 0-4.43-.81A6.56 6.56 0 0 0 21 2.87a6.74 6.74 0 0 0-1.08 2.05h-5a.43.43 0 0 0-.38.25c-.22.41-.6 1.24-.8 1.71-.11.27 0 .47.29.47h3c-.16.22-.28.41-.43.58a2.88 2.88 0 0 1-2.65 1 2.45 2.45 0 0 1-2.15-2.44 3.36 3.36 0 0 1 1.6-3 2.69 2.69 0 0 1 2.81-.23 2 2 0 0 1 .73.6c.2.23.22.21.45.15 1-.25 1.64-.45 2.62-.69.18 0 .24-.12.18-.25a5 5 0 0 0-1.8-2.2 5.48 5.48 0 0 0-4.13-.76A6.88 6.88 0 0 0 9.9 2.82a6.12 6.12 0 0 0-1.32 4.59 4.93 4.93 0 0 0 2 3.47 5.49 5.49 0 0 0 4.18 1 6.45 6.45 0 0 0 4.44-2.66 6.88 6.88 0 0 0 .64-1.07 5 5 0 0 0 1.54 2.43A5.77 5.77 0 0 0 25.16 12c.41-.05.78-.06 1.19-.14a7.21 7.21 0 0 0 3.79-2 6.07 6.07 0 0 0 1.8-5.31zm-5 4.1a2.83 2.83 0 0 1-2.55.09 2.58 2.58 0 0 1-1.42-2.95 3.37 3.37 0 0 1 2.72-2.73 2.58 2.58 0 0 1 3.18 2.09c0 .16 0 .32.05.52a3.44 3.44 0 0 1-2.02 2.98z"/></svg>
'''
'''--- assets/languages/java.svg ---
<svg xmlns="http://www.w3.org/2000/svg" width="21.779" height="29.53" preserveAspectRatio="xMidYMid"><path fill="#5382a1" d="M7.014 22.837s-1.128.657.803.879c2.34.267 3.536.228 6.115-.26 0 0 .678.425 1.625.794-5.78 2.477-13.083-.144-8.543-1.413m-.706-3.233s-1.266.937.667 1.137c2.5.258 4.474.28 7.89-.379 0 0 .472.48 1.215.741-6.99 2.044-14.774.162-9.772-1.499"/><path fill="#e76f00" d="M12.263 14.12c1.424 1.64-.374 3.116-.374 3.116s3.617-1.867 1.956-4.206c-1.552-2.18-2.742-3.263 3.699-6.999 0 0-10.11 2.525-5.281 8.089"/><path fill="#5382a1" d="M19.909 25.229s.835.688-.92 1.22c-3.337 1.011-13.888 1.316-16.82.04-1.053-.458.923-1.094 1.545-1.227.648-.14 1.018-.115 1.018-.115-1.172-.825-7.574 1.621-3.252 2.322 11.787 1.911 21.486-.861 18.429-2.24M7.557 16.255s-5.367 1.274-1.9 1.737c1.463.196 4.38.152 7.099-.076 2.22-.187 4.451-.586 4.451-.586s-.783.336-1.35.723c-5.45 1.433-15.979.766-12.948-.7 2.564-1.239 4.648-1.099 4.648-1.099m9.628 5.382c5.54-2.879 2.979-5.645 1.19-5.273-.438.092-.633.17-.633.17s.163-.254.473-.365c3.538-1.243 6.258 3.668-1.142 5.613 0 0 .086-.076.112-.145"/><path fill="#e76f00" d="M13.845 0s3.068 3.07-2.91 7.79c-4.795 3.785-1.094 5.944-.003 8.41-2.798-2.524-4.852-4.747-3.474-6.816C9.48 6.348 15.083 4.875 13.845 0"/><path fill="#5382a1" d="M8.101 29.438c5.318.34 13.485-.19 13.678-2.706 0 0-.371.954-4.395 1.712-4.539.854-10.137.754-13.458.207 0 0 .68.563 4.175.787"/></svg>
'''
'''--- assets/languages/js.svg ---
<svg width="20" height="20" viewBox="0 0 256 256" xmlns="http://www.w3.org/2000/svg" preserveAspectRatio="xMidYMid"><path d="M0 0h256v256H0V0z" fill="#F7DF1E"/><path d="m67.312 213.932 19.59-11.856c3.78 6.701 7.218 12.371 15.465 12.371 7.905 0 12.89-3.092 12.89-15.12v-81.798h24.057v82.138c0 24.917-14.606 36.259-35.916 36.259-19.245 0-30.416-9.967-36.087-21.996M152.381 211.354l19.588-11.341c5.157 8.421 11.859 14.607 23.715 14.607 9.969 0 16.325-4.984 16.325-11.858 0-8.248-6.53-11.17-17.528-15.98l-6.013-2.58c-17.357-7.387-28.87-16.667-28.87-36.257 0-18.044 13.747-31.792 35.228-31.792 15.294 0 26.292 5.328 34.196 19.247L210.29 147.43c-4.125-7.389-8.591-10.31-15.465-10.31-7.046 0-11.514 4.468-11.514 10.31 0 7.217 4.468 10.14 14.778 14.608l6.014 2.577c20.45 8.765 31.963 17.7 31.963 37.804 0 21.654-17.012 33.51-39.867 33.51-22.339 0-36.774-10.654-43.819-24.574"/></svg>
'''
'''--- assets/languages/lisp.svg ---
<?xml version="1.0" encoding="UTF-8"?>
<svg version="1.1" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg" width="24" height="24">
  <circle cx="256" cy="256" r="235" fill="#fff"/>
  <path stroke="#000" stroke-width="5" d="m255.56 20.008c-62.374 0.1169-122.17 24.922-166.3 68.992-92.236 92.091-92.353 241.52-0.2617 333.75 92.09 92.236 241.52 92.353 333.75 0.262 92.236-92.091 92.353-241.52 0.262-333.75-44.377-44.447-104.64-69.371-167.45-69.254zm2.281 1.0059c59.934 0.4846 119.39 23.809 164.46 68.953 91.701 91.845 91.585 240.64-0.259 332.34-45.922 45.851-120.32 45.793-166.17-0.129-45.851-45.922-45.793-120.32 0.129-166.17 46.412-46.339 46.471-121.53 0.13-167.94-37.084-37.141-94.457-46.553-140.66-21.658 42.416-31.541 92.711-45.798 142.37-45.396zm-190.84 130.26h40c9.943 42.147 25.204 79.418 40.75 116.43 15.9-41.326 33.203-81.249 55.25-116.43h40c-48.928 97.364-102.19 164.06-24 250h-40c-47.567-77.243-82.439-147.67-112-250z"/>
  <path d="m293 110.72c78.194 85.936 24.928 152.64-24 250h40c22.047-35.179 39.35-75.102 55.25-116.43 15.546 37.01 30.807 74.282 40.75 116.43h40c-29.561-102.33-64.433-172.76-112-250z"/>
</svg>

'''
'''--- assets/languages/php.svg ---
<svg width="27" height="14" data-name="Layer 1" xmlns="http://www.w3.org/2000/svg"><path d="M2.14 2.94h4A3.05 3.05 0 0 1 8.69 4a3.59 3.59 0 0 1 .53 2.82 5.59 5.59 0 0 1-.47 1.61 4.6 4.6 0 0 1-1 1.42 3.31 3.31 0 0 1-1.6 1 6.9 6.9 0 0 1-1.76.22H2.64L2.08 14H0zM3.88 4.7 3 9.33h.39a7.55 7.55 0 0 0 2.39-.29c.63-.22 1.06-1 1.28-2.25.18-1.08 0-1.7-.54-1.87a6.6 6.6 0 0 0-2-.23H3.87zM11.59 0h2.06l-.59 2.94h1.86a3.72 3.72 0 0 1 2.27.65c.51.41.66 1.19.45 2.35l-1 5.13h-2.1l1-4.9a1.44 1.44 0 0 0-.09-1.1 1.26 1.26 0 0 0-1-.32h-1.66l-1.29 6.32H9.43zM19.85 2.94h4A3.08 3.08 0 0 1 26.41 4a3.59 3.59 0 0 1 .52 2.82 5.56 5.56 0 0 1-.46 1.61 4.81 4.81 0 0 1-1 1.42 3.31 3.31 0 0 1-1.6 1 6.79 6.79 0 0 1-1.76.22h-1.75L19.79 14h-2.08zM21.6 4.7l-.9 4.63h.39a7.51 7.51 0 0 0 2.39-.29q1-.33 1.29-2.25c.17-1.08 0-1.7-.54-1.87a6.67 6.67 0 0 0-2-.23h-.66z"/></svg>
'''
'''--- assets/languages/postgres.svg ---
<svg width="21" height="22" viewBox="0 0 432.071 445.383" xml:space="preserve" xmlns="http://www.w3.org/2000/svg"><g style="fill-rule:nonzero;clip-rule:nonzero;fill:none;stroke:#fff;stroke-width:12.4651;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:4"><path style="fill:#000;stroke:#000;stroke-width:37.3953;stroke-linecap:butt;stroke-linejoin:miter" d="M323.205 324.227c2.833-23.601 1.984-27.062 19.563-23.239l4.463.392c13.517.615 31.199-2.174 41.587-7 22.362-10.376 35.622-27.7 13.572-23.148-50.297 10.376-53.755-6.655-53.755-6.655 53.111-78.803 75.313-178.836 56.149-203.322-52.27-66.789-142.748-35.206-144.262-34.386l-.482.089c-9.938-2.062-21.06-3.294-33.554-3.496-22.761-.374-40.032 5.967-53.133 15.904 0 0-161.408-66.498-153.899 83.628 1.597 31.936 45.777 241.655 98.47 178.31 19.259-23.163 37.871-42.748 37.871-42.748 9.242 6.14 20.307 9.272 31.912 8.147l.897-.765c-.281 2.876-.157 5.689.359 9.019-13.572 15.167-9.584 17.83-36.723 23.416-27.457 5.659-11.326 15.734-.797 18.367 12.768 3.193 42.305 7.716 62.268-20.224l-.795 3.188c5.325 4.26 4.965 30.619 5.72 49.452.756 18.834 2.017 36.409 5.856 46.771 3.839 10.36 8.369 37.05 44.036 29.406 29.809-6.388 52.6-15.582 54.677-101.107"/><path style="fill:#336791;stroke:none" d="M402.395 271.23c-50.302 10.376-53.76-6.655-53.76-6.655 53.111-78.808 75.313-178.843 56.153-203.326-52.27-66.785-142.752-35.2-144.262-34.38l-.486.087c-9.938-2.063-21.06-3.292-33.56-3.496-22.761-.373-40.026 5.967-53.127 15.902 0 0-161.411-66.495-153.904 83.63 1.597 31.938 45.776 241.657 98.471 178.312 19.26-23.163 37.869-42.748 37.869-42.748 9.243 6.14 20.308 9.272 31.908 8.147l.901-.765c-.28 2.876-.152 5.689.361 9.019-13.575 15.167-9.586 17.83-36.723 23.416-27.459 5.659-11.328 15.734-.796 18.367 12.768 3.193 42.307 7.716 62.266-20.224l-.796 3.188c5.319 4.26 9.054 27.711 8.428 48.969-.626 21.259-1.044 35.854 3.147 47.254 4.191 11.4 8.368 37.05 44.042 29.406 29.809-6.388 45.256-22.942 47.405-50.555 1.525-19.631 4.976-16.729 5.194-34.28l2.768-8.309c3.192-26.611.507-35.196 18.872-31.203l4.463.392c13.517.615 31.208-2.174 41.591-7 22.358-10.376 35.618-27.7 13.573-23.148z"/><path d="M215.866 286.484c-1.385 49.516.348 99.377 5.193 111.495 4.848 12.118 15.223 35.688 50.9 28.045 29.806-6.39 40.651-18.756 45.357-46.051 3.466-20.082 10.148-75.854 11.005-87.281M173.104 38.256S11.583-27.76 19.092 122.365c1.597 31.938 45.779 241.664 98.473 178.316 19.256-23.166 36.671-41.335 36.671-41.335M260.349 26.207c-5.591 1.753 89.848-34.889 144.087 34.417 19.159 24.484-3.043 124.519-56.153 203.329"/><path style="stroke-linejoin:bevel" d="M348.282 263.953s3.461 17.036 53.764 6.653c22.04-4.552 8.776 12.774-13.577 23.155-18.345 8.514-59.474 10.696-60.146-1.069-1.729-30.355 21.647-21.133 19.96-28.739-1.525-6.85-11.979-13.573-18.894-30.338-6.037-14.633-82.796-126.849 21.287-110.183 3.813-.789-27.146-99.002-124.553-100.599-97.385-1.597-94.19 119.762-94.19 119.762"/><path d="M188.604 274.334c-13.577 15.166-9.584 17.829-36.723 23.417-27.459 5.66-11.326 15.733-.797 18.365 12.768 3.195 42.307 7.718 62.266-20.229 6.078-8.509-.036-22.086-8.385-25.547-4.034-1.671-9.428-3.765-16.361 3.994z"/><path d="M187.715 274.069c-1.368-8.917 2.93-19.528 7.536-31.942 6.922-18.626 22.893-37.255 10.117-96.339-9.523-44.029-73.396-9.163-73.436-3.193-.039 5.968 2.889 30.26-1.067 58.548-5.162 36.913 23.488 68.132 56.479 64.938"/><path style="fill:#fff;stroke-width:4.155;stroke-linecap:butt;stroke-linejoin:miter" d="M172.517 141.7c-.288 2.039 3.733 7.48 8.976 8.207 5.234.73 9.714-3.522 9.998-5.559.284-2.039-3.732-4.285-8.977-5.015-5.237-.731-9.719.333-9.996 2.367z"/><path style="fill:#fff;stroke-width:2.0775;stroke-linecap:butt;stroke-linejoin:miter" d="M331.941 137.543c.284 2.039-3.732 7.48-8.976 8.207-5.238.73-9.718-3.522-10.005-5.559-.277-2.039 3.74-4.285 8.979-5.015 5.239-.73 9.718.333 10.002 2.368z"/><path d="M350.676 123.432c.863 15.994-3.445 26.888-3.988 43.914-.804 24.748 11.799 53.074-7.191 81.435"/></g></svg>
'''
'''--- assets/languages/python.svg ---
<svg width="20" height="20" data-name="Layer 1" xmlns="http://www.w3.org/2000/svg"><path d="M9.93 0C4.87 0 5.18 2.21 5.18 2.21V4.5H10v.7H3.25S0 4.82 0 10s2.84 5 2.84 5h1.69v-2.44A2.71 2.71 0 0 1 7.32 9.7h4.79a2.58 2.58 0 0 0 2.69-2.62V2.67S15.23 0 9.93 0zM7.26 1.53a.88.88 0 1 1-.87.88.88.88 0 0 1 .87-.88z" style="fill:#387ab0"/><path d="M10.07 20c5.06 0 4.75-2.21 4.75-2.21V15.5H10v-.7h6.76S20 15.18 20 10s-2.84-5-2.84-5h-1.69v2.44a2.71 2.71 0 0 1-2.79 2.86H7.89a2.58 2.58 0 0 0-2.69 2.62v4.41S4.77 20 10.07 20zm2.67-1.53a.88.88 0 1 1 .87-.88.88.88 0 0 1-.87.88z" style="fill:#feca3d"/></svg>
'''
'''--- assets/languages/r.svg ---
<svg width="24" height="18" xmlns="http://www.w3.org/2000/svg" preserveAspectRatio="xMidYMid" viewBox="0 0 724 561"><defs><linearGradient id="a" x1="0" x2="1" y1="0" y2="1" gradientUnits="objectBoundingBox" spreadMethod="pad"><stop offset="0" stop-color="#CBCED0"/><stop offset="1" stop-color="#84838B"/></linearGradient><linearGradient id="b" x1="0" x2="1" y1="0" y2="1" gradientUnits="objectBoundingBox" spreadMethod="pad"><stop offset="0" stop-color="#276DC3"/><stop offset="1" stop-color="#165CAA"/></linearGradient></defs><path d="M361.453 485.937C162.329 485.937.906 377.828.906 244.469.906 111.109 162.329 3 361.453 3 560.578 3 722 111.109 722 244.469c0 133.359-161.422 241.468-360.547 241.468zm55.188-388.531c-151.352 0-274.047 73.908-274.047 165.078s122.695 165.078 274.047 165.078c151.351 0 263.046-50.529 263.046-165.078 0-114.513-111.695-165.078-263.046-165.078z" fill="url(#a)" fill-rule="evenodd"/><path d="M550 377s21.822 6.585 34.5 13c4.399 2.226 12.01 6.668 17.5 12.5 5.378 5.712 8 11.5 8 11.5l86 145-139 .062L492 437s-13.31-22.869-21.5-29.5c-6.832-5.531-9.745-7.5-16.5-7.5h-33.026L421 558.974l-123 .052V152.938h247S657.5 154.967 657.5 262 550 377 550 377zm-53.5-135.976-74.463-.048-.037 69.05 74.5-.024s34.5-.107 34.5-35.125c0-35.722-34.5-33.853-34.5-33.853z" fill="url(#b)" fill-rule="evenodd"/></svg>
'''
'''--- assets/languages/ruby.svg ---
<svg width="20" height="20" id="Layer_1" data-name="Layer 1" xmlns="http://www.w3.org/2000/svg"><defs><style>.cls-1{fill:#b11917}</style></defs><path class="cls-1" d="m2 12.84-.42-1C1.21 11 .86 10.15.51 9.3a.08.08 0 0 1 0-.1C1.23 8 2 6.73 2.67 5.49a.61.61 0 0 1 .12-.16l3.9-3.65a.74.74 0 0 1 .21-.13L10.2.33a.11.11 0 0 1 .1 0l2.77 2v.09c-.47 1.48-.93 3-1.4 4.44a.41.41 0 0 1 0 .09l-5.28 4.87H6.3L2 12.83zM5.12 19.94Q6 16.38 6.9 12.83l6.78 2.63h-.07A22.89 22.89 0 0 1 11 17.58a14.7 14.7 0 0 1-4.29 2 13.17 13.17 0 0 1-1.54.33zM12.68 7.55l6.67-.15c-.08.19-.14.37-.22.55a18.59 18.59 0 0 1-2.54 4.3c-.54.7-1.11 1.35-1.69 2zM7.39 12.08l4.48-4.19 2.21 6.79zM15.29 15.19l.05-.05c.59-.64 1.17-1.29 1.71-2a21.07 21.07 0 0 0 2.38-3.74l.21-.4q-.37 4.79-.73 9.55M19.73 6.08l-5.34-3.9 3.22-1.8a.06.06 0 0 1 .06 0 3.5 3.5 0 0 1 2 2A4.49 4.49 0 0 1 20 3.54a6.5 6.5 0 0 1-.12 1.92c-.07.2-.11.4-.15.62zM18.46 19.09 9 19.74a21.13 21.13 0 0 0 5.66-4l.42.38M12.66 6.67c.4-1.27.8-2.53 1.19-3.8l5 3.65zM4.32 19.48c-.67-1.94-1.43-3.87-2.11-5.81L6 12.83c-.56 2.23-1.12 4.44-1.67 6.65zM1.64 14.4c.64 1.87 1.29 3.73 1.94 5.6h-.3A4.44 4.44 0 0 1 2 19.53 3.32 3.32 0 0 1 .54 18.1 4.57 4.57 0 0 1 .15 17a.19.19 0 0 1 0-.08l1.45-2.49zM11.33 0h5.15l-2.81 1.58h-.09L11.36 0zM0 15.47v-5.12.08c.39.94.79 1.89 1.18 2.84a.08.08 0 0 1 0 .1L0 15.42a.13.13 0 0 1 0 .05z"/></svg>
'''
'''--- assets/languages/rust.svg ---
<svg width="22" height="22" data-name="Layer 1" xmlns="http://www.w3.org/2000/svg"><path d="m21.85 10.73-.92-.57v-.27l.79-.74a.34.34 0 0 0 .1-.3.33.33 0 0 0-.2-.23l-1-.38a2 2 0 0 0-.13-.24l.64-.88a.34.34 0 0 0 0-.31.31.31 0 0 0-.24-.19l-1.07-.18c0-.08-.08-.16-.13-.24l.45-1a.31.31 0 0 0-.3-.45h-1.08l-.18-.21.25-1.06a.31.31 0 0 0-.38-.38l-1.06.25a1.73 1.73 0 0 0-.21-.17V2.09a.32.32 0 0 0-.14-.28.31.31 0 0 0-.31 0l-1 .45-.24-.13-.09-1.05a.31.31 0 0 0-.19-.24.34.34 0 0 0-.31 0l-.9.67-.26-.08-.38-1a.33.33 0 0 0-.23-.2.34.34 0 0 0-.3.1l-.74.79h-.27l-.55-.97a.32.32 0 0 0-.54 0l-.57.92h-.27L9.15.31a.32.32 0 0 0-.53.1l-.38 1-.24.1-.9-.64a.32.32 0 0 0-.5.21l-.18 1.07-.24.13-1-.45a.31.31 0 0 0-.31 0 .32.32 0 0 0-.14.28V3.2l-.21.17-1-.23a.31.31 0 0 0-.38.38l.25 1.06-.17.21H2.13a.32.32 0 0 0-.28.14.31.31 0 0 0 0 .31l.45 1-.13.24-1.09.12a.31.31 0 0 0-.24.19.34.34 0 0 0 0 .31l.67.9a2 2 0 0 0-.08.26l-1 .38a.33.33 0 0 0-.2.23.34.34 0 0 0 .1.3l.79.74v.27l-.92.57a.32.32 0 0 0 0 .54l.92.57v.27l-.79.74a.34.34 0 0 0-.1.3.33.33 0 0 0 .2.23l1 .38a2 2 0 0 0 .08.26l-.64.88a.34.34 0 0 0 0 .31.31.31 0 0 0 .24.19l1.07.18.13.24-.45 1a.31.31 0 0 0 0 .31.32.32 0 0 0 .28.14h1.09a1.73 1.73 0 0 0 .17.21l-.25 1.06a.31.31 0 0 0 .38.38l1.06-.25.21.17v1.09a.32.32 0 0 0 .14.28.31.31 0 0 0 .31 0l1-.45.24.13.18 1.07a.31.31 0 0 0 .19.24.34.34 0 0 0 .31 0l.83-.73.26.08.38 1a.33.33 0 0 0 .23.2.34.34 0 0 0 .3-.1l.74-.79h.27l.57.92a.32.32 0 0 0 .54 0l.57-.92h.27l.74.79a.34.34 0 0 0 .3.1.33.33 0 0 0 .23-.2l.38-1 .26-.08.88.64a.34.34 0 0 0 .31 0 .31.31 0 0 0 .19-.24l.18-1.07.24-.13 1 .45a.31.31 0 0 0 .31 0 .32.32 0 0 0 .14-.28v-1.09a1.73 1.73 0 0 0 .21-.17l1.06.25a.31.31 0 0 0 .38-.38l-.25-1.06.18-.21h1.08a.31.31 0 0 0 .3-.45l-.45-1c.05-.08.09-.16.13-.24l1.07-.18a.31.31 0 0 0 .24-.19.34.34 0 0 0 0-.31l-.75-.83a2 2 0 0 0 .08-.26l1-.38a.33.33 0 0 0 .2-.23.34.34 0 0 0-.1-.3l-.79-.74v-.27l.92-.57a.32.32 0 0 0 0-.54zm-6.18 7.66a.65.65 0 0 1 .27-1.28.66.66 0 0 1 .5.78.64.64 0 0 1-.77.5zm-.32-2.12a.59.59 0 0 0-.7.46l-.33 1.53A8 8 0 0 1 11 19a8.16 8.16 0 0 1-3.39-.74l-.33-1.56a.58.58 0 0 0-.7-.46l-1.35.29a7.88 7.88 0 0 1-.7-.83h6.57c.08 0 .12 0 .12-.08V13.3c0-.07 0-.08-.12-.08H9.18v-1.48h2.08a1.32 1.32 0 0 1 1.28 1.11c.08.33.26 1.38.38 1.72s.63 1.13 1.17 1.13h3.39a10.43 10.43 0 0 1-.74.88zm-9.09 2.09a.64.64 0 0 1-.77-.5.65.65 0 0 1 .5-.78.65.65 0 0 1 .27 1.28zM3.77 8.25a.65.65 0 1 1-.86-.33.64.64 0 0 1 .86.33zM3 10.07l1.41-.63a.59.59 0 0 0 .3-.79L4.42 8h1.14v5.14h-2.3A8.54 8.54 0 0 1 3 10.07zm6.18-.5V8.05h2.71c.14 0 1 .16 1 .8s-.65.72-1.18.72zM19 10.93v.6h-.8c-.09 0-.12.05-.12.13V12c0 .89-.5 1.09-.94 1.14s-.89-.18-.95-.44a2.81 2.81 0 0 0-1.31-2.2 3 3 0 0 0 1.65-2.29 2.59 2.59 0 0 0-1.26-2.12 3.63 3.63 0 0 0-1.73-.57H5A8.07 8.07 0 0 1 9.51 3l1 1.06a.6.6 0 0 0 .85 0L12.48 3A8.07 8.07 0 0 1 18 6.94l-.77 1.74a.59.59 0 0 0 .3.79l1.48.66a7 7 0 0 1-.01.8zm-8.53-8.8a.66.66 0 0 1 .93 0 .65.65 0 0 1 0 .92.66.66 0 0 1-.93 0 .65.65 0 0 1 .04-.92zm7.65 6.15A.65.65 0 0 1 19 8a.65.65 0 1 1-.86.33z"/></svg>
'''
'''--- assets/languages/swift.svg ---
<svg width="20" height="18" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 40 36"><defs><linearGradient id="a" x2="50%" x1="50%" y2="100%"><stop stop-color="#F88A36" offset="0"/><stop stop-color="#FD2020" offset="1"/></linearGradient></defs><path d="M29.885 33.047c-4.667 2.696-11.084 2.973-17.54.206C7.118 31.029 2.78 27.136 0 22.688c1.335 1.112 2.892 2.002 4.56 2.78 6.667 3.125 13.333 2.911 18.024.008l-.007-.008C15.904 20.352 10.232 13.679 6.006 8.23c-.89-.89-1.558-2.002-2.225-3.003 5.116 4.671 13.235 10.565 16.126 12.234C13.791 11.011 8.341 3.003 8.563 3.225c9.676 9.787 18.685 15.348 18.685 15.348.298.168.528.308.713.433.195-.496.366-1.01.51-1.545 1.557-5.672-.222-12.123-4.115-17.46 9.008 5.449 14.347 15.68 12.122 24.244-.058.231-.12.46-.189.683l.078.096c4.448 5.561 3.225 11.455 2.67 10.343-2.414-4.722-6.88-3.278-9.152-2.32z" fill="url(#a)"/></svg>
'''
'''--- benches/README.md ---
# Wasmer Benches

This directory contains small, punctual benches. Other benchmarks are
landing somewhere else. We will update this section soon.

'''
'''--- benches/many_functions.rs ---
use criterion::{black_box, criterion_group, criterion_main, BenchmarkId, Criterion};
use wasmer::*;
use wasmer_engine_universal::UniversalExecutableRef;

fn call_many_functions(n: usize) -> String {
    let fndefs = (0..n)
        .map(|idx| format!(r#"(func $fn{idx} return)"#, idx = idx))
        .collect::<String>();
    let calls = (0..n)
        .map(|idx| format!("call $fn{idx}\n", idx = idx))
        .collect::<String>();
    format!(
        r#"(module {fndefs} (func (export "main") {calls} return) (func (export "single") call $fn0 return))"#,
        fndefs = fndefs,
        calls = calls
    )
}

fn nops(c: &mut Criterion) {
    for size in [1, 10, 100, 1000, 10000] {
        let wat = call_many_functions(size);
        let store = Store::new(&Universal::new(Singlepass::new()).engine());
        let mut compile = c.benchmark_group("compile");
        compile.bench_with_input(BenchmarkId::from_parameter(size), &size, |b, _| {
            b.iter(|| {
                let module = Module::new(&store, &wat).unwrap();
                let imports = imports! {};
                let _ = Instance::new(&module, &imports).unwrap();
            })
        });
        drop(compile);

        let module = Module::new(&store, &wat).unwrap();
        let imports = imports! {};
        let instance = Instance::new(&module, &imports).unwrap();
        let mut get_main = c.benchmark_group("get_main");
        get_main.bench_with_input(BenchmarkId::from_parameter(size), &size, |b, _| {
            b.iter(|| {
                let _: Function = instance.lookup_function("main").unwrap();
            })
        });
        drop(get_main);
        let main: Function = instance.lookup_function("main").unwrap();
        let mut call_main = c.benchmark_group("call_main");
        call_main.bench_with_input(BenchmarkId::from_parameter(size), &size, |b, _| {
            b.iter(|| {
                black_box(main.call(&[]).unwrap());
            })
        });
        drop(call_main);

        let single: Function = instance.lookup_function("single").unwrap();
        let mut call_single = c.benchmark_group("call_single");
        call_single.bench_with_input(BenchmarkId::from_parameter(size), &size, |b, _| {
            b.iter(|| {
                black_box(single.call(&[]).unwrap());
            })
        });
        drop(call_single);

        let mut serialize = c.benchmark_group("serialize");
        let wasm = wat::parse_bytes(wat.as_ref()).unwrap();
        let executable = store.engine().compile(&wasm, store.tunables()).unwrap();
        serialize.bench_with_input(BenchmarkId::from_parameter(size), &size, |b, _| {
            b.iter(|| {
                black_box(executable.serialize().unwrap());
            })
        });
        drop(serialize);

        let serialized = executable.serialize().unwrap();
        let mut deserialize = c.benchmark_group("deserialize");
        deserialize.bench_with_input(BenchmarkId::from_parameter(size), &size, |b, _| {
            b.iter(|| unsafe {
                let deserialized = UniversalExecutableRef::deserialize(&serialized).unwrap();
                black_box(store.engine().load(&deserialized).unwrap());
            })
        });
    }
}

criterion_group!(benches, nops);

criterion_main!(benches);

'''
'''--- benches/static_and_dynamic_functions.rs ---
use criterion::{black_box, criterion_group, criterion_main, Criterion};

use wasmer::*;

static BASIC_WAT: &str = r#"(module
    (func $multiply (import "env" "multiply") (param i32 i32) (result i32))
    (func (export "add") (param i32 i32) (result i32)
       (i32.add (local.get 0)
                (local.get 1)))
    (func (export "add20") (param i32 i32 i32 i32 i32
                                  i32 i32 i32 i32 i32
                                  i32 i32 i32 i32 i32
                                  i32 i32 i32 i32 i32) (result i32)
       (i32.add
                (i32.add
                         (i32.add (i32.add (i32.add (local.get 0)  (local.get 1))
                                           (i32.add (local.get 2)  (local.get 3)))
                                  (i32.add (i32.add (local.get 4)  (local.get 5))
                                           (i32.add (local.get 6)  (local.get 7))))
                         (i32.add
                                  (i32.add (i32.add (local.get 8)  (local.get 9))
                                           (i32.add (local.get 10) (local.get 11)))
                                  (i32.add (i32.add (local.get 12) (local.get 13))
                                           (i32.add (local.get 14) (local.get 15)))))

                (i32.add (i32.add (local.get 16) (local.get 17))
                         (i32.add (local.get 18) (local.get 19))))
)
    (func (export "double_then_add") (param i32 i32) (result i32)
       (i32.add (call $multiply (local.get 0) (i32.const 2))
                (call $multiply (local.get 1) (i32.const 2))))
)"#;

pub fn run_basic_static_function(store: &Store, compiler_name: &str, c: &mut Criterion) {
    let module = Module::new(&store, BASIC_WAT).unwrap();
    let import_object = imports! {
        "env" => {
            "multiply" => Function::new_native(&store, |a: i32, b: i32| a * b),
        },
    };
    let instance = Instance::new(&module, &import_object).unwrap();
    let dyn_f = instance.lookup_function("add").unwrap();
    let f: NativeFunc<(i32, i32), i32> = dyn_f.native().unwrap();

    c.bench_function(&format!("basic static func {}", compiler_name), |b| {
        b.iter(|| {
            let result = black_box(f.call(4, 6).unwrap());
            assert_eq!(result, 10);
        })
    });

    let dyn_f_many = instance.lookup_function("add20").unwrap();
    let f_many: NativeFunc<
        (
            i32,
            i32,
            i32,
            i32,
            i32,
            i32,
            i32,
            i32,
            i32,
            i32,
            i32,
            i32,
            i32,
            i32,
            i32,
            i32,
            i32,
            i32,
            i32,
            i32,
        ),
        i32,
    > = dyn_f_many.native().unwrap();
    c.bench_function(
        &format!("basic static func with many args {}", compiler_name),
        |b| {
            b.iter(|| {
                let result = black_box(
                    f_many
                        .call(
                            1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,
                        )
                        .unwrap(),
                );
                assert_eq!(result, 210);
            })
        },
    );
}

pub fn run_basic_dynamic_function(store: &Store, compiler_name: &str, c: &mut Criterion) {
    let module = Module::new(&store, BASIC_WAT).unwrap();
    let import_object = imports! {
        "env" => {
            "multiply" => Function::new_native(&store, |a: i32, b: i32| a * b),
        },
    };
    let instance = Instance::new(&module, &import_object).unwrap();

    let dyn_f = instance.lookup_function("add").unwrap();
    c.bench_function(&format!("basic dynfunc {}", compiler_name), |b| {
        b.iter(|| {
            let dyn_result = black_box(dyn_f.call(&[Val::I32(4), Val::I32(6)]).unwrap());
            assert_eq!(dyn_result[0], Val::I32(10));
        })
    });

    let dyn_f_many = instance.lookup_function("add20").unwrap();
    c.bench_function(
        &format!("basic dynfunc with many args {}", compiler_name),
        |b| {
            b.iter(|| {
                let dyn_result = black_box(
                    dyn_f_many
                        .call(&[
                            Val::I32(1),
                            Val::I32(2),
                            Val::I32(3),
                            Val::I32(4),
                            Val::I32(5),
                            Val::I32(6),
                            Val::I32(7),
                            Val::I32(8),
                            Val::I32(9),
                            Val::I32(10),
                            Val::I32(11),
                            Val::I32(12),
                            Val::I32(13),
                            Val::I32(14),
                            Val::I32(15),
                            Val::I32(16),
                            Val::I32(17),
                            Val::I32(18),
                            Val::I32(19),
                            Val::I32(20),
                        ])
                        .unwrap(),
                );
                assert_eq!(dyn_result[0], Val::I32(210));
            })
        },
    );
}

fn run_static_benchmarks(_c: &mut Criterion) {
    #[cfg(feature = "llvm")]
    {
        let store = Store::new(&Universal::new(wasmer_compiler_llvm::LLVM::new()).engine());
        run_basic_static_function(&store, "llvm", _c);
    }

    #[cfg(feature = "cranelift")]
    {
        let store =
            Store::new(&Universal::new(wasmer_compiler_cranelift::Cranelift::new()).engine());
        run_basic_static_function(&store, "cranelift", _c);
    }

    #[cfg(feature = "singlepass")]
    {
        let store =
            Store::new(&Universal::new(wasmer_compiler_singlepass::Singlepass::new()).engine());
        run_basic_static_function(&store, "singlepass", _c);
    }
}

fn run_dynamic_benchmarks(_c: &mut Criterion) {
    #[cfg(feature = "llvm")]
    {
        let store = Store::new(&Universal::new(wasmer_compiler_llvm::LLVM::new()).engine());
        run_basic_dynamic_function(&store, "llvm", _c);
    }

    #[cfg(feature = "cranelift")]
    {
        let store =
            Store::new(&Universal::new(wasmer_compiler_cranelift::Cranelift::new()).engine());
        run_basic_dynamic_function(&store, "cranelift", _c);
    }

    #[cfg(feature = "singlepass")]
    {
        let store =
            Store::new(&Universal::new(wasmer_compiler_singlepass::Singlepass::new()).engine());
        run_basic_dynamic_function(&store, "singlepass", _c);
    }
}

criterion_group!(benches, run_static_benchmarks, run_dynamic_benchmarks);

criterion_main!(benches);

'''
'''--- bors.toml ---
status = [
    "Audit",
    "Code lint",
    "Test on linux-x64",
    # "Test on linux-musl-x64",
    # "Test on linux-aarch64",
    "Test on macos-x64",
    "Test on windows-x64",
    "Test cross-compile on linux",
    "Test cross-compile on macos",
]
required_approvals = 1
timeout_sec = 7200
delete_merged_branches = true 

'''
'''--- build.rs ---
//! The logic that gets executed before building the binary and tests.
//! We use it to auto-generate the Wasm spectests for each of the
//! available compilers.
//!
//! Please try to keep this file as clean as possible.

use std::env;
use std::fs;
use std::path::PathBuf;
use std::process::Command;
use test_generator::{
    test_directory, test_directory_module, wast_processor, with_test_module, Testsuite,
};

fn main() -> anyhow::Result<()> {
    // As rerun-if-changed doesn't support globs, we use another crate
    // to check changes in directories.

    let out_dir = PathBuf::from(
        env::var_os("OUT_DIR").expect("The OUT_DIR environment variable must be set"),
    );

    // Spectests test generation
    {
        let mut spectests = Testsuite {
            buffer: String::new(),
            path: vec![],
        };

        with_test_module(&mut spectests, "spec", |spectests| {
            let _spec_tests = test_directory(spectests, "tests/wast/spec", wast_processor)?;
            test_directory_module(
                spectests,
                "tests/wast/spec/proposals/multi-value",
                wast_processor,
            )?;
            test_directory_module(spectests, "tests/wast/spec/proposals/simd", wast_processor)?;
            // test_directory_module(spectests, "tests/wast/spec/proposals/bulk-memory-operations", wast_processor)?;
            Ok(())
        })?;
        with_test_module(&mut spectests, "wasmer", |spectests| {
            let _spec_tests = test_directory(spectests, "tests/wast/wasmer", wast_processor)?;
            Ok(())
        })?;

        let spectests_output = out_dir.join("generated_spectests.rs");
        fs::write(&spectests_output, spectests.buffer)?;

        // Write out our auto-generated tests and opportunistically format them with
        // `rustfmt` if it's installed.
        // Note: We need drop because we don't want to run `unwrap` or `expect` as
        // the command might fail, but we don't care about it's result.
        drop(Command::new("rustfmt").arg(&spectests_output).status());
    }

    Ok(())
}

'''
'''--- deny.toml ---
targets = []

[advisories]
db-path = "~/.cargo/advisory-db"
db-urls = ["https://github.com/rustsec/advisory-db"]
vulnerability = "deny"
unmaintained = "warn"
yanked = "deny"
notice = "warn"
ignore = []

[licenses]
unlicensed = "deny"
allow = ["MIT", "Apache-2.0", "Apache-2.0 WITH LLVM-exception"]
deny = ["AGPL-1.0", "AGPL-3.0",]
copyleft = "warn"
allow-osi-fsf-free = "either"
default = "deny"
confidence-threshold = 0.8
exceptions = []

[licenses.private]
ignore = false
registries = []

[bans]
multiple-versions = "deny"
wildcards = "allow"
highlight = "all"
allow = []
deny = []
skip = [
    { name = "ahash", version = "=0.4.7" },
    { name = "hashbrown", version = "=0.9.1" },
    { name = "gimli", version = "=0.25.0" },
    { name = "semver", version = "=0.11.0" },
]
skip-tree = [
    { name = "cranelift-frontend", version = "0.76.0" },
]

[sources]
unknown-registry = "deny"
unknown-git = "deny"
allow-registry = ["https://github.com/rust-lang/crates.io-index"]
allow-git = []

'''
'''--- docs/README.md ---
# Wasmer Documentation

Wasmer provides multiple documentations. Here are some pointers:

* [The Wasmer runtime
  `README.md`](https://github.com/wasmerio/wasmer/blob/master/README.md)
  is a good start for the first steps, like installations, first runs etc.,
* [The public documentation](https://docs.wasmer.io/) contains all the
  documentation you need to learn about Wasmer and WebAssembly,
* [The Rust crates documentations](https://wasmerio.github.io/wasmer/)
  contain all the documentations to use the `wasmer-*` Rust crates,
  with many examples,
* [The collection of
  examples](https://github.com/wasmerio/wasmer/blob/master/examples/README.md)
  illustrates how to use Wasmer and its numerous features through very
  commented examples,
* [Documentations for all embeddings/language
  integrations](https://github.com/wasmerio/wasmer/blob/master/README.md):
  the Wasmer runtime can be embeddeded in various languages or
  environments, each embedding provides its own documentation, book
  etc.,
* [OS distro packaging
  notes](https://github.com/wasmerio/wasmer/blob/master/PACKAGING.md)
  contains notes about how to package Wasmer for OS distributions.

'''
'''--- docs/cn/README.md ---
<div align="center">
  <a href="https://wasmer.io" target="_blank" rel="noopener noreferrer">
    <img width="300" src="https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/logo.png" alt="Wasmer logo">
  </a>

  <p>
    <a href="https://github.com/wasmerio/wasmer/actions?query=workflow%3Abuild">
      <img src="https://github.com/wasmerio/wasmer/workflows/build/badge.svg?style=flat-square" alt="Build Status">
    </a>
    <a href="https://github.com/wasmerio/wasmer/blob/master/LICENSE">
      <img src="https://img.shields.io/github/license/wasmerio/wasmer.svg?style=flat-square" alt="License">
    </a>
    <a href="https://slack.wasmer.io">
      <img src="https://img.shields.io/static/v1?label=Slack&message=join%20chat&color=brighgreen&style=flat-square" alt="Slack channel">
    </a> 
  </p>

  <h3>
    <a href="https://wasmer.io/">ÁΩëÁ´ô</a>
    <span> ‚Ä¢ </span>
    <a href="https://docs.wasmer.io">ÊñáÊ°£</a>
    <span> ‚Ä¢ </span>
    <a href="https://slack.wasmer.io/">ËÅäÂ§©</a>
  </h3>

</div>

<br />

[Wasmer](https://wasmer.io/) Êèê‰æõÂü∫‰∫é [WebAssembly](https://webassembly.org/) ÁöÑË∂ÖËΩªÈáèÁ∫ßÂÆπÂô®ÔºåÂÖ∂ÂèØ‰ª•Âú®‰ªª‰ΩïÂú∞ÊñπËøêË°åÔºö‰ªéÊ°åÈù¢Âà∞‰∫ë„ÄÅ‰ª•Âèä IoT ËÆæÂ§áÔºåÂπ∂‰∏îËÉΩ‰πüÂµåÂÖ•Âú® [*‰ªª‰ΩïÁºñÁ®ãËØ≠Ë®Ä*](https://github.com/wasmerio/wasmer#language-integrations).

> ÂÖ∂‰ªñËØ≠Ë®ÄÁöÑ Readme: [üá©üá™ Deutsch-Âæ∑Ë™û](https://github.com/wasmerio/wasmer/blob/master/docs/de/README.md) ‚Ä¢ [üá¨üáß English-Ëã±Êñá](https://github.com/wasmerio/wasmer/blob/master/README.md) ‚Ä¢ [üá™üá∏ Espa√±ol-Ë•øÁè≠ÁâôËØ≠](https://github.com/wasmerio/wasmer/blob/master/docs/es/README.md) ‚Ä¢ [üá´üá∑ Fran√ßais-Ê≥ïËØ≠](https://github.com/wasmerio/wasmer/blob/master/docs/fr/README.md) ‚Ä¢ [üáØüáµ Êó•Êú¨Ë™û-Êó•Êñá](https://github.com/wasmerio/wasmer/blob/master/docs/ja/README.md).

## ÁâπÊÄß

* **Âø´ÈÄüÂèàÂÆâÂÖ®**. WasmerÂú®ÂÆåÂÖ®Ê≤ôÁõíÂåñÁöÑÁéØÂ¢É‰∏≠‰ª•‚ÄúÊé•ËøëÊú¨Êú∫‚ÄùÁöÑÈÄüÂ∫¶ËøêË°å WebAssembly„ÄÇ

* **ÂèØÊèíÊãî**. Wasmer ÂèØ‰ª•Ê†πÊçÆ‰Ω†ÁöÑÈúÄÊ±ÇÊîØÊåÅ‰∏çÂêåÁöÑÁºñËØëÊ°ÜÊû∂ (LLVMÔºåCranelift ...).

* **ÈÄöÁî®ÁöÑ**. ‰Ω†ÂèØ‰ª•Âú®**‰ªª‰ΩïÂπ≥Âè∞**(macOS, Linux and Windows) ÂíåËäØÁâáÁªÑËøêË°å Wasmer.  

* **Á¨¶ÂêàÊ†áÂáÜ**. ËøêË°åÊó∂ÈÄöËøá‰∫Ü[ÂÆòÊñπWebAssemblyÊµãËØïÈõÜ](https://github.com/WebAssembly/testsuite) ÊîØÊåÅ[WASI](https://github.com/WebAssembly/WASI) Âíå[Emscripten](https://emscripten.org/).

## Âø´ÈÄüÂºÄÂßã

Wasmer ‰∏çÈúÄË¶ÅÂÆâË£ÖÂÖ∂‰ªñ‰æùËµñ. ‰Ω†ÂèØ‰ª•‰ΩøÁî®‰ª•‰∏ãÂÆâË£ÖÁ®ãÂ∫èËøõË°åÂÆâË£Ö:

```sh
curl https://get.wasmer.io -sSfL | sh
```

<details>
  <summary>‰ΩøÁî®Powershell (Windows)</summary>
  <p>

```powershell
iwr https://win.wasmer.io -useb | iex
```

</p>
</details>

> ÊúâÂÖ≥Êõ¥Â§öÂÆâË£ÖÈÄâÈ°πÔºåËØ∑ÂèÇËßÅ [wasmer-install](https://github.com/wasmerio/wasmer-install): Homebrew, Scoop, Cargo...

#### ÊâßË°åWebAssemblyÊñá‰ª∂

ÂÆâË£ÖWasmer‰πãÂêéÔºå‰Ω†Â∑≤ÁªèÂáÜÂ§áÂ•ΩÊâßË°åÁ¨¨‰∏Ä‰∏™WebAssembyÊñá‰ª∂‰∫Ü! üéâ

ÊÇ®ÂèØ‰ª•ÈÄöËøáËøêË°å QuickJS ÂºÄÂßã: [qjs.wasm](https://registry-cdn.wapm.io/contents/_/quickjs/0.0.3/build/qjs.wasm)

```bash
$ wasmer qjs.wasm
QuickJS - Type "\h" for help
qjs >
```

#### Êé•‰∏ãÊù•ÊòØ‰Ω†ÂèØ‰ª•ÂÅöÁöÑ:

- [Âú®‰Ω†ÁöÑRustÂ∫îÁî®Á®ãÂ∫è‰∏≠‰ΩøÁî®Wasmer](https://docs.wasmer.io/integrations/rust)
- [Âú®WAPM‰∏äÂèëÂ∏ÉWasmÁ®ãÂ∫èÂåÖ](https://docs.wasmer.io/ecosystem/wapm/publishing-your-package)
- [ÈòÖËØªÊúâÂÖ≥WasmerÁöÑÊõ¥Â§ö‰ø°ÊÅØ](https://medium.com/wasmer/)

## ËØ≠Ë®ÄÊï¥Âêà

üì¶ Wasmer ËøêË°åÊó∂ËÉΩ‰ª•Â∫ìÁöÑÂΩ¢Âºè**ÂµåÂÖ•Âà∞‰∏çÂêåÁöÑËØ≠Ë®Ä**ÔºåÂõ†Ê≠§‰Ω†ÂèØ‰ª•Âú®‰ªª‰ΩïÂú∞Êñπ‰ΩøÁî®WebAssembly.

| &nbsp; | ËØ≠Ë®Ä | Á®ãÂ∫èÂåÖ | ÊñáÊ°£ |
|-|-|-|-|
| ![Rust logo] | [**Rust**][Rust integration] | [`wasmer` Rust crate] | [ÊñáÊ°£][rust docs]
| ![C logo] | [**C/C++**][C integration] | [`wasmer.h` headers] | [ÊñáÊ°£][c docs] |
| ![C# logo] | [**C#**][C# integration] | [`WasmerSharp` NuGet package] | [ÊñáÊ°£][c# docs] |
| ![D logo] | [**D**][D integration] | [`wasmer` Dub package] | [ÊñáÊ°£][d docs] |
| ![Python logo] | [**Python**][Python integration] | [`wasmer` PyPI package] | [ÊñáÊ°£][python docs] |
| ![JS logo] | [**Javascript**][JS integration] | [`@wasmerio` NPM packages] | [ÊñáÊ°£][js docs] |
| ![Go logo] | [**Go**][Go integration] | [`wasmer` Go package] | [ÊñáÊ°£][go docs] |
| ![PHP logo] | [**PHP**][PHP integration] | [`wasm` PECL package] | [ÊñáÊ°£][php docs] |
| ![Ruby logo] | [**Ruby**][Ruby integration] | [`wasmer` Ruby Gem] | [ÊñáÊ°£][ruby docs] |
| ![Java logo] | [**Java**][Java integration] | [`wasmer/wasmer-jni` Bintray package] | [ÊñáÊ°£][java docs] |
| ![Elixir logo] | [**Elixir**][Elixir integration] | [`wasmex` hex package] | [ÊñáÊ°£][elixir docs] |
| ![R logo] | [**R**][R integration] | *Ê≤°ÊúâÂ∑≤ÂèëÂ∏ÉÁöÑËΩØ‰ª∂ÂåÖ* | [ÊñáÊ°£][r docs] |
| ![Postgres logo] | [**Postgres**][Postgres integration] | *Ê≤°ÊúâÂ∑≤ÂèëÂ∏ÉÁöÑËΩØ‰ª∂ÂåÖ* | [ÊñáÊ°£][postgres docs] |
|  | [**Swift**][Swift integration] | *Ê≤°ÊúâÂ∑≤ÂèëÂ∏ÉÁöÑËΩØ‰ª∂ÂåÖ* | |
| ![Zig logo] | [**Zig**][Zig integration] | *no published package* | |

[üëã Áº∫Â∞ëËØ≠Ë®ÄÔºü](https://github.com/wasmerio/wasmer/issues/new?assignees=&labels=%F0%9F%8E%89+enhancement&template=---feature-request.md&title=)

[rust logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/rust.svg
[rust integration]: https://github.com/wasmerio/wasmer/tree/master/lib/api
[`wasmer` rust crate]: https://crates.io/crates/wasmer/
[rust docs]: https://wasmerio.github.io/wasmer/crates/wasmer

[c logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/c.svg
[c integration]: https://github.com/wasmerio/wasmer/tree/master/lib/c-api
[`wasmer.h` headers]: https://wasmerio.github.io/wasmer/c/
[c docs]: https://wasmerio.github.io/wasmer/c/

[c# logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/csharp.svg
[c# integration]: https://github.com/migueldeicaza/WasmerSharp
[`wasmersharp` nuget package]: https://www.nuget.org/packages/WasmerSharp/
[c# docs]: https://migueldeicaza.github.io/WasmerSharp/

[d logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/d.svg
[d integration]: https://github.com/chances/wasmer-d
[`wasmer` Dub package]: https://code.dlang.org/packages/wasmer
[d docs]: https://chances.github.io/wasmer-d

[python logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/python.svg
[python integration]: https://github.com/wasmerio/wasmer-python
[`wasmer` pypi package]: https://pypi.org/project/wasmer/
[python docs]: https://github.com/wasmerio/wasmer-python#api-of-the-wasmer-extensionmodule

[go logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/go.svg
[go integration]: https://github.com/wasmerio/wasmer-go
[`wasmer` go package]: https://pkg.go.dev/github.com/wasmerio/wasmer-go/wasmer
[go docs]: https://pkg.go.dev/github.com/wasmerio/wasmer-go/wasmer?tab=doc

[php logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/php.svg
[php integration]: https://github.com/wasmerio/wasmer-php
[`wasm` pecl package]: https://pecl.php.net/package/wasm
[php docs]: https://wasmerio.github.io/wasmer-php/wasm/

[js logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/js.svg
[js integration]: https://github.com/wasmerio/wasmer-js
[`@wasmerio` npm packages]: https://www.npmjs.com/org/wasmer
[js docs]: https://docs.wasmer.io/integrations/js/reference-api

[ruby logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/ruby.svg
[ruby integration]: https://github.com/wasmerio/wasmer-ruby
[`wasmer` ruby gem]: https://rubygems.org/gems/wasmer
[ruby docs]: https://www.rubydoc.info/gems/wasmer/

[java logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/java.svg
[java integration]: https://github.com/wasmerio/wasmer-java
[`wasmer/wasmer-jni` bintray package]: https://bintray.com/wasmer/wasmer-jni/wasmer-jni
[java docs]: https://github.com/wasmerio/wasmer-java/#api-of-the-wasmer-library

[elixir logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/elixir.svg
[elixir integration]: https://github.com/tessi/wasmex
[elixir docs]: https://hexdocs.pm/wasmex/api-reference.html
[`wasmex` hex package]: https://hex.pm/packages/wasmex

[r logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/r.svg
[r integration]: https://github.com/dirkschumacher/wasmr
[r docs]: https://github.com/dirkschumacher/wasmr#example

[postgres logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/postgres.svg
[postgres integration]: https://github.com/wasmerio/wasmer-postgres
[postgres docs]: https://github.com/wasmerio/wasmer-postgres#usage--documentation

[swift integration]: https://github.com/AlwaysRightInstitute/SwiftyWasmer

[zig logo]: https://raw.githubusercontent.com/ziglang/logo/master/zig-favicon.png
[zig integration]: https://github.com/zigwasm/wasmer-zig

## Ë¥°ÁåÆ

**Êàë‰ª¨Ê¨¢Ëøé‰ªª‰ΩïÂΩ¢ÂºèÁöÑË¥°ÁåÆÔºåÂ∞§ÂÖ∂ÊòØÊù•Ëá™Á§æÂå∫Êñ∞ÊàêÂëòÁöÑË¥°ÁåÆ** üíú

‰Ω†ÂèØ‰ª•Âú®[Êàë‰ª¨ÁöÑÂá∫Ëâ≤ÊñáÊ°£](https://docs.wasmer.io/ecosystem/wasmer/building-from-source) ‰∏≠Â≠¶‰π†Â¶Ç‰ΩïÊûÑÂª∫ Wasmer ËøêË°åÊó∂!

### ÊµãËØï

ÊÉ≥Ë¶ÅÊµãËØïÂêó?  [ÂèÇËÄÉ Wasmer ÊñáÊ°£](https://docs.wasmer.io/ecosystem/wasmer/building-from-source/testing).

## Á§æÂå∫

Wasmer Êã•Êúâ‰∏Ä‰∏™Áî±Âá∫Ëâ≤ÁöÑÂºÄÂèë‰∫∫ÂëòÂíåË¥°ÁåÆËÄÖÁªÑÊàêÁöÑÁ§æÂå∫„ÄÇ Ê¨¢Ëøé‰Ω†ÔºåËØ∑Âä†ÂÖ•Êàë‰ª¨! üëã

### È¢ëÈÅì

- [Slack](https://slack.wasmer.io/)
- [Twitter](https://twitter.com/wasmerio)
- [Facebook](https://www.facebook.com/wasmerio)
- [Email](mailto:hello@wasmer.io)

'''
'''--- docs/de/README.md ---
<div align="center">
  <a href="https://wasmer.io" target="_blank" rel="noopener noreferrer">
    <img width="300" src="https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/logo.png" alt="Wasmer Logo">
  </a>

  <p>
    <a href="https://github.com/wasmerio/wasmer/actions?query=workflow%3Abuild">
      <img src="https://github.com/wasmerio/wasmer/workflows/build/badge.svg?style=flat-square" alt="Build Status">
    </a>
    <a href="https://github.com/wasmerio/wasmer/blob/master/LICENSE">
      <img src="https://img.shields.io/github/license/wasmerio/wasmer.svg" alt="Lizenz">
    </a>
    <a href="https://docs.wasmer.io">
      <img src="https://img.shields.io/static/v1?label=Docs&message=docs.wasmer.io&color=blue" alt="Wasmer Doku">
    </a>
    <a href="https://slack.wasmer.io">
      <img src="https://img.shields.io/static/v1?label=Slack&message=teilnehmen&color=brighgreen" alt="Slack Kanal">
    </a>
  </p>
</div>

<br />

Wasmer ist eine _schnelle_ und _sichere_ [**WebAssembly**](https://webassembly.org) Runtime, die das Ausf√ºhren von
_schlanken Containern_ √ºberall erm√∂glicht: auf dem *Desktop* in der *Cloud*, so wie auf *Edge* und *IoT* Ger√§ten.

> _Die README ist auch in folgenden Sprachen verf√ºgbar:
[üá®üá≥ ‰∏≠Êñá-Chinesisch](https://github.com/wasmerio/wasmer/blob/master/docs/cn/README.md) ‚Ä¢ 
[üá¨üáß English-Englisch](https://github.com/wasmerio/wasmer/blob/master/README.md) ‚Ä¢
[üá™üá∏ Espa√±ol-Spanisch](https://github.com/wasmerio/wasmer/blob/master/docs/es/README.md) ‚Ä¢ 
[üá´üá∑ Fran√ßais-Franz√∂sisch](https://github.com/wasmerio/wasmer/blob/master/docs/fr/README.md) ‚Ä¢ 
[üáØüáµ Êó•Êú¨Ë™û-Japanisch](https://github.com/wasmerio/wasmer/blob/master/docs/ja/README.md)_.

### Leistungsmerkmale

* Standardm√§√üig sicher. Kein Datei-, Netzwerk- oder Umgebungszugriff, sofern nicht explizit aktiviert.
* Unterst√ºtzt [WASI](https://github.com/WebAssembly/WASI) und [Emscripten](https://emscripten.org/) standardm√§√üig.
* Schnell. F√ºhrt WebAssembly in nahezu nativer Geschwindigkeit aus.
* Einbettbar in [mehrere Programmiersprachen](https://github.com/wasmerio/wasmer/#-language-integrations)
* Kompatibel mit den neuesten Empfehlungen f√ºr WebAssembly (SIMD, Referenztypen, Threads, ...)

### Installation

Wasmer CLI wird als eine einzige ausf√ºhrbare Datei ohne Abh√§ngigkeiten ausgeliefert.

```sh
curl https://get.wasmer.io -sSfL | sh
```

<details>
  <summary>Weitere Installationsm√∂glichkeiten (Powershell, Brew, Cargo, ...)</summary>
  
  _Wasmer kann √ºber verschiedene Paketmanager installiert werden. W√§hlen Sie den f√ºr Ihre Umgebung am besten geeigneten aus:_
  
  * Powershell (Windows)
    ```powershell
    iwr https://win.wasmer.io -useb | iex
    ```

  * <a href="https://formulae.brew.sh/formula/wasmer">Homebrew</a> (macOS, Linux)

    ```sh
    brew install wasmer
    ```

  * <a href="https://github.com/ScoopInstaller/Main/blob/master/bucket/wasmer.json">Scoop</a> (Windows)

    ```sh
    scoop install wasmer
    ```

  * <a href="https://chocolatey.org/packages/wasmer">Chocolatey</a> (Windows)

    ```sh
    choco install wasmer
    ```
  
  * <a href="https://crates.io/crates/wasmer-cli/">Cargo</a>

    _Note: All the available
    features are described in the [`wasmer-cli`
    crate docs](https://github.com/wasmerio/wasmer/tree/master/lib/cli/README.md)_

    ```sh
    cargo install wasmer-cli
    ```

  > Suchen Sie nach weiteren Installationsm√∂glichkeiten? Im [`wasmer-install`
  Repository](https://github.com/wasmerio/wasmer-install) k√∂nnen Si mehr erfahren!
</details>

### Schnellstart

Sie k√∂nnen beginnen,
[QuickJS](https://github.com/bellard/quickjs/) auszuf√ºhren, eine kleine und
einbettbare Javascript Engine, die als WebAssembly Modul kompiliert ist: ([`qjs.wasm`](https://registry-cdn.wapm.io/contents/_/quickjs/0.0.3/build/qjs.wasm)):

```bash
$ wasmer qjs.wasm
QuickJS - Type "\h" for help
qjs > const i = 1 + 2;
qjs > console.log("hello " + i);
hello 3
```

#### Folgendes k√∂nnen Sie als n√§chstes tun:

- [Wasmer f√ºr eine Rust Anwendung nutzen](https://docs.wasmer.io/integrations/rust)
- [Ein asm Paket auf WAPM ver√∂ffentlichen](https://docs.wasmer.io/ecosystem/wapm/publishing-your-package)
- [Mehr zu Wasmer lesen](https://medium.com/wasmer/)

## üì¶ Unterst√ºtzte Sprachen

Die Wasmer-Laufzeit kann als Bibliothek **eingebettet in verschiedenen
Sprachen** verwendet werden, so dass Sie WebAssembly _√ºberall_ einsetzen k√∂nnen.

| | Sprache | Paket | Dokumentation |
|-|-|-|-|
| ![Rust logo] | [**Rust**][Rust Integration] | [`wasmer` Rust crate] | [Lernen][rust docs]
| ![C logo] | [**C/C++**][C Integration] | [`wasmer.h` header] | [Lernen][c docs] |
| ![C# logo] | [**C#**][C# Integration] | [`WasmerSharp` NuGet Paket] | [Lernen][c# docs] |
| ![D logo] | [**D**][D Integration] | [`wasmer` Dub Paket] | [Lernen][d docs] |
| ![Python logo] | [**Python**][Python Integration] | [`wasmer` PyPI Paket] | [Lernen][python docs] |
| ![JS logo] | [**Javascript**][JS Integration] | [`@wasmerio` NPM Paket] | [Lernen][js docs] |
| ![Go logo] | [**Go**][Go Integration] | [`wasmer` Go Paket] | [Lernen][go docs] |
| ![PHP logo] | [**PHP**][PHP Integration] | [`wasm` PECL Paket] | [Lernen][php docs] |
| ![Ruby logo] | [**Ruby**][Ruby Integration] | [`wasmer` Ruby Gem] | [Lernen][ruby docs] |
| ![Java logo] | [**Java**][Java Integration] | [`wasmer/wasmer-jni` Bintray Paket] | [Lernen][java docs] |
| ![Elixir logo] | [**Elixir**][Elixir Integration] | [`wasmex` hex Paket] | [Lernen][elixir docs] |
| ![R logo] | [**R**][R Integration] | *kein Paket ver√∂ffentlicht* | [Lernen][r docs] |
| ![Postgres logo] | [**Postgres**][Postgres Integration] | *kein Paket ver√∂ffentlicht* | [Lernen][postgres docs] |
|  | [**Swift**][Swift Integration] | *kein Paket ver√∂ffentlicht* | |
| ![Zig logo] | [**Zig**][Zig Integration] | *kein Paket ver√∂ffentlicht* | |
| ![Dart logo] | [**Dart**][Dart Integration] | [`wasm` pub Paket] | |

[üëã&nbsp;&nbsp;Fehlt eine Sprache?](https://github.com/wasmerio/wasmer/issues/new?assignees=&labels=%F0%9F%8E%89+enhancement&template=---feature-request.md&title=)

[rust logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/rust.svg
[rust integration]: https://github.com/wasmerio/wasmer/tree/master/lib/api
[`wasmer` rust crate]: https://crates.io/crates/wasmer/
[rust docs]: https://wasmerio.github.io/wasmer/crates/wasmer

[c logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/c.svg
[c integration]: https://github.com/wasmerio/wasmer/tree/master/lib/c-api
[`wasmer.h` header]: https://github.com/wasmerio/wasmer/blob/master/lib/c-api/wasmer.h
[c docs]: https://wasmerio.github.io/wasmer/crates/wasmer_c_api

[c# logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/csharp.svg
[c# integration]: https://github.com/migueldeicaza/WasmerSharp
[`wasmersharp` nuget package]: https://www.nuget.org/packages/WasmerSharp/
[c# docs]: https://migueldeicaza.github.io/WasmerSharp/

[d logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/d.svg
[d integration]: https://github.com/chances/wasmer-d
[`wasmer` Dub package]: https://code.dlang.org/packages/wasmer
[d docs]: https://chances.github.io/wasmer-d

[python logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/python.svg
[python integration]: https://github.com/wasmerio/wasmer-python
[`wasmer` pypi package]: https://pypi.org/project/wasmer/
[python docs]: https://wasmerio.github.io/wasmer-python/api/wasmer/

[go logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/go.svg
[go integration]: https://github.com/wasmerio/wasmer-go
[`wasmer` go package]: https://pkg.go.dev/github.com/wasmerio/wasmer-go/wasmer
[go docs]: https://pkg.go.dev/github.com/wasmerio/wasmer-go/wasmer?tab=doc

[php logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/php.svg
[php integration]: https://github.com/wasmerio/wasmer-php
[`wasm` pecl package]: https://pecl.php.net/package/wasm
[php docs]: https://wasmerio.github.io/wasmer-php/wasm/

[js logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/js.svg
[js integration]: https://github.com/wasmerio/wasmer-js
[`@wasmerio` npm packages]: https://www.npmjs.com/org/wasmer
[js docs]: https://docs.wasmer.io/integrations/js/reference-api

[ruby logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/ruby.svg
[ruby integration]: https://github.com/wasmerio/wasmer-ruby
[`wasmer` ruby gem]: https://rubygems.org/gems/wasmer
[ruby docs]: https://wasmerio.github.io/wasmer-ruby/wasmer_ruby/index.html

[java logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/java.svg
[java integration]: https://github.com/wasmerio/wasmer-java
[`wasmer/wasmer-jni` bintray package]: https://bintray.com/wasmer/wasmer-jni/wasmer-jni
[java docs]: https://github.com/wasmerio/wasmer-java/#api-of-the-wasmer-library

[elixir logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/elixir.svg
[elixir integration]: https://github.com/tessi/wasmex
[elixir docs]: https://hexdocs.pm/wasmex/api-reference.html
[`wasmex` hex package]: https://hex.pm/packages/wasmex

[r logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/r.svg
[r integration]: https://github.com/dirkschumacher/wasmr
[r docs]: https://github.com/dirkschumacher/wasmr#example

[postgres logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/postgres.svg
[postgres integration]: https://github.com/wasmerio/wasmer-postgres
[postgres docs]: https://github.com/wasmerio/wasmer-postgres#usage--documentation

[swift integration]: https://github.com/AlwaysRightInstitute/SwiftyWasmer

[zig logo]: https://raw.githubusercontent.com/ziglang/logo/master/zig-favicon.png
[zig integration]: https://github.com/zigwasm/wasmer-zig

[dart logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/dart.svg
[dart integration]: https://github.com/dart-lang/wasm
[`wasm` pub package]: https://pub.dev/packages/wasm

## Unterst√ºtzen

Wir sind dankbar f√ºr Ihre Hilfe! üíú

Lesen Sie in unserer Dokumentation nach, wie man [Wasmer aus dem
Quellcode kompiliert](https://docs.wasmer.io/ecosystem/wasmer/building-from-source) oder [testen Sie √Ñnderungen](https://docs.wasmer.io/ecosystem/wasmer/building-from-source/testing).

## Community

Wasmer hat eine wunderbare Community von Entwicklern und Mitwirkenden. Sie sind herzlich willkommen, bitte machen Sie mit! üëã

- [Wasmer Community auf Slack](https://slack.wasmer.io/)
- [Wasmer auf Twitter](https://twitter.com/wasmerio)
- [Wasmer auf Facebook](https://www.facebook.com/wasmerio)
- [Email](mailto:hello@wasmer.io)

'''
'''--- docs/deps_dedup.svg ---
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
 "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<!-- Generated by graphviz version 2.47.1 (20210417.1919)
 -->
<!-- Title: dependencies Pages: 1 -->
<svg width="1111pt" height="554pt"
 viewBox="0.00 0.00 1111.00 554.00" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 550)">
<title>dependencies</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-550 1107,-550 1107,4 -4,4"/>
<g id="clust1" class="cluster">
<title>cluster_compiler</title>
<polygon fill="none" stroke="brown" points="422,-280 422,-355 1095,-355 1095,-280 422,-280"/>
<text text-anchor="middle" x="758.5" y="-339.8" font-family="Times,serif" font-size="14.00">Compilers</text>
</g>
<g id="clust2" class="cluster">
<title>cluster_engine</title>
<polygon fill="none" stroke="brown" points="8,-280 8,-355 414,-355 414,-280 8,-280"/>
<text text-anchor="middle" x="211" y="-339.8" font-family="Times,serif" font-size="14.00">Engines</text>
</g>
<g id="clust4" class="cluster">
<title>cluster_abi</title>
<polygon fill="none" stroke="brown" points="301,-427 301,-502 612,-502 612,-427 301,-427"/>
<text text-anchor="middle" x="456.5" y="-486.8" font-family="Times,serif" font-size="14.00">Provided ABIs</text>
</g>
<!-- n0 -->
<g id="node1" class="node">
<title>n0</title>
<ellipse fill="none" stroke="orange" cx="523" cy="-381" rx="38.19" ry="18"/>
<text text-anchor="middle" x="523" y="-377.3" font-family="Times,serif" font-size="14.00">wasmer</text>
</g>
<!-- n6 -->
<g id="node4" class="node">
<title>n6</title>
<ellipse fill="none" stroke="orange" cx="306" cy="-306" rx="100.18" ry="18"/>
<text text-anchor="middle" x="306" y="-302.3" font-family="Times,serif" font-size="14.00">wasmer&#45;engine&#45;universal</text>
</g>
<!-- n0&#45;&gt;n6 -->
<g id="edge12" class="edge">
<title>n0&#45;&gt;n6</title>
<path fill="none" stroke="orange" stroke-dasharray="5,2" d="M488.06,-373.25C467.43,-368.88 440.95,-362.59 418,-355 394.49,-347.23 369.06,-336.52 348.38,-327.2"/>
<polygon fill="orange" stroke="orange" points="349.79,-323.99 339.24,-323.02 346.88,-330.36 349.79,-323.99"/>
</g>
<!-- n7 -->
<g id="node5" class="node">
<title>n7</title>
<ellipse fill="none" stroke="orange" cx="102" cy="-306" rx="85.59" ry="18"/>
<text text-anchor="middle" x="102" y="-302.3" font-family="Times,serif" font-size="14.00">wasmer&#45;engine&#45;dylib</text>
</g>
<!-- n0&#45;&gt;n7 -->
<g id="edge13" class="edge">
<title>n0&#45;&gt;n7</title>
<path fill="none" stroke="orange" stroke-dasharray="5,2" d="M484.8,-378.93C410.73,-376.56 250.17,-369.84 197,-355 175.78,-349.08 153.72,-338.26 136.21,-328.39"/>
<polygon fill="orange" stroke="orange" points="137.9,-325.33 127.49,-323.35 134.39,-331.39 137.9,-325.33"/>
</g>
<!-- n2 -->
<g id="node13" class="node">
<title>n2</title>
<ellipse fill="none" stroke="orange" cx="982" cy="-306" rx="105.08" ry="18"/>
<text text-anchor="middle" x="982" y="-302.3" font-family="Times,serif" font-size="14.00">wasmer&#45;compiler&#45;cranelift</text>
</g>
<!-- n0&#45;&gt;n2 -->
<g id="edge9" class="edge">
<title>n0&#45;&gt;n2</title>
<path fill="none" stroke="orange" stroke-dasharray="5,2" d="M561.13,-378.78C638.16,-376.05 809.84,-368.62 867,-355 892.67,-348.88 919.98,-337.7 941.54,-327.67"/>
<polygon fill="orange" stroke="orange" points="943.18,-330.76 950.71,-323.31 940.18,-324.44 943.18,-330.76"/>
</g>
<!-- n3 -->
<g id="node14" class="node">
<title>n3</title>
<ellipse fill="none" stroke="orange" cx="523" cy="-306" rx="92.88" ry="18"/>
<text text-anchor="middle" x="523" y="-302.3" font-family="Times,serif" font-size="14.00">wasmer&#45;compiler&#45;llvm</text>
</g>
<!-- n0&#45;&gt;n3 -->
<g id="edge10" class="edge">
<title>n0&#45;&gt;n3</title>
<path fill="none" stroke="orange" stroke-dasharray="5,2" d="M523,-362.7C523,-354.25 523,-343.87 523,-334.37"/>
<polygon fill="orange" stroke="orange" points="526.5,-334.18 523,-324.18 519.5,-334.18 526.5,-334.18"/>
</g>
<!-- n4 -->
<g id="node15" class="node">
<title>n4</title>
<ellipse fill="none" stroke="orange" cx="746" cy="-306" rx="112.38" ry="18"/>
<text text-anchor="middle" x="746" y="-302.3" font-family="Times,serif" font-size="14.00">wasmer&#45;compiler&#45;singlepass</text>
</g>
<!-- n0&#45;&gt;n4 -->
<g id="edge11" class="edge">
<title>n0&#45;&gt;n4</title>
<path fill="none" stroke="orange" stroke-dasharray="5,2" d="M557.33,-372.82C577.27,-368.35 602.77,-362.1 625,-355 650.24,-346.94 677.75,-336.2 700.14,-326.91"/>
<polygon fill="orange" stroke="orange" points="701.53,-330.13 709.41,-323.04 698.83,-323.67 701.53,-330.13"/>
</g>
<!-- n1 -->
<g id="node2" class="node">
<title>n1</title>
<ellipse fill="none" stroke="orange" cx="634" cy="-162" rx="73.39" ry="18"/>
<text text-anchor="middle" x="634" y="-158.3" font-family="Times,serif" font-size="14.00">wasmer&#45;compiler</text>
</g>
<!-- n9 -->
<g id="node7" class="node">
<title>n9</title>
<ellipse fill="none" stroke="orange" cx="634" cy="-90" rx="53.09" ry="18"/>
<text text-anchor="middle" x="634" y="-86.3" font-family="Times,serif" font-size="14.00">wasmer&#45;vm</text>
</g>
<!-- n1&#45;&gt;n9 -->
<g id="edge20" class="edge">
<title>n1&#45;&gt;n9</title>
<path fill="none" stroke="orange" stroke-dasharray="5,2" d="M634,-143.7C634,-135.98 634,-126.71 634,-118.11"/>
<polygon fill="orange" stroke="orange" points="637.5,-118.1 634,-108.1 630.5,-118.1 637.5,-118.1"/>
</g>
<!-- n5 -->
<g id="node3" class="node">
<title>n5</title>
<ellipse fill="none" stroke="orange" cx="306" cy="-234" rx="64.99" ry="18"/>
<text text-anchor="middle" x="306" y="-230.3" font-family="Times,serif" font-size="14.00">wasmer&#45;engine</text>
</g>
<!-- n5&#45;&gt;n1 -->
<g id="edge19" class="edge">
<title>n5&#45;&gt;n1</title>
<path fill="none" stroke="orange" stroke-dasharray="5,2" d="M355.82,-222.37C413.2,-210.12 507.6,-189.98 570.28,-176.6"/>
<polygon fill="orange" stroke="orange" points="571.39,-179.94 580.44,-174.43 569.93,-173.09 571.39,-179.94"/>
</g>
<!-- n6&#45;&gt;n5 -->
<g id="edge17" class="edge">
<title>n6&#45;&gt;n5</title>
<path fill="none" stroke="orange" stroke-dasharray="5,2" d="M306,-287.7C306,-279.98 306,-270.71 306,-262.11"/>
<polygon fill="orange" stroke="orange" points="309.5,-262.1 306,-252.1 302.5,-262.1 309.5,-262.1"/>
</g>
<!-- n7&#45;&gt;n5 -->
<g id="edge18" class="edge">
<title>n7&#45;&gt;n5</title>
<path fill="none" stroke="orange" stroke-dasharray="5,2" d="M144.77,-290.33C177.52,-279.09 222.74,-263.57 256.96,-251.83"/>
<polygon fill="orange" stroke="orange" points="258.42,-255.03 266.75,-248.47 256.15,-248.4 258.42,-255.03"/>
</g>
<!-- n8 -->
<g id="node6" class="node">
<title>n8</title>
<ellipse fill="none" stroke="orange" cx="634" cy="-18" rx="59.59" ry="18"/>
<text text-anchor="middle" x="634" y="-14.3" font-family="Times,serif" font-size="14.00">wasmer&#45;types</text>
</g>
<!-- n9&#45;&gt;n8 -->
<g id="edge21" class="edge">
<title>n9&#45;&gt;n8</title>
<path fill="none" stroke="orange" stroke-dasharray="5,2" d="M634,-71.7C634,-63.98 634,-54.71 634,-46.11"/>
<polygon fill="orange" stroke="orange" points="637.5,-46.1 634,-36.1 630.5,-46.1 637.5,-46.1"/>
</g>
<!-- n10 -->
<g id="node8" class="node">
<title>n10</title>
<ellipse fill="none" stroke="orange" cx="382" cy="-528" rx="59.29" ry="18"/>
<text text-anchor="middle" x="382" y="-524.3" font-family="Times,serif" font-size="14.00">wasmer&#45;c&#45;api</text>
</g>
<!-- n11 -->
<g id="node9" class="node">
<title>n11</title>
<ellipse fill="none" stroke="orange" cx="523" cy="-453" rx="80.69" ry="18"/>
<text text-anchor="middle" x="523" y="-449.3" font-family="Times,serif" font-size="14.00">wasmer&#45;emscripten</text>
</g>
<!-- n10&#45;&gt;n11 -->
<g id="edge5" class="edge">
<title>n10&#45;&gt;n11</title>
<path fill="none" stroke="orange" stroke-dasharray="5,2" d="M412.64,-512.56C419.69,-509.16 427.13,-505.5 434,-502 451,-493.34 469.6,-483.38 485.39,-474.79"/>
<polygon fill="orange" stroke="orange" points="487.22,-477.77 494.32,-469.91 483.87,-471.63 487.22,-477.77"/>
</g>
<!-- n12 -->
<g id="node10" class="node">
<title>n12</title>
<ellipse fill="none" stroke="orange" cx="367" cy="-453" rx="57.69" ry="18"/>
<text text-anchor="middle" x="367" y="-449.3" font-family="Times,serif" font-size="14.00">wasmer&#45;wasi</text>
</g>
<!-- n10&#45;&gt;n12 -->
<g id="edge6" class="edge">
<title>n10&#45;&gt;n12</title>
<path fill="none" stroke="orange" stroke-dasharray="5,2" d="M378.45,-509.7C376.69,-501.15 374.53,-490.65 372.56,-481.07"/>
<polygon fill="orange" stroke="orange" points="375.97,-480.27 370.53,-471.18 369.11,-481.68 375.97,-480.27"/>
</g>
<!-- n11&#45;&gt;n0 -->
<g id="edge7" class="edge">
<title>n11&#45;&gt;n0</title>
<path fill="none" stroke="orange" stroke-dasharray="5,2" d="M523,-434.7C523,-426.98 523,-417.71 523,-409.11"/>
<polygon fill="orange" stroke="orange" points="526.5,-409.1 523,-399.1 519.5,-409.1 526.5,-409.1"/>
</g>
<!-- n12&#45;&gt;n0 -->
<g id="edge8" class="edge">
<title>n12&#45;&gt;n0</title>
<path fill="none" stroke="orange" stroke-dasharray="5,2" d="M398.57,-437.83C424.1,-426.38 460.07,-410.24 486.77,-398.26"/>
<polygon fill="orange" stroke="orange" points="488.46,-401.33 496.15,-394.05 485.6,-394.95 488.46,-401.33"/>
</g>
<!-- n13 -->
<g id="node11" class="node">
<title>n13</title>
<ellipse fill="none" stroke="orange" cx="683" cy="-453" rx="61.99" ry="18"/>
<text text-anchor="middle" x="683" y="-449.3" font-family="Times,serif" font-size="14.00">wasmer&#45;cache</text>
</g>
<!-- n13&#45;&gt;n0 -->
<g id="edge4" class="edge">
<title>n13&#45;&gt;n0</title>
<path fill="none" stroke="orange" stroke-dasharray="5,2" d="M650.24,-437.67C623.82,-426.11 586.7,-409.87 559.37,-397.91"/>
<polygon fill="orange" stroke="orange" points="560.71,-394.68 550.14,-393.87 557.9,-401.09 560.71,-394.68"/>
</g>
<!-- n14 -->
<g id="node12" class="node">
<title>n14</title>
<ellipse fill="none" stroke="orange" cx="523" cy="-528" rx="50.09" ry="18"/>
<text text-anchor="middle" x="523" y="-524.3" font-family="Times,serif" font-size="14.00">wasmer&#45;cli</text>
</g>
<!-- n14&#45;&gt;n11 -->
<g id="edge3" class="edge">
<title>n14&#45;&gt;n11</title>
<path fill="none" stroke="orange" stroke-dasharray="5,2" d="M523,-509.7C523,-501.25 523,-490.87 523,-481.37"/>
<polygon fill="orange" stroke="orange" points="526.5,-481.18 523,-471.18 519.5,-481.18 526.5,-481.18"/>
</g>
<!-- n14&#45;&gt;n12 -->
<g id="edge2" class="edge">
<title>n14&#45;&gt;n12</title>
<path fill="none" stroke="orange" stroke-dasharray="5,2" d="M480.19,-518.44C464.87,-514.52 447.75,-509.12 433,-502 419.04,-495.26 404.95,-485.51 393.4,-476.55"/>
<polygon fill="orange" stroke="orange" points="395.47,-473.73 385.47,-470.23 391.11,-479.2 395.47,-473.73"/>
</g>
<!-- n14&#45;&gt;n13 -->
<g id="edge1" class="edge">
<title>n14&#45;&gt;n13</title>
<path fill="none" stroke="orange" stroke-dasharray="5,2" d="M566.27,-518.78C582.34,-514.85 600.44,-509.37 616,-502 630.15,-495.3 644.45,-485.56 656.18,-476.6"/>
<polygon fill="orange" stroke="orange" points="658.53,-479.2 664.23,-470.26 654.21,-473.7 658.53,-479.2"/>
</g>
<!-- n2&#45;&gt;n1 -->
<g id="edge14" class="edge">
<title>n2&#45;&gt;n1</title>
<path fill="none" stroke="orange" stroke-dasharray="5,2" d="M943.09,-289.12C878.21,-262.65 748.27,-209.63 679.35,-181.5"/>
<polygon fill="orange" stroke="orange" points="680.62,-178.24 670.04,-177.71 677.98,-184.72 680.62,-178.24"/>
</g>
<!-- n3&#45;&gt;n1 -->
<g id="edge15" class="edge">
<title>n3&#45;&gt;n1</title>
<path fill="none" stroke="orange" stroke-dasharray="5,2" d="M536.17,-288.15C555.61,-263.28 592.12,-216.57 614.62,-187.79"/>
<polygon fill="orange" stroke="orange" points="617.45,-189.86 620.85,-179.82 611.93,-185.55 617.45,-189.86"/>
</g>
<!-- n4&#45;&gt;n1 -->
<g id="edge16" class="edge">
<title>n4&#45;&gt;n1</title>
<path fill="none" stroke="orange" stroke-dasharray="5,2" d="M732.49,-287.87C712.79,-262.9 676.08,-216.35 653.46,-187.68"/>
<polygon fill="orange" stroke="orange" points="656.15,-185.43 647.21,-179.75 650.65,-189.77 656.15,-185.43"/>
</g>
</g>
</svg>

'''
'''--- docs/es/README.md ---
<div align="center">
  <a href="https://wasmer.io" target="_blank" rel="noopener noreferrer">
    <img width="300" src="https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/logo.png" alt="Wasmer logo">
  </a>
  
  <p>
    <a href="https://github.com/wasmerio/wasmer/actions?query=workflow%3Abuild">
      <img src="https://github.com/wasmerio/wasmer/workflows/build/badge.svg?style=flat-square" alt="Build Status">
    </a>
    <a href="https://github.com/wasmerio/wasmer/blob/master/LICENSE">
      <img src="https://img.shields.io/github/license/wasmerio/wasmer.svg?style=flat-square" alt="License">
    </a>
    <a href="https://slack.wasmer.io">
      <img src="https://img.shields.io/static/v1?label=Slack&message=join%20chat&color=brighgreen&style=flat-square" alt="Slack channel">
    </a> 
  </p>

  <h3>
    <a href="https://wasmer.io/">Web</a>
    <span> ‚Ä¢ </span>
    <a href="https://docs.wasmer.io">Documentaci√≥n</a>
    <span> ‚Ä¢ </span>
    <a href="https://slack.wasmer.io/">Chat</a>
  </h3>

</div>

<br />

[Wasmer](https://wasmer.io/) hace posible tener contenedores ultraligeros basados en [WebAssembly](https://webassembly.org/) que pueden ser ejecutados en cualquier sitio: desde tu ordenador hasta la nube y dispositivos de IoT, adem√°s de poder ser ejecutados [*en cualquier lenguaje de programaci√≥n*](https://github.com/wasmerio/wasmer#language-integrations).

> This README is also available in: [üá©üá™ Deutsch-Alem√°n](https://github.com/wasmerio/wasmer/blob/master/docs/de/README.md) ‚Ä¢ [üá¨üáß English-Ingl√©s](https://github.com/wasmerio/wasmer/blob/master/README.md) ‚Ä¢ [üá´üá∑ Fran√ßais-Franc√©s](https://github.com/wasmerio/wasmer/blob/master/docs/fr/README.md) ‚Ä¢ [üá®üá≥ ‰∏≠Êñá-Chino](https://github.com/wasmerio/wasmer/blob/master/docs/cn/README.md) ‚Ä¢ [üáØüáµ Êó•Êú¨Ë™û-japon√©s](https://github.com/wasmerio/wasmer/blob/master/docs/ja/README.md).

## Funcionalidades

* **R√°pido y Seguro**. Wasmer ejecuta WebAssembly a velocidades *nativas* en un entorno completamente protegido.

* **Extendible**. Wasmer soporta diferentes m√©todos de compilaci√≥n dependiendo de tus necesidades (LLVM, Cranelift...).

* **Universal**. Puedes ejecutar Wasmer en cualquier *platforma* (macOS, Linux y Windows) y *chip*.

* **Respeta los est√°ndares**. Wasmer pasa los [tests oficiales de WebAssembly](https://github.com/WebAssembly/testsuite) siendo compatible con [WASI](https://github.com/WebAssembly/WASI) y [Emscripten](https://emscripten.org/).

## Empezamos?

Wasmer no requiere ninguna dependencia. Puedes instalarlo con uno de √©stos instaladores:

```sh
curl https://get.wasmer.io -sSfL | sh
```

<details>
  <summary>Con PowerShell (Windows)</summary>
  <p>

```powershell
iwr https://win.wasmer.io -useb | iex
```

</p>
</details>

> Visita [wasmer-install](https://github.com/wasmerio/wasmer-install) para m√°s opciones de instalaci√≥n: Homebrew, Scoop, Cargo...

#### Ejecuta un archivo WebAssembly

¬°Despu√©s de instalar Wasmer deber√≠as estar listo para ejecutar tu primer m√≥dulo de WebAssembly! üéâ

Puedes empezar corriendo QuickJS: [qjs.wasm](https://registry-cdn.wapm.io/contents/_/quickjs/0.0.3/build/qjs.wasm)

```bash
$ wasmer qjs.wasm
QuickJS - Type "\h" for help
qjs >
```

#### Esto es lo que puedes hacer:

- [Usa Wasmer desde tu aplicaci√≥n de Rust](https://docs.wasmer.io/integrations/rust)
- [Publica un paquete de Wasm en WAPM](https://docs.wasmer.io/ecosystem/wapm/publishing-your-package)
- [Lee m√°s sobre Wasmer](https://medium.com/wasmer/)

## Integraciones en diferentes Lenguajes

üì¶ Wasmer puede ser usado como una librer√≠a **integrado en diferentes lenguajes de programaci√≥n**, para que puedas ejecutar WebAssembly _en cualquier sitio_.

| &nbsp; | Lenguaje | Librer√≠a | Documentaci√≥n |
|-|-|-|-|
| ![Rust logo] | [**Rust**][Rust integration] | [`wasmer` en crates.io] | [Documentaci√≥n][rust docs]
| ![C logo] | [**C/C++**][C integration] | [cabecera `wasmer.h`] | [Documentaci√≥n][c docs] |
| ![C# logo] | [**C#**][C# integration] | [`WasmerSharp` en NuGet] | [Documentaci√≥n][c# docs] |
| ![D logo] | [**D**][D integration] | [`wasmer` en Dug] | [Documentaci√≥n][d docs] |
| ![Python logo] | [**Python**][Python integration] | [`wasmer` en PyPI] | [Documentaci√≥n][python docs] |
| ![JS logo] | [**Javascript**][JS integration] | [`@wasmerio` en NPM] | [Documentaci√≥n][js docs] |
| ![Go logo] | [**Go**][Go integration] | [`wasmer` en Go] | [Documentaci√≥n][go docs] |
| ![PHP logo] | [**PHP**][PHP integration] | [`wasm` en PECL] | [Documentaci√≥n][php docs] |
| ![Ruby logo] | [**Ruby**][Ruby integration] | [`wasmer` en Ruby Gems] | [Documentaci√≥n][ruby docs] |
| ![Java logo] | [**Java**][Java integration] | [`wasmer/wasmer-jni` en Bintray] | [Documentaci√≥n][java docs] |
| ![Elixir logo] | [**Elixir**][Elixir integration] | [`wasmex` en hex] | [Documentaci√≥n][elixir docs] |
| ![R logo] | [**R**][R integration] | *sin paquete publicado* | [Documentaci√≥n][r docs] |
| ![Postgres logo] | [**Postgres**][Postgres integration] | *sin paquete publicado* | [Documentaci√≥n][postgres docs] |
|  | [**Swift**][Swift integration] | *sin paquete publicado* | |
| ![Zig logo] | [**Zig**][Zig integration] | *no published package* | |

[üëã Falta alg√∫n lenguaje?](https://github.com/wasmerio/wasmer/issues/new?assignees=&labels=%F0%9F%8E%89+enhancement&template=---feature-request.md&title=)

[rust logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/rust.svg
[rust integration]: https://github.com/wasmerio/wasmer/tree/master/lib/api
[`wasmer` en crates.io]: https://crates.io/crates/wasmer/
[rust docs]: https://wasmerio.github.io/wasmer/crates/wasmer

[c logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/c.svg
[c integration]: https://github.com/wasmerio/wasmer/tree/master/lib/c-api
[cabecera `wasmer.h`]: https://wasmerio.github.io/wasmer/c/
[c docs]: https://wasmerio.github.io/wasmer/c/

[c# logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/csharp.svg
[c# integration]: https://github.com/migueldeicaza/WasmerSharp
[`wasmersharp` en NuGet]: https://www.nuget.org/packages/WasmerSharp/
[c# docs]: https://migueldeicaza.github.io/WasmerSharp/

[d logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/d.svg
[d integration]: https://github.com/chances/wasmer-d
[`wasmer` en Dub]: https://code.dlang.org/packages/wasmer
[d docs]: https://chances.github.io/wasmer-d

[python logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/python.svg
[python integration]: https://github.com/wasmerio/wasmer-python
[`wasmer` en pypi]: https://pypi.org/project/wasmer/
[python docs]: https://github.com/wasmerio/wasmer-python#api-of-the-wasmer-extensionmodule

[go logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/go.svg
[go integration]: https://github.com/wasmerio/wasmer-go
[`wasmer` en go]: https://pkg.go.dev/github.com/wasmerio/wasmer-go/wasmer
[go docs]: https://pkg.go.dev/github.com/wasmerio/wasmer-go/wasmer?tab=doc

[php logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/php.svg
[php integration]: https://github.com/wasmerio/wasmer-php
[php docs]: https://wasmerio.github.io/wasmer-php/wasm/
[`wasm` en pecl]: https://pecl.php.net/package/wasm

[js logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/js.svg
[js integration]: https://github.com/wasmerio/wasmer-js
[`@wasmerio` en npm]: https://www.npmjs.com/org/wasmer
[js docs]: https://docs.wasmer.io/integrations/js/reference-api

[ruby logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/ruby.svg
[ruby integration]: https://github.com/wasmerio/wasmer-ruby
[`wasmer` en ruby gems]: https://rubygems.org/gems/wasmer
[ruby docs]: https://www.rubydoc.info/gems/wasmer/

[java logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/java.svg
[java integration]: https://github.com/wasmerio/wasmer-java
[`wasmer/wasmer-jni` en bintray]: https://bintray.com/wasmer/wasmer-jni/wasmer-jni
[java docs]: https://github.com/wasmerio/wasmer-java/#api-of-the-wasmer-library

[elixir logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/elixir.svg
[elixir integration]: https://github.com/tessi/wasmex
[elixir docs]: https://hexdocs.pm/wasmex/api-reference.html
[`wasmex` en hex]: https://hex.pm/packages/wasmex

[r logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/r.svg
[r integration]: https://github.com/dirkschumacher/wasmr
[r docs]: https://github.com/dirkschumacher/wasmr#example

[postgres logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/postgres.svg
[postgres integration]: https://github.com/wasmerio/wasmer-postgres
[postgres docs]: https://github.com/wasmerio/wasmer-postgres#usage--documentation

[swift integration]: https://github.com/AlwaysRightInstitute/SwiftyWasmer

[zig logo]: https://raw.githubusercontent.com/ziglang/logo/master/zig-favicon.png
[zig integration]: https://github.com/zigwasm/wasmer-zig

## Contribuye

**Damos la bienvenida a cualquier forma de contribuci√≥n, especialmente a los nuevos miembros de la comunidad** üíú

¬°Puedes ver c√≥mo crear el binario de Wasmer con [nuestros incre√≠bles documentos](https://docs.wasmer.io/ecosystem/wasmer/building-from-source)!

### Tests

Testear quieres? Los [documentos de Wasmer te ense√±ar√°n c√≥mo](https://docs.wasmer.io/ecosystem/wasmer/building-from-source/testing).

## Comunidad

Wasmer tiene una comunidad incre√≠ble de desarrolladores y colaboradores ¬°Bienvenido, √∫nete a nosotros! üëã

### Medios

- [Slack](https://slack.wasmer.io/)
- [Twitter](https://twitter.com/wasmerio)
- [Facebook](https://www.facebook.com/wasmerio)
- [Email](mailto:hello@wasmer.io)

'''
'''--- docs/fr/README.md ---
<div align="center">
  <a href="https://wasmer.io" target="_blank" rel="noopener noreferrer">
    <img width="300" src="https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/logo.png" alt="Logo Wasmer">
  </a>
  
  <p>
    <a href="https://github.com/wasmerio/wasmer/actions?query=workflow%3Abuild">
      <img src="https://github.com/wasmerio/wasmer/workflows/build/badge.svg?style=flat-square" alt="√âtat des tests">
    </a>
    <a href="https://github.com/wasmerio/wasmer/blob/master/LICENSE">
      <img src="https://img.shields.io/github/license/wasmerio/wasmer.svg?style=flat-square" alt="Licence">
    </a>
    <a href="https://slack.wasmer.io">
      <img src="https://img.shields.io/static/v1?label=Slack&message=join%20chat&color=brighgreen&style=flat-square" alt="Salon Slack">
    </a> 
  </p>

  <h3>
    <a href="https://wasmer.io/">Web</a>
    <span> ‚Ä¢ </span>
    <a href="https://docs.wasmer.io">Documentation</a>
    <span> ‚Ä¢ </span>
    <a href="https://slack.wasmer.io/">Chat</a>
  </h3>

</div>

<br />

[Wasmer](https://wasmer.io/) permet l'utilisation de conteneurs super l√©gers bas√©s sur [WebAssembly](https://webassembly.org/) qui peuvent fonctionner n'importe o√π : du bureau au cloud en passant par les appareils IoT, et √©galement int√©gr√©s dans [*une multitude de langages de programmation*](https://github.com/wasmerio/wasmer#language-integrations).

> This readme is also available in: [üá©üá™ Deutsch-Allemand](https://github.com/wasmerio/wasmer/blob/master/docs/de/README.md) ‚Ä¢ [üá¨üáß English-Anglaise](https://github.com/wasmerio/wasmer/blob/master/README.md) ‚Ä¢ [üá™üá∏ Espa√±ol-Espagnol](https://github.com/wasmerio/wasmer/blob/master/docs/es/README.md) ‚Ä¢ [üá®üá≥ ‰∏≠Êñá-Chinoise](https://github.com/wasmerio/wasmer/blob/master/docs/cn/README.md) ‚Ä¢ [üáØüáµ Êó•Êú¨Ë™û-japonais](https://github.com/wasmerio/wasmer/blob/master/docs/ja/README.md)

## Fonctionnalit√©s

* **Rapide et s√ªr**. Wasmer ex√©cute WebAssembly √† une vitesse *quasi native* dans un environnement enti√®rement contr√¥l√© (bac √† sable, _sandbox_).

* **Modulaire**. Wasmer prend en charge diff√©rents frameworks de compilation pour r√©pondre au mieux √† vos besoins (LLVM, Cranelift ...).

* **Universel**. Vous pouvez ex√©cuter Wasmer sur n'importe quelle *plate-forme* (macOS, Linux et Windows) et *processeur*.

* **Conforme aux normes**. Wasmer passe [la suite de tests officielle de WebAssembly](https://github.com/WebAssembly/testsuite) prenant en charge [WASI](https://github.com/WebAssembly/WASI) et [Emscripten](https://emscripten.org/)

## Quickstart

Wasmer est livr√© sans aucune d√©pendance. Vous pouvez l'installer √† l'aide des programmes d'installation ci-dessous¬†:

```sh
curl https://get.wasmer.io -sSfL | sh
```

<details>
  <summary>Avec PowerShell (Windows)</summary>
  <p>

```powershell
iwr https://win.wasmer.io -useb | iex
```

</p>
</details>

> Voir [wasmer-install](https://github.com/wasmerio/wasmer-install) pour plus d'options d'installation: Homebrew, Scoop, Cargo...

#### Ex√©cution d'un fichier WebAssembly

Apr√®s avoir install√© Wasmer, vous devriez √™tre pr√™t √† ex√©cuter votre premier fichier WebAssemby¬†! üéâ

Vous pouvez commencer par ex√©cuter QuickJS¬†: [qjs.wasm](https://registry-cdn.wapm.io/contents/_/quickjs/0.0.3/build/qjs.wasm)

```bash
$ wasmer qjs.wasm
QuickJS - Type "\h" for help
qjs >
```

#### Voici ce que vous pouvez faire ensuite

- [Utilisez Wasmer depuis votre application Rust](https://docs.wasmer.io/integrations/rust)
- [Publier un paquet Wasm sur WAPM](https://docs.wasmer.io/ecosystem/wapm/publishing-your-package)
- [En savoir plus sur Wasmer](https://medium.com/wasmer/)

## Int√©grations

üì¶  Wasmer peut √™tre utilis√© comme une biblioth√®que **int√©gr√©e dans diff√©rents langages**, vous pouvez donc utiliser WebAssembly _n'import o√π_.

| &nbsp; | Langage de programmation | Package | Docs |
|-|-|-|-|
| ![Rust logo] | [**Rust**][Rust integration] | [`wasmer` Rust crate] | [Docs][rust docs]
| ![C logo] | [**C/C++**][C integration] | [`wasmer.h` headers] | [Docs][c docs] |
| ![C# logo] | [**C#**][C# integration] | [`WasmerSharp` NuGet package] | [Docs][c# docs] |
| ![D logo] | [**D**][D integration] | [`wasmer` Dub package] | [Docs][d docs] |
| ![Python logo] | [**Python**][Python integration] | [`wasmer` PyPI package] | [Docs][python docs] |
| ![JS logo] | [**Javascript**][JS integration] | [`@wasmerio` NPM packages] | [Docs][js docs] |
| ![Go logo] | [**Go**][Go integration] | [`wasmer` Go package] | [Docs][go docs] |
| ![PHP logo] | [**PHP**][PHP integration] | [`wasm` PECL package] | [Docs][php docs] |
| ![Ruby logo] | [**Ruby**][Ruby integration] | [`wasmer` Ruby Gem] | [Docs][ruby docs] |
| ![Java logo] | [**Java**][Java integration] | [`wasmer/wasmer-jni` Bintray package] | [Docs][java docs] |
| ![Elixir logo] | [**Elixir**][Elixir integration] | [`wasmex` hex package] | [Docs][elixir docs] |
| ![R logo] | [**R**][R integration] | *no published package* | [Docs][r docs] |
| ![Postgres logo] | [**Postgres**][Postgres integration] | *no published package* | [Docs][postgres docs] |
|  | [**Swift**][Swift integration] | *no published package* | |
| ![Zig logo] | [**Zig**][Zig integration] | *no published package* | |

[üëã  Il manque un langage ?](https://github.com/wasmerio/wasmer/issues/new?assignees=&labels=%F0%9F%8E%89+enhancement&template=---feature-request.md&title=)

[rust logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/rust.svg
[rust integration]: https://github.com/wasmerio/wasmer/tree/master/lib/api
[`wasmer` rust crate]: https://crates.io/crates/wasmer/
[rust docs]: https://wasmerio.github.io/wasmer/crates/wasmer

[c logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/c.svg
[c integration]: https://github.com/wasmerio/wasmer/tree/master/lib/c-api
[`wasmer.h` headers]: https://wasmerio.github.io/wasmer/c/
[c docs]: https://wasmerio.github.io/wasmer/c/

[c# logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/csharp.svg
[c# integration]: https://github.com/migueldeicaza/WasmerSharp
[`wasmersharp` nuget package]: https://www.nuget.org/packages/WasmerSharp/
[c# docs]: https://migueldeicaza.github.io/WasmerSharp/

[d logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/d.svg
[d integration]: https://github.com/chances/wasmer-d
[`wasmer` Dub package]: https://code.dlang.org/packages/wasmer
[d docs]: https://chances.github.io/wasmer-d

[python logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/python.svg
[python integration]: https://github.com/wasmerio/wasmer-python
[`wasmer` pypi package]: https://pypi.org/project/wasmer/
[python docs]: https://github.com/wasmerio/wasmer-python#api-of-the-wasmer-extensionmodule

[go logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/go.svg
[go integration]: https://github.com/wasmerio/wasmer-go
[`wasmer` go package]: https://pkg.go.dev/github.com/wasmerio/wasmer-go/wasmer
[go docs]: https://pkg.go.dev/github.com/wasmerio/wasmer-go/wasmer?tab=doc

[php logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/php.svg
[php integration]: https://github.com/wasmerio/wasmer-php
[`wasm` pecl package]: https://pecl.php.net/package/wasm
[php docs]: https://wasmerio.github.io/wasmer-php/wasm/

[js logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/js.svg
[js integration]: https://github.com/wasmerio/wasmer-js
[`@wasmerio` npm packages]: https://www.npmjs.com/org/wasmer
[js docs]: https://docs.wasmer.io/integrations/js/reference-api

[ruby logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/ruby.svg
[ruby integration]: https://github.com/wasmerio/wasmer-ruby
[`wasmer` ruby gem]: https://rubygems.org/gems/wasmer
[ruby docs]: https://www.rubydoc.info/gems/wasmer/

[java logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/java.svg
[java integration]: https://github.com/wasmerio/wasmer-java
[`wasmer/wasmer-jni` bintray package]: https://bintray.com/wasmer/wasmer-jni/wasmer-jni
[java docs]: https://github.com/wasmerio/wasmer-java/#api-of-the-wasmer-library

[elixir logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/elixir.svg
[elixir integration]: https://github.com/tessi/wasmex
[elixir docs]: https://hexdocs.pm/wasmex/api-reference.html
[`wasmex` hex package]: https://hex.pm/packages/wasmex

[r logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/r.svg
[r integration]: https://github.com/dirkschumacher/wasmr
[r docs]: https://github.com/dirkschumacher/wasmr#example

[postgres logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/postgres.svg
[postgres integration]: https://github.com/wasmerio/wasmer-postgres
[postgres docs]: https://github.com/wasmerio/wasmer-postgres#usage--documentation

[swift integration]: https://github.com/AlwaysRightInstitute/SwiftyWasmer

[zig logo]: https://raw.githubusercontent.com/ziglang/logo/master/zig-favicon.png
[zig integration]: https://github.com/zigwasm/wasmer-zig

## Contribuer

**Nous accueillons toutes formes de contributions, en particulier de la part des nouveaux membres de notre communaut√©**. üíú

Vous pouvez v√©rifier comment compiler Wasmer dans [notre documentation](https://docs.wasmer.io/ecosystem/wasmer/building-from-source)!

### Test

Vous voulez des tests ? La [documentation de Wasmer](https://docs.wasmer.io/ecosystem/wasmer/building-from-source/testing) vous montrera comment les ex√©cuter.

## Communaut√©

Wasmer a une incroyable communaut√© de d√©veloppeurs et de contributeurs. Bienvenue et rejoignez-nous ! üëã

### Canaux de communications

- [Slack](https://slack.wasmer.io/)
- [Twitter](https://twitter.com/wasmerio)
- [Facebook](https://www.facebook.com/wasmerio)
- [Email](mailto:hello@wasmer.io)

'''
'''--- docs/ja/README.md ---
<div align="center">
  <a href="https://wasmer.io" target="_blank" rel="noopener noreferrer">
    <img width="300" src="https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/logo.png" alt="Wasmer„É≠„Ç¥">
  </a>
  
  <p>
    <a href="https://github.com/wasmerio/wasmer/actions?query=workflow%3Abuild">
      <img src="https://github.com/wasmerio/wasmer/workflows/build/badge.svg?style=flat-square" alt="„Éì„É´„Éâ„Çπ„ÉÜ„Éº„Çø„Çπ">
    </a>
    <a href="https://github.com/wasmerio/wasmer/blob/master/LICENSE">
      <img src="https://img.shields.io/github/license/wasmerio/wasmer.svg?style=flat-square" alt="„É©„Ç§„Çª„É≥„Çπ">
    </a>
    <a href="https://slack.wasmer.io">
      <img src="https://img.shields.io/static/v1?label=Slack&message=join%20chat&color=brighgreen&style=flat-square" alt="Slack„ÉÅ„É£„É≥„Éç„É´">
    </a> 
  </p>

  <h3>
    <a href="https://wasmer.io/">Website</a>
    <span> ‚Ä¢ </span>
    <a href="https://docs.wasmer.io">Docs</a>
    <span> ‚Ä¢ </span>
    <a href="https://slack.wasmer.io/">Chat</a>
  </h3>

</div>

<br />

[Wasmer](https://wasmer.io/) „ÅØ„ÄÅ[WebAssembly](https://webassembly.org/) „Çí„Éô„Éº„Çπ„Å®„Åó„ÅüÈùûÂ∏∏„Å´ËªΩÈáè„Å™„Ç≥„É≥„ÉÜ„Éä„ÇíÂÆüÁèæ„Åó„Åæ„Åô„ÄÇ„Éá„Çπ„ÇØ„Éà„ÉÉ„Éó„Åã„Çâ„ÇØ„É©„Ç¶„Éâ„ÇÑ IoT „Éá„Éê„Ç§„Çπ‰∏ä„Åæ„Åß„ÄÅ„Å©„Çì„Å™Áí∞Â¢É„Åß„ÇÇÂÆüË°å„Åß„Åç„ÄÅ„Åï„Çâ„Å´[*‰ªªÊÑè„ÅÆ„Éó„É≠„Ç∞„É©„Éü„É≥„Ç∞Ë®ÄË™û*](#‰ªñ„ÅÆË®ÄË™û„Å®„ÅÆ„Ç§„É≥„ÉÜ„Ç∞„É¨„Éº„Ç∑„Éß„É≥)„Å´Âüã„ÇÅËæº„ÇÄ„Åì„Å®„ÇÇ„Åß„Åç„Åæ„Åô„ÄÇ

> „Åì„ÅÆ readme „ÅØ„ÄÅÊ¨°„ÅÆË®ÄË™û„Åß„ÇÇÂà©Áî®ÂèØËÉΩ„Åß„Åô„ÄÇ[üá©üá™ Deutsch-„Éâ„Ç§„ÉÑË™û](https://github.com/wasmerio/wasmer/blob/master/docs/de/README.md) ‚Ä¢ [üá®üá≥ ‰∏≠Êñá-Chinese](https://github.com/wasmerio/wasmer/blob/master/docs/cn/README.md) ‚Ä¢ [üá¨üáß English-Ëã±Ë™û](https://github.com/wasmerio/wasmer/blob/master/README.md) ‚Ä¢ [üá™üá∏ Espa√±ol-Spanish](https://github.com/wasmerio/wasmer/blob/master/docs/es/README.md) ‚Ä¢ [üá´üá∑ Fran√ßais-French](https://github.com/wasmerio/wasmer/blob/master/docs/fr/README.md)

## Ê©üËÉΩ

* **È´òÈÄü„Åã„Å§ÂÆâÂÖ®**„ÄÇWebAssembly „ÇíÂÆåÂÖ®„Å™„Çµ„É≥„Éâ„Éú„ÉÉ„ÇØ„ÇπÁí∞Â¢ÉÂÜÖ„Åß*„Éç„Ç§„ÉÜ„Ç£„Éñ„Å´Ëøë„ÅÑ*„Çπ„Éî„Éº„Éâ„ÅßÂÆüË°å„Åó„Åæ„Åô„ÄÇ

* **„Éó„É©„Ç¨„Éñ„É´**„ÄÇÁï∞„Å™„Çã„Ç≥„É≥„Éë„Ç§„É´„Éï„É¨„Éº„É†„ÉØ„Éº„ÇØ (LLVM„ÄÅCranelift „Å™„Å©...) „Çí„Çµ„Éù„Éº„Éà„Åó„Å¶„ÅÑ„Çã„Åü„ÇÅ„ÄÅ„Éã„Éº„Ç∫„Å´Âêà„Å£„ÅüÊúÄÈÅ©„Å™„Éï„É¨„Éº„É†„ÉØ„Éº„ÇØ„ÇíÈÅ∏Êäû„Åß„Åç„Åæ„Åô„ÄÇ

* **„É¶„Éã„Éê„Éº„Çµ„É´**„ÄÇ„Å©„Çì„Å™„Éó„É©„ÉÉ„Éà„Éï„Ç©„Éº„É†‰∏ä (macOS„ÄÅLinux„ÄÅWindows) „Åß„ÇÇ„ÄÅ„Å©„Çì„Å™*„ÉÅ„ÉÉ„Éó„Çª„ÉÉ„Éà*‰∏ä„Åß„ÇÇÂÆüË°å„Åß„Åç„Åæ„Åô„ÄÇ

* **Ê®ôÊ∫ñ„Å´Ê∫ñÊã†**„ÄÇ„É©„É≥„Çø„Ç§„É†„ÅØ[ÂÖ¨Âºè„ÅÆ WebAssembly „ÉÜ„Çπ„Éà„Çπ„Ç§„Éº„Éà](https://github.com/WebAssembly/testsuite)„Å´ÈÄö„Å£„Å¶„Åä„Çä„ÄÅ[WASI](https://github.com/WebAssembly/WASI) „Å® [Emscripten](https://emscripten.org/) „Çí„Çµ„Éù„Éº„Éà„Åó„Åæ„Åô„ÄÇ

## „ÇØ„Ç§„ÉÉ„ÇØ„Çπ„Çø„Éº„Éà

Wasmer „ÅØ‰æùÂ≠òÈñ¢‰øÇ„Å™„Åó„ÅßÂãï‰Ωú„Åó„Åæ„Åô„ÄÇ‰ª•‰∏ã„ÅÆ„Ç≥„Éû„É≥„Éâ„Åß„Ç§„É≥„Çπ„Éà„Éº„É©„Éº„Çí‰ΩøÁî®„Åó„Å¶„Ç§„É≥„Çπ„Éà„Éº„É´„Åß„Åç„Åæ„Åô„ÄÇ

```sh
curl https://get.wasmer.io -sSfL | sh
```

<details>
  <summary>PowerShell „ÅÆÂ†¥Âêà (Windows)</summary>
  <p>

```powershell
iwr https://win.wasmer.io -useb | iex
```

</p>
</details>

> Homebrew„ÄÅScoop„ÄÅCargo „Å™„Å©„ÄÅ‰ªñ„ÅÆ„Ç§„É≥„Çπ„Éà„Éº„É´ÊñπÊ≥ï„Å´„Å§„ÅÑ„Å¶„ÅØ„ÄÅ[wasmer-install](https://github.com/wasmerio/wasmer-install) „ÇíÂèÇÁÖß„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

#### WebAssembly „Éï„Ç°„Ç§„É´„ÅÆÂÆüË°å

Wasmer „Çí„Ç§„É≥„Çπ„Éà„Éº„É´„Åó„Åü„Çâ„ÄÅÂàù„ÇÅ„Å¶„ÅÆ WebAssembly „Éï„Ç°„Ç§„É´„ÅÆÂÆüË°åÊ∫ñÂÇô„ÅåÂÆå‰∫Ü„Åß„ÅôÔºÅ üéâ

QuickJS ([qjs.wasm](https://registry-cdn.wapm.io/contents/_/quickjs/0.0.3/build/qjs.wasm)) „ÇíÂÆüË°å„Åô„Çã„Åì„Å®„Åß„ÄÅ„Åô„Åê„Å´Âßã„ÇÅ„Çâ„Çå„Åæ„Åô„ÄÇ

```bash
$ wasmer qjs.wasm
QuickJS - Type "\h" for help
qjs >
```

#### Ê¨°„Å´„Åß„Åç„Çã„Åì„Å®

- [Rust „Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥„Åã„Çâ Wasmer „Çí‰ΩøÁî®„Åô„Çã](https://docs.wasmer.io/integrations/rust)
- [WAPM „Åß Wasm „Éë„ÉÉ„Ç±„Éº„Ç∏„ÇíÂÖ¨Èñã„Åô„Çã](https://docs.wasmer.io/ecosystem/wapm/publishing-your-package)
- [Wasmer „Å´„Å§„ÅÑ„Å¶„Åï„Çâ„Å´Â≠¶„Å∂](https://medium.com/wasmer/)

## ‰ªñ„ÅÆË®ÄË™û„Å®„ÅÆ„Ç§„É≥„ÉÜ„Ç∞„É¨„Éº„Ç∑„Éß„É≥

üì¶ Wasmer „É©„É≥„Çø„Ç§„É†„ÅØ**‰ªñ„ÅÆË®ÄË™û„Å´ÁµÑ„ÅøËæº„Çì„Åß**‰ΩøÁî®„Åß„Åç„Çã„Åü„ÇÅ„ÄÅWebAssembly „ÅØ_„Å©„Çì„Å™Â†¥ÊâÄ„Åß„ÇÇ_Âà©Áî®„Åß„Åç„Åæ„Åô„ÄÇ

| &nbsp; | Language | Package | Docs |
|-|-|-|-|
| ![Rust logo] | [**Rust**][Rust integration] | [`wasmer` Rust crate] | [Docs][rust docs]
| ![C logo] | [**C/C++**][C integration] | [`wasmer.h` headers] | [Docs][c docs] |
| ![C# logo] | [**C#**][C# integration] | [`WasmerSharp` NuGet package] | [Docs][c# docs] |
| ![D logo] | [**D**][D integration] | [`wasmer` Dub package] | [Docs][d docs] |
| ![Python logo] | [**Python**][Python integration] | [`wasmer` PyPI package] | [Docs][python docs] |
| ![JS logo] | [**Javascript**][JS integration] | [`@wasmerio` NPM packages] | [Docs][js docs] |
| ![Go logo] | [**Go**][Go integration] | [`wasmer` Go package] | [Docs][go docs] |
| ![PHP logo] | [**PHP**][PHP integration] | [`wasm` PECL package] | [Docs][php docs] |
| ![Ruby logo] | [**Ruby**][Ruby integration] | [`wasmer` Ruby Gem] | [Docs][ruby docs] |
| ![Java logo] | [**Java**][Java integration] | [`wasmer/wasmer-jni` Bintray package] | [Docs][java docs] |
| ![Elixir logo] | [**Elixir**][Elixir integration] | [`wasmex` hex package] | [Docs][elixir docs] |
| ![R logo] | [**R**][R integration] | *ÂÖ¨Èñã„Éë„ÉÉ„Ç±„Éº„Ç∏„Å™„Åó* | [Docs][r docs] |
| ![Postgres logo] | [**Postgres**][Postgres integration] | *ÂÖ¨Èñã„Éë„ÉÉ„Ç±„Éº„Ç∏„Å™„Åó* | [Docs][postgres docs] |
|  | [**Swift**][Swift integration] | *ÂÖ¨Èñã„Éë„ÉÉ„Ç±„Éº„Ç∏„Å™„Åó* | |
| ![Zig logo] | [**Zig**][Zig integration] | *no published package* | |

[üëã Ë®ÄË™û„ÅåË¶ãÂΩì„Åü„Çâ„Å™„ÅÑÔºü](https://github.com/wasmerio/wasmer/issues/new?assignees=&labels=%F0%9F%8E%89+enhancement&template=---feature-request.md&title=)

[rust logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/rust.svg
[rust integration]: https://github.com/wasmerio/wasmer/tree/master/lib/api
[`wasmer` rust crate]: https://crates.io/crates/wasmer/
[rust docs]: https://wasmerio.github.io/wasmer/crates/wasmer

[c logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/c.svg
[c integration]: https://github.com/wasmerio/wasmer/tree/master/lib/c-api
[`wasmer.h` headers]: https://wasmerio.github.io/wasmer/c/
[c docs]: https://wasmerio.github.io/wasmer/c/

[c# logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/csharp.svg
[c# integration]: https://github.com/migueldeicaza/WasmerSharp
[`wasmersharp` nuget package]: https://www.nuget.org/packages/WasmerSharp/
[c# docs]: https://migueldeicaza.github.io/WasmerSharp/

[d logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/d.svg
[d integration]: https://github.com/chances/wasmer-d
[`wasmer` Dub package]: https://code.dlang.org/packages/wasmer
[d docs]: https://chances.github.io/wasmer-d

[python logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/python.svg
[python integration]: https://github.com/wasmerio/wasmer-python
[`wasmer` pypi package]: https://pypi.org/project/wasmer/
[python docs]: https://github.com/wasmerio/wasmer-python#api-of-the-wasmer-extensionmodule

[go logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/go.svg
[go integration]: https://github.com/wasmerio/wasmer-go
[`wasmer` go package]: https://pkg.go.dev/github.com/wasmerio/wasmer-go/wasmer
[go docs]: https://pkg.go.dev/github.com/wasmerio/wasmer-go/wasmer?tab=doc

[php logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/php.svg
[php integration]: https://github.com/wasmerio/wasmer-php
[`wasm` pecl package]: https://pecl.php.net/package/wasm
[php docs]: https://wasmerio.github.io/wasmer-php/wasm/

[js logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/js.svg
[js integration]: https://github.com/wasmerio/wasmer-js
[`@wasmerio` npm packages]: https://www.npmjs.com/org/wasmer
[js docs]: https://docs.wasmer.io/integrations/js/reference-api

[ruby logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/ruby.svg
[ruby integration]: https://github.com/wasmerio/wasmer-ruby
[`wasmer` ruby gem]: https://rubygems.org/gems/wasmer
[ruby docs]: https://www.rubydoc.info/gems/wasmer/

[java logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/java.svg
[java integration]: https://github.com/wasmerio/wasmer-java
[`wasmer/wasmer-jni` bintray package]: https://bintray.com/wasmer/wasmer-jni/wasmer-jni
[java docs]: https://github.com/wasmerio/wasmer-java/#api-of-the-wasmer-library

[elixir logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/elixir.svg
[elixir integration]: https://github.com/tessi/wasmex
[elixir docs]: https://hexdocs.pm/wasmex/api-reference.html
[`wasmex` hex package]: https://hex.pm/packages/wasmex

[r logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/r.svg
[r integration]: https://github.com/dirkschumacher/wasmr
[r docs]: https://github.com/dirkschumacher/wasmr#example

[postgres logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/postgres.svg
[postgres integration]: https://github.com/wasmerio/wasmer-postgres
[postgres docs]: https://github.com/wasmerio/wasmer-postgres#usage--documentation

[swift integration]: https://github.com/AlwaysRightInstitute/SwiftyWasmer

[zig logo]: https://raw.githubusercontent.com/ziglang/logo/master/zig-favicon.png
[zig integration]: https://github.com/zigwasm/wasmer-zig

## „Ç≥„É≥„Éà„É™„Éì„É•„Éº„Ç∑„Éß„É≥

**„Å©„Çì„Å™ÂΩ¢„Åß„ÅÆË≤¢ÁåÆ„ÇÇÊ≠ìËøé„Åß„Åô„ÄÇ„Ç≥„Éü„É•„Éã„ÉÜ„Ç£„ÅÆÊñ∞„Åó„ÅÑ„É°„É≥„Éê„Éº„Åã„Çâ„ÅÆË≤¢ÁåÆ„ÅØÁâπ„Å´Ê≠ìËøé„Åó„Åæ„Åô„ÄÇ** üíú

Wasmer „É©„É≥„Çø„Ç§„É†„ÅÆ„Éì„É´„ÉâÊñπÊ≥ï„ÅØ„ÄÅ[Á¥†Êô¥„Çâ„Åó„ÅÑ„Éâ„Ç≠„É•„É°„É≥„Éà](https://docs.wasmer.io/ecosystem/wasmer/building-from-source)„ÅßÁ¢∫Ë™ç„Åß„Åç„Åæ„ÅôÔºÅ

### „ÉÜ„Çπ„Éà

„ÉÜ„Çπ„Éà„ÇíÂÆüË°å„Åó„Åü„ÅÑ„Åß„Åô„ÅãÔºü [Wasmer docs „ÅßÊñπÊ≥ï„ÇíË™¨Êòé](https://docs.wasmer.io/ecosystem/wasmer/building-from-source/testing)„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ

## „Ç≥„Éü„É•„Éã„ÉÜ„Ç£

Wasmer „Å´„ÅØ„ÄÅÈñãÁô∫ËÄÖ„Å®„Ç≥„É≥„Éà„É™„Éì„É•„Éº„Çø„Éº„ÅÆÁ¥†Êô¥„Çâ„Åó„ÅÑ„Ç≥„Éü„É•„Éã„ÉÜ„Ç£„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ„Çà„ÅÜ„Åì„ÅùÔºÅ „ÅÇ„Å™„Åü„ÇÇÊòØÈùûÂèÇÂä†„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºÅ üëã

### „ÉÅ„É£„É≥„Éç„É´

- [Slack](https://slack.wasmer.io/)
- [Twitter](https://twitter.com/wasmerio)
- [Facebook](https://www.facebook.com/wasmerio)
- [Email](mailto:hello@wasmer.io)

'''
'''--- docs/ko/README.md ---
<div align="center">
  <a href="https://wasmer.io" target="_blank" rel="noopener noreferrer">
    <img width="300" src="https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/logo.png" alt="Wasmer logo">
  </a>

  <p>
    <a href="https://github.com/wasmerio/wasmer/actions?query=workflow%3Abuild">
      <img src="https://github.com/wasmerio/wasmer/workflows/build/badge.svg?style=flat-square" alt="Build Status">
    </a>
    <a href="https://github.com/wasmerio/wasmer/blob/master/LICENSE">
      <img src="https://img.shields.io/github/license/wasmerio/wasmer.svg" alt="License">
    </a>
    <a href="https://docs.wasmer.io">
      <img src="https://img.shields.io/static/v1?label=Docs&message=docs.wasmer.io&color=blue" alt="Wasmer Docs">
    </a>
    <a href="https://slack.wasmer.io">
      <img src="https://img.shields.io/static/v1?label=Slack&message=join%20us!&color=brighgreen" alt="Slack channel">
    </a>
  </p>
</div>

<br />

WasmerÎäî _Ï¥àÍ≤ΩÎüâ Ïª®ÌÖåÏù¥ÎÑà_ Î•º *Desktop*ÏóêÏÑúÎ∂ÄÌÑ∞ *Cloud*, *Edge*, *IoT* Í∏∞Í∏∞Îì§ÍπåÏßÄ Ïñ¥ÎîîÏóêÏÑúÎÇò Ïã§ÌñâÌï† Ïàò ÏûàÎäî _Îπ†Î•¥Í≥† ÏïàÏ†ÑÌïú_ [**WebAssembly**](https://webassembly.org) Îü∞ÌÉÄÏûÑ ÏûÖÎãàÎã§.

> _Ïù¥ Î¨∏ÏÑúÎäî ÏïÑÎûòÏôÄ Í∞ôÏùÄ Ïñ∏Ïñ¥Îì§ÏùÑ ÏßÄÏõêÌï©ÎãàÎã§.:
[üá®üá≥ ‰∏≠ Êñá -Chinese](https://github.com/wasmerio/wasmer/blob/master/docs/cn/README.md) ‚Ä¢ 
[üá©üá™ Deutsch-German](https://github.com/wasmerio/wasmer/blob/master/docs/de/README.md) ‚Ä¢ 
[üá™üá∏ Espa√±ol-Spanish](https://github.com/wasmerio/wasmer/blob/master/docs/es/README.md) ‚Ä¢ 
[üá´üá∑ Fran√ßais-French](https://github.com/wasmerio/wasmer/blob/master/docs/fr/README.md) ‚Ä¢ 
[üáØüáµ Êó•Êú¨ Ë™û -Japanese](https://github.com/wasmerio/wasmer/blob/master/docs/ja/README.md)_.
[üá∞üá∑ ÌïúÍµ≠Ïñ¥ -Korean](https://github.com/wasmerio/wasmer/blob/master/docs/ko/README.md)_.

### ÌäπÏßï

* Í∏∞Î≥∏Ï†ÅÏúºÎ°ú ÏïàÏ†ÑÌï©ÎãàÎã§. ÌååÏùº, ÎÑ§Ìä∏ÏõåÌÅ¨, ÌôòÍ≤Ω Ï†ëÍ∑ºÏù¥ Î™ÖÏãúÏ†ÅÏúºÎ°ú ÌôúÏÑ±Ìôî ÎêòÏßÄ ÏïäÏäµÎãàÎã§.
* [WASI](https://github.com/WebAssembly/WASI)ÏôÄ [Emscripten](https://emscripten.org/)ÏùÑ Ï¶âÏãú ÏßÄÏõêÌï©ÎãàÎã§.
* Îπ†Î¶ÖÎãàÎã§. nativeÏóê Í∞ÄÍπåÏö¥ ÏÜçÎèÑÎ°ú WebAssemblyÎ•º Ïã§ÌñâÌï©ÎãàÎã§.
* [Ïó¨Îü¨ ÌîÑÎ°úÍ∑∏ÎûòÎ∞ç Ïñ∏Ïñ¥](https://github.com/wasmerio/wasmer/#-language-integrations)Ïóê ÏûÑÎ≤†ÎîîÎìú Í∞ÄÎä•Ìï©ÎãàÎã§.
* ÏµúÏã† WebAssembly Ï†úÏïà(SIMD, Reference Types, Threads, ...)ÏùÑ Ï§ÄÏàòÌï©ÎãàÎã§.

### ÏÑ§Ïπò

Wasmer CLIÎäî Ï¢ÖÏÜçÏÑ±Ïù¥ ÏóÜÎäî Îã®Ïùº Ïã§Ìñâ ÌååÏùºÎ°ú Ï†úÍ≥µÎê©ÎãàÎã§.

```sh
curl https://get.wasmer.io -sSfL | sh
```

<details>
  <summary>Îã§Î•∏ ÏÑ§Ïπò ÏòµÏÖò (Powershell, Brew, Cargo, ...)</summary>
  
  _WasmerÎäî Îã§ÏñëÌïú Ìå®ÌÇ§ÏßÄ Îß§ÎãàÏ†ÄÎ•º ÌÜµÌï¥ ÏÑ§Ïπò Ìï† Ïàò ÏûàÏäµÎãàÎã§. ÌôòÍ≤ΩÏóê Í∞ÄÏû• Ï†ÅÌï©Ìïú Í≤ÉÏùÑ ÏÑ†ÌÉùÌïòÏã≠ÏãúÏò§.:_
  
  * Powershell (Windows)
    ```powershell
    iwr https://win.wasmer.io -useb | iex
    ```

  * <a href="https://formulae.brew.sh/formula/wasmer">Homebrew</a> (macOS, Linux)

    ```sh
    brew install wasmer
    ```

  * <a href="https://github.com/ScoopInstaller/Main/blob/master/bucket/wasmer.json">Scoop</a> (Windows)

    ```sh
    scoop install wasmer
    ```

  * <a href="https://chocolatey.org/packages/wasmer">Chocolatey</a> (Windows)

    ```sh
    choco install wasmer
    ```
  
  * <a href="https://crates.io/crates/wasmer-cli/">Cargo</a>

    _Note: ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îì† Í∏∞Îä•ÏùÄ [`wasmer-cli`
    crate docs](https://github.com/wasmerio/wasmer/tree/master/lib/cli/README.md) Î¨∏ÏÑúÏóê ÏÑ§Î™ÖÎêòÏñ¥ ÏûàÏäµÎãàÎã§._

    ```sh
    cargo install wasmer-cli
    ```

  > Îçî ÎßéÏùÄ ÏÑ§Ïπò ÏòµÏÖòÏùÑ Ï∞æÍ≥† Í≥ÑÏã≠ÎãàÍπå? ÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÄ [the `wasmer-install`
  repository](https://github.com/wasmerio/wasmer-install)Î•º Ï∞∏Ï°∞ÌïòÏã≠ÏãúÏò§!
</details>

### Îπ†Î•∏ ÏãúÏûë

WebAssembly Î™®Îìà([`qjs.wasm`](https://registry-cdn.wapm.io/contents/_/quickjs/0.0.3/build/qjs.wasm))Î°ú Ïª¥ÌååÏùºÎêú
ÏûëÍ≥† Ìè¨Ìï® Í∞ÄÎä•Ìïú Javascript ÏóîÏßÑÏù∏ [QuickJS](https://github.com/bellard/quickjs/)Î•º Ïã§ÌñâÌïòÏó¨ ÏãúÏûëÌï† Ïàò ÏûàÏäµÎãàÎã§.:

```bash
$ wasmer qjs.wasm
QuickJS - Type "\h" for help
qjs > const i = 1 + 2;
qjs > console.log("hello " + i);
hello 3
```

#### Îã§ÏùåÏóê Ìï† Ïàò ÏûàÎäî Ïùº :

- [Ïñ¥ÌîåÎ¶¨ÏºÄÏù¥ÏÖòÏóêÏÑú wasmer ÏÇ¨Ïö©](https://docs.wasmer.io/integrations/rust)
- [WAPMÏóê wasm Ìå®ÌÇ§ÏßÄ Í≤åÏãú](https://docs.wasmer.io/ecosystem/wapm/publishing-your-package)
- [WasmerÏóê ÎåÄÌï¥ ÏûêÏÑ∏Ìûà ÏïåÏïÑÎ≥¥Í∏∞](https://medium.com/wasmer/)

## üì¶ Îã§Î•∏ Ïñ∏Ïñ¥ÏôÄÏùò ÌÜµÌï©

Wasmer Îü∞ÌÉÄÏûÑÏùÄ **Îã§Î•∏ Ïñ∏Ïñ¥Ïóê ÎÇ¥Ïû•Îêú** ÎùºÏù¥Î∏åÎü¨Î¶¨Î°ú ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏúºÎØÄÎ°ú _Ïñ¥ÎîîÏóêÏÑúÎÇò_ WebAssemblyÎ•º ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏäµÎãàÎã§.

| | Language | Package | Documentation |
|-|-|-|-|
| ![Rust logo] | [**Rust**][Rust integration] | [`wasmer` Rust crate] | [Learn][rust docs]
| ![C logo] | [**C/C++**][C integration] | [`wasmer.h` header] | [Learn][c docs] |
| ![C# logo] | [**C#**][C# integration] | [`WasmerSharp` NuGet package] | [Learn][c# docs] |
| ![D logo] | [**D**][D integration] | [`wasmer` Dub package] | [Learn][d docs] |
| ![Python logo] | [**Python**][Python integration] | [`wasmer` PyPI package] | [Learn][python docs] |
| ![JS logo] | [**Javascript**][JS integration] | [`@wasmerio` NPM packages] | [Learn][js docs] |
| ![Go logo] | [**Go**][Go integration] | [`wasmer` Go package] | [Learn][go docs] |
| ![PHP logo] | [**PHP**][PHP integration] | [`wasm` PECL package] | [Learn][php docs] |
| ![Ruby logo] | [**Ruby**][Ruby integration] | [`wasmer` Ruby Gem] | [Learn][ruby docs] |
| ![Java logo] | [**Java**][Java integration] | [`wasmer/wasmer-jni` Bintray package] | [Learn][java docs] |
| ![Elixir logo] | [**Elixir**][Elixir integration] | [`wasmex` hex package] | [Learn][elixir docs] |
| ![R logo] | [**R**][R integration] | *Í≥µÍ∞ú Ìå®ÌÇ§ÏßÄ ÏóÜÏùå* | [Learn][r docs] |
| ![Postgres logo] | [**Postgres**][Postgres integration] | *Í≥µÍ∞ú Ìå®ÌÇ§ÏßÄ ÏóÜÏùå* | [Learn][postgres docs] |
|  | [**Swift**][Swift integration] | *Í≥µÍ∞ú Ìå®ÌÇ§ÏßÄ ÏóÜÏùå* | |
| ![Zig logo] | [**Zig**][Zig integration] | *Í≥µÍ∞ú Ìå®ÌÇ§ÏßÄ ÏóÜÏùå* | |
| ![Dart logo] | [**Dart**][Dart integration] | [`wasm` pub package] | |
|  | [**Lisp**][Lisp integration] | *under heavy development - no published package* | |

[üëã&nbsp;&nbsp;ÏóÜÎäî Ïñ∏Ïñ¥Í∞Ä ÏûàÏäµÎãàÍπå?](https://github.com/wasmerio/wasmer/issues/new?assignees=&labels=%F0%9F%8E%89+enhancement&template=---feature-request.md&title=)

[rust logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/rust.svg
[rust integration]: https://github.com/wasmerio/wasmer/tree/master/lib/api
[`wasmer` rust crate]: https://crates.io/crates/wasmer/
[rust docs]: https://docs.rs/wasmer/

[c logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/c.svg
[c integration]: https://github.com/wasmerio/wasmer/tree/master/lib/c-api
[`wasmer.h` header]: https://github.com/wasmerio/wasmer/blob/master/lib/c-api/wasmer.h
[c docs]: https://docs.rs/wasmer-c-api/*/wasmer_c_api/wasm_c_api/index.html

[c# logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/csharp.svg
[c# integration]: https://github.com/migueldeicaza/WasmerSharp
[`wasmersharp` nuget package]: https://www.nuget.org/packages/WasmerSharp/
[c# docs]: https://migueldeicaza.github.io/WasmerSharp/

[d logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/d.svg
[d integration]: https://github.com/chances/wasmer-d
[`wasmer` Dub package]: https://code.dlang.org/packages/wasmer
[d docs]: https://chances.github.io/wasmer-d

[python logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/python.svg
[python integration]: https://github.com/wasmerio/wasmer-python
[`wasmer` pypi package]: https://pypi.org/project/wasmer/
[python docs]: https://wasmerio.github.io/wasmer-python/api/wasmer/

[go logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/go.svg
[go integration]: https://github.com/wasmerio/wasmer-go
[`wasmer` go package]: https://pkg.go.dev/github.com/wasmerio/wasmer-go/wasmer
[go docs]: https://pkg.go.dev/github.com/wasmerio/wasmer-go/wasmer?tab=doc

[php logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/php.svg
[php integration]: https://github.com/wasmerio/wasmer-php
[`wasm` pecl package]: https://pecl.php.net/package/wasm
[php docs]: https://wasmerio.github.io/wasmer-php/

[js logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/js.svg
[js integration]: https://github.com/wasmerio/wasmer-js
[`@wasmerio` npm packages]: https://www.npmjs.com/org/wasmer
[js docs]: https://docs.wasmer.io/integrations/js/reference-api

[ruby logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/ruby.svg
[ruby integration]: https://github.com/wasmerio/wasmer-ruby
[`wasmer` ruby gem]: https://rubygems.org/gems/wasmer
[ruby docs]: https://wasmerio.github.io/wasmer-ruby/wasmer_ruby/index.html

[java logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/java.svg
[java integration]: https://github.com/wasmerio/wasmer-java
[`wasmer/wasmer-jni` bintray package]: https://bintray.com/wasmer/wasmer-jni/wasmer-jni
[java docs]: https://github.com/wasmerio/wasmer-java/#api-of-the-wasmer-library

[elixir logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/elixir.svg
[elixir integration]: https://github.com/tessi/wasmex
[elixir docs]: https://hexdocs.pm/wasmex/api-reference.html
[`wasmex` hex package]: https://hex.pm/packages/wasmex

[r logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/r.svg
[r integration]: https://github.com/dirkschumacher/wasmr
[r docs]: https://github.com/dirkschumacher/wasmr#example

[postgres logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/postgres.svg
[postgres integration]: https://github.com/wasmerio/wasmer-postgres
[postgres docs]: https://github.com/wasmerio/wasmer-postgres#usage--documentation

[swift integration]: https://github.com/AlwaysRightInstitute/SwiftyWasmer

[zig logo]: https://raw.githubusercontent.com/ziglang/logo/master/zig-favicon.png
[zig integration]: https://github.com/zigwasm/wasmer-zig

[dart logo]: https://raw.githubusercontent.com/wasmerio/wasmer/master/assets/languages/dart.svg
[dart integration]: https://github.com/dart-lang/wasm
[`wasm` pub package]: https://pub.dev/packages/wasm

[lisp integration]: https://github.com/helmutkian/cl-wasm-runtime

## Í∏∞Ïó¨

ÎèÑÏõÄÏùÑ Ï£ºÏÖîÏÑú Í∞êÏÇ¨Ìï©ÎãàÎã§! üíú

[WasmerÎ•º ÎπåÎìú](https://docs.wasmer.io/ecosystem/wasmer/building-from-source)ÌïòÍ±∞ÎÇò [Î≥ÄÍ≤Ω ÏÇ¨Ìï≠ÏùÑ ÌÖåÏä§Ìä∏](https://docs.wasmer.io/ecosystem/wasmer/building-from-source/testing)ÌïòÎäî Î∞©Î≤ïÏóê ÎåÄÌïú Î¨∏ÏÑúÎ•º ÌôïÏù∏ÌïòÏã≠ÏãúÏò§.

## Ïª§ÎÆ§ÎãàÌã∞
WasmerÏóêÎäî Í∞úÎ∞úÏûêÏùò Í∏∞Ïó¨Í∞Ä ÏûàÎäî ÌõåÎ•≠Ìïú Ïª§ÎÆ§ÎãàÌã∞Í∞Ä ÏûàÏäµÎãàÎã§. ÌôòÏòÅÌï©ÎãàÎã§! Íº≠ Ï∞∏Ïó¨Ìï¥Ï£ºÏÑ∏Ïöî! üëã

- [Wasmer Community Slack](https://slack.wasmer.io/)
- [Wasmer on Twitter](https://twitter.com/wasmerio)
- [Wasmer on Facebook](https://www.facebook.com/wasmerio)
- [Email](mailto:hello@wasmer.io)

'''
'''--- docs/migration_to_1.0.0.md ---
# Migrating from Wasmer 0.x to Wasmer 1.0.0

Wasmer 1.0.0 is getting ready for a full release and is our primary focus. This document will
describe the differences between Wasmer 0.x and Wasmer 1.0.0 and provide examples
to make migrating to the new API as simple as possible.

## Table of Contents

- [Rationale for changes in 1.0.0](#rationale-for-changes-in-100)
- [How to use Wasmer 1.0.0](#how-to-use-wasmer-100)
  - [Installing Wasmer CLI](#installing-wamser-cli)
  - [Using Wasmer 1.0.0](#using-wamser-100)
- [Project structure](#project-structure)
- [Differences](#differences)
  - [Instantiating modules](#instantiating-modules)
  - [Passing host functions](#passing-host-functions)
  - [Accessing the environment as a host function](#accessing-the-environment-as-a-host-function)
  - [Error handling](#error-handling)
  - [Caching modules](#caching-modules)

## Rationale for changes in 1.0.0

Wasmer 0.x was great but as the Wasm community and standards evolve we felt the need to make Wasmer also follow these 
changes.

Wasmer 1.x is what we think a necessary rewrite of a big part of the project to make it more future-proof. 

This version introduces many new features and makes using Wasmer more natural. We did a hard work making it as close 
to the standard API as possible while always providing good performances, flexibility and stability. 

The rewrite of the Wasmer Runtime also comes with a rewrite of the languages integrations to achieve the same goals: 
providing a clearer API and improving the feature set.

In this document you will discover the major changes between Wasmer 0.x and Wasmer 1.x by highlighting how to use the 
new Rust API.

## How to use Wasmer 1.0.0

### Installing Wasmer CLI

See [wasmer.io] for installation instructions.

If you already have wasmer installed, run `wasmer self-update`.

Install the latest versions of Wasmer with [wasmer-nightly] or by following the steps described in the 
documentation: [Getting Started][getting-started].

### Using Wasmer 1.0.0

The CLI interface for Wasmer 1.0.0 is mostly the same as it was in Wasmer 0.x.

One difference is that rather than specifying the compiler with `--backend=cranelift`,
in Wasmer 1.0.0 we prefer using the name of the backend as a flag directly,
for example: `--cranelift`.

The top-level crates that users will usually interface with are:

- [wasmer] - Wasmer's core runtime API
- [wasmer-wasi] - Wasmer's WASI implementation
- [wasmer-emscripten] - Wasmer's Emscripten implementation
- TODO:

See the [examples] to find out how to do specific things in Wasmer 1.0.0.

## Project Structure

![Wasmer dependencies graph](./deps_dedup.svg)

The figure above shows the core Wasmer crates and their dependencies with transitive dependencies deduplicated.

Wasmer 1.0.0 has two core architectural abstractions: engines and compilers.

An engine is a system that processes Wasm with a compiler and prepares it to be executed.

A compiler is a system that translates Wasm into a format that can be understood
more directly by a real computer: machine code.

For example, in the [examples] you'll see that we are using the JIT engine and the Cranelift compiler. The JIT engine 
will generate machine code at runtime, using Cranelift, and then execute it.  

For most uses, users will primarily use the [wasmer] crate directly, perhaps with one of our
provided ABIs such as [wasmer-wasi]. However, for users that need finer grained control over
the behavior of wasmer, other crates such as [wasmer-compiler] and [wasmer-engine] may be used
to implement custom compilers and engines respectively.

## Differences

### Instantiating modules

With Wasmer 0.x, instantiating a module was a matter of calling `wasmer::compiler::compile` and then calling 
`instantiate` on the compiled module.

While simple, this did not give you full-control over Wasmer's configuration. For example, choosing another compiler
was not straightforward.

With Wasmer 1.x, we changed this part and made the API look more like how Wasmer works internally to give you more 
control:

```diff
- let module = compile(&wasm_bytes[..])?; 
+ let engine = JIT::new(Cranelift::default()).engine();
+ let store = Store::new(&engine);
+ let module = Module::new(&store, wasm_bytes)?;
- let instance = module.instantiate(&imports)?;
+ let instance = Instance::new(&module, &import_object)?;
```

Note that we did not cover how to create the import object here. This is because this part works the same as it used to 
with Wasmer 0.x.

To get more information on how instantiation now works, have a look at the [dedicated example][instance-example]

### Passing host functions

With Wasmer 0.x passing host functions to the guest was primarily done using the `func!` macro or by directly using 
`Func::new` or `DynamicFunc::new`.

In Wasmer 1.0 the equivalent of `Func::new` is `Function::new_native` /
`Function::new_native_with_env` and the equivalent of `DynamicFunc::new` 
is `Function::new` / `Function::new_with_env`.

Given we have a function like:

```rust
fn sum(a: i32, b: i32) -> i32 {
    a + b
}
```

We want to import this function in the guest module, let's have a look at how it differs between Wasmer 0.x and 
Wasmer 1.x:

```diff
let import_object = imports! {
    "env" => {
-         "sum" => func!(sum),
+         "sum" => Function::new_native(&store, sum),
    }
}
```

The above example illustrates how to import what we call "native functions". There were already available in Wasmer 
0.x through the `func!` macro or with `Func::new`.

There is a second flavor for imported functions: dynamic functions. With Wasmer 0.x you would have created such a 
function using `DynamicFunc::new`, here is how it's done with Wasmer 1.x:

```rust
let sum_signature = FunctionType::new(vec![Type::I32, Type::I32], vec![Type::I32]);
let sum = Function::new(&store, &sum_signature, |args| {
    let result = args[0].unwrap_I32() + args[1].unwrap_i32();
    
    Ok(vec![Value::I32(result)])
});
```

Both examples address different needs and have their own pros and cons. We encourage you to have a look at the 
dedicated example: [Exposing host functions][host-functions-example].

Note that having this API for functions now looks more like the other entities APIs: globals, memories, tables. Here is 
a quick example introducing each of them: [Imports & Exports][imports-exports-example]

### Accessing the environment as a host function

With Wasmer 0.x each function had its own `vm::Ctx`. This was your entrypoint to the module internals and allowed
access to the context of the currently running instance.

With Wasmer 1.0.0 this was changed to provide a simpler yet powerful API.

Let's see an example where we want to have access to the module memory. Here is how that changed from 0.x to 1.0.0:

```diff
+ #[derive(WasmerEnv, Clone, Default)]
+ struct MyEnv {
+     #[wasmer(export)]
+     memory: LazyInit<Memory>,
+ }
+ let env = MyEnv::default();
+
- let get_at = |ctx: &mut vm::Ctx, ptr: WasmPtr<u8, Array>, len: u32| {
+ let get_at = |env: &MyEnv, ptr: WasmPtr<u8, Array>, len: u32| {
-     let mem_desc = ctx.memory(0);
-     let mem = mem_desc.deref();
+     let mem = env.memory_ref().unwrap();
  
      println!("Memory: {:?}", mem);
  
      let string = ptr.get_utf8_string(mem, len).unwrap();
      println!("string: {}", string);
  };

  let import_object = imports! {
      "env" => {
-         "get_at" => func!(get_at),
+         "get_at" => Function::new_native_with_env(&store, env.clone(), get_at),
      }
  };

- let instance = instantiate(wasm_bytes, &import_object)?;
+ let instance = Instance::new(&wasm_bytes, &import_object)?;
```

Here we have a module which provides one exported function: `get`. Each time we call this function it will in turn 
call our imported function `get_at`.

The `get_at` function is responsible for reading the guest module-s memory through the `vm::Ctx`.

With Wasmer 1.0.0 (where the `vm::Ctx` does not exist anymore) we can achieve the same result with something more 
natural: we only use imports and exports to read from the memory and write to it.

However in order to provide an easy to use interface, we now have a trait
that can be implemented with a derive macro: `WasmerEnv`. We must make our
env types implement `WasmerEnv` and `Clone`. We mark an internal field
wrapped with `LazyInit` with `#[wasmer(export)]` to indicate that the type
should be found in the Wasm exports with the name of the field
(`"memory"`). The derive macro then generates helper functions such as
`memory_ref` on the type for easy access to these fields.

See the [`WasmerEnv`](https://docs.rs/wasmer/*/wasmer/trait.WasmerEnv.html)
docs for more information.

Take a look at the following examples to get more details:
* [Interacting with memory][memory]
* [Using memory pointers][memory-pointers]

The other thing where `vm::Ctx` was useful was to pass data from and to host functions. This has also been made simpler
with Wasmer 1.x:

```rust
let shared_counter: Arc<RefCell<i32>> = Arc::new(RefCell::new(0));

#[derive(WasmerEnv, Clone)]
struct Env {
    counter: Arc<RefCell<i32>>,
}

fn get_counter(env: &Env) -> i32 {
    *env.counter.borrow()
}

let get_counter_func = Function::new_native_with_env(
    &store, 
    Env { counter: shared_counter.clone() }, 
    get_counter
);
```

A dedicated example describes how to use this feature: [Exposing host functions][host-functions].

### Error handling

Handling errors with Wasmer 0.x was a bit hard, especially, the `wasmer_runtime::error::RuntimeError`. It was rather 
complex: it had many variants that you had to handle when pattern matching results. This has been made way simpler with 
Wasmer 1.0.0:

```diff
// Retrieve the `get` function from module's exports and then call it
let result = get.call(0, 13);

match result {
-     Err(RuntimeError::InvokeError(InvokeError::TrapCode { .. })) => {
-         // ...
-     }
-     Err(RuntimeError::InvokeError(InvokeError::FailedWithNoError)) => {
-         // ...
-     }
-     Err(RuntimeError::InvokeError(InvokeError::UnknownTrap { .. })) => {
-         // ...
-     }
-     Err(RuntimeError::InvokeError(InvokeError::UnknownTrapCode { .. })) => {
-         // ...
-     }
-     Err(RuntimeError::InvokeError(InvokeError::EarlyTrap(_))) => {
-         // ...
-     }
-     Err(RuntimeError::InvokeError(InvokeError::Breakpoint(_))) => {
-         // ...
-     }
-     Err(RuntimeError::Metering(_)) => {
-         // ...
-     }
-     Err(RuntimeError::InstanceImage(_)) => {
-         // ...
-     }
-     Err(RuntimeError::User(_)) => {
-         // ...
-     }
+     Error(e) => {
+         println!("Error caught from `div_by_zero`: {}", e.message());
+          
+         let frames = e.trace();
+         let frames_len = frames.len();
+         
+         // ...
+     }
    Ok(_) => {
        // ...
    },
}
``` 

As you can see here, handling errors is really easy now! You may find the following examples useful to get more familiar
with this topic:
* [Handling Errors][errors]
* [Interrupting Execution][exit-early]

Note that with Wasmer 1.0.0, each function that is part of the API has its own kind of error. For example:
* Instantiating a module may return `InstantiationError`s;
* Getting exports from the guest module may return `ExportError`s;
* Calling an exported function may return `RuntimeError`s;
* ...

### Caching modules

You may be aware Wasmer, since 0.x, allows you to cache compiled module so that further executions of your program
will be faster.

Because caching may bring a significant boost when running Wasm modules we wanted to make it easier to use with 
Wasmer 1.0.0.

With Wasmer 0.x you had to handle the whole caching process inside your program's code. With Wasmer 1.0.0
you'll be able to delegate most of the work to Wasmer:

```diff
- let artifact = module.cache().unwrap();
- let bytes = artifact.serialize().unwrap();
- 
- let path = "module_cached.so";
- fs::write(path, bytes).unwrap();
+ module.serialize_to_file(path)?;

- let mut file = File::open(path).unwrap();
- let cached_bytes = &mut vec![];
- file.read_to_end(cached_bytes);
- drop(file);
- 
- let cached_artifact = Artifact::deserialize(&cached_bytes).unwrap();
- let cached_module = unsafe { load_cache_with(cached_artifact, &default_compiler()) }.unwrap();
+ let cached_module = unsafe { Module::deserialize_from_file(&store, path) }?;
``` 

[examples]: https://docs.wasmer.io/integrations/examples
[wasmer]: https://crates.io/crates/wasmer
[wasmer-wasi]: https://crates.io/crates/wasmer-wasi
[wasmer-emscripten]: https://crates.io/crates/wasmer-emscripten
[wasmer-engine]: https://crates.io/crates/wasmer-engine
[wasmer-compiler]: https://crates.io/crates/wasmer-compiler
[wasmer.io]: https://wasmer.io
[wasmer-nightly]: https://github.com/wasmerio/wasmer-nightly/
[getting-started]: https://docs.wasmer.io/ecosystem/wasmer/getting-started
[instance-example]: https://docs.wasmer.io/integrations/examples/instance
[imports-exports-example]: https://docs.wasmer.io/integrations/examples/imports-and-exports
[host-functions-example]: https://docs.wasmer.io/integrations/examples/host-functions
[memory]: https://docs.wasmer.io/integrations/examples/memory
[memory-pointers]: https://docs.wasmer.io/integrations/examples/memory-pointers
[host-functions]: https://docs.wasmer.io/integrations/examples/host-functions
[errors]: https://docs.wasmer.io/integrations/examples/errors
[exit-early]: https://docs.wasmer.io/integrations/examples/exit-early

'''
'''--- examples/README.md ---
# Wasmer Examples

This directory contains a collection of examples. This isn't an
exhaustive collection though, if one example is missing, please ask,
we will be happy to fulfill your needs!

## Handy Diagrams

As a quick introduction to Wasmer's main workflows, here are three
diagrams. We hope it provides an overview of how the crates assemble
together.

1. **Module compilation**, illustrates how WebAssembly bytes are
   validated, parsed, and compiled, with the help of the
   `wasmer::Module`, `wasmer_engine::Engine`, and
   `wasmer_compiler::Compiler` API.

   ![Module compilation](../assets/diagrams/Diagram_module_compilation.png)

2. **Module serialization**, illustrates how a module can be
   serialized and deserialized, with the help of
   `wasmer::Module::serialize` and `wasmer::Module::deserialize`. The
   important part is that the engine can changed between those two
   steps, and thus how a headless engine can be used for the
   deserialization.

   ![Module serialization](../assets/diagrams/Diagram_module_serialization.png)

3. **Module instantiation**, illustrates what happens when
   `wasmer::Instance::new` is called.

   ![Module instantiation](../assets/diagrams/Diagram_module_instantiation.png)

## Examples

The examples are written in a difficulty/discovery order. Concepts that
are explained in an example is not necessarily re-explained in a next
example.

### Basics

1. [**Hello World**][hello-world], explains the core concepts of the Wasmer
   API for compiling and executing WebAssembly.

   _Keywords_: introduction, instance, module.

   <details>
    <summary><em>Execute the example</em></summary>

    ```shell
    $ cargo run --example hello-world --release --features "cranelift"
    ```

   </details>

2. [**Instantiating a module**][instance], explains the basics of using Wasmer
   and how to create an instance out of a Wasm module.
   
   _Keywords_: instance, module.
   
   <details>
    <summary><em>Execute the example</em></summary>

    ```shell
    $ cargo run --example instance --release --features "cranelift"
    ```

   </details>

3. [**Handling errors**][errors], explains the basics of interacting with
   Wasm module memory.
   
   _Keywords_: instance, error.
   
   <details>
    <summary><em>Execute the example</em></summary>

    ```shell
    $ cargo run --example errors --release --features "cranelift"
    ```

   </details>

4. [**Interacting with memory**][memory], explains the basics of interacting with
   Wasm module memory.
   
   _Keywords_: memory, module.
   
   <details>
    <summary><em>Execute the example</em></summary>

    ```shell
    $ cargo run --example memory --release --features "cranelift"
    ```

   </details>

### Exports

1. [**Exported global**][exported-global], explains how to work with
   exported globals: get/set their value, have information about their 
   type.
   
   _Keywords_: export, global.

   <details>
   <summary><em>Execute the example</em></summary>

   ```shell
   $ cargo run --example exported-global --release --features "cranelift"
   ```

   </details>
   
2. [**Exported function**][exported-function], explains how to get and
   how to call an exported function. They come in 2 flavors: dynamic,
   and ‚Äústatic‚Äù/native. The pros and cons are discussed briefly.
   
   _Keywords_: export, function, dynamic, static, native.

   <details>
   <summary><em>Execute the example</em></summary>

   ```shell
   $ cargo run --example exported-function --release --features "cranelift"
   ```

   </details>

3. [**Exported memory**][exported-memory], explains how to read from 
    and write to exported memory.
   
   _Keywords_: export, memory.

   <details>
   <summary><em>Execute the example</em></summary>

   ```shell
   $ cargo run --example exported-memory --release --features "cranelift"
   ```

   </details>

### Imports

1. [**Imported global**][imported-global], explains how to work with
   imported globals: create globals, import them, get/set their value.
   
   _Keywords_: import, global.

   <details>
   <summary><em>Execute the example</em></summary>

   ```shell
   $ cargo run --example imported-global --release --features "cranelift"
   ```

   </details>

2. [**Imported function**][imported-function], explains how to define 
   an imported function. They come in 2 flavors: dynamic,
   and ‚Äústatic‚Äù/native.
   
   _Keywords_: import, function, dynamic, static, native.

   <details>
   <summary><em>Execute the example</em></summary>

   ```shell
   $ cargo run --example imported-function --release --features "cranelift"
   ```

   </details>

### Externs

1. [**Table**][table], explains how to use Wasm Tables from the Wasmer API.

   _Keywords_: basic, table, call_indirect

   <details>
   <summary><em>Execute the example</em></summary>

   ```shell
   $ cargo run --example table --release --features "cranelift"
   ```

   </details>
   
2. [**Memory**][memory], explains how to use Wasm Memories from
   the Wasmer API.  Memory example is a work in progress.

   _Keywords_: basic, memory

   <details>
   <summary><em>Execute the example</em></summary>

   ```shell
   $ cargo run --example memory --release --features "cranelift"
   ```

   </details>

### Tunables

1. [**Limit memory**][tunables-limit-memory], explains how to use Tunables to limit the
   size of an exported Wasm memory

   _Keywords_: basic, tunables, memory

   <details>
   <summary><em>Execute the example</em></summary>

   ```shell
   $ cargo run --example tunables-limit-memory --release --features "cranelift"
   ```

   </details>

### Engines

1. [**Universal engine**][engine-universal], explains what an engine is, what the
   Universal engine is, and how to set it up. The example completes itself
   with the compilation of the Wasm module, its instantiation, and
   finally, by calling an exported function.
   
   _Keywords_: Universal, engine, in-memory, executable code.
   
   <details>
   <summary><em>Execute the example</em></summary>

   ```shell
   $ cargo run --example engine-universal --release --features "cranelift"
   ```

   </details>

2. [**Dylib engine**][engine-dylib], explains what a Dylib engine
   is, and how to set it up. The example completes itself with the
   compilation of the Wasm module, its instantiation, and finally, by
   calling an exported function.
   
   _Keywords_: native, engine, shared library, dynamic library,
   executable code.

   <details>
   <summary><em>Execute the example</em></summary>

   ```shell
   $ cargo run --example engine-dylib --release --features "cranelift"
   ```

   </details>

3. [**Headless engines**][engine-headless], explains what a headless
   engine is, what problem it does solve, and what are the benefits of
   it. The example completes itself with the instantiation of a
   pre-compiled Wasm module, and finally, by calling an exported
   function.
   
   _Keywords_: native, engine, constrained environment, ahead-of-time
   compilation, cross-compilation, executable code, serialization.

   <details>
   <summary><em>Execute the example</em></summary>

   ```shell
   $ cargo run --example engine-headless --release --features "cranelift"
   ```

   </details>

4. [**Cross-compilation**][cross-compilation], illustrates the power
   of the abstraction over the engines and the compilers, such as it
   is possible to cross-compile a Wasm module for a custom target.
   
   _Keywords_: engine, compiler, cross-compilation.

   <details>
   <summary><em>Execute the example</em></summary>

   ```shell
   $ cargo run --example cross-compilation --release --features "cranelift"
   ```

   </details>
   
5. [**Features**][features], illustrates how to enable WebAssembly
   features that aren't yet stable.
   
   _Keywords_: engine, features.
   
   <details>
   <summary><em>Execute the example</em></summary>
   
   ```shell
   $ cargo run --example features --release --features "cranelift"
   ```
   
   </details>

### Compilers

1. [**Singlepass compiler**][compiler-singlepass], explains how to use
   the [`wasmer-compiler-singlepass`] compiler.
   
   _Keywords_: compiler, singlepass.

   <details>
   <summary><em>Execute the example</em></summary>

   ```shell
   $ cargo run --example compiler-singlepass --release --features "singlepass"
   ```

   </details>

2. [**Cranelift compiler**][compiler-cranelift], explains how to use
   the [`wasmer-compiler-cranelift`] compiler.
   
   _Keywords_: compiler, cranelift.

   <details>
   <summary><em>Execute the example</em></summary>

   ```shell
   $ cargo run --example compiler-cranelift --release --features "cranelift"
   ```

   </details>

3. [**LLVM compiler**][compiler-llvm], explains how to use the
   [`wasmer-compiler-llvm`] compiler.
   
   _Keywords_: compiler, llvm.

   <details>
   <summary><em>Execute the example</em></summary>

   ```shell
   $ cargo run --example compiler-llvm --release --features "llvm"
   ```

   </details>

### Integrations

1. [**WASI**][wasi], explains how to use the [WebAssembly System
   Interface][WASI] (WASI), i.e. the [`wasmer-wasi`] crate.
   
   _Keywords_: wasi, system, interface

   <details>
   <summary><em>Execute the example</em></summary>

   ```shell
   $ cargo run --example wasi --release --features "cranelift,wasi"
   ```

   </details>

2. [**WASI Pipes**][wasi-pipes], builds on the WASI example to show off
   stdio piping in Wasmer.

   _Keywords_: wasi, system, interface

   <details>
   <summary><em>Execute the example</em></summary>

   ```shell
   $ cargo run --example wasi-pipes --release --features "cranelift,wasi"
   ```

   </details>

[hello-world]: ./hello_world.rs
[engine-universal]: ./engine_universal.rs
[engine-dylib]: ./engine_dylib.rs
[engine-headless]: ./engine_headless.rs
[compiler-singlepass]: ./compiler_singlepass.rs
[compiler-cranelift]: ./compiler_cranelift.rs
[compiler-llvm]: ./compiler_llvm.rs
[cross-compilation]: ./engine_cross_compilation.rs
[exported-global]: ./exports_global.rs
[exported-function]: ./exports_function.rs
[exported-memory]: ./exports_memory.rs
[imported-global]: ./imports_global.rs
[imported-function]: ./imports_function.rs
[instance]: ./instance.rs
[wasi]: ./wasi.rs
[wasi-pipes]: ./wasi_pipes.rs
[table]: ./table.rs
[memory]: ./memory.rs
[errors]: ./errors.rs
[tunables-limit-memory]: ./tunables_limit_memory.rs
[features]: ./features.rs
[`wasmer-compiler-singlepass`]: https://github.com/wasmerio/wasmer/tree/master/lib/compiler-singlepass
[`wasmer-compiler-cranelift`]: https://github.com/wasmerio/wasmer/tree/master/lib/compiler-cranelift
[`wasmer-compiler-llvm`]: https://github.com/wasmerio/wasmer/tree/master/lib/compiler-llvm
[`wasmer-wasi`]: https://github.com/wasmerio/wasmer/tree/master/lib/wasi
[WASI]: https://github.com/WebAssembly/WASI

'''
'''--- examples/compiler_cranelift.rs ---
//! A Wasm module can be compiled with multiple compilers.
//!
//! This example illustrates how to use the Cranelift compiler.
//!
//! You can run the example directly by executing in Wasmer root:
//!
//! ```shell
//! cargo run --example compiler-cranelift --release --features "cranelift"
//! ```
//!
//! Ready?

use wasmer::{imports, wat2wasm, Instance, Module, Store, Value};
use wasmer_compiler_cranelift::Cranelift;
use wasmer_engine_universal::Universal;

fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Let's declare the Wasm module with the text representation.
    let wasm_bytes = wat2wasm(
        r#"
(module
  (type $sum_t (func (param i32 i32) (result i32)))
  (func $sum_f (type $sum_t) (param $x i32) (param $y i32) (result i32)
    local.get $x
    local.get $y
    i32.add)
  (export "sum" (func $sum_f)))
"#
        .as_bytes(),
    )?;

    // Use Cranelift compiler with the default settings
    let compiler = Cranelift::default();

    // Create the store
    let store = Store::new(&Universal::new(compiler).engine());

    println!("Compiling module...");
    // Let's compile the Wasm module.
    let module = Module::new(&store, wasm_bytes)?;

    // Create an empty import object.
    let import_object = imports! {};

    println!("Instantiating module...");
    // Let's instantiate the Wasm module.
    let instance = Instance::new(&module, &import_object)?;

    let sum = instance.exports.get_function("sum")?;

    println!("Calling `sum` function...");
    // Let's call the `sum` exported function. The parameters are a
    // slice of `Value`s. The results are a boxed slice of `Value`s.
    let results = sum.call(&[Value::I32(1), Value::I32(2)])?;

    println!("Results: {:?}", results);
    assert_eq!(results.to_vec(), vec![Value::I32(3)]);

    Ok(())
}

#[test]
#[cfg(feature = "cranelift")]
fn test_compiler_cranelift() -> Result<(), Box<dyn std::error::Error>> {
    main()
}

'''
'''--- examples/compiler_llvm.rs ---
//! A Wasm module can be compiled with multiple compilers.
//!
//! This example illustrates how to use the LLVM compiler.
//!
//! You can run the example directly by executing in Wasmer root:
//!
//! ```shell
//! cargo run --example compiler-llvm --release --features "llvm"
//! ```
//!
//! Ready?

use wasmer::{imports, wat2wasm, Instance, Module, Store, Value};
use wasmer_compiler_llvm::LLVM;
use wasmer_engine_universal::Universal;

fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Let's declare the Wasm module with the text representation.
    let wasm_bytes = wat2wasm(
        r#"
(module
  (type $sum_t (func (param i32 i32) (result i32)))
  (func $sum_f (type $sum_t) (param $x i32) (param $y i32) (result i32)
    local.get $x
    local.get $y
    i32.add)
  (export "sum" (func $sum_f)))
"#
        .as_bytes(),
    )?;

    // Use LLVM compiler with the default settings
    let compiler = LLVM::default();

    // Create the store
    let store = Store::new(&Universal::new(compiler).engine());

    println!("Compiling module...");
    // Let's compile the Wasm module.
    let module = Module::new(&store, wasm_bytes)?;

    // Create an empty import object.
    let import_object = imports! {};

    println!("Instantiating module...");
    // Let's instantiate the Wasm module.
    let instance = Instance::new(&module, &import_object)?;

    let sum = instance.exports.get_function("sum")?;

    println!("Calling `sum` function...");
    // Let's call the `sum` exported function. The parameters are a
    // slice of `Value`s. The results are a boxed slice of `Value`s.
    let results = sum.call(&[Value::I32(1), Value::I32(2)])?;

    println!("Results: {:?}", results);
    assert_eq!(results.to_vec(), vec![Value::I32(3)]);

    Ok(())
}

#[test]
#[cfg(feature = "llvm")]
fn test_compiler_llvm() -> Result<(), Box<dyn std::error::Error>> {
    main()
}

'''
'''--- examples/compiler_singlepass.rs ---
//! A Wasm module can be compiled with multiple compilers.
//!
//! This example illustrates how to use the Singlepass compiler.
//!
//! You can run the example directly by executing in Wasmer root:
//!
//! ```shell
//! cargo run --example compiler-singlepass --release --features "singlepass"
//! ```
//!
//! Ready?

use wasmer::{imports, wat2wasm, Instance, Module, Store, Value};
use wasmer_compiler_singlepass::Singlepass;
use wasmer_engine_universal::Universal;

fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Let's declare the Wasm module with the text representation.
    let wasm_bytes = wat2wasm(
        r#"
(module
  (type $sum_t (func (param i32 i32) (result i32)))
  (func $sum_f (type $sum_t) (param $x i32) (param $y i32) (result i32)
    local.get $x
    local.get $y
    i32.add)
  (export "sum" (func $sum_f)))
"#
        .as_bytes(),
    )?;

    // Use Singlepass compiler with the default settings
    let compiler = Singlepass::default();

    // Create the store
    let store = Store::new(&Universal::new(compiler).engine());

    println!("Compiling module...");
    // Let's compile the Wasm module.
    let module = Module::new(&store, wasm_bytes)?;

    // Create an empty import object.
    let import_object = imports! {};

    println!("Instantiating module...");
    // Let's instantiate the Wasm module.
    let instance = Instance::new(&module, &import_object)?;

    let sum = instance.lookup_function("sum").expect("function lookup");

    println!("Calling `sum` function...");
    // Let's call the `sum` exported function. The parameters are a
    // slice of `Value`s. The results are a boxed slice of `Value`s.
    let results = sum.call(&[Value::I32(1), Value::I32(2)])?;

    println!("Results: {:?}", results);
    assert_eq!(results.to_vec(), vec![Value::I32(3)]);

    Ok(())
}

#[test]
#[cfg(feature = "singlepass")]
fn test_compiler_singlepass() -> Result<(), Box<dyn std::error::Error>> {
    main()
}

'''
'''--- examples/early_exit.rs ---
//! There are cases where you may want to interrupt this synchronous execution of the Wasm module
//! while the it is calling a host function. This can be useful for saving resources, and not
//! returning back to the guest Wasm for execution, when you already know the Wasm execution will
//! fail, or no longer be needed.
//!
//! In this example, we will run a Wasm module that calls the imported host function
//! interrupt_execution. This host function will immediately stop executing the WebAssembly module.
//!
//! You can run the example directly by executing in Wasmer root:
//!
//! ```shell
//! cargo run --example early-exit --release --features "cranelift"
//! ```
//!
//! Ready?

use anyhow::bail;
use std::fmt;
use wasmer::{imports, wat2wasm, Function, Instance, Module, NativeFunc, RuntimeError, Store};
use wasmer_compiler_cranelift::Cranelift;
use wasmer_engine_universal::Universal;

// First we need to create an error type that we'll use to signal the end of execution.
#[derive(Debug, Clone, Copy)]
struct ExitCode(u32);

// This type must implement `std::error::Error` so we must also implement `std::fmt::Display` for it.
impl fmt::Display for ExitCode {
    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
        write!(f, "{}", self.0)
    }
}

// And then we implement `std::error::Error`.
impl std::error::Error for ExitCode {}

fn main() -> anyhow::Result<()> {
    // Let's declare the Wasm module with the text representation.
    let wasm_bytes = wat2wasm(
        br#"
(module
  (type $run_t (func (param i32 i32) (result i32)))
  (type $early_exit_t (func (param) (result)))
  (import "env" "early_exit" (func $early_exit (type $early_exit_t)))
  (func $run (type $run_t) (param $x i32) (param $y i32) (result i32)
    (call $early_exit)
    (i32.add
        local.get $x
        local.get $y))
  (export "run" (func $run)))
"#,
    )?;

    // Create a Store.
    // Note that we don't need to specify the engine/compiler if we want to use
    // the default provided by Wasmer.
    // You can use `Store::default()` for that.
    let store = Store::new(&Universal::new(Cranelift::default()).engine());

    println!("Compiling module...");
    // Let's compile the Wasm module.
    let module = Module::new(&store, wasm_bytes)?;

    // We declare the host function that we'll use to terminate execution.
    fn early_exit() {
        // This is where it happens.
        RuntimeError::raise(Box::new(ExitCode(1)));
    }

    // Create an import object.
    let import_object = imports! {
        "env" => {
            "early_exit" => Function::new_native(&store, early_exit),
        }
    };

    println!("Instantiating module...");
    // Let's instantiate the Wasm module.
    let instance = Instance::new(&module, &import_object)?;

    // Here we go.
    //
    // Get the `run` function which we'll use as our entrypoint.
    println!("Calling `run` function...");
    let run_func: NativeFunc<(i32, i32), i32> = instance.exports.get_native_function("run")?;

    // When we call a function it can either succeed or fail. We expect it to fail.
    match run_func.call(1, 7) {
        Ok(result) => {
            bail!(
                "Expected early termination with `ExitCode`, found: {}",
                result
            );
        }
        // In case of a failure, which we expect, we attempt to downcast the error into the error
        // type that we were expecting.
        Err(e) => match e.downcast::<ExitCode>() {
            // We found the exit code used to terminate execution.
            Ok(exit_code) => {
                println!("Exited early with exit code: {}", exit_code);

                Ok(())
            }
            Err(e) => {
                bail!("Unknown error `{}` found. expected `ErrorCode`", e);
            }
        },
    }
}

'''
'''--- examples/engine_cross_compilation.rs ---
//! Defining an engine in Wasmer is one of the fundamental steps.
//!
//! As a reminder, an engine applies roughly 2 steps:
//!
//!   1. It compiles the Wasm module bytes to executable code, through
//!      the intervention of a compiler,
//!   2. It stores the executable code somewhere.
//!
//! This example focuses on the first step: the compiler. It
//! illustrates how the abstraction over the compiler is so powerful
//! that it is possible to cross-compile a Wasm module.
//!
//! You can run the example directly by executing in Wasmer root:
//!
//! ```shell
//! cargo run --example cross-compilation --release --features "cranelift"
//! ```
//!
//! Ready?

use std::str::FromStr;
use wasmer::{wat2wasm, Module, RuntimeError, Store};
use wasmer_compiler::{CpuFeature, Target, Triple};
use wasmer_compiler_cranelift::Cranelift;
use wasmer_engine_dylib::Dylib;

fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Let's declare the Wasm module with the text representation.
    let wasm_bytes = wat2wasm(
        br#"
(module
  (type $sum_t (func (param i32 i32) (result i32)))
  (func $sum_f (type $sum_t) (param $x i32) (param $y i32) (result i32)
    local.get $x
    local.get $y
    i32.add)
  (export "sum" (func $sum_f)))
"#,
    )?;

    // Define a compiler configuration.
    //
    // In this situation, the compiler is
    // `wasmer_compiler_cranelift`. The compiler is responsible to
    // compile the Wasm module into executable code.
    let compiler_config = Cranelift::default();

    // Here we go.
    //
    // Let's define the target ‚Äútriple‚Äù. Historically, such things had
    // three fields, though additional fields have been added over
    // time.
    let triple = Triple::from_str("x86_64-linux-musl")
        .map_err(|error| RuntimeError::new(error.to_string()))?;

    // Here we go again.
    //
    // Let's define a CPU feature.
    let mut cpu_feature = CpuFeature::set();
    cpu_feature.insert(CpuFeature::from_str("sse2")?);

    // Here we go finally.
    //
    // Let's build the target.
    let target = Target::new(triple, cpu_feature);
    println!("Chosen target: {:?}", target);

    // Define the engine that will drive everything.
    //
    // In this case, the engine is `wasmer_engine_dylib` which means
    // that a shared object is going to be generated.
    //
    // That's where we specify the target for the compiler.
    //
    // Use the Dylib engine.
    let engine = Dylib::new(compiler_config)
        // Here we go.
        // Pass the target to the engine! The engine will share
        // this information with the compiler.
        .target(target)
        // Get the engine.
        .engine();

    // Create a store, that holds the engine.
    let store = Store::new(&engine);

    println!("Compiling module...");
    // Let's compile the Wasm module.
    let _module = Module::new(&store, wasm_bytes)?;

    println!("Module compiled successfully.");

    // Congrats, the Wasm module is cross-compiled!
    //
    // What to do with that? It is possible to use an engine (probably
    // a headless engine) to execute the cross-compiled Wasm module an
    // the targeted platform.

    Ok(())
}

#[test]
#[cfg(not(any(
    windows,
    // We don't support yet crosscompilation in macOS
    all(target_os = "macos"),
    target_env = "musl",
)))]
fn test_cross_compilation() -> Result<(), Box<dyn std::error::Error>> {
    main()
}

'''
'''--- examples/engine_headless.rs ---
//! Defining an engine in Wasmer is one of the fundamental steps.
//!
//! This example illustrates a neat feature of engines: their ability
//! to run in a headless mode. At the time of writing, all engines
//! have a headless mode, but it's not a requirement of the `Engine`
//! trait (defined in the `wasmer_engine` crate).
//!
//! What problem does it solve, and what does it mean?
//!
//! Once a Wasm module is compiled into executable code and stored
//! somewhere (e.g. in memory with the Universal engine, or in a
//! shared object file with the Dylib engine), the module can be
//! instantiated and executed. But imagine for a second the following
//! scenario:
//!
//!   * Modules are compiled ahead of time, to be instantiated later
//!     on.
//!   * Modules are cross-compiled on a machine ahead of time
//!     to be run on another machine later one.
//!
//! In both scenarios, the environment where the compiled Wasm module
//! will be executed can be very constrained. For such particular
//! contexts, Wasmer can be compiled _without_ the compilers, so that
//! the `wasmer` binary is as small as possible. Indeed, there is no
//! need for a compiler since the Wasm module is already compiled. All
//! we need is an engine that _only_ drives the instantiation and
//! execution of the Wasm module.
//!
//! And that, that's a headless engine.
//!
//! To achieve such a scenario, a Wasm module must be compiled, then
//! serialized ‚Äîfor example into a file‚Äî, then later, potentially on
//! another machine, deserialized. The next steps are classical: The
//! Wasm module is instantiated and executed.
//!
//! This example uses a `compiler` because it illustrates the entire
//! workflow, but keep in mind the compiler isn't required after the
//! compilation step.
//!
//! You can run the example directly by executing in Wasmer root:
//!
//! ```shell
//! cargo run --example engine-headless --release --features "cranelift"
//! ```
//!
//! Ready?

use tempfile::NamedTempFile;
use wasmer::imports;
use wasmer::wat2wasm;
use wasmer::Instance;
use wasmer::Module;
use wasmer::Store;
use wasmer::Value;
use wasmer_compiler_cranelift::Cranelift;
use wasmer_engine_dylib::Dylib;

fn main() -> Result<(), Box<dyn std::error::Error>> {
    // First step, let's compile the Wasm module and serialize it.
    // Note: we need a compiler here.
    let serialized_module_file = {
        // Let's declare the Wasm module with the text representation.
        let wasm_bytes = wat2wasm(
            r#"
(module
  (type $sum_t (func (param i32 i32) (result i32)))
  (func $sum_f (type $sum_t) (param $x i32) (param $y i32) (result i32)
    local.get $x
    local.get $y
    i32.add)
  (export "sum" (func $sum_f)))
"#
            .as_bytes(),
        )?;

        // Define a compiler configuration.
        //
        // In this situation, the compiler is
        // `wasmer_compiler_cranelift`. The compiler is responsible to
        // compile the Wasm module into executable code.
        let compiler_config = Cranelift::default();

        println!("Creating Dylib engine...");
        // Define the engine that will drive everything.
        //
        // In this case, the engine is `wasmer_engine_dylib` which
        // means that a shared object is going to be generated. So
        // when we are going to serialize the compiled Wasm module, we
        // are going to store it in a file with the `.so` extension
        // for example (or `.dylib`, or `.dll` depending of the
        // platform).
        let engine = Dylib::new(compiler_config).engine();

        // Create a store, that holds the engine.
        let store = Store::new(&engine);

        println!("Compiling module...");
        // Let's compile the Wasm module.
        let module = Module::new(&store, wasm_bytes)?;

        println!("Serializing module...");
        // Here we go. Let's serialize the compiled Wasm module in a
        // file.
        let serialized_module_file = NamedTempFile::new()?;
        module.serialize_to_file(&serialized_module_file)?;

        serialized_module_file
    };

    // Second step, deserialize the compiled Wasm module, and execute
    // it, for example with Wasmer without a compiler.
    {
        println!("Creating headless Dylib engine...");
        // We create a headless Dylib engine.
        let engine = Dylib::headless().engine();
        let store = Store::new(&engine);

        println!("Deserializing module...");
        // Here we go.
        //
        // Deserialize the compiled Wasm module. This code is unsafe
        // because Wasmer can't assert the bytes are valid (see the
        // `wasmer::Module::deserialize`'s documentation to learn
        // more).
        let module = unsafe { Module::deserialize_from_file(&store, serialized_module_file) }?;

        // Congrats, the Wasm module has been deserialized! Now let's
        // execute it for the sake of having a complete example.

        // Create an import object. Since our Wasm module didn't declare
        // any imports, it's an empty object.
        let import_object = imports! {};

        println!("Instantiating module...");
        // Let's instantiate the Wasm module.
        let instance = Instance::new(&module, &import_object)?;

        println!("Calling `sum` function...");
        // The Wasm module exports a function called `sum`.
        let sum = instance.exports.get_function("sum")?;
        let results = sum.call(&[Value::I32(1), Value::I32(2)])?;

        println!("Results: {:?}", results);
        assert_eq!(results.to_vec(), vec![Value::I32(3)]);
    }

    Ok(())
}

#[test]
#[cfg(not(any(windows, target_arch = "aarch64", target_env = "musl")))]
fn test_engine_headless() -> Result<(), Box<dyn std::error::Error>> {
    main()
}

'''
'''--- examples/engine_universal.rs ---
//! Defining an engine in Wasmer is one of the fundamental steps.
//!
//! This example illustrates how to use the `wasmer_engine_universal`,
//! aka the Universal engine. An engine applies roughly 2 steps:
//!
//!   1. It compiles the Wasm module bytes to executable code, through
//!      the intervention of a compiler,
//!   2. It stores the executable code somewhere.
//!
//! In the particular context of the Universal engine, the executable
//! code is stored in memory.
//!
//! You can run the example directly by executing in Wasmer root:
//!
//! ```shell
//! cargo run --example engine-universal --release --features "cranelift"
//! ```
//!
//! Ready?

use wasmer::{imports, wat2wasm, Instance, Module, Store, Value};
use wasmer_compiler_cranelift::Cranelift;
use wasmer_engine_universal::Universal;

fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Let's declare the Wasm module with the text representation.
    let wasm_bytes = wat2wasm(
        r#"
(module
  (type $sum_t (func (param i32 i32) (result i32)))
  (func $sum_f (type $sum_t) (param $x i32) (param $y i32) (result i32)
    local.get $x
    local.get $y
    i32.add)
  (export "sum" (func $sum_f)))
"#
        .as_bytes(),
    )?;

    // Define a compiler configuration.
    //
    // In this situation, the compiler is
    // `wasmer_compiler_cranelift`. The compiler is responsible to
    // compile the Wasm module into executable code.
    let compiler_config = Cranelift::default();

    println!("Creating Universal engine...");
    // Define the engine that will drive everything.
    //
    // In this case, the engine is `wasmer_engine_universal` which roughly
    // means that the executable code will live in memory.
    let engine = Universal::new(compiler_config).engine();

    // Create a store, that holds the engine.
    let store = Store::new(&engine);

    println!("Compiling module...");
    // Here we go.
    //
    // Let's compile the Wasm module. It is at this step that the Wasm
    // text is transformed into Wasm bytes (if necessary), and then
    // compiled to executable code by the compiler, which is then
    // stored in memory by the engine.
    let module = Module::new(&store, wasm_bytes)?;

    // Congrats, the Wasm module is compiled! Now let's execute it for
    // the sake of having a complete example.

    // Create an import object. Since our Wasm module didn't declare
    // any imports, it's an empty object.
    let import_object = imports! {};

    println!("Instantiating module...");
    // And here we go again. Let's instantiate the Wasm module.
    let instance = Instance::new(&module, &import_object)?;

    println!("Calling `sum` function...");
    // The Wasm module exports a function called `sum`.
    let sum = instance.exports.get_function("sum")?;
    let results = sum.call(&[Value::I32(1), Value::I32(2)])?;

    println!("Results: {:?}", results);
    assert_eq!(results.to_vec(), vec![Value::I32(3)]);

    Ok(())
}

#[test]
fn test_engine_universal() -> Result<(), Box<dyn std::error::Error>> {
    main()
}

'''
'''--- examples/errors.rs ---
//! A Wasm module can sometimes be invalid or trigger traps, and in those case we will get
//! an error back from the API.
//!
//! In this example we'll see how to handle such errors in the most
//! basic way. To do that we'll use a Wasm module that we know will
//! produce an error.
//!
//! You can run the example directly by executing in Wasmer root:
//!
//! ```shell
//! cargo run --example errors --release --features "cranelift"
//! ```
//!
//! Ready?

use wasmer::{imports, wat2wasm, Instance, Module, Store};
use wasmer_compiler_cranelift::Cranelift;
use wasmer_engine_universal::Universal;

fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Let's declare the Wasm module with the text representation.
    let wasm_bytes = wat2wasm(
        br#"
(module
  (type $do_div_by_zero_t (func (result i32)))
  (func $do_div_by_zero_f (type $do_div_by_zero_t) (result i32)
    i32.const 4
    i32.const 0
    i32.div_s)

  (type $div_by_zero_t (func (result i32)))
  (func $div_by_zero_f (type $div_by_zero_t) (result i32)
    call $do_div_by_zero_f)
  (export "div_by_zero" (func $div_by_zero_f)))
"#,
    )?;

    // Create a Store.
    // Note that we don't need to specify the engine/compiler if we want to use
    // the default provided by Wasmer.
    // You can use `Store::default()` for that.
    let store = Store::new(&Universal::new(Cranelift::default()).engine());

    println!("Compiling module...");
    // Let's compile the Wasm module.
    let module = Module::new(&store, wasm_bytes)?;

    // Create an import object.
    let import_object = imports! {};

    println!("Instantiating module...");
    // Let's instantiate the Wasm module.
    let instance = Instance::new(&module, &import_object)?;

    // Here we go.
    //
    // The Wasm module exports a function called `div_by_zero`. As its name
    // implies, this function will try to do a division by zero and thus
    // produce an error.
    //
    // Let's get it.
    let div_by_zero = instance
        .exports
        .get_function("div_by_zero")?
        .native::<(), i32>()?;

    println!("Calling `div_by_zero` function...");
    // Let's call the `div_by_zero` exported function.
    let result = div_by_zero.call();

    // When we call a function it can either succeed or fail. We expect it to fail.
    match result {
        Ok(_) => {
            // This should have thrown an error, return an error
            panic!("div_by_zero did not error");
        }
        Err(e) => {
            // Log the error
            println!("Error caught from `div_by_zero`: {}", e.message());

            // Errors come with a trace we can inspect to get more
            // information on the execution flow.
            let frames = e.trace();
            let frames_len = frames.len();

            for i in 0..frames_len {
                println!(
                    "  Frame #{}: {:?}::{:?}",
                    frames_len - i,
                    frames[i].module_name(),
                    frames[i].function_name().or(Some("<func>")).unwrap()
                );
            }
        }
    }

    Ok(())
}

#[test]
fn test_exported_function() -> Result<(), Box<dyn std::error::Error>> {
    main()
}

'''
'''--- examples/exports_function.rs ---
//! A Wasm module can export entities, like functions, memories,
//! globals and tables.
//!
//! This example illustrates how to use exported functions. They come
//! in 2 flavors:
//!
//!   1. Dynamic functions, where parameters and results are of a
//!      slice of `Value`,
//!   2. Native function, where parameters and results are statically
//!      typed Rust values.
//!
//! You can run the example directly by executing in Wasmer root:
//!
//! ```shell
//! cargo run --example exported-function --release --features "cranelift"
//! ```
//!
//! Ready?

use wasmer::{imports, wat2wasm, Instance, Module, Store, Value};
use wasmer_compiler_cranelift::Cranelift;
use wasmer_engine_universal::Universal;

fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Let's declare the Wasm module with the text representation.
    let wasm_bytes = wat2wasm(
        r#"
(module
  (type $sum_t (func (param i32 i32) (result i32)))
  (func $sum_f (type $sum_t) (param $x i32) (param $y i32) (result i32)
    local.get $x
    local.get $y
    i32.add)
  (export "sum" (func $sum_f)))
"#
        .as_bytes(),
    )?;

    // Create a Store.
    // Note that we don't need to specify the engine/compiler if we want to use
    // the default provided by Wasmer.
    // You can use `Store::default()` for that.
    let store = Store::new(&Universal::new(Cranelift::default()).engine());

    println!("Compiling module...");
    // Let's compile the Wasm module.
    let module = Module::new(&store, wasm_bytes)?;

    // Create an empty import object.
    let import_object = imports! {};

    println!("Instantiating module...");
    // Let's instantiate the Wasm module.
    let instance = Instance::new(&module, &import_object)?;

    // Here we go.
    //
    // The Wasm module exports a function called `sum`. Let's get
    // it. Note that
    //
    //     ```
    //     get_function(name)
    //     ```
    //
    // is just an alias to
    //
    //     ```
    //     get::<Function>(name)`.
    //     ```
    let sum = instance.exports.get_function("sum")?;

    println!("Calling `sum` function...");
    // Let's call the `sum` exported function. The parameters are a
    // slice of `Value`s. The results are a boxed slice of `Value`s.
    let args = [Value::I32(1), Value::I32(2)];
    let result = sum.call(&args)?;

    println!("Results: {:?}", result);
    assert_eq!(result.to_vec(), vec![Value::I32(3)]);

    // That was fun. But what if we can get rid of the `Value`s? Well,
    // that's possible with the `NativeFunction` API. The function
    // will use native Rust values.
    //
    // Note that `native` takes 2 generic parameters: `Args` and
    // `Rets`, respectively for the parameters and the results. If
    // those values don't match the exported function signature, an
    // error will be raised.
    let sum_native = sum.native::<(i32, i32), i32>()?;

    println!("Calling `sum` function (natively)...");
    // Let's call the `sum` exported function. The parameters are
    // statically typed Rust values of type `i32` and `i32`. The
    // result, in this case particular case, in a unit of type `i32`.
    let result = sum_native.call(3, 4)?;

    println!("Results: {:?}", result);
    assert_eq!(result, 7);

    // Much nicer, isn't it?
    //
    // Those two API exist because they address different needs. The
    // former has a more dynamic approach, while the second has a more
    // static approach.

    Ok(())
}

#[test]
fn test_exported_function() -> Result<(), Box<dyn std::error::Error>> {
    main()
}

'''
'''--- examples/exports_global.rs ---
//! A Wasm module can export entities, like functions, memories,
//! globals and tables.
//!
//! This example illustrates how to use exported globals. They come
//! in 2 flavors:
//!
//!   1. Immutable globals (const),
//!   2. Mutable globals.
//!
//! You can run the example directly by executing in Wasmer root:
//!
//! ```shell
//! cargo run --example exported-global --release --features "cranelift"
//! ```
//!
//! Ready?

use wasmer::{imports, wat2wasm, Instance, Module, Mutability, Store, Type, Value};
use wasmer_compiler_cranelift::Cranelift;
use wasmer_engine_universal::Universal;

fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Let's declare the Wasm module with the text representation.
    let wasm_bytes = wat2wasm(
        br#"
(module
  (global $one (export "one") f32 (f32.const 1))
  (global $some (export "some") (mut f32) (f32.const 0))

  (func (export "get_one") (result f32) (global.get $one))
  (func (export "get_some") (result f32) (global.get $some))

  (func (export "set_some") (param f32) (global.set $some (local.get 0))))
"#,
    )?;

    // Create a Store.
    // Note that we don't need to specify the engine/compiler if we want to use
    // the default provided by Wasmer.
    // You can use `Store::default()` for that.
    let store = Store::new(&Universal::new(Cranelift::default()).engine());

    println!("Compiling module...");
    // Let's compile the Wasm module.
    let module = Module::new(&store, wasm_bytes)?;

    // Create an empty import object.
    let import_object = imports! {};

    println!("Instantiating module...");
    // Let's instantiate the Wasm module.
    let instance = Instance::new(&module, &import_object)?;

    // Here we go.
    //
    // The Wasm module exports some globals. Let's get them.
    // Note that
    //
    //     ```
    //     get_global(name)
    //     ```
    //
    // is just an alias to
    //
    //     ```
    //     get::<Global>(name)`.
    //     ```
    let one = instance.exports.get_global("one")?;
    let some = instance.exports.get_global("some")?;

    println!("Getting globals types information...");
    // Let's get the globals types. The results are `GlobalType`s.
    let one_type = one.ty();
    let some_type = some.ty();

    println!("`one` type: {:?} {:?}", one_type.mutability, one_type.ty);
    assert_eq!(one_type.mutability, Mutability::Const);
    assert_eq!(one_type.ty, Type::F32);

    println!("`some` type: {:?} {:?}", some_type.mutability, some_type.ty);
    assert_eq!(some_type.mutability, Mutability::Var);
    assert_eq!(some_type.ty, Type::F32);

    println!("Getting global values...");
    // Getting the values of globals can be done in two ways:
    //   1. Through an exported function,
    //   2. Using the Global API directly.
    //
    // We will use an exported function for the `one` global
    // and the Global API for `some`.
    let get_one = instance
        .exports
        .get_function("get_one")?
        .native::<(), f32>()?;

    let one_value = get_one.call()?;
    let some_value = some.get();

    println!("`one` value: {:?}", one_value);
    assert_eq!(one_value, 1.0);

    println!("`some` value: {:?}", some_value);
    assert_eq!(some_value, Value::F32(0.0));

    println!("Setting global values...");
    // Trying to set the value of a immutable global (`const`)
    // will result in a `RuntimeError`.
    let result = one.set(Value::F32(42.0));
    assert_eq!(
        result.expect_err("Expected an error").message(),
        "Attempted to set an immutable global"
    );

    let one_result = one.get();
    println!("`one` value after `set`: {:?}", one_result);
    assert_eq!(one_result, Value::F32(1.0));

    // Setting the values of globals can be done in two ways:
    //   1. Through an exported function,
    //   2. Using the Global API directly.
    //
    // We will use both for the `some` global.
    let set_some = instance
        .exports
        .get_function("set_some")?
        .native::<f32, ()>()?;
    set_some.call(21.0)?;
    let some_result = some.get();
    println!("`some` value after `set_some`: {:?}", some_result);
    assert_eq!(some_result, Value::F32(21.0));

    some.set(Value::F32(42.0))?;
    let some_result = some.get();
    println!("`some` value after `set`: {:?}", some_result);
    assert_eq!(some_result, Value::F32(42.0));

    Ok(())
}

#[test]
fn test_exported_global() -> Result<(), Box<dyn std::error::Error>> {
    main()
}

'''
'''--- examples/exports_memory.rs ---
//! A Wasm module can export entities, like functions, memories,
//! globals and tables.
//!
//! This example illustrates how to use exported memories
//!
//! You can run the example directly by executing in Wasmer root:
//!
//! ```shell
//! cargo run --example exported-memory --release --features "cranelift"
//! ```
//!
//! Ready?

use wasmer::{imports, wat2wasm, Array, Instance, Module, Store, WasmPtr};
use wasmer_compiler_cranelift::Cranelift;
use wasmer_engine_universal::Universal;

fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Let's declare the Wasm module with the text representation.
    let wasm_bytes = wat2wasm(
        br#"
(module
  (memory (export "mem") 1)

  (global $offset i32 (i32.const 42))
  (global $length (mut i32) (i32.const 13))

  (func (export "load") (result i32 i32)
    global.get $offset
    global.get $length)

  (data (global.get $offset) "Hello, World!"))
"#,
    )?;

    // Create a Store.
    // Note that we don't need to specify the engine/compiler if we want to use
    // the default provided by Wasmer.
    // You can use `Store::default()` for that.
    let store = Store::new(&Universal::new(Cranelift::default()).engine());

    println!("Compiling module...");
    // Let's compile the Wasm module.
    let module = Module::new(&store, wasm_bytes)?;

    // Create an empty import object.
    let import_object = imports! {};

    println!("Instantiating module...");
    // Let's instantiate the Wasm module.
    let instance = Instance::new(&module, &import_object)?;

    let load = instance
        .exports
        .get_native_function::<(), (WasmPtr<u8, Array>, i32)>("load")?;

    // Here we go.
    //
    // The Wasm module exports a memory under "mem". Let's get it.
    let memory = instance.exports.get_memory("mem")?;

    // Now that we have the exported memory, let's get some
    // information about it.
    //
    // The first thing we might be intersted in is the size of the memory.
    // Let's get it!
    println!("Memory size (pages) {:?}", memory.size());
    println!("Memory size (bytes) {:?}", memory.data_size());

    // Next, we'll want to read the contents of the memory.
    //
    // To do so, we have to get a `View` of the memory.
    //let view = memory.view::<u8>();

    // Oh! Wait, before reading the contents, we need to know
    // where to find what we are looking for.
    //
    // Fortunately, the Wasm module exports a `load` function
    // which will tell us the offset and length of the string.
    let (ptr, length) = load.call()?;
    println!("String offset: {:?}", ptr.offset());
    println!("String length: {:?}", length);

    // We now know where to fin our string, let's read it.
    //
    // We will get bytes out of the memory so we need to
    // decode them into a string.
    let str = ptr.get_utf8_string(memory, length as u32).unwrap();
    println!("Memory contents: {:?}", str);

    // What about changing the contents of the memory with a more
    // appropriate string?
    //
    // To do that, we'll dereference our pointer and change the content
    // of each `Cell`
    let new_str = b"Hello, Wasmer!";
    let values = ptr.deref(memory, 0, new_str.len() as u32).unwrap();
    for i in 0..new_str.len() {
        values[i].set(new_str[i]);
    }

    // And now, let's see the result.
    //
    // Since the new strings is bigger than the older one, we
    // query the length again. The offset remains the same as
    // before.
    println!("New string length: {:?}", new_str.len());

    let str = ptr.get_utf8_string(memory, new_str.len() as u32).unwrap();
    println!("New memory contents: {:?}", str);

    // Much better, don't you think?

    Ok(())
}

#[test]
fn test_exported_memory() -> Result<(), Box<dyn std::error::Error>> {
    main()
}

'''
'''--- examples/features.rs ---
//! WebAssembly is a living standard. Wasmer integrates some
//! WebAssembly features that aren't yet stable but can still be
//! turned on. This example explains how.
//!
//! You can run the example directly by executing in Wasmer root:
//!
//! ```shell
//! cargo run --example features --release --features "cranelift"
//! ```
//!
//! Ready?

use wasmer::{imports, wat2wasm, Features, Instance, Module, Store, Value};
use wasmer_compiler_cranelift::Cranelift;
use wasmer_engine_universal::Universal;

fn main() -> anyhow::Result<()> {
    // Let's declare the Wasm module with the text representation.
    let wasm_bytes = wat2wasm(
        br#"
(module
  (type $swap_t (func (param i32 i64) (result i64 i32)))
  (func $swap (type $swap_t) (param $x i32) (param $y i64) (result i64 i32)
    (local.get $y)
    (local.get $x))
  (export "swap" (func $swap)))
"#,
    )?;

    // Set up the compiler.
    let compiler = Cranelift::default();

    // Let's declare the features.
    let mut features = Features::new();
    // Enable the multi-value feature.
    features.multi_value(true);

    // Set up the engine. That's where we define the features!
    let engine = Universal::new(compiler).features(features);

    // Now, let's define the store, and compile the module.
    let store = Store::new(&engine.engine());
    let module = Module::new(&store, wasm_bytes)?;

    // Finally, let's instantiate the module, and execute something
    // :-).
    let import_object = imports! {};
    let instance = Instance::new(&module, &import_object)?;
    let swap = instance.exports.get_function("swap")?;

    let results = swap.call(&[Value::I32(1), Value::I64(2)])?;

    assert_eq!(results.to_vec(), vec![Value::I64(2), Value::I32(1)]);

    Ok(())
}

'''
'''--- examples/hello_world.rs ---
//! This is a simple example introducing the core concepts of the Wasmer API.
//!
//! You can run the example directly by executing the following in the Wasmer root:
//!
//! ```shell
//! cargo run --example hello-world --release --features "cranelift"
//! ```

use wasmer::{imports, wat2wasm, Function, Instance, Module, NativeFunc, Store};
use wasmer_compiler_cranelift::Cranelift;
use wasmer_engine_universal::Universal;

fn main() -> anyhow::Result<()> {
    // First we create a simple Wasm program to use with Wasmer.
    // We use the WebAssembly text format and use `wasmer::wat2wasm` to compile
    // it into a WebAssembly binary.
    //
    // Most WebAssembly programs come from compiling source code in a high level
    // language and will already be in the binary format.
    let wasm_bytes = wat2wasm(
        br#"
(module
  ;; First we define a type with no parameters and no results.
  (type $no_args_no_rets_t (func (param) (result)))

  ;; Then we declare that we want to import a function named "env" "say_hello" with
  ;; that type signature.
  (import "env" "say_hello" (func $say_hello (type $no_args_no_rets_t)))

  ;; Finally we create an entrypoint that calls our imported function.
  (func $run (type $no_args_no_rets_t)
    (call $say_hello))
  ;; And mark it as an exported function named "run".
  (export "run" (func $run)))
"#,
    )?;

    // Next we create the `Store`, the top level type in the Wasmer API.
    //
    // Note that we don't need to specify the engine/compiler if we want to use
    // the default provided by Wasmer.
    // You can use `Store::default()` for that.
    //
    // However for the purposes of showing what's happening, we create a compiler
    // (`Cranelift`) and pass it to an engine (`Universal`). We then pass the engine to
    // the store and are now ready to compile and run WebAssembly!
    let store = Store::new(&Universal::new(Cranelift::default()).engine());

    // We then use our store and Wasm bytes to compile a `Module`.
    // A `Module` is a compiled WebAssembly module that isn't ready to execute yet.
    let module = Module::new(&store, wasm_bytes)?;

    // Next we'll set up our `Module` so that we can execute it.

    // We define a function to act as our "env" "say_hello" function imported in the
    // Wasm program above.
    fn say_hello_world() {
        println!("Hello, world!")
    }

    // We then create an import object so that the `Module`'s imports can be satisfied.
    let import_object = imports! {
        // We use the default namespace "env".
        "env" => {
            // And call our function "say_hello".
            "say_hello" => Function::new_native(&store, say_hello_world),
        }
    };

    // We then use the `Module` and the import object to create an `Instance`.
    //
    // An `Instance` is a compiled WebAssembly module that has been set up
    // and is ready to execute.
    let instance = Instance::new(&module, &import_object)?;

    // We get the `NativeFunc` with no parameters and no results from the instance.
    //
    // Recall that the Wasm module exported a function named "run", this is getting
    // that exported function from the `Instance`.
    let run_func: NativeFunc<(), ()> = instance.exports.get_native_function("run")?;

    // Finally, we call our exported Wasm function which will call our "say_hello"
    // function and return.
    run_func.call()?;

    Ok(())
}

'''
'''--- examples/imports_exports.rs ---
//! A Wasm module can import and export entities, like functions, memories, globals and tables.
//! This example illustrates the basics of using these entities.
//!
//! In this example we'll be using a sample Wasm module which exports some entities and requires us
//! to also import some of them.
//!
//! The goal here is to give you an idea of how to work with imports and exports. We won't go into
//! the details of each entities, they'll be covered in more details in the other examples.
//!
//! You can run the example directly by executing in Wasmer root:
//!
//! ```shell
//! cargo run --example imports-exports --release --features "cranelift"
//! ```
//!
//! Ready?

use wasmer::{
    imports, wat2wasm, Function, FunctionType, Global, Instance, Memory, Module, Store, Table,
    Type, Value,
};
use wasmer_compiler_cranelift::Cranelift;
use wasmer_engine_universal::Universal;

fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Let's declare the Wasm module.
    //
    // We are using the text representation of the module here but you can also load `.wasm`
    // files using the `include_bytes!` macro.
    let wasm_bytes = wat2wasm(
        br#"
(module
  (func $host_function (import "" "host_function") (result i32))
  (global $host_global (import "env" "host_global") i32)

  (func $function (export "guest_function") (result i32) (global.get $global))
  (global $global (export "guest_global") i32 (i32.const 42))
  (table $table (export "guest_table") 1 1 funcref)
  (memory $memory (export "guest_memory") 1))
"#,
    )?;

    // Create a Store.
    // Note that we don't need to specify the engine/compiler if we want to use
    // the default provided by Wasmer.
    // You can use `Store::default()` for that.
    let store = Store::new(&Universal::new(Cranelift::default()).engine());

    println!("Compiling module...");
    // Let's compile the Wasm module.
    let module = Module::new(&store, wasm_bytes)?;

    // Here we go.
    //
    // Before we can instantiate our module, we need to define
    // the entities we will import.
    //
    // We won't go into details here as creating entities will be
    // covered in more detail in other examples.
    println!("Creating the imported function...");
    let host_function_signature = FunctionType::new(vec![], vec![Type::I32]);
    let host_function = Function::new(&store, &host_function_signature, |_args| {
        Ok(vec![Value::I32(42)])
    });

    println!("Creating the imported global...");
    let host_global = Global::new(&store, Value::I32(42));

    // Create an import object.
    //
    // Imports are stored in namespaces. We'll need to register each of the
    // namespaces with a name and add the imported entities there.
    //
    // Note that the namespace can also have an empty name.
    //
    // Our module requires us to import:
    //   * A function `host_function` in a namespace with an empty name;
    //   * A global `host_global` in the `env` namespace.
    //
    // Let's do this!
    let import_object = imports! {
        "" => {
            "host_function" => host_function,
        },
        "env" => {
            "host_global" => host_global,
        },
    };

    println!("Instantiating module...");
    // Let's instantiate the Wasm module.
    let instance = Instance::new(&module, &import_object)?;

    // Here we go.
    //
    // The Wasm module exports some entities:
    //   * A function: `guest_function`
    //   * A global: `guest_global`
    //   * A memory: `guest_memory`
    //   * A table: `guest_table`
    //
    // Let's get them.
    println!("Getting the exported function...");
    let function = instance.exports.get::<Function>("guest_function")?;
    println!("Got exported function of type: {:?}", function.ty());

    println!("Getting the exported global...");
    let global = instance.exports.get::<Global>("guest_global")?;
    println!("Got exported global of type: {:?}", global.ty());

    println!("Getting the exported memory...");
    let memory = instance.exports.get::<Memory>("guest_memory")?;
    println!("Got exported memory of type: {:?}", memory.ty());

    println!("Getting the exported table...");
    let table = instance.exports.get::<Table>("guest_table")?;
    println!("Got exported table of type: {:?}", table.ty());

    Ok(())
}

#[test]
fn test_imports_exports() -> Result<(), Box<dyn std::error::Error>> {
    main()
}

'''
'''--- examples/imports_function.rs ---
//! A Wasm module can import entities, like functions, memories,
//! globals and tables.
//!
//! This example illustrates how to use imported functions. They come
//! in 2 flavors:
//!
//!   1. Dynamic functions, where parameters and results are of a
//!      slice of `Value`,
//!   2. Native function, where parameters and results are statically
//!      typed Rust values.
//!
//! You can run the example directly by executing in Wasmer root:
//!
//! ```shell
//! cargo run --example imported-function --release --features "cranelift"
//! ```
//!
//! Ready?

use wasmer::{imports, wat2wasm, Function, FunctionType, Instance, Module, Store, Type, Value};
use wasmer_compiler_cranelift::Cranelift;
use wasmer_engine_universal::Universal;

fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Let's declare the Wasm module with the text representation.
    let wasm_bytes = wat2wasm(
        br#"
(module
  (func $multiply_dynamic (import "env" "multiply_dynamic") (param i32) (result i32))
  (func $multiply_native (import "env" "multiply_native") (param i32) (result i32))

  (type $sum_t (func (param i32) (param i32) (result i32)))
  (func $sum_f (type $sum_t) (param $x i32) (param $y i32) (result i32)
    (call $multiply_dynamic (local.get $x))
    (call $multiply_native (local.get $y))
    i32.add)
  (export "sum" (func $sum_f)))
"#,
    )?;

    // Create a Store.
    // Note that we don't need to specify the engine/compiler if we want to use
    // the default provided by Wasmer.
    // You can use `Store::default()` for that.
    let store = Store::new(&Universal::new(Cranelift::default()).engine());

    println!("Compiling module...");
    // Let's compile the Wasm module.
    let module = Module::new(&store, wasm_bytes)?;

    // Create the functions
    let multiply_dynamic_signature = FunctionType::new(vec![Type::I32], vec![Type::I32]);
    let multiply_dynamic = Function::new(&store, &multiply_dynamic_signature, |args| {
        println!("Calling `multiply_dynamic`...");

        let result = args[0].unwrap_i32() * 2;

        println!("Result of `multiply_dynamic`: {:?}", result);

        Ok(vec![Value::I32(result)])
    });

    fn multiply(a: i32) -> i32 {
        println!("Calling `multiply_native`...");
        let result = a * 3;

        println!("Result of `multiply_native`: {:?}", result);

        result
    }
    let multiply_native = Function::new_native(&store, multiply);

    // Create an import object.
    let import_object = imports! {
        "env" => {
            "multiply_dynamic" => multiply_dynamic,
            "multiply_native" => multiply_native,
        }
    };

    println!("Instantiating module...");
    // Let's instantiate the Wasm module.
    let instance = Instance::new(&module, &import_object)?;

    // Here we go.
    //
    // The Wasm module exports a function called `sum`. Let's get it.
    let sum = instance
        .exports
        .get_function("sum")?
        .native::<(i32, i32), i32>()?;

    println!("Calling `sum` function...");
    // Let's call the `sum` exported function. It will call each
    // of the imported functions.
    let result = sum.call(1, 2)?;

    println!("Results of `sum`: {:?}", result);
    assert_eq!(result, 8);

    Ok(())
}

#[test]
fn test_exported_function() -> Result<(), Box<dyn std::error::Error>> {
    main()
}

'''
'''--- examples/imports_function_env.rs ---
//! A Wasm module can import entities, like functions, memories,
//! globals and tables.
//!
//! In this example, we'll create a system for getting and adjusting a counter value. However, host
//! functions are not limited to storing data outside of Wasm, they're normal host functions and
//! can do anything that the host can do.
//!
//!   1. There will be a `get_counter` function that will return an i32 of
//!      the current global counter,
//!   2. There will be an `add_to_counter` function will add the passed
//!      i32 value to the counter, and return an i32 of the current
//!      global counter.
//!
//! You can run the example directly by executing in Wasmer root:
//!
//! ```shell
//! cargo run --example imported-function-env --release --features "cranelift"
//! ```
//!
//! Ready?

use std::sync::{Arc, Mutex};
use wasmer::{imports, wat2wasm, Function, Instance, Module, Store, WasmerEnv};
use wasmer_compiler_cranelift::Cranelift;
use wasmer_engine_universal::Universal;

fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Let's declare the Wasm module with the text representation.
    let wasm_bytes = wat2wasm(
        br#"
(module
  (func $get_counter (import "env" "get_counter") (result i32))
  (func $add_to_counter (import "env" "add_to_counter") (param i32) (result i32))

  (type $increment_t (func (param i32) (result i32)))
  (func $increment_f (type $increment_t) (param $x i32) (result i32)
    (block
      (loop
        (call $add_to_counter (i32.const 1))
        (set_local $x (i32.sub (get_local $x) (i32.const 1)))
        (br_if 1 (i32.eq (get_local $x) (i32.const 0)))
        (br 0)))
    call $get_counter)
  (export "increment_counter_loop" (func $increment_f)))
"#,
    )?;

    // Create a Store.
    // Note that we don't need to specify the engine/compiler if we want to use
    // the default provided by Wasmer.
    // You can use `Store::default()` for that.
    let store = Store::new(&Universal::new(Cranelift::default()).engine());

    println!("Compiling module...");
    // Let's compile the Wasm module.
    let module = Module::new(&store, wasm_bytes)?;

    // We create some shared data here, `Arc` is required because we may
    // move our WebAssembly instance to another thread to run it. Mutex
    // lets us get shared mutabilty which is fine because we know we won't
    // run host calls concurrently.  If concurrency is a possibilty, we'd have
    // to use a `Mutex`.
    let shared_counter: Arc<Mutex<i32>> = Arc::new(Mutex::new(0));

    // Once we have our counter we'll wrap it inside en `Env` which we'll pass
    // to our imported functions.
    //
    // This struct may have been anything. The only constraint is it must be
    // possible to know the size of the `Env` at compile time (i.e it has to
    // implement the `Sized` trait) and that it implement the `WasmerEnv` trait.
    // We derive a default implementation of `WasmerEnv` here.
    #[derive(WasmerEnv, Clone)]
    struct Env {
        counter: Arc<Mutex<i32>>,
    }

    // Create the functions
    fn get_counter(env: &Env) -> i32 {
        *env.counter.lock().unwrap()
    }
    fn add_to_counter(env: &Env, add: i32) -> i32 {
        let mut counter_ref = env.counter.lock().unwrap();

        *counter_ref += add;
        *counter_ref
    }

    // Create an import object.
    let import_object = imports! {
        "env" => {
            "get_counter" => Function::new_native_with_env(&store, Env { counter: shared_counter.clone() }, get_counter),
            "add_to_counter" => Function::new_native_with_env(&store, Env { counter: shared_counter.clone() }, add_to_counter),
        }
    };

    println!("Instantiating module...");
    // Let's instantiate the Wasm module.
    let instance = Instance::new(&module, &import_object)?;

    // Here we go.
    //
    // The Wasm module exports a function called `increment_counter_loop`. Let's get it.
    let increment_counter_loop = instance
        .exports
        .get_function("increment_counter_loop")?
        .native::<i32, i32>()?;

    let counter_value: i32 = *shared_counter.lock().unwrap();
    println!("Initial ounter value: {:?}", counter_value);

    println!("Calling `increment_counter_loop` function...");
    // Let's call the `increment_counter_loop` exported function.
    //
    // It will loop five times thus incrementing our counter five times.
    let result = increment_counter_loop.call(5)?;

    let counter_value: i32 = *shared_counter.lock().unwrap();
    println!("New counter value (host): {:?}", counter_value);
    assert_eq!(counter_value, 5);

    println!("New counter value (guest): {:?}", result);
    assert_eq!(result, 5);

    Ok(())
}

#[test]
fn test_imported_function_env() -> Result<(), Box<dyn std::error::Error>> {
    main()
}

'''
'''--- examples/imports_global.rs ---
//! A Wasm module can import entities, like functions, memories,
//! globals and tables.
//!
//! This example illustrates how to use imported globals. They come
//! in 2 flavors:
//!
//!   1. Immutable globals (const),
//!   2. Mutable globals.
//!
//! You can run the example directly by executing in Wasmer root:
//!
//! ```shell
//! cargo run --example imported-global --release --features "cranelift"
//! ```
//!
//! Ready?

use wasmer::{imports, wat2wasm, Global, Instance, Module, Store, Value};
use wasmer_compiler_cranelift::Cranelift;
use wasmer_engine_universal::Universal;

fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Let's declare the Wasm module with the text representation.
    let wasm_bytes = wat2wasm(
        br#"
(module
  (global $some (import "env" "some") f32)
  (global $other (import "env" "other") (mut f32))

  (func (export "get_some") (result f32) (global.get $some))
  (func (export "get_other") (result f32) (global.get $other))

  (func (export "set_other") (param f32) (global.set $other (local.get 0))))
"#,
    )?;

    // Create a Store.
    // Note that we don't need to specify the engine/compiler if we want to use
    // the default provided by Wasmer.
    // You can use `Store::default()` for that.
    let store = Store::new(&Universal::new(Cranelift::default()).engine());

    println!("Compiling module...");
    // Let's compile the Wasm module.
    let module = Module::new(&store, wasm_bytes)?;

    // Create the globals
    let some = Global::new(&store, Value::F32(1.0));
    let other = Global::new_mut(&store, Value::F32(2.0));

    // Create an import object.
    // We add the two required globals in the `env` namespace.
    let import_object = imports! {
        "env" => {
            "some" => some.clone(),
            "other" => other.clone(),
        }
    };

    println!("Instantiating module...");
    // Let's instantiate the Wasm module.
    let instance = Instance::new(&module, &import_object)?;

    // Here we go.
    //
    // The Wasm module only imports some globals. We'll have to interact
    // with them either using the Global API or exported functions.
    let get_some = instance
        .exports
        .get_function("get_some")?
        .native::<(), f32>()?;
    let get_other = instance
        .exports
        .get_function("get_other")?
        .native::<(), f32>()?;

    let some_result = get_some.call()?;
    let other_result = get_other.call()?;

    println!("some value (via `get_some`): {:?}", some_result);
    println!("some value (via Global API): {:?}", some.get());
    println!("other value (via `get_other`): {:?}", other_result);
    println!("other value (via Global API): {:?}", other.get());

    assert_eq!(some_result, some.get().f32().unwrap());
    assert_eq!(other_result, other.get().f32().unwrap());

    println!("Setting global values...");
    // Trying to set the value of a immutable global (`const`)
    // will result in a `RuntimeError`.
    let result = some.set(Value::F32(42.0));
    assert_eq!(
        result.expect_err("Expected an error").message(),
        "Attempted to set an immutable global"
    );

    other.set(Value::F32(21.0))?;
    let other_result = other.get();
    println!("other value after `set`: {:?}", other_result);
    assert_eq!(other_result, Value::F32(21.0));

    println!("Altering global values through exported functions...");
    // Changes made to global through exported functions will
    // be reflected on the host side.
    let set_other = instance
        .exports
        .get_function("set_other")?
        .native::<f32, ()>()?;
    set_other.call(42.0)?;

    println!("other value (via Global API): {:?}", other.get());
    assert_eq!(other.get(), Value::F32(42.0));

    Ok(())
}

#[test]
fn test_imported_global() -> Result<(), Box<dyn std::error::Error>> {
    main()
}

'''
'''--- examples/instance.rs ---
//! Wasmer will let you easily run Wasm module in a Rust host.
//!
//! This example illustrates the basics of using Wasmer through a "Hello World"-like project:
//!
//!   1. How to load a Wasm modules as bytes
//!   2. How to compile the module
//!   3. How to create an instance of the module
//!
//! You can run the example directly by executing in Wasmer root:
//!
//! ```shell
//! cargo run --example instance --release --features "cranelift"
//! ```
//!
//! Ready?

use wasmer::{imports, wat2wasm, Instance, Module, Store};
use wasmer_compiler_cranelift::Cranelift;
use wasmer_engine_universal::Universal;

fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Let's declare the Wasm module.
    //
    // We are using the text representation of the module here but you can also load `.wasm`
    // files using the `include_bytes!` macro.
    let wasm_bytes = wat2wasm(
        br#"
(module
  (type $add_one_t (func (param i32) (result i32)))
  (func $add_one_f (type $add_one_t) (param $value i32) (result i32)
    local.get $value
    i32.const 1
    i32.add)
  (export "add_one" (func $add_one_f)))
"#,
    )?;

    // Create a Store.
    // Note that we don't need to specify the engine/compiler if we want to use
    // the default provided by Wasmer.
    // You can use `Store::default()` for that.
    let store = Store::new(&Universal::new(Cranelift::default()).engine());

    println!("Compiling module...");
    // Let's compile the Wasm module.
    let module = Module::new(&store, wasm_bytes)?;

    // Create an empty import object.
    let import_object = imports! {};

    println!("Instantiating module...");
    // Let's instantiate the Wasm module.
    let instance = Instance::new(&module, &import_object)?;

    // We now have an instance ready to be used.
    //
    // From an `Instance` we can retrieve any exported entities.
    // Each of these entities is covered in others examples.
    //
    // Here we are retrieving the exported function. We won't go into details here
    // as the main focus of this example is to show how to create an instance out
    // of a Wasm module and have basic interactions with it.
    let add_one = instance
        .exports
        .get_function("add_one")?
        .native::<i32, i32>()?;

    println!("Calling `add_one` function...");
    let result = add_one.call(1)?;

    println!("Results of `add_one`: {:?}", result);
    assert_eq!(result, 2);

    Ok(())
}

#[test]
fn test_exported_function() -> Result<(), Box<dyn std::error::Error>> {
    main()
}

'''
'''--- examples/memory.rs ---
//! With Wasmer you'll be able to interact with guest module memory.
//!
//! This example illustrates the basics of interacting with Wasm module memory.:
//!
//!   1. How to load a Wasm modules as bytes
//!   2. How to compile the module
//!   3. How to create an instance of the module
//!
//! You can run the example directly by executing in Wasmer root:
//!
//! ```shell
//! cargo run --example memory --release --features "cranelift"
//! ```
//!
//! Ready?

use std::mem;
use wasmer::{imports, wat2wasm, Bytes, Instance, Module, NativeFunc, Pages, Store};
use wasmer_compiler_cranelift::Cranelift;
use wasmer_engine_universal::Universal;

// this example is a work in progress:
// TODO: clean it up and comment it https://github.com/wasmerio/wasmer/issues/1749

fn main() -> anyhow::Result<()> {
    // Let's declare the Wasm module.
    //
    // We are using the text representation of the module here but you can also load `.wasm`
    // files using the `include_bytes!` macro.
    let wasm_bytes = wat2wasm(
        r#"
(module
  (type $mem_size_t (func (result i32)))
  (type $get_at_t (func (param i32) (result i32)))
  (type $set_at_t (func (param i32) (param i32)))

  (memory $mem 1)

  (func $get_at (type $get_at_t) (param $idx i32) (result i32)
    (i32.load (local.get $idx)))

  (func $set_at (type $set_at_t) (param $idx i32) (param $val i32)
    (i32.store (local.get $idx) (local.get $val)))

  (func $mem_size (type $mem_size_t) (result i32)
    (memory.size))

  (export "get_at" (func $get_at))
  (export "set_at" (func $set_at))
  (export "mem_size" (func $mem_size))
  (export "memory" (memory $mem)))
"#
        .as_bytes(),
    )?;

    // Create a Store.
    // Note that we don't need to specify the engine/compiler if we want to use
    // the default provided by Wasmer.
    // You can use `Store::default()` for that.
    let store = Store::new(&Universal::new(Cranelift::default()).engine());

    println!("Compiling module...");
    // Let's compile the Wasm module.
    let module = Module::new(&store, wasm_bytes)?;

    // Create an empty import object.
    let import_object = imports! {};

    println!("Instantiating module...");
    // Let's instantiate the Wasm module.
    let instance = Instance::new(&module, &import_object)?;

    // The module exports some utility functions, let's get them.
    //
    // These function will be used later in this example.
    let mem_size: NativeFunc<(), i32> = instance.exports.get_native_function("mem_size")?;
    let get_at: NativeFunc<i32, i32> = instance.exports.get_native_function("get_at")?;
    let set_at: NativeFunc<(i32, i32), ()> = instance.exports.get_native_function("set_at")?;
    let memory = instance.exports.get_memory("memory")?;

    // We now have an instance ready to be used.
    //
    // We will start by querying the most intersting information
    // about the memory: its size. There are mainly two ways of getting
    // this:
    // * the size as a number of `Page`s
    // * the size as a number of bytes
    //
    // The size in bytes can be found either by querying its pages or by
    // querying the memory directly.
    println!("Querying memory size...");
    assert_eq!(memory.size(), Pages::from(1));
    assert_eq!(memory.size().bytes(), Bytes::from(65536 as usize));
    assert_eq!(memory.data_size(), 65536);

    // Sometimes, the guest module may also export a function to let you
    // query the memory. Here we have a `mem_size` function, let's try it:
    let result = mem_size.call()?;
    println!("Memory size: {:?}", result);
    assert_eq!(Pages::from(result as u32), memory.size());

    // Now that we know the size of our memory, it's time to see how wa
    // can change this.
    //
    // A memory can be grown to allow storing more things into it. Let's
    // see how we can do that:
    println!("Growing memory...");
    // Here we are requesting two more pages for our memory.
    memory.grow(2)?;
    assert_eq!(memory.size(), Pages::from(3));
    assert_eq!(memory.data_size(), 65536 * 3);

    // Now that we know how to query and adjust the size of the memory,
    // let's see how wa can write to it or read from it.
    //
    // We'll only focus on how to do this using exported functions, the goal
    // is to show how to work with memory addresses. Here we'll use absolute
    // addresses to write and read a value.
    let mem_addr = 0x2220;
    let val = 0xFEFEFFE;
    set_at.call(mem_addr, val)?;

    let result = get_at.call(mem_addr)?;
    println!("Value at {:#x?}: {:?}", mem_addr, result);
    assert_eq!(result, val);

    // Now instead of using hard coded memory addresses, let's try to write
    // something at the end of the second memory page and read it.
    let page_size = 0x1_0000;
    let mem_addr = (page_size * 2) - mem::size_of_val(&val) as i32;
    let val = 0xFEA09;
    set_at.call(mem_addr, val)?;

    let result = get_at.call(mem_addr)?;
    println!("Value at {:#x?}: {:?}", mem_addr, result);
    assert_eq!(result, val);

    Ok(())
}

'''
'''--- examples/metering.rs ---
//! Wasmer will let you easily run Wasm module in a Rust host.
//!
//! This example illustrates the basics of using Wasmer metering features:
//!
//!   1. How to enable metering in a module
//!   2. How to meter a specific function call
//!   3. How to make execution fails if cost exceeds a given limit
//!
//! You can run the example directly by executing in Wasmer root:
//!
//! ```shell
//! cargo run --example metering --release --features "cranelift"
//! ```
//!
//! Ready?

use anyhow::bail;
use std::sync::Arc;
use wasmer::wasmparser::Operator;
use wasmer::CompilerConfig;
use wasmer::{imports, wat2wasm, Instance, Module, Store};
use wasmer_compiler_cranelift::Cranelift;
use wasmer_engine_universal::Universal;
use wasmer_middlewares::{
    metering::{get_remaining_points, set_remaining_points, MeteringPoints},
    Metering,
};

fn main() -> anyhow::Result<()> {
    // Let's declare the Wasm module.
    //
    // We are using the text representation of the module here but you can also load `.wasm`
    // files using the `include_bytes!` macro.
    let wasm_bytes = wat2wasm(
        br#"
(module
  (type $add_t (func (param i32) (result i32)))
  (func $add_one_f (type $add_t) (param $value i32) (result i32)
    local.get $value
    i32.const 1
    i32.add)
  (export "add_one" (func $add_one_f)))
"#,
    )?;

    // Let's define our cost function.
    //
    // This function will be called for each `Operator` encountered during
    // the Wasm module execution. It should return the cost of the operator
    // that it received as it first argument.
    let cost_function = |operator: &Operator| -> u64 {
        match operator {
            Operator::LocalGet { .. } | Operator::I32Const { .. } => 1,
            Operator::I32Add { .. } => 2,
            _ => 0,
        }
    };

    // Now let's create our metering middleware.
    //
    // `Metering` needs to be configured with a limit and a cost function.
    //
    // For each `Operator`, the metering middleware will call the cost
    // function and subtract the cost from the remaining points.
    let metering = Arc::new(Metering::new(10, cost_function));
    let mut compiler_config = Cranelift::default();
    compiler_config.push_middleware(metering);

    // Create a Store.
    //
    // We use our previously create compiler configuration
    // with the Universal engine.
    let store = Store::new(&Universal::new(compiler_config).engine());

    println!("Compiling module...");
    // Let's compile the Wasm module.
    let module = Module::new(&store, wasm_bytes)?;

    // Create an empty import object.
    let import_object = imports! {};

    println!("Instantiating module...");
    // Let's instantiate the Wasm module.
    let instance = Instance::new(&module, &import_object)?;

    // We now have an instance ready to be used.
    //
    // Our module exports a single `add_one`  function. We want to
    // measure the cost of executing this function.
    let add_one = instance
        .exports
        .get_function("add_one")?
        .native::<i32, i32>()?;

    println!("Calling `add_one` function once...");
    add_one.call(1)?;

    // As you can see here, after the first call we have 6 remaining points.
    //
    // This is correct, here are the details of how it has been computed:
    // * `local.get $value` is a `Operator::LocalGet` which costs 1 point;
    // * `i32.const` is a `Operator::I32Const` which costs 1 point;
    // * `i32.add` is a `Operator::I32Add` which costs 2 points.
    let remaining_points_after_first_call = get_remaining_points(&instance);
    assert_eq!(
        remaining_points_after_first_call,
        MeteringPoints::Remaining(6)
    );

    println!(
        "Remaining points after the first call: {:?}",
        remaining_points_after_first_call
    );

    println!("Calling `add_one` function twice...");
    add_one.call(1)?;

    // We spent 4 more points with the second call.
    // We have 2 remaining points.
    let remaining_points_after_second_call = get_remaining_points(&instance);
    assert_eq!(
        remaining_points_after_second_call,
        MeteringPoints::Remaining(2)
    );

    println!(
        "Remaining points after the second call: {:?}",
        remaining_points_after_second_call
    );

    // Because calling our `add_one` function consumes 4 points,
    // calling it a third time will fail: we already consume 8
    // points, there are only two remaining.
    println!("Calling `add_one` function a third time...");
    match add_one.call(1) {
        Ok(result) => {
            bail!(
                "Expected failure while calling `add_one`, found: {}",
                result
            );
        }
        Err(_) => {
            println!("Calling `add_one` failed.");

            // Because the last needed more than the remaining points, we should have an error.
            let remaining_points = get_remaining_points(&instance);

            match remaining_points {
                MeteringPoints::Remaining(..) => {
                    bail!("No metering error: there are remaining points")
                }
                MeteringPoints::Exhausted => println!("Not enough points remaining"),
            }
        }
    }

    // Now let's see how we can set a new limit...
    println!("Set new remaining points to 10");
    let new_limit = 10;
    set_remaining_points(&instance, new_limit);

    let remaining_points = get_remaining_points(&instance);
    assert_eq!(remaining_points, MeteringPoints::Remaining(new_limit));

    println!("Remaining points: {:?}", remaining_points);

    Ok(())
}

#[test]
fn test_metering() -> anyhow::Result<()> {
    main()
}

'''
'''--- examples/platform_ios_headless.rs ---
//! Defining an engine in Wasmer is one of the fundamental steps.
//!
//! This example builds on that of 'engine_headless.rs' but instead of
//! serializing a module and then deserializing it again for your host machines target,
//! We instead create an engine for our target architecture (In this case an ARM64 iOS device),
//! serialize a simple module to a .dylib file that can be copied to an iOS project and
//! deserialized/ran using the 'Headless C-API'.
//!
//! ```shell
//! cargo run --example platform-headless-ios --release --features "cranelift"
//! ```
//!
//! Ready?

use std::path::Path;
use std::str::FromStr;
use wasmer::{wat2wasm, Module, RuntimeError, Store};
use wasmer_compiler::{CpuFeature, Target, Triple};
use wasmer_compiler_cranelift::Cranelift;
use wasmer_engine_dylib::Dylib;

fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Let's declare the Wasm module with the text representation.
    let wasm_bytes = wat2wasm(
        r#"
(module
(type $sum_t (func (param i32 i32) (result i32)))
(func $sum_f (type $sum_t) (param $x i32) (param $y i32) (result i32)
local.get $x
local.get $y
i32.add)
(export "sum" (func $sum_f)))
"#
        .as_bytes(),
    )?;

    // Create a compiler for iOS
    let compiler_config = Cranelift::default();
    // Change it to `x86_64-apple-ios` if you want to target the iOS simulator
    let triple = Triple::from_str("aarch64-apple-ios")
        .map_err(|error| RuntimeError::new(error.to_string()))?;

    // Let's build the target.
    let mut cpu_feature = CpuFeature::set();
    cpu_feature.insert(CpuFeature::from_str("sse2")?);
    let target = Target::new(triple, cpu_feature);
    println!("Chosen target: {:?}", target);

    println!("Creating Dylib engine...");
    let engine = Dylib::new(compiler_config).target(target).engine();

    // Create a store, that holds the engine.
    let store = Store::new(&engine);

    println!("Compiling module...");
    // Let's compile the Wasm module.
    let module = Module::new(&store, wasm_bytes)?;
    // Here we go. Let's serialize the compiled Wasm module in a
    // file.
    println!("Serializing module...");
    let dylib_file = Path::new("./sum.dylib");
    module.serialize_to_file(dylib_file)?;

    Ok(())
}

#[test]
#[cfg(target_os = "macos")]
fn test_engine_headless_ios() -> Result<(), Box<dyn std::error::Error>> {
    main()
}

'''
'''--- examples/table.rs ---
use wasmer::{
    imports, wat2wasm, Function, Instance, Module, NativeFunc, Store, TableType, Type, Value,
};
use wasmer_compiler_cranelift::Cranelift;
use wasmer_engine_universal::Universal;

/// A function we'll call through a table.
fn host_callback(arg1: i32, arg2: i32) -> i32 {
    arg1 + arg2
}

fn main() -> anyhow::Result<()> {
    let wasm_bytes = wat2wasm(
        r#"
(module
  ;; All our callbacks will take 2 i32s and return an i32.
  ;; Wasm tables are not limited to 1 type of function, but the code using the
  ;; table must have code to handle the type it finds.
  (type $callback_t (func (param i32 i32) (result i32)))

  ;; We'll call a callback by passing a table index as an i32 and then the two
  ;; arguments that the function expects.
  (type $call_callback_t (func (param i32 i32 i32) (result i32)))

  ;; Our table of functions that's exactly size 3 (min 3, max 3).
  (table $t1 3 6 funcref)

  ;; Call the function at the given index with the two supplied arguments.
  (func $call_callback (type $call_callback_t) (param $idx i32)
                                               (param $arg1 i32) (param $arg2 i32)
                                               (result i32)
    (call_indirect (type $callback_t) 
                   (local.get $arg1) (local.get $arg2)
                   (local.get $idx)))

  ;; A default function that we'll pad the table with.
  ;; This function doubles both its inputs and then sums them.
  (func $default_fn (type $callback_t) (param $a i32) (param $b i32) (result i32)
     (i32.add 
       (i32.mul (local.get $a) (i32.const 2))
       (i32.mul (local.get $b) (i32.const 2))))

  ;; Fill our table with the default function.
  (elem $t1 (i32.const 0) $default_fn $default_fn $default_fn)

  ;; Export things for the host to call.
  (export "call_callback" (func $call_callback))
  (export "__indirect_function_table" (table $t1)))
"#
        .as_bytes(),
    )?;

    // We set up our store with an engine and a compiler.
    let store = Store::new(&Universal::new(Cranelift::default()).engine());
    // Then compile our Wasm.
    let module = Module::new(&store, wasm_bytes)?;
    let import_object = imports! {};
    // And instantiate it with no imports.
    let instance = Instance::new(&module, &import_object)?;

    // We get our function that calls (i32, i32) -> i32 functions via table.
    // The first argument is the table index and the next 2 are the 2 arguments
    // to be passed to the function found in the table.
    let call_via_table: NativeFunc<(i32, i32, i32), i32> =
        instance.exports.get_native_function("call_callback")?;

    // And then call it with table index 1 and arguments 2 and 7.
    let result = call_via_table.call(1, 2, 7)?;
    // Because it's the default function, we expect it to double each number and
    // then sum it, giving us 18.
    assert_eq!(result, 18);

    // We then get the table from the instance.
    let guest_table = instance.exports.get_table("__indirect_function_table")?;
    // And demonstrate that it has the properties that we set in the Wasm.
    assert_eq!(guest_table.size(), 3);
    assert_eq!(
        guest_table.ty(),
        &TableType {
            ty: Type::FuncRef,
            minimum: 3,
            maximum: Some(6)
        }
    );

    // == Setting elements in a table ==

    // We first construct a `Function` over our host_callback.
    let func = Function::new_native(&store, host_callback);

    // And set table index 1 of that table to the host_callback `Function`.
    guest_table.set(1, func.into())?;

    // We then repeat the call from before but this time it will find the host function
    // that we put at table index 1.
    let result = call_via_table.call(1, 2, 7)?;
    // And because our host function simply sums the numbers, we expect 9.
    assert_eq!(result, 9);

    // == Growing a table ==

    // We again construct a `Function` over our host_callback.
    let func = Function::new_native(&store, host_callback);

    // And grow the table by 3 elements, filling in our host_callback in all the
    // new elements of the table.
    let previous_size = guest_table.grow(3, func.into())?;
    assert_eq!(previous_size, 3);

    assert_eq!(guest_table.size(), 6);
    assert_eq!(
        guest_table.ty(),
        &TableType {
            ty: Type::FuncRef,
            minimum: 3,
            maximum: Some(6)
        }
    );
    // Now demonstrate that the function we grew the table with is actually in the table.
    for table_index in 3..6 {
        if let Value::FuncRef(Some(f)) = guest_table.get(table_index as _).unwrap() {
            let result = f.call(&[Value::I32(1), Value::I32(9)])?;
            assert_eq!(result[0], Value::I32(10));
        } else {
            panic!("expected to find funcref in table!");
        }
    }

    // Call function at index 0 to show that it's still the same.
    let result = call_via_table.call(0, 2, 7)?;
    assert_eq!(result, 18);

    // Now overwrite index 0 with our host_callback.
    let func = Function::new_native(&store, host_callback);
    guest_table.set(0, func.into())?;
    // And verify that it does what we expect.
    let result = call_via_table.call(0, 2, 7)?;
    assert_eq!(result, 9);

    // Now demonstrate that the host and guest see the same table and that both
    // get the same result.
    for table_index in 3..6 {
        if let Value::FuncRef(Some(f)) = guest_table.get(table_index as _).unwrap() {
            let result = f.call(&[Value::I32(1), Value::I32(9)])?;
            assert_eq!(result[0], Value::I32(10));
        } else {
            panic!("expected to find funcref in table!");
        }
        let result = call_via_table.call(table_index, 1, 9)?;
        assert_eq!(result, 10);
    }

    Ok(())
}

'''
'''--- examples/tunables_limit_memory.rs ---
use std::ptr::NonNull;
use std::sync::Arc;

use wasmer::{
    imports,
    vm::{self, MemoryError, MemoryStyle, TableStyle, VMMemoryDefinition, VMTableDefinition},
    wat2wasm, BaseTunables, Instance, Memory, MemoryType, Module, Pages, Store, TableType, Target,
    Tunables,
};
use wasmer_compiler_cranelift::Cranelift;
use wasmer_engine_universal::Universal;

/// A custom tunables that allows you to set a memory limit.
///
/// After adjusting the memory limits, it delegates all other logic
/// to the base tunables.
pub struct LimitingTunables<T: Tunables> {
    /// The maximum a linear memory is allowed to be (in Wasm pages, 64 KiB each).
    /// Since Wasmer ensures there is only none or one memory, this is practically
    /// an upper limit for the guest memory.
    limit: Pages,
    /// The base implementation we delegate all the logic to
    base: T,
}

impl<T: Tunables> LimitingTunables<T> {
    pub fn new(base: T, limit: Pages) -> Self {
        Self { limit, base }
    }

    /// Takes an input memory type as requested by the guest and sets
    /// a maximum if missing. The resulting memory type is final if
    /// valid. However, this can produce invalid types, such that
    /// validate_memory must be called before creating the memory.
    fn adjust_memory(&self, requested: &MemoryType) -> MemoryType {
        let mut adjusted = requested.clone();
        if requested.maximum.is_none() {
            adjusted.maximum = Some(self.limit);
        }
        adjusted
    }

    /// Ensures the a given memory type does not exceed the memory limit.
    /// Call this after adjusting the memory.
    fn validate_memory(&self, ty: &MemoryType) -> Result<(), MemoryError> {
        if ty.minimum > self.limit {
            return Err(MemoryError::Generic(
                "Minimum exceeds the allowed memory limit".to_string(),
            ));
        }

        if let Some(max) = ty.maximum {
            if max > self.limit {
                return Err(MemoryError::Generic(
                    "Maximum exceeds the allowed memory limit".to_string(),
                ));
            }
        } else {
            return Err(MemoryError::Generic("Maximum unset".to_string()));
        }

        Ok(())
    }
}

impl<T: Tunables> Tunables for LimitingTunables<T> {
    /// Construct a `MemoryStyle` for the provided `MemoryType`
    ///
    /// Delegated to base.
    fn memory_style(&self, memory: &MemoryType) -> MemoryStyle {
        let adjusted = self.adjust_memory(memory);
        self.base.memory_style(&adjusted)
    }

    /// Construct a `TableStyle` for the provided `TableType`
    ///
    /// Delegated to base.
    fn table_style(&self, table: &TableType) -> TableStyle {
        self.base.table_style(table)
    }

    /// Create a memory owned by the host given a [`MemoryType`] and a [`MemoryStyle`].
    ///
    /// The requested memory type is validated, adjusted to the limited and then passed to base.
    fn create_host_memory(
        &self,
        ty: &MemoryType,
        style: &MemoryStyle,
    ) -> Result<Arc<dyn vm::Memory>, MemoryError> {
        let adjusted = self.adjust_memory(ty);
        self.validate_memory(&adjusted)?;
        self.base.create_host_memory(&adjusted, style)
    }

    /// Create a memory owned by the VM given a [`MemoryType`] and a [`MemoryStyle`].
    ///
    /// Delegated to base.
    unsafe fn create_vm_memory(
        &self,
        ty: &MemoryType,
        style: &MemoryStyle,
        vm_definition_location: NonNull<VMMemoryDefinition>,
    ) -> Result<Arc<dyn vm::Memory>, MemoryError> {
        let adjusted = self.adjust_memory(ty);
        self.validate_memory(&adjusted)?;
        self.base
            .create_vm_memory(&adjusted, style, vm_definition_location)
    }

    /// Create a table owned by the host given a [`TableType`] and a [`TableStyle`].
    ///
    /// Delegated to base.
    fn create_host_table(
        &self,
        ty: &TableType,
        style: &TableStyle,
    ) -> Result<Arc<dyn vm::Table>, String> {
        self.base.create_host_table(ty, style)
    }

    /// Create a table owned by the VM given a [`TableType`] and a [`TableStyle`].
    ///
    /// Delegated to base.
    unsafe fn create_vm_table(
        &self,
        ty: &TableType,
        style: &TableStyle,
        vm_definition_location: NonNull<VMTableDefinition>,
    ) -> Result<Arc<dyn vm::Table>, String> {
        self.base.create_vm_table(ty, style, vm_definition_location)
    }
}

fn main() -> Result<(), Box<dyn std::error::Error>> {
    // A Wasm module with one exported memory (min: 7 pages, max: unset)
    let wat = br#"(module (memory 7) (export "memory" (memory 0)))"#;

    // Alternatively: A Wasm module with one exported memory (min: 7 pages, max: 80 pages)
    // let wat = br#"(module (memory 7 80) (export "memory" (memory 0)))"#;

    let wasm_bytes = wat2wasm(wat)?;

    // Any compiler and any engine do the job here
    let compiler = Cranelift::default();
    let engine = Universal::new(compiler).engine();

    // Here is where the fun begins

    let base = BaseTunables::for_target(&Target::default());
    let tunables = LimitingTunables::new(base, Pages(24));

    // Create a store, that holds the engine and our custom tunables
    let store = Store::new_with_tunables(&engine, tunables);

    println!("Compiling module...");
    let module = Module::new(&store, wasm_bytes)?;

    println!("Instantiating module...");
    let import_object = imports! {};

    // Now at this point, our custom tunables are used
    let instance = Instance::new(&module, &import_object)?;

    // Check what happened
    let mut memories: Vec<Memory> = instance
        .exports
        .iter()
        .memories()
        .map(|pair| pair.1.clone())
        .collect();
    assert_eq!(memories.len(), 1);

    let first_memory = memories.pop().unwrap();
    println!("Memory of this instance: {:?}", first_memory);
    assert_eq!(first_memory.ty().maximum.unwrap(), Pages(24));

    Ok(())
}

#[test]
fn test_tunables_limit_memory() -> Result<(), Box<dyn std::error::Error>> {
    main()
}

'''
'''--- fuzz/Cargo.toml ---
[package]
name = "wasmer-bin-fuzz"
version = "0.0.0"
authors = ["Automatically generated"]
publish = false
edition = "2018"

[package.metadata]
cargo-fuzz = true

[dependencies]
anyhow = "1"
wasm-smith = "0.4.4"
libfuzzer-sys = "0.4.0"
wasmer = { path = "../lib/api", package = "wasmer-near" }
wasmer-compiler-cranelift = { path = "../lib/compiler-cranelift", optional = true }
wasmer-compiler-llvm = { path = "../lib/compiler-llvm", optional = true }
wasmer-compiler-singlepass = { path = "../lib/compiler-singlepass", package = "wasmer-compiler-singlepass-near", optional = true }
wasmer-engine-universal = { path = "../lib/engine-universal", package = "wasmer-engine-universal-near", optional = true }
wasmprinter = "0.2"

[features]
cranelift = [ "wasmer-compiler-cranelift" ]
llvm = [ "wasmer-compiler-llvm" ]
singlepass = [ "wasmer-compiler-singlepass" ]
universal = [ "wasmer-engine-universal" ]

[[bin]]
name = "equivalence_universal"
path = "fuzz_targets/equivalence_universal.rs"
required-features = ["universal"]

[[bin]]
name = "universal_cranelift"
path = "fuzz_targets/universal_cranelift.rs"
required-features = ["universal", "cranelift"]

[[bin]]
name = "universal_llvm"
path = "fuzz_targets/universal_llvm.rs"
required-features = ["universal", "llvm"]

[[bin]]
name = "universal_singlepass"
path = "fuzz_targets/universal_singlepass.rs"
required-features = ["universal", "singlepass"]

[[bin]]
name = "metering"
path = "fuzz_targets/metering.rs"
required-features = ["universal", "cranelift"]

[[bin]]
name = "deterministic"
path = "fuzz_targets/deterministic.rs"
required-features = ["universal", "cranelift", "llvm", "singlepass"]

'''
'''--- fuzz/README.md ---
# Wasmer Fuzz Testing

[Fuzz testing](https://en.wikipedia.org/wiki/Fuzzing) is:

> An automated testing technique that involves providing invalid,
> unexpected, or random data as inputs to a program.

We use fuzz testing to automatically discover bugs in the Wasmer runtime.

This `fuzz/` directory contains the configuration and the fuzz tests
for Wasmer. To generate and to run the fuzz tests, we use the
[`cargo-fuzz`] library.

## Installation

You may need to install the [`cargo-fuzz`] library to get the `cargo
fuzz` subcommand. Use

```sh
$ cargo install cargo-fuzz
```

`cargo-fuzz` is documented in the [Rust Fuzz
Book](https://rust-fuzz.github.io/book/cargo-fuzz.html).

## Running a fuzzer

This directory provides multiple fuzzers, like for example `validate`. You can run it with:

```sh
$ cargo fuzz run validate
```

Another example with the `universal_cranelift` fuzzer:

```sh
$ cargo fuzz run universal_cranelift
```

See the
[`fuzz/fuzz_targets`](https://github.com/wasmerio/wasmer/tree/fuzz/fuzz_targets/)
directory for the full list of fuzzers.

You should see output that looks something like this:

```
#1408022        NEW    cov: 115073 ft: 503843 corp: 4659/1807Kb lim: 4096 exec/s: 889 rss: 857Mb L: 2588/4096 MS: 1 ChangeASCIIInt-
#1408273        NEW    cov: 115073 ft: 503844 corp: 4660/1808Kb lim: 4096 exec/s: 888 rss: 857Mb L: 1197/4096 MS: 1 ShuffleBytes-
#1408534        NEW    cov: 115073 ft: 503866 corp: 4661/1809Kb lim: 4096 exec/s: 886 rss: 857Mb L: 977/4096 MS: 1 ShuffleBytes-
#1408540        NEW    cov: 115073 ft: 503869 corp: 4662/1811Kb lim: 4096 exec/s: 886 rss: 857Mb L: 2067/4096 MS: 1 ChangeBit-
#1408831        NEW    cov: 115073 ft: 503945 corp: 4663/1811Kb lim: 4096 exec/s: 885 rss: 857Mb L: 460/4096 MS: 1 CMP- DE: "\x16\x00\x00\x00\x00\x00\x00\x00"-
#1408977        NEW    cov: 115073 ft: 503946 corp: 4664/1813Kb lim: 4096 exec/s: 885 rss: 857Mb L: 1972/4096 MS: 1 ShuffleBytes-
#1408999        NEW    cov: 115073 ft: 503949 corp: 4665/1814Kb lim: 4096 exec/s: 884 rss: 857Mb L: 964/4096 MS: 2 ChangeBit-ShuffleBytes-
#1409040        NEW    cov: 115073 ft: 503950 corp: 4666/1814Kb lim: 4096 exec/s: 884 rss: 857Mb L: 90/4096 MS: 1 ChangeBit-
#1409042        NEW    cov: 115073 ft: 503951 corp: 4667/1814Kb lim: 4096 exec/s: 884 rss: 857Mb L: 174/4096 MS: 2 ChangeByte-ChangeASCIIInt-
```

It will continue to generate random inputs forever, until it finds a
bug or is terminated. The testcases for bugs it finds go into
`fuzz/artifacts/universal_cranelift` and you can rerun the fuzzer on a
single input by passing it on the command line `cargo fuzz run
universal_cranelift /path/to/testcase`.

## The corpus

Each fuzzer has an individual corpus under `fuzz/corpus/test_name`,
created on first run if not already present. The fuzzers use
`wasm-smith` which means that the testcase files are random number
seeds input to the Wasm generator, not `.wasm` files themselves. In
order to debug a testcase, you may find that you need to convert it
into a `.wasm` file. Using the standalone `wasm-smith` tool doesn't
work for this purpose because we use a custom configuration to our
`wasm_smith::Module`. Instead, our fuzzers use an environment variable
`DUMP_TESTCASE=path`. For example:

```sh
$ DUMP_TESTCASE=/tmp/crash.wasm cargo fuzz run --features=universal,singlepass universal_singlepass fuzz/artifacts/universal_singlepass/crash-0966412eab4f89c52ce5d681807c8030349470f6
```

[`cargo-fuzz`]: https://github.com/rust-fuzz/cargo-fuzz

'''
'''--- fuzz/fuzz_targets/deterministic.rs ---
#![no_main]

use libfuzzer_sys::{arbitrary, arbitrary::Arbitrary, fuzz_target};
use wasm_smith::{Config, ConfiguredModule};
use wasmer::{CompilerConfig, Engine, Module, Store};
use wasmer_compiler_cranelift::Cranelift;
use wasmer_compiler_llvm::LLVM;
use wasmer_compiler_singlepass::Singlepass;
use wasmer_engine_dylib::Dylib;
use wasmer_engine_universal::Universal;

#[derive(Arbitrary, Debug, Default, Copy, Clone)]
struct NoImportsConfig;
impl Config for NoImportsConfig {
    fn max_imports(&self) -> usize {
        0
    }
    fn max_memory_pages(&self) -> u32 {
        // https://github.com/wasmerio/wasmer/issues/2187
        65535
    }
    fn allow_start_export(&self) -> bool {
        false
    }
}

fn compile_and_compare(name: &str, engine: impl Engine, wasm: &[u8]) {
    let store = Store::new(&engine);

    // compile for first time
    let module = Module::new(&store, wasm).unwrap();
    let first = module.serialize().unwrap();

    // compile for second time
    let module = Module::new(&store, wasm).unwrap();
    let second = module.serialize().unwrap();

    if first != second {
        panic!("non-deterministic compilation from {}", name);
    }
}

fuzz_target!(|module: ConfiguredModule<NoImportsConfig>| {
    let wasm_bytes = module.to_bytes();

    let mut compiler = Cranelift::default();
    compiler.canonicalize_nans(true);
    compiler.enable_verifier();
    compile_and_compare(
        "universal-cranelift",
        Universal::new(compiler.clone()).engine(),
        &wasm_bytes,
    );
    //compile_and_compare(
    //    "dylib-cranelift",
    //    Dylib::new(compiler).engine(),
    //    &wasm_bytes,
    //);

    let mut compiler = LLVM::default();
    compiler.canonicalize_nans(true);
    compiler.enable_verifier();
    compile_and_compare(
        "universal-llvm",
        Universal::new(compiler.clone()).engine(),
        &wasm_bytes,
    );
    //compile_and_compare("dylib-llvm", Dylib::new(compiler).engine(), &wasm_bytes);

    let compiler = Singlepass::default();
    compile_and_compare(
        "universal-singlepass",
        Universal::new(compiler.clone()).engine(),
        &wasm_bytes,
    );
    //compile_and_compare(
    //    "dylib-singlepass",
    //    Dylib::new(compiler).engine(),
    //    &wasm_bytes,
    //);
});

'''
'''--- fuzz/fuzz_targets/equivalence_universal.rs ---
#![no_main]
#![deny(unused_variables)]

use anyhow::Result;
use libfuzzer_sys::{arbitrary, arbitrary::Arbitrary, fuzz_target};
use wasm_smith::{Config, ConfiguredModule};
use wasmer::{imports, CompilerConfig, Instance, Module, Store, Val};
#[cfg(feature = "cranelift")]
use wasmer_compiler_cranelift::Cranelift;
#[cfg(feature = "llvm")]
use wasmer_compiler_llvm::LLVM;
#[cfg(feature = "singlepass")]
use wasmer_compiler_singlepass::Singlepass;
use wasmer_engine_universal::Universal;

#[derive(Arbitrary, Debug, Default, Copy, Clone)]
struct ExportedFunctionConfig;
impl Config for ExportedFunctionConfig {
    fn max_imports(&self) -> usize {
        0
    }
    fn max_memory_pages(&self) -> u32 {
        // https://github.com/wasmerio/wasmer/issues/2187
        65535
    }
    fn min_funcs(&self) -> usize {
        1
    }
    fn min_exports(&self) -> usize {
        1
    }
}

struct WasmSmithModule(ConfiguredModule<ExportedFunctionConfig>);
impl<'a> arbitrary::Arbitrary<'a> for WasmSmithModule {
    fn arbitrary(u: &mut arbitrary::Unstructured<'a>) -> arbitrary::Result<Self> {
        let mut module = ConfiguredModule::<ExportedFunctionConfig>::arbitrary(u)?;
        module.ensure_termination(100000);
        Ok(WasmSmithModule(module))
    }
}
impl std::fmt::Debug for WasmSmithModule {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        f.write_str(&wasmprinter::print_bytes(self.0.to_bytes()).unwrap())
    }
}

#[cfg(feature = "singlepass")]
fn maybe_instantiate_singlepass(wasm_bytes: &[u8]) -> Result<Option<Instance>> {
    let compiler = Singlepass::default();
    let store = Store::new(&Universal::new(compiler).engine());
    let module = Module::new(&store, &wasm_bytes);
    let module = match module {
        Ok(m) => m,
        Err(e) => {
            let error_message = format!("{}", e);
            if error_message.contains("Validation error: invalid result arity: func type returns multiple values") || error_message.contains("Validation error: blocks, loops, and ifs accept no parameters when multi-value is not enabled") || error_message.contains("multi-value returns not yet implemented") {
                return Ok(None);
            }
            return Err(e.into());
        }
    };
    let instance = Instance::new(&module, &imports! {})?;
    Ok(Some(instance))
}

#[cfg(feature = "cranelift")]
fn maybe_instantiate_cranelift(wasm_bytes: &[u8]) -> Result<Option<Instance>> {
    let mut compiler = Cranelift::default();
    compiler.canonicalize_nans(true);
    compiler.enable_verifier();
    let store = Store::new(&Universal::new(compiler).engine());
    let module = Module::new(&store, &wasm_bytes)?;
    let instance = Instance::new(&module, &imports! {})?;
    Ok(Some(instance))
}

#[cfg(feature = "llvm")]
fn maybe_instantiate_llvm(wasm_bytes: &[u8]) -> Result<Option<Instance>> {
    let mut compiler = LLVM::default();
    compiler.canonicalize_nans(true);
    compiler.enable_verifier();
    let store = Store::new(&Universal::new(compiler).engine());
    let module = Module::new(&store, &wasm_bytes)?;
    let instance = Instance::new(&module, &imports! {})?;
    Ok(Some(instance))
}

#[derive(Debug)]
enum FunctionResult {
    Error(String),
    Values(Vec<Val>),
}

#[derive(Debug, PartialEq, Eq)]
enum InstanceResult {
    Error(String),
    Functions(Vec<FunctionResult>),
}

impl PartialEq for FunctionResult {
    fn eq(&self, other: &Self) -> bool {
        /*
        match self {
            FunctionResult::Error(self_message) => {
                if let FunctionResult::Error(other_message) = other {
                    return self_message == other_message;
                }
            }
            FunctionResult::Values(self_values) => {
                if let FunctionResult::Values(other_values) = other {
                    return self_values == other_values;
                }
            }
        }
        false
         */
        match (self, other) {
            (FunctionResult::Values(self_values), FunctionResult::Values(other_values)) => {
                self_values.len() == other_values.len()
                    && self_values
                        .iter()
                        .zip(other_values.iter())
                        .all(|(x, y)| match (x, y) {
                            (Val::F32(x), Val::F32(y)) => x.to_bits() == y.to_bits(),
                            (Val::F64(x), Val::F64(y)) => x.to_bits() == y.to_bits(),
                            _ => x == y,
                        })
            }
            _ => true,
        }
    }
}

impl Eq for FunctionResult {}

fn evaluate_instance(instance: Result<Instance>) -> InstanceResult {
    if let Err(_err) = instance {
        /*let mut error_message = format!("{}", err);
        // Remove the stack trace.
        if error_message.starts_with("RuntimeError: unreachable\n") {
            error_message = "RuntimeError: unreachable\n".into();
        }
        InstanceResult::Error(error_message)*/
        InstanceResult::Error("".into())
    } else {
        let instance = instance.unwrap();
        let mut results = vec![];
        for it in instance.exports.iter().functions() {
            let (_, f) = it;
            // TODO: support functions which take params.
            if f.ty().params().is_empty() {
                let result = f.call(&[]);
                let result = if let Ok(values) = result {
                    FunctionResult::Values(values.into())
                } else {
                    /*
                    let err = result.unwrap_err();
                    let error_message = err.message();
                    FunctionResult::Error(error_message)
                     */
                    FunctionResult::Error("".into())
                };
                results.push(result);
            }
        }
        InstanceResult::Functions(results)
    }
}

fuzz_target!(|module: WasmSmithModule| {
    let wasm_bytes = module.0.to_bytes();

    if let Ok(path) = std::env::var("DUMP_TESTCASE") {
        use std::fs::File;
        use std::io::Write;
        let mut file = File::create(path).unwrap();
        file.write_all(&wasm_bytes).unwrap();
        return;
    }

    #[cfg(feature = "singlepass")]
    let singlepass = maybe_instantiate_singlepass(&wasm_bytes)
        .transpose()
        .map(evaluate_instance);
    #[cfg(feature = "cranelift")]
    let cranelift = maybe_instantiate_cranelift(&wasm_bytes)
        .transpose()
        .map(evaluate_instance);
    #[cfg(feature = "llvm")]
    let llvm = maybe_instantiate_llvm(&wasm_bytes)
        .transpose()
        .map(evaluate_instance);

    #[cfg(all(feature = "singlepass", feature = "cranelift"))]
    if singlepass.is_some() && cranelift.is_some() {
        assert_eq!(singlepass.as_ref().unwrap(), cranelift.as_ref().unwrap());
    }
    #[cfg(all(feature = "singlepass", feature = "llvm"))]
    if singlepass.is_some() && llvm.is_some() {
        assert_eq!(singlepass.as_ref().unwrap(), llvm.as_ref().unwrap());
    }
    #[cfg(all(feature = "cranelift", feature = "llvm"))]
    if cranelift.is_some() && llvm.is_some() {
        assert_eq!(cranelift.as_ref().unwrap(), llvm.as_ref().unwrap());
    }
});

'''
'''--- fuzz/fuzz_targets/metering.rs ---
#![no_main]

use libfuzzer_sys::{arbitrary, arbitrary::Arbitrary, fuzz_target};
use std::sync::Arc;
use wasm_smith::{Config, ConfiguredModule};
use wasmer::wasmparser::Operator;
use wasmer::{imports, CompilerConfig, Instance, Module, Store};
use wasmer_compiler_cranelift::Cranelift;
use wasmer_engine_universal::Universal;
use wasmer_middlewares::Metering;

#[derive(Arbitrary, Debug, Default, Copy, Clone)]
struct NoImportsConfig;
impl Config for NoImportsConfig {
    fn max_imports(&self) -> usize {
        0
    }
    fn max_memory_pages(&self) -> u32 {
        // https://github.com/wasmerio/wasmer/issues/2187
        65535
    }
    fn allow_start_export(&self) -> bool {
        false
    }
}

#[derive(Arbitrary)]
struct WasmSmithModule(ConfiguredModule<NoImportsConfig>);
impl std::fmt::Debug for WasmSmithModule {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        f.write_str(&wasmprinter::print_bytes(self.0.to_bytes()).unwrap())
    }
}

fn cost(operator: &Operator) -> u64 {
    match operator {
        Operator::LocalGet { .. } | Operator::I32Const { .. } => 1,
        Operator::I32Add { .. } => 2,
        _ => 0,
    }
}

fuzz_target!(|module: WasmSmithModule| {
    let wasm_bytes = module.0.to_bytes();

    if let Ok(path) = std::env::var("DUMP_TESTCASE") {
        use std::fs::File;
        use std::io::Write;
        let mut file = File::create(path).unwrap();
        file.write_all(&wasm_bytes).unwrap();
        return;
    }

    let mut compiler = Cranelift::default();
    compiler.canonicalize_nans(true);
    compiler.enable_verifier();
    let metering = Arc::new(Metering::new(10, cost));
    compiler.push_middleware(metering);
    let store = Store::new(&Universal::new(compiler).engine());
    let module = Module::new(&store, &wasm_bytes).unwrap();
    match Instance::new(&module, &imports! {}) {
        Ok(_) => {}
        Err(e) => {
            let error_message = format!("{}", e);
            if error_message.starts_with("RuntimeError: ")
                && error_message.contains("out of bounds")
            {
                return;
            }
            panic!("{}", e);
        }
    }
});

'''
'''--- fuzz/fuzz_targets/universal_cranelift.rs ---
#![no_main]

use libfuzzer_sys::{arbitrary, arbitrary::Arbitrary, fuzz_target};
use wasm_smith::{Config, ConfiguredModule};
use wasmer::{imports, CompilerConfig, Instance, Module, Store};
use wasmer_compiler_cranelift::Cranelift;
use wasmer_engine_universal::Universal;

#[derive(Arbitrary, Debug, Default, Copy, Clone)]
struct NoImportsConfig;
impl Config for NoImportsConfig {
    fn max_imports(&self) -> usize {
        0
    }
    fn max_memory_pages(&self) -> u32 {
        // https://github.com/wasmerio/wasmer/issues/2187
        65535
    }
    fn allow_start_export(&self) -> bool {
        false
    }
}
#[derive(Arbitrary)]
struct WasmSmithModule(ConfiguredModule<NoImportsConfig>);
impl std::fmt::Debug for WasmSmithModule {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        f.write_str(&wasmprinter::print_bytes(self.0.to_bytes()).unwrap())
    }
}

fuzz_target!(|module: WasmSmithModule| {
    let wasm_bytes = module.0.to_bytes();

    if let Ok(path) = std::env::var("DUMP_TESTCASE") {
        use std::fs::File;
        use std::io::Write;
        let mut file = File::create(path).unwrap();
        file.write_all(&wasm_bytes).unwrap();
        return;
    }

    let mut compiler = Cranelift::default();
    compiler.canonicalize_nans(true);
    compiler.enable_verifier();
    let store = Store::new(&Universal::new(compiler).engine());
    let module = Module::new(&store, &wasm_bytes).unwrap();
    match Instance::new(&module, &imports! {}) {
        Ok(_) => {}
        Err(e) => {
            let error_message = format!("{}", e);
            if error_message.starts_with("RuntimeError: ")
                && error_message.contains("out of bounds")
            {
                return;
            }
            panic!("{}", e);
        }
    }
});

'''
'''--- fuzz/fuzz_targets/universal_llvm.rs ---
#![no_main]

use libfuzzer_sys::{arbitrary, arbitrary::Arbitrary, fuzz_target};
use wasm_smith::{Config, ConfiguredModule};
use wasmer::{imports, CompilerConfig, Instance, Module, Store};
use wasmer_compiler_llvm::LLVM;
use wasmer_engine_universal::Universal;

#[derive(Arbitrary, Debug, Default, Copy, Clone)]
struct NoImportsConfig;
impl Config for NoImportsConfig {
    fn max_imports(&self) -> usize {
        0
    }
    fn max_memory_pages(&self) -> u32 {
        // https://github.com/wasmerio/wasmer/issues/2187
        65535
    }
    fn allow_start_export(&self) -> bool {
        false
    }
}
#[derive(Arbitrary)]
struct WasmSmithModule(ConfiguredModule<NoImportsConfig>);
impl std::fmt::Debug for WasmSmithModule {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        f.write_str(&wasmprinter::print_bytes(self.0.to_bytes()).unwrap())
    }
}

fuzz_target!(|module: WasmSmithModule| {
    let wasm_bytes = module.0.to_bytes();

    if let Ok(path) = std::env::var("DUMP_TESTCASE") {
        use std::fs::File;
        use std::io::Write;
        let mut file = File::create(path).unwrap();
        file.write_all(&wasm_bytes).unwrap();
        return;
    }

    let mut compiler = LLVM::default();
    compiler.canonicalize_nans(true);
    compiler.enable_verifier();
    let store = Store::new(&Universal::new(compiler).engine());
    let module = Module::new(&store, &wasm_bytes).unwrap();
    match Instance::new(&module, &imports! {}) {
        Ok(_) => {}
        Err(e) => {
            let error_message = format!("{}", e);
            if error_message.starts_with("RuntimeError: ")
                && error_message.contains("out of bounds")
            {
                return;
            }
            panic!("{}", e);
        }
    }
});

'''
'''--- fuzz/fuzz_targets/universal_singlepass.rs ---
#![no_main]

use libfuzzer_sys::{arbitrary, arbitrary::Arbitrary, fuzz_target};
use wasm_smith::{Config, ConfiguredModule};
use wasmer::{imports, Instance, Module, Store};
use wasmer_compiler_singlepass::Singlepass;
use wasmer_engine_universal::Universal;

#[derive(Arbitrary, Debug, Default, Copy, Clone)]
struct NoImportsConfig;
impl Config for NoImportsConfig {
    fn max_imports(&self) -> usize {
        0
    }
    fn max_memory_pages(&self) -> u32 {
        // https://github.com/wasmerio/wasmer/issues/2187
        65535
    }
    fn allow_start_export(&self) -> bool {
        false
    }
}
#[derive(Arbitrary)]
struct WasmSmithModule(ConfiguredModule<NoImportsConfig>);
impl std::fmt::Debug for WasmSmithModule {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        f.write_str(&wasmprinter::print_bytes(self.0.to_bytes()).unwrap())
    }
}

fuzz_target!(|module: WasmSmithModule| {
    let wasm_bytes = module.0.to_bytes();

    if let Ok(path) = std::env::var("DUMP_TESTCASE") {
        use std::fs::File;
        use std::io::Write;
        let mut file = File::create(path).unwrap();
        file.write_all(&wasm_bytes).unwrap();
        return;
    }

    let compiler = Singlepass::default();
    let store = Store::new(&Universal::new(compiler).engine());
    let module = Module::new(&store, &wasm_bytes);
    let module = match module {
        Ok(m) => m,
        Err(e) => {
            let error_message = format!("{}", e);
            if error_message.contains("Validation error: invalid result arity: func type returns multiple values") || error_message.contains("Validation error: blocks, loops, and ifs accept no parameters when multi-value is not enabled") || error_message.contains("multi-value returns not yet implemented") {
                return;
            }
            panic!("{}", e);
        }
    };
    match Instance::new(&module, &imports! {}) {
        Ok(_) => {}
        Err(e) => {
            let error_message = format!("{}", e);
            if error_message.starts_with("RuntimeError: ")
                && error_message.contains("out of bounds")
            {
                return;
            }
            panic!("{}", e);
        }
    }
});

'''
'''--- lib/README.md ---
# The Wasmer runtime crates

The philosophy of Wasmer is to be very modular by design. It's
composed of a set of crates. We can group them as follows:

* `api` ‚Äî The public Rust API exposes everything a user needs to use Wasmer programatically through
  the `wasmer` crate,
* `cache` ‚Äî The traits and types to cache compiled WebAssembly modules,
* `cli` ‚Äî The Wasmer CLI itself,
* `compiler` ‚Äî The base for the compiler implementations, it defines
  the framework for the compilers and provides everything they need:
  * `compiler-cranelift` ‚Äî A WebAssembly compiler based on the Cranelift compiler infrastructure,
  * `compiler-llvm` ‚Äî A WebAssembly compiler based on the LLVM compiler infrastructure; recommended
    for runtime speed performance,
  * `compiler-singlepass` ‚Äî A WebAssembly compiler based on our own compilation infrastructure;
    recommended for compilation-time speed performance.
* `derive` ‚Äî A set of procedural macros used inside Wasmer,
* `engine` ‚Äî The general abstraction for creating an engine, which is responsible of leading the
  compiling and running flow. Using the same compiler, the runtime performance will be
  approximately the same, however the way it stores and loads the executable code will differ:
  * `engine-universal` ‚Äî stores the code in a custom file format, and loads it in memory,
* `types` ‚Äî The basic structures to use WebAssembly,
* `vm` ‚Äî The Wasmer VM runtime library, the low-level base of
  everything.

'''
'''--- lib/api/Cargo.toml ---
[package]
name = "wasmer-near"
version = "2.4.1"
description = "High-performance WebAssembly runtime"
categories = ["wasm"]
keywords = ["wasm", "webassembly", "runtime", "vm"]
authors = ["Wasmer Engineering Team <engineering@wasmer.io>"]
repository = "https://github.com/wasmerio/wasmer"
license = "MIT"
readme = "README.md"
edition = "2018"

[lib]
name = "wasmer"

# Shared dependencies.
[dependencies]
# - Mandatory shared dependencies.
indexmap = { version = "1.6" }
cfg-if = "1.0"
thiserror = "1.0"
more-asserts = "0.2"
# - Optional shared dependencies.
wat = { version = "1.0", optional = true }

# Dependencies and Development Dependencies for `sys`.
[target.'cfg(not(target_arch = "wasm32"))'.dependencies]
# - Mandatory dependencies for `sys`.
wasmer-vm = { path = "../vm", version = "=2.4.1", package = "wasmer-vm-near" }
wasmer-compiler = { path = "../compiler", version = "=2.4.1", package = "wasmer-compiler-near" }
wasmer-derive = { path = "../derive", version = "=2.4.1", package = "wasmer-derive-near" }
wasmer-engine = { path = "../engine", version = "=2.4.1", package = "wasmer-engine-near" }
wasmer-types = { path = "../types", version = "=2.4.1", package = "wasmer-types-near" }
target-lexicon = { version = "0.12.2", default-features = false }
# - Optional dependencies for `sys`.
wasmer-compiler-singlepass = { path = "../compiler-singlepass", package = "wasmer-compiler-singlepass-near", version = "=2.4.1", optional = true}
wasmer-compiler-cranelift = { path = "../compiler-cranelift", version = "2.1.0", optional = true }
wasmer-compiler-llvm = { path = "../compiler-llvm", version = "2.1.0", optional = true }
wasmer-engine-universal = { path = "../engine-universal", package = "wasmer-engine-universal-near", version = "=2.4.1", optional = true }
# - Mandatory dependencies for `sys` on Windows.
[target.'cfg(all(not(target_arch = "wasm32"), target_os = "windows"))'.dependencies]
winapi = "0.3"
# - Development Dependencies for `sys`.
[target.'cfg(not(target_arch = "wasm32"))'.dev-dependencies]
wat = "1.0"
tempfile = "3.1"
anyhow = "1.0"

[badges]
maintenance = { status = "actively-developed" }

[features]
default = ["sys-default"]
std = []
core = []

# Features for `sys`.
sys = []
sys-default = ["sys", "wat", "default-singlepass", "default-universal"]
# - Compilers.
compiler = [
    "sys",
    "wasmer-compiler/translator",
    "wasmer-engine-universal/compiler",
]
    singlepass = [
        "compiler",
        "wasmer-compiler-singlepass",
    ]
    cranelift = [
        "compiler",
        "wasmer-compiler-cranelift",
    ]
    llvm = [
        "compiler",
        "wasmer-compiler-llvm",
    ]
default-compiler = []
    default-singlepass = [
        "default-compiler",
        "singlepass",
    ]
    default-cranelift = [
        "default-compiler",
        "cranelift",
    ]
    default-llvm = [
        "default-compiler",
        "llvm",
    ]
# - Engines.
engine = ["sys"]
    universal = [
        "engine",
        "wasmer-engine-universal",
    ]
default-engine = []
    default-universal = [
        "default-engine",
        "universal",
    ]
# - Experimental / in-development features
experimental-reference-types-extern-ref = [
    "sys",
    "wasmer-types/experimental-reference-types-extern-ref",
]

[package.metadata.docs.rs]
features = ["compiler", "core", "cranelift", "default-compiler", "default-engine", "engine", "jit", "llvm", "native", "singlepass", "sys", "sys-default", "universal"]

'''
'''--- lib/api/README.md ---
# `wasmer` [![Build Status](https://github.com/wasmerio/wasmer/workflows/build/badge.svg?style=flat-square)](https://github.com/wasmerio/wasmer/actions?query=workflow%3Abuild) [![Join Wasmer Slack](https://img.shields.io/static/v1?label=Slack&message=join%20chat&color=brighgreen&style=flat-square)](https://slack.wasmer.io) [![MIT License](https://img.shields.io/github/license/wasmerio/wasmer.svg?style=flat-square)](https://github.com/wasmerio/wasmer/blob/master/LICENSE) [![crates.io](https://img.shields.io/crates/v/wasmer.svg)](https://crates.io/crates/wasmer)

[`Wasmer`](https://wasmer.io/) is the most popular
[WebAssembly](https://webassembly.org/) runtime for Rust. It supports
JIT (Just In Time) and AOT (Ahead Of Time) compilation as well as
pluggable compilers suited to your needs.

It's designed to be safe and secure, and runnable in any kind of environment.

## Usage

Here is a small example of using Wasmer to run a WebAssembly module
written with its WAT format (textual format):

```rust
use wasmer::{Store, Module, Instance, Value, imports};

fn main() -> anyhow::Result<()> {
    let module_wat = r#"
    (module
    (type $t0 (func (param i32) (result i32)))
    (func $add_one (export "add_one") (type $t0) (param $p0 i32) (result i32)
        get_local $p0
        i32.const 1
        i32.add))
    "#;

    let store = Store::default();
    let module = Module::new(&store, &module_wat)?;
    // The module doesn't import anything, so we create an empty import object.
    let import_object = imports! {};
    let instance = Instance::new(&module, &import_object)?;

    let add_one = instance.exports.get_function("add_one")?;
    let result = add_one.call(&[Value::I32(42)])?;
    assert_eq!(result[0], Value::I32(43));

    Ok(())
}
```

[Discover the full collection of examples](https://github.com/wasmerio/wasmer/tree/master/examples).

## Features

Wasmer is not only fast, but also designed to be *highly customizable*:

* **Pluggable engines** ‚Äî An engine is responsible to drive the
  compilation process and to store the generated executable code
  somewhere, either:
  * in-memory (with [`wasmer-engine-universal`]),
  * in a native shared object file (with [`wasmer-engine-dylib`],
    `.dylib`, `.so`, `.dll`), then load it with `dlopen`,
  * in a native static object file (with [`wasmer-engine-staticlib`]),
    in addition to emitting a C header file, which both can be linked
    against a sandboxed WebAssembly runtime environment for the
    compiled module with no need for runtime compilation.

* **Pluggable compilers** ‚Äî A compiler is used by an engine to
  transform WebAssembly into executable code:
  * [`wasmer-compiler-singlepass`] provides a fast compilation-time
    but an unoptimized runtime speed,
  * [`wasmer-compiler-cranelift`] provides the right balance between
    compilation-time and runtime performance, useful for development,
  * [`wasmer-compiler-llvm`] provides a deeply optimized executable
    code with the fastest runtime speed, ideal for production.
    
* **Headless mode** ‚Äî Once a WebAssembly module has been compiled, it
  is possible to serialize it in a file for example, and later execute
  it with Wasmer with headless mode turned on. Headless Wasmer has no
  compiler, which makes it more portable and faster to load. It's
  ideal for constrainted environments.
  
* **Cross-compilation** ‚Äî Most compilers support cross-compilation. It
  means it possible to pre-compile a WebAssembly module targetting a
  different architecture or platform and serialize it, to then run it
  on the targetted architecture and platform later.

* **Run Wasmer in a JavaScript environment** ‚Äî With the `js` Cargo
  feature, it is possible to compile a Rust program using Wasmer to
  WebAssembly. In this context, the resulting WebAssembly module will
  expect to run in a JavaScript environment, like a browser, Node.js,
  Deno and so on. In this specific scenario, there is no engines or
  compilers available, it's the one available in the JavaScript
  environment that will be used.

Wasmer ships by default with the Cranelift compiler as its great for
development purposes.  However, we strongly encourage to use the LLVM
compiler in production as it performs about 50% faster, achieving
near-native speeds.

Note: if one wants to use multiple compilers at the same time, it's
also possible! One will need to import them directly via each of the
compiler crates.

Read [the documentation to learn
more](https://wasmerio.github.io/wasmer/crates/doc/wasmer/).

---

Made with ‚ù§Ô∏è by the Wasmer team, for the community

[`wasmer-engine-universal`]: https://github.com/wasmerio/wasmer/tree/master/lib/engine-universal
[`wasmer-engine-dylib`]: https://github.com/wasmerio/wasmer/tree/master/lib/engine-dylib
[`wasmer-engine-staticlib`]: https://github.com/wasmerio/wasmer/tree/master/lib/engine-staticlib 
[`wasmer-compiler-singlepass`]: https://github.com/wasmerio/wasmer/tree/master/lib/compiler-singlepass
[`wasmer-compiler-cranelift`]: https://github.com/wasmerio/wasmer/tree/master/lib/compiler-cranelift
[`wasmer-compiler-llvm`]: https://github.com/wasmerio/wasmer/tree/master/lib/compiler-llvm

'''
'''--- lib/api/src/lib.rs ---
#![doc(
    html_logo_url = "https://github.com/wasmerio.png?size=200",
    html_favicon_url = "https://wasmer.io/images/icons/favicon-32x32.png"
)]
#![deny(
    missing_docs,
    trivial_numeric_casts,
    unused_extern_crates,
    broken_intra_doc_links
)]
#![warn(unused_import_braces)]
#![cfg_attr(
    feature = "cargo-clippy",
    allow(clippy::new_without_default, vtable_address_comparisons)
)]
#![cfg_attr(
    feature = "cargo-clippy",
    warn(
        clippy::float_arithmetic,
        clippy::mut_mut,
        clippy::nonminimal_bool,
        clippy::option_map_unwrap_or,
        clippy::option_map_unwrap_or_else,
        clippy::print_stdout,
        clippy::unicode_not_nfc,
        clippy::use_self
    )
)]
//! [`Wasmer`](https://wasmer.io/) is the most popular
//! [WebAssembly](https://webassembly.org/) runtime for Rust. It supports
//! JIT (Just In Time) and AOT (Ahead Of Time) compilation as well as
//! pluggable compilers suited to your needs.
//!
//! It's designed to be safe and secure, and runnable in any kind of environment.
//!
//! # Usage
//!
//! Here is a small example of using Wasmer to run a WebAssembly module
//! written with its WAT format (textual format):
//!
//! ```rust
//! use wasmer::{Store, Module, Instance, Value, Export, imports};
//!
//! fn main() -> anyhow::Result<()> {
//!     let module_wat = r#"
//!     (module
//!       (type $t0 (func (param i32) (result i32)))
//!       (func $add_one (export "add_one") (type $t0) (param $p0 i32) (result i32)
//!         get_local $p0
//!         i32.const 1
//!         i32.add))
//!     "#;
//!
//!     let store = Store::default();
//!     let module = Module::new(&store, &module_wat)?;
//!     // The module doesn't import anything, so we create an empty import object.
//!     let import_object = imports! {};
//!     let instance = Instance::new(&module, &import_object)?;
//!
//!     let add_one = instance.lookup_function("add_one").unwrap();
//!     let result = add_one.call(&[Value::I32(42)])?;
//!     assert_eq!(result[0], Value::I32(43));
//!
//!     Ok(())
//! }
//! ```
//!
//! [Discover the full collection of examples](https://github.com/wasmerio/wasmer/tree/master/examples).
//!
//! # Overview of the Features
//!
//! Wasmer is not only fast, but also designed to be *highly customizable*:
//!
//! * **Pluggable engines** ‚Äî An engine is responsible to drive the
//!   compilation process and to store the generated executable code
//!   somewhere, either:
//!   * in-memory (with [`wasmer-engine-universal`]),
//!   * in a native shared object file (with [`wasmer-engine-dylib`],
//!     `.dylib`, `.so`, `.dll`), then load it with `dlopen`,
//!   * in a native static object file (with [`wasmer-engine-staticlib`]),
//!     in addition to emitting a C header file, which both can be linked
//!     against a sandboxed WebAssembly runtime environment for the
//!     compiled module with no need for runtime compilation.
//!
//! * **Pluggable compilers** ‚Äî A compiler is used by an engine to
//!   transform WebAssembly into executable code:
//!   * [`wasmer-compiler-singlepass`] provides a fast compilation-time
//!     but an unoptimized runtime speed,
//!   * [`wasmer-compiler-cranelift`] provides the right balance between
//!     compilation-time and runtime performance, useful for development,
//!   * [`wasmer-compiler-llvm`] provides a deeply optimized executable
//!     code with the fastest runtime speed, ideal for production.
//!
//! * **Headless mode** ‚Äî Once a WebAssembly module has been compiled, it
//!   is possible to serialize it in a file for example, and later execute
//!   it with Wasmer with headless mode turned on. Headless Wasmer has no
//!   compiler, which makes it more portable and faster to load. It's
//!   ideal for constrainted environments.
//!
//! * **Cross-compilation** ‚Äî Most compilers support cross-compilation. It
//!   means it possible to pre-compile a WebAssembly module targetting a
//!   different architecture or platform and serialize it, to then run it
//!   on the targetted architecture and platform later.
//!
//! * **Run Wasmer in a JavaScript environment** ‚Äî With the `js` Cargo
//!   feature, it is possible to compile a Rust program using Wasmer to
//!   WebAssembly. In this context, the resulting WebAssembly module will
//!   expect to run in a JavaScript environment, like a browser, Node.js,
//!   Deno and so on. In this specific scenario, there is no engines or
//!   compilers available, it's the one available in the JavaScript
//!   environment that will be used.
//!
//! Wasmer ships by default with the Cranelift compiler as its great for
//! development purposes.  However, we strongly encourage to use the LLVM
//! compiler in production as it performs about 50% faster, achieving
//! near-native speeds.
//!
//! Note: if one wants to use multiple compilers at the same time, it's
//! also possible! One will need to import them directly via each of the
//! compiler crates.
//!
//! # Table of Contents
//!
//! - [WebAssembly Primitives](#webassembly-primitives)
//!   - [Externs](#externs)
//!     - [Functions](#functions)
//!     - [Memories](#memories)
//!     - [Globals](#globals)
//!     - [Tables](#tables)
//! - [Project Layout](#project-layout)
//!   - [Engines](#engines)
//!   - [Compilers](#compilers)
//! - [Cargo Features](#cargo-features)
//! - [Using Wasmer in a JavaScript environment](#using-wasmer-in-a-javascript-environment)
//!
//!
//! # WebAssembly Primitives
//!
//! In order to make use of the power of the `wasmer` API, it's important
//! to understand the primitives around which the API is built.
//!
//! Wasm only deals with a small number of core data types, these data
//! types can be found in the [`Value`] type.
//!
//! In addition to the core Wasm types, the core types of the API are
//! referred to as "externs".
//!
//! ## Externs
//!
//! An [`Extern`] is a type that can be imported or exported from a Wasm
//! module.
//!
//! To import an extern, simply give it a namespace and a name with the
//! [`imports`] macro:
//!
//! ```
//! # use wasmer::{imports, Function, Memory, MemoryType, Store, ImportObject};
//! # fn imports_example(store: &Store) -> ImportObject {
//! let memory = Memory::new(&store, MemoryType::new(1, None, false)).unwrap();
//! imports! {
//!     "env" => {
//!          "my_function" => Function::new_native(store, || println!("Hello")),
//!          "memory" => memory,
//!     }
//! }
//! # }
//! ```
//!
//! And to access an exported extern, see the [`Exports`] API, accessible
//! from any instance via `instance.exports`:
//!
//! ```
//! # use wasmer::{imports, Instance, Function, Memory, NativeFunc};
//! # fn exports_example(instance: &Instance) -> anyhow::Result<()> {
//! let memory = instance.lookup("memory").unwrap();
//! let memory = instance.lookup("some_other_memory").unwrap();
//! let add: NativeFunc<(i32, i32), i32> = instance.get_native_function("add").unwrap();
//! let result = add.call(5, 37)?;
//! assert_eq!(result, 42);
//! # Ok(())
//! # }
//! ```
//!
//! These are the primary types that the `wasmer` API uses.
//!
//! ### Functions
//!
//! There are 2 types of functions in `wasmer`:
//! 1. Wasm functions,
//! 2. Host functions.
//!
//! A Wasm function is a function defined in a WebAssembly module that can
//! only perform computation without side effects and call other functions.
//!
//! Wasm functions take 0 or more arguments and return 0 or more results.
//! Wasm functions can only deal with the primitive types defined in
//! [`Value`].
//!
//! A Host function is any function implemented on the host, in this case in
//! Rust.
//!
//! Host functions can optionally be created with an environment that
//! implements [`WasmerEnv`]. This environment is useful for maintaining
//! host state (for example the filesystem in WASI).
//!
//! Thus WebAssembly modules by themselves cannot do anything but computation
//! on the core types in [`Value`]. In order to make them more useful we
//! give them access to the outside world with [`imports`].
//!
//! If you're looking for a sandboxed, POSIX-like environment to execute Wasm
//! in, check out the [`wasmer-wasi`] crate for our implementation of WASI,
//! the WebAssembly System Interface.
//!
//! In the `wasmer` API we support functions which take their arguments and
//! return their results dynamically, [`Function`], and functions which
//! take their arguments and return their results statically, [`NativeFunc`].
//!
//! ### Memories
//!
//! Memories store data.
//!
//! In most Wasm programs, nearly all data will live in a [`Memory`].
//!
//! This data can be shared between the host and guest to allow for more
//! interesting programs.
//!
//! ### Globals
//!
//! A [`Global`] is a type that may be either mutable or immutable, and
//! contains one of the core Wasm types defined in [`Value`].
//!
//! ### Tables
//!
//! A [`Table`] is an indexed list of items.
//!
//! # Project Layout
//!
//! The Wasmer project is divided into a number of crates, below is a dependency
//! graph with transitive dependencies removed.
//!
//! <div>
//! <img src="https://raw.githubusercontent.com/wasmerio/wasmer/master/docs/deps_dedup.svg" />
//! </div>
//!
//! While this crate is the top level API, we also publish crates built
//! on top of this API that you may be interested in using, including:
//!
//! - [`wasmer-cache`] for caching compiled Wasm modules,
//! - [`wasmer-emscripten`] for running Wasm modules compiled to the
//!   Emscripten ABI,
//! - [`wasmer-wasi`] for running Wasm modules compiled to the WASI ABI.
//!
//! The Wasmer project has two major abstractions:
//! 1. [Engines][wasmer-engine],
//! 2. [Compilers][wasmer-compiler].
//!
//! These two abstractions have multiple options that can be enabled
//! with features.
//!
//! ## Engines
//!
//! An engine is a system that uses a compiler to make a WebAssembly
//! module executable.
//!
//! ## Compilers
//!
//! A compiler is a system that handles the details of making a Wasm
//! module executable. For example, by generating native machine code
//! for each Wasm function.
//!
//! # Cargo Features
//!
//! This crate comes in 1 flavor:
//!
//! 1. `sys`
//!    where `wasmer` will be compiled to a native executable
//!    which provides compilers, engines, a full VM etc.
#![cfg_attr(
    feature = "sys",
    doc = "## Features for the `sys` feature group (enabled)"
)]
#![cfg_attr(
    not(feature = "sys"),
    doc = "## Features for the `sys` feature group (disabled)"
)]
//!
//! The default features can be enabled with the `sys-default` feature.
//!
//! The features for the `sys` feature group can be broken down into 2
//! kinds: features that enable new functionality and features that
//! set defaults.
//!
//! The features that enable new functionality are:
//! - `cranelift`
#![cfg_attr(feature = "cranelift", doc = "(enabled),")]
#![cfg_attr(not(feature = "cranelift"), doc = "(disabled),")]
//!   enables Wasmer's [Cranelift compiler][wasmer-compiler-cranelift],
//! - `llvm`
#![cfg_attr(feature = "llvm", doc = "(enabled),")]
#![cfg_attr(not(feature = "llvm"), doc = "(disabled),")]
//!   enables Wasmer's [LLVM compiler][wasmer-compiler-lvm],
//! - `singlepass`
#![cfg_attr(feature = "singlepass", doc = "(enabled),")]
#![cfg_attr(not(feature = "singlepass"), doc = "(disabled),")]
//!   enables Wasmer's [Singlepass compiler][wasmer-compiler-singlepass],
//! - `wat`
#![cfg_attr(feature = "wat", doc = "(enabled),")]
#![cfg_attr(not(feature = "wat"), doc = "(disabled),")]
//!   enables `wasmer` to parse the WebAssembly text format,
//! - `universal`
#![cfg_attr(feature = "universal", doc = "(enabled),")]
#![cfg_attr(not(feature = "universal"), doc = "(disabled),")]
//!   enables [the Universal engine][`wasmer-engine-universal`],
//! - `dylib`
#![cfg_attr(feature = "dylib", doc = "(enabled),")]
#![cfg_attr(not(feature = "dylib"), doc = "(disabled),")]
//!   enables [the Dylib engine][`wasmer-engine-dylib`].
//!
//! The features that set defaults come in sets that are mutually exclusive.
//!
//! The first set is the default compiler set:
//! - `default-cranelift`
#![cfg_attr(feature = "default-cranelift", doc = "(enabled),")]
#![cfg_attr(not(feature = "default-cranelift"), doc = "(disabled),")]
//!   set Wasmer's Cranelift compiler as the default,
//! - `default-llvm`
#![cfg_attr(feature = "default-llvm", doc = "(enabled),")]
#![cfg_attr(not(feature = "default-llvm"), doc = "(disabled),")]
//!   set Wasmer's LLVM compiler as the default,
//! - `default-singlepass`
#![cfg_attr(feature = "default-singlepass", doc = "(enabled),")]
#![cfg_attr(not(feature = "default-singlepass"), doc = "(disabled),")]
//!   set Wasmer's Singlepass compiler as the default.
//!
//! The next set is the default engine set:
//! - `default-universal`
#![cfg_attr(feature = "default-universal", doc = "(enabled),")]
#![cfg_attr(not(feature = "default-universal"), doc = "(disabled),")]
//!   set the Universal engine as the default,
//! - `default-dylib`
#![cfg_attr(feature = "default-dylib", doc = "(enabled),")]
#![cfg_attr(not(feature = "default-dylib"), doc = "(disabled),")]
//!   set the Dylib engine as the default.
//!
//! - `wat`
#![cfg_attr(feature = "wat", doc = "(enabled),")]
#![cfg_attr(not(feature = "wat"), doc = "(disabled),")]
//!  allows to read a Wasm file in its text format. This feature is
//!  normally used only in development environments. It will add
//!  around 650kb to the Wasm bundle (120Kb gzipped).
//!
//! [wasm]: https://webassembly.org/
//! [wasmer-examples]: https://github.com/wasmerio/wasmer/tree/master/examples
//! [`wasmer-cache`]: https://docs.rs/wasmer-cache/
//! [wasmer-compiler]: https://docs.rs/wasmer-compiler/
//! [`wasmer-emscripten`]: https://docs.rs/wasmer-emscripten/
//! [wasmer-engine]: https://docs.rs/wasmer-engine/
//! [`wasmer-engine-universal`]: https://docs.rs/wasmer-engine-universal/
//! [`wasmer-engine-dylib`]: https://docs.rs/wasmer-engine-dylib/
//! [`wasmer-engine-staticlib`]: https://docs.rs/wasmer-engine-staticlib/
//! [`wasmer-compiler-singlepass`]: https://docs.rs/wasmer-compiler-singlepass/
//! [`wasmer-compiler-llvm`]: https://docs.rs/wasmer-compiler-llvm/
//! [`wasmer-compiler-cranelift`]: https://docs.rs/wasmer-compiler-cranelift/
//! [`wasmer-wasi`]: https://docs.rs/wasmer-wasi/
//! [`wasm-pack`]: https://github.com/rustwasm/wasm-pack/
//! [`wasm-bindgen`]: https://github.com/rustwasm/wasm-bindgen

#[cfg(not(feature = "sys"))]
compile_error!("At least the `sys` or the `js` feature must be enabled. Please, pick one.");

#[cfg(all(feature = "sys", target_arch = "wasm32"))]
compile_error!("The `sys` feature must be enabled only for non-`wasm32` target.");

#[cfg(feature = "sys")]
mod sys;

#[cfg(feature = "sys")]
pub use sys::*;

'''
'''--- lib/api/src/sys/cell.rs ---
pub use std::cell::Cell;

use core::cmp::Ordering;
use core::fmt::{self, Debug};

/// A mutable Wasm-memory location.
#[repr(transparent)]
pub struct WasmCell<'a, T: ?Sized> {
    inner: &'a Cell<T>,
}

unsafe impl<T: ?Sized> Send for WasmCell<'_, T> where T: Send {}

unsafe impl<T: ?Sized> Sync for WasmCell<'_, T> {}

impl<'a, T: Copy> Clone for WasmCell<'a, T> {
    #[inline]
    fn clone(&self) -> WasmCell<'a, T> {
        WasmCell { inner: self.inner }
    }
}

impl<T: PartialEq + Copy> PartialEq for WasmCell<'_, T> {
    #[inline]
    fn eq(&self, other: &WasmCell<T>) -> bool {
        self.inner.eq(&other.inner)
    }
}

impl<T: Eq + Copy> Eq for WasmCell<'_, T> {}

impl<T: PartialOrd + Copy> PartialOrd for WasmCell<'_, T> {
    #[inline]
    fn partial_cmp(&self, other: &WasmCell<T>) -> Option<Ordering> {
        self.inner.partial_cmp(&other.inner)
    }

    #[inline]
    fn lt(&self, other: &WasmCell<T>) -> bool {
        self.inner < other.inner
    }

    #[inline]
    fn le(&self, other: &WasmCell<T>) -> bool {
        self.inner <= other.inner
    }

    #[inline]
    fn gt(&self, other: &WasmCell<T>) -> bool {
        self.inner > other.inner
    }

    #[inline]
    fn ge(&self, other: &WasmCell<T>) -> bool {
        self.inner >= other.inner
    }
}

impl<T: Ord + Copy> Ord for WasmCell<'_, T> {
    #[inline]
    fn cmp(&self, other: &WasmCell<T>) -> Ordering {
        self.inner.cmp(&other.inner)
    }
}

impl<'a, T> WasmCell<'a, T> {
    /// Creates a new `WasmCell` containing the given value.
    ///
    /// # Examples
    ///
    /// ```
    /// use std::cell::Cell;
    /// use wasmer::WasmCell;
    ///
    /// let cell = Cell::new(5);
    /// let wasm_cell = WasmCell::new(&cell);
    /// ```
    #[inline]
    pub const fn new(cell: &'a Cell<T>) -> WasmCell<'a, T> {
        WasmCell { inner: cell }
    }
}

impl<'a, T: Copy> WasmCell<'a, T> {
    /// Returns a copy of the contained value.
    ///
    /// # Examples
    ///
    /// ```
    /// use std::cell::Cell;
    /// use wasmer::WasmCell;
    ///
    /// let cell = Cell::new(5);
    /// let wasm_cell = WasmCell::new(&cell);
    /// let five = wasm_cell.get();
    /// ```
    #[inline]
    pub fn get(&self) -> T {
        self.inner.get()
    }

    /// Get an unsafe mutable pointer to the inner item
    /// in the Cell.
    ///
    /// # Safety
    ///
    /// This method is highly discouraged to use. We have it for
    /// compatibility reasons with Emscripten.
    /// It is unsafe because changing an item inline will change
    /// the underlying memory.
    ///
    /// It's highly encouraged to use the `set` method instead.
    #[deprecated(
        since = "2.0.0",
        note = "Please use the memory-safe set method instead"
    )]
    #[doc(hidden)]
    pub unsafe fn get_mut(&self) -> &'a mut T {
        &mut *self.inner.as_ptr()
    }
}

impl<T: Debug + Copy> Debug for WasmCell<'_, T> {
    #[inline]
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(f, "WasmCell({:?})", self.inner.get())
    }
}

impl<T: Sized> WasmCell<'_, T> {
    /// Sets the contained value.
    ///
    /// # Examples
    ///
    /// ```
    /// use std::cell::Cell;
    /// use wasmer::WasmCell;
    ///
    /// let cell = Cell::new(5);
    /// let wasm_cell = WasmCell::new(&cell);
    /// wasm_cell.set(10);
    /// assert_eq!(cell.get(), 10);
    /// ```
    #[inline]
    pub fn set(&self, val: T) {
        self.inner.set(val);
    }
}

'''
'''--- lib/api/src/sys/env.rs ---
use crate::sys::{ExportError, Instance};
use thiserror::Error;

/// An error while initializing the user supplied host env with the `WasmerEnv` trait.
#[derive(Error, Debug)]
#[error("Host env initialization error")]
pub enum HostEnvInitError {
    /// An error occurred when accessing an export
    Export(ExportError),
    /// Incorrect gas metering config
    IncorrectGasMeteringConfig,
}

impl From<ExportError> for HostEnvInitError {
    fn from(other: ExportError) -> Self {
        Self::Export(other)
    }
}

/// Trait for initializing the environments passed to host functions after
/// instantiation but before execution.
///
/// This is useful for filling an environment with data that can only be accesed
/// after instantiation. For example, exported items such as memories and
/// functions which don't exist prior to instantiation can be accessed here so
/// that host functions can use them.
///
/// # Examples
///
/// This trait can be derived like so:
///
/// ```
/// use wasmer::{WasmerEnv, LazyInit, Memory, NativeFunc};
///
/// #[derive(WasmerEnv, Clone)]
/// pub struct MyEnvWithNoInstanceData {
///     non_instance_data: u8,
/// }
///
/// #[derive(WasmerEnv, Clone)]
/// pub struct MyEnvWithInstanceData {
///     non_instance_data: u8,
///     #[wasmer(export)]
///     memory: LazyInit<Memory>,
///     #[wasmer(export(name = "real_name"))]
///     func: LazyInit<NativeFunc<(i32, i32), i32>>,
///     #[wasmer(export(optional = true, alias = "memory2", alias = "_memory2"))]
///     optional_memory: LazyInit<Memory>,
/// }
///
/// ```
///
/// When deriving `WasmerEnv`, you must wrap your types to be initialized in
/// [`LazyInit`]. The derive macro will also generate helper methods of the form
/// `<field_name>_ref` and `<field_name>_ref_unchecked` for easy access to the
/// data.
///
/// The valid arguments to `export` are:
/// - `name = "string"`: specify the name of this item in the Wasm module. If this is not specified, it will default to the name of the field.
/// - `optional = true`: specify whether this export is optional. Defaults to
/// `false`. Being optional means that if the export can't be found, the
/// [`LazyInit`] will be left uninitialized.
/// - `alias = "string"`: specify additional names to look for in the Wasm module.
/// `alias` may be specified multiple times to search for multiple aliases.
/// -------
///
/// This trait may also be implemented manually:
/// ```
/// # use wasmer::{WasmerEnv, LazyInit, Memory, Instance, HostEnvInitError};
/// #[derive(Clone)]
/// pub struct MyEnv {
///    memory: LazyInit<Memory>,
/// }
///
/// impl WasmerEnv for MyEnv {
///     fn init_with_instance(&mut self, instance: &Instance) -> Result<(), HostEnvInitError> {
///         let memory: Memory = instance.get_with_generics_weak("memory").unwrap();
///         self.memory.initialize(memory.clone());
///         Ok(())
///     }
/// }
/// ```
///
/// When implementing the trait manually, it's important to get a "weak" export to
/// prevent a cyclic reference leaking memory. You can access a "weak" export with
/// a method like `get_with_generics_weak`.
pub trait WasmerEnv: Clone + Send + Sync {
    /// The function that Wasmer will call on your type to let it finish
    /// setting up the environment with data from the `Instance`.
    ///
    /// This function is called after `Instance` is created but before it is
    /// returned to the user via `Instance::new`.
    fn init_with_instance(&mut self, _instance: &Instance) -> Result<(), HostEnvInitError> {
        Ok(())
    }
}

impl WasmerEnv for u8 {}
impl WasmerEnv for i8 {}
impl WasmerEnv for u16 {}
impl WasmerEnv for i16 {}
impl WasmerEnv for u32 {}
impl WasmerEnv for i32 {}
impl WasmerEnv for u64 {}
impl WasmerEnv for i64 {}
impl WasmerEnv for u128 {}
impl WasmerEnv for i128 {}
impl WasmerEnv for f32 {}
impl WasmerEnv for f64 {}
impl WasmerEnv for usize {}
impl WasmerEnv for isize {}
impl WasmerEnv for char {}
impl WasmerEnv for bool {}
impl WasmerEnv for String {}
impl<'a> WasmerEnv for &'a ::std::sync::atomic::AtomicBool {}
impl<'a> WasmerEnv for &'a ::std::sync::atomic::AtomicI8 {}
impl<'a> WasmerEnv for &'a ::std::sync::atomic::AtomicU8 {}
impl<'a> WasmerEnv for &'a ::std::sync::atomic::AtomicI16 {}
impl<'a> WasmerEnv for &'a ::std::sync::atomic::AtomicU16 {}
impl<'a> WasmerEnv for &'a ::std::sync::atomic::AtomicI32 {}
impl<'a> WasmerEnv for &'a ::std::sync::atomic::AtomicU32 {}
impl<'a> WasmerEnv for &'a ::std::sync::atomic::AtomicI64 {}
impl<'a> WasmerEnv for &'a ::std::sync::atomic::AtomicUsize {}
impl<'a> WasmerEnv for &'a ::std::sync::atomic::AtomicIsize {}
impl<T: WasmerEnv> WasmerEnv for Box<T> {
    fn init_with_instance(&mut self, instance: &Instance) -> Result<(), HostEnvInitError> {
        (&mut **self).init_with_instance(instance)
    }
}

impl<T: WasmerEnv> WasmerEnv for ::std::sync::Arc<::std::sync::Mutex<T>> {
    fn init_with_instance(&mut self, instance: &Instance) -> Result<(), HostEnvInitError> {
        let mut guard = self.lock().unwrap();
        guard.init_with_instance(instance)
    }
}

/// Lazily init an item
pub struct LazyInit<T: Sized> {
    /// The data to be initialized
    data: std::mem::MaybeUninit<T>,
    /// Whether or not the data has been initialized
    initialized: bool,
}

impl<T> LazyInit<T> {
    /// Creates an unitialized value.
    pub fn new() -> Self {
        Self {
            data: std::mem::MaybeUninit::uninit(),
            initialized: false,
        }
    }

    /// # Safety
    /// - The data must be initialized first
    pub unsafe fn get_unchecked(&self) -> &T {
        &*self.data.as_ptr()
    }

    /// Get the inner data.
    pub fn get_ref(&self) -> Option<&T> {
        if !self.initialized {
            None
        } else {
            Some(unsafe { self.get_unchecked() })
        }
    }

    /// Sets a value and marks the data as initialized.
    pub fn initialize(&mut self, value: T) -> bool {
        if self.initialized {
            return false;
        }
        unsafe {
            self.data.as_mut_ptr().write(value);
        }
        self.initialized = true;
        true
    }
}

impl<T: std::fmt::Debug> std::fmt::Debug for LazyInit<T> {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        f.debug_struct("LazyInit")
            .field("data", &self.get_ref())
            .finish()
    }
}

impl<T: Clone> Clone for LazyInit<T> {
    fn clone(&self) -> Self {
        if let Some(inner) = self.get_ref() {
            Self {
                data: std::mem::MaybeUninit::new(inner.clone()),
                initialized: true,
            }
        } else {
            Self {
                data: std::mem::MaybeUninit::uninit(),
                initialized: false,
            }
        }
    }
}

impl<T> Drop for LazyInit<T> {
    fn drop(&mut self) {
        if self.initialized {
            unsafe {
                let ptr = self.data.as_mut_ptr();
                std::ptr::drop_in_place(ptr);
            };
        }
    }
}

impl<T> Default for LazyInit<T> {
    fn default() -> Self {
        Self::new()
    }
}

unsafe impl<T: Send> Send for LazyInit<T> {}
// I thought we could opt out of sync..., look into this
// unsafe impl<T> !Sync for InitWithInstance<T> {}

'''
'''--- lib/api/src/sys/exports.rs ---
use crate::sys::externals::{Extern, Function, Global, Memory, Table};
use crate::sys::import_object::LikeNamespace;
use crate::sys::native::NativeFunc;
use crate::sys::WasmTypeList;
use indexmap::IndexMap;
use std::fmt;
use std::iter::{ExactSizeIterator, FromIterator};
use std::sync::Arc;
use thiserror::Error;
use wasmer_vm::Export;

/// The `ExportError` can happen when trying to get a specific
/// export [`Extern`] from the [`Instance`] exports.
///
/// [`Instance`]: crate::Instance
///
/// # Examples
///
/// ## Incompatible export type
///
/// ```should_panic
/// # use wasmer::{imports, wat2wasm, Function, Instance, Module, Store, Type, Value, ExportError};
/// # let store = Store::default();
/// # let wasm_bytes = wat2wasm(r#"
/// # (module
/// #   (global $one (export "glob") f32 (f32.const 1)))
/// # "#.as_bytes()).unwrap();
/// # let module = Module::new(&store, wasm_bytes).unwrap();
/// # let import_object = imports! {};
/// # let instance = Instance::new(&module, &import_object).unwrap();
/// #
/// // This results in an error.
/// let export = instance.lookup_function("glob").unwrap();
/// ```
///
/// ## Missing export
///
/// ```should_panic
/// # use wasmer::{imports, wat2wasm, Function, Instance, Module, Store, Type, Value, ExportError};
/// # let store = Store::default();
/// # let wasm_bytes = wat2wasm("(module)".as_bytes()).unwrap();
/// # let module = Module::new(&store, wasm_bytes).unwrap();
/// # let import_object = imports! {};
/// # let instance = Instance::new(&module, &import_object).unwrap();
/// #
/// // This results with an error: `ExportError::Missing`.
/// let export = instance.lookup("unknown").unwrap();
/// ```
#[derive(Error, Debug)]
pub enum ExportError {
    /// An error than occurs when the exported type and the expected type
    /// are incompatible.
    #[error("Incompatible Export Type")]
    IncompatibleType,
    /// This error arises when an export is missing
    #[error("Missing export {0}")]
    Missing(String),
}

/// Exports is a special kind of map that allows easily unwrapping
/// the types of instances.
///
/// TODO: add examples of using exports
#[derive(Clone, Default)]
pub struct Exports {
    map: Arc<IndexMap<String, Extern>>,
}

impl Exports {
    /// Creates a new `Exports`.
    pub fn new() -> Self {
        Default::default()
    }

    /// Creates a new `Exports` with capacity `n`.
    pub fn with_capacity(n: usize) -> Self {
        Self {
            map: Arc::new(IndexMap::with_capacity(n)),
        }
    }

    /// Return the number of exports in the `Exports` map.
    pub fn len(&self) -> usize {
        self.map.len()
    }

    /// Return whether or not there are no exports
    pub fn is_empty(&self) -> bool {
        self.len() == 0
    }

    /// Insert a new export into this `Exports` map.
    pub fn insert<S, E>(&mut self, name: S, value: E)
    where
        S: Into<String>,
        E: Into<Extern>,
    {
        Arc::get_mut(&mut self.map)
            .unwrap()
            .insert(name.into(), value.into());
    }

    /// Get an export given a `name`.
    ///
    /// The `get` method is specifically made for usage inside of
    /// Rust APIs, as we can detect what's the desired type easily.
    ///
    /// If you want to get an export dynamically with type checking
    /// please use the following functions: `get_func`, `get_memory`,
    /// `get_table` or `get_global` instead.
    ///
    /// If you want to get an export dynamically handling manually
    /// type checking manually, please use `get_extern`.
    pub fn get<'a, T: Exportable<'a>>(&'a self, name: &str) -> Result<T, ExportError> {
        match self.map.get(name) {
            None => Err(ExportError::Missing(name.to_string())),
            Some(extern_) => T::get_self_from_extern(extern_.clone()),
        }
    }

    /// Get an export as a `Global`.
    pub fn get_global(&self, name: &str) -> Result<Global, ExportError> {
        self.get(name)
    }

    /// Get an export as a `Memory`.
    pub fn get_memory(&self, name: &str) -> Result<Memory, ExportError> {
        self.get(name)
    }

    /// Get an export as a `Table`.
    pub fn get_table(&self, name: &str) -> Result<Table, ExportError> {
        self.get(name)
    }

    /// Get an export as a `Func`.
    pub fn get_function(&self, name: &str) -> Result<Function, ExportError> {
        self.get(name)
    }

    /// Get an export as a `NativeFunc`.
    pub fn get_native_function<Args, Rets>(
        &self,
        name: &str,
    ) -> Result<NativeFunc<Args, Rets>, ExportError>
    where
        Args: WasmTypeList,
        Rets: WasmTypeList,
    {
        self.get_function(name)?
            .native()
            .map_err(|_| ExportError::IncompatibleType)
    }

    /// Hack to get this working with nativefunc too
    pub fn get_with_generics<'a, T, Args, Rets>(&'a self, name: &str) -> Result<T, ExportError>
    where
        Args: WasmTypeList,
        Rets: WasmTypeList,
        T: ExportableWithGenerics<'a, Args, Rets>,
    {
        match self.map.get(name) {
            None => Err(ExportError::Missing(name.to_string())),
            Some(extern_) => T::get_self_from_extern_with_generics(extern_.clone()),
        }
    }

    /// Like `get_with_generics` but with a WeakReference to the `InstanceRef` internally.
    /// This is useful for passing data into `WasmerEnv`, for example.
    pub fn get_with_generics_weak<'a, T, Args, Rets>(&'a self, name: &str) -> Result<T, ExportError>
    where
        Args: WasmTypeList,
        Rets: WasmTypeList,
        T: ExportableWithGenerics<'a, Args, Rets>,
    {
        let mut out: T = self.get_with_generics(name)?;
        out.into_weak_instance_ref();
        Ok(out)
    }

    /// Get an export as an `Extern`.
    pub fn get_extern(&self, name: &str) -> Option<&Extern> {
        self.map.get(name)
    }

    /// Returns true if the `Exports` contains the given export name.
    pub fn contains<S>(&self, name: S) -> bool
    where
        S: Into<String>,
    {
        self.map.contains_key(&name.into())
    }

    /// Get an iterator over the exports.
    pub fn iter(&self) -> ExportsIterator<impl Iterator<Item = (&String, &Extern)>> {
        ExportsIterator {
            iter: self.map.iter(),
        }
    }
}

impl fmt::Debug for Exports {
    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
        f.debug_set().entries(self.iter()).finish()
    }
}

/// An iterator over exports.
pub struct ExportsIterator<'a, I>
where
    I: Iterator<Item = (&'a String, &'a Extern)> + Sized,
{
    iter: I,
}

impl<'a, I> Iterator for ExportsIterator<'a, I>
where
    I: Iterator<Item = (&'a String, &'a Extern)> + Sized,
{
    type Item = (&'a String, &'a Extern);

    fn next(&mut self) -> Option<Self::Item> {
        self.iter.next()
    }
}

impl<'a, I> ExactSizeIterator for ExportsIterator<'a, I>
where
    I: Iterator<Item = (&'a String, &'a Extern)> + ExactSizeIterator + Sized,
{
    fn len(&self) -> usize {
        self.iter.len()
    }
}

impl<'a, I> ExportsIterator<'a, I>
where
    I: Iterator<Item = (&'a String, &'a Extern)> + Sized,
{
    /// Get only the functions.
    pub fn functions(self) -> impl Iterator<Item = (&'a String, &'a Function)> + Sized {
        self.iter.filter_map(|(name, export)| match export {
            Extern::Function(function) => Some((name, function)),
            _ => None,
        })
    }

    /// Get only the memories.
    pub fn memories(self) -> impl Iterator<Item = (&'a String, &'a Memory)> + Sized {
        self.iter.filter_map(|(name, export)| match export {
            Extern::Memory(memory) => Some((name, memory)),
            _ => None,
        })
    }

    /// Get only the globals.
    pub fn globals(self) -> impl Iterator<Item = (&'a String, &'a Global)> + Sized {
        self.iter.filter_map(|(name, export)| match export {
            Extern::Global(global) => Some((name, global)),
            _ => None,
        })
    }

    /// Get only the tables.
    pub fn tables(self) -> impl Iterator<Item = (&'a String, &'a Table)> + Sized {
        self.iter.filter_map(|(name, export)| match export {
            Extern::Table(table) => Some((name, table)),
            _ => None,
        })
    }
}

impl FromIterator<(String, Extern)> for Exports {
    fn from_iter<I: IntoIterator<Item = (String, Extern)>>(iter: I) -> Self {
        Self {
            map: Arc::new(IndexMap::from_iter(iter)),
        }
    }
}

impl LikeNamespace for Exports {
    fn get_namespace_export(&self, name: &str) -> Option<Export> {
        self.map.get(name).map(|is_export| is_export.to_export())
    }

    fn get_namespace_exports(&self) -> Vec<(String, Export)> {
        self.map
            .iter()
            .map(|(k, v)| (k.clone(), v.to_export()))
            .collect()
    }

    fn as_exports(&self) -> Option<Exports> {
        Some(self.clone())
    }
}

/// This trait is used to mark types as gettable from an [`Instance`].
///
/// [`Instance`]: crate::Instance
pub trait Exportable<'a>: Sized {
    /// This function is used when providedd the [`Extern`] as exportable, so it
    /// can be used while instantiating the [`Module`].
    ///
    /// [`Module`]: crate::Module
    fn to_export(&self) -> Export;

    /// Implementation of how to get the export corresponding to the implementing type
    /// from an [`Instance`] by name.
    ///
    /// [`Instance`]: crate::Instance
    fn get_self_from_extern(_extern: Extern) -> Result<Self, ExportError>;

    /// Convert the extern internally to hold a weak reference to the `InstanceRef`.
    /// This is useful for preventing cycles, for example for data stored in a
    /// type implementing `WasmerEnv`.
    fn into_weak_instance_ref(&mut self);
}

/// A trait for accessing exports (like [`Exportable`]) but it takes generic
/// `Args` and `Rets` parameters so that `NativeFunc` can be accessed directly
/// as well.
pub trait ExportableWithGenerics<'a, Args: WasmTypeList, Rets: WasmTypeList>: Sized {
    /// Get an export with the given generics.
    fn get_self_from_extern_with_generics(_extern: Extern) -> Result<Self, ExportError>;
    /// Convert the extern internally to hold a weak reference to the `InstanceRef`.
    /// This is useful for preventing cycles, for example for data stored in a
    /// type implementing `WasmerEnv`.
    fn into_weak_instance_ref(&mut self);
}

/// We implement it for all concrete [`Exportable`] types (that are `Clone`)
/// with empty `Args` and `Rets`.
impl<'a, T: Exportable<'a> + Clone + 'static> ExportableWithGenerics<'a, (), ()> for T {
    fn get_self_from_extern_with_generics(_extern: Extern) -> Result<Self, ExportError> {
        T::get_self_from_extern(_extern).map(|i| i.clone())
    }

    fn into_weak_instance_ref(&mut self) {
        <Self as Exportable>::into_weak_instance_ref(self);
    }
}

'''
'''--- lib/api/src/sys/externals/function.rs ---
use crate::sys::exports::{ExportError, Exportable};
use crate::sys::externals::Extern;
use crate::sys::store::Store;
use crate::sys::types::{Val, ValFuncRef};
use crate::sys::FunctionType;
use crate::sys::NativeFunc;
use crate::sys::RuntimeError;
use crate::sys::WasmerEnv;
pub use inner::{FromToNativeWasmType, HostFunction, WasmTypeList, WithEnv, WithoutEnv};

use std::cmp::max;
use std::ffi::c_void;
use std::fmt;
use std::sync::Arc;
use wasmer_vm::{
    raise_user_trap, resume_panic, wasmer_call_trampoline, Export, ExportFunction,
    ExportFunctionMetadata, ImportInitializerFuncPtr, VMCallerCheckedAnyfunc,
    VMDynamicFunctionContext, VMFuncRef, VMFunction, VMFunctionBody, VMFunctionEnvironment,
    VMFunctionKind, VMTrampoline,
};

/// A WebAssembly `function` instance.
///
/// A function instance is the runtime representation of a function.
/// It effectively is a closure of the original function (defined in either
/// the host or the WebAssembly module) over the runtime `Instance` of its
/// originating `Module`.
///
/// The module instance is used to resolve references to other definitions
/// during execution of the function.
///
/// Spec: <https://webassembly.github.io/spec/core/exec/runtime.html#function-instances>
///
/// # Panics
/// - Closures (functions with captured environments) are not currently supported
///   with native functions. Attempting to create a native `Function` with one will
///   result in a panic.
///   [Closures as host functions tracking issue](https://github.com/wasmerio/wasmer/issues/1840)
#[derive(PartialEq)]
pub struct Function {
    pub(crate) store: Store,
    pub(crate) exported: ExportFunction,
}

impl wasmer_types::WasmValueType for Function {
    /// Write the value.
    unsafe fn write_value_to(&self, p: *mut i128) {
        let func_ref =
            Val::into_vm_funcref(&Val::FuncRef(Some(self.clone())), &self.store).unwrap();
        std::ptr::write(p as *mut VMFuncRef, func_ref);
    }

    /// Read the value.
    // TODO(reftypes): this entire function should be cleaned up, `dyn Any` should
    // ideally be removed
    unsafe fn read_value_from(store: &dyn std::any::Any, p: *const i128) -> Self {
        let func_ref = std::ptr::read(p as *const VMFuncRef);
        let store = store.downcast_ref::<Store>().expect("Store expected in `Function::read_value_from`. If you see this error message it likely means you're using a function ref in a place we don't yet support it -- sorry about the inconvenience.");
        match Val::from_vm_funcref(func_ref, store) {
            Val::FuncRef(Some(fr)) => fr,
            // these bottom two cases indicate bugs in `wasmer-types` or elsewhere.
            // They should never be triggered, so we just panic.
            Val::FuncRef(None) => panic!("Null funcref found in `Function::read_value_from`!"),
            other => panic!("Invalid value in `Function::read_value_from`: {:?}", other),
        }
    }
}

fn build_export_function_metadata<Env>(
    env: Env,
    import_init_function_ptr: for<'a> fn(
        &'a mut Env,
        &'a crate::Instance,
    ) -> Result<(), crate::HostEnvInitError>,
) -> (*mut c_void, ExportFunctionMetadata)
where
    Env: Clone + Sized + 'static + Send + Sync,
{
    let import_init_function_ptr = Some(unsafe {
        std::mem::transmute::<_, ImportInitializerFuncPtr>(import_init_function_ptr)
    });
    let host_env_clone_fn = |ptr: *mut c_void| -> *mut c_void {
        let env_ref: &Env = unsafe {
            ptr.cast::<Env>()
                .as_ref()
                .expect("`ptr` to the environment is null when cloning it")
        };
        Box::into_raw(Box::new(env_ref.clone())) as _
    };
    let host_env_drop_fn = |ptr: *mut c_void| {
        unsafe { Box::from_raw(ptr.cast::<Env>()) };
    };
    let env = Box::into_raw(Box::new(env)) as _;

    // # Safety
    // - All these functions work on all threads
    // - The host env is `Send`.
    let metadata = unsafe {
        ExportFunctionMetadata::new(
            env,
            import_init_function_ptr,
            host_env_clone_fn,
            host_env_drop_fn,
        )
    };

    (env, metadata)
}

impl WasmerEnv for WithoutEnv {}

impl Function {
    /// Creates a new host `Function` (dynamic) with the provided signature.
    ///
    /// If you know the signature of the host function at compile time,
    /// consider using [`Function::new_native`] for less runtime overhead.
    ///
    /// # Examples
    ///
    /// ```
    /// # use wasmer::{Function, FunctionType, Type, Store, Value};
    /// # let store = Store::default();
    /// #
    /// let signature = FunctionType::new(vec![Type::I32, Type::I32], vec![Type::I32]);
    ///
    /// let f = Function::new(&store, &signature, |args| {
    ///     let sum = args[0].unwrap_i32() + args[1].unwrap_i32();
    ///     Ok(vec![Value::I32(sum)])
    /// });
    /// ```
    ///
    /// With constant signature:
    ///
    /// ```
    /// # use wasmer::{Function, FunctionType, Type, Store, Value};
    /// # let store = Store::default();
    /// #
    /// const I32_I32_TO_I32: ([Type; 2], [Type; 1]) = ([Type::I32, Type::I32], [Type::I32]);
    ///
    /// let f = Function::new(&store, I32_I32_TO_I32, |args| {
    ///     let sum = args[0].unwrap_i32() + args[1].unwrap_i32();
    ///     Ok(vec![Value::I32(sum)])
    /// });
    /// ```
    #[allow(clippy::cast_ptr_alignment)]
    pub fn new<FT, F>(store: &Store, ty: FT, func: F) -> Self
    where
        FT: Into<FunctionType>,
        F: Fn(&[Val]) -> Result<Vec<Val>, RuntimeError> + 'static + Send + Sync,
    {
        let wrapped_func =
            move |_env: &WithoutEnv, args: &[Val]| -> Result<Vec<Val>, RuntimeError> { func(args) };
        Self::new_with_env(store, ty, WithoutEnv, wrapped_func)
    }

    /// Creates a new host `Function` (dynamic) with the provided signature and environment.
    ///
    /// If you know the signature of the host function at compile time,
    /// consider using [`Function::new_native_with_env`] for less runtime
    /// overhead.
    ///
    /// # Examples
    ///
    /// ```
    /// # use wasmer::{Function, FunctionType, Type, Store, Value, WasmerEnv};
    /// # let store = Store::default();
    /// #
    /// #[derive(WasmerEnv, Clone)]
    /// struct Env {
    ///   multiplier: i32,
    /// };
    /// let env = Env { multiplier: 2 };
    ///
    /// let signature = FunctionType::new(vec![Type::I32, Type::I32], vec![Type::I32]);
    ///
    /// let f = Function::new_with_env(&store, &signature, env, |env, args| {
    ///     let result = env.multiplier * (args[0].unwrap_i32() + args[1].unwrap_i32());
    ///     Ok(vec![Value::I32(result)])
    /// });
    /// ```
    ///
    /// With constant signature:
    ///
    /// ```
    /// # use wasmer::{Function, FunctionType, Type, Store, Value, WasmerEnv};
    /// # let store = Store::default();
    /// const I32_I32_TO_I32: ([Type; 2], [Type; 1]) = ([Type::I32, Type::I32], [Type::I32]);
    ///
    /// #[derive(WasmerEnv, Clone)]
    /// struct Env {
    ///   multiplier: i32,
    /// };
    /// let env = Env { multiplier: 2 };
    ///
    /// let f = Function::new_with_env(&store, I32_I32_TO_I32, env, |env, args| {
    ///     let result = env.multiplier * (args[0].unwrap_i32() + args[1].unwrap_i32());
    ///     Ok(vec![Value::I32(result)])
    /// });
    /// ```
    #[allow(clippy::cast_ptr_alignment)]
    pub fn new_with_env<FT, F, Env>(store: &Store, ty: FT, env: Env, func: F) -> Self
    where
        FT: Into<FunctionType>,
        F: Fn(&Env, &[Val]) -> Result<Vec<Val>, RuntimeError> + 'static + Send + Sync,
        Env: Sized + WasmerEnv + 'static,
    {
        let ty: FunctionType = ty.into();
        let dynamic_ctx: VMDynamicFunctionContext<DynamicFunction<Env>> =
            VMDynamicFunctionContext::from_context(DynamicFunction {
                env: Box::new(env),
                func: Arc::new(func),
                store: store.clone(),
                function_type: ty.clone(),
            });

        let import_init_function_ptr: for<'a> fn(&'a mut _, &'a _) -> Result<(), _> =
            |env: &mut VMDynamicFunctionContext<DynamicFunction<Env>>,
             instance: &crate::Instance| {
                Env::init_with_instance(&mut *env.ctx.env, instance)
            };

        let (host_env, metadata) = build_export_function_metadata::<
            VMDynamicFunctionContext<DynamicFunction<Env>>,
        >(dynamic_ctx, import_init_function_ptr);

        // We don't yet have the address with the Wasm ABI signature.
        // The engine linker will replace the address with one pointing to a
        // generated dynamic trampoline.
        let address = std::ptr::null() as *const VMFunctionBody;
        let vmctx = VMFunctionEnvironment { host_env };
        let signature = store
            .engine()
            // TODO(0-copy):
            .register_signature((&ty).into());

        Self {
            store: store.clone(),
            exported: ExportFunction {
                metadata: Some(Arc::new(metadata)),
                vm_function: VMFunction {
                    address,
                    kind: VMFunctionKind::Dynamic,
                    vmctx,
                    signature,
                    call_trampoline: None,
                    instance_ref: None,
                },
            },
        }
    }

    /// Creates a new host `Function` from a native function.
    ///
    /// The function signature is automatically retrieved using the
    /// Rust typing system.
    ///
    /// # Example
    ///
    /// ```
    /// # use wasmer::{Store, Function};
    /// # let store = Store::default();
    /// #
    /// fn sum(a: i32, b: i32) -> i32 {
    ///     a + b
    /// }
    ///
    /// let f = Function::new_native(&store, sum);
    /// ```
    pub fn new_native<F, Args, Rets, Env>(store: &Store, func: F) -> Self
    where
        F: HostFunction<Args, Rets, WithoutEnv, Env>,
        Args: WasmTypeList,
        Rets: WasmTypeList,
        Env: Sized + 'static,
    {
        if std::mem::size_of::<F>() != 0 {
            Self::closures_unsupported_panic();
        }
        let function = inner::Function::<Args, Rets>::new(func);
        let address = function.address() as *const VMFunctionBody;
        let vmctx = VMFunctionEnvironment {
            host_env: std::ptr::null_mut() as *mut _,
        };
        let signature = store
            .engine()
            // TODO(0-copy):
            .register_signature((&function.ty()).into());

        Self {
            store: store.clone(),
            exported: ExportFunction {
                // TODO: figure out what's going on in this function: it takes an `Env`
                // param but also marks itself as not having an env
                metadata: None,
                vm_function: VMFunction {
                    address,
                    vmctx,
                    signature,
                    kind: VMFunctionKind::Static,
                    call_trampoline: None,
                    instance_ref: None,
                },
            },
        }
    }

    /// Creates a new host `Function` from a native function and a provided environment.
    ///
    /// The function signature is automatically retrieved using the
    /// Rust typing system.
    ///
    /// # Example
    ///
    /// ```
    /// # use wasmer::{Store, Function, WasmerEnv};
    /// # let store = Store::default();
    /// #
    /// #[derive(WasmerEnv, Clone)]
    /// struct Env {
    ///     multiplier: i32,
    /// };
    /// let env = Env { multiplier: 2 };
    ///
    /// fn sum_and_multiply(env: &Env, a: i32, b: i32) -> i32 {
    ///     (a + b) * env.multiplier
    /// }
    ///
    /// let f = Function::new_native_with_env(&store, env, sum_and_multiply);
    /// ```
    pub fn new_native_with_env<F, Args, Rets, Env>(store: &Store, env: Env, func: F) -> Self
    where
        F: HostFunction<Args, Rets, WithEnv, Env>,
        Args: WasmTypeList,
        Rets: WasmTypeList,
        Env: Sized + WasmerEnv + 'static,
    {
        if std::mem::size_of::<F>() != 0 {
            Self::closures_unsupported_panic();
        }
        let function = inner::Function::<Args, Rets>::new(func);
        let address = function.address();

        let (host_env, metadata) =
            build_export_function_metadata::<Env>(env, Env::init_with_instance);

        let vmctx = VMFunctionEnvironment { host_env };
        let signature = store.engine().register_signature((&function.ty()).into());
        Self {
            store: store.clone(),
            exported: ExportFunction {
                metadata: Some(Arc::new(metadata)),
                vm_function: VMFunction {
                    address,
                    kind: VMFunctionKind::Static,
                    vmctx,
                    signature,
                    call_trampoline: None,
                    instance_ref: None,
                },
            },
        }
    }

    /// Returns the [`FunctionType`] of the `Function`.
    ///
    /// # Example
    ///
    /// ```
    /// # use wasmer::{Function, Store, Type};
    /// # let store = Store::default();
    /// #
    /// fn sum(a: i32, b: i32) -> i32 {
    ///     a + b
    /// }
    ///
    /// let f = Function::new_native(&store, sum);
    ///
    /// assert_eq!(f.ty().params(), vec![Type::I32, Type::I32]);
    /// assert_eq!(f.ty().results(), vec![Type::I32]);
    /// ```
    pub fn ty(&self) -> FunctionType {
        self.store
            .engine()
            .lookup_signature(self.exported.vm_function.signature)
            .expect("Could not resolve VMSharedFunctionIndex! Mixing engines?")
    }

    /// Returns the [`Store`] where the `Function` belongs.
    pub fn store(&self) -> &Store {
        &self.store
    }

    fn call_wasm(
        &self,
        trampoline: VMTrampoline,
        params: &[Val],
        results: &mut [Val],
    ) -> Result<(), RuntimeError> {
        let format_types_for_error_message = |items: &[Val]| {
            items
                .iter()
                .map(|param| param.ty().to_string())
                .collect::<Vec<String>>()
                .join(", ")
        };
        let signature = self.ty();
        if signature.params().len() != params.len() {
            return Err(RuntimeError::new(format!(
                "Parameters of type [{}] did not match signature {}",
                format_types_for_error_message(params),
                &signature
            )));
        }
        if signature.results().len() != results.len() {
            return Err(RuntimeError::new(format!(
                "Results of type [{}] did not match signature {}",
                format_types_for_error_message(results),
                &signature,
            )));
        }

        let mut values_vec = vec![0; max(params.len(), results.len())];

        // Store the argument values into `values_vec`.
        let param_tys = signature.params().iter();
        for ((arg, slot), ty) in params.iter().zip(&mut values_vec).zip(param_tys) {
            if arg.ty() != *ty {
                let param_types = format_types_for_error_message(params);
                return Err(RuntimeError::new(format!(
                    "Parameters of type [{}] did not match signature {}",
                    param_types, &signature,
                )));
            }
            unsafe {
                arg.write_value_to(slot);
            }
        }

        // Call the trampoline.
        if let Err(error) = unsafe {
            wasmer_call_trampoline(
                self.exported.vm_function.vmctx,
                trampoline,
                self.exported.vm_function.address,
                values_vec.as_mut_ptr() as *mut u8,
            )
        } {
            return Err(RuntimeError::from_trap(error));
        }

        // Load the return values out of `values_vec`.
        for (index, &value_type) in signature.results().iter().enumerate() {
            unsafe {
                let ptr = values_vec.as_ptr().add(index);
                results[index] = Val::read_value_from(&self.store, ptr, value_type);
            }
        }

        Ok(())
    }

    /// Returns the number of parameters that this function takes.
    ///
    /// # Example
    ///
    /// ```
    /// # use wasmer::{Function, Store, Type};
    /// # let store = Store::default();
    /// #
    /// fn sum(a: i32, b: i32) -> i32 {
    ///     a + b
    /// }
    ///
    /// let f = Function::new_native(&store, sum);
    ///
    /// assert_eq!(f.param_arity(), 2);
    /// ```
    pub fn param_arity(&self) -> usize {
        self.ty().params().len()
    }

    /// Returns the number of results this function produces.
    ///
    /// # Example
    ///
    /// ```
    /// # use wasmer::{Function, Store, Type};
    /// # let store = Store::default();
    /// #
    /// fn sum(a: i32, b: i32) -> i32 {
    ///     a + b
    /// }
    ///
    /// let f = Function::new_native(&store, sum);
    ///
    /// assert_eq!(f.result_arity(), 1);
    /// ```
    pub fn result_arity(&self) -> usize {
        self.ty().results().len()
    }

    /// Call the `Function` function.
    ///
    /// Depending on where the Function is defined, it will call it.
    /// 1. If the function is defined inside a WebAssembly, it will call the trampoline
    ///    for the function signature.
    /// 2. If the function is defined in the host (in a native way), it will
    ///    call the trampoline.
    ///
    /// # Examples
    ///
    /// ```
    /// # use wasmer::{imports, wat2wasm, Function, Instance, Module, Store, Type, Value};
    /// # let store = Store::default();
    /// # let wasm_bytes = wat2wasm(r#"
    /// # (module
    /// #   (func (export "sum") (param $x i32) (param $y i32) (result i32)
    /// #     local.get $x
    /// #     local.get $y
    /// #     i32.add
    /// #   ))
    /// # "#.as_bytes()).unwrap();
    /// # let module = Module::new(&store, wasm_bytes).unwrap();
    /// # let import_object = imports! {};
    /// # let instance = Instance::new(&module, &import_object).unwrap();
    /// #
    /// let sum = instance.lookup_function("sum").unwrap();
    ///
    /// assert_eq!(sum.call(&[Value::I32(1), Value::I32(2)]).unwrap().to_vec(), vec![Value::I32(3)]);
    /// ```
    pub fn call(&self, params: &[Val]) -> Result<Box<[Val]>, RuntimeError> {
        // If it's a function defined in the Wasm, it will always have a call_trampoline
        if let Some(trampoline) = self.exported.vm_function.call_trampoline {
            let mut results = vec![Val::null(); self.result_arity()];
            self.call_wasm(trampoline, params, &mut results)?;
            return Ok(results.into_boxed_slice());
        }

        // If it's a function defined in the host
        match self.exported.vm_function.kind {
            VMFunctionKind::Dynamic => unsafe {
                type VMContextWithEnv = VMDynamicFunctionContext<DynamicFunction<std::ffi::c_void>>;
                let ctx = self.exported.vm_function.vmctx.host_env as *mut VMContextWithEnv;
                Ok((*ctx).ctx.call(&params)?.into_boxed_slice())
            },
            VMFunctionKind::Static => {
                unimplemented!(
                    "Native function definitions can't be directly called from the host yet"
                );
            }
        }
    }

    pub(crate) fn from_vm_export(store: &Store, wasmer_export: ExportFunction) -> Self {
        Self {
            store: store.clone(),
            exported: wasmer_export,
        }
    }

    pub(crate) fn vm_funcref(&self) -> VMFuncRef {
        let engine = self.store.engine();
        engine.register_function_metadata(VMCallerCheckedAnyfunc {
            func_ptr: self.exported.vm_function.address,
            type_index: self.exported.vm_function.signature,
            vmctx: self.exported.vm_function.vmctx,
        })
    }

    /// Transform this WebAssembly function into a function with the
    /// native ABI. See [`NativeFunc`] to learn more.
    ///
    /// # Examples
    ///
    /// ```
    /// # use wasmer::{imports, wat2wasm, Function, Instance, Module, Store, Type, Value};
    /// # let store = Store::default();
    /// # let wasm_bytes = wat2wasm(r#"
    /// # (module
    /// #   (func (export "sum") (param $x i32) (param $y i32) (result i32)
    /// #     local.get $x
    /// #     local.get $y
    /// #     i32.add
    /// #   ))
    /// # "#.as_bytes()).unwrap();
    /// # let module = Module::new(&store, wasm_bytes).unwrap();
    /// # let import_object = imports! {};
    /// # let instance = Instance::new(&module, &import_object).unwrap();
    /// #
    /// let sum = instance.lookup_function("sum").unwrap();
    /// let sum_native = sum.native::<(i32, i32), i32>().unwrap();
    ///
    /// assert_eq!(sum_native.call(1, 2).unwrap(), 3);
    /// ```
    ///
    /// # Errors
    ///
    /// If the `Args` generic parameter does not match the exported function
    /// an error will be raised:
    ///
    /// ```should_panic
    /// # use wasmer::{imports, wat2wasm, Function, Instance, Module, Store, Type, Value};
    /// # let store = Store::default();
    /// # let wasm_bytes = wat2wasm(r#"
    /// # (module
    /// #   (func (export "sum") (param $x i32) (param $y i32) (result i32)
    /// #     local.get $x
    /// #     local.get $y
    /// #     i32.add
    /// #   ))
    /// # "#.as_bytes()).unwrap();
    /// # let module = Module::new(&store, wasm_bytes).unwrap();
    /// # let import_object = imports! {};
    /// # let instance = Instance::new(&module, &import_object).unwrap();
    /// #
    /// let sum = instance.lookup_function("sum").unwrap();
    ///
    /// // This results in an error: `RuntimeError`
    /// let sum_native = sum.native::<(i64, i64), i32>().unwrap();
    /// ```
    ///
    /// If the `Rets` generic parameter does not match the exported function
    /// an error will be raised:
    ///
    /// ```should_panic
    /// # use wasmer::{imports, wat2wasm, Function, Instance, Module, Store, Type, Value};
    /// # let store = Store::default();
    /// # let wasm_bytes = wat2wasm(r#"
    /// # (module
    /// #   (func (export "sum") (param $x i32) (param $y i32) (result i32)
    /// #     local.get $x
    /// #     local.get $y
    /// #     i32.add
    /// #   ))
    /// # "#.as_bytes()).unwrap();
    /// # let module = Module::new(&store, wasm_bytes).unwrap();
    /// # let import_object = imports! {};
    /// # let instance = Instance::new(&module, &import_object).unwrap();
    /// #
    /// let sum = instance.lookup_function("sum").unwrap();
    ///
    /// // This results in an error: `RuntimeError`
    /// let sum_native = sum.native::<(i32, i32), i64>().unwrap();
    /// ```
    pub fn native<Args, Rets>(&self) -> Result<NativeFunc<Args, Rets>, RuntimeError>
    where
        Args: WasmTypeList,
        Rets: WasmTypeList,
    {
        let engine = self.store().engine();
        let signature = engine
            .lookup_signature(self.exported.vm_function.signature)
            .expect("Could not resolve VMSharedSignatureIndex! Wrong engine?");
        // type check
        let expected = signature.params();
        let given = Args::wasm_types();
        if expected != given {
            return Err(RuntimeError::new(format!(
                "types (`{:?}`) for the function arguments don't match the actual types (`{:?}`)",
                given, expected,
            )));
        }
        let expected = signature.results();
        let given = Rets::wasm_types();
        if expected != given {
            // todo: error result types don't match
            return Err(RuntimeError::new(format!(
                "types (`{:?}`) for the function results don't match the actual types (`{:?}`)",
                given, expected,
            )));
        }

        Ok(NativeFunc::new(self.store.clone(), self.exported.clone()))
    }

    #[track_caller]
    fn closures_unsupported_panic() -> ! {
        unimplemented!("Closures (functions with captured environments) are currently unsupported with native functions. See: https://github.com/wasmerio/wasmer/issues/1840")
    }

    /// Get access to the backing VM value for this extern. This function is for
    /// tests it should not be called by users of the Wasmer API.
    ///
    /// # Safety
    /// This function is unsafe to call outside of tests for the wasmer crate
    /// because there is no stability guarantee for the returned type and we may
    /// make breaking changes to it at any time or remove this method.
    #[doc(hidden)]
    pub unsafe fn get_vm_function(&self) -> &VMFunction {
        &self.exported.vm_function
    }
}

impl<'a> Exportable<'a> for Function {
    fn to_export(&self) -> Export {
        self.exported.clone().into()
    }

    fn get_self_from_extern(_extern: Extern) -> Result<Self, ExportError> {
        match _extern {
            Extern::Function(func) => Ok(func),
            _ => Err(ExportError::IncompatibleType),
        }
    }

    fn into_weak_instance_ref(&mut self) {
        self.exported
            .vm_function
            .instance_ref
            .as_mut()
            .map(|v| *v = v.downgrade());
    }
}

impl Clone for Function {
    fn clone(&self) -> Self {
        let mut exported = self.exported.clone();
        exported.vm_function.upgrade_instance_ref().unwrap();

        Self {
            store: self.store.clone(),
            exported,
        }
    }
}

impl fmt::Debug for Function {
    fn fmt(&self, formatter: &mut fmt::Formatter) -> fmt::Result {
        formatter
            .debug_struct("Function")
            .field("ty", &self.ty())
            .finish()
    }
}

/// This trait is one that all dynamic functions must fulfill.
pub(crate) trait VMDynamicFunction: Send + Sync {
    fn call(&self, args: &[Val]) -> Result<Vec<Val>, RuntimeError>;
    fn function_type(&self) -> &FunctionType;
    fn store(&self) -> &Store;
}

pub(crate) struct DynamicFunction<Env>
where
    Env: Sized + 'static + Send + Sync,
{
    function_type: FunctionType,
    #[allow(clippy::type_complexity)]
    func: Arc<dyn Fn(&Env, &[Val]) -> Result<Vec<Val>, RuntimeError> + 'static + Send + Sync>,
    store: Store,
    env: Box<Env>,
}

impl<Env: Sized + Clone + 'static + Send + Sync> Clone for DynamicFunction<Env> {
    fn clone(&self) -> Self {
        Self {
            env: self.env.clone(),
            function_type: self.function_type.clone(),
            store: self.store.clone(),
            func: self.func.clone(),
        }
    }
}

impl<Env> VMDynamicFunction for DynamicFunction<Env>
where
    Env: Sized + 'static + Send + Sync,
{
    fn call(&self, args: &[Val]) -> Result<Vec<Val>, RuntimeError> {
        (*self.func)(&*self.env, &args)
    }
    fn function_type(&self) -> &FunctionType {
        &self.function_type
    }
    fn store(&self) -> &Store {
        &self.store
    }
}

trait VMDynamicFunctionCall<T: VMDynamicFunction> {
    fn from_context(ctx: T) -> Self;
    fn address_ptr() -> *const VMFunctionBody;
    unsafe fn func_wrapper(&self, values_vec: *mut i128);
}

impl<T: VMDynamicFunction> VMDynamicFunctionCall<T> for VMDynamicFunctionContext<T> {
    fn from_context(ctx: T) -> Self {
        Self {
            address: Self::address_ptr(),
            ctx,
        }
    }

    fn address_ptr() -> *const VMFunctionBody {
        Self::func_wrapper as *const () as *const VMFunctionBody
    }

    // This function wraps our func, to make it compatible with the
    // reverse trampoline signature
    unsafe fn func_wrapper(
        // Note: we use the trick that the first param to this function is the `VMDynamicFunctionContext`
        // itself, so rather than doing `dynamic_ctx: &VMDynamicFunctionContext<T>`, we simplify it a bit
        &self,
        values_vec: *mut i128,
    ) {
        use std::panic::{self, AssertUnwindSafe};
        let result = panic::catch_unwind(AssertUnwindSafe(|| {
            let func_ty = self.ctx.function_type();
            let mut args = Vec::with_capacity(func_ty.params().len());
            let store = self.ctx.store();
            for (i, ty) in func_ty.params().iter().enumerate() {
                args.push(Val::read_value_from(store, values_vec.add(i), *ty));
            }
            let returns = self.ctx.call(&args)?;

            // We need to dynamically check that the returns
            // match the expected types, as well as expected length.
            let return_types = returns.iter().map(|ret| ret.ty()).collect::<Vec<_>>();
            if return_types != func_ty.results() {
                return Err(RuntimeError::new(format!(
                    "Dynamic function returned wrong signature. Expected {:?} but got {:?}",
                    func_ty.results(),
                    return_types
                )));
            }
            for (i, ret) in returns.iter().enumerate() {
                ret.write_value_to(values_vec.add(i));
            }
            Ok(())
        })); // We get extern ref drops at the end of this block that we don't need.
             // By preventing extern ref incs in the code above we can save the work of
             // incrementing and decrementing. However the logic as-is is correct.

        match result {
            Ok(Ok(())) => {}
            Ok(Err(trap)) => raise_user_trap(Box::new(trap)),
            Err(panic) => resume_panic(panic),
        }
    }
}

/// This private inner module contains the low-level implementation
/// for `Function` and its siblings.
mod inner {
    use std::array::TryFromSliceError;
    use std::convert::{Infallible, TryInto};
    use std::error::Error;
    use std::marker::PhantomData;
    use std::panic::{self, AssertUnwindSafe};

    #[cfg(feature = "experimental-reference-types-extern-ref")]
    pub use wasmer_types::{ExternRef, VMExternRef};
    use wasmer_types::{FunctionType, NativeWasmType, Type};
    use wasmer_vm::{raise_user_trap, resume_panic, VMFunctionBody};

    /// A trait to convert a Rust value to a `WasmNativeType` value,
    /// or to convert `WasmNativeType` value to a Rust value.
    ///
    /// This trait should ideally be split into two traits:
    /// `FromNativeWasmType` and `ToNativeWasmType` but it creates a
    /// non-negligible complexity in the `WasmTypeList`
    /// implementation.
    pub unsafe trait FromToNativeWasmType
    where
        Self: Sized,
    {
        /// Native Wasm type.
        type Native: NativeWasmType;

        /// Convert a value of kind `Self::Native` to `Self`.
        ///
        /// # Panics
        ///
        /// This method panics if `native` cannot fit in the `Self`
        /// type`.
        fn from_native(native: Self::Native) -> Self;

        /// Convert self to `Self::Native`.
        ///
        /// # Panics
        ///
        /// This method panics if `self` cannot fit in the
        /// `Self::Native` type.
        fn to_native(self) -> Self::Native;
    }

    macro_rules! from_to_native_wasm_type {
        ( $( $type:ty => $native_type:ty ),* ) => {
            $(
                #[allow(clippy::use_self)]
                unsafe impl FromToNativeWasmType for $type {
                    type Native = $native_type;

                    #[inline]
                    fn from_native(native: Self::Native) -> Self {
                        native as Self
                    }

                    #[inline]
                    fn to_native(self) -> Self::Native {
                        self as Self::Native
                    }
                }
            )*
        };
    }

    macro_rules! from_to_native_wasm_type_same_size {
        ( $( $type:ty => $native_type:ty ),* ) => {
            $(
                #[allow(clippy::use_self)]
                unsafe impl FromToNativeWasmType for $type {
                    type Native = $native_type;

                    #[inline]
                    fn from_native(native: Self::Native) -> Self {
                        Self::from_ne_bytes(Self::Native::to_ne_bytes(native))
                    }

                    #[inline]
                    fn to_native(self) -> Self::Native {
                        Self::Native::from_ne_bytes(Self::to_ne_bytes(self))
                    }
                }
            )*
        };
    }

    from_to_native_wasm_type!(
        i8 => i32,
        u8 => i32,
        i16 => i32,
        u16 => i32
    );

    from_to_native_wasm_type_same_size!(
        i32 => i32,
        u32 => i32,
        i64 => i64,
        u64 => i64,
        f32 => f32,
        f64 => f64
    );

    #[cfg(feature = "experimental-reference-types-extern-ref")]
    unsafe impl FromToNativeWasmType for ExternRef {
        type Native = VMExternRef;

        fn to_native(self) -> Self::Native {
            self.into()
        }
        fn from_native(n: Self::Native) -> Self {
            n.into()
        }
    }

    #[cfg(test)]
    mod test_from_to_native_wasm_type {
        use super::*;

        #[test]
        fn test_to_native() {
            assert_eq!(7i8.to_native(), 7i32);
            assert_eq!(7u8.to_native(), 7i32);
            assert_eq!(7i16.to_native(), 7i32);
            assert_eq!(7u16.to_native(), 7i32);
            assert_eq!(u32::MAX.to_native(), -1);
        }

        #[test]
        fn test_to_native_same_size() {
            assert_eq!(7i32.to_native(), 7i32);
            assert_eq!(7u32.to_native(), 7i32);
            assert_eq!(7i64.to_native(), 7i64);
            assert_eq!(7u64.to_native(), 7i64);
            assert_eq!(7f32.to_native(), 7f32);
            assert_eq!(7f64.to_native(), 7f64);
        }
    }

    /// The `WasmTypeList` trait represents a tuple (list) of Wasm
    /// typed values. It is used to get low-level representation of
    /// such a tuple.
    pub trait WasmTypeList
    where
        Self: Sized,
    {
        /// The C type (a struct) that can hold/represent all the
        /// represented values.
        type CStruct;

        /// The array type that can hold all the represented values.
        ///
        /// Note that all values are stored in their binary form.
        type Array: AsMut<[i128]>;

        /// Constructs `Self` based on an array of values.
        fn from_array(array: Self::Array) -> Self;

        /// Constructs `Self` based on a slice of values.
        ///
        /// `from_slice` returns a `Result` because it is possible
        /// that the slice doesn't have the same size than
        /// `Self::Array`, in which circumstance an error of kind
        /// `TryFromSliceError` will be returned.
        fn from_slice(slice: &[i128]) -> Result<Self, TryFromSliceError>;

        /// Builds and returns an array of type `Array` from a tuple
        /// (list) of values.
        fn into_array(self) -> Self::Array;

        /// Allocates and return an empty array of type `Array` that
        /// will hold a tuple (list) of values, usually to hold the
        /// returned values of a WebAssembly function call.
        fn empty_array() -> Self::Array;

        /// Builds a tuple (list) of values from a C struct of type
        /// `CStruct`.
        fn from_c_struct(c_struct: Self::CStruct) -> Self;

        /// Builds and returns a C struct of type `CStruct` from a
        /// tuple (list) of values.
        fn into_c_struct(self) -> Self::CStruct;

        /// Get the Wasm types for the tuple (list) of currently
        /// represented values.
        fn wasm_types() -> &'static [Type];
    }

    /// The `IntoResult` trait turns a `WasmTypeList` into a
    /// `Result<WasmTypeList, Self::Error>`.
    ///
    /// It is mostly used to turn result values of a Wasm function
    /// call into a `Result`.
    pub trait IntoResult<T>
    where
        T: WasmTypeList,
    {
        /// The error type for this trait.
        type Error: Error + Sync + Send + 'static;

        /// Transforms `Self` into a `Result`.
        fn into_result(self) -> Result<T, Self::Error>;
    }

    impl<T> IntoResult<T> for T
    where
        T: WasmTypeList,
    {
        // `T` is not a `Result`, it's already a value, so no error
        // can be built.
        type Error = Infallible;

        fn into_result(self) -> Result<Self, Infallible> {
            Ok(self)
        }
    }

    impl<T, E> IntoResult<T> for Result<T, E>
    where
        T: WasmTypeList,
        E: Error + Sync + Send + 'static,
    {
        type Error = E;

        fn into_result(self) -> Self {
            self
        }
    }

    #[cfg(test)]
    mod test_into_result {
        use super::*;
        use std::convert::Infallible;

        #[test]
        fn test_into_result_over_t() {
            let x: i32 = 42;
            let result_of_x: Result<i32, Infallible> = x.into_result();

            assert_eq!(result_of_x.unwrap(), x);
        }

        #[test]
        fn test_into_result_over_result() {
            {
                let x: Result<i32, Infallible> = Ok(42);
                let result_of_x: Result<i32, Infallible> = x.into_result();

                assert_eq!(result_of_x, x);
            }

            {
                use std::{error, fmt};

                #[derive(Debug, PartialEq)]
                struct E;

                impl fmt::Display for E {
                    fn fmt(&self, formatter: &mut fmt::Formatter<'_>) -> fmt::Result {
                        write!(formatter, "E")
                    }
                }

                impl error::Error for E {}

                let x: Result<Infallible, E> = Err(E);
                let result_of_x: Result<Infallible, E> = x.into_result();

                assert_eq!(result_of_x.unwrap_err(), E);
            }
        }
    }

    /// The `HostFunction` trait represents the set of functions that
    /// can be used as host function. To uphold this statement, it is
    /// necessary for a function to be transformed into a pointer to
    /// `VMFunctionBody`.
    pub trait HostFunction<Args, Rets, Kind, T>
    where
        Args: WasmTypeList,
        Rets: WasmTypeList,
        Kind: HostFunctionKind,
        T: Sized,
        Self: Sized,
    {
        /// Get the pointer to the function body.
        fn function_body_ptr(self) -> *const VMFunctionBody;
    }

    /// Empty trait to specify the kind of `HostFunction`: With or
    /// without an environment.
    ///
    /// This trait is never aimed to be used by a user. It is used by
    /// the trait system to automatically generate the appropriate
    /// host functions.
    #[doc(hidden)]
    pub trait HostFunctionKind {}

    /// An empty struct to help Rust typing to determine
    /// when a `HostFunction` does have an environment.
    pub struct WithEnv;

    impl HostFunctionKind for WithEnv {}

    /// An empty struct to help Rust typing to determine
    /// when a `HostFunction` does not have an environment.
    #[derive(Clone)]
    pub struct WithoutEnv;

    impl HostFunctionKind for WithoutEnv {}

    /// Represents a low-level Wasm static host function. See
    /// `super::Function::new` and `super::Function::new_env` to learn
    /// more.
    #[derive(Clone, Debug, Hash, PartialEq, Eq)]
    pub struct Function<Args = (), Rets = ()> {
        address: *const VMFunctionBody,
        _phantom: PhantomData<(Args, Rets)>,
    }

    unsafe impl<Args, Rets> Send for Function<Args, Rets> {}

    impl<Args, Rets> Function<Args, Rets>
    where
        Args: WasmTypeList,
        Rets: WasmTypeList,
    {
        /// Creates a new `Function`.
        pub fn new<F, T, E>(function: F) -> Self
        where
            F: HostFunction<Args, Rets, T, E>,
            T: HostFunctionKind,
            E: Sized,
        {
            Self {
                address: function.function_body_ptr(),
                _phantom: PhantomData,
            }
        }

        /// Get the function type of this `Function`.
        pub fn ty(&self) -> FunctionType {
            FunctionType::new(Args::wasm_types(), Rets::wasm_types())
        }

        /// Get the address of this `Function`.
        pub fn address(&self) -> *const VMFunctionBody {
            self.address
        }
    }

    macro_rules! impl_host_function {
        ( [$c_struct_representation:ident]
           $c_struct_name:ident,
           $( $x:ident ),* ) => {

            /// A structure with a C-compatible representation that can hold a set of Wasm values.
            /// This type is used by `WasmTypeList::CStruct`.
            #[repr($c_struct_representation)]
            pub struct $c_struct_name< $( $x ),* > ( $( <$x as FromToNativeWasmType>::Native ),* )
            where
                $( $x: FromToNativeWasmType ),*;

            // Implement `WasmTypeList` for a specific tuple.
            #[allow(unused_parens, dead_code)]
            impl< $( $x ),* >
                WasmTypeList
            for
                ( $( $x ),* )
            where
                $( $x: FromToNativeWasmType ),*
            {
                type CStruct = $c_struct_name< $( $x ),* >;

                type Array = [i128; count_idents!( $( $x ),* )];

                fn from_array(array: Self::Array) -> Self {
                    // Unpack items of the array.
                    #[allow(non_snake_case)]
                    let [ $( $x ),* ] = array;

                    // Build the tuple.
                    (
                        $(
                            FromToNativeWasmType::from_native(NativeWasmType::from_binary($x))
                        ),*
                    )
                }

                fn from_slice(slice: &[i128]) -> Result<Self, TryFromSliceError> {
                    Ok(Self::from_array(slice.try_into()?))
                }

                fn into_array(self) -> Self::Array {
                    // Unpack items of the tuple.
                    #[allow(non_snake_case)]
                    let ( $( $x ),* ) = self;

                    // Build the array.
                    [
                        $(
                            FromToNativeWasmType::to_native($x).to_binary()
                        ),*
                    ]
                }

                fn empty_array() -> Self::Array {
                    // Build an array initialized with `0`.
                    [0; count_idents!( $( $x ),* )]
                }

                fn from_c_struct(c_struct: Self::CStruct) -> Self {
                    // Unpack items of the C structure.
                    #[allow(non_snake_case)]
                    let $c_struct_name( $( $x ),* ) = c_struct;

                    (
                        $(
                            FromToNativeWasmType::from_native($x)
                        ),*
                    )
                }

                #[allow(unused_parens, non_snake_case)]
                fn into_c_struct(self) -> Self::CStruct {
                    // Unpack items of the tuple.
                    let ( $( $x ),* ) = self;

                    // Build the C structure.
                    $c_struct_name(
                        $(
                            FromToNativeWasmType::to_native($x)
                        ),*
                    )
                }

                fn wasm_types() -> &'static [Type] {
                    &[
                        $(
                            $x::Native::WASM_TYPE
                        ),*
                    ]
                }
            }

            // Implement `HostFunction` for a function that has the same arity than the tuple.
            // This specific function has no environment.
            #[allow(unused_parens)]
            impl< $( $x, )* Rets, RetsAsResult, Func >
                HostFunction<( $( $x ),* ), Rets, WithoutEnv, ()>
            for
                Func
            where
                $( $x: FromToNativeWasmType, )*
                Rets: WasmTypeList,
                RetsAsResult: IntoResult<Rets>,
                Func: Fn($( $x , )*) -> RetsAsResult + 'static + Send,
            {
                #[allow(non_snake_case)]
                fn function_body_ptr(self) -> *const VMFunctionBody {
                    /// This is a function that wraps the real host
                    /// function. Its address will be used inside the
                    /// runtime.
                    extern fn func_wrapper<$( $x, )* Rets, RetsAsResult, Func>( _: usize, $( $x: $x::Native, )* ) -> Rets::CStruct
                    where
                        $( $x: FromToNativeWasmType, )*
                        Rets: WasmTypeList,
                        RetsAsResult: IntoResult<Rets>,
                        Func: Fn( $( $x ),* ) -> RetsAsResult + 'static
                    {
                        let func: &Func = unsafe { &*(&() as *const () as *const Func) };
                        let result = panic::catch_unwind(AssertUnwindSafe(|| {
                            func( $( FromToNativeWasmType::from_native($x) ),* ).into_result()
                        }));

                        match result {
                            Ok(Ok(result)) => return result.into_c_struct(),
                            Ok(Err(trap)) => unsafe { raise_user_trap(Box::new(trap)) },
                            Err(panic) => unsafe { resume_panic(panic) },
                        }
                    }

                    func_wrapper::< $( $x, )* Rets, RetsAsResult, Self > as *const VMFunctionBody
                }
            }

            // Implement `HostFunction` for a function that has the same arity than the tuple.
            // This specific function has an environment.
            #[allow(unused_parens)]
            impl< $( $x, )* Rets, RetsAsResult, Env, Func >
                HostFunction<( $( $x ),* ), Rets, WithEnv, Env>
            for
                Func
            where
                $( $x: FromToNativeWasmType, )*
                Rets: WasmTypeList,
                RetsAsResult: IntoResult<Rets>,
                Env: Sized,
                Func: Fn(&Env, $( $x , )*) -> RetsAsResult + Send + 'static,
            {
                #[allow(non_snake_case)]
                fn function_body_ptr(self) -> *const VMFunctionBody {
                    /// This is a function that wraps the real host
                    /// function. Its address will be used inside the
                    /// runtime.
                    extern fn func_wrapper<$( $x, )* Rets, RetsAsResult, Env, Func>( env: &Env, $( $x: $x::Native, )* ) -> Rets::CStruct
                    where
                        $( $x: FromToNativeWasmType, )*
                        Rets: WasmTypeList,
                        RetsAsResult: IntoResult<Rets>,
                        Env: Sized,
                        Func: Fn(&Env, $( $x ),* ) -> RetsAsResult + 'static
                    {
                        let func: &Func = unsafe { &*(&() as *const () as *const Func) };

                        let result = panic::catch_unwind(AssertUnwindSafe(|| {
                            func(env, $( FromToNativeWasmType::from_native($x) ),* ).into_result()
                        }));

                        match result {
                            Ok(Ok(result)) => return result.into_c_struct(),
                            Ok(Err(trap)) => unsafe { raise_user_trap(Box::new(trap)) },
                            Err(panic) => unsafe { resume_panic(panic) },
                        }
                    }

                    func_wrapper::< $( $x, )* Rets, RetsAsResult, Env, Self > as *const VMFunctionBody
                }
            }
        };
    }

    // Black-magic to count the number of identifiers at compile-time.
    macro_rules! count_idents {
        ( $($idents:ident),* ) => {
            {
                #[allow(dead_code, non_camel_case_types)]
                enum Idents { $( $idents, )* __CountIdentsLast }
                const COUNT: usize = Idents::__CountIdentsLast as usize;
                COUNT
            }
        };
    }

    // Here we go! Let's generate all the C struct, `WasmTypeList`
    // implementations and `HostFunction` implementations.
    impl_host_function!([C] S0,);
    impl_host_function!([transparent] S1, A1);
    impl_host_function!([C] S2, A1, A2);
    impl_host_function!([C] S3, A1, A2, A3);
    impl_host_function!([C] S4, A1, A2, A3, A4);
    impl_host_function!([C] S5, A1, A2, A3, A4, A5);
    impl_host_function!([C] S6, A1, A2, A3, A4, A5, A6);
    impl_host_function!([C] S7, A1, A2, A3, A4, A5, A6, A7);
    impl_host_function!([C] S8, A1, A2, A3, A4, A5, A6, A7, A8);
    impl_host_function!([C] S9, A1, A2, A3, A4, A5, A6, A7, A8, A9);
    impl_host_function!([C] S10, A1, A2, A3, A4, A5, A6, A7, A8, A9, A10);
    impl_host_function!([C] S11, A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11);
    impl_host_function!([C] S12, A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12);
    impl_host_function!([C] S13, A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12, A13);
    impl_host_function!([C] S14, A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12, A13, A14);
    impl_host_function!([C] S15, A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12, A13, A14, A15);
    impl_host_function!([C] S16, A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12, A13, A14, A15, A16);
    impl_host_function!([C] S17, A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12, A13, A14, A15, A16, A17);
    impl_host_function!([C] S18, A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12, A13, A14, A15, A16, A17, A18);
    impl_host_function!([C] S19, A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12, A13, A14, A15, A16, A17, A18, A19);
    impl_host_function!([C] S20, A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12, A13, A14, A15, A16, A17, A18, A19, A20);
    impl_host_function!([C] S21, A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12, A13, A14, A15, A16, A17, A18, A19, A20, A21);
    impl_host_function!([C] S22, A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12, A13, A14, A15, A16, A17, A18, A19, A20, A21, A22);
    impl_host_function!([C] S23, A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12, A13, A14, A15, A16, A17, A18, A19, A20, A21, A22, A23);
    impl_host_function!([C] S24, A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12, A13, A14, A15, A16, A17, A18, A19, A20, A21, A22, A23, A24);
    impl_host_function!([C] S25, A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12, A13, A14, A15, A16, A17, A18, A19, A20, A21, A22, A23, A24, A25);
    impl_host_function!([C] S26, A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12, A13, A14, A15, A16, A17, A18, A19, A20, A21, A22, A23, A24, A25, A26);

    // Implement `WasmTypeList` on `Infallible`, which means that
    // `Infallible` can be used as a returned type of a host function
    // to express that it doesn't return, or to express that it cannot
    // fail (with `Result<_, Infallible>`).
    impl WasmTypeList for Infallible {
        type CStruct = Self;
        type Array = [i128; 0];

        fn from_array(_: Self::Array) -> Self {
            unreachable!()
        }

        fn from_slice(_: &[i128]) -> Result<Self, TryFromSliceError> {
            unreachable!()
        }

        fn into_array(self) -> Self::Array {
            []
        }

        fn empty_array() -> Self::Array {
            unreachable!()
        }

        fn from_c_struct(_: Self::CStruct) -> Self {
            unreachable!()
        }

        fn into_c_struct(self) -> Self::CStruct {
            unreachable!()
        }

        fn wasm_types() -> &'static [Type] {
            &[]
        }
    }

    #[cfg(test)]
    mod test_wasm_type_list {
        use super::*;
        use wasmer_types::Type;

        #[test]
        fn test_from_array() {
            assert_eq!(<()>::from_array([]), ());
            assert_eq!(<i32>::from_array([1]), (1i32));
            assert_eq!(<(i32, i64)>::from_array([1, 2]), (1i32, 2i64));
            assert_eq!(
                <(i32, i64, f32, f64)>::from_array([
                    1,
                    2,
                    (3.1f32).to_bits().into(),
                    (4.2f64).to_bits().into()
                ]),
                (1, 2, 3.1f32, 4.2f64)
            );
        }

        #[test]
        fn test_into_array() {
            assert_eq!(().into_array(), [0i128; 0]);
            assert_eq!((1).into_array(), [1]);
            assert_eq!((1i32, 2i64).into_array(), [1, 2]);
            assert_eq!(
                (1i32, 2i32, 3.1f32, 4.2f64).into_array(),
                [1, 2, (3.1f32).to_bits().into(), (4.2f64).to_bits().into()]
            );
        }

        #[test]
        fn test_empty_array() {
            assert_eq!(<()>::empty_array().len(), 0);
            assert_eq!(<i32>::empty_array().len(), 1);
            assert_eq!(<(i32, i64)>::empty_array().len(), 2);
        }

        #[test]
        fn test_from_c_struct() {
            assert_eq!(<()>::from_c_struct(S0()), ());
            assert_eq!(<i32>::from_c_struct(S1(1)), (1i32));
            assert_eq!(<(i32, i64)>::from_c_struct(S2(1, 2)), (1i32, 2i64));
            assert_eq!(
                <(i32, i64, f32, f64)>::from_c_struct(S4(1, 2, 3.1, 4.2)),
                (1i32, 2i64, 3.1f32, 4.2f64)
            );
        }

        #[test]
        fn test_wasm_types_for_uni_values() {
            assert_eq!(<i32>::wasm_types(), [Type::I32]);
            assert_eq!(<i64>::wasm_types(), [Type::I64]);
            assert_eq!(<f32>::wasm_types(), [Type::F32]);
            assert_eq!(<f64>::wasm_types(), [Type::F64]);
        }

        #[test]
        fn test_wasm_types_for_multi_values() {
            assert_eq!(<(i32, i32)>::wasm_types(), [Type::I32, Type::I32]);
            assert_eq!(<(i64, i64)>::wasm_types(), [Type::I64, Type::I64]);
            assert_eq!(<(f32, f32)>::wasm_types(), [Type::F32, Type::F32]);
            assert_eq!(<(f64, f64)>::wasm_types(), [Type::F64, Type::F64]);

            assert_eq!(
                <(i32, i64, f32, f64)>::wasm_types(),
                [Type::I32, Type::I64, Type::F32, Type::F64]
            );
        }
    }

    #[allow(non_snake_case)]
    #[cfg(test)]
    mod test_function {
        use super::*;
        use wasmer_types::Type;

        fn func() {}
        fn func__i32() -> i32 {
            0
        }
        fn func_i32(_a: i32) {}
        fn func_i32__i32(a: i32) -> i32 {
            a * 2
        }
        fn func_i32_i32__i32(a: i32, b: i32) -> i32 {
            a + b
        }
        fn func_i32_i32__i32_i32(a: i32, b: i32) -> (i32, i32) {
            (a, b)
        }
        fn func_f32_i32__i32_f32(a: f32, b: i32) -> (i32, f32) {
            (b, a)
        }

        #[test]
        fn test_function_types() {
            assert_eq!(Function::new(func).ty(), FunctionType::new(vec![], vec![]));
            assert_eq!(
                Function::new(func__i32).ty(),
                FunctionType::new(vec![], vec![Type::I32])
            );
            assert_eq!(
                Function::new(func_i32).ty(),
                FunctionType::new(vec![Type::I32], vec![])
            );
            assert_eq!(
                Function::new(func_i32__i32).ty(),
                FunctionType::new(vec![Type::I32], vec![Type::I32])
            );
            assert_eq!(
                Function::new(func_i32_i32__i32).ty(),
                FunctionType::new(vec![Type::I32, Type::I32], vec![Type::I32])
            );
            assert_eq!(
                Function::new(func_i32_i32__i32_i32).ty(),
                FunctionType::new(vec![Type::I32, Type::I32], vec![Type::I32, Type::I32])
            );
            assert_eq!(
                Function::new(func_f32_i32__i32_f32).ty(),
                FunctionType::new(vec![Type::F32, Type::I32], vec![Type::I32, Type::F32])
            );
        }

        #[test]
        fn test_function_pointer() {
            let f = Function::new(func_i32__i32);
            let function = unsafe { std::mem::transmute::<_, fn(usize, i32) -> i32>(f.address) };
            assert_eq!(function(0, 3), 6);
        }
    }
}

'''
'''--- lib/api/src/sys/externals/global.rs ---
use crate::sys::exports::{ExportError, Exportable};
use crate::sys::externals::Extern;
use crate::sys::store::{Store, StoreObject};
use crate::sys::types::Val;
use crate::sys::GlobalType;
use crate::sys::Mutability;
use crate::sys::RuntimeError;
use std::fmt;
use std::sync::Arc;
use wasmer_vm::{Export, Global as RuntimeGlobal, VMGlobal};

/// A WebAssembly `global` instance.
///
/// A global instance is the runtime representation of a global variable.
/// It consists of an individual value and a flag indicating whether it is mutable.
///
/// Spec: <https://webassembly.github.io/spec/core/exec/runtime.html#global-instances>
pub struct Global {
    store: Store,
    vm_global: VMGlobal,
}

impl Global {
    /// Create a new `Global` with the initial value [`Val`].
    ///
    /// # Example
    ///
    /// ```
    /// # use wasmer::{Global, Mutability, Store, Value};
    /// # let store = Store::default();
    /// #
    /// let g = Global::new(&store, Value::I32(1));
    ///
    /// assert_eq!(g.get(), Value::I32(1));
    /// assert_eq!(g.ty().mutability, Mutability::Const);
    /// ```
    pub fn new(store: &Store, val: Val) -> Self {
        Self::from_value(store, val, Mutability::Const).unwrap()
    }

    /// Create a mutable `Global` with the initial value [`Val`].
    ///
    /// # Example
    ///
    /// ```
    /// # use wasmer::{Global, Mutability, Store, Value};
    /// # let store = Store::default();
    /// #
    /// let g = Global::new_mut(&store, Value::I32(1));
    ///
    /// assert_eq!(g.get(), Value::I32(1));
    /// assert_eq!(g.ty().mutability, Mutability::Var);
    /// ```
    pub fn new_mut(store: &Store, val: Val) -> Self {
        Self::from_value(store, val, Mutability::Var).unwrap()
    }

    /// Create a `Global` with the initial value [`Val`] and the provided [`Mutability`].
    fn from_value(store: &Store, val: Val, mutability: Mutability) -> Result<Self, RuntimeError> {
        if !val.comes_from_same_store(store) {
            return Err(RuntimeError::new("cross-`Store` globals are not supported"));
        }
        let global = RuntimeGlobal::new(GlobalType {
            mutability,
            ty: val.ty(),
        });
        unsafe {
            global
                .set_unchecked(val.clone())
                .map_err(|e| RuntimeError::new(format!("create global for {:?}: {}", val, e)))?;
        };

        Ok(Self {
            store: store.clone(),
            vm_global: VMGlobal {
                from: Arc::new(global),
                instance_ref: None,
            },
        })
    }

    /// Returns the [`GlobalType`] of the `Global`.
    ///
    /// # Example
    ///
    /// ```
    /// # use wasmer::{Global, Mutability, Store, Type, Value, GlobalType};
    /// # let store = Store::default();
    /// #
    /// let c = Global::new(&store, Value::I32(1));
    /// let v = Global::new_mut(&store, Value::I64(1));
    ///
    /// assert_eq!(c.ty(), &GlobalType::new(Type::I32, Mutability::Const));
    /// assert_eq!(v.ty(), &GlobalType::new(Type::I64, Mutability::Var));
    /// ```
    pub fn ty(&self) -> &GlobalType {
        self.vm_global.from.ty()
    }

    /// Returns the [`Store`] where the `Global` belongs.
    ///
    /// # Example
    ///
    /// ```
    /// # use wasmer::{Global, Store, Value};
    /// # let store = Store::default();
    /// #
    /// let g = Global::new(&store, Value::I32(1));
    ///
    /// assert_eq!(g.store(), &store);
    /// ```
    pub fn store(&self) -> &Store {
        &self.store
    }

    /// Retrieves the current value [`Val`] that the Global has.
    ///
    /// # Example
    ///
    /// ```
    /// # use wasmer::{Global, Store, Value};
    /// # let store = Store::default();
    /// #
    /// let g = Global::new(&store, Value::I32(1));
    ///
    /// assert_eq!(g.get(), Value::I32(1));
    /// ```
    pub fn get(&self) -> Val {
        self.vm_global.from.get(&self.store)
    }

    /// Sets a custom value [`Val`] to the runtime Global.
    ///
    /// # Example
    ///
    /// ```
    /// # use wasmer::{Global, Store, Value};
    /// # let store = Store::default();
    /// #
    /// let g = Global::new_mut(&store, Value::I32(1));
    ///
    /// assert_eq!(g.get(), Value::I32(1));
    ///
    /// g.set(Value::I32(2));
    ///
    /// assert_eq!(g.get(), Value::I32(2));
    /// ```
    ///
    /// # Errors
    ///
    /// Trying to mutate a immutable global will raise an error:
    ///
    /// ```should_panic
    /// # use wasmer::{Global, Store, Value};
    /// # let store = Store::default();
    /// #
    /// let g = Global::new(&store, Value::I32(1));
    ///
    /// g.set(Value::I32(2)).unwrap();
    /// ```
    ///
    /// Trying to set a value of a incompatible type will raise an error:
    ///
    /// ```should_panic
    /// # use wasmer::{Global, Store, Value};
    /// # let store = Store::default();
    /// #
    /// let g = Global::new(&store, Value::I32(1));
    ///
    /// // This results in an error: `RuntimeError`.
    /// g.set(Value::I64(2)).unwrap();
    /// ```
    pub fn set(&self, val: Val) -> Result<(), RuntimeError> {
        if !val.comes_from_same_store(&self.store) {
            return Err(RuntimeError::new("cross-`Store` values are not supported"));
        }
        unsafe {
            self.vm_global
                .from
                .set(val)
                .map_err(|e| RuntimeError::new(format!("{}", e)))?;
        }
        Ok(())
    }

    pub(crate) fn from_vm_export(store: &Store, vm_global: VMGlobal) -> Self {
        Self {
            store: store.clone(),
            vm_global,
        }
    }

    /// Returns whether or not these two globals refer to the same data.
    ///
    /// # Example
    ///
    /// ```
    /// # use wasmer::{Global, Store, Value};
    /// # let store = Store::default();
    /// #
    /// let g = Global::new(&store, Value::I32(1));
    ///
    /// assert!(g.same(&g));
    /// ```
    pub fn same(&self, other: &Self) -> bool {
        Arc::ptr_eq(&self.vm_global.from, &other.vm_global.from)
    }

    /// Get access to the backing VM value for this extern. This function is for
    /// tests it should not be called by users of the Wasmer API.
    ///
    /// # Safety
    /// This function is unsafe to call outside of tests for the wasmer crate
    /// because there is no stability guarantee for the returned type and we may
    /// make breaking changes to it at any time or remove this method.
    #[doc(hidden)]
    pub unsafe fn get_vm_global(&self) -> &VMGlobal {
        &self.vm_global
    }
}

impl Clone for Global {
    fn clone(&self) -> Self {
        let mut vm_global = self.vm_global.clone();
        vm_global.upgrade_instance_ref().unwrap();

        Self {
            store: self.store.clone(),
            vm_global,
        }
    }
}

impl fmt::Debug for Global {
    fn fmt(&self, formatter: &mut fmt::Formatter) -> fmt::Result {
        formatter
            .debug_struct("Global")
            .field("ty", &self.ty())
            .field("value", &self.get())
            .finish()
    }
}

impl<'a> Exportable<'a> for Global {
    fn to_export(&self) -> Export {
        self.vm_global.clone().into()
    }

    fn get_self_from_extern(_extern: Extern) -> Result<Self, ExportError> {
        match _extern {
            Extern::Global(global) => Ok(global),
            _ => Err(ExportError::IncompatibleType),
        }
    }

    fn into_weak_instance_ref(&mut self) {
        self.vm_global
            .instance_ref
            .as_mut()
            .map(|v| *v = v.downgrade());
    }
}

'''
'''--- lib/api/src/sys/externals/memory.rs ---
use crate::sys::exports::{ExportError, Exportable};
use crate::sys::externals::Extern;
use crate::sys::store::Store;
use crate::sys::{MemoryType, MemoryView};
use std::convert::TryInto;
use std::slice;
use std::sync::Arc;
use wasmer_types::{Pages, ValueType};
use wasmer_vm::{Export, MemoryError, VMMemory};

/// A WebAssembly `memory` instance.
///
/// A memory instance is the runtime representation of a linear memory.
/// It consists of a vector of bytes and an optional maximum size.
///
/// The length of the vector always is a multiple of the WebAssembly
/// page size, which is defined to be the constant 65536 ‚Äì abbreviated 64Ki.
/// Like in a memory type, the maximum size in a memory instance is
/// given in units of this page size.
///
/// A memory created by the host or in WebAssembly code will be accessible and
/// mutable from both host and WebAssembly.
///
/// Spec: <https://webassembly.github.io/spec/core/exec/runtime.html#memory-instances>
#[derive(Debug)]
pub struct Memory {
    store: Store,
    vm_memory: VMMemory,
}

impl Memory {
    /// Creates a new host `Memory` from the provided [`MemoryType`].
    ///
    /// This function will construct the `Memory` using the store
    /// [`BaseTunables`][crate::sys::BaseTunables].
    ///
    /// # Example
    ///
    /// ```
    /// # use wasmer::{Memory, MemoryType, Pages, Store, Type, Value};
    /// # let store = Store::default();
    /// #
    /// let m = Memory::new(&store, MemoryType::new(1, None, false)).unwrap();
    /// ```
    pub fn new(store: &Store, ty: MemoryType) -> Result<Self, MemoryError> {
        let tunables = store.tunables();
        let style = tunables.memory_style(&ty);
        let memory = tunables.create_host_memory(&ty, &style)?;

        Ok(Self {
            store: store.clone(),
            vm_memory: VMMemory {
                from: memory,
                // We are creating it from the host, and therefore there is no
                // associated instance with this memory
                instance_ref: None,
            },
        })
    }

    /// Returns the [`MemoryType`] of the `Memory`.
    ///
    /// # Example
    ///
    /// ```
    /// # use wasmer::{Memory, MemoryType, Pages, Store, Type, Value};
    /// # let store = Store::default();
    /// #
    /// let mt = MemoryType::new(1, None, false);
    /// let m = Memory::new(&store, mt).unwrap();
    ///
    /// assert_eq!(m.ty(), mt);
    /// ```
    pub fn ty(&self) -> MemoryType {
        self.vm_memory.from.ty()
    }

    /// Returns the [`Store`] where the `Memory` belongs.
    ///
    /// # Example
    ///
    /// ```
    /// # use wasmer::{Memory, MemoryType, Pages, Store, Type, Value};
    /// # let store = Store::default();
    /// #
    /// let m = Memory::new(&store, MemoryType::new(1, None, false)).unwrap();
    ///
    /// assert_eq!(m.store(), &store);
    /// ```
    pub fn store(&self) -> &Store {
        &self.store
    }

    /// Retrieve a slice of the memory contents.
    ///
    /// # Safety
    ///
    /// Until the returned slice is dropped, it is undefined behaviour to
    /// modify the memory contents in any way including by calling a wasm
    /// function that writes to the memory or by resizing the memory.
    pub unsafe fn data_unchecked(&self) -> &[u8] {
        self.data_unchecked_mut()
    }

    /// Retrieve a mutable slice of the memory contents.
    ///
    /// # Safety
    ///
    /// This method provides interior mutability without an UnsafeCell. Until
    /// the returned value is dropped, it is undefined behaviour to read or
    /// write to the pointed-to memory in any way except through this slice,
    /// including by calling a wasm function that reads the memory contents or
    /// by resizing this Memory.
    #[allow(clippy::mut_from_ref)]
    pub unsafe fn data_unchecked_mut(&self) -> &mut [u8] {
        let definition = self.vm_memory.from.vmmemory();
        let def = definition.as_ref();
        slice::from_raw_parts_mut(def.base, def.current_length.try_into().unwrap())
    }

    /// Returns the pointer to the raw bytes of the `Memory`.
    pub fn data_ptr(&self) -> *mut u8 {
        let definition = self.vm_memory.from.vmmemory();
        let def = unsafe { definition.as_ref() };
        def.base
    }

    /// Returns the size (in bytes) of the `Memory`.
    pub fn data_size(&self) -> u64 {
        let definition = self.vm_memory.from.vmmemory();
        let def = unsafe { definition.as_ref() };
        def.current_length.try_into().unwrap()
    }

    /// Returns the size (in [`Pages`]) of the `Memory`.
    ///
    /// # Example
    ///
    /// ```
    /// # use wasmer::{Memory, MemoryType, Pages, Store, Type, Value};
    /// # let store = Store::default();
    /// #
    /// let m = Memory::new(&store, MemoryType::new(1, None, false)).unwrap();
    ///
    /// assert_eq!(m.size(), Pages(1));
    /// ```
    pub fn size(&self) -> Pages {
        self.vm_memory.from.size()
    }

    /// Grow memory by the specified amount of WebAssembly [`Pages`] and return
    /// the previous memory size.
    ///
    /// # Example
    ///
    /// ```
    /// # use wasmer::{Memory, MemoryType, Pages, Store, Type, Value, WASM_MAX_PAGES};
    /// # let store = Store::default();
    /// #
    /// let m = Memory::new(&store, MemoryType::new(1, Some(3), false)).unwrap();
    /// let p = m.grow(2).unwrap();
    ///
    /// assert_eq!(p, Pages(1));
    /// assert_eq!(m.size(), Pages(3));
    /// ```
    ///
    /// # Errors
    ///
    /// Returns an error if memory can't be grown by the specified amount
    /// of pages.
    ///
    /// ```should_panic
    /// # use wasmer::{Memory, MemoryType, Pages, Store, Type, Value, WASM_MAX_PAGES};
    /// # let store = Store::default();
    /// #
    /// let m = Memory::new(&store, MemoryType::new(1, Some(1), false)).unwrap();
    ///
    /// // This results in an error: `MemoryError::CouldNotGrow`.
    /// let s = m.grow(1).unwrap();
    /// ```
    pub fn grow<IntoPages>(&self, delta: IntoPages) -> Result<Pages, MemoryError>
    where
        IntoPages: Into<Pages>,
    {
        self.vm_memory.from.grow(delta.into())
    }

    /// Return a "view" of the currently accessible memory. By
    /// default, the view is unsynchronized, using regular memory
    /// accesses. You can force a memory view to use atomic accesses
    /// by calling the [`MemoryView::atomically`] method.
    ///
    /// # Notes:
    ///
    /// This method is safe (as in, it won't cause the host to crash or have UB),
    /// but it doesn't obey rust's rules involving data races, especially concurrent ones.
    /// Therefore, if this memory is shared between multiple threads, a single memory
    /// location can be mutated concurrently without synchronization.
    ///
    /// # Usage:
    ///
    /// ```
    /// # use wasmer::{Memory, MemoryView};
    /// # use std::{cell::Cell, sync::atomic::Ordering};
    /// # fn view_memory(memory: Memory) {
    /// // Without synchronization.
    /// let view: MemoryView<u8> = memory.view();
    /// for byte in view[0x1000 .. 0x1010].iter().map(Cell::get) {
    ///     println!("byte: {}", byte);
    /// }
    ///
    /// // With synchronization.
    /// let atomic_view = view.atomically();
    /// for byte in atomic_view[0x1000 .. 0x1010].iter().map(|atom| atom.load(Ordering::SeqCst)) {
    ///     println!("byte: {}", byte);
    /// }
    /// # }
    /// ```
    pub fn view<T: ValueType>(&self) -> MemoryView<T> {
        let base = self.data_ptr();

        let length = self.size().bytes().0 / std::mem::size_of::<T>();

        unsafe { MemoryView::new(base as _, length as u32) }
    }

    /// A shortcut to [`Self::view::<u8>`][self::view].
    ///
    /// This code is going to be refactored. Use it as your own risks.
    #[doc(hidden)]
    pub fn uint8view(&self) -> MemoryView<u8> {
        self.view()
    }

    pub(crate) fn from_vm_export(store: &Store, vm_memory: VMMemory) -> Self {
        Self {
            store: store.clone(),
            vm_memory,
        }
    }

    /// Returns whether or not these two memories refer to the same data.
    ///
    /// # Example
    ///
    /// ```
    /// # use wasmer::{Memory, MemoryType, Store, Value};
    /// # let store = Store::default();
    /// #
    /// let m = Memory::new(&store, MemoryType::new(1, None, false)).unwrap();
    ///
    /// assert!(m.same(&m));
    /// ```
    pub fn same(&self, other: &Self) -> bool {
        Arc::ptr_eq(&self.vm_memory.from, &other.vm_memory.from)
    }

    /// Get access to the backing VM value for this extern. This function is for
    /// tests it should not be called by users of the Wasmer API.
    ///
    /// # Safety
    /// This function is unsafe to call outside of tests for the wasmer crate
    /// because there is no stability guarantee for the returned type and we may
    /// make breaking changes to it at any time or remove this method.
    #[doc(hidden)]
    pub unsafe fn get_vm_memory(&self) -> &VMMemory {
        &self.vm_memory
    }
}

impl Clone for Memory {
    fn clone(&self) -> Self {
        let mut vm_memory = self.vm_memory.clone();
        vm_memory.upgrade_instance_ref().unwrap();

        Self {
            store: self.store.clone(),
            vm_memory,
        }
    }
}

impl<'a> Exportable<'a> for Memory {
    fn to_export(&self) -> Export {
        self.vm_memory.clone().into()
    }

    fn get_self_from_extern(_extern: Extern) -> Result<Self, ExportError> {
        match _extern {
            Extern::Memory(memory) => Ok(memory),
            _ => Err(ExportError::IncompatibleType),
        }
    }

    fn into_weak_instance_ref(&mut self) {
        self.vm_memory
            .instance_ref
            .as_mut()
            .map(|v| *v = v.downgrade());
    }
}

'''
'''--- lib/api/src/sys/externals/mod.rs ---
pub(crate) mod function;
mod global;
mod memory;
mod table;

pub use self::function::{
    FromToNativeWasmType, Function, HostFunction, WasmTypeList, WithEnv, WithoutEnv,
};

pub use self::global::Global;
pub use self::memory::Memory;
pub use self::table::Table;

use crate::sys::exports::{ExportError, Exportable};
use crate::sys::store::{Store, StoreObject};
use crate::sys::ExternType;
use std::fmt;
use wasmer_vm::Export;

/// An `Extern` is the runtime representation of an entity that
/// can be imported or exported.
///
/// Spec: <https://webassembly.github.io/spec/core/exec/runtime.html#external-values>
#[derive(Clone)]
pub enum Extern {
    /// A external [`Function`].
    Function(Function),
    /// A external [`Global`].
    Global(Global),
    /// A external [`Table`].
    Table(Table),
    /// A external [`Memory`].
    Memory(Memory),
}

impl Extern {
    /// Return the underlying type of the inner `Extern`.
    pub fn ty(&self) -> ExternType {
        match self {
            Self::Function(ft) => ExternType::Function(ft.ty().clone()),
            Self::Memory(ft) => ExternType::Memory(ft.ty()),
            Self::Table(tt) => ExternType::Table(*tt.ty()),
            Self::Global(gt) => ExternType::Global(*gt.ty()),
        }
    }

    /// Create an `Extern` from an `wasmer_engine::Export`.
    pub fn from_vm_export(store: &Store, export: Export) -> Self {
        match export {
            Export::Function(f) => Self::Function(Function::from_vm_export(store, f)),
            Export::Memory(m) => Self::Memory(Memory::from_vm_export(store, m)),
            Export::Global(g) => Self::Global(Global::from_vm_export(store, g)),
            Export::Table(t) => Self::Table(Table::from_vm_export(store, t)),
        }
    }
}

impl<'a> Exportable<'a> for Extern {
    fn to_export(&self) -> Export {
        match self {
            Self::Function(f) => f.to_export(),
            Self::Global(g) => g.to_export(),
            Self::Memory(m) => m.to_export(),
            Self::Table(t) => t.to_export(),
        }
    }

    fn get_self_from_extern(_extern: Self) -> Result<Self, ExportError> {
        // Since this is already an extern, we can just return it.
        Ok(_extern)
    }

    fn into_weak_instance_ref(&mut self) {
        match self {
            Self::Function(f) => f.into_weak_instance_ref(),
            Self::Global(g) => g.into_weak_instance_ref(),
            Self::Memory(m) => m.into_weak_instance_ref(),
            Self::Table(t) => t.into_weak_instance_ref(),
        }
    }
}

impl StoreObject for Extern {
    fn comes_from_same_store(&self, store: &Store) -> bool {
        let my_store = match self {
            Self::Function(f) => f.store(),
            Self::Global(g) => g.store(),
            Self::Memory(m) => m.store(),
            Self::Table(t) => t.store(),
        };
        Store::same(my_store, store)
    }
}

impl fmt::Debug for Extern {
    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
        write!(
            f,
            "{}",
            match self {
                Self::Function(_) => "Function(...)",
                Self::Global(_) => "Global(...)",
                Self::Memory(_) => "Memory(...)",
                Self::Table(_) => "Table(...)",
            }
        )
    }
}

impl From<Function> for Extern {
    fn from(r: Function) -> Self {
        Self::Function(r)
    }
}

impl From<Global> for Extern {
    fn from(r: Global) -> Self {
        Self::Global(r)
    }
}

impl From<Memory> for Extern {
    fn from(r: Memory) -> Self {
        Self::Memory(r)
    }
}

impl From<Table> for Extern {
    fn from(r: Table) -> Self {
        Self::Table(r)
    }
}

'''
'''--- lib/api/src/sys/externals/table.rs ---
use crate::sys::exports::{ExportError, Exportable};
use crate::sys::externals::Extern;
use crate::sys::store::Store;
use crate::sys::types::{Val, ValFuncRef};
use crate::sys::RuntimeError;
use crate::sys::TableType;
use std::sync::Arc;
use wasmer_vm::{Export, Table as RuntimeTable, TableElement, VMTable};

/// A WebAssembly `table` instance.
///
/// The `Table` struct is an array-like structure representing a WebAssembly Table,
/// which stores function references.
///
/// A table created by the host or in WebAssembly code will be accessible and
/// mutable from both host and WebAssembly.
///
/// Spec: <https://webassembly.github.io/spec/core/exec/runtime.html#table-instances>
pub struct Table {
    store: Store,
    vm_table: VMTable,
}

fn set_table_item(
    table: &dyn RuntimeTable,
    item_index: u32,
    item: TableElement,
) -> Result<(), RuntimeError> {
    table.set(item_index, item).map_err(|e| e.into())
}

impl Table {
    /// Creates a new `Table` with the provided [`TableType`] definition.
    ///
    /// All the elements in the table will be set to the `init` value.
    ///
    /// This function will construct the `Table` using the store
    /// [`BaseTunables`][crate::sys::BaseTunables].
    pub fn new(store: &Store, ty: TableType, init: Val) -> Result<Self, RuntimeError> {
        let item = init.into_table_reference(store)?;
        let tunables = store.tunables();
        let style = tunables.table_style(&ty);
        let table = tunables
            .create_host_table(&ty, &style)
            .map_err(RuntimeError::new)?;

        let num_elements = table.size();
        for i in 0..num_elements {
            set_table_item(table.as_ref(), i, item.clone())?;
        }

        Ok(Self {
            store: store.clone(),
            vm_table: VMTable {
                from: table,
                instance_ref: None,
            },
        })
    }

    /// Returns the [`TableType`] of the `Table`.
    pub fn ty(&self) -> &TableType {
        self.vm_table.from.ty()
    }

    /// Returns the [`Store`] where the `Table` belongs.
    pub fn store(&self) -> &Store {
        &self.store
    }

    /// Retrieves an element of the table at the provided `index`.
    pub fn get(&self, index: u32) -> Option<Val> {
        let item = self.vm_table.from.get(index)?;
        Some(ValFuncRef::from_table_reference(item, &self.store))
    }

    /// Sets an element `val` in the Table at the provided `index`.
    pub fn set(&self, index: u32, val: Val) -> Result<(), RuntimeError> {
        let item = val.into_table_reference(&self.store)?;
        set_table_item(self.vm_table.from.as_ref(), index, item)
    }

    /// Retrieves the size of the `Table` (in elements)
    pub fn size(&self) -> u32 {
        self.vm_table.from.size()
    }

    /// Grows the size of the `Table` by `delta`, initializating
    /// the elements with the provided `init` value.
    ///
    /// It returns the previous size of the `Table` in case is able
    /// to grow the Table successfully.
    ///
    /// # Errors
    ///
    /// Returns an error if the `delta` is out of bounds for the table.
    pub fn grow(&self, delta: u32, init: Val) -> Result<u32, RuntimeError> {
        let item = init.into_table_reference(&self.store)?;
        self.vm_table
            .from
            .grow(delta, item)
            .ok_or_else(|| RuntimeError::new(format!("failed to grow table by `{}`", delta)))
    }

    /// Copies the `len` elements of `src_table` starting at `src_index`
    /// to the destination table `dst_table` at index `dst_index`.
    ///
    /// # Errors
    ///
    /// Returns an error if the range is out of bounds of either the source or
    /// destination tables.
    pub fn copy(
        dst_table: &Self,
        dst_index: u32,
        src_table: &Self,
        src_index: u32,
        len: u32,
    ) -> Result<(), RuntimeError> {
        if !Store::same(&dst_table.store, &src_table.store) {
            return Err(RuntimeError::new(
                "cross-`Store` table copies are not supported",
            ));
        }
        RuntimeTable::copy(
            dst_table.vm_table.from.as_ref(),
            src_table.vm_table.from.as_ref(),
            dst_index,
            src_index,
            len,
        )
        .map_err(RuntimeError::from_trap)?;
        Ok(())
    }

    pub(crate) fn from_vm_export(store: &Store, vm_table: VMTable) -> Self {
        Self {
            store: store.clone(),
            vm_table,
        }
    }

    /// Returns whether or not these two tables refer to the same data.
    pub fn same(&self, other: &Self) -> bool {
        Arc::ptr_eq(&self.vm_table.from, &other.vm_table.from)
    }

    /// Get access to the backing VM value for this extern. This function is for
    /// tests it should not be called by users of the Wasmer API.
    ///
    /// # Safety
    /// This function is unsafe to call outside of tests for the wasmer crate
    /// because there is no stability guarantee for the returned type and we may
    /// make breaking changes to it at any time or remove this method.
    #[doc(hidden)]
    pub unsafe fn get_vm_table(&self) -> &VMTable {
        &self.vm_table
    }
}

impl Clone for Table {
    fn clone(&self) -> Self {
        let mut vm_table = self.vm_table.clone();
        vm_table.upgrade_instance_ref().unwrap();

        Self {
            store: self.store.clone(),
            vm_table,
        }
    }
}

impl<'a> Exportable<'a> for Table {
    fn to_export(&self) -> Export {
        self.vm_table.clone().into()
    }

    fn get_self_from_extern(_extern: Extern) -> Result<Self, ExportError> {
        match _extern {
            Extern::Table(table) => Ok(table),
            _ => Err(ExportError::IncompatibleType),
        }
    }

    fn into_weak_instance_ref(&mut self) {
        self.vm_table
            .instance_ref
            .as_mut()
            .map(|v| *v = v.downgrade());
    }
}

'''
'''--- lib/api/src/sys/import_object.rs ---
//! The import module contains the implementation data structures and helper functions used to
//! manipulate and access a wasm module's imports including memories, tables, globals, and
//! functions.
use crate::Exports;
use std::borrow::{Borrow, BorrowMut};
use std::collections::VecDeque;
use std::collections::{hash_map::Entry, HashMap};
use std::fmt;
use std::sync::{Arc, Mutex};
use wasmer_vm::{Export, NamedResolver};

/// The `LikeNamespace` trait represents objects that act as a namespace for imports.
/// For example, an `Instance` or `Namespace` could be
/// considered namespaces that could provide imports to an instance.
pub trait LikeNamespace {
    /// Gets an export by name.
    fn get_namespace_export(&self, name: &str) -> Option<Export>;
    /// Gets all exports in the namespace.
    fn get_namespace_exports(&self) -> Vec<(String, Export)>;
    /// Returns the contents of this namespace as an `Exports`.
    ///
    /// This is used by `ImportObject::get_namespace_exports`.
    fn as_exports(&self) -> Option<Exports> {
        None
    }
}

/// All of the import data used when instantiating.
///
/// It's suggested that you use the [`imports!`] macro
/// instead of creating an `ImportObject` by hand.
///
/// [`imports!`]: macro.imports.html
///
/// # Usage:
/// ```ignore
/// use wasmer::{Exports, ImportObject, Function};
///
/// let mut import_object = ImportObject::new();
/// let mut env = Exports::new();
///
/// env.insert("foo", Function::new_native(foo));
/// import_object.register("env", env);
///
/// fn foo(n: i32) -> i32 {
///     n
/// }
/// ```
#[derive(Clone, Default)]
pub struct ImportObject {
    map: Arc<Mutex<HashMap<String, Box<dyn LikeNamespace + Send + Sync>>>>,
}

impl ImportObject {
    /// Create a new `ImportObject`.
    pub fn new() -> Self {
        Default::default()
    }

    /// Gets an export given a module and a name
    ///
    /// # Usage
    /// ```ignore
    /// # use wasmer_vm::{ImportObject, Instance, Namespace};
    /// let mut import_object = ImportObject::new();
    /// import_object.get_export("module", "name");
    /// ```
    pub fn get_export(&self, module: &str, name: &str) -> Option<Export> {
        let guard = self.map.lock().unwrap();
        let map_ref = guard.borrow();
        if map_ref.contains_key(module) {
            let namespace = map_ref[module].as_ref();
            return namespace.get_namespace_export(name);
        }
        None
    }

    /// Returns true if the ImportObject contains namespace with the provided name.
    pub fn contains_namespace(&self, name: &str) -> bool {
        self.map.lock().unwrap().borrow().contains_key(name)
    }

    /// Register anything that implements `LikeNamespace` as a namespace.
    ///
    /// # Usage:
    /// ```ignore
    /// # use wasmer_vm::{ImportObject, Instance, Namespace};
    /// let mut import_object = ImportObject::new();
    ///
    /// import_object.register("namespace0", instance);
    /// import_object.register("namespace1", namespace);
    /// // ...
    /// ```
    pub fn register<S, N>(&mut self, name: S, namespace: N) -> Option<Box<dyn LikeNamespace>>
    where
        S: Into<String>,
        N: LikeNamespace + Send + Sync + 'static,
    {
        let mut guard = self.map.lock().unwrap();
        let map = guard.borrow_mut();

        match map.entry(name.into()) {
            Entry::Vacant(empty) => {
                empty.insert(Box::new(namespace));
                None
            }
            Entry::Occupied(mut occupied) => Some(occupied.insert(Box::new(namespace))),
        }
    }

    /// Returns the contents of a namespace as an `Exports`.
    ///
    /// Returns `None` if the namespace doesn't exist or doesn't implement the
    /// `as_exports` method.
    pub fn get_namespace_exports(&self, name: &str) -> Option<Exports> {
        let map = self.map.lock().unwrap();
        map.get(name).and_then(|ns| ns.as_exports())
    }

    fn get_objects(&self) -> VecDeque<((String, String), Export)> {
        let mut out = VecDeque::new();
        let guard = self.map.lock().unwrap();
        let map = guard.borrow();
        for (name, ns) in map.iter() {
            for (id, exp) in ns.get_namespace_exports() {
                out.push_back(((name.clone(), id), exp));
            }
        }
        out
    }
}

impl NamedResolver for ImportObject {
    fn resolve_by_name(&self, module: &str, name: &str) -> Option<Export> {
        self.get_export(module, name)
    }
}

/// Iterator for an `ImportObject`'s exports.
pub struct ImportObjectIterator {
    elements: VecDeque<((String, String), Export)>,
}

impl Iterator for ImportObjectIterator {
    type Item = ((String, String), Export);
    fn next(&mut self) -> Option<Self::Item> {
        self.elements.pop_front()
    }
}

impl IntoIterator for ImportObject {
    type IntoIter = ImportObjectIterator;
    type Item = ((String, String), Export);

    fn into_iter(self) -> Self::IntoIter {
        ImportObjectIterator {
            elements: self.get_objects(),
        }
    }
}

impl fmt::Debug for ImportObject {
    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
        enum SecretMap {
            Empty,
            Some(usize),
        }

        impl SecretMap {
            fn new(len: usize) -> Self {
                if len == 0 {
                    Self::Empty
                } else {
                    Self::Some(len)
                }
            }
        }

        impl fmt::Debug for SecretMap {
            fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
                match self {
                    Self::Empty => write!(f, "(empty)"),
                    Self::Some(len) => write!(f, "(... {} item(s) ...)", len),
                }
            }
        }

        f.debug_struct("ImportObject")
            .field(
                "map",
                &SecretMap::new(self.map.lock().unwrap().borrow().len()),
            )
            .finish()
    }
}

// The import! macro for ImportObject

/// Generate an [`ImportObject`] easily with the `imports!` macro.
///
/// [`ImportObject`]: struct.ImportObject.html
///
/// # Usage
///
/// ```
/// # use wasmer::{Function, Store};
/// # let store = Store::default();
/// use wasmer::imports;
///
/// let import_object = imports! {
///     "env" => {
///         "foo" => Function::new_native(&store, foo)
///     },
/// };
///
/// fn foo(n: i32) -> i32 {
///     n
/// }
/// ```
#[macro_export]
macro_rules! imports {
    ( $( $ns_name:expr => $ns:tt ),* $(,)? ) => {
        {
            let mut import_object = $crate::ImportObject::new();

            $({
                let namespace = $crate::import_namespace!($ns);

                import_object.register($ns_name, namespace);
            })*

            import_object
        }
    };
}

#[macro_export]
#[doc(hidden)]
macro_rules! namespace {
    ($( $import_name:expr => $import_item:expr ),* $(,)? ) => {
        $crate::import_namespace!( { $( $import_name => $import_item, )* } )
    };
}

#[macro_export]
#[doc(hidden)]
macro_rules! import_namespace {
    ( { $( $import_name:expr => $import_item:expr ),* $(,)? } ) => {{
        let mut namespace = $crate::Exports::new();

        $(
            namespace.insert($import_name, $import_item);
        )*

        namespace
    }};

    ( $namespace:ident ) => {
        $namespace
    };
}

#[cfg(test)]
mod test {
    use super::*;
    use crate::sys::{Global, Store, Val};
    use wasmer_types::Type;
    use wasmer_vm::ChainableNamedResolver;

    #[test]
    fn chaining_works() {
        let store = Store::default();
        let g = Global::new(&store, Val::I32(0));

        let imports1 = imports! {
            "dog" => {
                "happy" => g.clone()
            }
        };

        let imports2 = imports! {
            "dog" => {
                "small" => g.clone()
            },
            "cat" => {
                "small" => g.clone()
            }
        };

        let resolver = imports1.chain_front(imports2);

        let small_cat_export = resolver.resolve_by_name("cat", "small");
        assert!(small_cat_export.is_some());

        let happy = resolver.resolve_by_name("dog", "happy");
        let small = resolver.resolve_by_name("dog", "small");
        assert!(happy.is_some());
        assert!(small.is_some());
    }

    #[test]
    fn extending_conflict_overwrites() {
        let store = Store::default();
        let g1 = Global::new(&store, Val::I32(0));
        let g2 = Global::new(&store, Val::I64(0));

        let imports1 = imports! {
            "dog" => {
                "happy" => g1,
            },
        };

        let imports2 = imports! {
            "dog" => {
                "happy" => g2,
            },
        };

        let resolver = imports1.chain_front(imports2);
        let happy_dog_entry = resolver.resolve_by_name("dog", "happy").unwrap();

        assert!(if let Export::Global(happy_dog_global) = happy_dog_entry {
            happy_dog_global.from.ty().ty == Type::I64
        } else {
            false
        });

        // now test it in reverse
        let store = Store::default();
        let g1 = Global::new(&store, Val::I32(0));
        let g2 = Global::new(&store, Val::I64(0));

        let imports1 = imports! {
            "dog" => {
                "happy" => g1,
            },
        };

        let imports2 = imports! {
            "dog" => {
                "happy" => g2,
            },
        };

        let resolver = imports1.chain_back(imports2);
        let happy_dog_entry = resolver.resolve_by_name("dog", "happy").unwrap();

        assert!(if let Export::Global(happy_dog_global) = happy_dog_entry {
            happy_dog_global.from.ty().ty == Type::I32
        } else {
            false
        });
    }

    #[test]
    fn namespace() {
        let store = Store::default();
        let g1 = Global::new(&store, Val::I32(0));
        let namespace = namespace! {
            "happy" => g1
        };
        let imports1 = imports! {
            "dog" => namespace
        };

        let happy_dog_entry = imports1.resolve_by_name("dog", "happy").unwrap();

        assert!(if let Export::Global(happy_dog_global) = happy_dog_entry {
            happy_dog_global.from.ty().ty == Type::I32
        } else {
            false
        });
    }

    #[test]
    fn imports_macro_allows_trailing_comma_and_none() {
        use crate::sys::Function;

        let store = Default::default();

        fn func(arg: i32) -> i32 {
            arg + 1
        }

        let _ = imports! {
            "env" => {
                "func" => Function::new_native(&store, func),
            },
        };
        let _ = imports! {
            "env" => {
                "func" => Function::new_native(&store, func),
            }
        };
        let _ = imports! {
            "env" => {
                "func" => Function::new_native(&store, func),
            },
            "abc" => {
                "def" => Function::new_native(&store, func),
            }
        };
        let _ = imports! {
            "env" => {
                "func" => Function::new_native(&store, func)
            },
        };
        let _ = imports! {
            "env" => {
                "func" => Function::new_native(&store, func)
            }
        };
        let _ = imports! {
            "env" => {
                "func1" => Function::new_native(&store, func),
                "func2" => Function::new_native(&store, func)
            }
        };
        let _ = imports! {
            "env" => {
                "func1" => Function::new_native(&store, func),
                "func2" => Function::new_native(&store, func),
            }
        };
    }
}

'''
'''--- lib/api/src/sys/instance.rs ---
use crate::sys::module::Module;
use crate::sys::store::Store;
use crate::sys::{HostEnvInitError, LinkError, RuntimeError};
use crate::{ExportError, NativeFunc, WasmTypeList};
use std::fmt;
use std::sync::{Arc, Mutex};
use thiserror::Error;
use wasmer_types::InstanceConfig;
use wasmer_vm::{InstanceHandle, Resolver, VMContext};

use super::exports::ExportableWithGenerics;

/// A WebAssembly Instance is a stateful, executable
/// instance of a WebAssembly [`Module`].
///
/// Instance objects contain all the exported WebAssembly
/// functions, memories, tables and globals that allow
/// interacting with WebAssembly.
///
/// Spec: <https://webassembly.github.io/spec/core/exec/runtime.html#module-instances>
#[derive(Clone)]
pub struct Instance {
    handle: Arc<Mutex<InstanceHandle>>,
    module: Module,
}

#[cfg(test)]
mod send_test {
    use super::*;

    fn is_send<T: Send>() -> bool {
        true
    }

    #[test]
    fn instance_is_send() {
        assert!(is_send::<Instance>());
    }
}

/// An error while instantiating a module.
///
/// This is not a common WebAssembly error, however
/// we need to differentiate from a `LinkError` (an error
/// that happens while linking, on instantiation), a
/// Trap that occurs when calling the WebAssembly module
/// start function, and an error when initializing the user's
/// host environments.
#[derive(Error, Debug)]
pub enum InstantiationError {
    /// A linking ocurred during instantiation.
    #[error(transparent)]
    Link(LinkError),

    /// A runtime error occured while invoking the start function
    #[error("could not invoke the start function: {0}")]
    Start(RuntimeError),

    /// The module was compiled with a CPU feature that is not available on
    /// the current host.
    #[error("missing requires CPU features: {0:?}")]
    CpuFeature(String),

    /// Error occurred when initializing the host environment.
    #[error(transparent)]
    HostEnvInitialization(HostEnvInitError),
}

impl From<wasmer_engine::InstantiationError> for InstantiationError {
    fn from(other: wasmer_engine::InstantiationError) -> Self {
        match other {
            wasmer_engine::InstantiationError::Link(e) => Self::Link(e),
            wasmer_engine::InstantiationError::Start(e) => Self::Start(e),
            wasmer_engine::InstantiationError::CpuFeature(e) => Self::CpuFeature(e),
        }
    }
}

impl From<HostEnvInitError> for InstantiationError {
    fn from(other: HostEnvInitError) -> Self {
        Self::HostEnvInitialization(other)
    }
}

impl Instance {
    /// Creates a new `Instance` from a WebAssembly [`Module`] and a
    /// set of imports resolved by the [`Resolver`].
    ///
    /// The resolver can be anything that implements the [`Resolver`] trait,
    /// so you can plug custom resolution for the imports, if you wish not
    /// to use [`ImportObject`].
    ///
    /// The [`ImportObject`] is the easiest way to provide imports to the instance.
    ///
    /// [`ImportObject`]: crate::ImportObject
    ///
    /// ```
    /// # use wasmer::{imports, Store, Module, Global, Value, Instance};
    /// # fn main() -> anyhow::Result<()> {
    /// let store = Store::default();
    /// let module = Module::new(&store, "(module)")?;
    /// let imports = imports!{
    ///   "host" => {
    ///     "var" => Global::new(&store, Value::I32(2))
    ///   }
    /// };
    /// let instance = Instance::new(&module, &imports)?;
    /// # Ok(())
    /// # }
    /// ```
    ///
    /// ## Errors
    ///
    /// The function can return [`InstantiationError`]s.
    ///
    /// Those are, as defined by the spec:
    ///  * Link errors that happen when plugging the imports into the instance
    ///  * Runtime errors that happen when running the module `start` function.
    pub fn new(module: &Module, resolver: &dyn Resolver) -> Result<Self, InstantiationError> {
        Instance::new_with_config(module, InstanceConfig::default(), resolver)
    }

    /// New instance with config.
    pub fn new_with_config(
        module: &Module,
        config: InstanceConfig,
        resolver: &dyn Resolver,
    ) -> Result<Self, InstantiationError> {
        unsafe {
            if (*config.gas_counter).opcode_cost > i32::MAX as u64 {
                // Fast gas counter logic assumes that individual opcode cost is not too big.
                return Err(InstantiationError::HostEnvInitialization(
                    HostEnvInitError::IncorrectGasMeteringConfig,
                ));
            }
        }
        let handle = module.instantiate(resolver, config)?;
        let instance = Self {
            handle: Arc::new(Mutex::new(handle)),
            module: module.clone(),
        };

        // # Safety
        // `initialize_host_envs` should be called after instantiation but before
        // returning an `Instance` to the user. We set up the host environments
        // via `WasmerEnv::init_with_instance`.
        //
        // This usage is correct because we pass a valid pointer to `instance` and the
        // correct error type returned by `WasmerEnv::init_with_instance` as a generic
        // parameter.
        unsafe {
            wasmer_vm::initialize_host_envs::<HostEnvInitError>(
                &*instance.handle,
                &instance as *const _ as *const _,
            )?;
        }

        Ok(instance)
    }

    /// Gets the [`Module`] associated with this instance.
    pub fn module(&self) -> &Module {
        &self.module
    }

    /// Returns the [`Store`] where the `Instance` belongs.
    pub fn store(&self) -> &Store {
        self.module.store()
    }

    /// Lookup an exported entity by its name.
    pub fn lookup(&self, field: &str) -> Option<crate::Export> {
        let vmextern = self.handle.lock().unwrap().lookup(field)?;
        Some(vmextern.into())
    }

    /// Lookup an exported function by its name.
    pub fn lookup_function(&self, field: &str) -> Option<crate::Function> {
        if let crate::Export::Function(f) = self.lookup(field)? {
            Some(crate::Function::from_vm_export(self.store(), f))
        } else {
            None
        }
    }

    /// Get an export as a `NativeFunc`.
    pub fn get_native_function<Args, Rets>(
        &self,
        name: &str,
    ) -> Result<NativeFunc<Args, Rets>, ExportError>
    where
        Args: WasmTypeList,
        Rets: WasmTypeList,
    {
        match self.lookup(name) {
            Some(crate::Export::Function(f)) => crate::Function::from_vm_export(self.store(), f)
                .native()
                .map_err(|_| ExportError::IncompatibleType),
            Some(_) => Err(ExportError::IncompatibleType),
            None => Err(ExportError::Missing("not found".into())),
        }
    }

    /// Hack to get this working with nativefunc too
    pub fn get_with_generics<'a, T, Args, Rets>(&'a self, name: &str) -> Result<T, ExportError>
    where
        Args: WasmTypeList,
        Rets: WasmTypeList,
        T: ExportableWithGenerics<'a, Args, Rets>,
    {
        let export = self
            .lookup(name)
            .ok_or_else(|| ExportError::Missing(name.to_string()))?;
        let ext = crate::Extern::from_vm_export(self.store(), export);
        T::get_self_from_extern_with_generics(ext)
    }

    /// Like `get_with_generics` but with a WeakReference to the `InstanceRef` internally.
    /// This is useful for passing data into `WasmerEnv`, for example.
    pub fn get_with_generics_weak<'a, T, Args, Rets>(&'a self, name: &str) -> Result<T, ExportError>
    where
        Args: WasmTypeList,
        Rets: WasmTypeList,
        T: ExportableWithGenerics<'a, Args, Rets>,
    {
        let mut out: T = self.get_with_generics(name)?;
        out.into_weak_instance_ref();
        Ok(out)
    }

    #[doc(hidden)]
    pub fn vmctx_ptr(&self) -> *mut VMContext {
        self.handle.lock().unwrap().vmctx_ptr()
    }
}

impl fmt::Debug for Instance {
    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
        f.debug_struct("Instance").finish()
    }
}

'''
'''--- lib/api/src/sys/mod.rs ---
mod cell;
mod env;
mod exports;
mod externals;
mod import_object;
mod instance;
mod module;
mod native;
mod ptr;
mod store;
mod tunables;
mod types;
mod utils;

/// Implement [`WasmerEnv`] for your type with `#[derive(WasmerEnv)]`.
///
/// See the [`WasmerEnv`] trait for more information.
pub use wasmer_derive::WasmerEnv;

#[doc(hidden)]
pub mod internals {
    //! We use the internals module for exporting types that are only
    //! intended to use in internal crates such as the compatibility crate
    //! `wasmer-vm`. Please don't use any of this types directly, as
    //! they might change frequently or be removed in the future.

    pub use crate::sys::externals::{WithEnv, WithoutEnv};
}

pub use crate::sys::cell::WasmCell;
pub use crate::sys::env::{HostEnvInitError, LazyInit, WasmerEnv};
pub use crate::sys::exports::{ExportError, Exportable, Exports, ExportsIterator};
pub use crate::sys::externals::{
    Extern, FromToNativeWasmType, Function, Global, HostFunction, Memory, Table, WasmTypeList,
};
pub use crate::sys::import_object::{ImportObject, ImportObjectIterator, LikeNamespace};
pub use crate::sys::instance::{Instance, InstantiationError};
pub use crate::sys::module::Module;
pub use crate::sys::native::NativeFunc;
pub use crate::sys::ptr::{Array, Item, WasmPtr};
pub use crate::sys::store::{Store, StoreObject};
pub use crate::sys::tunables::BaseTunables;
pub use crate::sys::types::{
    ExportType, ExternType, FunctionType, GlobalType, MemoryType, Mutability, TableType, Val,
    ValType,
};
pub use crate::sys::types::{Val as Value, ValType as Type};
pub use crate::sys::utils::is_wasm;
pub use target_lexicon::{Architecture, CallingConvention, OperatingSystem, Triple, HOST};
#[cfg(feature = "compiler")]
pub use wasmer_compiler::{wasmparser, CompilerConfig};
pub use wasmer_compiler::{
    CompileError, CpuFeature, Features, ParseCpuFeatureError, Target, WasmError, WasmResult,
};
pub use wasmer_engine::{DeserializeError, Engine, FrameInfo, LinkError, RuntimeError};
#[cfg(feature = "experimental-reference-types-extern-ref")]
pub use wasmer_types::ExternRef;
pub use wasmer_types::{
    Atomically, Bytes, ExportIndex, GlobalInit, LocalFunctionIndex, MemoryView, Pages, ValueType,
    WASM_MAX_PAGES, WASM_MIN_PAGES, WASM_PAGE_SIZE,
};
pub use wasmer_vm::{
    ChainableNamedResolver, Export, NamedResolver, NamedResolverChain, Resolver, Tunables,
};

// TODO: should those be moved into wasmer::vm as well?
pub use wasmer_vm::{raise_user_trap, MemoryError};
pub mod vm {
    //! The `vm` module re-exports wasmer-vm types.

    pub use wasmer_vm::{
        Memory, MemoryError, MemoryStyle, Table, TableStyle, VMExtern, VMMemoryDefinition,
        VMTableDefinition,
    };
}

#[cfg(feature = "wat")]
pub use wat::parse_bytes as wat2wasm;

// The compilers are mutually exclusive
#[cfg(any(
    all(
        feature = "default-llvm",
        any(feature = "default-cranelift", feature = "default-singlepass")
    ),
    all(feature = "default-cranelift", feature = "default-singlepass")
))]
compile_error!(
    r#"The `default-singlepass`, `default-cranelift` and `default-llvm` features are mutually exclusive.
If you wish to use more than one compiler, you can simply create the own store. Eg.:

```
use wasmer::{Store, Universal, Singlepass};

let engine = Universal::new(Singlepass::default()).engine();
let store = Store::new(&engine);
```"#
);

#[cfg(feature = "singlepass")]
pub use wasmer_compiler_singlepass::Singlepass;

#[cfg(feature = "cranelift")]
pub use wasmer_compiler_cranelift::{Cranelift, CraneliftOptLevel};

#[cfg(feature = "llvm")]
pub use wasmer_compiler_llvm::{LLVMOptLevel, LLVM};

#[cfg(feature = "universal")]
pub use wasmer_engine_universal::{Universal, UniversalArtifact, UniversalEngine};

#[cfg(feature = "dylib")]
pub use wasmer_engine_dylib::{Dylib, DylibArtifact, DylibEngine};

/// Version number of this crate.
pub const VERSION: &str = env!("CARGO_PKG_VERSION");

/// The Deprecated JIT Engine (please use `Universal` instead)
#[cfg(feature = "jit")]
#[deprecated(since = "2.0.0", note = "Please use the `universal` feature instead")]
pub type JIT = Universal;

/// The Deprecated Native Engine (please use `Dylib` instead)
#[cfg(feature = "native")]
#[deprecated(since = "2.0.0", note = "Please use the `native` feature instead")]
pub type Native = Dylib;

'''
'''--- lib/api/src/sys/module.rs ---
use crate::sys::store::Store;
use crate::sys::InstantiationError;
use std::fmt;
use std::io;
use std::path::Path;
use std::sync::Arc;
use thiserror::Error;
use wasmer_compiler::CompileError;
#[cfg(feature = "wat")]
use wasmer_compiler::WasmError;
use wasmer_engine::RuntimeError;
use wasmer_engine_universal::UniversalArtifact;
use wasmer_types::InstanceConfig;
use wasmer_vm::{InstanceHandle, Instantiatable, Resolver};

#[derive(Error, Debug)]
pub enum IoCompileError {
    /// An IO error
    #[error(transparent)]
    Io(#[from] io::Error),
    /// A compilation error
    #[error(transparent)]
    Compile(#[from] CompileError),
}

/// A WebAssembly Module contains stateless WebAssembly
/// code that has already been compiled and can be instantiated
/// multiple times.
///
/// ## Cloning a module
///
/// Cloning a module is cheap: it does a shallow copy of the compiled
/// contents rather than a deep copy.
#[derive(Clone)]
pub struct Module {
    store: Store,
    artifact: Arc<wasmer_engine_universal::UniversalArtifact>,
}

impl Module {
    /// Creates a new WebAssembly Module given the configuration
    /// in the store.
    ///
    /// If the provided bytes are not WebAssembly-like (start with `b"\0asm"`),
    /// and the "wat" feature is enabled for this crate, this function will try to
    /// to convert the bytes assuming they correspond to the WebAssembly text
    /// format.
    ///
    /// ## Security
    ///
    /// Before the code is compiled, it will be validated using the store
    /// features.
    ///
    /// ## Errors
    ///
    /// Creating a WebAssembly module from bytecode can result in a
    /// [`CompileError`] since this operation requires to transorm the Wasm
    /// bytecode into code the machine can easily execute.
    ///
    /// ## Example
    ///
    /// Reading from a WAT file.
    ///
    /// ```
    /// use wasmer::*;
    /// # fn main() -> anyhow::Result<()> {
    /// # let store = Store::default();
    /// let wat = "(module)";
    /// let module = Module::new(&store, wat)?;
    /// # Ok(())
    /// # }
    /// ```
    ///
    /// Reading from bytes:
    ///
    /// ```
    /// use wasmer::*;
    /// # fn main() -> anyhow::Result<()> {
    /// # let store = Store::default();
    /// // The following is the same as:
    /// // (module
    /// //   (type $t0 (func (param i32) (result i32)))
    /// //   (func $add_one (export "add_one") (type $t0) (param $p0 i32) (result i32)
    /// //     get_local $p0
    /// //     i32.const 1
    /// //     i32.add)
    /// // )
    /// let bytes: Vec<u8> = vec![
    ///     0x00, 0x61, 0x73, 0x6d, 0x01, 0x00, 0x00, 0x00, 0x01, 0x06, 0x01, 0x60,
    ///     0x01, 0x7f, 0x01, 0x7f, 0x03, 0x02, 0x01, 0x00, 0x07, 0x0b, 0x01, 0x07,
    ///     0x61, 0x64, 0x64, 0x5f, 0x6f, 0x6e, 0x65, 0x00, 0x00, 0x0a, 0x09, 0x01,
    ///     0x07, 0x00, 0x20, 0x00, 0x41, 0x01, 0x6a, 0x0b, 0x00, 0x1a, 0x04, 0x6e,
    ///     0x61, 0x6d, 0x65, 0x01, 0x0a, 0x01, 0x00, 0x07, 0x61, 0x64, 0x64, 0x5f,
    ///     0x6f, 0x6e, 0x65, 0x02, 0x07, 0x01, 0x00, 0x01, 0x00, 0x02, 0x70, 0x30,
    /// ];
    /// let module = Module::new(&store, bytes)?;
    /// # Ok(())
    /// # }
    /// ```
    #[allow(unreachable_code)]
    pub fn new(store: &Store, bytes: impl AsRef<[u8]>) -> Result<Self, CompileError> {
        #[cfg(feature = "wat")]
        let bytes = wat::parse_bytes(bytes.as_ref()).map_err(|e| {
            CompileError::Wasm(WasmError::Generic(format!(
                "Error when converting wat: {}",
                e
            )))
        })?;

        Self::from_binary(store, bytes.as_ref())
    }

    /// Creates a new WebAssembly module from a file path.
    pub fn from_file(store: &Store, file: impl AsRef<Path>) -> Result<Self, IoCompileError> {
        let file_ref = file.as_ref();
        let wasm_bytes = std::fs::read(file_ref)?;
        let module = Self::new(store, &wasm_bytes)?;
        // Set the module name to the absolute path of the filename.
        // This is useful for debugging the stack traces.
        Ok(module)
    }

    /// Creates a new WebAssembly module from a binary.
    ///
    /// Opposed to [`Module::new`], this function is not compatible with
    /// the WebAssembly text format (if the "wat" feature is enabled for
    /// this crate).
    pub fn from_binary(store: &Store, binary: &[u8]) -> Result<Self, CompileError> {
        Self::validate(store, binary)?;
        unsafe { Self::from_binary_unchecked(store, binary) }
    }

    /// Creates a new WebAssembly module skipping any kind of validation.
    ///
    /// # Safety
    ///
    /// This can speed up compilation time a bit, but it should be only used
    /// in environments where the WebAssembly modules are trusted and validated
    /// beforehand.
    pub unsafe fn from_binary_unchecked(
        store: &Store,
        binary: &[u8],
    ) -> Result<Self, CompileError> {
        let module = Self::compile(store, binary)?;
        Ok(module)
    }

    /// Validates a new WebAssembly Module given the configuration
    /// in the Store.
    ///
    /// This validation is normally pretty fast and checks the enabled
    /// WebAssembly features in the Store Engine to assure deterministic
    /// validation of the Module.
    pub fn validate(store: &Store, binary: &[u8]) -> Result<(), CompileError> {
        store.engine().validate(binary)
    }

    fn compile(store: &Store, binary: &[u8]) -> Result<Self, CompileError> {
        let executable = store.engine().compile(binary, store.tunables())?;
        let artifact = store.engine().load(&*executable)?;
        match artifact.downcast_arc::<UniversalArtifact>() {
            Ok(universal) => Ok(Self::from_universal_artifact(store, universal)),
            // We're are probably given an externally defined artifact type
            // which I imagine we don't care about for now since this entire crate
            // is only used for tests and this crate only defines universal engine.
            Err(_) => panic!("unhandled artifact type"),
        }
    }

    /// Make a Module from Artifact...
    pub fn from_universal_artifact(
        store: &Store,
        artifact: Arc<wasmer_engine_universal::UniversalArtifact>,
    ) -> Self {
        Self {
            store: store.clone(),
            artifact,
        }
    }

    pub(crate) fn instantiate(
        &self,
        resolver: &dyn Resolver,
        config: InstanceConfig,
    ) -> Result<InstanceHandle, InstantiationError> {
        unsafe {
            let instance_handle = Arc::clone(&self.artifact).instantiate(
                self.store.tunables(),
                resolver,
                Box::new((self.store.clone(), Arc::clone(&self.artifact))),
                config,
            )?;

            // After the instance handle is created, we need to initialize
            // the data, call the start function and so. However, if any
            // of this steps traps, we still need to keep the instance alive
            // as some of the Instance elements may have placed in other
            // instance tables.
            instance_handle
                .finish_instantiation()
                .map_err(|t| InstantiationError::Start(RuntimeError::from_trap(t)))?;

            Ok(instance_handle)
        }
    }

    /// Returns the [`Store`] where the `Instance` belongs.
    pub fn store(&self) -> &Store {
        &self.store
    }
}

impl fmt::Debug for Module {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        f.debug_struct("Module").finish()
    }
}

'''
'''--- lib/api/src/sys/native.rs ---
//! Native Functions.
//!
//! This module creates the helper `NativeFunc` that let us call WebAssembly
//! functions with the native ABI, that is:
//!
//! ```ignore
//! let add_one = instance.exports.get_function("function_name")?;
//! let add_one_native: NativeFunc<i32, i32> = add_one.native().unwrap();
//! ```
use std::marker::PhantomData;

use crate::sys::externals::function::{DynamicFunction, VMDynamicFunction};
use crate::sys::{FromToNativeWasmType, Function, RuntimeError, Store, WasmTypeList};
use std::panic::{catch_unwind, AssertUnwindSafe};
use wasmer_types::NativeWasmType;
use wasmer_vm::{
    ExportFunction, VMDynamicFunctionContext, VMFunctionBody, VMFunctionEnvironment, VMFunctionKind,
};

/// A WebAssembly function that can be called natively
/// (using the Native ABI).
pub struct NativeFunc<Args = (), Rets = ()> {
    store: Store,
    exported: ExportFunction,
    _phantom: PhantomData<(Args, Rets)>,
}

unsafe impl<Args, Rets> Send for NativeFunc<Args, Rets> {}

impl<Args, Rets> NativeFunc<Args, Rets>
where
    Args: WasmTypeList,
    Rets: WasmTypeList,
{
    pub(crate) fn new(store: Store, exported: ExportFunction) -> Self {
        Self {
            store,
            exported,
            _phantom: PhantomData,
        }
    }

    pub(crate) fn is_host(&self) -> bool {
        self.exported.vm_function.instance_ref.is_none()
    }

    pub(crate) fn vmctx(&self) -> VMFunctionEnvironment {
        self.exported.vm_function.vmctx
    }

    pub(crate) fn address(&self) -> *const VMFunctionBody {
        self.exported.vm_function.address
    }

    pub(crate) fn arg_kind(&self) -> VMFunctionKind {
        self.exported.vm_function.kind
    }

    /// Get access to the backing VM value for this extern. This function is for
    /// tests it should not be called by users of the Wasmer API.
    ///
    /// # Safety
    /// This function is unsafe to call outside of tests for the wasmer crate
    /// because there is no stability guarantee for the returned type and we may
    /// make breaking changes to it at any time or remove this method.
    #[doc(hidden)]
    pub unsafe fn get_vm_function(&self) -> &wasmer_vm::VMFunction {
        &self.exported.vm_function
    }
}

/*
impl<Args, Rets> From<&NativeFunc<Args, Rets>> for VMFunction
where
    Args: WasmTypeList,
    Rets: WasmTypeList,
{
    fn from(other: &NativeFunc<Args, Rets>) -> Self {
        let signature = FunctionType::new(Args::wasm_types(), Rets::wasm_types());
        Self {
            address: other.address,
            vmctx: other.vmctx,
            signature,
            kind: other.arg_kind,
            call_trampoline: None,
            instance_ref: None,
        }
    }
}*/

impl<Args: WasmTypeList, Rets: WasmTypeList> Clone for NativeFunc<Args, Rets> {
    fn clone(&self) -> Self {
        let mut exported = self.exported.clone();
        exported.vm_function.upgrade_instance_ref().unwrap();

        Self {
            store: self.store.clone(),
            exported,
            _phantom: PhantomData,
        }
    }
}

impl<Args, Rets> From<&NativeFunc<Args, Rets>> for ExportFunction
where
    Args: WasmTypeList,
    Rets: WasmTypeList,
{
    fn from(other: &NativeFunc<Args, Rets>) -> Self {
        other.exported.clone()
    }
}

impl<Args, Rets> From<NativeFunc<Args, Rets>> for Function
where
    Args: WasmTypeList,
    Rets: WasmTypeList,
{
    fn from(other: NativeFunc<Args, Rets>) -> Self {
        Self {
            store: other.store,
            exported: other.exported,
        }
    }
}

macro_rules! impl_native_traits {
    (  $( $x:ident ),* ) => {
        #[allow(unused_parens, non_snake_case)]
        impl<$( $x , )* Rets> NativeFunc<( $( $x ),* ), Rets>
        where
            $( $x: FromToNativeWasmType, )*
            Rets: WasmTypeList,
        {
            /// Call the typed func and return results.
            pub fn call(&self, $( $x: $x, )* ) -> Result<Rets, RuntimeError> {
                if !self.is_host() {
                    // We assume the trampoline is always going to be present for
                    // Wasm functions
                    let trampoline = self.exported.vm_function.call_trampoline.expect("Call trampoline not found in wasm function");
                    // TODO: when `const fn` related features mature more, we can declare a single array
                    // of the correct size here.
                    let mut params_list = [ $( $x.to_native().to_binary() ),* ];
                    let mut rets_list_array = Rets::empty_array();
                    let rets_list = rets_list_array.as_mut();
                    let using_rets_array;
                    let args_rets: &mut [i128] = if params_list.len() > rets_list.len() {
                        using_rets_array = false;
                        params_list.as_mut()
                    } else {
                        using_rets_array = true;
                        for (i, &arg) in params_list.iter().enumerate() {
                            rets_list[i] = arg;
                        }
                        rets_list.as_mut()
                    };
                    unsafe {
                        wasmer_vm::wasmer_call_trampoline(
                            self.vmctx(),
                            trampoline,
                            self.address(),
                            args_rets.as_mut_ptr() as *mut u8,
                        )
                    }?;
                    let num_rets = rets_list.len();
                    if !using_rets_array && num_rets > 0 {
                        let src_pointer = params_list.as_ptr();
                        let rets_list = &mut rets_list_array.as_mut()[0] as *mut i128;
                        unsafe {
                            // TODO: we can probably remove this copy by doing some clever `transmute`s.
                            // we know it's not overlapping because `using_rets_array` is false
                            std::ptr::copy_nonoverlapping(src_pointer,
                                                          rets_list,
                                                          num_rets);
                        }
                    }
                    Ok(Rets::from_array(rets_list_array))
                    // TODO: When the Host ABI and Wasm ABI are the same, we could do this instead:
                    // but we can't currently detect whether that's safe.
                    //
                    // let results = unsafe {
                    //     wasmer_vm::catch_traps_with_result(self.vmctx, || {
                    //         let f = std::mem::transmute::<_, unsafe extern "C" fn( *mut VMContext, $( $x, )*) -> Rets::CStruct>(self.address());
                    //         // We always pass the vmctx
                    //         f( self.vmctx, $( $x, )* )
                    //     }).map_err(RuntimeError::from_trap)?
                    // };
                    // Ok(Rets::from_c_struct(results))

                }
                else {
                    match self.arg_kind() {
                        VMFunctionKind::Static => {
                            let results = catch_unwind(AssertUnwindSafe(|| unsafe {
                                let f = std::mem::transmute::<_, unsafe extern "C" fn( VMFunctionEnvironment, $( $x, )*) -> Rets::CStruct>(self.address());
                                // We always pass the vmctx
                                f( self.vmctx(), $( $x, )* )
                            })).map_err(|e| RuntimeError::new(format!("{:?}", e)))?;
                            Ok(Rets::from_c_struct(results))
                        },
                        VMFunctionKind::Dynamic => {
                            let params_list = [ $( $x.to_native().to_value() ),* ];
                            let results = {
                                type VMContextWithEnv = VMDynamicFunctionContext<DynamicFunction<std::ffi::c_void>>;
                                unsafe {
                                    let ctx = self.vmctx().host_env as *mut VMContextWithEnv;
                                    (*ctx).ctx.call(&params_list)?
                                }
                            };
                            let mut rets_list_array = Rets::empty_array();
                            let mut_rets = rets_list_array.as_mut() as *mut [i128] as *mut i128;
                            for (i, ret) in results.iter().enumerate() {
                                unsafe {
                                    ret.write_value_to(mut_rets.add(i));
                                }
                            }
                            Ok(Rets::from_array(rets_list_array))
                        }
                    }
                }
            }

        }

        #[allow(unused_parens)]
        impl<'a, $( $x, )* Rets> crate::sys::exports::ExportableWithGenerics<'a, ($( $x ),*), Rets> for NativeFunc<( $( $x ),* ), Rets>
        where
            $( $x: FromToNativeWasmType, )*
            Rets: WasmTypeList,
        {
            fn get_self_from_extern_with_generics(_extern: crate::sys::externals::Extern) -> Result<Self, crate::sys::exports::ExportError> {
                use crate::sys::exports::Exportable;
                crate::Function::get_self_from_extern(_extern)?.native().map_err(|_| crate::sys::exports::ExportError::IncompatibleType)
            }

            fn into_weak_instance_ref(&mut self) {
                self.exported.vm_function.instance_ref.as_mut().map(|v| *v = v.downgrade());
            }
        }
    };
}

impl_native_traits!();
impl_native_traits!(A1);
impl_native_traits!(A1, A2);
impl_native_traits!(A1, A2, A3);
impl_native_traits!(A1, A2, A3, A4);
impl_native_traits!(A1, A2, A3, A4, A5);
impl_native_traits!(A1, A2, A3, A4, A5, A6);
impl_native_traits!(A1, A2, A3, A4, A5, A6, A7);
impl_native_traits!(A1, A2, A3, A4, A5, A6, A7, A8);
impl_native_traits!(A1, A2, A3, A4, A5, A6, A7, A8, A9);
impl_native_traits!(A1, A2, A3, A4, A5, A6, A7, A8, A9, A10);
impl_native_traits!(A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11);
impl_native_traits!(A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12);
impl_native_traits!(A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12, A13);
impl_native_traits!(A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12, A13, A14);
impl_native_traits!(A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12, A13, A14, A15);
impl_native_traits!(A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12, A13, A14, A15, A16);
impl_native_traits!(A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12, A13, A14, A15, A16, A17);
impl_native_traits!(
    A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12, A13, A14, A15, A16, A17, A18
);
impl_native_traits!(
    A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12, A13, A14, A15, A16, A17, A18, A19
);
impl_native_traits!(
    A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12, A13, A14, A15, A16, A17, A18, A19, A20
);

'''
'''--- lib/api/src/sys/ptr.rs ---
//! Types for a reusable pointer abstraction for accessing Wasm linear memory.
//!
//! This abstraction is safe: it ensures the memory is in bounds and that the pointer
//! is aligned (avoiding undefined behavior).
//!
//! Therefore, you should use this abstraction whenever possible to avoid memory
//! related bugs when implementing an ABI.

use crate::sys::cell::WasmCell;
use crate::sys::{externals::Memory, FromToNativeWasmType};
use std::{cell::Cell, fmt, marker::PhantomData, mem};
use wasmer_types::ValueType;

/// The `Array` marker type. This type can be used like `WasmPtr<T, Array>`
/// to get access to methods
pub struct Array;
/// The `Item` marker type. This is the default and does not usually need to be
/// specified.
pub struct Item;

/// A zero-cost type that represents a pointer to something in Wasm linear
/// memory.
///
/// This type can be used directly in the host function arguments:
/// ```
/// # use wasmer::Memory;
/// # use wasmer::WasmPtr;
/// pub fn host_import(memory: Memory, ptr: WasmPtr<u32>) {
///     let derefed_ptr = ptr.deref(&memory).expect("pointer in bounds");
///     let inner_val: u32 = derefed_ptr.get();
///     println!("Got {} from Wasm memory address 0x{:X}", inner_val, ptr.offset());
///     // update the value being pointed to
///     derefed_ptr.set(inner_val + 1);
/// }
/// ```
///
/// This type can also be used with primitive-filled structs, but be careful of
/// guarantees required by `ValueType`.
/// ```
/// # use wasmer::Memory;
/// # use wasmer::WasmPtr;
/// # use wasmer::ValueType;
///
/// #[derive(Copy, Clone, Debug)]
/// #[repr(C)]
/// struct V3 {
///     x: f32,
///     y: f32,
///     z: f32
/// }
/// // This is safe as the 12 bytes represented by this struct
/// // are valid for all bit combinations.
/// unsafe impl ValueType for V3 {
/// }
///
/// fn update_vector_3(memory: Memory, ptr: WasmPtr<V3>) {
///     let derefed_ptr = ptr.deref(&memory).expect("pointer in bounds");
///     let mut inner_val: V3 = derefed_ptr.get();
///     println!("Got {:?} from Wasm memory address 0x{:X}", inner_val, ptr.offset());
///     // update the value being pointed to
///     inner_val.x = 10.4;
///     derefed_ptr.set(inner_val);
/// }
/// ```
#[repr(transparent)]
pub struct WasmPtr<T: Copy, Ty = Item> {
    offset: u32,
    _phantom: PhantomData<(T, Ty)>,
}

/// Methods relevant to all types of `WasmPtr`.
impl<T: Copy, Ty> WasmPtr<T, Ty> {
    /// Create a new `WasmPtr` at the given offset.
    #[inline]
    pub fn new(offset: u32) -> Self {
        Self {
            offset,
            _phantom: PhantomData,
        }
    }

    /// Get the offset into Wasm linear memory for this `WasmPtr`.
    #[inline]
    pub fn offset(self) -> u32 {
        self.offset
    }
}

#[inline(always)]
fn align_pointer(ptr: usize, align: usize) -> usize {
    // clears bits below aligment amount (assumes power of 2) to align pointer
    debug_assert!(align.count_ones() == 1);
    ptr & !(align - 1)
}

/// Methods for `WasmPtr`s to data that can be dereferenced, namely to types
/// that implement [`ValueType`], meaning that they're valid for all possible
/// bit patterns.
impl<T: Copy + ValueType> WasmPtr<T, Item> {
    /// Dereference the `WasmPtr` getting access to a `&Cell<T>` allowing for
    /// reading and mutating of the inner value.
    ///
    /// This method is unsound if used with unsynchronized shared memory.
    /// If you're unsure what that means, it likely does not apply to you.
    /// This invariant will be enforced in the future.
    #[inline]
    pub fn deref<'a>(self, memory: &'a Memory) -> Option<WasmCell<'a, T>> {
        if (self.offset as usize) + mem::size_of::<T>() > memory.size().bytes().0
            || mem::size_of::<T>() == 0
        {
            return None;
        }
        unsafe {
            let cell_ptr = align_pointer(
                memory.view::<u8>().as_ptr().add(self.offset as usize) as usize,
                mem::align_of::<T>(),
            ) as *const Cell<T>;
            Some(WasmCell::new(&*cell_ptr))
        }
    }
}

/// Methods for `WasmPtr`s to arrays of data that can be dereferenced, namely to
/// types that implement [`ValueType`], meaning that they're valid for all
/// possible bit patterns.
impl<T: Copy + ValueType> WasmPtr<T, Array> {
    /// Dereference the `WasmPtr` getting access to a `&[Cell<T>]` allowing for
    /// reading and mutating of the inner values.
    ///
    /// This method is unsound if used with unsynchronized shared memory.
    /// If you're unsure what that means, it likely does not apply to you.
    /// This invariant will be enforced in the future.
    #[inline]
    pub fn deref<'a>(
        self,
        memory: &'a Memory,
        index: u32,
        length: u32,
    ) -> Option<Vec<WasmCell<'a, T>>> {
        // gets the size of the item in the array with padding added such that
        // for any index, we will always result an aligned memory access
        let item_size = mem::size_of::<T>();
        let slice_full_len = index as usize + length as usize;
        let memory_size = memory.size().bytes().0;

        if (self.offset as usize) + (item_size * slice_full_len) > memory_size
            || (self.offset as usize) >= memory_size
            || item_size == 0
        {
            return None;
        }
        let cell_ptrs = unsafe {
            let cell_ptr = align_pointer(
                memory.view::<u8>().as_ptr().add(self.offset as usize) as usize,
                mem::align_of::<T>(),
            ) as *const Cell<T>;
            &std::slice::from_raw_parts(cell_ptr, slice_full_len)[index as usize..slice_full_len]
        };

        let wasm_cells = cell_ptrs
            .iter()
            .map(|ptr| WasmCell::new(ptr))
            .collect::<Vec<_>>();
        Some(wasm_cells)
    }

    /// Get a UTF-8 string from the `WasmPtr` with the given length.
    ///
    /// Note that . The
    /// underlying data can be mutated if the Wasm is allowed to execute or
    /// an aliasing `WasmPtr` is used to mutate memory.
    ///
    /// # Safety
    /// This method returns a reference to Wasm linear memory. The underlying
    /// data can be mutated if the Wasm is allowed to execute or an aliasing
    /// `WasmPtr` is used to mutate memory.
    ///
    /// `str` has invariants that must not be broken by mutating Wasm memory.
    /// Thus the caller must ensure that the backing memory is not modified
    /// while the reference is held.
    ///
    /// Additionally, if `memory` is dynamic, the caller must also ensure that `memory`
    /// is not grown while the reference is held.
    pub unsafe fn get_utf8_str<'a>(self, memory: &'a Memory, str_len: u32) -> Option<&'a str> {
        let memory_size = memory.size().bytes().0;

        if self.offset as usize + str_len as usize > memory.size().bytes().0
            || self.offset as usize >= memory_size
        {
            return None;
        }
        let ptr = memory.view::<u8>().as_ptr().add(self.offset as usize) as *const u8;
        let slice: &[u8] = std::slice::from_raw_parts(ptr, str_len as usize);
        std::str::from_utf8(slice).ok()
    }

    /// Get a UTF-8 `String` from the `WasmPtr` with the given length.
    ///
    /// an aliasing `WasmPtr` is used to mutate memory.
    pub fn get_utf8_string(self, memory: &Memory, str_len: u32) -> Option<String> {
        let memory_size = memory.size().bytes().0;
        if self.offset as usize + str_len as usize > memory.size().bytes().0
            || self.offset as usize >= memory_size
        {
            return None;
        }

        // TODO: benchmark the internals of this function: there is likely room for
        // micro-optimization here and this may be a fairly common function in user code.
        let view = memory.view::<u8>();

        let mut vec: Vec<u8> = Vec::with_capacity(str_len as usize);
        let base = self.offset as usize;
        for i in 0..(str_len as usize) {
            let byte = view[base + i].get();
            vec.push(byte);
        }

        String::from_utf8(vec).ok()
    }

    /// Get a UTF-8 string from the `WasmPtr`, where the string is nul-terminated.
    ///
    /// Note that this does not account for UTF-8 strings that _contain_ nul themselves,
    /// [`WasmPtr::get_utf8_str`] has to be used for those.
    ///
    /// # Safety
    /// This method behaves similarly to [`WasmPtr::get_utf8_str`], all safety invariants on
    /// that method must also be upheld here.
    pub unsafe fn get_utf8_str_with_nul<'a>(self, memory: &'a Memory) -> Option<&'a str> {
        memory.view::<u8>()[(self.offset as usize)..]
            .iter()
            .map(|cell| cell.get())
            .position(|byte| byte == 0)
            .and_then(|length| self.get_utf8_str(memory, length as u32))
    }

    /// Get a UTF-8 `String` from the `WasmPtr`, where the string is nul-terminated.
    ///
    /// Note that this does not account for UTF-8 strings that _contain_ nul themselves,
    /// [`WasmPtr::get_utf8_string`] has to be used for those.
    pub fn get_utf8_string_with_nul(self, memory: &Memory) -> Option<String> {
        unsafe { self.get_utf8_str_with_nul(memory) }.map(|s| s.to_owned())
    }
}

unsafe impl<T: Copy, Ty> FromToNativeWasmType for WasmPtr<T, Ty> {
    type Native = i32;

    fn to_native(self) -> Self::Native {
        self.offset as i32
    }
    fn from_native(n: Self::Native) -> Self {
        Self {
            offset: n as u32,
            _phantom: PhantomData,
        }
    }
}

unsafe impl<T: Copy, Ty> ValueType for WasmPtr<T, Ty> {}

impl<T: Copy, Ty> Clone for WasmPtr<T, Ty> {
    fn clone(&self) -> Self {
        Self {
            offset: self.offset,
            _phantom: PhantomData,
        }
    }
}

impl<T: Copy, Ty> Copy for WasmPtr<T, Ty> {}

impl<T: Copy, Ty> PartialEq for WasmPtr<T, Ty> {
    fn eq(&self, other: &Self) -> bool {
        self.offset == other.offset
    }
}

impl<T: Copy, Ty> Eq for WasmPtr<T, Ty> {}

impl<T: Copy, Ty> fmt::Debug for WasmPtr<T, Ty> {
    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
        write!(
            f,
            "WasmPtr(offset: {}, pointer: {:#x}, align: {})",
            self.offset,
            self.offset,
            mem::align_of::<T>()
        )
    }
}

#[cfg(test)]
mod test {
    use super::*;
    use crate::sys::{Memory, MemoryType, Store};

    /// Ensure that memory accesses work on the edges of memory and that out of
    /// bounds errors are caught with `deref`
    #[test]
    fn wasm_ptr_memory_bounds_checks_hold() {
        // create a memory
        let store = Store::default();
        let memory_descriptor = MemoryType::new(1, Some(1), false);
        let memory = Memory::new(&store, memory_descriptor).unwrap();

        // test that basic access works and that len = 0 works, but oob does not
        let start_wasm_ptr: WasmPtr<u8> = WasmPtr::new(0);
        let start_wasm_ptr_array: WasmPtr<u8, Array> = WasmPtr::new(0);

        assert!(start_wasm_ptr.deref(&memory).is_some());
        assert!(start_wasm_ptr_array.deref(&memory, 0, 0).is_some());
        assert!(unsafe { start_wasm_ptr_array.get_utf8_str(&memory, 0).is_some() });
        assert!(start_wasm_ptr_array.get_utf8_string(&memory, 0).is_some());
        assert!(start_wasm_ptr_array.deref(&memory, 0, 1).is_some());

        // test that accessing the last valid memory address works correctly and OOB is caught
        let last_valid_address_for_u8 = (memory.size().bytes().0 - 1) as u32;
        let end_wasm_ptr: WasmPtr<u8> = WasmPtr::new(last_valid_address_for_u8);
        assert!(end_wasm_ptr.deref(&memory).is_some());

        let end_wasm_ptr_array: WasmPtr<u8, Array> = WasmPtr::new(last_valid_address_for_u8);

        assert!(end_wasm_ptr_array.deref(&memory, 0, 1).is_some());
        let invalid_idx_len_combos: [(u32, u32); 3] =
            [(last_valid_address_for_u8 + 1, 0), (0, 2), (1, 1)];
        for &(idx, len) in invalid_idx_len_combos.iter() {
            assert!(end_wasm_ptr_array.deref(&memory, idx, len).is_none());
        }
        assert!(unsafe { end_wasm_ptr_array.get_utf8_str(&memory, 2).is_none() });
        assert!(end_wasm_ptr_array.get_utf8_string(&memory, 2).is_none());

        // test that accesing the last valid memory address for a u32 is valid
        // (same as above test but with more edge cases to assert on)
        let last_valid_address_for_u32 = (memory.size().bytes().0 - 4) as u32;
        let end_wasm_ptr: WasmPtr<u32> = WasmPtr::new(last_valid_address_for_u32);
        assert!(end_wasm_ptr.deref(&memory).is_some());
        assert!(end_wasm_ptr.deref(&memory).is_some());

        let end_wasm_ptr_oob_array: [WasmPtr<u32>; 4] = [
            WasmPtr::new(last_valid_address_for_u32 + 1),
            WasmPtr::new(last_valid_address_for_u32 + 2),
            WasmPtr::new(last_valid_address_for_u32 + 3),
            WasmPtr::new(last_valid_address_for_u32 + 4),
        ];
        for oob_end_ptr in end_wasm_ptr_oob_array.iter() {
            assert!(oob_end_ptr.deref(&memory).is_none());
        }
        let end_wasm_ptr_array: WasmPtr<u32, Array> = WasmPtr::new(last_valid_address_for_u32);
        assert!(end_wasm_ptr_array.deref(&memory, 0, 1).is_some());

        let invalid_idx_len_combos: [(u32, u32); 3] =
            [(last_valid_address_for_u32 + 1, 0), (0, 2), (1, 1)];
        for &(idx, len) in invalid_idx_len_combos.iter() {
            assert!(end_wasm_ptr_array.deref(&memory, idx, len).is_none());
        }

        let end_wasm_ptr_array_oob_array: [WasmPtr<u32, Array>; 4] = [
            WasmPtr::new(last_valid_address_for_u32 + 1),
            WasmPtr::new(last_valid_address_for_u32 + 2),
            WasmPtr::new(last_valid_address_for_u32 + 3),
            WasmPtr::new(last_valid_address_for_u32 + 4),
        ];

        for oob_end_array_ptr in end_wasm_ptr_array_oob_array.iter() {
            assert!(oob_end_array_ptr.deref(&memory, 0, 1).is_none());
            assert!(oob_end_array_ptr.deref(&memory, 1, 0).is_none());
        }
    }
}

'''
'''--- lib/api/src/sys/store.rs ---
use crate::sys::tunables::BaseTunables;
use std::fmt;
use std::sync::Arc;
#[cfg(all(feature = "compiler", feature = "engine"))]
use wasmer_compiler::CompilerConfig;
use wasmer_engine::Engine;
use wasmer_vm::Tunables;

/// The store represents all global state that can be manipulated by
/// WebAssembly programs. It consists of the runtime representation
/// of all instances of functions, tables, memories, and globals that
/// have been allocated during the lifetime of the abstract machine.
///
/// The `Store` holds the engine (that is ‚Äîamongst many things‚Äî used to compile
/// the Wasm bytes into a valid module artifact), in addition to the
/// [`Tunables`] (that are used to create the memories, tables and globals).
///
/// Spec: <https://webassembly.github.io/spec/core/exec/runtime.html#store>
#[derive(Clone)]
pub struct Store {
    engine: Arc<dyn Engine + Send + Sync>,
    tunables: Arc<dyn Tunables + Send + Sync>,
}

impl Store {
    /// Creates a new `Store` with a specific [`Engine`].
    pub fn new<E>(engine: &E) -> Self
    where
        E: Engine + ?Sized,
    {
        Self::new_with_tunables(engine, BaseTunables::for_target(engine.target()))
    }

    /// Creates a new `Store` with a specific [`Engine`] and [`Tunables`].
    pub fn new_with_tunables<E>(engine: &E, tunables: impl Tunables + Send + Sync + 'static) -> Self
    where
        E: Engine + ?Sized,
    {
        Self {
            engine: engine.cloned(),
            tunables: Arc::new(tunables),
        }
    }

    /// Returns the [`Tunables`].
    pub fn tunables(&self) -> &dyn Tunables {
        self.tunables.as_ref()
    }

    /// Returns the [`Engine`].
    pub fn engine(&self) -> &Arc<dyn Engine + Send + Sync> {
        &self.engine
    }

    /// Checks whether two stores are identical. A store is considered
    /// equal to another store if both have the same engine. The
    /// tunables are excluded from the logic.
    pub fn same(a: &Self, b: &Self) -> bool {
        a.engine.id() == b.engine.id()
    }
}

impl PartialEq for Store {
    fn eq(&self, other: &Self) -> bool {
        Self::same(self, other)
    }
}

unsafe impl Send for Store {}
unsafe impl Sync for Store {}

// We only implement default if we have assigned a default compiler and engine
#[cfg(all(feature = "default-compiler", feature = "default-engine"))]
impl Default for Store {
    fn default() -> Self {
        // We store them on a function that returns to make
        // sure this function doesn't emit a compile error even if
        // more than one compiler is enabled.
        #[allow(unreachable_code)]
        fn get_config() -> impl CompilerConfig + 'static {
            cfg_if::cfg_if! {
                if #[cfg(feature = "default-cranelift")] {
                    wasmer_compiler_cranelift::Cranelift::default()
                } else if #[cfg(feature = "default-llvm")] {
                    wasmer_compiler_llvm::LLVM::default()
                } else if #[cfg(feature = "default-singlepass")] {
                    wasmer_compiler_singlepass::Singlepass::default()
                } else {
                    compile_error!("No default compiler chosen")
                }
            }
        }

        #[allow(unreachable_code, unused_mut)]
        fn get_engine(mut config: impl CompilerConfig + 'static) -> impl Engine + Send + Sync {
            cfg_if::cfg_if! {
                if #[cfg(feature = "default-universal")] {
                    wasmer_engine_universal::Universal::new(config)
                        .engine()
                } else if #[cfg(feature = "default-dylib")] {
                    wasmer_engine_dylib::Dylib::new(config)
                        .engine()
                } else {
                    compile_error!("No default engine chosen")
                }
            }
        }

        let config = get_config();
        let engine = get_engine(config);
        let tunables = BaseTunables::for_target(engine.target());
        Self::new_with_tunables(&engine, tunables)
    }
}

impl fmt::Debug for Store {
    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
        f.debug_struct("Store").finish()
    }
}

/// A trait represinting any object that lives in the `Store`.
pub trait StoreObject {
    /// Return true if the object `Store` is the same as the provided `Store`.
    fn comes_from_same_store(&self, store: &Store) -> bool;
}

'''
'''--- lib/api/src/sys/tunables.rs ---
use crate::sys::{MemoryType, Pages, TableType};
use std::ptr::NonNull;
use std::sync::Arc;
use target_lexicon::PointerWidth;
use wasmer_compiler::Target;
use wasmer_vm::MemoryError;
use wasmer_vm::{
    LinearMemory, LinearTable, Memory, MemoryStyle, Table, TableStyle, Tunables,
    VMMemoryDefinition, VMTableDefinition,
};

/// Tunable parameters for WebAssembly compilation.
/// This is the reference implementation of the `Tunables` trait,
/// used by default.
///
/// You can use this as a template for creating a custom Tunables
/// implementation or use composition to wrap your Tunables around
/// this one. The later approach is demonstrated in the
/// tunables-limit-memory example.
#[derive(Clone)]
pub struct BaseTunables {
    /// For static heaps, the size in wasm pages of the heap protected by bounds checking.
    pub static_memory_bound: Pages,

    /// The size in bytes of the offset guard for static heaps.
    pub static_memory_offset_guard_size: u64,

    /// The size in bytes of the offset guard for dynamic heaps.
    pub dynamic_memory_offset_guard_size: u64,
}

impl BaseTunables {
    /// Get the `BaseTunables` for a specific Target
    pub fn for_target(target: &Target) -> Self {
        let triple = target.triple();
        let pointer_width: PointerWidth = triple.pointer_width().unwrap();
        let (static_memory_bound, static_memory_offset_guard_size): (Pages, u64) =
            match pointer_width {
                PointerWidth::U16 => (0x400.into(), 0x1000),
                PointerWidth::U32 => (0x4000.into(), 0x1_0000),
                // Static Memory Bound:
                //   Allocating 4 GiB of address space let us avoid the
                //   need for explicit bounds checks.
                // Static Memory Guard size:
                //   Allocating 2 GiB of address space lets us translate wasm
                //   offsets into x86 offsets as aggressively as we can.
                PointerWidth::U64 => (0x1_0000.into(), 0x8000_0000),
            };

        // Allocate a small guard to optimize common cases but without
        // wasting too much memory.
        // The Windows memory manager seems more laxed than the other ones
        // And a guard of just 1 page may not be enough is some borderline cases
        // So using 2 pages for guard on this platform
        #[cfg(target_os = "windows")]
        let dynamic_memory_offset_guard_size: u64 = 0x2_0000;
        #[cfg(not(target_os = "windows"))]
        let dynamic_memory_offset_guard_size: u64 = 0x1_0000;

        Self {
            static_memory_bound,
            static_memory_offset_guard_size,
            dynamic_memory_offset_guard_size,
        }
    }
}

impl Tunables for BaseTunables {
    /// Get a `MemoryStyle` for the provided `MemoryType`
    fn memory_style(&self, memory: &MemoryType) -> MemoryStyle {
        // A heap with a maximum that doesn't exceed the static memory bound specified by the
        // tunables make it static.
        //
        // If the module doesn't declare an explicit maximum treat it as 4GiB.
        let maximum = memory.maximum.unwrap_or_else(Pages::max_value);
        if maximum <= self.static_memory_bound {
            MemoryStyle::Static {
                // Bound can be larger than the maximum for performance reasons
                bound: self.static_memory_bound,
                offset_guard_size: self.static_memory_offset_guard_size,
            }
        } else {
            MemoryStyle::Dynamic {
                offset_guard_size: self.dynamic_memory_offset_guard_size,
            }
        }
    }

    /// Get a [`TableStyle`] for the provided [`TableType`].
    fn table_style(&self, _table: &TableType) -> TableStyle {
        TableStyle::CallerChecksSignature
    }

    /// Create a memory owned by the host given a [`MemoryType`] and a [`MemoryStyle`].
    fn create_host_memory(
        &self,
        ty: &MemoryType,
        style: &MemoryStyle,
    ) -> Result<Arc<dyn Memory>, MemoryError> {
        Ok(Arc::new(LinearMemory::new(&ty, &style)?))
    }

    /// Create a memory owned by the VM given a [`MemoryType`] and a [`MemoryStyle`].
    ///
    /// # Safety
    /// - `vm_definition_location` must point to a valid, owned `VMMemoryDefinition`,
    ///   for example in `VMContext`.
    unsafe fn create_vm_memory(
        &self,
        ty: &MemoryType,
        style: &MemoryStyle,
        vm_definition_location: NonNull<VMMemoryDefinition>,
    ) -> Result<Arc<dyn Memory>, MemoryError> {
        Ok(Arc::new(LinearMemory::from_definition(
            &ty,
            &style,
            vm_definition_location,
        )?))
    }

    /// Create a table owned by the host given a [`TableType`] and a [`TableStyle`].
    fn create_host_table(
        &self,
        ty: &TableType,
        style: &TableStyle,
    ) -> Result<Arc<dyn Table>, String> {
        Ok(Arc::new(LinearTable::new(&ty, &style)?))
    }

    /// Create a table owned by the VM given a [`TableType`] and a [`TableStyle`].
    ///
    /// # Safety
    /// - `vm_definition_location` must point to a valid, owned `VMTableDefinition`,
    ///   for example in `VMContext`.
    unsafe fn create_vm_table(
        &self,
        ty: &TableType,
        style: &TableStyle,
        vm_definition_location: NonNull<VMTableDefinition>,
    ) -> Result<Arc<dyn Table>, String> {
        Ok(Arc::new(LinearTable::from_definition(
            &ty,
            &style,
            vm_definition_location,
        )?))
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn memory_style() {
        let tunables = BaseTunables {
            static_memory_bound: Pages(2048),
            static_memory_offset_guard_size: 128,
            dynamic_memory_offset_guard_size: 256,
        };

        // No maximum
        let requested = MemoryType::new(3, None, true);
        let style = tunables.memory_style(&requested);
        match style {
            MemoryStyle::Dynamic { offset_guard_size } => assert_eq!(offset_guard_size, 256),
            s => panic!("Unexpected memory style: {:?}", s),
        }

        // Large maximum
        let requested = MemoryType::new(3, Some(5_000_000), true);
        let style = tunables.memory_style(&requested);
        match style {
            MemoryStyle::Dynamic { offset_guard_size } => assert_eq!(offset_guard_size, 256),
            s => panic!("Unexpected memory style: {:?}", s),
        }

        // Small maximum
        let requested = MemoryType::new(3, Some(16), true);
        let style = tunables.memory_style(&requested);
        match style {
            MemoryStyle::Static {
                bound,
                offset_guard_size,
            } => {
                assert_eq!(bound, Pages(2048));
                assert_eq!(offset_guard_size, 128);
            }
            s => panic!("Unexpected memory style: {:?}", s),
        }
    }
}

'''
'''--- lib/api/src/sys/types.rs ---
use crate::sys::externals::Function;
use crate::sys::store::{Store, StoreObject};
use crate::sys::RuntimeError;
use wasmer_types::Value;
pub use wasmer_types::{
    ExportType, ExternType, FunctionType, GlobalType, MemoryType, Mutability, TableType,
    Type as ValType,
};
use wasmer_vm::VMFuncRef;

/// WebAssembly computations manipulate values of basic value types:
/// * Integers (32 or 64 bit width)
/// * Floating-point (32 or 64 bit width)
/// * Vectors (128 bits, with 32 or 64 bit lanes)
///
/// Spec: <https://webassembly.github.io/spec/core/exec/runtime.html#values>
pub type Val = Value<Function>;

impl StoreObject for Val {
    fn comes_from_same_store(&self, store: &Store) -> bool {
        match self {
            Self::FuncRef(None) => true,
            Self::FuncRef(Some(f)) => Store::same(store, f.store()),
            // `ExternRef`s are not tied to specific stores
            Self::ExternRef(_) => true,
            Self::I32(_) | Self::I64(_) | Self::F32(_) | Self::F64(_) | Self::V128(_) => true,
        }
    }
}

impl From<Function> for Val {
    fn from(val: Function) -> Self {
        Self::FuncRef(Some(val))
    }
}

/// It provides useful functions for converting back and forth
/// from [`Val`] into `FuncRef`.
pub trait ValFuncRef {
    fn into_vm_funcref(&self, store: &Store) -> Result<VMFuncRef, RuntimeError>;

    fn from_vm_funcref(item: VMFuncRef, store: &Store) -> Self;

    fn into_table_reference(&self, store: &Store) -> Result<wasmer_vm::TableElement, RuntimeError>;

    fn from_table_reference(item: wasmer_vm::TableElement, store: &Store) -> Self;
}

impl ValFuncRef for Val {
    fn into_vm_funcref(&self, store: &Store) -> Result<VMFuncRef, RuntimeError> {
        if !self.comes_from_same_store(store) {
            return Err(RuntimeError::new("cross-`Store` values are not supported"));
        }
        Ok(match self {
            Self::FuncRef(None) => VMFuncRef::null(),
            Self::FuncRef(Some(f)) => f.vm_funcref(),
            _ => return Err(RuntimeError::new("val is not func ref")),
        })
    }

    fn from_vm_funcref(func_ref: VMFuncRef, store: &Store) -> Self {
        if func_ref.is_null() {
            return Self::FuncRef(None);
        }
        let item: &wasmer_vm::VMCallerCheckedAnyfunc = unsafe {
            let anyfunc: *const wasmer_vm::VMCallerCheckedAnyfunc = *func_ref;
            &*anyfunc
        };
        let export = wasmer_vm::ExportFunction {
            // TODO:
            // figure out if we ever need a value here: need testing with complicated import patterns
            metadata: None,
            vm_function: wasmer_vm::VMFunction {
                address: item.func_ptr,
                signature: item.type_index,
                // TODO: review this comment (unclear if it's still correct):
                // All functions in tables are already Static (as dynamic functions
                // are converted to use the trampolines with static signatures).
                kind: wasmer_vm::VMFunctionKind::Static,
                vmctx: item.vmctx,
                call_trampoline: None,
                instance_ref: None,
            },
        };
        let f = Function::from_vm_export(store, export);
        Self::FuncRef(Some(f))
    }

    fn into_table_reference(&self, store: &Store) -> Result<wasmer_vm::TableElement, RuntimeError> {
        if !self.comes_from_same_store(store) {
            return Err(RuntimeError::new("cross-`Store` values are not supported"));
        }
        Ok(match self {
            // TODO(reftypes): review this clone
            Self::ExternRef(extern_ref) => {
                wasmer_vm::TableElement::ExternRef(extern_ref.clone().into())
            }
            Self::FuncRef(None) => wasmer_vm::TableElement::FuncRef(VMFuncRef::null()),
            Self::FuncRef(Some(f)) => wasmer_vm::TableElement::FuncRef(f.vm_funcref()),
            _ => return Err(RuntimeError::new("val is not reference")),
        })
    }

    fn from_table_reference(item: wasmer_vm::TableElement, store: &Store) -> Self {
        match item {
            wasmer_vm::TableElement::FuncRef(f) => Self::from_vm_funcref(f, store),
            wasmer_vm::TableElement::ExternRef(extern_ref) => Self::ExternRef(extern_ref.into()),
        }
    }
}

'''
'''--- lib/api/src/sys/utils.rs ---
/// Check if the provided bytes are wasm-like
pub fn is_wasm(bytes: impl AsRef<[u8]>) -> bool {
    bytes.as_ref().starts_with(b"\0asm")
}

'''
'''--- lib/api/tests/sys_export.rs ---
#[cfg(feature = "sys")]
mod sys {
    use anyhow::Result;
    use wasmer::*;
    use wasmer_vm::{VMGlobal, VMMemory, VMTable, WeakOrStrongInstanceRef};

    const MEM_WAT: &str = "
    (module
      (func $host_fn (import \"env\" \"host_fn\") (param) (result))
      (func (export \"call_host_fn\") (param) (result)
          (call $host_fn))

      (memory $mem 0)
      (export \"memory\" (memory $mem))
      )
";

    const GLOBAL_WAT: &str = "
    (module
      (func $host_fn (import \"env\" \"host_fn\") (param) (result))
      (func (export \"call_host_fn\") (param) (result)
          (call $host_fn))

      (global $global i32 (i32.const 11))
      (export \"global\" (global $global))
      )
";

    const TABLE_WAT: &str = "
    (module
      (func $host_fn (import \"env\" \"host_fn\") (param) (result))
      (func (export \"call_host_fn\") (param) (result)
          (call $host_fn))

      (table $table 4 4 funcref)
      (export \"table\" (table $table))
      )
";

    const FUNCTION_WAT: &str = "
    (module
      (func $host_fn (import \"env\" \"host_fn\") (param) (result))
      (func (export \"call_host_fn\") (param) (result)
          (call $host_fn))
      )
";

    fn is_memory_instance_ref_strong(memory: &VMMemory) -> Option<bool> {
        // This is safe because we're calling it from a test to test the internals
        memory
            .instance_ref
            .as_ref()
            .map(|v| matches!(v, WeakOrStrongInstanceRef::Strong(_)))
    }

    fn is_table_instance_ref_strong(table: &VMTable) -> Option<bool> {
        // This is safe because we're calling it from a test to test the internals
        table
            .instance_ref
            .as_ref()
            .map(|v| matches!(v, WeakOrStrongInstanceRef::Strong(_)))
    }

    fn is_global_instance_ref_strong(global: &VMGlobal) -> Option<bool> {
        // This is safe because we're calling it from a test to test the internals
        global
            .instance_ref
            .as_ref()
            .map(|v| matches!(v, WeakOrStrongInstanceRef::Strong(_)))
    }

    fn is_function_instance_ref_strong(f: &Function) -> Option<bool> {
        // This is safe because we're calling it from a test to test the internals
        unsafe {
            f.get_vm_function()
                .instance_ref
                .as_ref()
                .map(|v| matches!(v, WeakOrStrongInstanceRef::Strong(_)))
        }
    }

    fn is_native_function_instance_ref_strong<Args, Rets>(
        f: &NativeFunc<Args, Rets>,
    ) -> Option<bool>
    where
        Args: WasmTypeList,
        Rets: WasmTypeList,
    {
        // This is safe because we're calling it from a test to test the internals
        unsafe {
            f.get_vm_function()
                .instance_ref
                .as_ref()
                .map(|v| matches!(v, WeakOrStrongInstanceRef::Strong(_)))
        }
    }

    #[test]
    fn strong_weak_behavior_works_memory() -> Result<()> {
        #[derive(Clone, Debug, WasmerEnv, Default)]
        struct MemEnv {
            #[wasmer(export)]
            memory: LazyInit<Memory>,
        }

        let host_fn = |env: &MemEnv| unsafe {
            let mem = env.memory_ref().unwrap();
            assert_eq!(
                is_memory_instance_ref_strong(&mem.get_vm_memory()),
                Some(false)
            );
            let mem_clone = mem.clone();
            assert_eq!(
                is_memory_instance_ref_strong(&mem_clone.get_vm_memory()),
                Some(true)
            );
            assert_eq!(
                is_memory_instance_ref_strong(&mem.get_vm_memory()),
                Some(false)
            );
        };

        let f: NativeFunc<(), ()> = {
            let store = Store::default();
            let module = Module::new(&store, MEM_WAT)?;
            let env = MemEnv::default();

            let instance = Instance::new(
                &module,
                &imports! {
                    "env" => {
                        "host_fn" => Function::new_native_with_env(&store, env, host_fn)
                    }
                },
            )?;

            {
                if let Some(Export::Memory(mem)) = instance.lookup("memory") {
                    assert_eq!(is_memory_instance_ref_strong(&mem), Some(true));
                } else {
                    panic!("not a memory");
                }
            }

            let f: NativeFunc<(), ()> = instance.get_native_function("call_host_fn").unwrap();
            f.call()?;
            f
        };
        f.call()?;

        Ok(())
    }

    #[test]
    fn strong_weak_behavior_works_global() -> Result<()> {
        #[derive(Clone, Debug, WasmerEnv, Default)]
        struct GlobalEnv {
            #[wasmer(export)]
            global: LazyInit<Global>,
        }

        let host_fn = |env: &GlobalEnv| unsafe {
            let global = env.global_ref().unwrap();
            assert_eq!(
                is_global_instance_ref_strong(&global.get_vm_global()),
                Some(false)
            );
            let global_clone = global.clone();
            assert_eq!(
                is_global_instance_ref_strong(&global_clone.get_vm_global()),
                Some(true)
            );
            assert_eq!(
                is_global_instance_ref_strong(&global.get_vm_global()),
                Some(false)
            );
        };

        let f: NativeFunc<(), ()> = {
            let store = Store::default();
            let module = Module::new(&store, GLOBAL_WAT)?;
            let env = GlobalEnv::default();

            let instance = Instance::new(
                &module,
                &imports! {
                    "env" => {
                        "host_fn" => Function::new_native_with_env(&store, env, host_fn)
                    }
                },
            )?;

            {
                if let Some(Export::Global(global)) = instance.lookup("global") {
                    assert_eq!(is_global_instance_ref_strong(&global), Some(true));
                } else {
                    panic!("not a global");
                }
            }

            let f: NativeFunc<(), ()> = instance.get_native_function("call_host_fn").unwrap();
            f.call()?;
            f
        };
        f.call()?;

        Ok(())
    }

    #[test]
    fn strong_weak_behavior_works_table() -> Result<()> {
        #[derive(Clone, WasmerEnv, Default)]
        struct TableEnv {
            #[wasmer(export)]
            table: LazyInit<Table>,
        }

        let host_fn = |env: &TableEnv| unsafe {
            let table = env.table_ref().unwrap();
            assert_eq!(
                is_table_instance_ref_strong(&table.get_vm_table()),
                Some(false)
            );
            let table_clone = table.clone();
            assert_eq!(
                is_table_instance_ref_strong(&table_clone.get_vm_table()),
                Some(true)
            );
            assert_eq!(
                is_table_instance_ref_strong(&table.get_vm_table()),
                Some(false)
            );
        };

        let f: NativeFunc<(), ()> = {
            let store = Store::default();
            let module = Module::new(&store, TABLE_WAT)?;
            let env = TableEnv::default();

            let instance = Instance::new(
                &module,
                &imports! {
                    "env" => {
                        "host_fn" => Function::new_native_with_env(&store, env, host_fn)
                    }
                },
            )?;

            {
                if let Some(Export::Table(table)) = instance.lookup("table") {
                    assert_eq!(is_table_instance_ref_strong(&table), Some(true));
                } else {
                    panic!("not a table");
                }
            }

            let f: NativeFunc<(), ()> = instance.get_native_function("call_host_fn").unwrap();
            f.call()?;
            f
        };
        f.call()?;

        Ok(())
    }

    #[test]
    fn strong_weak_behavior_works_function() -> Result<()> {
        #[derive(Clone, WasmerEnv, Default)]
        struct FunctionEnv {
            #[wasmer(export)]
            call_host_fn: LazyInit<Function>,
        }

        let host_fn = |env: &FunctionEnv| {
            let function = env.call_host_fn_ref().unwrap();
            assert_eq!(is_function_instance_ref_strong(&function), Some(false));
            let function_clone = function.clone();
            assert_eq!(is_function_instance_ref_strong(&function_clone), Some(true));
            assert_eq!(is_function_instance_ref_strong(&function), Some(false));
        };

        let f: NativeFunc<(), ()> = {
            let store = Store::default();
            let module = Module::new(&store, FUNCTION_WAT)?;
            let env = FunctionEnv::default();

            let instance = Instance::new(
                &module,
                &imports! {
                    "env" => {
                        "host_fn" => Function::new_native_with_env(&store, env, host_fn)
                    }
                },
            )?;

            {
                let function = instance.lookup_function("call_host_fn").unwrap();
                assert_eq!(is_function_instance_ref_strong(&function), Some(true));
            }

            let f: NativeFunc<(), ()> = instance.get_native_function("call_host_fn").unwrap();
            f.call()?;
            f
        };
        f.call()?;

        Ok(())
    }

    #[test]
    fn strong_weak_behavior_works_native_function() -> Result<()> {
        #[derive(Clone, WasmerEnv, Default)]
        struct FunctionEnv {
            #[wasmer(export)]
            call_host_fn: LazyInit<NativeFunc<(), ()>>,
        }

        let host_fn = |env: &FunctionEnv| {
            let function = env.call_host_fn_ref().unwrap();
            assert_eq!(
                is_native_function_instance_ref_strong(&function),
                Some(false)
            );
            let function_clone = function.clone();
            assert_eq!(
                is_native_function_instance_ref_strong(&function_clone),
                Some(true)
            );
            assert_eq!(
                is_native_function_instance_ref_strong(&function),
                Some(false)
            );
        };

        let f: NativeFunc<(), ()> = {
            let store = Store::default();
            let module = Module::new(&store, FUNCTION_WAT)?;
            let env = FunctionEnv::default();

            let instance = Instance::new(
                &module,
                &imports! {
                    "env" => {
                        "host_fn" => Function::new_native_with_env(&store, env, host_fn)
                    }
                },
            )?;

            {
                let function: NativeFunc<(), ()> =
                    instance.get_native_function("call_host_fn").unwrap();
                assert_eq!(
                    is_native_function_instance_ref_strong(&function),
                    Some(true)
                );
            }

            let f: NativeFunc<(), ()> = instance.get_native_function("call_host_fn").unwrap();
            f.call()?;
            f
        };
        f.call()?;

        Ok(())
    }
}

'''
'''--- lib/api/tests/sys_externals.rs ---
#[cfg(feature = "sys")]
mod sys {
    use anyhow::Result;
    use wasmer::*;

    #[test]
    fn global_new() -> Result<()> {
        let store = Store::default();
        let global = Global::new(&store, Value::I32(10));
        assert_eq!(
            *global.ty(),
            GlobalType {
                ty: Type::I32,
                mutability: Mutability::Const
            }
        );

        let global_mut = Global::new_mut(&store, Value::I32(10));
        assert_eq!(
            *global_mut.ty(),
            GlobalType {
                ty: Type::I32,
                mutability: Mutability::Var
            }
        );

        Ok(())
    }

    #[test]
    fn global_get() -> Result<()> {
        let store = Store::default();
        let global_i32 = Global::new(&store, Value::I32(10));
        assert_eq!(global_i32.get(), Value::I32(10));
        let global_i64 = Global::new(&store, Value::I64(20));
        assert_eq!(global_i64.get(), Value::I64(20));
        let global_f32 = Global::new(&store, Value::F32(10.0));
        assert_eq!(global_f32.get(), Value::F32(10.0));
        let global_f64 = Global::new(&store, Value::F64(20.0));
        assert_eq!(global_f64.get(), Value::F64(20.0));

        Ok(())
    }

    #[test]
    fn global_set() -> Result<()> {
        let store = Store::default();
        let global_i32 = Global::new(&store, Value::I32(10));
        // Set on a constant should error
        assert!(global_i32.set(Value::I32(20)).is_err());

        let global_i32_mut = Global::new_mut(&store, Value::I32(10));
        // Set on different type should error
        assert!(global_i32_mut.set(Value::I64(20)).is_err());

        // Set on same type should succeed
        global_i32_mut.set(Value::I32(20))?;
        assert_eq!(global_i32_mut.get(), Value::I32(20));

        Ok(())
    }

    #[test]
    fn table_new() -> Result<()> {
        let store = Store::default();
        let table_type = TableType {
            ty: Type::FuncRef,
            minimum: 0,
            maximum: None,
        };
        let f = Function::new_native(&store, || {});
        let table = Table::new(&store, table_type, Value::FuncRef(Some(f)))?;
        assert_eq!(*table.ty(), table_type);

        // Anyrefs not yet supported
        // let table_type = TableType {
        //     ty: Type::ExternRef,
        //     minimum: 0,
        //     maximum: None,
        // };
        // let table = Table::new(&store, table_type, Value::ExternRef(ExternRef::Null))?;
        // assert_eq!(*table.ty(), table_type);

        Ok(())
    }

    #[test]
    #[ignore]
    fn table_get() -> Result<()> {
        let store = Store::default();
        let table_type = TableType {
            ty: Type::FuncRef,
            minimum: 0,
            maximum: Some(1),
        };
        let f = Function::new_native(&store, |num: i32| num + 1);
        let table = Table::new(&store, table_type, Value::FuncRef(Some(f.clone())))?;
        assert_eq!(*table.ty(), table_type);
        let _elem = table.get(0).unwrap();
        // assert_eq!(elem.funcref().unwrap(), f);
        Ok(())
    }

    #[test]
    #[ignore]
    fn table_set() -> Result<()> {
        // Table set not yet tested
        Ok(())
    }

    #[test]
    fn table_grow() -> Result<()> {
        let store = Store::default();
        let table_type = TableType {
            ty: Type::FuncRef,
            minimum: 0,
            maximum: Some(10),
        };
        let f = Function::new_native(&store, |num: i32| num + 1);
        let table = Table::new(&store, table_type, Value::FuncRef(Some(f.clone())))?;
        // Growing to a bigger maximum should return None
        let old_len = table.grow(12, Value::FuncRef(Some(f.clone())));
        assert!(old_len.is_err());

        // Growing to a bigger maximum should return None
        let old_len = table.grow(5, Value::FuncRef(Some(f.clone())))?;
        assert_eq!(old_len, 0);

        Ok(())
    }

    #[test]
    #[ignore]
    fn table_copy() -> Result<()> {
        // TODO: table copy test not yet implemented
        Ok(())
    }

    #[test]
    fn memory_new() -> Result<()> {
        let store = Store::default();
        let memory_type = MemoryType {
            shared: false,
            minimum: Pages(0),
            maximum: Some(Pages(10)),
        };
        let memory = Memory::new(&store, memory_type)?;
        assert_eq!(memory.size(), Pages(0));
        assert_eq!(memory.ty(), memory_type);
        Ok(())
    }

    #[test]
    fn memory_grow() -> Result<()> {
        let store = Store::default();

        let desc = MemoryType::new(Pages(10), Some(Pages(16)), false);
        let memory = Memory::new(&store, desc)?;
        assert_eq!(memory.size(), Pages(10));

        let result = memory.grow(Pages(2)).unwrap();
        assert_eq!(result, Pages(10));
        assert_eq!(memory.size(), Pages(12));

        let result = memory.grow(Pages(10));
        assert_eq!(
            result,
            Err(MemoryError::CouldNotGrow {
                current: 12.into(),
                attempted_delta: 10.into()
            })
        );

        let bad_desc = MemoryType::new(Pages(15), Some(Pages(10)), false);
        let bad_result = Memory::new(&store, bad_desc);

        assert!(matches!(bad_result, Err(MemoryError::InvalidMemory { .. })));

        Ok(())
    }

    #[test]
    fn function_new() -> Result<()> {
        let store = Store::default();
        let function = Function::new_native(&store, || {});
        assert_eq!(function.ty().clone(), FunctionType::new(vec![], vec![]));
        let function = Function::new_native(&store, |_a: i32| {});
        assert_eq!(
            function.ty().clone(),
            FunctionType::new(vec![Type::I32], vec![])
        );
        let function = Function::new_native(&store, |_a: i32, _b: i64, _c: f32, _d: f64| {});
        assert_eq!(
            function.ty().clone(),
            FunctionType::new(vec![Type::I32, Type::I64, Type::F32, Type::F64], vec![])
        );
        let function = Function::new_native(&store, || -> i32 { 1 });
        assert_eq!(
            function.ty().clone(),
            FunctionType::new(vec![], vec![Type::I32])
        );
        let function =
            Function::new_native(&store, || -> (i32, i64, f32, f64) { (1, 2, 3.0, 4.0) });
        assert_eq!(
            function.ty().clone(),
            FunctionType::new(vec![], vec![Type::I32, Type::I64, Type::F32, Type::F64])
        );
        Ok(())
    }

    #[test]
    fn function_new_env() -> Result<()> {
        let store = Store::default();
        #[derive(Clone, WasmerEnv)]
        struct MyEnv {}

        let my_env = MyEnv {};
        let function = Function::new_native_with_env(&store, my_env.clone(), |_env: &MyEnv| {});
        assert_eq!(function.ty().clone(), FunctionType::new(vec![], vec![]));
        let function =
            Function::new_native_with_env(&store, my_env.clone(), |_env: &MyEnv, _a: i32| {});
        assert_eq!(
            function.ty().clone(),
            FunctionType::new(vec![Type::I32], vec![])
        );
        let function = Function::new_native_with_env(
            &store,
            my_env.clone(),
            |_env: &MyEnv, _a: i32, _b: i64, _c: f32, _d: f64| {},
        );
        assert_eq!(
            function.ty().clone(),
            FunctionType::new(vec![Type::I32, Type::I64, Type::F32, Type::F64], vec![])
        );
        let function =
            Function::new_native_with_env(&store, my_env.clone(), |_env: &MyEnv| -> i32 { 1 });
        assert_eq!(
            function.ty().clone(),
            FunctionType::new(vec![], vec![Type::I32])
        );
        let function = Function::new_native_with_env(
            &store,
            my_env.clone(),
            |_env: &MyEnv| -> (i32, i64, f32, f64) { (1, 2, 3.0, 4.0) },
        );
        assert_eq!(
            function.ty().clone(),
            FunctionType::new(vec![], vec![Type::I32, Type::I64, Type::F32, Type::F64])
        );
        Ok(())
    }

    #[test]
    fn function_new_dynamic() -> Result<()> {
        let store = Store::default();

        // Using &FunctionType signature
        let function_type = FunctionType::new(vec![], vec![]);
        let function = Function::new(&store, &function_type, |_values: &[Value]| unimplemented!());
        assert_eq!(function.ty().clone(), function_type);
        let function_type = FunctionType::new(vec![Type::I32], vec![]);
        let function = Function::new(&store, &function_type, |_values: &[Value]| unimplemented!());
        assert_eq!(function.ty().clone(), function_type);
        let function_type =
            FunctionType::new(vec![Type::I32, Type::I64, Type::F32, Type::F64], vec![]);
        let function = Function::new(&store, &function_type, |_values: &[Value]| unimplemented!());
        assert_eq!(function.ty().clone(), function_type);
        let function_type = FunctionType::new(vec![], vec![Type::I32]);
        let function = Function::new(&store, &function_type, |_values: &[Value]| unimplemented!());
        assert_eq!(function.ty().clone(), function_type);
        let function_type =
            FunctionType::new(vec![], vec![Type::I32, Type::I64, Type::F32, Type::F64]);
        let function = Function::new(&store, &function_type, |_values: &[Value]| unimplemented!());
        assert_eq!(function.ty().clone(), function_type);

        // Using array signature
        let function_type = ([Type::V128], [Type::I32, Type::F32, Type::F64]);
        let function = Function::new(&store, function_type, |_values: &[Value]| unimplemented!());
        assert_eq!(function.ty().params(), [Type::V128]);
        assert_eq!(function.ty().results(), [Type::I32, Type::F32, Type::F64]);

        Ok(())
    }

    #[test]
    fn function_new_dynamic_env() -> Result<()> {
        let store = Store::default();
        #[derive(Clone, WasmerEnv)]
        struct MyEnv {}
        let my_env = MyEnv {};

        // Using &FunctionType signature
        let function_type = FunctionType::new(vec![], vec![]);
        let function = Function::new_with_env(
            &store,
            &function_type,
            my_env.clone(),
            |_env: &MyEnv, _values: &[Value]| unimplemented!(),
        );
        assert_eq!(function.ty().clone(), function_type);
        let function_type = FunctionType::new(vec![Type::I32], vec![]);
        let function = Function::new_with_env(
            &store,
            &function_type,
            my_env.clone(),
            |_env: &MyEnv, _values: &[Value]| unimplemented!(),
        );
        assert_eq!(function.ty().clone(), function_type);
        let function_type =
            FunctionType::new(vec![Type::I32, Type::I64, Type::F32, Type::F64], vec![]);
        let function = Function::new_with_env(
            &store,
            &function_type,
            my_env.clone(),
            |_env: &MyEnv, _values: &[Value]| unimplemented!(),
        );
        assert_eq!(function.ty().clone(), function_type);
        let function_type = FunctionType::new(vec![], vec![Type::I32]);
        let function = Function::new_with_env(
            &store,
            &function_type,
            my_env.clone(),
            |_env: &MyEnv, _values: &[Value]| unimplemented!(),
        );
        assert_eq!(function.ty().clone(), function_type);
        let function_type =
            FunctionType::new(vec![], vec![Type::I32, Type::I64, Type::F32, Type::F64]);
        let function = Function::new_with_env(
            &store,
            &function_type,
            my_env.clone(),
            |_env: &MyEnv, _values: &[Value]| unimplemented!(),
        );
        assert_eq!(function.ty().clone(), function_type);

        // Using array signature
        let function_type = ([Type::V128], [Type::I32, Type::F32, Type::F64]);
        let function = Function::new_with_env(
            &store,
            function_type,
            my_env.clone(),
            |_env: &MyEnv, _values: &[Value]| unimplemented!(),
        );
        assert_eq!(function.ty().params(), [Type::V128]);
        assert_eq!(function.ty().results(), [Type::I32, Type::F32, Type::F64]);

        Ok(())
    }

    #[test]
    fn native_function_works() -> Result<()> {
        let store = Store::default();
        let function = Function::new_native(&store, || {});
        let native_function: NativeFunc<(), ()> = function.native().unwrap();
        let result = native_function.call();
        assert!(result.is_ok());

        let function = Function::new_native(&store, |a: i32| -> i32 { a + 1 });
        let native_function: NativeFunc<i32, i32> = function.native().unwrap();
        assert_eq!(native_function.call(3).unwrap(), 4);

        fn rust_abi(a: i32, b: i64, c: f32, d: f64) -> u64 {
            (a as u64 * 1000) + (b as u64 * 100) + (c as u64 * 10) + (d as u64)
        }
        let function = Function::new_native(&store, rust_abi);
        let native_function: NativeFunc<(i32, i64, f32, f64), u64> = function.native().unwrap();
        assert_eq!(native_function.call(8, 4, 1.5, 5.).unwrap(), 8415);

        let function = Function::new_native(&store, || -> i32 { 1 });
        let native_function: NativeFunc<(), i32> = function.native().unwrap();
        assert_eq!(native_function.call().unwrap(), 1);

        let function = Function::new_native(&store, |_a: i32| {});
        let native_function: NativeFunc<i32, ()> = function.native().unwrap();
        assert!(native_function.call(4).is_ok());

        let function =
            Function::new_native(&store, || -> (i32, i64, f32, f64) { (1, 2, 3.0, 4.0) });
        let native_function: NativeFunc<(), (i32, i64, f32, f64)> = function.native().unwrap();
        assert_eq!(native_function.call().unwrap(), (1, 2, 3.0, 4.0));

        Ok(())
    }

    #[test]
    fn function_outlives_instance() -> Result<()> {
        let store = Store::default();
        let wat = r#"(module
  (type $sum_t (func (param i32 i32) (result i32)))
  (func $sum_f (type $sum_t) (param $x i32) (param $y i32) (result i32)
    local.get $x
    local.get $y
    i32.add)
  (export "sum" (func $sum_f)))
"#;

        let f = {
            let module = Module::new(&store, wat)?;
            let instance = Instance::new(&module, &imports! {})?;
            let f: NativeFunc<(i32, i32), i32> = instance.get_native_function("sum").unwrap();

            assert_eq!(f.call(4, 5)?, 9);
            f
        };

        assert_eq!(f.call(4, 5)?, 9);

        Ok(())
    }

    #[test]
    fn weak_instance_ref_externs_after_instance() -> Result<()> {
        let store = Store::default();
        let wat = r#"(module
  (memory (export "mem") 1)
  (type $sum_t (func (param i32 i32) (result i32)))
  (func $sum_f (type $sum_t) (param $x i32) (param $y i32) (result i32)
    local.get $x
    local.get $y
    i32.add)
  (export "sum" (func $sum_f)))
"#;

        let f = {
            let module = Module::new(&store, wat)?;
            let instance = Instance::new(&module, &imports! {})?;
            let f: NativeFunc<(i32, i32), i32> = instance.get_native_function("sum").unwrap();
            assert_eq!(f.call(4, 5)?, 9);
            f
        };

        assert_eq!(f.call(4, 5)?, 9);

        Ok(())
    }

    #[test]
    fn manually_generate_wasmer_env() -> Result<()> {
        let store = Store::default();
        #[derive(WasmerEnv, Clone)]
        struct MyEnv {
            val: u32,
            memory: LazyInit<Memory>,
        }

        fn host_function(env: &mut MyEnv, arg1: u32, arg2: u32) -> u32 {
            env.val + arg1 + arg2
        }

        let mut env = MyEnv {
            val: 5,
            memory: LazyInit::new(),
        };

        let result = host_function(&mut env, 7, 9);
        assert_eq!(result, 21);

        let memory = Memory::new(&store, MemoryType::new(0, None, false))?;
        env.memory.initialize(memory);

        let result = host_function(&mut env, 1, 2);
        assert_eq!(result, 8);

        Ok(())
    }
}

'''
'''--- lib/api/tests/sys_instance.rs ---
#[cfg(feature = "sys")]
mod sys {
    use anyhow::Result;
    use wasmer::*;

    #[test]
    fn exports_work_after_multiple_instances_have_been_freed() -> Result<()> {
        let store = Store::default();
        let module = Module::new(
            &store,
            "
    (module
      (type $sum_t (func (param i32 i32) (result i32)))
      (func $sum_f (type $sum_t) (param $x i32) (param $y i32) (result i32)
        local.get $x
        local.get $y
        i32.add)
      (export \"sum\" (func $sum_f)))
",
        )?;

        let import_object = ImportObject::new();
        let instance = Instance::new(&module, &import_object)?;
        let instance2 = instance.clone();
        let instance3 = instance.clone();

        // The function is cloned to ‚Äúbreak‚Äù the connection with `instance`.
        let sum = instance.lookup_function("sum").unwrap().clone();

        drop(instance);
        drop(instance2);
        drop(instance3);

        // All instances have been dropped, but `sum` continues to work!
        assert_eq!(
            sum.call(&[Value::I32(1), Value::I32(2)])?.into_vec(),
            vec![Value::I32(3)],
        );

        Ok(())
    }

    #[test]
    fn unit_native_function_env() -> Result<()> {
        let store = Store::default();
        #[derive(WasmerEnv, Clone)]
        struct Env {
            multiplier: u32,
        }

        fn imported_fn(env: &Env, args: &[Val]) -> Result<Vec<Val>, RuntimeError> {
            let value = env.multiplier * args[0].unwrap_i32() as u32;
            return Ok(vec![Val::I32(value as _)]);
        }

        let imported_signature = FunctionType::new(vec![Type::I32], vec![Type::I32]);
        let imported = Function::new_with_env(
            &store,
            imported_signature,
            Env { multiplier: 3 },
            imported_fn,
        );

        let expected = vec![Val::I32(12)].into_boxed_slice();
        let result = imported.call(&[Val::I32(4)])?;
        assert_eq!(result, expected);

        Ok(())
    }
}

'''
'''--- lib/api/tests/sys_module.rs ---
#[cfg(feature = "sys")]
mod sys {
    use anyhow::Result;
    use wasmer::*;

    #[test]
    fn calling_host_functions_with_negative_values_works() -> Result<()> {
        let store = Store::default();
        let wat = r#"(module
    (import "host" "host_func1" (func (param i64)))
    (import "host" "host_func2" (func (param i32)))
    (import "host" "host_func3" (func (param i64)))
    (import "host" "host_func4" (func (param i32)))
    (import "host" "host_func5" (func (param i32)))
    (import "host" "host_func6" (func (param i32)))
    (import "host" "host_func7" (func (param i32)))
    (import "host" "host_func8" (func (param i32)))

    (func (export "call_host_func1")
          (call 0 (i64.const -1)))
    (func (export "call_host_func2")
          (call 1 (i32.const -1)))
    (func (export "call_host_func3")
          (call 2 (i64.const -1)))
    (func (export "call_host_func4")
          (call 3 (i32.const -1)))
    (func (export "call_host_func5")
          (call 4 (i32.const -1)))
    (func (export "call_host_func6")
          (call 5 (i32.const -1)))
    (func (export "call_host_func7")
          (call 6 (i32.const -1)))
    (func (export "call_host_func8")
          (call 7 (i32.const -1)))
)"#;
        let module = Module::new(&store, wat)?;
        let imports = imports! {
            "host" => {
                "host_func1" => Function::new_native(&store, |p: u64| {
                    println!("host_func1: Found number {}", p);
                    assert_eq!(p, u64::max_value());
                }),
                "host_func2" => Function::new_native(&store, |p: u32| {
                    println!("host_func2: Found number {}", p);
                    assert_eq!(p, u32::max_value());
                }),
                "host_func3" => Function::new_native(&store, |p: i64| {
                    println!("host_func3: Found number {}", p);
                    assert_eq!(p, -1);
                }),
                "host_func4" => Function::new_native(&store, |p: i32| {
                    println!("host_func4: Found number {}", p);
                    assert_eq!(p, -1);
                }),
                "host_func5" => Function::new_native(&store, |p: i16| {
                    println!("host_func5: Found number {}", p);
                    assert_eq!(p, -1);
                }),
                "host_func6" => Function::new_native(&store, |p: u16| {
                    println!("host_func6: Found number {}", p);
                    assert_eq!(p, u16::max_value());
                }),
                "host_func7" => Function::new_native(&store, |p: i8| {
                    println!("host_func7: Found number {}", p);
                    assert_eq!(p, -1);
                }),
                "host_func8" => Function::new_native(&store, |p: u8| {
                    println!("host_func8: Found number {}", p);
                    assert_eq!(p, u8::max_value());
                }),
            }
        };
        let instance = Instance::new(&module, &imports)?;

        let f1: NativeFunc<(), ()> = instance.get_native_function("call_host_func1")?;
        let f2: NativeFunc<(), ()> = instance.get_native_function("call_host_func2")?;
        let f3: NativeFunc<(), ()> = instance.get_native_function("call_host_func3")?;
        let f4: NativeFunc<(), ()> = instance.get_native_function("call_host_func4")?;
        let f5: NativeFunc<(), ()> = instance.get_native_function("call_host_func5")?;
        let f6: NativeFunc<(), ()> = instance.get_native_function("call_host_func6")?;
        let f7: NativeFunc<(), ()> = instance.get_native_function("call_host_func7")?;
        let f8: NativeFunc<(), ()> = instance.get_native_function("call_host_func8")?;

        f1.call()?;
        f2.call()?;
        f3.call()?;
        f4.call()?;
        f5.call()?;
        f6.call()?;
        f7.call()?;
        f8.call()?;

        Ok(())
    }
}

'''
'''--- lib/api/tests/sys_reference_types.rs ---
#[cfg(feature = "sys")]
mod sys {
    use anyhow::Result;
    use std::collections::HashMap;
    use std::sync::atomic::{AtomicBool, Ordering};
    use std::sync::Arc;
    use wasmer::*;

    #[test]
    #[cfg_attr(feature = "singlepass", ignore)] // singlepass does not support funcref args.
    fn func_ref_passed_and_returned() -> Result<()> {
        let store = Store::default();
        let wat = r#"(module
    (import "env" "func_ref_identity" (func (param funcref) (result funcref)))
    (type $ret_i32_ty (func (result i32)))
    (table $table (export "table") 2 2 funcref)

    (func (export "run") (param) (result funcref)
          (call 0 (ref.null func)))
    (func (export "call_set_value") (param $fr funcref) (result i32)
          (table.set $table (i32.const 0) (local.get $fr))
          (call_indirect $table (type $ret_i32_ty) (i32.const 0)))
)"#;
        let module = Module::new(&store, wat)?;
        let func_ref_identity = Function::new(
            &store,
            FunctionType::new(vec![Type::FuncRef], vec![Type::FuncRef]),
            |values| -> Result<Vec<_>, _> { Ok(vec![values[0].clone()]) },
        );
        let imports = imports! {
            "env" => {
                "func_ref_identity" => func_ref_identity
            },
        };

        let instance = Instance::new(&module, &imports)?;

        let f: Function = instance.lookup_function("run").unwrap();
        let results = f.call(&[]).unwrap();
        if let Value::FuncRef(fr) = &results[0] {
            assert!(fr.is_none());
        } else {
            panic!("funcref not found!");
        }

        #[derive(Clone, Debug, WasmerEnv)]
        pub struct Env(Arc<AtomicBool>);
        let env = Env(Arc::new(AtomicBool::new(false)));

        let func_to_call = Function::new_native_with_env(&store, env.clone(), |env: &Env| -> i32 {
            env.0.store(true, Ordering::SeqCst);
            343
        });
        let call_set_value: Function = instance.lookup_function("call_set_value").unwrap();
        let results: Box<[Value]> = call_set_value.call(&[Value::FuncRef(Some(func_to_call))])?;
        assert!(env.0.load(Ordering::SeqCst));
        assert_eq!(&*results, &[Value::I32(343)]);

        Ok(())
    }

    #[test]
    #[cfg_attr(feature = "singlepass", ignore)] // singlepass does not support funcref args.
    fn func_ref_passed_and_called() -> Result<()> {
        let store = Store::default();
        let wat = r#"(module
    (func $func_ref_call (import "env" "func_ref_call") (param funcref) (result i32))
    (type $ret_i32_ty (func (result i32)))
    (table $table (export "table") 2 2 funcref)

    (func $product (param $x i32) (param $y i32) (result i32)
          (i32.mul (local.get $x) (local.get $y)))
    ;; TODO: figure out exactly why this statement is needed
    (elem declare func $product)
    (func (export "call_set_value") (param $fr funcref) (result i32)
          (table.set $table (i32.const 0) (local.get $fr))
          (call_indirect $table (type $ret_i32_ty) (i32.const 0)))
    (func (export "call_func") (param $fr funcref) (result i32)
          (call $func_ref_call (local.get $fr)))
    (func (export "call_host_func_with_wasm_func") (result i32)
          (call $func_ref_call (ref.func $product)))
)"#;
        let module = Module::new(&store, wat)?;

        fn func_ref_call(values: &[Value]) -> Result<Vec<Value>, RuntimeError> {
            // TODO: look into `Box<[Value]>` being returned breakage
            let f = values[0].unwrap_funcref().as_ref().unwrap();
            let f: NativeFunc<(i32, i32), i32> = f.native()?;
            Ok(vec![Value::I32(f.call(7, 9)?)])
        }

        let func_ref_call = Function::new(
            &store,
            FunctionType::new(vec![Type::FuncRef], vec![Type::I32]),
            func_ref_call,
        );
        let imports = imports! {
            "env" => {
                "func_ref_call" => func_ref_call,
                // TODO(reftypes): this should work
                /*
                "func_ref_call_native" => Function::new_native(&store, |f: Function| -> Result<i32, RuntimeError> {
                    let f: NativeFunc::<(i32, i32), i32> = f.native()?;
                    f.call(7, 9)
                })
                */
            },
        };

        let instance = Instance::new(&module, &imports)?;
        {
            fn sum(a: i32, b: i32) -> i32 {
                a + b
            }
            let sum_func = Function::new_native(&store, sum);

            let call_func: Function = instance.lookup_function("call_func").unwrap();
            let result = call_func.call(&[Value::FuncRef(Some(sum_func))])?;
            assert_eq!(result[0].unwrap_i32(), 16);
        }

        {
            let f: NativeFunc<(), i32> = instance
                .get_native_function("call_host_func_with_wasm_func")
                .unwrap();
            let result = f.call()?;
            assert_eq!(result, 63);
        }

        Ok(())
    }

    #[cfg(feature = "experimental-reference-types-extern-ref")]
    #[test]
    #[cfg_attr(feature = "singlepass", ignore)] // singlepass does not support funcref args.
    fn extern_ref_passed_and_returned() -> Result<()> {
        let store = Store::default();
        let wat = r#"(module
    (func $extern_ref_identity (import "env" "extern_ref_identity") (param externref) (result externref))
    (func $extern_ref_identity_native (import "env" "extern_ref_identity_native") (param externref) (result externref))
    (func $get_new_extern_ref (import "env" "get_new_extern_ref") (result externref))
    (func $get_new_extern_ref_native (import "env" "get_new_extern_ref_native") (result externref))

    (func (export "run") (param) (result externref)
          (call $extern_ref_identity (ref.null extern)))
    (func (export "run_native") (param) (result externref)
          (call $extern_ref_identity_native (ref.null extern)))
    (func (export "get_hashmap") (param) (result externref)
          (call $get_new_extern_ref))
    (func (export "get_hashmap_native") (param) (result externref)
          (call $get_new_extern_ref_native))
)"#;
        let module = Module::new(&store, wat)?;
        let extern_ref_identity = Function::new(
            &store,
            FunctionType::new(vec![Type::ExternRef], vec![Type::ExternRef]),
            |values| -> Result<Vec<_>, _> { Ok(vec![values[0].clone()]) },
        );
        let extern_ref_identity_native =
            Function::new_native(&store, |er: ExternRef| -> ExternRef { er });
        let get_new_extern_ref = Function::new(
            &store,
            FunctionType::new(vec![], vec![Type::ExternRef]),
            |_| -> Result<Vec<_>, _> {
                let inner = [
                    ("hello".to_string(), "world".to_string()),
                    ("color".to_string(), "orange".to_string()),
                ]
                .iter()
                .cloned()
                .collect::<HashMap<String, String>>();
                let new_extern_ref = ExternRef::new(inner);
                Ok(vec![Value::ExternRef(new_extern_ref)])
            },
        );
        let get_new_extern_ref_native = Function::new_native(&store, || -> ExternRef {
            let inner = [
                ("hello".to_string(), "world".to_string()),
                ("color".to_string(), "orange".to_string()),
            ]
            .iter()
            .cloned()
            .collect::<HashMap<String, String>>();
            ExternRef::new(inner)
        });
        let imports = imports! {
            "env" => {
                "extern_ref_identity" => extern_ref_identity,
                "extern_ref_identity_native" => extern_ref_identity_native,
                "get_new_extern_ref" => get_new_extern_ref,
                "get_new_extern_ref_native" => get_new_extern_ref_native,
            },
        };

        let instance = Instance::new(&module, &imports)?;
        for run in &["run", "run_native"] {
            let f: Function = instance.lookup_function(run).unwrap();
            let results = f.call(&[]).unwrap();
            if let Value::ExternRef(er) = &results[0] {
                assert!(er.is_null());
            } else {
                panic!("result is not an extern ref!");
            }

            let f: NativeFunc<(), ExternRef> = instance.get_native_function(run).unwrap();
            let result: ExternRef = f.call()?;
            assert!(result.is_null());
        }

        for get_hashmap in &["get_hashmap", "get_hashmap_native"] {
            let f: Function = instance.lookup_function(get_hashmap).unwrap();
            let results = f.call(&[]).unwrap();
            if let Value::ExternRef(er) = &results[0] {
                let inner: &HashMap<String, String> = er.downcast().unwrap();
                assert_eq!(inner["hello"], "world");
                assert_eq!(inner["color"], "orange");
            } else {
                panic!("result is not an extern ref!");
            }

            let f: NativeFunc<(), ExternRef> = instance.get_native_function(get_hashmap).unwrap();

            let result: ExternRef = f.call()?;
            let inner: &HashMap<String, String> = result.downcast().unwrap();
            assert_eq!(inner["hello"], "world");
            assert_eq!(inner["color"], "orange");
        }

        Ok(())
    }

    #[cfg(feature = "experimental-reference-types-extern-ref")]
    #[test]
    // TODO(reftypes): reenable this test
    #[ignore]
    fn extern_ref_ref_counting_basic() -> Result<()> {
        let store = Store::default();
        let wat = r#"(module
    (func (export "drop") (param $er externref) (result)
          (drop (local.get $er)))
)"#;
        let module = Module::new(&store, wat)?;
        let instance = Instance::new(&module, &imports! {})?;
        let f: NativeFunc<ExternRef, ()> = instance.get_native_function("drop").unwrap();

        let er = ExternRef::new(3u32);
        f.call(er.clone())?;

        assert_eq!(er.downcast::<u32>().unwrap(), &3);
        assert_eq!(er.strong_count(), 1);

        Ok(())
    }

    #[cfg(feature = "experimental-reference-types-extern-ref")]
    #[test]
    fn refs_in_globals() -> Result<()> {
        let store = Store::default();
        let wat = r#"(module
    (global $er_global (export "er_global") (mut externref) (ref.null extern))
    (global $fr_global (export "fr_global") (mut funcref) (ref.null func))
    (global $fr_immutable_global (export "fr_immutable_global") funcref (ref.func $hello))
    (func $hello (param) (result i32)
          (i32.const 73))
)"#;
        let module = Module::new(&store, wat)?;
        let instance = Instance::new(&module, &imports! {})?;
        {
            let er_global = if let Some(Export::Global(g)) = instance.lookup("er_global") {
                g.from
            } else {
                panic!("no global");
            };

            if let Value::ExternRef(er) = er_global.get(&store) {
                assert!(er.is_null());
            } else {
                panic!("Did not find extern ref in the global");
            }

            unsafe {
                er_global.set(Val::ExternRef(ExternRef::new(3u32)))?;
            }

            if let Value::ExternRef(er) = er_global.get(&store) {
                assert_eq!(er.downcast::<u32>().unwrap(), &3);
                assert_eq!(er.strong_count(), 1);
            } else {
                panic!("Did not find extern ref in the global");
            }
        }

        {
            let fr_global = if let Some(Export::Global(g)) = instance.lookup("fr_immutable_global")
            {
                g.from
            } else {
                panic!("no global");
            };

            if let Value::FuncRef(Some(f)) = fr_global.get(&store) {
                let native_func: NativeFunc<(), u32> = f.native()?;
                assert_eq!(native_func.call()?, 73);
            } else {
                panic!("Did not find non-null func ref in the global");
            }
        }

        {
            let fr_global = if let Some(Export::Global(g)) = instance.lookup("fr_global") {
                g.from
            } else {
                panic!("no global");
            };

            if let Value::FuncRef(None) = fr_global.get(&store) {
            } else {
                panic!("Did not find a null func ref in the global");
            }

            let f = Function::new_native(&store, |arg1: i32, arg2: i32| -> i32 { arg1 + arg2 });

            unsafe {
                fr_global.set(Val::FuncRef(Some(f)))?;
            }

            if let Value::FuncRef(Some(f)) = fr_global.get(&store) {
                let native: NativeFunc<(i32, i32), i32> = f.native()?;
                assert_eq!(native.call(5, 7)?, 12);
            } else {
                panic!("Did not find extern ref in the global");
            }
        }

        Ok(())
    }

    #[cfg(feature = "experimental-reference-types-extern-ref")]
    #[test]
    #[cfg_attr(feature = "singlepass", ignore)] // singlepass does not support funcref args.
    fn extern_ref_ref_counting_table_basic() -> Result<()> {
        use wasmer_vm::TableElement;

        let store = Store::default();
        let wat = r#"(module
    (global $global (export "global") (mut externref) (ref.null extern))
    (table $table (export "table") 4 4 externref)
    (func $insert (param $er externref) (param $idx i32)
           (table.set $table (local.get $idx) (local.get $er)))
    (func $intermediate (param $er externref) (param $idx i32)
          (call $insert (local.get $er) (local.get $idx)))
    (func $insert_into_table (export "insert_into_table") (param $er externref) (param $idx i32) (result externref)
          (call $intermediate (local.get $er) (local.get $idx))
          (local.get $er))
)"#;
        let module = Module::new(&store, wat)?;
        let instance = Instance::new(&module, &imports! {})?;

        let f: NativeFunc<(ExternRef, i32), ExternRef> =
            instance.get_native_function("insert_into_table").unwrap();

        let er = ExternRef::new(3usize);

        let er = f.call(er, 1)?;
        assert_eq!(er.strong_count(), 2);

        let table = if let Some(Export::Table(t)) = instance.lookup("table") {
            t.from
        } else {
            panic!("no table");
        };

        if let TableElement::ExternRef(er2) = table.get(1).unwrap() {
            assert_eq!(er2.strong_count(), 3);
        }

        assert_eq!(er.strong_count(), 2);
        table
            .set(1, TableElement::ExternRef(ExternRef::null()))
            .unwrap();

        assert_eq!(er.strong_count(), 1);

        Ok(())
    }

    #[cfg(feature = "experimental-reference-types-extern-ref")]
    #[test]
    // TODO(reftypes): reenable this test
    #[ignore]
    fn extern_ref_ref_counting_global_basic() -> Result<()> {
        let store = Store::default();
        let wat = r#"(module
    (global $global (export "global") (mut externref) (ref.null extern))
    (func $get_from_global (export "get_from_global") (result externref)
          (drop (global.get $global))
          (global.get $global))
)"#;
        let module = Module::new(&store, wat)?;
        let instance = Instance::new(&module, &imports! {})?;

        let global = if let Some(Export::Global(g)) = instance.lookup("global") {
            g.from
        } else {
            panic!("not a global");
        };
        {
            let er = ExternRef::new(3usize);
            unsafe {
                global.set(Val::ExternRef(er.clone()))?;
            }
            assert_eq!(er.strong_count(), 2);
        }
        let get_from_global: NativeFunc<(), ExternRef> =
            instance.get_native_function("get_from_global").unwrap();

        let er = get_from_global.call()?;
        assert_eq!(er.strong_count(), 2);
        unsafe {
            global.set(Val::ExternRef(ExternRef::null()))?;
        }
        assert_eq!(er.strong_count(), 1);

        Ok(())
    }

    #[cfg(feature = "experimental-reference-types-extern-ref")]
    #[test]
    // TODO(reftypes): reenable this test
    #[ignore]
    fn extern_ref_ref_counting_traps() -> Result<()> {
        let store = Store::default();
        let wat = r#"(module
    (func $pass_er (export "pass_extern_ref") (param externref)
          (local.get 0)
          (unreachable))
)"#;
        let module = Module::new(&store, wat)?;
        let instance = Instance::new(&module, &imports! {})?;

        let pass_extern_ref: NativeFunc<ExternRef, ()> =
            instance.get_native_function("pass_extern_ref").unwrap();

        let er = ExternRef::new(3usize);
        assert_eq!(er.strong_count(), 1);

        let result = pass_extern_ref.call(er.clone());
        assert!(result.is_err());
        assert_eq!(er.strong_count(), 1);

        Ok(())
    }

    #[cfg(feature = "experimental-reference-types-extern-ref")]
    #[test]
    #[cfg_attr(feature = "singlepass", ignore)] // singlepass does not support funcref args.
    fn extern_ref_ref_counting_table_instructions() -> Result<()> {
        use wasmer_vm::TableElement;

        let store = Store::default();
        let wat = r#"(module
    (table $table1 (export "table1") 2 12 externref)
    (table $table2 (export "table2") 6 12 externref)
    (func $grow_table_with_ref (export "grow_table_with_ref") (param $er externref) (param $size i32) (result i32)
          (table.grow $table1 (local.get $er) (local.get $size)))
    (func $fill_table_with_ref (export "fill_table_with_ref") (param $er externref) (param $start i32) (param $end i32)
          (table.fill $table1 (local.get $start) (local.get $er) (local.get $end)))
    (func $copy_into_table2 (export "copy_into_table2")
          (table.copy $table2 $table1 (i32.const 0) (i32.const 0) (i32.const 4)))
)"#;
        let module = Module::new(&store, wat)?;
        let instance = Instance::new(&module, &imports! {})?;

        let grow_table_with_ref: NativeFunc<(ExternRef, i32), i32> =
            instance.get_native_function("grow_table_with_ref").unwrap();
        let fill_table_with_ref: NativeFunc<(ExternRef, i32, i32), ()> =
            instance.get_native_function("fill_table_with_ref").unwrap();
        let copy_into_table2: NativeFunc<(), ()> =
            instance.get_native_function("copy_into_table2").unwrap();
        let (table1, table2) = if let (Some(Export::Table(t1)), Some(Export::Table(t2))) =
            (instance.lookup("table1"), instance.lookup("table2"))
        {
            (t1.from, t2.from)
        } else {
            panic!("can't get tables");
        };

        let er1 = ExternRef::new(3usize);
        let er2 = ExternRef::new(5usize);
        let er3 = ExternRef::new(7usize);
        {
            let result = grow_table_with_ref.call(er1.clone(), 0)?;
            assert_eq!(result, 2);
            assert_eq!(er1.strong_count(), 1);

            let result = grow_table_with_ref.call(er1.clone(), 10_000)?;
            assert_eq!(result, -1);
            assert_eq!(er1.strong_count(), 1);

            let result = grow_table_with_ref.call(er1.clone(), 8)?;
            assert_eq!(result, 2);
            assert_eq!(er1.strong_count(), 9);

            for i in 2..10 {
                if let TableElement::ExternRef(e) = table1.get(i).unwrap() {
                    assert_eq!(*e.downcast::<usize>().unwrap(), 3);
                    assert_eq!(&e, &er1);
                }
            }
            assert_eq!(er1.strong_count(), 9);
        }

        {
            fill_table_with_ref.call(er2.clone(), 0, 2)?;
            assert_eq!(er2.strong_count(), 3);
        }

        {
            table2.set(0, TableElement::ExternRef(er3.clone())).unwrap();
            table2.set(1, TableElement::ExternRef(er3.clone())).unwrap();
            table2.set(2, TableElement::ExternRef(er3.clone())).unwrap();
            table2.set(3, TableElement::ExternRef(er3.clone())).unwrap();
            table2.set(4, TableElement::ExternRef(er3.clone())).unwrap();
            assert_eq!(er3.strong_count(), 6);
        }

        {
            copy_into_table2.call()?;
            assert_eq!(er3.strong_count(), 2);
            assert_eq!(er2.strong_count(), 5);
            assert_eq!(er1.strong_count(), 11);
            for i in 1..5 {
                if let TableElement::ExternRef(e) = table2.get(i).unwrap() {
                    let value = e.downcast::<usize>().unwrap();
                    match i {
                        0 | 1 => assert_eq!(*value, 5),
                        4 => assert_eq!(*value, 7),
                        _ => assert_eq!(*value, 3),
                    }
                } else {
                    panic!("not extern ref");
                }
            }
        }

        {
            for i in 0..table1.size() {
                table1
                    .set(i, TableElement::ExternRef(ExternRef::null()))
                    .unwrap();
            }
            for i in 0..table2.size() {
                table2
                    .set(i, TableElement::ExternRef(ExternRef::null()))
                    .unwrap();
            }
        }

        assert_eq!(er1.strong_count(), 1);
        assert_eq!(er2.strong_count(), 1);
        assert_eq!(er3.strong_count(), 1);

        Ok(())
    }
}

'''
'''--- lib/cli/Cargo.toml ---
[package]
name = "wasmer-cli"
version = "2.1.0"
description = "Wasmer CLI"
categories = ["wasm", "command-line-interface"]
keywords = ["wasm", "webassembly", "cli"]
authors = ["Wasmer Engineering Team <engineering@wasmer.io>"]
repository = "https://github.com/wasmerio/wasmer"
license = "MIT"
readme = "README.md"
edition = "2018"
default-run = "wasmer"
build = "build.rs"

[[bin]]
name = "wasmer"
path = "src/bin/wasmer.rs"
doc = false

[[bin]]
name = "wasmer-headless"
path = "src/bin/wasmer_headless.rs"
doc = false
required-features = ["headless"]

[dependencies]
wasmer = { version = "2.0.3", path = "../api", package = "wasmer-near", default-features = false }
wasmer-compiler = { version = "2.0.3", path = "../compiler", package = "wasmer-compiler-near"  }
wasmer-compiler-cranelift = { version = "2.0.0", path = "../compiler-cranelift", optional = true }
wasmer-compiler-singlepass = { version = "2.0.3", path = "../compiler-singlepass", package = "wasmer-compiler-singlepass-near", optional = true }
wasmer-compiler-llvm = { version = "2.0.0", path = "../compiler-llvm", optional = true }
wasmer-engine = { version = "2.0.3", path = "../engine", package = "wasmer-engine-near" }
wasmer-engine-universal = { version = "2.0.3", path = "../engine-universal", package = "wasmer-engine-universal-near", optional = true }
wasmer-vm = { version = "2.0.3", path = "../vm", package = "wasmer-vm-near" }
wasmer-wast = { version = "2.0.0", path = "../../tests/lib/wast", optional = true }
wasmer-types = { version = "2.0.3", path = "../types", package = "wasmer-types-near" }
atty = "0.2"
colored = "2.0"
anyhow = "1.0"
structopt = { version = "0.3", features = ["suggestions"] }
# For the function names autosuggestion
distance = "0.4"
# For the inspect subcommand
bytesize = "1.0"
cfg-if = "1.0"
# For debug feature
fern = { version = "0.6", features = ["colored"], optional = true }
log = { version = "0.4", optional = true }
tempfile = "3"

[features]
# Don't add the compiler features in default, please add them on the Makefile
# since we might want to autoconfigure them depending on the availability on the host.
default = [
    "wat",
    "wast",
    "universal",
]
engine = []
universal = [
    "wasmer-engine-universal",
    "engine",
]
wast = ["wasmer-wast"]
wat = ["wasmer/wat"]
compiler = [
    "wasmer-compiler/translator",
    "wasmer-engine-universal/compiler",
]
singlepass = [
    "wasmer-compiler-singlepass",
    "compiler",
]
cranelift = [
    "wasmer-compiler-cranelift",
    "compiler",
]
llvm = [
    "wasmer-compiler-llvm",
    "compiler",
]
debug = ["fern", "log"]
disable-all-logging = []
headless = []
headless-minimal = ["headless", "disable-all-logging", "universal"]

# Deprecated features.
jit = ["universal"]

'''
'''--- lib/cli/README.md ---
# `wasmer-cli` [![Build Status](https://github.com/wasmerio/wasmer/workflows/build/badge.svg?style=flat-square)](https://github.com/wasmerio/wasmer/actions?query=workflow%3Abuild) [![Join Wasmer Slack](https://img.shields.io/static/v1?label=Slack&message=join%20chat&color=brighgreen&style=flat-square)](https://slack.wasmer.io) [![MIT License](https://img.shields.io/github/license/wasmerio/wasmer.svg?style=flat-square)](https://github.com/wasmerio/wasmer/blob/master/LICENSE)

This crate is the Wasmer CLI.

The recommended way to install `wasmer` is via the [wasmer-installer](https://github.com/wasmerio/wasmer-install).

However, you can also install `wasmer` via Cargo (you will need to specify the compilers to use):

```bash
cargo install wasmer-cli --features "singlepass,cranelift"
```

Or by building it inside the codebase:

```bash
cargo build --release --features "singlepass,cranelift"
```

> Note: installing `wasmer` via Cargo (or manual install) will not install
> the WAPM cli. If you want to use them together, please use the [wasmer installer](https://github.com/wasmerio/wasmer-install).

## Features

The Wasmer supports the following features:
* `wat` (default): support for executing WebAssembly text files.
* `wast`(default): support for running wast test files.
* `universal` (default): support for the [Universal engine].
* `dylib` (default): support for the [Dylib engine].
* `cache` (default): support or automatically caching compiled artifacts.
* `wasi` (default): support for [WASI].
* `experimental-io-devices`: support for experimental IO devices in WASI.
* `emscripten` (default): support for [Emscripten].
* `singlepass`: support for the [Singlepass compiler].
* `cranelift`: support for the [Cranelift compiler].
* `llvm`: support for the [LLVM compiler].

[Universal engine]: https://github.com/wasmerio/wasmer/tree/master/lib/engine-universal/
[Dylib engine]: https://github.com/wasmerio/wasmer/tree/master/lib/engine-dylib/
[WASI]: https://github.com/wasmerio/wasmer/tree/master/lib/wasi/
[Emscripten]: https://github.com/wasmerio/wasmer/tree/master/lib/emscripten/
[Singlepass compiler]: https://github.com/wasmerio/wasmer/tree/master/lib/compiler-singlepass/
[Cranelift compiler]: https://github.com/wasmerio/wasmer/tree/master/lib/compiler-cranelift/
[LLVM compiler]: https://github.com/wasmerio/wasmer/tree/master/lib/compiler-llvm/

## CLI commands

Once you have Wasmer installed, you can start executing WebAssembly files easily:

Get the current Wasmer version:

```bash
wasmer -V
```

Execute a WebAssembly file:

```bash
wasmer run myfile.wasm
```

Compile a WebAssembly file:

```bash
wasmer compile myfile.wasm -o myfile.so --dylib
```

Run a compiled WebAssembly file (fastest):

```bash
wasmer run myfile.so
```

'''
'''--- lib/cli/build.rs ---
pub fn main() {
    println!("cargo:rerun-if-changed=build.rs");
    println!("cargo:rerun-if-env-changed=WASMER_INSTALL_PREFIX");
}

'''
'''--- lib/cli/src/bin/wasmer.rs ---
use wasmer_cli::cli::wasmer_main;

#[cfg(not(any(feature = "cranelift", feature = "singlepass", feature = "llvm")))]
compile_error!(
    "Either enable at least one compiler, or compile the wasmer-headless binary instead"
);

fn main() {
    wasmer_main();
}

'''
'''--- lib/cli/src/bin/wasmer_headless.rs ---
use wasmer_cli::cli::wasmer_main;

fn main() {
    wasmer_main();
}

'''
'''--- lib/cli/src/c_gen/mod.rs ---
//! A convenient little abstraction for building up C expressions and generating
//! simple C code.

pub mod staticlib_header;

/// An identifier in C.
pub type CIdent = String;

/// A Type in the C language.
#[derive(Debug, Clone)]
pub enum CType {
    /// C `void` type.
    Void,
    /// A pointer to some other type.
    PointerTo {
        /// Whether the pointer is `const`.
        is_const: bool,
        /// The type that the pointer points to.
        inner: Box<CType>,
    },
    /// C 8 bit unsigned integer type.
    U8,
    /// C 16 bit unsigned integer type.
    U16,
    /// C 32 bit unsigned integer type.
    U32,
    /// C 64 bit unsigned integer type.
    U64,
    /// C pointer sized unsigned integer type.
    USize,
    /// C 8 bit signed integer type.
    I8,
    /// C 16 bit signed integer type.
    I16,
    /// C 32 bit signed integer type.
    I32,
    /// C 64 bit signed integer type.
    I64,
    /// C pointer sized signed integer type.
    ISize,
    /// A function or function pointer.
    Function {
        /// The arguments the function takes.
        arguments: Vec<CType>,
        /// The return value if it has one
        ///
        /// None is equivalent to Some(Box(Ctype::Void)).
        return_value: Option<Box<CType>>,
    },
    /// C constant array.
    Array {
        /// The type of the array.
        inner: Box<CType>,
    },
    /// A user defined type.
    TypeDef(String),
}

impl CType {
    /// Convenience function to get a mutable void pointer type.
    pub fn void_ptr() -> Self {
        CType::PointerTo {
            is_const: false,
            inner: Box::new(CType::Void),
        }
    }

    /// Convenience function to get a const void pointer type.
    #[allow(dead_code)]
    pub fn const_void_ptr() -> Self {
        CType::PointerTo {
            is_const: true,
            inner: Box::new(CType::Void),
        }
    }

    /// Generate the C source code for a type into the given `String`.
    fn generate_c(&self, w: &mut String) {
        match &self {
            Self::Void => {
                w.push_str("void");
            }
            Self::PointerTo { is_const, inner } => {
                if *is_const {
                    w.push_str("const ");
                }
                inner.generate_c(w);
                w.push('*');
            }
            Self::U8 => {
                w.push_str("unsigned char");
            }
            Self::U16 => {
                w.push_str("unsigned short");
            }
            Self::U32 => {
                w.push_str("unsigned int");
            }
            Self::U64 => {
                w.push_str("unsigned long long");
            }
            Self::USize => {
                w.push_str("unsigned size_t");
            }
            Self::I8 => {
                w.push_str("char");
            }
            Self::I16 => {
                w.push_str("short");
            }
            Self::I32 => {
                w.push_str("int");
            }
            Self::I64 => {
                w.push_str("long long");
            }
            Self::ISize => {
                w.push_str("size_t");
            }
            Self::Function {
                arguments,
                return_value,
            } => {
                // function with no, name, assume it's a function pointer
                let ret: CType = return_value
                    .as_ref()
                    .map(|i: &Box<CType>| (&**i).clone())
                    .unwrap_or_default();
                ret.generate_c(w);
                w.push(' ');
                w.push_str("(*)");
                w.push('(');
                if arguments.len() > 1 {
                    for arg in &arguments[..arguments.len() - 1] {
                        arg.generate_c(w);
                        w.push_str(", ");
                    }
                    arguments.last().unwrap().generate_c(w);
                } else if arguments.len() == 1 {
                    arguments[0].generate_c(w);
                }
                w.push(')');
            }
            Self::Array { inner } => {
                inner.generate_c(w);
                w.push_str("[]");
            }
            Self::TypeDef(inner) => {
                w.push_str(&inner);
            }
        }
    }

    /// Generate the C source code for a type with a nameinto the given `String`.
    fn generate_c_with_name(&self, name: &str, w: &mut String) {
        match &self {
            Self::PointerTo { .. }
            | Self::TypeDef { .. }
            | Self::Void
            | Self::U8
            | Self::U16
            | Self::U32
            | Self::U64
            | Self::USize
            | Self::I8
            | Self::I16
            | Self::I32
            | Self::I64
            | Self::ISize => {
                self.generate_c(w);
                w.push(' ');
                w.push_str(name);
            }
            Self::Function {
                arguments,
                return_value,
            } => {
                let ret: CType = return_value
                    .as_ref()
                    .map(|i: &Box<CType>| (&**i).clone())
                    .unwrap_or_default();
                ret.generate_c(w);
                w.push(' ');
                w.push_str(&name);
                w.push('(');
                if arguments.len() > 1 {
                    for arg in &arguments[..arguments.len() - 1] {
                        arg.generate_c(w);
                        w.push_str(", ");
                    }
                    arguments.last().unwrap().generate_c(w);
                } else if arguments.len() == 1 {
                    arguments[0].generate_c(w);
                }
                w.push(')');
            }
            Self::Array { inner } => {
                inner.generate_c(w);
                w.push(' ');
                w.push_str(&name);
                w.push_str("[]");
            }
        }
    }
}

impl Default for CType {
    fn default() -> CType {
        CType::Void
    }
}

/// A statement in the C programming language. This may not be exact to what an
/// AST would look like or what the C standard says about the C language, it's
/// simply a structed way to organize data for generating C code.
#[derive(Debug, Clone)]
pub enum CStatement {
    /// A declaration of some kind.
    Declaration {
        /// The name of the thing being declared.
        name: CIdent,
        /// Whether the thing being declared is `extern`.
        is_extern: bool,
        /// Whether the thing being declared is `const`.
        is_const: bool,
        /// The type of the thing being declared.
        ctype: CType,
        /// The definition of the thing being declared.
        ///
        /// This is useful for initializing constant arrays, for example.
        definition: Option<Box<CStatement>>,
    },

    /// A literal array of CStatements.
    LiteralArray {
        /// The contents of the array.
        items: Vec<CStatement>,
    },

    /// A literal constant value, passed through directly as a string.
    LiteralConstant {
        /// The raw value acting as a constant.
        value: String,
    },

    /// A C-style cast
    Cast {
        /// The type to cast to.
        target_type: CType,
        /// The thing being cast.
        expression: Box<CStatement>,
    },

    /// Typedef one type to another.
    TypeDef {
        /// The type of the thing being typedef'd.
        source_type: CType,
        /// The new name by which this type may be called.
        new_name: CIdent,
    },
}

impl CStatement {
    /// Generate C source code for the given CStatement.
    fn generate_c(&self, w: &mut String) {
        match &self {
            Self::Declaration {
                name,
                is_extern,
                is_const,
                ctype,
                definition,
            } => {
                if *is_const {
                    w.push_str("const ");
                }
                if *is_extern {
                    w.push_str("extern ");
                }
                ctype.generate_c_with_name(name, w);
                if let Some(def) = definition {
                    w.push_str(" = ");
                    def.generate_c(w);
                }
                w.push(';');
                w.push('\n');
            }
            Self::LiteralArray { items } => {
                w.push('{');
                if !items.is_empty() {
                    w.push('\n');
                }
                for item in items {
                    w.push('\t');
                    item.generate_c(w);
                    w.push(',');
                    w.push('\n');
                }
                w.push('}');
            }
            Self::LiteralConstant { value } => {
                w.push_str(&value);
            }
            Self::Cast {
                target_type,
                expression,
            } => {
                w.push('(');
                target_type.generate_c(w);
                w.push(')');
                w.push(' ');
                expression.generate_c(w);
            }
            Self::TypeDef {
                source_type,
                new_name,
            } => {
                w.push_str("typedef ");
                // leaky abstraction / hack, doesn't fully solve the problem
                if let CType::Function { .. } = source_type {
                    source_type.generate_c_with_name(&format!("(*{})", new_name), w);
                } else {
                    source_type.generate_c(w);
                    w.push(' ');
                    w.push_str(&new_name);
                }
                w.push(';');
                w.push('\n');
            }
        }
    }
}

/// Generate C source code from some `CStatements` into a String.
// TODO: add config section
pub fn generate_c(statements: &[CStatement]) -> String {
    let mut out = String::new();
    for statement in statements {
        statement.generate_c(&mut out);
    }
    out
}

#[cfg(test)]
mod test {
    use super::*;

    #[test]
    fn generate_types() {
        macro_rules! assert_c_type {
            ($ctype:expr, $expected:expr) => {
                let mut w = String::new();
                let ctype = $ctype;
                ctype.generate_c(&mut w);
                assert_eq!(w, $expected);
            };
        }

        assert_c_type!(CType::Void, "void");
        assert_c_type!(CType::void_ptr(), "void*");
        assert_c_type!(CType::const_void_ptr(), "const void*");
        assert_c_type!(CType::U8, "unsigned char");
        assert_c_type!(CType::U16, "unsigned short");
        assert_c_type!(CType::U32, "unsigned int");
        assert_c_type!(CType::U64, "unsigned long long");
        assert_c_type!(CType::USize, "unsigned size_t");
        assert_c_type!(CType::I8, "char");
        assert_c_type!(CType::I16, "short");
        assert_c_type!(CType::I32, "int");
        assert_c_type!(CType::I64, "long long");
        assert_c_type!(CType::ISize, "size_t");
        assert_c_type!(CType::TypeDef("my_type".to_string()), "my_type");
        assert_c_type!(
            CType::Function {
                arguments: vec![CType::U8, CType::ISize],
                return_value: None
            },
            "void (*)(unsigned char, size_t)"
        );
        assert_c_type!(
            CType::Function {
                arguments: vec![],
                return_value: Some(Box::new(CType::ISize))
            },
            "size_t (*)()"
        );
        assert_c_type!(
            CType::PointerTo {
                is_const: true,
                inner: Box::new(CType::PointerTo {
                    is_const: false,
                    inner: Box::new(CType::U32)
                })
            },
            "const unsigned int**"
        );
        // TODO: test more complicated const correctness rules: there are bugs relating to it.
    }

    #[test]
    fn generate_types_with_names() {
        macro_rules! assert_c_type {
            ($ctype:expr, $name:literal, $expected:expr) => {
                let mut w = String::new();
                let ctype = $ctype;
                ctype.generate_c_with_name($name, &mut w);
                assert_eq!(w, $expected);
            };
        }

        assert_c_type!(CType::Void, "main", "void main");
        assert_c_type!(CType::void_ptr(), "data", "void* data");
        assert_c_type!(CType::const_void_ptr(), "data", "const void* data");
        assert_c_type!(CType::U8, "data", "unsigned char data");
        assert_c_type!(CType::U16, "data", "unsigned short data");
        assert_c_type!(CType::U32, "data", "unsigned int data");
        assert_c_type!(CType::U64, "data", "unsigned long long data");
        assert_c_type!(CType::USize, "data", "unsigned size_t data");
        assert_c_type!(CType::I8, "data", "char data");
        assert_c_type!(CType::I16, "data", "short data");
        assert_c_type!(CType::I32, "data", "int data");
        assert_c_type!(CType::I64, "data", "long long data");
        assert_c_type!(CType::ISize, "data", "size_t data");
        assert_c_type!(
            CType::TypeDef("my_type".to_string()),
            "data",
            "my_type data"
        );
        assert_c_type!(
            CType::Function {
                arguments: vec![CType::U8, CType::ISize],
                return_value: None
            },
            "my_func",
            "void my_func(unsigned char, size_t)"
        );
        assert_c_type!(
            CType::Function {
                arguments: vec![],
                return_value: Some(Box::new(CType::ISize))
            },
            "my_func",
            "size_t my_func()"
        );
        assert_c_type!(
            CType::PointerTo {
                is_const: true,
                inner: Box::new(CType::PointerTo {
                    is_const: false,
                    inner: Box::new(CType::U32)
                })
            },
            "data",
            "const unsigned int** data"
        );
        // TODO: test more complicated const correctness rules: there are bugs relating to it.
    }

    #[test]
    fn generate_expressions_works() {
        macro_rules! assert_c_expr {
            ($cexpr:expr, $expected:expr) => {
                let mut w = String::new();
                let cexpr = $cexpr;
                cexpr.generate_c(&mut w);
                assert_eq!(w, $expected);
            };
        }

        assert_c_expr!(
            CStatement::LiteralConstant {
                value: "\"Hello, world!\"".to_string()
            },
            "\"Hello, world!\""
        );
        assert_c_expr!(
            CStatement::TypeDef {
                source_type: CType::Function {
                    arguments: vec![CType::I32, CType::I32],
                    return_value: None,
                },
                new_name: "my_func_ptr".to_string(),
            },
            "typedef void (*my_func_ptr)(int, int);\n"
        );
        assert_c_expr!(
            CStatement::LiteralArray {
                items: vec![
                    CStatement::LiteralConstant {
                        value: "1".to_string()
                    },
                    CStatement::LiteralConstant {
                        value: "2".to_string()
                    },
                    CStatement::LiteralConstant {
                        value: "3".to_string()
                    },
                ]
            },
            "{\n\t1,\n\t2,\n\t3,\n}"
        );
        assert_c_expr!(CStatement::LiteralArray { items: vec![] }, "{}");
        assert_c_expr!(
            CStatement::Declaration {
                name: "my_array".to_string(),
                is_extern: false,
                is_const: true,
                ctype: CType::Array {
                    inner: Box::new(CType::I32)
                },
                definition: Some(Box::new(CStatement::LiteralArray {
                    items: vec![
                        CStatement::LiteralConstant {
                            value: "1".to_string()
                        },
                        CStatement::LiteralConstant {
                            value: "2".to_string()
                        },
                        CStatement::LiteralConstant {
                            value: "3".to_string()
                        },
                    ]
                }))
            },
            "const int my_array[] = {\n\t1,\n\t2,\n\t3,\n};\n"
        );
        assert_c_expr!(
            CStatement::Declaration {
                name: "my_array".to_string(),
                is_extern: true,
                is_const: true,
                ctype: CType::Array {
                    inner: Box::new(CType::I32)
                },
                definition: None,
            },
            "const extern int my_array[];\n"
        );
    }
}

'''
'''--- lib/cli/src/c_gen/staticlib_header.rs ---
//! Generate a header file for the object file produced by the Staticlib engine.

use super::{generate_c, CStatement, CType};
use wasmer_compiler::{Symbol, SymbolRegistry};
use wasmer_types::ModuleInfo;

/// Helper functions to simplify the usage of the Staticlib engine.
const HELPER_FUNCTIONS: &str = r#"
wasm_byte_vec_t generate_serialized_data() {
        // We need to pass all the bytes as one big buffer so we have to do all this logic to memcpy
        // the various pieces together from the generated header file.
        //
        // We should provide a `deseralize_vectored` function to avoid requiring this extra work.

        char* byte_ptr = (char*)&WASMER_METADATA[0];

        size_t num_function_pointers
                = sizeof(function_pointers) / sizeof(void*);
        size_t num_function_trampolines
                = sizeof(function_trampolines) / sizeof(void*);
        size_t num_dynamic_function_trampoline_pointers
                = sizeof(dynamic_function_trampoline_pointers) / sizeof(void*);

        size_t buffer_size = module_bytes_len
                + sizeof(size_t) + sizeof(function_pointers)
                + sizeof(size_t) + sizeof(function_trampolines)
                + sizeof(size_t) + sizeof(dynamic_function_trampoline_pointers);

        char* memory_buffer = (char*) malloc(buffer_size);
        size_t current_offset = 0;

        memcpy(memory_buffer + current_offset, byte_ptr, module_bytes_len);
        current_offset += module_bytes_len;

        memcpy(memory_buffer + current_offset, (void*)&num_function_pointers, sizeof(size_t));
        current_offset += sizeof(size_t);

        memcpy(memory_buffer + current_offset, (void*)&function_pointers[0], sizeof(function_pointers));
        current_offset += sizeof(function_pointers);

        memcpy(memory_buffer + current_offset, (void*)&num_function_trampolines, sizeof(size_t));
        current_offset += sizeof(size_t);

        memcpy(memory_buffer + current_offset, (void*)&function_trampolines[0], sizeof(function_trampolines));
        current_offset += sizeof(function_trampolines);

        memcpy(memory_buffer + current_offset, (void*)&num_dynamic_function_trampoline_pointers, sizeof(size_t));
        current_offset += sizeof(size_t);

        memcpy(memory_buffer + current_offset, (void*)&dynamic_function_trampoline_pointers[0], sizeof(dynamic_function_trampoline_pointers));
        current_offset += sizeof(dynamic_function_trampoline_pointers);

        wasm_byte_vec_t module_byte_vec = {
                .size = buffer_size,
                .data = memory_buffer,
        };
        return module_byte_vec;
}

wasm_module_t* wasmer_staticlib_engine_new(wasm_store_t* store, const char* wasm_name) {
        // wasm_name intentionally unused for now: will be used in the future.
        wasm_byte_vec_t module_byte_vec = generate_serialized_data();
        wasm_module_t* module = wasm_module_deserialize(store, &module_byte_vec);
        free(module_byte_vec.data);

        return module;
}
"#;

/// Generate the header file that goes with the generated object file.
pub fn generate_header_file(
    module_info: &ModuleInfo,
    symbol_registry: &dyn SymbolRegistry,
    metadata_length: usize,
) -> String {
    let mut c_statements = vec![];
    c_statements.push(CStatement::LiteralConstant {
        value: "#include <stdlib.h>\n#include <string.h>\n\n".to_string(),
    });
    c_statements.push(CStatement::LiteralConstant {
        value: "#ifdef __cplusplus\nextern \"C\" {\n#endif\n\n".to_string(),
    });
    c_statements.push(CStatement::Declaration {
        name: "module_bytes_len".to_string(),
        is_extern: false,
        is_const: true,
        ctype: CType::U32,
        definition: Some(Box::new(CStatement::LiteralConstant {
            value: metadata_length.to_string(),
        })),
    });
    c_statements.push(CStatement::Declaration {
        name: "WASMER_METADATA".to_string(),
        is_extern: true,
        is_const: true,
        ctype: CType::Array {
            inner: Box::new(CType::U8),
        },
        definition: None,
    });
    let function_declarations = module_info
        .functions
        .iter()
        .filter_map(|(f_index, sig_index)| {
            Some((module_info.local_func_index(f_index)?, sig_index))
        })
        .map(|(function_local_index, _sig_index)| {
            let function_name =
                symbol_registry.symbol_to_name(Symbol::LocalFunction(function_local_index));
            // TODO: figure out the signature here too
            CStatement::Declaration {
                name: function_name,
                is_extern: false,
                is_const: false,
                ctype: CType::Function {
                    arguments: vec![CType::Void],
                    return_value: None,
                },
                definition: None,
            }
        });
    c_statements.push(CStatement::LiteralConstant {
        value: r#"
// Compiled Wasm function pointers ordered by function index: the order they
// appeared in in the Wasm module.
"#
        .to_string(),
    });
    c_statements.extend(function_declarations);

    // function pointer array
    {
        let function_pointer_array_statements = module_info
            .functions
            .iter()
            .filter_map(|(f_index, sig_index)| {
                Some((module_info.local_func_index(f_index)?, sig_index))
            })
            .map(|(function_local_index, _sig_index)| {
                let function_name =
                    symbol_registry.symbol_to_name(Symbol::LocalFunction(function_local_index));
                // TODO: figure out the signature here too

                CStatement::Cast {
                    target_type: CType::void_ptr(),
                    expression: Box::new(CStatement::LiteralConstant {
                        value: function_name,
                    }),
                }
            })
            .collect::<Vec<_>>();

        c_statements.push(CStatement::Declaration {
            name: "function_pointers".to_string(),
            is_extern: false,
            is_const: true,
            ctype: CType::Array {
                inner: Box::new(CType::void_ptr()),
            },
            definition: Some(Box::new(CStatement::LiteralArray {
                items: function_pointer_array_statements,
            })),
        });
    }

    let func_trampoline_declarations =
        module_info
            .signatures
            .iter()
            .map(|(sig_index, _func_type)| {
                let function_name =
                    symbol_registry.symbol_to_name(Symbol::FunctionCallTrampoline(sig_index));

                CStatement::Declaration {
                    name: function_name,
                    is_extern: false,
                    is_const: false,
                    ctype: CType::Function {
                        arguments: vec![CType::void_ptr(), CType::void_ptr(), CType::void_ptr()],
                        return_value: None,
                    },
                    definition: None,
                }
            });
    c_statements.push(CStatement::LiteralConstant {
        value: r#"
// Trampolines (functions by which we can call into Wasm) ordered by signature.
// There is 1 trampoline per function signature in the order they appear in
// the Wasm module.
"#
        .to_string(),
    });
    c_statements.extend(func_trampoline_declarations);

    // function trampolines
    {
        let function_trampoline_statements = module_info
            .signatures
            .iter()
            .map(|(sig_index, _vm_shared_index)| {
                let function_name =
                    symbol_registry.symbol_to_name(Symbol::FunctionCallTrampoline(sig_index));
                CStatement::LiteralConstant {
                    value: function_name,
                }
            })
            .collect::<Vec<_>>();

        c_statements.push(CStatement::Declaration {
            name: "function_trampolines".to_string(),
            is_extern: false,
            is_const: true,
            ctype: CType::Array {
                inner: Box::new(CType::void_ptr()),
            },
            definition: Some(Box::new(CStatement::LiteralArray {
                items: function_trampoline_statements,
            })),
        });
    }

    let dyn_func_declarations = module_info
        .functions
        .keys()
        .take(module_info.num_imported_functions)
        .map(|func_index| {
            let function_name =
                symbol_registry.symbol_to_name(Symbol::DynamicFunctionTrampoline(func_index));
            // TODO: figure out the signature here
            CStatement::Declaration {
                name: function_name,
                is_extern: false,
                is_const: false,
                ctype: CType::Function {
                    arguments: vec![CType::void_ptr(), CType::void_ptr(), CType::void_ptr()],
                    return_value: None,
                },
                definition: None,
            }
        });
    c_statements.push(CStatement::LiteralConstant {
        value: r#"
// Dynamic trampolines are per-function and are used for each function where
// the type signature is not known statically. In this case, this corresponds to
// the imported functions.
"#
        .to_string(),
    });
    c_statements.extend(dyn_func_declarations);

    c_statements.push(CStatement::TypeDef {
        source_type: CType::Function {
            arguments: vec![CType::void_ptr(), CType::void_ptr(), CType::void_ptr()],
            return_value: None,
        },
        new_name: "dyn_func_trampoline_t".to_string(),
    });

    // dynamic function trampoline pointer array
    {
        let dynamic_function_trampoline_statements = module_info
            .functions
            .keys()
            .take(module_info.num_imported_functions)
            .map(|func_index| {
                let function_name =
                    symbol_registry.symbol_to_name(Symbol::DynamicFunctionTrampoline(func_index));
                CStatement::LiteralConstant {
                    value: function_name,
                }
            })
            .collect::<Vec<_>>();
        c_statements.push(CStatement::Declaration {
            name: "dynamic_function_trampoline_pointers".to_string(),
            is_extern: false,
            is_const: true,
            ctype: CType::Array {
                inner: Box::new(CType::TypeDef("dyn_func_trampoline_t".to_string())),
            },
            definition: Some(Box::new(CStatement::LiteralArray {
                items: dynamic_function_trampoline_statements,
            })),
        });
    }

    c_statements.push(CStatement::LiteralConstant {
        value: HELPER_FUNCTIONS.to_string(),
    });

    c_statements.push(CStatement::LiteralConstant {
        value: "\n#ifdef __cplusplus\n}\n#endif\n\n".to_string(),
    });

    generate_c(&c_statements)
}

'''
'''--- lib/cli/src/cli.rs ---
//! The logic for the Wasmer CLI tool.

#[cfg(target_os = "linux")]
use crate::commands::Binfmt;
#[cfg(feature = "compiler")]
use crate::commands::Compile;
#[cfg(all(feature = "staticlib", feature = "compiler"))]
use crate::commands::CreateExe;
#[cfg(feature = "wast")]
use crate::commands::Wast;
use crate::commands::{Cache, Config, Inspect, Run, SelfUpdate, Validate};
use crate::error::PrettyError;
use anyhow::Result;

use structopt::{clap::ErrorKind, StructOpt};

#[derive(StructOpt)]
#[cfg_attr(
    not(feature = "headless"),
    structopt(name = "wasmer", about = "WebAssembly standalone runtime.", author)
)]
#[cfg_attr(
    feature = "headless",
    structopt(
        name = "wasmer-headless",
        about = "Headless WebAssembly standalone runtime.",
        author
    )
)]
/// The options for the wasmer Command Line Interface
enum WasmerCLIOptions {
    /// Run a WebAssembly file. Formats accepted: wasm, wat
    #[structopt(name = "run")]
    Run(Run),

    /// Wasmer cache
    #[structopt(name = "cache")]
    Cache(Cache),

    /// Validate a WebAssembly binary
    #[structopt(name = "validate")]
    Validate(Validate),

    /// Compile a WebAssembly binary
    #[cfg(feature = "compiler")]
    #[structopt(name = "compile")]
    Compile(Compile),

    /// Compile a WebAssembly binary into a native executable
    #[cfg(all(feature = "staticlib", feature = "compiler"))]
    #[structopt(name = "create-exe")]
    CreateExe(CreateExe),

    /// Get various configuration information needed
    /// to compile programs which use Wasmer
    #[structopt(name = "config")]
    Config(Config),

    /// Update wasmer to the latest version
    #[structopt(name = "self-update")]
    SelfUpdate(SelfUpdate),

    /// Inspect a WebAssembly file
    #[structopt(name = "inspect")]
    Inspect(Inspect),

    /// Run spec testsuite
    #[cfg(feature = "wast")]
    #[structopt(name = "wast")]
    Wast(Wast),

    /// Unregister and/or register wasmer as binfmt interpreter
    #[cfg(target_os = "linux")]
    #[structopt(name = "binfmt")]
    Binfmt(Binfmt),
}

impl WasmerCLIOptions {
    fn execute(&self) -> Result<()> {
        match self {
            Self::Run(options) => options.execute(),
            Self::SelfUpdate(options) => options.execute(),
            Self::Cache(cache) => cache.execute(),
            Self::Validate(validate) => validate.execute(),
            #[cfg(feature = "compiler")]
            Self::Compile(compile) => compile.execute(),
            #[cfg(all(feature = "staticlib", feature = "compiler"))]
            Self::CreateExe(create_exe) => create_exe.execute(),
            Self::Config(config) => config.execute(),
            Self::Inspect(inspect) => inspect.execute(),
            #[cfg(feature = "wast")]
            Self::Wast(wast) => wast.execute(),
            #[cfg(target_os = "linux")]
            Self::Binfmt(binfmt) => binfmt.execute(),
        }
    }
}

/// The main function for the Wasmer CLI tool.
pub fn wasmer_main() {
    // We allow windows to print properly colors
    #[cfg(windows)]
    colored::control::set_virtual_terminal(true).unwrap();

    // We try to run wasmer with the normal arguments.
    // Eg. `wasmer <SUBCOMMAND>`
    // In case that fails, we fallback trying the Run subcommand directly.
    // Eg. `wasmer myfile.wasm --dir=.`
    //
    // In case we've been run as wasmer-binfmt-interpreter myfile.wasm args,
    // we assume that we're registered via binfmt_misc
    let args = std::env::args().collect::<Vec<_>>();
    let binpath = args.get(0).map(|s| s.as_ref()).unwrap_or("");
    let command = args.get(1);
    let options = if cfg!(target_os = "linux") && binpath.ends_with("wasmer-binfmt-interpreter") {
        WasmerCLIOptions::Run(Run::from_binfmt_args())
    } else {
        match command.unwrap_or(&"".to_string()).as_ref() {
            "cache" | "compile" | "config" | "create-exe" | "help" | "inspect" | "run"
            | "self-update" | "validate" | "wast" | "binfmt" => WasmerCLIOptions::from_args(),
            _ => {
                WasmerCLIOptions::from_iter_safe(args.iter()).unwrap_or_else(|e| {
                    match e.kind {
                        // This fixes a issue that:
                        // 1. Shows the version twice when doing `wasmer -V`
                        // 2. Shows the run help (instead of normal help) when doing `wasmer --help`
                        ErrorKind::VersionDisplayed | ErrorKind::HelpDisplayed => e.exit(),
                        _ => WasmerCLIOptions::Run(Run::from_args()),
                    }
                })
            }
        }
    };

    PrettyError::report(options.execute());
}

'''
'''--- lib/cli/src/commands.rs ---
//! The commands available in the Wasmer binary.
#[cfg(target_os = "linux")]
mod binfmt;
mod cache;
#[cfg(feature = "compiler")]
mod compile;
mod config;
#[cfg(all(feature = "staticlib", feature = "compiler"))]
mod create_exe;
mod inspect;
mod run;
mod self_update;
mod validate;
#[cfg(feature = "wast")]
mod wast;

#[cfg(target_os = "linux")]
pub use binfmt::*;
#[cfg(feature = "compiler")]
pub use compile::*;
#[cfg(all(feature = "staticlib", feature = "compiler"))]
pub use create_exe::*;
#[cfg(feature = "wast")]
pub use wast::*;
pub use {cache::*, config::*, inspect::*, run::*, self_update::*, validate::*};

'''
'''--- lib/cli/src/commands/binfmt.rs ---
use anyhow::{Context, Result};
use std::env;
use std::fs;
use std::io::Write;
use std::os::unix::ffi::OsStrExt;
use std::os::unix::fs::MetadataExt;
use std::path::{Path, PathBuf};
use structopt::StructOpt;
use Action::*;

#[derive(StructOpt, Clone, Copy)]
enum Action {
    /// Register wasmer as binfmt interpreter
    Register,
    /// Unregister a binfmt interpreter for wasm32
    Unregister,
    /// Soft unregister, and register
    Reregister,
}

/// Unregister and/or register wasmer as binfmt interpreter
///
/// Check the wasmer repository for a systemd service definition example
/// to automate the process at start-up.
#[derive(StructOpt)]
pub struct Binfmt {
    // Might be better to traverse the mount list
    /// Mount point of binfmt_misc fs
    #[structopt(long, default_value = "/proc/sys/fs/binfmt_misc/")]
    binfmt_misc: PathBuf,

    #[structopt(subcommand)]
    action: Action,
}

// Quick safety check:
// This folder isn't world writeable (or else its sticky bit is set), and neither are its parents.
//
// If somebody mounted /tmp wrong, this might result in a TOCTOU problem.
fn seccheck(path: &Path) -> Result<()> {
    if let Some(parent) = path.parent() {
        seccheck(parent)?;
    }
    let m = std::fs::metadata(path)
        .with_context(|| format!("Can't check permissions of {}", path.to_string_lossy()))?;
    anyhow::ensure!(
        m.mode() & 0o2 == 0 || m.mode() & 0o1000 != 0,
        "{} is world writeable and not sticky",
        path.to_string_lossy()
    );
    Ok(())
}

impl Binfmt {
    /// execute [Binfmt]
    pub fn execute(&self) -> Result<()> {
        if !self.binfmt_misc.exists() {
            panic!("{} does not exist", self.binfmt_misc.to_string_lossy());
        }
        let temp_dir;
        let specs = match self.action {
            Register | Reregister => {
                temp_dir = tempfile::tempdir().context("Make temporary directory")?;
                seccheck(temp_dir.path())?;
                let bin_path_orig: PathBuf = env::args_os()
                    .nth(0)
                    .map(Into::into)
                    .filter(|p: &PathBuf| p.exists())
                    .context("Cannot get path to wasmer executable")?;
                let bin_path = temp_dir.path().join("wasmer-binfmt-interpreter");
                fs::copy(&bin_path_orig, &bin_path).context("Copy wasmer binary to temp folder")?;
                let bin_path = fs::canonicalize(&bin_path).with_context(|| {
                    format!(
                        "Couldn't get absolute path for {}",
                        bin_path.to_string_lossy()
                    )
                })?;
                Some([
                    [
                        b":wasm32:M::\\x00asm\\x01\\x00\\x00::".as_ref(),
                        bin_path.as_os_str().as_bytes(),
                        b":PFC",
                    ]
                    .concat(),
                    [
                        b":wasm32-wat:E::wat::".as_ref(),
                        bin_path.as_os_str().as_bytes(),
                        b":PFC",
                    ]
                    .concat(),
                ])
            }
            _ => None,
        };
        let wasm_registration = self.binfmt_misc.join("wasm32");
        let wat_registration = self.binfmt_misc.join("wasm32-wat");
        match self.action {
            Reregister | Unregister => {
                let unregister = [wasm_registration, wat_registration]
                    .iter()
                    .map(|registration| {
                        if registration.exists() {
                            let mut registration = fs::OpenOptions::new()
                                .write(true)
                                .open(registration)
                                .context("Open existing binfmt entry to remove")?;
                            registration
                                .write_all(b"-1")
                                .context("Couldn't write binfmt unregister request")?;
                            Ok(true)
                        } else {
                            eprintln!(
                                "Warning: {} does not exist, not unregistered.",
                                registration.to_string_lossy()
                            );
                            Ok(false)
                        }
                    })
                    .collect::<Vec<_>>()
                    .into_iter()
                    .collect::<Result<Vec<_>>>()?;
                match (self.action, unregister.into_iter().any(|b| b)) {
                    (Unregister, false) => bail!("Nothing unregistered"),
                    _ => (),
                }
            }
            _ => (),
        };
        if let Some(specs) = specs {
            if cfg!(target_env = "gnu") {
                // Approximate. ELF parsing for a proper check feels like overkill here.
                eprintln!("Warning: wasmer has been compiled for glibc, and is thus likely dynamically linked. Invoking wasm binaries in chroots or mount namespaces (lxc, docker, ...) may not work.");
            }
            specs
                .iter()
                .map(|spec| {
                    let register = self.binfmt_misc.join("register");
                    let mut register = fs::OpenOptions::new()
                        .write(true)
                        .open(register)
                        .context("Open binfmt misc for registration")?;
                    register
                        .write_all(&spec)
                        .context("Couldn't register binfmt")?;
                    Ok(())
                })
                .collect::<Vec<_>>()
                .into_iter()
                .collect::<Result<Vec<_>>>()?;
        }
        Ok(())
    }
}

'''
'''--- lib/cli/src/commands/cache.rs ---
use crate::common::get_cache_dir;
use anyhow::{Context, Result};
use std::fs;
use structopt::StructOpt;

#[derive(Debug, StructOpt)]
/// The options for the `wasmer cache` subcommand
pub enum Cache {
    /// Clear the cache
    #[structopt(name = "clean")]
    Clean,

    /// Display the location of the cache
    #[structopt(name = "dir")]
    Dir,
}

impl Cache {
    /// Execute the cache command
    pub fn execute(&self) -> Result<()> {
        match &self {
            Cache::Clean => {
                self.clean().context("failed to clean wasmer cache.")?;
            }
            Cache::Dir => {
                self.dir()?;
            }
        }
        Ok(())
    }
    fn clean(&self) -> Result<()> {
        let cache_dir = get_cache_dir();
        if cache_dir.exists() {
            fs::remove_dir_all(cache_dir.clone())?;
        }
        fs::create_dir_all(cache_dir)?;
        eprintln!("Wasmer cache cleaned successfully.");
        Ok(())
    }
    fn dir(&self) -> Result<()> {
        println!("{}", get_cache_dir().to_string_lossy());
        Ok(())
    }
}

'''
'''--- lib/cli/src/commands/compile.rs ---
use crate::store::{EngineType, StoreOptions};
use crate::warning;
use anyhow::{Context, Result};
use std::path::PathBuf;
use structopt::StructOpt;
use wasmer::*;

#[derive(Debug, StructOpt)]
/// The options for the `wasmer compile` subcommand
pub struct Compile {
    /// Input file
    #[structopt(name = "FILE", parse(from_os_str))]
    path: PathBuf,

    /// Output file
    #[structopt(name = "OUTPUT PATH", short = "o", parse(from_os_str))]
    output: PathBuf,

    /// Output path for generated header file
    #[structopt(name = "HEADER PATH", long = "header", parse(from_os_str))]
    header_path: Option<PathBuf>,

    /// Compilation Target triple
    #[structopt(long = "target")]
    target_triple: Option<Triple>,

    #[structopt(flatten)]
    store: StoreOptions,

    #[structopt(short = "m", multiple = true, number_of_values = 1)]
    cpu_features: Vec<CpuFeature>,
}

impl Compile {
    /// Runs logic for the `compile` subcommand
    pub fn execute(&self) -> Result<()> {
        self.inner_execute()
            .context(format!("failed to compile `{}`", self.path.display()))
    }

    pub(crate) fn get_recommend_extension(
        engine_type: &EngineType,
        target_triple: &Triple,
    ) -> Result<&'static str> {
        Ok(match engine_type {
            #[cfg(feature = "dylib")]
            EngineType::Dylib => {
                wasmer_engine_dylib::DylibArtifact::get_default_extension(target_triple)
            }
            #[cfg(feature = "universal")]
            EngineType::Universal => {
                wasmer_engine_universal::UniversalArtifact::get_default_extension(target_triple)
            }
            #[cfg(feature = "staticlib")]
            EngineType::Staticlib => {
                wasmer_engine_staticlib::StaticlibArtifact::get_default_extension(target_triple)
            }
            #[cfg(not(all(feature = "dylib", feature = "universal", feature = "staticlib")))]
            _ => bail!("selected engine type is not compiled in"),
        })
    }

    fn inner_execute(&self) -> Result<()> {
        let target = self
            .target_triple
            .as_ref()
            .map(|target_triple| {
                let mut features = self
                    .cpu_features
                    .clone()
                    .into_iter()
                    .fold(CpuFeature::set(), |a, b| a | b);
                // Cranelift requires SSE2, so we have this "hack" for now to facilitate
                // usage
                features |= CpuFeature::SSE2;
                Target::new(target_triple.clone(), features)
            })
            .unwrap_or_default();
        let (store, engine_type, compiler_type) =
            self.store.get_store_for_target(target.clone())?;
        let output_filename = self
            .output
            .file_stem()
            .map(|osstr| osstr.to_string_lossy().to_string())
            .unwrap_or_default();
        let recommended_extension = Self::get_recommend_extension(&engine_type, target.triple())?;
        match self.output.extension() {
            Some(ext) => {
                if ext != recommended_extension {
                    warning!("the output file has a wrong extension. We recommend using `{}.{}` for the chosen target", &output_filename, &recommended_extension)
                }
            }
            None => {
                warning!("the output file has no extension. We recommend using `{}.{}` for the chosen target", &output_filename, &recommended_extension)
            }
        }
        println!("Engine: {}", engine_type.to_string());
        println!("Compiler: {}", compiler_type.to_string());
        println!("Target: {}", target.triple());

        let module = Module::from_file(&store, &self.path)?;

        let _ = module.serialize_to_file(&self.output)?;
        eprintln!(
            "‚úî File compiled successfully to `{}`.",
            self.output.display(),
        );

        #[cfg(feature = "staticlib")]
        if engine_type == EngineType::Staticlib {
            let artifact: &wasmer_engine_staticlib::StaticlibArtifact =
                module.artifact().as_ref().downcast_ref().context("Engine type is Staticlib but could not downcast artifact into StaticlibArtifact")?;
            let symbol_registry = artifact.symbol_registry();
            let metadata_length = artifact.metadata_length();
            let module_info = module.info();
            let header_file_src = crate::c_gen::staticlib_header::generate_header_file(
                module_info,
                symbol_registry,
                metadata_length,
            );

            let header_path = self.header_path.as_ref().cloned().unwrap_or_else(|| {
                let mut hp = PathBuf::from(
                    self.path
                        .file_stem()
                        .map(|fs| fs.to_string_lossy().to_string())
                        .unwrap_or_else(|| "wasm_out".to_string()),
                );
                hp.set_extension("h");
                hp
            });
            // for C code
            let mut header = std::fs::OpenOptions::new()
                .create(true)
                .truncate(true)
                .write(true)
                .open(&header_path)?;

            use std::io::Write;
            header.write_all(header_file_src.as_bytes())?;
            eprintln!(
                "‚úî Header file generated successfully at `{}`.",
                header_path.display(),
            );
        }
        Ok(())
    }
}

'''
'''--- lib/cli/src/commands/config.rs ---
use crate::VERSION;
use anyhow::{Context, Result};
use std::env;
use std::path::PathBuf;
use structopt::StructOpt;

#[derive(Debug, StructOpt)]
/// The options for the `wasmer config` subcommand
pub struct Config {
    /// Print the installation prefix.
    #[structopt(long, conflicts_with = "pkg-config")]
    prefix: bool,

    /// Directory containing Wasmer executables.
    #[structopt(long, conflicts_with = "pkg-config")]
    bindir: bool,

    /// Directory containing Wasmer headers.
    #[structopt(long, conflicts_with = "pkg-config")]
    includedir: bool,

    /// Directory containing Wasmer libraries.
    #[structopt(long, conflicts_with = "pkg-config")]
    libdir: bool,

    /// Libraries needed to link against Wasmer components.
    #[structopt(long, conflicts_with = "pkg-config")]
    libs: bool,

    /// C compiler flags for files that include Wasmer headers.
    #[structopt(long, conflicts_with = "pkg-config")]
    cflags: bool,

    /// It outputs the necessary details for compiling
    /// and linking a program to Wasmer, using the `pkg-config` format.
    #[structopt(long)]
    pkg_config: bool,
}

impl Config {
    /// Runs logic for the `config` subcommand
    pub fn execute(&self) -> Result<()> {
        self.inner_execute()
            .context("failed to retrieve the wasmer config".to_string())
    }
    fn inner_execute(&self) -> Result<()> {
        let key = "WASMER_DIR";
        let wasmer_dir = env::var(key)
            .or_else(|e| {
                option_env!("WASMER_INSTALL_PREFIX")
                    .map(str::to_string)
                    .ok_or(e)
            })
            .context(format!(
                "failed to retrieve the {} environment variables",
                key
            ))?;

        let prefix = PathBuf::from(wasmer_dir);

        let prefixdir = prefix.display().to_string();
        let bindir = prefix.join("bin").display().to_string();
        let includedir = prefix.join("include").display().to_string();
        let libdir = prefix.join("lib").display().to_string();
        let cflags = format!("-I{}", includedir);
        let libs = format!("-L{} -lwasmer", libdir);

        if self.pkg_config {
            println!("prefix={}", prefixdir);
            println!("exec_prefix={}", bindir);
            println!("includedir={}", includedir);
            println!("libdir={}", libdir);
            println!();
            println!("Name: wasmer");
            println!("Description: The Wasmer library for running WebAssembly");
            println!("Version: {}", VERSION);
            println!("Cflags: {}", cflags);
            println!("Libs: {}", libs);
            return Ok(());
        }

        if self.prefix {
            println!("{}", prefixdir);
        }
        if self.bindir {
            println!("{}", bindir);
        }
        if self.includedir {
            println!("{}", includedir);
        }
        if self.libdir {
            println!("{}", libdir);
        }
        if self.libs {
            println!("{}", libs);
        }
        if self.cflags {
            println!("{}", cflags);
        }
        Ok(())
    }
}

'''
'''--- lib/cli/src/commands/create_exe.rs ---
//! Create a standalone native executable for a given Wasm file.

use crate::store::{CompilerOptions, EngineType};
use anyhow::{Context, Result};
use std::env;
use std::fs;
use std::path::{Path, PathBuf};
use std::process::Command;
use structopt::StructOpt;
use wasmer::*;

const WASMER_MAIN_C_SOURCE: &[u8] = include_bytes!("wasmer_create_exe_main.c");

#[derive(Debug, StructOpt)]
/// The options for the `wasmer create-exe` subcommand
pub struct CreateExe {
    /// Input file
    #[structopt(name = "FILE", parse(from_os_str))]
    path: PathBuf,

    /// Output file
    #[structopt(name = "OUTPUT PATH", short = "o", parse(from_os_str))]
    output: PathBuf,

    /// Compilation Target triple
    #[structopt(long = "target")]
    target_triple: Option<Triple>,

    #[structopt(flatten)]
    compiler: CompilerOptions,

    #[structopt(short = "m", multiple = true, number_of_values = 1)]
    cpu_features: Vec<CpuFeature>,

    /// Additional libraries to link against.
    /// This is useful for fixing linker errors that may occur on some systems.
    #[structopt(short = "l", multiple = true, number_of_values = 1)]
    libraries: Vec<String>,
}

impl CreateExe {
    /// Runs logic for the `compile` subcommand
    pub fn execute(&self) -> Result<()> {
        let target = self
            .target_triple
            .as_ref()
            .map(|target_triple| {
                let mut features = self
                    .cpu_features
                    .clone()
                    .into_iter()
                    .fold(CpuFeature::set(), |a, b| a | b);
                // Cranelift requires SSE2, so we have this "hack" for now to facilitate
                // usage
                features |= CpuFeature::SSE2;
                Target::new(target_triple.clone(), features)
            })
            .unwrap_or_default();
        let engine_type = EngineType::Staticlib;
        let (store, compiler_type) = self
            .compiler
            .get_store_for_target_and_engine(target.clone(), engine_type)?;

        println!("Engine: {}", engine_type.to_string());
        println!("Compiler: {}", compiler_type.to_string());
        println!("Target: {}", target.triple());

        let working_dir = tempfile::tempdir()?;
        let starting_cd = env::current_dir()?;
        let output_path = starting_cd.join(&self.output);
        env::set_current_dir(&working_dir)?;

        #[cfg(not(windows))]
        let wasm_object_path = PathBuf::from("wasm.o");
        #[cfg(windows)]
        let wasm_object_path = PathBuf::from("wasm.obj");

        let wasm_module_path = starting_cd.join(&self.path);

        let module =
            Module::from_file(&store, &wasm_module_path).context("failed to compile Wasm")?;
        let _ = module.serialize_to_file(&wasm_object_path)?;

        let artifact: &wasmer_engine_staticlib::StaticlibArtifact =
            module.artifact().as_ref().downcast_ref().context(
                "Engine type is Staticlib but could not downcast artifact into StaticlibArtifact",
            )?;
        let symbol_registry = artifact.symbol_registry();
        let metadata_length = artifact.metadata_length();
        let module_info = module.info();
        let header_file_src = crate::c_gen::staticlib_header::generate_header_file(
            module_info,
            symbol_registry,
            metadata_length,
        );

        generate_header(header_file_src.as_bytes())?;
        self.compile_c(wasm_object_path, output_path)?;

        eprintln!(
            "‚úî Native executable compiled successfully to `{}`.",
            self.output.display(),
        );

        Ok(())
    }

    fn compile_c(&self, wasm_object_path: PathBuf, output_path: PathBuf) -> anyhow::Result<()> {
        use std::io::Write;

        // write C src to disk
        let c_src_path = Path::new("wasmer_main.c");
        #[cfg(not(windows))]
        let c_src_obj = PathBuf::from("wasmer_main.o");
        #[cfg(windows)]
        let c_src_obj = PathBuf::from("wasmer_main.obj");

        {
            let mut c_src_file = fs::OpenOptions::new()
                .create_new(true)
                .write(true)
                .open(&c_src_path)
                .context("Failed to open C source code file")?;
            c_src_file.write_all(WASMER_MAIN_C_SOURCE)?;
        }
        run_c_compile(&c_src_path, &c_src_obj, self.target_triple.clone())
            .context("Failed to compile C source code")?;
        LinkCode {
            object_paths: vec![c_src_obj, wasm_object_path],
            output_path,
            additional_libraries: self.libraries.clone(),
            target: self.target_triple.clone(),
            ..Default::default()
        }
        .run()
        .context("Failed to link objects together")?;

        Ok(())
    }
}

fn generate_header(header_file_src: &[u8]) -> anyhow::Result<()> {
    let header_file_path = Path::new("my_wasm.h");
    let mut header = std::fs::OpenOptions::new()
        .create(true)
        .truncate(true)
        .write(true)
        .open(&header_file_path)?;

    use std::io::Write;
    header.write_all(header_file_src)?;

    Ok(())
}

fn get_wasmer_dir() -> anyhow::Result<PathBuf> {
    Ok(PathBuf::from(
        env::var("WASMER_DIR")
            .or_else(|e| {
                option_env!("WASMER_INSTALL_PREFIX")
                    .map(str::to_string)
                    .ok_or(e)
            })
            .context("Trying to read env var `WASMER_DIR`")?,
    ))
}

fn get_wasmer_include_directory() -> anyhow::Result<PathBuf> {
    let mut path = get_wasmer_dir()?;
    path.push("include");
    Ok(path)
}

/// path to the static libwasmer
fn get_libwasmer_path() -> anyhow::Result<PathBuf> {
    let mut path = get_wasmer_dir()?;
    path.push("lib");

    // TODO: prefer headless Wasmer if/when it's a separate library.
    #[cfg(not(windows))]
    path.push("libwasmer.a");
    #[cfg(windows)]
    path.push("wasmer.lib");

    Ok(path)
}

/// Compile the C code.
fn run_c_compile(
    path_to_c_src: &Path,
    output_name: &Path,
    target: Option<Triple>,
) -> anyhow::Result<()> {
    #[cfg(not(windows))]
    let c_compiler = "cc";
    // We must use a C++ compiler on Windows because wasm.h uses `static_assert`
    // which isn't available in `clang` on Windows.
    #[cfg(windows)]
    let c_compiler = "clang++";

    let mut command = Command::new(c_compiler);
    let command = command
        .arg("-O2")
        .arg("-c")
        .arg(path_to_c_src)
        .arg("-I")
        .arg(get_wasmer_include_directory()?);

    let command = if let Some(target) = target {
        command.arg("-target").arg(format!("{}", target))
    } else {
        command
    };

    let output = command.arg("-o").arg(output_name).output()?;

    if !output.status.success() {
        bail!(
            "C code compile failed with: stdout: {}\n\nstderr: {}",
            std::str::from_utf8(&output.stdout)
                .expect("stdout is not utf8! need to handle arbitrary bytes"),
            std::str::from_utf8(&output.stderr)
                .expect("stderr is not utf8! need to handle arbitrary bytes")
        );
    }
    Ok(())
}

/// Data used to run a linking command for generated artifacts.
#[derive(Debug)]
struct LinkCode {
    /// Path to the linker used to run the linking command.
    linker_path: PathBuf,
    /// String used as an optimization flag.
    optimization_flag: String,
    /// Paths of objects to link.
    object_paths: Vec<PathBuf>,
    /// Additional libraries to link against.
    additional_libraries: Vec<String>,
    /// Path to the output target.
    output_path: PathBuf,
    /// Path to the dir containing the static libwasmer library.
    libwasmer_path: PathBuf,
    /// The target to link the executable for.
    target: Option<Triple>,
}

impl Default for LinkCode {
    fn default() -> Self {
        #[cfg(not(windows))]
        let linker = "cc";
        #[cfg(windows)]
        let linker = "clang";
        Self {
            linker_path: PathBuf::from(linker),
            optimization_flag: String::from("-O2"),
            object_paths: vec![],
            additional_libraries: vec![],
            output_path: PathBuf::from("a.out"),
            libwasmer_path: get_libwasmer_path().unwrap(),
            target: None,
        }
    }
}

impl LinkCode {
    fn run(&self) -> anyhow::Result<()> {
        let mut command = Command::new(&self.linker_path);
        let command = command
            .arg(&self.optimization_flag)
            .args(
                self.object_paths
                    .iter()
                    .map(|path| path.canonicalize().unwrap()),
            )
            .arg(
                &self
                    .libwasmer_path
                    .canonicalize()
                    .context("Failed to find libwasmer")?,
            );
        let command = if let Some(target) = &self.target {
            command.arg("-target").arg(format!("{}", target))
        } else {
            command
        };
        // Add libraries required per platform.
        // We need userenv, sockets (Ws2_32), advapi32 for some system calls and bcrypt for random numbers.
        #[cfg(windows)]
        let command = command
            .arg("-luserenv")
            .arg("-lWs2_32")
            .arg("-ladvapi32")
            .arg("-lbcrypt");
        // On unix we need dlopen-related symbols, libmath for a few things, and pthreads.
        #[cfg(not(windows))]
        let command = command.arg("-ldl").arg("-lm").arg("-pthread");
        let link_aganist_extra_libs = self
            .additional_libraries
            .iter()
            .map(|lib| format!("-l{}", lib));
        let command = command.args(link_aganist_extra_libs);
        let output = command.arg("-o").arg(&self.output_path).output()?;

        if !output.status.success() {
            bail!(
                "linking failed with: stdout: {}\n\nstderr: {}",
                std::str::from_utf8(&output.stdout)
                    .expect("stdout is not utf8! need to handle arbitrary bytes"),
                std::str::from_utf8(&output.stderr)
                    .expect("stderr is not utf8! need to handle arbitrary bytes")
            );
        }
        Ok(())
    }
}

'''
'''--- lib/cli/src/commands/inspect.rs ---
use crate::store::StoreOptions;
use anyhow::{Context, Result};
use bytesize::ByteSize;
use std::path::PathBuf;
use structopt::StructOpt;
use wasmer::*;

#[derive(Debug, StructOpt)]
/// The options for the `wasmer validate` subcommand
pub struct Inspect {
    /// File to validate as WebAssembly
    #[structopt(name = "FILE", parse(from_os_str))]
    path: PathBuf,

    #[structopt(flatten)]
    store: StoreOptions,
}

impl Inspect {
    /// Runs logic for the `validate` subcommand
    pub fn execute(&self) -> Result<()> {
        self.inner_execute()
            .context(format!("failed to inspect `{}`", self.path.display()))
    }
    fn inner_execute(&self) -> Result<()> {
        let (store, _engine_type, _compiler_type) = self.store.get_store()?;
        let module_contents = std::fs::read(&self.path)?;
        let module = Module::new(&store, &module_contents)?;
        println!(
            "Type: {}",
            if !is_wasm(&module_contents) {
                "wat"
            } else {
                "wasm"
            }
        );
        println!("Size: {}", ByteSize(module_contents.len() as _));
        println!("Imports:");
        println!("  Functions:");
        for f in module.imports().functions() {
            println!("    \"{}\".\"{}\": {}", f.module(), f.name(), f.ty());
        }
        println!("  Memories:");
        for f in module.imports().memories() {
            println!("    \"{}\".\"{}\": {}", f.module(), f.name(), f.ty());
        }
        println!("  Tables:");
        for f in module.imports().tables() {
            println!("    \"{}\".\"{}\": {}", f.module(), f.name(), f.ty());
        }
        println!("  Globals:");
        for f in module.imports().globals() {
            println!("    \"{}\".\"{}\": {}", f.module(), f.name(), f.ty());
        }
        println!("Exports:");
        println!("  Functions:");
        for f in module.exports().functions() {
            println!("    \"{}\": {}", f.name(), f.ty());
        }
        println!("  Memories:");
        for f in module.exports().memories() {
            println!("    \"{}\": {}", f.name(), f.ty());
        }
        println!("  Tables:");
        for f in module.exports().tables() {
            println!("    \"{}\": {}", f.name(), f.ty());
        }
        println!("  Globals:");
        for f in module.exports().globals() {
            println!("    \"{}\": {}", f.name(), f.ty());
        }
        Ok(())
    }
}

'''
'''--- lib/cli/src/commands/run.rs ---
use crate::common::get_cache_dir;
#[cfg(feature = "debug")]
use crate::logging;
use crate::store::{CompilerType, EngineType, StoreOptions};
use crate::suggestions::suggest_function_exports;
use crate::warning;
use anyhow::{anyhow, Context, Result};
use std::path::PathBuf;
use std::str::FromStr;
use wasmer::*;

use structopt::StructOpt;

#[derive(Debug, StructOpt, Clone, Default)]
/// The options for the `wasmer run` subcommand
pub struct Run {
    /// File to run
    #[structopt(name = "FILE", parse(from_os_str))]
    path: PathBuf,

    /// Invoke a specified function
    #[structopt(long = "invoke", short = "i")]
    invoke: Option<String>,

    /// The command name is a string that will override the first argument passed
    /// to the wasm program. This is used in wapm to provide nicer output in
    /// help commands and error messages of the running wasm program
    #[structopt(long = "command-name", hidden = true)]
    command_name: Option<String>,

    #[structopt(flatten)]
    store: StoreOptions,

    /// Enable debug output
    #[cfg(feature = "debug")]
    #[structopt(long = "debug", short = "d")]
    debug: bool,

    #[cfg(feature = "debug")]
    #[structopt(short, long, parse(from_occurrences))]
    verbose: u8,

    /// Application arguments
    #[structopt(value_name = "ARGS")]
    args: Vec<String>,
}

impl Run {
    /// Execute the run command
    pub fn execute(&self) -> Result<()> {
        #[cfg(feature = "debug")]
        if self.debug {
            logging::set_up_logging(self.verbose).unwrap();
        }
        self.inner_execute().with_context(|| {
            format!(
                "failed to run `{}`{}",
                self.path.display(),
                if CompilerType::enabled().is_empty() {
                    " (no compilers enabled)"
                } else {
                    ""
                }
            )
        })
    }

    fn inner_execute(&self) -> Result<()> {
        let module = self.get_module()?;
        let instance = Instance::new(&module, &imports! {})?;

        // If this module exports an _initialize function, run that first.
        if let Ok(initialize) = instance.exports.get_function("_initialize") {
            initialize
                .call(&[])
                .with_context(|| "failed to run _initialize function")?;
        }

        // Do we want to invoke a function?
        if let Some(ref invoke) = self.invoke {
            let imports = imports! {};
            let instance = Instance::new(&module, &imports)?;
            let result = self.invoke_function(&instance, &invoke, &self.args)?;
            println!(
                "{}",
                result
                    .iter()
                    .map(|val| val.to_string())
                    .collect::<Vec<String>>()
                    .join(" ")
            );
        } else {
            let start: Function = self.try_find_function(&instance, "_start", &[])?;
            let result = start.call(&[]);
            result?;
        }

        Ok(())
    }

    fn get_module(&self) -> Result<Module> {
        let contents = std::fs::read(self.path.clone())?;
        #[cfg(feature = "universal")]
        {
            use wasmer_engine_universal::{Universal, UniversalArtifact, UniversalExecutable};

            if UniversalExecutable::verify_serialized(&contents) {
                unsafe {
                    let executable = UniversalExecutable::archive_from_slice(&contents)?;
                    let engine = wasmer_engine_universal::Universal::headless().engine();
                    let artifact = engine.load(&executable);
                    let store = Store::new(&engine);
                    let module = unsafe { Module::deserialize_from_file(&store, &self.path)? };
                }
                return Ok(module);
            }
        }
        let (store, engine_type, compiler_type) = self.store.get_store()?;
        let module_result = Module::new(&store, &contents);

        let mut module = module_result.with_context(|| {
            format!(
                "module instantiation failed (engine: {}, compiler: {})",
                engine_type.to_string(),
                compiler_type.to_string()
            )
        })?;
        // We set the name outside the cache, to make sure we dont cache the name
        module.set_name(&self.path.file_name().unwrap_or_default().to_string_lossy());

        Ok(module)
    }

    #[cfg(feature = "cache")]
    fn get_module_from_cache(
        &self,
        store: &Store,
        contents: &[u8],
        engine_type: &EngineType,
        compiler_type: &CompilerType,
    ) -> Result<Module> {
        // We try to get it from cache, in case caching is enabled
        // and the file length is greater than 4KB.
        // For files smaller than 4KB caching is not worth,
        // as it takes space and the speedup is minimal.
        let mut cache = self.get_cache(engine_type, compiler_type)?;
        // Try to get the hash from the provided `--cache-key`, otherwise
        // generate one from the provided file `.wasm` contents.
        let hash = self
            .cache_key
            .as_ref()
            .and_then(|key| Hash::from_str(&key).ok())
            .unwrap_or_else(|| Hash::generate(&contents));
        match unsafe { cache.load(&store, hash) } {
            Ok(module) => Ok(module),
            Err(e) => {
                match e {
                    DeserializeError::Io(_) => {
                        // Do not notify on IO errors
                    }
                    err => {
                        warning!("cached module is corrupted: {}", err);
                    }
                }
                let module = Module::new(&store, &contents)?;
                // Store the compiled Module in cache
                cache.store(hash, &module)?;
                Ok(module)
            }
        }
    }

    #[cfg(feature = "cache")]
    /// Get the Compiler Filesystem cache
    fn get_cache(
        &self,
        engine_type: &EngineType,
        compiler_type: &CompilerType,
    ) -> Result<FileSystemCache> {
        let mut cache_dir_root = get_cache_dir();
        cache_dir_root.push(compiler_type.to_string());
        let mut cache = FileSystemCache::new(cache_dir_root)?;

        // Important: Dylib files need to have a `.dll` extension on
        // Windows, otherwise they will not load, so we just add an
        // extension always to make it easier to recognize as well.
        #[allow(unreachable_patterns)]
        let extension = match *engine_type {
            #[cfg(feature = "dylib")]
            EngineType::Dylib => {
                wasmer_engine_dylib::DylibArtifact::get_default_extension(&Triple::host())
                    .to_string()
            }
            #[cfg(feature = "universal")]
            EngineType::Universal => {
                wasmer_engine_universal::UniversalArtifact::get_default_extension(&Triple::host())
                    .to_string()
            }
            // We use the compiler type as the default extension
            _ => compiler_type.to_string(),
        };
        cache.set_cache_extension(Some(extension));
        Ok(cache)
    }

    fn try_find_function(
        &self,
        instance: &Instance,
        name: &str,
        args: &[String],
    ) -> Result<Function> {
        Ok(instance
            .exports
            .get_function(&name)
            .map_err(|e| {
                if instance.module().info().functions.is_empty() {
                    anyhow!("The module has no exported functions to call.")
                } else {
                    let suggested_functions = suggest_function_exports(instance.module(), "");
                    let names = suggested_functions
                        .iter()
                        .take(3)
                        .map(|arg| format!("`{}`", arg))
                        .collect::<Vec<_>>()
                        .join(", ");
                    let suggested_command = format!(
                        "wasmer {} -i {} {}",
                        self.path.display(),
                        suggested_functions.get(0).unwrap_or(&String::new()),
                        args.join(" ")
                    );
                    let suggestion = if suggested_functions.len() == 0 {
                        String::from("Can not find any export functions.")
                    } else {
                        format!(
                            "Similar functions found: {}.\nTry with: {}",
                            names, suggested_command
                        )
                    };
                    match e {
                        ExportError::Missing(_) => {
                            anyhow!("No export `{}` found in the module.\n{}", name, suggestion)
                        }
                        ExportError::IncompatibleType => anyhow!(
                            "Export `{}` found, but is not a function.\n{}",
                            name,
                            suggestion
                        ),
                    }
                }
            })?
            .clone())
    }

    fn invoke_function(
        &self,
        instance: &Instance,
        invoke: &str,
        args: &[String],
    ) -> Result<Box<[Val]>> {
        let func: Function = self.try_find_function(&instance, invoke, args)?;
        let func_ty = func.ty();
        let required_arguments = func_ty.params().len();
        let provided_arguments = args.len();
        if required_arguments != provided_arguments {
            bail!(
                "Function expected {} arguments, but received {}: \"{}\"",
                required_arguments,
                provided_arguments,
                self.args.join(" ")
            );
        }
        let invoke_args = args
            .iter()
            .zip(func_ty.params().iter())
            .map(|(arg, param_type)| match param_type {
                ValType::I32 => {
                    Ok(Val::I32(arg.parse().map_err(|_| {
                        anyhow!("Can't convert `{}` into a i32", arg)
                    })?))
                }
                ValType::I64 => {
                    Ok(Val::I64(arg.parse().map_err(|_| {
                        anyhow!("Can't convert `{}` into a i64", arg)
                    })?))
                }
                ValType::F32 => {
                    Ok(Val::F32(arg.parse().map_err(|_| {
                        anyhow!("Can't convert `{}` into a f32", arg)
                    })?))
                }
                ValType::F64 => {
                    Ok(Val::F64(arg.parse().map_err(|_| {
                        anyhow!("Can't convert `{}` into a f64", arg)
                    })?))
                }
                _ => Err(anyhow!(
                    "Don't know how to convert {} into {:?}",
                    arg,
                    param_type
                )),
            })
            .collect::<Result<Vec<_>>>()?;
        Ok(func.call(&invoke_args)?)
    }

    /// Create Run instance for arguments/env,
    /// assuming we're being run from a CFP binfmt interpreter.
    pub fn from_binfmt_args() -> Run {
        Self::from_binfmt_args_fallible().unwrap_or_else(|e| {
            crate::error::PrettyError::report::<()>(
                Err(e).context("Failed to set up wasmer binfmt invocation"),
            )
        })
    }

    #[cfg(target_os = "linux")]
    fn from_binfmt_args_fallible() -> Result<Run> {
        let argv = std::env::args_os().collect::<Vec<_>>();
        let (_interpreter, executable, original_executable, args) = match &argv[..] {
            [a, b, c, d @ ..] => (a, b, c, d),
            _ => {
                bail!("Wasmer binfmt interpreter needs at least three arguments (including $0) - must be registered as binfmt interpreter with the CFP flags. (Got arguments: {:?})", argv);
            }
        };
        // TODO: Optimally, args and env would be passed as an UTF-8 Vec.
        // (Can be pulled out of std::os::unix::ffi::OsStrExt)
        // But I don't want to duplicate or rewrite run.rs today.
        let args = args
            .iter()
            .enumerate()
            .map(|(i, s)| {
                s.clone().into_string().map_err(|s| {
                    anyhow!(
                        "Cannot convert argument {} ({:?}) to UTF-8 string",
                        i + 1,
                        s
                    )
                })
            })
            .collect::<Result<Vec<_>>>()?;
        let original_executable = original_executable
            .clone()
            .into_string()
            .map_err(|s| anyhow!("Cannot convert executable name {:?} to UTF-8 string", s))?;
        let store = StoreOptions::default();
        // TODO: store.compiler.features.all = true; ?
        Ok(Self {
            args,
            path: executable.into(),
            command_name: Some(original_executable),
            store,
            ..Self::default()
        })
    }
    #[cfg(not(target_os = "linux"))]
    fn from_binfmt_args_fallible() -> Result<Run> {
        bail!("binfmt_misc is only available on linux.")
    }
}

'''
'''--- lib/cli/src/commands/self_update.rs ---
//! When wasmer self-update is executed, this is what gets executed
use anyhow::{Context, Result};
#[cfg(not(target_os = "windows"))]
use std::process::{Command, Stdio};
use structopt::StructOpt;

/// The options for the `wasmer self-update` subcommand
#[derive(Debug, StructOpt)]
pub struct SelfUpdate {}

impl SelfUpdate {
    /// Runs logic for the `self-update` subcommand
    pub fn execute(&self) -> Result<()> {
        self.inner_execute().context("failed to self-update wasmer")
    }

    #[cfg(not(target_os = "windows"))]
    fn inner_execute(&self) -> Result<()> {
        println!("Fetching latest installer");
        let cmd = Command::new("curl")
            .arg("https://get.wasmer.io")
            .arg("-sSfL")
            .stdout(Stdio::piped())
            .spawn()?;

        let mut process = Command::new("sh")
            .stdin(cmd.stdout.unwrap())
            .stdout(Stdio::inherit())
            .spawn()?;

        process.wait().unwrap();
        Ok(())
    }

    #[cfg(target_os = "windows")]
    fn inner_execute(&self) -> Result<()> {
        bail!("Self update is not supported on Windows. Use install instructions on the Wasmer homepage: https://wasmer.io");
    }
}

'''
'''--- lib/cli/src/commands/validate.rs ---
use crate::store::StoreOptions;
use anyhow::{bail, Context, Result};
use std::path::PathBuf;
use structopt::StructOpt;
use wasmer::*;

#[derive(Debug, StructOpt)]
/// The options for the `wasmer validate` subcommand
pub struct Validate {
    /// File to validate as WebAssembly
    #[structopt(name = "FILE", parse(from_os_str))]
    path: PathBuf,

    #[structopt(flatten)]
    store: StoreOptions,
}

impl Validate {
    /// Runs logic for the `validate` subcommand
    pub fn execute(&self) -> Result<()> {
        self.inner_execute()
            .context(format!("failed to validate `{}`", self.path.display()))
    }
    fn inner_execute(&self) -> Result<()> {
        let (store, _engine_type, _compiler_type) = self.store.get_store()?;
        let module_contents = std::fs::read(&self.path)?;
        if !is_wasm(&module_contents) {
            bail!("`wasmer validate` only validates WebAssembly files");
        }
        Module::validate(&store, &module_contents)?;
        eprintln!("Validation passed for `{}`.", self.path.display());
        Ok(())
    }
}

'''
'''--- lib/cli/src/commands/wasmer_create_exe_main.c ---
#include "wasmer.h"
#include "my_wasm.h"

#include <stdio.h>
#include <stdlib.h>
#include <string.h>

#define own

// TODO: make this define templated so that the Rust code can toggle it on/off
#define WASI

static void print_wasmer_error() {
  int error_len = wasmer_last_error_length();
  printf("Error len: `%d`\n", error_len);
  char *error_str = (char *)malloc(error_len);
  wasmer_last_error_message(error_str, error_len);
  printf("%s\n", error_str);
  free(error_str);
}

#ifdef WASI
static void pass_mapdir_arg(wasi_config_t *wasi_config, char *mapdir) {
  int colon_location = strchr(mapdir, ':') - mapdir;
  if (colon_location == 0) {
    // error malformed argument
    fprintf(stderr, "Expected mapdir argument of the form alias:directory\n");
    exit(-1);
  }

  char *alias = (char *)malloc(colon_location + 1);
  memcpy(alias, mapdir, colon_location);
  alias[colon_location] = '\0';

  int dir_len = strlen(mapdir) - colon_location;
  char *dir = (char *)malloc(dir_len + 1);
  memcpy(dir, &mapdir[colon_location + 1], dir_len);
  dir[dir_len] = '\0';

  wasi_config_mapdir(wasi_config, alias, dir);
  free(alias);
  free(dir);
}

// We try to parse out `--dir` and `--mapdir` ahead of time and process those
// specially. All other arguments are passed to the guest program.
static void handle_arguments(wasi_config_t *wasi_config, int argc,
                             char *argv[]) {
  for (int i = 1; i < argc; ++i) {
    // We probably want special args like `--dir` and `--mapdir` to not be
    // passed directly
    if (strcmp(argv[i], "--dir") == 0) {
      // next arg is a preopen directory
      if ((i + 1) < argc) {
        i++;
        wasi_config_preopen_dir(wasi_config, argv[i]);
      } else {
        fprintf(stderr, "--dir expects a following argument specifying which "
                        "directory to preopen\n");
        exit(-1);
      }
    } else if (strcmp(argv[i], "--mapdir") == 0) {
      // next arg is a mapdir
      if ((i + 1) < argc) {
        i++;
        pass_mapdir_arg(wasi_config, argv[i]);
      } else {
        fprintf(stderr,
                "--mapdir expects a following argument specifying which "
                "directory to preopen in the form alias:directory\n");
        exit(-1);
      }
    } else if (strncmp(argv[i], "--dir=", strlen("--dir=")) == 0) {
      // this arg is a preopen dir
      char *dir = argv[i] + strlen("--dir=");
      wasi_config_preopen_dir(wasi_config, dir);
    } else if (strncmp(argv[i], "--mapdir=", strlen("--mapdir=")) == 0) {
      // this arg is a mapdir
      char *mapdir = argv[i] + strlen("--mapdir=");
      pass_mapdir_arg(wasi_config, mapdir);
    } else {
      // guest argument
      wasi_config_arg(wasi_config, argv[i]);
    }
  }
}
#endif

int main(int argc, char *argv[]) {
  wasm_config_t *config = wasm_config_new();
  wasm_config_set_engine(config, STATICLIB);
  wasm_engine_t *engine = wasm_engine_new_with_config(config);
  wasm_store_t *store = wasm_store_new(engine);

  wasm_module_t *module = wasmer_staticlib_engine_new(store, argv[0]);

  if (!module) {
    fprintf(stderr, "Failed to create module\n");
    print_wasmer_error();
    return -1;
  }

  // We have now finished the memory buffer book keeping and we have a valid
  // Module.

#ifdef WASI
  wasi_config_t *wasi_config = wasi_config_new(argv[0]);
  handle_arguments(wasi_config, argc, argv);

  wasi_env_t *wasi_env = wasi_env_new(wasi_config);
  if (!wasi_env) {
    fprintf(stderr, "Error building WASI env!\n");
    print_wasmer_error();
    return 1;
  }
#endif

  wasm_importtype_vec_t import_types;
  wasm_module_imports(module, &import_types);

  wasm_extern_vec_t imports;
  wasm_extern_vec_new_uninitialized(&imports, import_types.size);
  wasm_importtype_vec_delete(&import_types);

#ifdef WASI
  bool get_imports_result = wasi_get_imports(store, module, wasi_env, &imports);
  wasi_env_delete(wasi_env);

  if (!get_imports_result) {
    fprintf(stderr, "Error getting WASI imports!\n");
    print_wasmer_error();

    return 1;
  }
#endif

  wasm_instance_t *instance = wasm_instance_new(store, module, &imports, NULL);

  if (!instance) {
    fprintf(stderr, "Failed to create instance\n");
    print_wasmer_error();
    return -1;
  }

#ifdef WASI
  own wasm_func_t *start_function = wasi_get_start_function(instance);
  if (!start_function) {
    fprintf(stderr, "`_start` function not found\n");
    print_wasmer_error();
    return -1;
  }

  wasm_val_vec_t args = WASM_EMPTY_VEC;
  wasm_val_vec_t results = WASM_EMPTY_VEC;
  own wasm_trap_t *trap = wasm_func_call(start_function, &args, &results);
  if (trap) {
    fprintf(stderr, "Trap is not NULL: TODO:\n");
    return -1;
  }
#endif

  // TODO: handle non-WASI start (maybe with invoke?)

  wasm_instance_delete(instance);
  wasm_module_delete(module);
  wasm_store_delete(store);
  wasm_engine_delete(engine);
  return 0;
}

'''
'''--- lib/cli/src/commands/wast.rs ---
//! Runs a .wast WebAssembly test suites
use crate::store::StoreOptions;
use anyhow::{Context, Result};
use std::path::PathBuf;
use structopt::StructOpt;
use wasmer_wast::Wast as WastSpectest;

#[derive(Debug, StructOpt)]
/// The options for the `wasmer wast` subcommand
pub struct Wast {
    /// Wast file to run
    #[structopt(name = "FILE", parse(from_os_str))]
    path: PathBuf,

    #[structopt(flatten)]
    store: StoreOptions,

    #[structopt(short, long)]
    /// A flag to indicate wast stop at the first error or continue.
    fail_fast: bool,
}

impl Wast {
    /// Runs logic for the `validate` subcommand
    pub fn execute(&self) -> Result<()> {
        self.inner_execute()
            .context(format!("failed to test the wast `{}`", self.path.display()))
    }
    fn inner_execute(&self) -> Result<()> {
        let (store, _engine_name, _compiler_name) = self.store.get_store()?;
        let mut wast = WastSpectest::new_with_spectest(store);
        wast.fail_fast = self.fail_fast;
        wast.run_file(&self.path).with_context(|| "tests failed")?;
        eprintln!("Wast tests succeeded for `{}`.", self.path.display());
        Ok(())
    }
}

'''
'''--- lib/cli/src/common.rs ---
//! Common module with common used structures across different
//! commands.
use crate::VERSION;
use std::env;
use std::path::PathBuf;
use structopt::StructOpt;

#[derive(Debug, StructOpt, Clone, Default)]
/// The WebAssembly features that can be passed through the
/// Command Line args.
pub struct WasmFeatures {
    /// Enable support for the SIMD proposal.
    #[structopt(long = "enable-simd")]
    pub simd: bool,

    /// Enable support for the threads proposal.
    #[structopt(long = "enable-threads")]
    pub threads: bool,

    /// Enable support for the reference types proposal.
    #[structopt(long = "enable-reference-types")]
    pub reference_types: bool,

    /// Enable support for the multi value proposal.
    #[structopt(long = "enable-multi-value")]
    pub multi_value: bool,

    /// Enable support for the bulk memory proposal.
    #[structopt(long = "enable-bulk-memory")]
    pub bulk_memory: bool,

    /// Enable support for all pre-standard proposals.
    #[structopt(long = "enable-all")]
    pub all: bool,
}

/// Get the cache dir
pub fn get_cache_dir() -> PathBuf {
    match env::var("WASMER_CACHE_DIR") {
        Ok(dir) => {
            let mut path = PathBuf::from(dir);
            path.push(VERSION);
            path
        }
        Err(_) => {
            // We use a temporal directory for saving cache files
            let mut temp_dir = env::temp_dir();
            temp_dir.push("wasmer");
            temp_dir.push(VERSION);
            temp_dir
        }
    }
}

'''
'''--- lib/cli/src/compilers/llvm.rs ---
#[derive(Debug, StructOpt, Clone)]
/// LLVM backend flags.
pub struct LLVMCLIOptions {
    /// Emit LLVM IR before optimization pipeline.
    #[structopt(long = "llvm-pre-opt-ir", parse(from_os_str))]
    pre_opt_ir: Option<PathBuf>,

    /// Emit LLVM IR after optimization pipeline.
    #[structopt(long = "llvm-post-opt-ir", parse(from_os_str))]
    post_opt_ir: Option<PathBuf>,

    /// Emit LLVM generated native code object file.
    #[structopt(long = "llvm-object-file", parse(from_os_str))]
    obj_file: Option<PathBuf>,
}

impl LLVMCallbacks for LLVMCLIOptions {
    fn preopt_ir_callback(&mut self, module: &InkwellModule) {
        if let Some(filename) = &self.pre_opt_ir {
            module.print_to_file(filename).unwrap();
        }
    }

    fn postopt_ir_callback(&mut self, module: &InkwellModule) {
        if let Some(filename) = &self.post_opt_ir {
            module.print_to_file(filename).unwrap();
        }
    }

    fn obj_memory_buffer_callback(&mut self, memory_buffer: &InkwellMemoryBuffer) {
        if let Some(filename) = &self.obj_file {
            let mem_buf_slice = memory_buffer.as_slice();
            let mut file = fs::File::create(filename).unwrap();
            let mut pos = 0;
            while pos < mem_buf_slice.len() {
                pos += file.write(&mem_buf_slice[pos..]).unwrap();
            }
        }
    }
}

'''
'''--- lib/cli/src/error.rs ---
//! Implements `PretyError` to print pretty errors in the CLI (when they happen)

use anyhow::{Chain, Error};
use colored::*;
use std::fmt::{self, Debug, Write};

/// A `PrettyError` for printing `anyhow::Error` nicely.
pub struct PrettyError {
    error: Error,
}

/// A macro that prints a warning with nice colors
#[macro_export]
macro_rules! warning {
    ($($arg:tt)*) => ({
        use colored::*;
        eprintln!("{}: {}", "warning".yellow().bold(), format!($($arg)*));
    })
}

impl PrettyError {
    /// Process a `Result` printing any errors and exiting
    /// the process after
    pub fn report<T>(result: Result<T, Error>) -> ! {
        std::process::exit(match result {
            Ok(_t) => 0,
            Err(error) => {
                eprintln!("{:?}", PrettyError { error });
                1
            }
        });
    }
}

impl Debug for PrettyError {
    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
        let error = &self.error;

        if f.alternate() {
            return Debug::fmt(&error, f);
        }

        write!(f, "{}", format!("{}: {}", "error".red(), error).bold())?;
        // write!(f, "{}", error)?;

        if let Some(cause) = error.source() {
            // write!(f, "\n{}:", "caused by".bold().blue())?;
            let chain = Chain::new(cause);
            let (total_errors, _) = chain.size_hint();
            for (n, error) in chain.enumerate() {
                writeln!(f)?;
                let mut indented = Indented {
                    inner: f,
                    number: Some(n + 1),
                    is_last: n == total_errors - 1,
                    started: false,
                };
                write!(indented, "{}", error)?;
            }
        }
        Ok(())
    }
}

struct Indented<'a, D> {
    inner: &'a mut D,
    number: Option<usize>,
    started: bool,
    is_last: bool,
}

impl<T> Write for Indented<'_, T>
where
    T: Write,
{
    fn write_str(&mut self, s: &str) -> fmt::Result {
        for (i, line) in s.split('\n').enumerate() {
            if !self.started {
                self.started = true;
                match self.number {
                    Some(number) => {
                        if !self.is_last {
                            write!(
                                self.inner,
                                "{} {: >4} ",
                                "‚îÇ".bold().blue(),
                                format!("{}:", number).dimmed()
                            )?
                        } else {
                            write!(
                                self.inner,
                                "{}{: >2}: ",
                                "‚ï∞‚îÄ‚ñ∂".bold().blue(),
                                format!("{}", number).bold().blue()
                            )?
                        }
                    }
                    None => self.inner.write_str("    ")?,
                }
            } else if i > 0 {
                self.inner.write_char('\n')?;
                if self.number.is_some() {
                    self.inner.write_str("       ")?;
                } else {
                    self.inner.write_str("    ")?;
                }
            }

            self.inner.write_str(line)?;
        }

        Ok(())
    }
}

'''
'''--- lib/cli/src/lib.rs ---
//! The Wasmer binary lib

#![deny(
    missing_docs,
    dead_code,
    nonstandard_style,
    unused_mut,
    unused_variables,
    unused_unsafe,
    unreachable_patterns,
    unstable_features
)]
#![doc(html_favicon_url = "https://wasmer.io/images/icons/favicon-32x32.png")]
#![doc(html_logo_url = "https://github.com/wasmerio.png?size=200")]

#[macro_use]
extern crate anyhow;

pub mod commands;
pub mod common;
#[macro_use]
pub mod error;
pub mod c_gen;
pub mod cli;
#[cfg(feature = "debug")]
pub mod logging;
pub mod store;
pub mod suggestions;
pub mod utils;

/// Version number of this crate.
pub const VERSION: &str = env!("CARGO_PKG_VERSION");

'''
'''--- lib/cli/src/logging.rs ---
//! Logging functions for the debug feature.
use crate::utils::wasmer_should_print_color;
use anyhow::Result;
use fern::colors::{Color, ColoredLevelConfig};
use std::time;

/// The debug level
pub type DebugLevel = log::LevelFilter;

/// Subroutine to instantiate the loggers
pub fn set_up_logging(verbose: u8) -> Result<(), String> {
    let colors_line = ColoredLevelConfig::new()
        .error(Color::Red)
        .warn(Color::Yellow)
        .trace(Color::BrightBlack);
    let should_color = wasmer_should_print_color();

    let colors_level = colors_line.info(Color::Green);
    let level = match verbose {
        1 => DebugLevel::Debug,
        _ => DebugLevel::Trace,
    };
    let dispatch = fern::Dispatch::new()
        .level(level)
        .chain({
            let base = if should_color {
                fern::Dispatch::new().format(move |out, message, record| {
                    let time = time::SystemTime::now().duration_since(time::UNIX_EPOCH).expect("Can't get time");
                    out.finish(format_args!(
                        "{color_line}[{seconds}.{millis} {level} {target}{color_line}]{ansi_close} {message}",
                        color_line = format_args!(
                            "\x1B[{}m",
                            colors_line.get_color(&record.level()).to_fg_str()
                        ),
                        seconds = time.as_secs(),
                        millis = time.subsec_millis(),
                        level = colors_level.color(record.level()),
                        target = record.target(),
                        ansi_close = "\x1B[0m",
                        message = message,
                    ));
                })
            } else {
                // default formatter without color
                fern::Dispatch::new().format(move |out, message, record| {
                    let time = time::SystemTime::now().duration_since(time::UNIX_EPOCH).expect("Can't get time");
                    out.finish(format_args!(
                        "[{seconds}.{millis} {level} {target}] {message}",
                        seconds = time.as_secs(),
                        millis = time.subsec_millis(),
                        level = record.level(),
                        target = record.target(),
                        message = message,
                    ));
                })
            };

            base
                .filter(|metadata| {
                    metadata.target().starts_with("wasmer")
                })
                .chain(std::io::stdout())
        });

    dispatch.apply().map_err(|e| format!("{}", e))?;

    Ok(())
}

'''
'''--- lib/cli/src/store.rs ---
//! Common module with common used structures across different
//! commands.

use crate::common::WasmFeatures;
use anyhow::Result;
#[cfg(feature = "llvm")]
use std::path::PathBuf;
use std::string::ToString;
#[allow(unused_imports)]
use std::sync::Arc;
use structopt::StructOpt;
use wasmer::*;
#[cfg(feature = "compiler")]
use wasmer_compiler::CompilerConfig;

#[derive(Debug, Clone, StructOpt, Default)]
/// The compiler and engine options
pub struct StoreOptions {
    #[cfg(feature = "compiler")]
    #[structopt(flatten)]
    compiler: CompilerOptions,

    /// Use the Universal Engine.
    #[structopt(long, conflicts_with_all = &["dylib", "staticlib", "jit", "native", "object_file"])]
    universal: bool,

    /// Use the Dylib Engine.
    #[structopt(long, conflicts_with_all = &["universal", "staticlib", "jit", "native", "object_file"])]
    dylib: bool,

    /// Use the Staticlib Engine.
    #[structopt(long, conflicts_with_all = &["universal", "dylib", "jit", "native", "object_file"])]
    staticlib: bool,

    /// Use the JIT (Universal) Engine.
    #[structopt(long, hidden = true, conflicts_with_all = &["universal", "dylib", "staticlib", "native", "object_file"])]
    jit: bool,

    /// Use the Native (Dylib) Engine.
    #[structopt(long, hidden = true, conflicts_with_all = &["universal", "dylib", "staticlib", "jit", "object_file"])]
    native: bool,

    /// Use the ObjectFile (Staticlib) Engine.
    #[structopt(long, hidden = true, conflicts_with_all = &["universal", "dylib", "staticlib", "jit", "native"])]
    object_file: bool,
}

#[cfg(feature = "compiler")]
#[derive(Debug, Clone, StructOpt, Default)]
/// The compiler options
pub struct CompilerOptions {
    /// Use Singlepass compiler.
    #[structopt(long, conflicts_with_all = &["cranelift", "llvm"])]
    singlepass: bool,

    /// Use Cranelift compiler.
    #[structopt(long, conflicts_with_all = &["singlepass", "llvm"])]
    cranelift: bool,

    /// Use LLVM compiler.
    #[structopt(long, conflicts_with_all = &["singlepass", "cranelift"])]
    llvm: bool,

    /// Enable compiler internal verification.
    #[structopt(long)]
    enable_verifier: bool,

    /// LLVM debug directory, where IR and object files will be written to.
    #[cfg(feature = "llvm")]
    #[structopt(long, parse(from_os_str))]
    llvm_debug_dir: Option<PathBuf>,

    #[structopt(flatten)]
    features: WasmFeatures,
}

#[cfg(feature = "compiler")]
impl CompilerOptions {
    // depending on compiler flags some branches may end up the same
    #[allow(clippy::if_same_then_else)]
    fn get_compiler(&self) -> Result<CompilerType> {
        if self.cranelift {
            Ok(CompilerType::Cranelift)
        } else if self.llvm {
            Ok(CompilerType::LLVM)
        } else if self.singlepass {
            Ok(CompilerType::Singlepass)
        } else {
            // Auto mode, we choose the best compiler for that platform
            cfg_if::cfg_if! {
                if #[cfg(all(feature = "cranelift", any(target_arch = "x86_64", target_arch = "aarch64")))] {
                    Ok(CompilerType::Cranelift)
                }
                else if #[cfg(all(feature = "singlepass", target_arch = "x86_64"))] {
                    Ok(CompilerType::Singlepass)
                }
                else if #[cfg(feature = "llvm")] {
                    Ok(CompilerType::LLVM)
                } else {
                    bail!("There are no available compilers for your architecture");
                }
            }
        }
    }

    /// Get the enaled Wasm features.
    pub fn get_features(&self, mut features: Features) -> Result<Features> {
        if self.features.threads || self.features.all {
            features.threads(true);
        }
        if self.features.multi_value || self.features.all {
            features.multi_value(true);
        }
        if self.features.simd || self.features.all {
            features.simd(true);
        }
        if self.features.bulk_memory || self.features.all {
            features.bulk_memory(true);
        }
        if self.features.reference_types || self.features.all {
            features.reference_types(true);
        }
        Ok(features)
    }

    /// Gets the Store for a given target and engine.
    pub fn get_store_for_target_and_engine(
        &self,
        target: Target,
        engine_type: EngineType,
    ) -> Result<(Store, CompilerType)> {
        let (compiler_config, compiler_type) = self.get_compiler_config()?;
        let engine = self.get_engine_by_type(target, compiler_config, engine_type)?;
        let store = Store::new(&*engine);
        Ok((store, compiler_type))
    }

    fn get_engine_by_type(
        &self,
        target: Target,
        compiler_config: Box<dyn CompilerConfig>,
        engine_type: EngineType,
    ) -> Result<Box<dyn Engine + Send + Sync>> {
        let features = self.get_features(compiler_config.default_features_for_target(&target))?;
        let engine: Box<dyn Engine + Send + Sync> = match engine_type {
            #[cfg(feature = "universal")]
            EngineType::Universal => Box::new(
                wasmer_engine_universal::Universal::new(compiler_config)
                    .features(features)
                    .target(target)
                    .engine(),
            ),
            #[cfg(feature = "dylib")]
            EngineType::Dylib => Box::new(
                wasmer_engine_dylib::Dylib::new(compiler_config)
                    .target(target)
                    .features(features)
                    .engine(),
            ),
            #[cfg(feature = "staticlib")]
            EngineType::Staticlib => Box::new(
                wasmer_engine_staticlib::Staticlib::new(compiler_config)
                    .target(target)
                    .features(features)
                    .engine(),
            ),
            #[cfg(not(all(feature = "universal", feature = "dylib", feature = "staticlib")))]
            engine => bail!(
                "The `{}` engine is not included in this binary.",
                engine.to_string()
            ),
        };

        Ok(engine)
    }

    /// Get the Compiler Config for the current options
    #[allow(unused_variables)]
    pub(crate) fn get_compiler_config(&self) -> Result<(Box<dyn CompilerConfig>, CompilerType)> {
        let compiler = self.get_compiler()?;
        let compiler_config: Box<dyn CompilerConfig> = match compiler {
            CompilerType::Headless => bail!("The headless engine can't be chosen"),
            #[cfg(feature = "singlepass")]
            CompilerType::Singlepass => {
                let mut config = wasmer_compiler_singlepass::Singlepass::new();
                if self.enable_verifier {
                    config.enable_verifier();
                }
                Box::new(config)
            }
            #[cfg(feature = "cranelift")]
            CompilerType::Cranelift => {
                let mut config = wasmer_compiler_cranelift::Cranelift::new();
                if self.enable_verifier {
                    config.enable_verifier();
                }
                Box::new(config)
            }
            #[cfg(feature = "llvm")]
            CompilerType::LLVM => {
                use std::fmt;
                use std::fs::File;
                use std::io::Write;
                use wasmer_compiler_llvm::{
                    CompiledKind, InkwellMemoryBuffer, InkwellModule, LLVMCallbacks, LLVM,
                };
                use wasmer_types::entity::EntityRef;
                let mut config = LLVM::new();
                struct Callbacks {
                    debug_dir: PathBuf,
                }
                impl Callbacks {
                    fn new(debug_dir: PathBuf) -> Result<Self> {
                        // Create the debug dir in case it doesn't exist
                        std::fs::create_dir_all(&debug_dir)?;
                        Ok(Self { debug_dir })
                    }
                }
                // Converts a kind into a filename, that we will use to dump
                // the contents of the IR object file to.
                fn types_to_signature(types: &[Type]) -> String {
                    types
                        .iter()
                        .map(|ty| match ty {
                            Type::I32 => "i".to_string(),
                            Type::I64 => "I".to_string(),
                            Type::F32 => "f".to_string(),
                            Type::F64 => "F".to_string(),
                            Type::V128 => "v".to_string(),
                            Type::ExternRef => "e".to_string(),
                            Type::FuncRef => "r".to_string(),
                        })
                        .collect::<Vec<_>>()
                        .join("")
                }
                // Converts a kind into a filename, that we will use to dump
                // the contents of the IR object file to.
                fn function_kind_to_filename(kind: &CompiledKind) -> String {
                    match kind {
                        CompiledKind::Local(local_index) => {
                            format!("function_{}", local_index.index())
                        }
                        CompiledKind::FunctionCallTrampoline(func_type) => format!(
                            "trampoline_call_{}_{}",
                            types_to_signature(&func_type.params()),
                            types_to_signature(&func_type.results())
                        ),
                        CompiledKind::DynamicFunctionTrampoline(func_type) => format!(
                            "trampoline_dynamic_{}_{}",
                            types_to_signature(&func_type.params()),
                            types_to_signature(&func_type.results())
                        ),
                        CompiledKind::Module => "module".into(),
                    }
                }
                impl LLVMCallbacks for Callbacks {
                    fn preopt_ir(&self, kind: &CompiledKind, module: &InkwellModule) {
                        let mut path = self.debug_dir.clone();
                        path.push(format!("{}.preopt.ll", function_kind_to_filename(kind)));
                        module
                            .print_to_file(&path)
                            .expect("Error while dumping pre optimized LLVM IR");
                    }
                    fn postopt_ir(&self, kind: &CompiledKind, module: &InkwellModule) {
                        let mut path = self.debug_dir.clone();
                        path.push(format!("{}.postopt.ll", function_kind_to_filename(kind)));
                        module
                            .print_to_file(&path)
                            .expect("Error while dumping post optimized LLVM IR");
                    }
                    fn obj_memory_buffer(
                        &self,
                        kind: &CompiledKind,
                        memory_buffer: &InkwellMemoryBuffer,
                    ) {
                        let mut path = self.debug_dir.clone();
                        path.push(format!("{}.o", function_kind_to_filename(kind)));
                        let mem_buf_slice = memory_buffer.as_slice();
                        let mut file = File::create(path)
                            .expect("Error while creating debug object file from LLVM IR");
                        let mut pos = 0;
                        while pos < mem_buf_slice.len() {
                            pos += file.write(&mem_buf_slice[pos..]).unwrap();
                        }
                    }
                }

                impl fmt::Debug for Callbacks {
                    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
                        write!(f, "LLVMCallbacks")
                    }
                }

                if let Some(ref llvm_debug_dir) = self.llvm_debug_dir {
                    config.callbacks(Some(Arc::new(Callbacks::new(llvm_debug_dir.clone())?)));
                }
                if self.enable_verifier {
                    config.enable_verifier();
                }
                Box::new(config)
            }
            #[cfg(not(all(feature = "singlepass", feature = "cranelift", feature = "llvm",)))]
            compiler => {
                bail!(
                    "The `{}` compiler is not included in this binary.",
                    compiler.to_string()
                )
            }
        };

        #[allow(unreachable_code)]
        Ok((compiler_config, compiler))
    }
}

/// The compiler used for the store
#[derive(Debug, PartialEq, Eq)]
pub enum CompilerType {
    /// Singlepass compiler
    Singlepass,
    /// Cranelift compiler
    Cranelift,
    /// LLVM compiler
    LLVM,
    /// Headless compiler
    Headless,
}

impl CompilerType {
    /// Return all enabled compilers
    pub fn enabled() -> Vec<CompilerType> {
        vec![
            #[cfg(feature = "singlepass")]
            Self::Singlepass,
            #[cfg(feature = "cranelift")]
            Self::Cranelift,
            #[cfg(feature = "llvm")]
            Self::LLVM,
        ]
    }
}

impl ToString for CompilerType {
    fn to_string(&self) -> String {
        match self {
            Self::Singlepass => "singlepass".to_string(),
            Self::Cranelift => "cranelift".to_string(),
            Self::LLVM => "llvm".to_string(),
            Self::Headless => "headless".to_string(),
        }
    }
}

/// The engine used for the store
#[derive(Debug, PartialEq, Eq, Clone, Copy)]
pub enum EngineType {
    /// Universal Engine
    Universal,
    /// Dylib Engine
    Dylib,
    /// Static Engine
    Staticlib,
}

impl ToString for EngineType {
    fn to_string(&self) -> String {
        match self {
            Self::Universal => "universal".to_string(),
            Self::Dylib => "dylib".to_string(),
            Self::Staticlib => "staticlib".to_string(),
        }
    }
}

#[cfg(all(feature = "compiler", feature = "engine"))]
impl StoreOptions {
    /// Gets the store for the host target, with the engine name and compiler name selected
    pub fn get_store(&self) -> Result<(Store, EngineType, CompilerType)> {
        let target = Target::default();
        self.get_store_for_target(target)
    }

    /// Gets the store for a given target, with the engine name and compiler name selected, as
    pub fn get_store_for_target(
        &self,
        target: Target,
    ) -> Result<(Store, EngineType, CompilerType)> {
        let (compiler_config, compiler_type) = self.compiler.get_compiler_config()?;
        let (engine, engine_type) = self.get_engine_with_compiler(target, compiler_config)?;
        let store = Store::new(&*engine);
        Ok((store, engine_type, compiler_type))
    }

    fn get_engine_with_compiler(
        &self,
        target: Target,
        compiler_config: Box<dyn CompilerConfig>,
    ) -> Result<(Box<dyn Engine + Send + Sync>, EngineType)> {
        let engine_type = self.get_engine()?;
        let engine = self
            .compiler
            .get_engine_by_type(target, compiler_config, engine_type)?;

        Ok((engine, engine_type))
    }
}

#[cfg(feature = "engine")]
impl StoreOptions {
    fn get_engine(&self) -> Result<EngineType> {
        if self.universal || self.jit {
            Ok(EngineType::Universal)
        } else if self.dylib || self.native {
            Ok(EngineType::Dylib)
        } else if self.staticlib || self.object_file {
            Ok(EngineType::Staticlib)
        } else {
            // Auto mode, we choose the best engine for that platform
            if cfg!(feature = "universal") {
                Ok(EngineType::Universal)
            } else if cfg!(feature = "dylib") {
                Ok(EngineType::Dylib)
            } else if cfg!(feature = "staticlib") {
                Ok(EngineType::Staticlib)
            } else {
                bail!("There are no available engines for your architecture")
            }
        }
    }
}

// If we don't have a compiler, but we have an engine
#[cfg(all(not(feature = "compiler"), feature = "engine"))]
impl StoreOptions {
    fn get_engine_headless(&self) -> Result<(Arc<dyn Engine + Send + Sync>, EngineType)> {
        let engine_type = self.get_engine()?;
        let engine: Arc<dyn Engine + Send + Sync> = match engine_type {
            #[cfg(feature = "universal")]
            EngineType::Universal => {
                Arc::new(wasmer_engine_universal::Universal::headless().engine())
            }
            #[cfg(feature = "dylib")]
            EngineType::Dylib => Arc::new(wasmer_engine_dylib::Dylib::headless().engine()),
            #[cfg(feature = "staticlib")]
            EngineType::Staticlib => {
                Arc::new(wasmer_engine_staticlib::Staticlib::headless().engine())
            }
            #[cfg(not(all(feature = "universal", feature = "dylib", feature = "staticlib")))]
            engine => bail!(
                "The `{}` engine is not included in this binary.",
                engine.to_string()
            ),
        };
        Ok((engine, engine_type))
    }

    /// Get the store (headless engine)
    pub fn get_store(&self) -> Result<(Store, EngineType, CompilerType)> {
        let (engine, engine_type) = self.get_engine_headless()?;
        let store = Store::new(&*engine);
        Ok((store, engine_type, CompilerType::Headless))
    }

    /// Gets the store for provided host target
    pub fn get_store_for_target(
        &self,
        _target: Target,
    ) -> Result<(Store, EngineType, CompilerType)> {
        bail!("You need compilers to retrieve a store for a specific target");
    }
}

// If we don't have any engine enabled
#[cfg(not(feature = "engine"))]
impl StoreOptions {
    /// Get the store (headless engine)
    pub fn get_store(&self) -> Result<(Store, EngineType, CompilerType)> {
        bail!("No engines are enabled");
    }

    /// Gets the store for the host target
    pub fn get_store_for_target(
        &self,
        _target: Target,
    ) -> Result<(Store, EngineType, CompilerType)> {
        bail!("No engines are enabled");
    }
}

'''
'''--- lib/cli/src/suggestions.rs ---
//! This file provides suggestions for the user, to help them on the
//! usage of WebAssembly
use distance::damerau_levenshtein;
use wasmer::Module;

/// Suggest function exports for the module
pub fn suggest_function_exports(module: &Module, query: &str) -> Vec<String> {
    let mut function_names = module
        .exports()
        .functions()
        .map(|extern_fn| {
            let name = extern_fn.name();
            name.to_string()
        })
        .collect::<Vec<_>>();
    function_names.sort_by_key(|name| damerau_levenshtein(name, query));
    function_names
}

'''
'''--- lib/cli/src/utils.rs ---
//! Utility functions for the WebAssembly module
use anyhow::{bail, Result};
use std::env;
use std::path::PathBuf;

/// Whether or not Wasmer should print with color
pub fn wasmer_should_print_color() -> bool {
    env::var("WASMER_COLOR")
        .ok()
        .and_then(|inner| inner.parse::<bool>().ok())
        .unwrap_or_else(|| atty::is(atty::Stream::Stdout))
}

fn retrieve_alias_pathbuf(alias: &str, real_dir: &str) -> Result<(String, PathBuf)> {
    let pb = PathBuf::from(&real_dir);
    if let Ok(pb_metadata) = pb.metadata() {
        if !pb_metadata.is_dir() {
            bail!("\"{}\" exists, but it is not a directory", &real_dir);
        }
    } else {
        bail!("Directory \"{}\" does not exist", &real_dir);
    }
    Ok((alias.to_string(), pb))
}

/// Parses a mapdir from a string
pub fn parse_mapdir(entry: &str) -> Result<(String, PathBuf)> {
    // We try first splitting by `::`
    if let [alias, real_dir] = entry.split("::").collect::<Vec<&str>>()[..] {
        retrieve_alias_pathbuf(alias, real_dir)
    }
    // And then we try splitting by `:` (for compatibility with previous API)
    else if let [alias, real_dir] = entry.split(':').collect::<Vec<&str>>()[..] {
        retrieve_alias_pathbuf(alias, real_dir)
    } else {
        bail!(
            "Directory mappings must consist of two paths separate by a `::` or `:`. Found {}",
            &entry
        )
    }
}

/// Parses an environment variable.
pub fn parse_envvar(entry: &str) -> Result<(String, String)> {
    let entry = entry.trim();

    match entry.find('=') {
        None => bail!(
            "Environment variable must be of the form `<name>=<value>`; found `{}`",
            &entry
        ),

        Some(0) => bail!(
            "Environment variable is not well formed, the `name` is missing in `<name>=<value>`; got `{}`",
            &entry
        ),

        Some(position) if position == entry.len() - 1 => bail!(
            "Environment variable is not well formed, the `value` is missing in `<name>=<value>`; got `{}`",
            &entry
        ),

        Some(position) => Ok((entry[..position].into(), entry[position + 1..].into())),
    }
}

#[cfg(test)]
mod tests {
    use super::parse_envvar;

    #[test]
    fn test_parse_envvar() {
        assert_eq!(
            parse_envvar("A").unwrap_err().to_string(),
            "Environment variable must be of the form `<name>=<value>`; found `A`"
        );
        assert_eq!(
            parse_envvar("=A").unwrap_err().to_string(),
            "Environment variable is not well formed, the `name` is missing in `<name>=<value>`; got `=A`"
        );
        assert_eq!(
            parse_envvar("A=").unwrap_err().to_string(),
            "Environment variable is not well formed, the `value` is missing in `<name>=<value>`; got `A=`"
        );
        assert_eq!(parse_envvar("A=B").unwrap(), ("A".into(), "B".into()));
        assert_eq!(parse_envvar("   A=B\t").unwrap(), ("A".into(), "B".into()));
        assert_eq!(
            parse_envvar("A=B=C=D").unwrap(),
            ("A".into(), "B=C=D".into())
        );
    }
}

'''
'''--- lib/compiler-cranelift/Cargo.toml ---
[package]
name = "wasmer-compiler-cranelift"
version = "2.1.0"
description = "Cranelift compiler for Wasmer WebAssembly runtime"
categories = ["wasm"]
keywords = ["wasm", "webassembly", "compiler", "cranelift"]
authors = ["Wasmer Engineering Team <engineering@wasmer.io>"]
repository = "https://github.com/wasmerio/wasmer"
documentation = "https://docs.rs/wasmer-compiler-cranelift/"
license = "MIT OR Apache-2.0 WITH LLVM-exception"
readme = "README.md"
edition = "2018"

[dependencies]
wasmer-compiler = { path = "../compiler", version = "=2.4.1", package = "wasmer-compiler-near", features = ["translator"], default-features = false }
wasmer-vm = { path = "../vm", version = "=2.4.1", package = "wasmer-vm-near" }
wasmer-types = { path = "../types", version = "=2.4.1", package = "wasmer-types-near", default-features = false, features = ["std"] }
cranelift-entity = { version = "0.76", default-features = false }
cranelift-codegen = { version = "0.76", default-features = false, features = ["x86", "arm64"] }
cranelift-frontend = { version = "0.76", default-features = false }
tracing = "0.1"
hashbrown = { version = "0.11", optional = true }
rayon = "1.5"
more-asserts = "0.2"
gimli = { version = "0.25", optional = true }
smallvec = "1.6"
target-lexicon = { version = "0.12.2", default-features = false }

[dev-dependencies]
cranelift-codegen = { version = "0.76", features = ["all-arch"] }
lazy_static = "1.4"

[badges]
maintenance = { status = "actively-developed" }

[features]
default = ["std", "unwind"]
unwind = ["cranelift-codegen/unwind", "gimli"]
std = ["cranelift-codegen/std", "cranelift-frontend/std", "wasmer-compiler/std", "wasmer-types/std"]
core = ["hashbrown", "cranelift-codegen/core", "cranelift-frontend/core"]

'''
'''--- lib/compiler-cranelift/README.md ---
# `wasmer-compiler-cranelift` [![Build Status](https://github.com/wasmerio/wasmer/workflows/build/badge.svg?style=flat-square)](https://github.com/wasmerio/wasmer/actions?query=workflow%3Abuild) [![Join Wasmer Slack](https://img.shields.io/static/v1?label=Slack&message=join%20chat&color=brighgreen&style=flat-square)](https://slack.wasmer.io) [![MIT License](https://img.shields.io/github/license/wasmerio/wasmer.svg?style=flat-square)](https://github.com/wasmerio/wasmer/blob/master/LICENSE) [![crates.io](https://img.shields.io/crates/v/wasmer-compiler-cranelift.svg)](https://crates.io/crates/wasmer-compiler-cranelift)

This crate contains a compiler implementation based on Cranelift.

## Usage

```rust
use wasmer::{Store, Universal};
use wasmer_compiler_cranelift::Cranelift;

let compiler = Cranelift::new();
// Put it into an engine and add it to the store
let store = Store::new(&Universal::new(compiler).engine());
```

*Note: you can find a [full working example using Cranelift compiler
here][example].*

## When to use Cranelift

We recommend using this compiler crate **only for development
proposes**. For production we recommend using [`wasmer-compiler-llvm`]
as it offers a much better runtime speed (50% faster on average).

### Acknowledgments

This project borrowed some of the function lowering from
[`cranelift-wasm`].

Please check [Wasmer `ATTRIBUTIONS`] to further see licenses and other
attributions of the project.

[example]: https://github.com/wasmerio/wasmer/blob/master/examples/compiler_cranelift.rs
[`wasmer-compiler-llvm`]: https://github.com/wasmerio/wasmer/tree/master/lib/compiler-llvm
[`cranelift-wasm`]: https://crates.io/crates/cranelift-wasm
[Wasmer `ATTRIBUTIONS`]: https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md

'''
'''--- lib/compiler-cranelift/build.rs ---
//! Wasmer Cranelift compiler build script.
//!
//! Sets the git revsion? for $PURPOSE
//! TODO(syrus): explain what's happening here

use std::process::Command;
use std::str;

fn main() {
    let git_rev = match Command::new("git").args(&["rev-parse", "HEAD"]).output() {
        Ok(output) => str::from_utf8(&output.stdout).unwrap().trim().to_string(),
        Err(_) => env!("CARGO_PKG_VERSION").to_string(),
    };
    println!("cargo:rustc-env=GIT_REV={}", git_rev);
}

'''
'''--- lib/compiler-cranelift/src/address_map.rs ---
// This file contains code from external sources.
// Attributions: https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md

use cranelift_codegen::MachSrcLoc;
use cranelift_codegen::{isa, Context};
use wasmer_compiler::{wasmparser::Range, FunctionAddressMap, InstructionAddressMap, SourceLoc};

pub fn get_function_address_map<'data>(
    context: &Context,
    range: Range,
    body_len: usize,
    isa: &dyn isa::TargetIsa,
) -> FunctionAddressMap {
    let mut instructions = Vec::new();

    if let Some(ref mcr) = &context.mach_compile_result {
        // New-style backend: we have a `MachCompileResult` that will give us `MachSrcLoc` mapping
        // tuples.
        for &MachSrcLoc { start, end, loc } in mcr.buffer.get_srclocs_sorted() {
            instructions.push(InstructionAddressMap {
                srcloc: SourceLoc::new(loc.bits()),
                code_offset: start as usize,
                code_len: (end - start) as usize,
            });
        }
    } else {
        let func = &context.func;
        let mut blocks = func.layout.blocks().collect::<Vec<_>>();
        blocks.sort_by_key(|block| func.offsets[*block]); // Ensure inst offsets always increase

        let encinfo = isa.encoding_info();
        for block in blocks {
            for (offset, inst, size) in func.inst_offsets(block, &encinfo) {
                let srcloc = func.srclocs[inst];
                instructions.push(InstructionAddressMap {
                    srcloc: SourceLoc::new(srcloc.bits()),
                    code_offset: offset as usize,
                    code_len: size as usize,
                });
            }
        }
    }

    // Generate artificial srcloc for function start/end to identify boundary
    // within module. Similar to FuncTranslator::cur_srcloc(): it will wrap around
    // if byte code is larger than 4 GB.
    let start_srcloc = SourceLoc::new(range.start as u32);
    let end_srcloc = SourceLoc::new(range.end as u32);

    FunctionAddressMap {
        instructions,
        start_srcloc,
        end_srcloc,
        body_offset: 0,
        body_len,
    }
}

'''
'''--- lib/compiler-cranelift/src/compiler.rs ---
//! Support for compiling with Cranelift.

use crate::address_map::get_function_address_map;
use crate::config::Cranelift;
#[cfg(feature = "unwind")]
use crate::dwarf::WriterRelocate;
use crate::func_environ::{get_function_name, FuncEnvironment};
use crate::sink::{RelocSink, TrapSink};
use crate::trampoline::{
    make_trampoline_dynamic_function, make_trampoline_function_call, FunctionBuilderContext,
};
use crate::translator::{
    compiled_function_unwind_info, signature_to_cranelift_ir, transform_jump_table,
    CraneliftUnwindInfo, FuncTranslator,
};
use cranelift_codegen::ir;
use cranelift_codegen::print_errors::pretty_error;
use cranelift_codegen::{binemit, Context};
#[cfg(feature = "unwind")]
use gimli::write::{Address, EhFrame, FrameTable};
use rayon::prelude::{IntoParallelRefIterator, ParallelIterator};
use target_lexicon::{Architecture, OperatingSystem};
use wasmer_compiler::CompileError;
use wasmer_compiler::{CallingConvention, ModuleTranslationState, Target};
use wasmer_compiler::{
    Compilation, CompileModuleInfo, CompiledFunction, CompiledFunctionFrameInfo,
    CompiledFunctionUnwindInfo, Compiler, Dwarf, FunctionBody, FunctionBodyData, SectionIndex,
};
use wasmer_compiler::{
    CustomSection, CustomSectionProtection, Relocation, RelocationKind, RelocationTarget,
    SectionBody,
};
use wasmer_types::entity::{EntityRef, PrimaryMap};
use wasmer_types::{FunctionIndex, LocalFunctionIndex, SignatureIndex};
use wasmer_vm::libcalls::LibCall;

/// A compiler that compiles a WebAssembly module with Cranelift, translating the Wasm to Cranelift IR,
/// optimizing it and then translating to assembly.
pub struct CraneliftCompiler {
    config: Cranelift,
}

impl CraneliftCompiler {
    /// Creates a new Cranelift compiler
    pub fn new(config: Cranelift) -> Self {
        Self { config }
    }

    /// Gets the WebAssembly features for this Compiler
    pub fn config(&self) -> &Cranelift {
        &self.config
    }
}

impl Compiler for CraneliftCompiler {
    /// Compile the module using Cranelift, producing a compilation result with
    /// associated relocations.
    fn compile_module(
        &self,
        target: &Target,
        compile_info: &CompileModuleInfo,
        module_translation_state: &ModuleTranslationState,
        function_body_inputs: PrimaryMap<LocalFunctionIndex, FunctionBodyData<'_>>,
    ) -> Result<Compilation, CompileError> {
        let isa = self.config().isa(target);
        let frontend_config = isa.frontend_config();
        let memory_styles = &compile_info.memory_styles;
        let table_styles = &compile_info.table_styles;
        let module = &compile_info.module;
        let signatures = module
            .signatures
            .iter()
            .map(|(_sig_index, func_type)| signature_to_cranelift_ir(func_type, frontend_config))
            .collect::<PrimaryMap<SignatureIndex, ir::Signature>>();

        // Generate the frametable
        #[cfg(feature = "unwind")]
        let dwarf_frametable = if function_body_inputs.is_empty() {
            // If we have no function body inputs, we don't need to
            // construct the `FrameTable`. Constructing it, with empty
            // FDEs will cause some issues in Linux.
            None
        } else {
            match target.triple().default_calling_convention() {
                Ok(CallingConvention::SystemV) => {
                    match isa.create_systemv_cie() {
                        Some(cie) => {
                            let mut dwarf_frametable = FrameTable::default();
                            let cie_id = dwarf_frametable.add_cie(cie);
                            Some((dwarf_frametable, cie_id))
                        }
                        // Even though we are in a SystemV system, Cranelift doesn't support it
                        None => None,
                    }
                }
                _ => None,
            }
        };

        let mut custom_sections = PrimaryMap::new();

        let probestack_trampoline_relocation_target = if target.triple().operating_system
            == OperatingSystem::Linux
            && target.triple().architecture == Architecture::X86_64
        {
            let probestack_trampoline = CustomSection {
                protection: CustomSectionProtection::ReadExecute,
                // We create a jump to an absolute 64bits address
                // with an indrect jump immediatly followed but the absolute address
                // JMP [IP+0]   FF 25 00 00 00 00
                // 64bits ADDR  00 00 00 00 00 00 00 00 preset to 0 until the relocation takes place
                bytes: SectionBody::new_with_vec(vec![
                    0xff, 0x25, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
                    0x00,
                ]),
                relocations: vec![Relocation {
                    kind: RelocationKind::Abs8,
                    reloc_target: RelocationTarget::LibCall(LibCall::Probestack),
                    // 6 is the size of the jmp instruction. The relocated address must follow
                    offset: 6,
                    addend: 0,
                }],
            };
            custom_sections.push(probestack_trampoline);

            Some(SectionIndex::new(custom_sections.len() - 1))
        } else {
            None
        };

        let (functions, fdes): (Vec<CompiledFunction>, Vec<_>) = function_body_inputs
            .iter()
            .collect::<Vec<(LocalFunctionIndex, &FunctionBodyData<'_>)>>()
            .par_iter()
            .map_init(FuncTranslator::new, |func_translator, (i, input)| {
                let func_index = module.func_index(*i);
                let mut context = Context::new();
                let mut func_env = FuncEnvironment::new(
                    isa.frontend_config(),
                    module,
                    &signatures,
                    &memory_styles,
                    &table_styles,
                );
                context.func.name = get_function_name(func_index);
                context.func.signature = signatures[module.functions[func_index]].clone();
                // if generate_debug_info {
                //     context.func.collect_debug_info();
                // }
                let mut reader =
                    wasmer_compiler::FunctionReader::new(input.module_offset, input.data);
                func_translator.translate(
                    module_translation_state,
                    &mut reader,
                    &mut context.func,
                    &mut func_env,
                    *i,
                )?;

                let mut code_buf: Vec<u8> = Vec::new();
                let mut reloc_sink =
                    RelocSink::new(&module, func_index, probestack_trampoline_relocation_target);
                let mut trap_sink = TrapSink::new();
                let mut stackmap_sink = binemit::NullStackMapSink {};
                context
                    .compile_and_emit(
                        &*isa,
                        &mut code_buf,
                        &mut reloc_sink,
                        &mut trap_sink,
                        &mut stackmap_sink,
                    )
                    .map_err(|error| {
                        CompileError::Codegen(pretty_error(&context.func, Some(&*isa), error))
                    })?;

                let (unwind_info, fde) = match compiled_function_unwind_info(&*isa, &context)? {
                    #[cfg(feature = "unwind")]
                    CraneliftUnwindInfo::FDE(fde) => {
                        if dwarf_frametable.is_some() {
                            let fde = fde.to_fde(Address::Symbol {
                                // The symbol is the kind of relocation.
                                // "0" is used for functions
                                symbol: WriterRelocate::FUNCTION_SYMBOL,
                                // We use the addend as a way to specify the
                                // function index
                                addend: i.index() as _,
                            });
                            // The unwind information is inserted into the dwarf section
                            (Some(CompiledFunctionUnwindInfo::Dwarf), Some(fde))
                        } else {
                            (None, None)
                        }
                    }
                    #[cfg(feature = "unwind")]
                    other => (other.maybe_into_to_windows_unwind(), None),

                    // This is a bit hacky, but necessary since gimli is not
                    // available when the "unwind" feature is disabled.
                    #[cfg(not(feature = "unwind"))]
                    other => (other.maybe_into_to_windows_unwind(), None::<()>),
                };

                let range = reader.range();
                let address_map = get_function_address_map(&context, range, code_buf.len(), &*isa);

                // We transform the Cranelift JumpTable's into compiler JumpTables
                let func_jt_offsets = transform_jump_table(context.func.jt_offsets);

                Ok((
                    CompiledFunction {
                        body: FunctionBody {
                            body: code_buf,
                            unwind_info,
                        },
                        jt_offsets: func_jt_offsets,
                        relocations: reloc_sink.func_relocs,
                        frame_info: CompiledFunctionFrameInfo {
                            address_map,
                            traps: trap_sink.traps,
                        },
                    },
                    fde,
                ))
            })
            .collect::<Result<Vec<_>, CompileError>>()?
            .into_iter()
            .unzip();

        #[cfg(feature = "unwind")]
        let dwarf = if let Some((mut dwarf_frametable, cie_id)) = dwarf_frametable {
            for fde in fdes {
                if let Some(fde) = fde {
                    dwarf_frametable.add_fde(cie_id, fde);
                }
            }
            let mut eh_frame = EhFrame(WriterRelocate::new(target.triple().endianness().ok()));
            dwarf_frametable.write_eh_frame(&mut eh_frame).unwrap();

            let eh_frame_section = eh_frame.0.into_section();
            custom_sections.push(eh_frame_section);
            Some(Dwarf::new(SectionIndex::new(custom_sections.len() - 1)))
        } else {
            None
        };
        #[cfg(not(feature = "unwind"))]
        let dwarf = None;

        // function call trampolines (only for local functions, by signature)
        let function_call_trampolines = module
            .signatures
            .values()
            .collect::<Vec<_>>()
            .par_iter()
            .map_init(FunctionBuilderContext::new, |mut cx, sig| {
                make_trampoline_function_call(&*isa, &mut cx, sig)
            })
            .collect::<Result<Vec<FunctionBody>, CompileError>>()?
            .into_iter()
            .collect::<PrimaryMap<SignatureIndex, FunctionBody>>();

        use wasmer_vm::VMOffsets;
        let offsets = VMOffsets::new(frontend_config.pointer_bytes());
        // dynamic function trampolines (only for imported functions)
        let dynamic_function_trampolines = module
            .imported_function_types()
            .collect::<Vec<_>>()
            .par_iter()
            .map_init(FunctionBuilderContext::new, |mut cx, func_type| {
                make_trampoline_dynamic_function(&*isa, &offsets, &mut cx, &func_type)
            })
            .collect::<Result<Vec<_>, CompileError>>()?
            .into_iter()
            .collect::<PrimaryMap<FunctionIndex, FunctionBody>>();

        Ok(Compilation::new(
            functions.into_iter().collect(),
            custom_sections,
            function_call_trampolines,
            dynamic_function_trampolines,
            dwarf,
            None,
        ))
    }
}

'''
'''--- lib/compiler-cranelift/src/config.rs ---
use crate::compiler::CraneliftCompiler;
use cranelift_codegen::isa::{lookup, TargetIsa};
use cranelift_codegen::settings::{self, Configurable};
use wasmer_compiler::{Architecture, Compiler, CompilerConfig, CpuFeature, Target};

// Runtime Environment

/// Possible optimization levels for the Cranelift codegen backend.
#[non_exhaustive]
#[derive(Clone, Debug)]
pub enum CraneliftOptLevel {
    /// No optimizations performed, minimizes compilation time by disabling most
    /// optimizations.
    None,
    /// Generates the fastest possible code, but may take longer.
    Speed,
    /// Similar to `speed`, but also performs transformations aimed at reducing
    /// code size.
    SpeedAndSize,
}

/// Global configuration options used to create an
/// `wasmer_engine::Engine` and customize its behavior.
///
/// This structure exposes a builder-like interface and is primarily
/// consumed by `wasmer_engine::Engine::new`.
#[derive(Debug, Clone)]
pub struct Cranelift {
    enable_nan_canonicalization: bool,
    enable_verifier: bool,
    enable_pic: bool,
    opt_level: CraneliftOptLevel,
}

impl Cranelift {
    /// Creates a new configuration object with the default configuration
    /// specified.
    pub fn new() -> Self {
        Self {
            enable_nan_canonicalization: false,
            enable_verifier: false,
            opt_level: CraneliftOptLevel::Speed,
            enable_pic: false,
        }
    }

    /// Enable NaN canonicalization.
    ///
    /// NaN canonicalization is useful when trying to run WebAssembly
    /// deterministically across different architectures.
    pub fn canonicalize_nans(&mut self, enable: bool) -> &mut Self {
        self.enable_nan_canonicalization = enable;
        self
    }

    /// The optimization levels when optimizing the IR.
    pub fn opt_level(&mut self, opt_level: CraneliftOptLevel) -> &mut Self {
        self.opt_level = opt_level;
        self
    }

    /// Generates the ISA for the provided target
    pub fn isa(&self, target: &Target) -> Box<dyn TargetIsa> {
        let mut builder =
            lookup(target.triple().clone()).expect("construct Cranelift ISA for triple");
        // Cpu Features
        let cpu_features = target.cpu_features();
        if target.triple().architecture == Architecture::X86_64
            && !cpu_features.contains(CpuFeature::SSE2)
        {
            panic!("x86 support requires SSE2");
        }
        if cpu_features.contains(CpuFeature::SSE3) {
            builder.enable("has_sse3").expect("should be valid flag");
        }
        if cpu_features.contains(CpuFeature::SSSE3) {
            builder.enable("has_ssse3").expect("should be valid flag");
        }
        if cpu_features.contains(CpuFeature::SSE41) {
            builder.enable("has_sse41").expect("should be valid flag");
        }
        if cpu_features.contains(CpuFeature::SSE42) {
            builder.enable("has_sse42").expect("should be valid flag");
        }
        if cpu_features.contains(CpuFeature::POPCNT) {
            builder.enable("has_popcnt").expect("should be valid flag");
        }
        if cpu_features.contains(CpuFeature::AVX) {
            builder.enable("has_avx").expect("should be valid flag");
        }
        if cpu_features.contains(CpuFeature::BMI1) {
            builder.enable("has_bmi1").expect("should be valid flag");
        }
        if cpu_features.contains(CpuFeature::BMI2) {
            builder.enable("has_bmi2").expect("should be valid flag");
        }
        if cpu_features.contains(CpuFeature::AVX2) {
            builder.enable("has_avx2").expect("should be valid flag");
        }
        if cpu_features.contains(CpuFeature::AVX512DQ) {
            builder
                .enable("has_avx512dq")
                .expect("should be valid flag");
        }
        if cpu_features.contains(CpuFeature::AVX512VL) {
            builder
                .enable("has_avx512vl")
                .expect("should be valid flag");
        }
        if cpu_features.contains(CpuFeature::LZCNT) {
            builder.enable("has_lzcnt").expect("should be valid flag");
        }

        builder.finish(self.flags())
    }

    /// Generates the flags for the compiler
    pub fn flags(&self) -> settings::Flags {
        let mut flags = settings::builder();

        // There are two possible traps for division, and this way
        // we get the proper one if code traps.
        flags
            .enable("avoid_div_traps")
            .expect("should be valid flag");

        if self.enable_pic {
            flags.enable("is_pic").expect("should be a valid flag");
        }

        // Invert cranelift's default-on verification to instead default off.
        let enable_verifier = if self.enable_verifier {
            "true"
        } else {
            "false"
        };
        flags
            .set("enable_verifier", enable_verifier)
            .expect("should be valid flag");
        flags
            .set("enable_safepoints", "true")
            .expect("should be valid flag");

        flags
            .set(
                "opt_level",
                match self.opt_level {
                    CraneliftOptLevel::None => "none",
                    CraneliftOptLevel::Speed => "speed",
                    CraneliftOptLevel::SpeedAndSize => "speed_and_size",
                },
            )
            .expect("should be valid flag");

        flags
            .set("enable_simd", "true")
            .expect("should be valid flag");

        let enable_nan_canonicalization = if self.enable_nan_canonicalization {
            "true"
        } else {
            "false"
        };
        flags
            .set("enable_nan_canonicalization", enable_nan_canonicalization)
            .expect("should be valid flag");

        settings::Flags::new(flags)
    }
}

impl CompilerConfig for Cranelift {
    fn enable_pic(&mut self) {
        self.enable_pic = true;
    }

    fn enable_verifier(&mut self) {
        self.enable_verifier = true;
    }

    fn enable_nan_canonicalization(&mut self) {
        self.enable_nan_canonicalization = true;
    }

    fn canonicalize_nans(&mut self, enable: bool) {
        self.enable_nan_canonicalization = enable;
    }

    /// Transform it into the compiler
    fn compiler(self: Box<Self>) -> Box<dyn Compiler> {
        Box::new(CraneliftCompiler::new(*self))
    }
}

impl Default for Cranelift {
    fn default() -> Self {
        Self::new()
    }
}

'''
'''--- lib/compiler-cranelift/src/debug/address_map.rs ---
//! Data structures to provide transformation of the source
// addresses of a WebAssembly module into the native code.

use cranelift_codegen::ir;
use wasmer_types::entity::PrimaryMap;
use wasmer_types::LocalFunctionIndex;

/// Value ranges for functions.
pub type ValueLabelsRanges = PrimaryMap<LocalFunctionIndex, cranelift_codegen::ValueLabelsRanges>;

/// Stack slots for functions.
pub type StackSlots = PrimaryMap<LocalFunctionIndex, ir::StackSlots>;

/// Memory definition offset in the VMContext structure.
#[derive(Debug, Clone)]
pub enum ModuleInfoMemoryOffset {
    /// Not available.
    None,
    /// Offset to the defined memory.
    Defined(u32),
    /// Offset to the imported memory.
    Imported(u32),
}

/// ModuleInfo `vmctx` related info.
#[derive(Debug, Clone)]
pub struct ModuleInfoVmctxInfo {
    /// The memory definition offset in the VMContext structure.
    pub memory_offset: ModuleInfoMemoryOffset,

    /// The functions stack slots.
    pub stack_slots: StackSlots,
}

'''
'''--- lib/compiler-cranelift/src/debug/mod.rs ---
mod address_map;

pub use self::address_map::{ModuleInfoMemoryOffset, ModuleInfoVmctxInfo, ValueLabelsRanges};

'''
'''--- lib/compiler-cranelift/src/dwarf.rs ---
use gimli::write::{Address, EndianVec, Result, Writer};
use gimli::{RunTimeEndian, SectionId};
use wasmer_compiler::{CustomSection, CustomSectionProtection, SectionBody};
use wasmer_compiler::{Endianness, Relocation, RelocationKind, RelocationTarget};
use wasmer_types::entity::EntityRef;
use wasmer_types::LocalFunctionIndex;

#[derive(Clone, Debug)]
pub struct WriterRelocate {
    pub relocs: Vec<Relocation>,
    writer: EndianVec<RunTimeEndian>,
}

impl WriterRelocate {
    pub const FUNCTION_SYMBOL: usize = 0;
    pub fn new(endianness: Option<Endianness>) -> Self {
        let endianness = match endianness {
            Some(Endianness::Little) => RunTimeEndian::Little,
            Some(Endianness::Big) => RunTimeEndian::Big,
            // We autodetect it, based on the host
            None => RunTimeEndian::default(),
        };
        WriterRelocate {
            relocs: Vec::new(),
            writer: EndianVec::new(endianness),
        }
    }

    pub fn into_section(mut self) -> CustomSection {
        // GCC expects a terminating "empty" length, so write a 0 length at the end of the table.
        self.writer.write_u32(0).unwrap();
        let data = self.writer.into_vec();
        CustomSection {
            protection: CustomSectionProtection::Read,
            bytes: SectionBody::new_with_vec(data),
            relocations: self.relocs,
        }
    }
}

impl Writer for WriterRelocate {
    type Endian = RunTimeEndian;

    fn endian(&self) -> Self::Endian {
        self.writer.endian()
    }

    fn len(&self) -> usize {
        self.writer.len()
    }

    fn write(&mut self, bytes: &[u8]) -> Result<()> {
        self.writer.write(bytes)
    }

    fn write_at(&mut self, offset: usize, bytes: &[u8]) -> Result<()> {
        self.writer.write_at(offset, bytes)
    }

    fn write_address(&mut self, address: Address, size: u8) -> Result<()> {
        match address {
            Address::Constant(val) => self.write_udata(val, size),
            Address::Symbol { symbol, addend } => {
                // Is a function relocation
                if symbol == Self::FUNCTION_SYMBOL {
                    // We use the addend to detect the function index
                    let function_index = LocalFunctionIndex::new(addend as _);
                    let reloc_target = RelocationTarget::LocalFunc(function_index);
                    let offset = self.len() as u32;
                    let kind = match size {
                        8 => RelocationKind::Abs8,
                        _ => unimplemented!("dwarf relocation size not yet supported: {}", size),
                    };
                    let addend = 0;
                    self.relocs.push(Relocation {
                        kind,
                        reloc_target,
                        offset,
                        addend,
                    });
                    self.write_udata(addend as u64, size)
                } else {
                    unreachable!("Symbol {} in DWARF not recognized", symbol);
                }
            }
        }
    }

    fn write_offset(&mut self, _val: usize, _section: SectionId, _size: u8) -> Result<()> {
        unimplemented!("write_offset not yet implemented");
    }

    fn write_offset_at(
        &mut self,
        _offset: usize,
        _val: usize,
        _section: SectionId,
        _size: u8,
    ) -> Result<()> {
        unimplemented!("write_offset_at not yet implemented");
    }
}

'''
'''--- lib/compiler-cranelift/src/func_environ.rs ---
// This file contains code from external sources.
// Attributions: https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md

use crate::translator::{
    type_to_irtype, FuncEnvironment as BaseFuncEnvironment, GlobalVariable, TargetEnvironment,
};
use cranelift_codegen::cursor::FuncCursor;
use cranelift_codegen::ir;
use cranelift_codegen::ir::condcodes::*;
use cranelift_codegen::ir::immediates::{Offset32, Uimm64};
use cranelift_codegen::ir::types::*;
use cranelift_codegen::ir::{AbiParam, ArgumentPurpose, Function, InstBuilder, Signature};
use cranelift_codegen::isa::TargetFrontendConfig;
use cranelift_frontend::{FunctionBuilder, Variable};
use std::convert::TryFrom;
use wasmer_compiler::wasmparser::Type;
use wasmer_compiler::{WasmError, WasmResult};
use wasmer_types::entity::EntityRef;
use wasmer_types::entity::PrimaryMap;
use wasmer_types::{
    FunctionIndex, FunctionType, GlobalIndex, LocalFunctionIndex, MemoryIndex, ModuleInfo,
    SignatureIndex, TableIndex, Type as WasmerType,
};
use wasmer_vm::VMBuiltinFunctionIndex;
use wasmer_vm::VMOffsets;
use wasmer_vm::{MemoryStyle, TableStyle};

/// Compute an `ir::ExternalName` for a given wasm function index.
pub fn get_function_name(func_index: FunctionIndex) -> ir::ExternalName {
    ir::ExternalName::user(0, func_index.as_u32())
}

/// The type of the `current_elements` field.
pub fn type_of_vmtable_definition_current_elements(vmoffsets: &VMOffsets) -> ir::Type {
    ir::Type::int(u16::from(vmoffsets.size_of_vmtable_definition_current_elements()) * 8).unwrap()
}

/// The `FuncEnvironment` implementation for use by the `ModuleEnvironment`.
pub struct FuncEnvironment<'module_environment> {
    /// Target-specified configuration.
    target_config: TargetFrontendConfig,

    /// The module-level environment which this function-level environment belongs to.
    module: &'module_environment ModuleInfo,

    /// A stack tracking the type of local variables.
    type_stack: Vec<WasmerType>,

    /// The module function signatures
    signatures: &'module_environment PrimaryMap<SignatureIndex, ir::Signature>,

    /// The Cranelift global holding the vmctx address.
    vmctx: Option<ir::GlobalValue>,

    /// The external function signature for implementing wasm's `memory.size`
    /// for locally-defined 32-bit memories.
    memory32_size_sig: Option<ir::SigRef>,

    /// The external function signature for implementing wasm's `table.size`
    /// for locally-defined tables.
    table_size_sig: Option<ir::SigRef>,

    /// The external function signature for implementing wasm's `memory.grow`
    /// for locally-defined memories.
    memory_grow_sig: Option<ir::SigRef>,

    /// The external function signature for implementing wasm's `table.grow`
    /// for locally-defined tables.
    table_grow_sig: Option<ir::SigRef>,

    /// The external function signature for implementing wasm's `table.copy`
    /// (it's the same for both local and imported tables).
    table_copy_sig: Option<ir::SigRef>,

    /// The external function signature for implementing wasm's `table.init`.
    table_init_sig: Option<ir::SigRef>,

    /// The external function signature for implementing wasm's `elem.drop`.
    elem_drop_sig: Option<ir::SigRef>,

    /// The external function signature for implementing wasm's `memory.copy`
    /// (it's the same for both local and imported memories).
    memory_copy_sig: Option<ir::SigRef>,

    /// The external function signature for implementing wasm's `memory.fill`
    /// (it's the same for both local and imported memories).
    memory_fill_sig: Option<ir::SigRef>,

    /// The external function signature for implementing wasm's `memory.init`.
    memory_init_sig: Option<ir::SigRef>,

    /// The external function signature for implementing wasm's `data.drop`.
    data_drop_sig: Option<ir::SigRef>,

    /// The external function signature for implementing wasm's `table.get`.
    table_get_sig: Option<ir::SigRef>,

    /// The external function signature for implementing wasm's `table.set`.
    table_set_sig: Option<ir::SigRef>,

    /// The external function signature for implementing wasm's `func.ref`.
    func_ref_sig: Option<ir::SigRef>,

    /// The external function signature for implementing wasm's `table.fill`.
    table_fill_sig: Option<ir::SigRef>,

    /// The external function signature for implementing reference increment for `extern.ref`.
    externref_inc_sig: Option<ir::SigRef>,

    /// The external function signature for implementing reference decrement for `extern.ref`.
    externref_dec_sig: Option<ir::SigRef>,
    /// Offsets to struct fields accessed by JIT code.
    offsets: VMOffsets,

    /// The memory styles
    memory_styles: &'module_environment PrimaryMap<MemoryIndex, MemoryStyle>,

    /// The table styles
    table_styles: &'module_environment PrimaryMap<TableIndex, TableStyle>,
}

impl<'module_environment> FuncEnvironment<'module_environment> {
    pub fn new(
        target_config: TargetFrontendConfig,
        module: &'module_environment ModuleInfo,
        signatures: &'module_environment PrimaryMap<SignatureIndex, ir::Signature>,
        memory_styles: &'module_environment PrimaryMap<MemoryIndex, MemoryStyle>,
        table_styles: &'module_environment PrimaryMap<TableIndex, TableStyle>,
    ) -> Self {
        Self {
            target_config,
            module,
            signatures,
            type_stack: vec![],
            vmctx: None,
            memory32_size_sig: None,
            table_size_sig: None,
            memory_grow_sig: None,
            table_grow_sig: None,
            table_copy_sig: None,
            table_init_sig: None,
            elem_drop_sig: None,
            memory_copy_sig: None,
            memory_fill_sig: None,
            memory_init_sig: None,
            table_get_sig: None,
            table_set_sig: None,
            data_drop_sig: None,
            func_ref_sig: None,
            table_fill_sig: None,
            externref_inc_sig: None,
            externref_dec_sig: None,
            offsets: VMOffsets::new(target_config.pointer_bytes()).with_module_info(module),
            memory_styles,
            table_styles,
        }
    }

    fn pointer_type(&self) -> ir::Type {
        self.target_config.pointer_type()
    }

    fn vmctx(&mut self, func: &mut Function) -> ir::GlobalValue {
        self.vmctx.unwrap_or_else(|| {
            let vmctx = func.create_global_value(ir::GlobalValueData::VMContext);
            self.vmctx = Some(vmctx);
            vmctx
        })
    }

    fn get_table_fill_sig(&mut self, func: &mut Function) -> ir::SigRef {
        let sig = self.table_fill_sig.unwrap_or_else(|| {
            func.import_signature(Signature {
                params: vec![
                    AbiParam::special(self.pointer_type(), ArgumentPurpose::VMContext),
                    // table index
                    AbiParam::new(I32),
                    // dst
                    AbiParam::new(I32),
                    // value
                    AbiParam::new(R64),
                    // len
                    AbiParam::new(I32),
                ],
                returns: vec![],
                call_conv: self.target_config.default_call_conv,
            })
        });
        self.table_fill_sig = Some(sig);
        sig
    }

    fn get_table_fill_func(
        &mut self,
        func: &mut Function,
        table_index: TableIndex,
    ) -> (ir::SigRef, usize, VMBuiltinFunctionIndex) {
        (
            self.get_table_fill_sig(func),
            table_index.index(),
            VMBuiltinFunctionIndex::get_table_fill_index(),
        )
    }

    fn get_externref_inc_sig(&mut self, func: &mut Function) -> ir::SigRef {
        let sig = self.externref_inc_sig.unwrap_or_else(|| {
            func.import_signature(Signature {
                params: vec![AbiParam::new(R64)],
                returns: vec![],
                call_conv: self.target_config.default_call_conv,
            })
        });
        self.externref_inc_sig = Some(sig);
        sig
    }

    fn get_externref_inc_func(
        &mut self,
        func: &mut Function,
    ) -> (ir::SigRef, VMBuiltinFunctionIndex) {
        (
            self.get_externref_inc_sig(func),
            VMBuiltinFunctionIndex::get_externref_inc_index(),
        )
    }

    fn get_externref_dec_sig(&mut self, func: &mut Function) -> ir::SigRef {
        let sig = self.externref_dec_sig.unwrap_or_else(|| {
            func.import_signature(Signature {
                params: vec![AbiParam::new(R64)],
                returns: vec![],
                call_conv: self.target_config.default_call_conv,
            })
        });
        self.externref_dec_sig = Some(sig);
        sig
    }

    fn get_externref_dec_func(
        &mut self,
        func: &mut Function,
    ) -> (ir::SigRef, VMBuiltinFunctionIndex) {
        (
            self.get_externref_dec_sig(func),
            VMBuiltinFunctionIndex::get_externref_dec_index(),
        )
    }

    fn get_func_ref_sig(&mut self, func: &mut Function) -> ir::SigRef {
        let sig = self.func_ref_sig.unwrap_or_else(|| {
            func.import_signature(Signature {
                params: vec![
                    AbiParam::special(self.pointer_type(), ArgumentPurpose::VMContext),
                    AbiParam::new(I32),
                ],
                returns: vec![AbiParam::new(R64)],
                call_conv: self.target_config.default_call_conv,
            })
        });
        self.func_ref_sig = Some(sig);
        sig
    }

    fn get_func_ref_func(
        &mut self,
        func: &mut Function,
        function_index: FunctionIndex,
    ) -> (ir::SigRef, usize, VMBuiltinFunctionIndex) {
        (
            self.get_func_ref_sig(func),
            function_index.index(),
            VMBuiltinFunctionIndex::get_func_ref_index(),
        )
    }

    fn get_table_get_sig(&mut self, func: &mut Function) -> ir::SigRef {
        let sig = self.table_get_sig.unwrap_or_else(|| {
            func.import_signature(Signature {
                params: vec![
                    AbiParam::special(self.pointer_type(), ArgumentPurpose::VMContext),
                    AbiParam::new(I32),
                    AbiParam::new(I32),
                ],
                returns: vec![AbiParam::new(R64)],
                call_conv: self.target_config.default_call_conv,
            })
        });
        self.table_get_sig = Some(sig);
        sig
    }

    fn get_table_get_func(
        &mut self,
        func: &mut Function,
        table_index: TableIndex,
    ) -> (ir::SigRef, usize, VMBuiltinFunctionIndex) {
        if self.module.is_imported_table(table_index) {
            (
                self.get_table_get_sig(func),
                table_index.index(),
                VMBuiltinFunctionIndex::get_imported_table_get_index(),
            )
        } else {
            (
                self.get_table_get_sig(func),
                self.module.local_table_index(table_index).unwrap().index(),
                VMBuiltinFunctionIndex::get_table_get_index(),
            )
        }
    }

    fn get_table_set_sig(&mut self, func: &mut Function) -> ir::SigRef {
        let sig = self.table_set_sig.unwrap_or_else(|| {
            func.import_signature(Signature {
                params: vec![
                    AbiParam::special(self.pointer_type(), ArgumentPurpose::VMContext),
                    AbiParam::new(I32),
                    AbiParam::new(I32),
                    AbiParam::new(R64),
                ],
                returns: vec![],
                call_conv: self.target_config.default_call_conv,
            })
        });
        self.table_set_sig = Some(sig);
        sig
    }

    fn get_table_set_func(
        &mut self,
        func: &mut Function,
        table_index: TableIndex,
    ) -> (ir::SigRef, usize, VMBuiltinFunctionIndex) {
        if self.module.is_imported_table(table_index) {
            (
                self.get_table_set_sig(func),
                table_index.index(),
                VMBuiltinFunctionIndex::get_imported_table_set_index(),
            )
        } else {
            (
                self.get_table_set_sig(func),
                self.module.local_table_index(table_index).unwrap().index(),
                VMBuiltinFunctionIndex::get_table_set_index(),
            )
        }
    }

    fn get_table_grow_sig(&mut self, func: &mut Function) -> ir::SigRef {
        let sig = self.table_grow_sig.unwrap_or_else(|| {
            func.import_signature(Signature {
                params: vec![
                    AbiParam::special(self.pointer_type(), ArgumentPurpose::VMContext),
                    // TODO: figure out what the representation of a Wasm value is
                    AbiParam::new(R64),
                    AbiParam::new(I32),
                    AbiParam::new(I32),
                ],
                returns: vec![AbiParam::new(I32)],
                call_conv: self.target_config.default_call_conv,
            })
        });
        self.table_grow_sig = Some(sig);
        sig
    }

    /// Return the table.grow function signature to call for the given index, along with the
    /// translated index value to pass to it and its index in `VMBuiltinFunctionsArray`.
    fn get_table_grow_func(
        &mut self,
        func: &mut Function,
        index: TableIndex,
    ) -> (ir::SigRef, usize, VMBuiltinFunctionIndex) {
        if self.module.is_imported_table(index) {
            (
                self.get_table_grow_sig(func),
                index.index(),
                VMBuiltinFunctionIndex::get_imported_table_grow_index(),
            )
        } else {
            (
                self.get_table_grow_sig(func),
                self.module.local_table_index(index).unwrap().index(),
                VMBuiltinFunctionIndex::get_table_grow_index(),
            )
        }
    }

    fn get_memory_grow_sig(&mut self, func: &mut Function) -> ir::SigRef {
        let sig = self.memory_grow_sig.unwrap_or_else(|| {
            func.import_signature(Signature {
                params: vec![
                    AbiParam::special(self.pointer_type(), ArgumentPurpose::VMContext),
                    AbiParam::new(I32),
                    AbiParam::new(I32),
                ],
                returns: vec![AbiParam::new(I32)],
                call_conv: self.target_config.default_call_conv,
            })
        });
        self.memory_grow_sig = Some(sig);
        sig
    }

    /// Return the memory.grow function signature to call for the given index, along with the
    /// translated index value to pass to it and its index in `VMBuiltinFunctionsArray`.
    fn get_memory_grow_func(
        &mut self,
        func: &mut Function,
        index: MemoryIndex,
    ) -> (ir::SigRef, usize, VMBuiltinFunctionIndex) {
        if self.module.is_imported_memory(index) {
            (
                self.get_memory_grow_sig(func),
                index.index(),
                VMBuiltinFunctionIndex::get_imported_memory32_grow_index(),
            )
        } else {
            (
                self.get_memory_grow_sig(func),
                self.module.local_memory_index(index).unwrap().index(),
                VMBuiltinFunctionIndex::get_memory32_grow_index(),
            )
        }
    }

    fn get_table_size_sig(&mut self, func: &mut Function) -> ir::SigRef {
        let sig = self.table_size_sig.unwrap_or_else(|| {
            func.import_signature(Signature {
                params: vec![
                    AbiParam::special(self.pointer_type(), ArgumentPurpose::VMContext),
                    AbiParam::new(I32),
                ],
                returns: vec![AbiParam::new(I32)],
                call_conv: self.target_config.default_call_conv,
            })
        });
        self.table_size_sig = Some(sig);
        sig
    }

    /// Return the memory.size function signature to call for the given index, along with the
    /// translated index value to pass to it and its index in `VMBuiltinFunctionsArray`.
    fn get_table_size_func(
        &mut self,
        func: &mut Function,
        index: TableIndex,
    ) -> (ir::SigRef, usize, VMBuiltinFunctionIndex) {
        if self.module.is_imported_table(index) {
            (
                self.get_table_size_sig(func),
                index.index(),
                VMBuiltinFunctionIndex::get_imported_table_size_index(),
            )
        } else {
            (
                self.get_table_size_sig(func),
                self.module.local_table_index(index).unwrap().index(),
                VMBuiltinFunctionIndex::get_table_size_index(),
            )
        }
    }

    fn get_memory32_size_sig(&mut self, func: &mut Function) -> ir::SigRef {
        let sig = self.memory32_size_sig.unwrap_or_else(|| {
            func.import_signature(Signature {
                params: vec![
                    AbiParam::special(self.pointer_type(), ArgumentPurpose::VMContext),
                    AbiParam::new(I32),
                ],
                returns: vec![AbiParam::new(I32)],
                call_conv: self.target_config.default_call_conv,
            })
        });
        self.memory32_size_sig = Some(sig);
        sig
    }

    /// Return the memory.size function signature to call for the given index, along with the
    /// translated index value to pass to it and its index in `VMBuiltinFunctionsArray`.
    fn get_memory_size_func(
        &mut self,
        func: &mut Function,
        index: MemoryIndex,
    ) -> (ir::SigRef, usize, VMBuiltinFunctionIndex) {
        if self.module.is_imported_memory(index) {
            (
                self.get_memory32_size_sig(func),
                index.index(),
                VMBuiltinFunctionIndex::get_imported_memory32_size_index(),
            )
        } else {
            (
                self.get_memory32_size_sig(func),
                self.module.local_memory_index(index).unwrap().index(),
                VMBuiltinFunctionIndex::get_memory32_size_index(),
            )
        }
    }

    fn get_table_copy_sig(&mut self, func: &mut Function) -> ir::SigRef {
        let sig = self.table_copy_sig.unwrap_or_else(|| {
            func.import_signature(Signature {
                params: vec![
                    AbiParam::special(self.pointer_type(), ArgumentPurpose::VMContext),
                    // Destination table index.
                    AbiParam::new(I32),
                    // Source table index.
                    AbiParam::new(I32),
                    // Index within destination table.
                    AbiParam::new(I32),
                    // Index within source table.
                    AbiParam::new(I32),
                    // Number of elements to copy.
                    AbiParam::new(I32),
                ],
                returns: vec![],
                call_conv: self.target_config.default_call_conv,
            })
        });
        self.table_copy_sig = Some(sig);
        sig
    }

    fn get_table_copy_func(
        &mut self,
        func: &mut Function,
        dst_table_index: TableIndex,
        src_table_index: TableIndex,
    ) -> (ir::SigRef, usize, usize, VMBuiltinFunctionIndex) {
        let sig = self.get_table_copy_sig(func);
        (
            sig,
            dst_table_index.as_u32() as usize,
            src_table_index.as_u32() as usize,
            VMBuiltinFunctionIndex::get_table_copy_index(),
        )
    }

    fn get_table_init_sig(&mut self, func: &mut Function) -> ir::SigRef {
        let sig = self.table_init_sig.unwrap_or_else(|| {
            func.import_signature(Signature {
                params: vec![
                    AbiParam::special(self.pointer_type(), ArgumentPurpose::VMContext),
                    // Table index.
                    AbiParam::new(I32),
                    // Segment index.
                    AbiParam::new(I32),
                    // Destination index within table.
                    AbiParam::new(I32),
                    // Source index within segment.
                    AbiParam::new(I32),
                    // Number of elements to initialize.
                    AbiParam::new(I32),
                ],
                returns: vec![],
                call_conv: self.target_config.default_call_conv,
            })
        });
        self.table_init_sig = Some(sig);
        sig
    }

    fn get_table_init_func(
        &mut self,
        func: &mut Function,
        table_index: TableIndex,
    ) -> (ir::SigRef, usize, VMBuiltinFunctionIndex) {
        let sig = self.get_table_init_sig(func);
        let table_index = table_index.as_u32() as usize;
        (
            sig,
            table_index,
            VMBuiltinFunctionIndex::get_table_init_index(),
        )
    }

    fn get_elem_drop_sig(&mut self, func: &mut Function) -> ir::SigRef {
        let sig = self.elem_drop_sig.unwrap_or_else(|| {
            func.import_signature(Signature {
                params: vec![
                    AbiParam::special(self.pointer_type(), ArgumentPurpose::VMContext),
                    // Element index.
                    AbiParam::new(I32),
                ],
                returns: vec![],
                call_conv: self.target_config.default_call_conv,
            })
        });
        self.elem_drop_sig = Some(sig);
        sig
    }

    fn get_elem_drop_func(&mut self, func: &mut Function) -> (ir::SigRef, VMBuiltinFunctionIndex) {
        let sig = self.get_elem_drop_sig(func);
        (sig, VMBuiltinFunctionIndex::get_elem_drop_index())
    }

    fn get_memory_copy_sig(&mut self, func: &mut Function) -> ir::SigRef {
        let sig = self.memory_copy_sig.unwrap_or_else(|| {
            func.import_signature(Signature {
                params: vec![
                    AbiParam::special(self.pointer_type(), ArgumentPurpose::VMContext),
                    // Memory index.
                    AbiParam::new(I32),
                    // Destination address.
                    AbiParam::new(I32),
                    // Source address.
                    AbiParam::new(I32),
                    // Length.
                    AbiParam::new(I32),
                ],
                returns: vec![],
                call_conv: self.target_config.default_call_conv,
            })
        });
        self.memory_copy_sig = Some(sig);
        sig
    }

    fn get_memory_copy_func(
        &mut self,
        func: &mut Function,
        memory_index: MemoryIndex,
    ) -> (ir::SigRef, usize, VMBuiltinFunctionIndex) {
        let sig = self.get_memory_copy_sig(func);
        if let Some(local_memory_index) = self.module.local_memory_index(memory_index) {
            (
                sig,
                local_memory_index.index(),
                VMBuiltinFunctionIndex::get_memory_copy_index(),
            )
        } else {
            (
                sig,
                memory_index.index(),
                VMBuiltinFunctionIndex::get_imported_memory_copy_index(),
            )
        }
    }

    fn get_memory_fill_sig(&mut self, func: &mut Function) -> ir::SigRef {
        let sig = self.memory_fill_sig.unwrap_or_else(|| {
            func.import_signature(Signature {
                params: vec![
                    AbiParam::special(self.pointer_type(), ArgumentPurpose::VMContext),
                    // Memory index.
                    AbiParam::new(I32),
                    // Destination address.
                    AbiParam::new(I32),
                    // Value.
                    AbiParam::new(I32),
                    // Length.
                    AbiParam::new(I32),
                ],
                returns: vec![],
                call_conv: self.target_config.default_call_conv,
            })
        });
        self.memory_fill_sig = Some(sig);
        sig
    }

    fn get_memory_fill_func(
        &mut self,
        func: &mut Function,
        memory_index: MemoryIndex,
    ) -> (ir::SigRef, usize, VMBuiltinFunctionIndex) {
        let sig = self.get_memory_fill_sig(func);
        if let Some(local_memory_index) = self.module.local_memory_index(memory_index) {
            (
                sig,
                local_memory_index.index(),
                VMBuiltinFunctionIndex::get_memory_fill_index(),
            )
        } else {
            (
                sig,
                memory_index.index(),
                VMBuiltinFunctionIndex::get_imported_memory_fill_index(),
            )
        }
    }

    fn get_memory_init_sig(&mut self, func: &mut Function) -> ir::SigRef {
        let sig = self.memory_init_sig.unwrap_or_else(|| {
            func.import_signature(Signature {
                params: vec![
                    AbiParam::special(self.pointer_type(), ArgumentPurpose::VMContext),
                    // Memory index.
                    AbiParam::new(I32),
                    // Data index.
                    AbiParam::new(I32),
                    // Destination address.
                    AbiParam::new(I32),
                    // Source index within the data segment.
                    AbiParam::new(I32),
                    // Length.
                    AbiParam::new(I32),
                ],
                returns: vec![],
                call_conv: self.target_config.default_call_conv,
            })
        });
        self.memory_init_sig = Some(sig);
        sig
    }

    fn get_memory_init_func(
        &mut self,
        func: &mut Function,
    ) -> (ir::SigRef, VMBuiltinFunctionIndex) {
        let sig = self.get_memory_init_sig(func);
        (sig, VMBuiltinFunctionIndex::get_memory_init_index())
    }

    fn get_data_drop_sig(&mut self, func: &mut Function) -> ir::SigRef {
        let sig = self.data_drop_sig.unwrap_or_else(|| {
            func.import_signature(Signature {
                params: vec![
                    AbiParam::special(self.pointer_type(), ArgumentPurpose::VMContext),
                    // Data index.
                    AbiParam::new(I32),
                ],
                returns: vec![],
                call_conv: self.target_config.default_call_conv,
            })
        });
        self.data_drop_sig = Some(sig);
        sig
    }

    fn get_data_drop_func(&mut self, func: &mut Function) -> (ir::SigRef, VMBuiltinFunctionIndex) {
        let sig = self.get_data_drop_sig(func);
        (sig, VMBuiltinFunctionIndex::get_data_drop_index())
    }

    /// Translates load of builtin function and returns a pair of values `vmctx`
    /// and address of the loaded function.
    fn translate_load_builtin_function_address(
        &mut self,
        pos: &mut FuncCursor<'_>,
        callee_func_idx: VMBuiltinFunctionIndex,
    ) -> (ir::Value, ir::Value) {
        // We use an indirect call so that we don't have to patch the code at runtime.
        let pointer_type = self.pointer_type();
        let vmctx = self.vmctx(&mut pos.func);
        let base = pos.ins().global_value(pointer_type, vmctx);

        let mut mem_flags = ir::MemFlags::trusted();
        mem_flags.set_readonly();

        // Load the callee address.
        let body_offset =
            i32::try_from(self.offsets.vmctx_builtin_function(callee_func_idx)).unwrap();
        let func_addr = pos.ins().load(pointer_type, mem_flags, base, body_offset);

        (base, func_addr)
    }
}

impl<'module_environment> TargetEnvironment for FuncEnvironment<'module_environment> {
    fn target_config(&self) -> TargetFrontendConfig {
        self.target_config
    }
}

impl<'module_environment> BaseFuncEnvironment for FuncEnvironment<'module_environment> {
    fn is_wasm_parameter(&self, _signature: &ir::Signature, index: usize) -> bool {
        // The first parameter is the vmctx. The rest are the wasm parameters.
        index >= 1
    }

    fn make_table(&mut self, func: &mut ir::Function, index: TableIndex) -> WasmResult<ir::Table> {
        let pointer_type = self.pointer_type();

        let (ptr, base_offset, current_elements_offset) = {
            let vmctx = self.vmctx(func);
            if let Some(def_index) = self.module.local_table_index(index) {
                let base_offset =
                    i32::try_from(self.offsets.vmctx_vmtable_definition_base(def_index)).unwrap();
                let current_elements_offset = i32::try_from(
                    self.offsets
                        .vmctx_vmtable_definition_current_elements(def_index),
                )
                .unwrap();
                (vmctx, base_offset, current_elements_offset)
            } else {
                let from_offset = self.offsets.vmctx_vmtable_import_definition(index);
                let table = func.create_global_value(ir::GlobalValueData::Load {
                    base: vmctx,
                    offset: Offset32::new(i32::try_from(from_offset).unwrap()),
                    global_type: pointer_type,
                    readonly: true,
                });
                let base_offset = i32::from(self.offsets.vmtable_definition_base());
                let current_elements_offset =
                    i32::from(self.offsets.vmtable_definition_current_elements());
                (table, base_offset, current_elements_offset)
            }
        };

        let base_gv = func.create_global_value(ir::GlobalValueData::Load {
            base: ptr,
            offset: Offset32::new(base_offset),
            global_type: pointer_type,
            readonly: false,
        });
        let bound_gv = func.create_global_value(ir::GlobalValueData::Load {
            base: ptr,
            offset: Offset32::new(current_elements_offset),
            global_type: type_of_vmtable_definition_current_elements(&self.offsets),
            readonly: false,
        });

        let element_size = match self.table_styles[index] {
            TableStyle::CallerChecksSignature => u64::from(self.offsets.size_of_vm_funcref()),
        };

        Ok(func.create_table(ir::TableData {
            base_gv,
            min_size: Uimm64::new(0),
            bound_gv,
            element_size: Uimm64::new(element_size),
            index_type: I32,
        }))
    }

    fn translate_table_grow(
        &mut self,
        mut pos: cranelift_codegen::cursor::FuncCursor<'_>,
        table_index: TableIndex,
        _table: ir::Table,
        delta: ir::Value,
        init_value: ir::Value,
    ) -> WasmResult<ir::Value> {
        let (func_sig, index_arg, func_idx) = self.get_table_grow_func(&mut pos.func, table_index);
        let table_index = pos.ins().iconst(I32, index_arg as i64);
        let (vmctx, func_addr) = self.translate_load_builtin_function_address(&mut pos, func_idx);
        let call_inst = pos.ins().call_indirect(
            func_sig,
            func_addr,
            &[vmctx, init_value, delta, table_index],
        );
        Ok(*pos.func.dfg.inst_results(call_inst).first().unwrap())
    }

    fn translate_table_get(
        &mut self,
        builder: &mut FunctionBuilder,
        table_index: TableIndex,
        _table: ir::Table,
        index: ir::Value,
    ) -> WasmResult<ir::Value> {
        let mut pos = builder.cursor();

        let (func_sig, table_index_arg, func_idx) =
            self.get_table_get_func(&mut pos.func, table_index);
        let table_index = pos.ins().iconst(I32, table_index_arg as i64);
        let (vmctx, func_addr) = self.translate_load_builtin_function_address(&mut pos, func_idx);
        let call_inst = pos
            .ins()
            .call_indirect(func_sig, func_addr, &[vmctx, table_index, index]);
        Ok(*pos.func.dfg.inst_results(call_inst).first().unwrap())
    }

    fn translate_table_set(
        &mut self,
        builder: &mut FunctionBuilder,
        table_index: TableIndex,
        _table: ir::Table,
        value: ir::Value,
        index: ir::Value,
    ) -> WasmResult<()> {
        let mut pos = builder.cursor();

        let (func_sig, table_index_arg, func_idx) =
            self.get_table_set_func(&mut pos.func, table_index);
        let table_index = pos.ins().iconst(I32, table_index_arg as i64);
        let (vmctx, func_addr) = self.translate_load_builtin_function_address(&mut pos, func_idx);
        pos.ins()
            .call_indirect(func_sig, func_addr, &[vmctx, table_index, index, value]);
        Ok(())
    }

    fn translate_table_fill(
        &mut self,
        mut pos: cranelift_codegen::cursor::FuncCursor<'_>,
        table_index: TableIndex,
        dst: ir::Value,
        val: ir::Value,
        len: ir::Value,
    ) -> WasmResult<()> {
        let (func_sig, table_index_arg, func_idx) =
            self.get_table_fill_func(&mut pos.func, table_index);
        let (vmctx, func_addr) = self.translate_load_builtin_function_address(&mut pos, func_idx);

        let table_index_arg = pos.ins().iconst(I32, table_index_arg as i64);
        pos.ins().call_indirect(
            func_sig,
            func_addr,
            &[vmctx, table_index_arg, dst, val, len],
        );

        Ok(())
    }

    fn translate_externref_inc(
        &mut self,
        mut pos: cranelift_codegen::cursor::FuncCursor<'_>,
        externref: ir::Value,
    ) -> WasmResult<()> {
        let (func_sig, func_idx) = self.get_externref_inc_func(&mut pos.func);
        let (_vmctx, func_addr) = self.translate_load_builtin_function_address(&mut pos, func_idx);

        pos.ins().call_indirect(func_sig, func_addr, &[externref]);

        Ok(())
    }

    fn translate_externref_dec(
        &mut self,
        mut pos: cranelift_codegen::cursor::FuncCursor<'_>,
        externref: ir::Value,
    ) -> WasmResult<()> {
        let (func_sig, func_idx) = self.get_externref_dec_func(&mut pos.func);
        let (_vmctx, func_addr) = self.translate_load_builtin_function_address(&mut pos, func_idx);

        pos.ins().call_indirect(func_sig, func_addr, &[externref]);

        Ok(())
    }

    fn translate_ref_null(
        &mut self,
        mut pos: cranelift_codegen::cursor::FuncCursor,
        ty: Type,
    ) -> WasmResult<ir::Value> {
        Ok(match ty {
            Type::FuncRef => pos.ins().null(self.reference_type()),
            Type::ExternRef => pos.ins().null(self.reference_type()),
            _ => {
                return Err(WasmError::Unsupported(
                    "`ref.null T` that is not a `funcref` or an `externref`".into(),
                ));
            }
        })
    }

    fn translate_ref_is_null(
        &mut self,
        mut pos: cranelift_codegen::cursor::FuncCursor,
        value: ir::Value,
    ) -> WasmResult<ir::Value> {
        let bool_is_null = match pos.func.dfg.value_type(value) {
            // `externref`
            ty if ty.is_ref() => pos.ins().is_null(value),
            // `funcref`
            ty if ty == self.pointer_type() => {
                pos.ins()
                    .icmp_imm(cranelift_codegen::ir::condcodes::IntCC::Equal, value, 0)
            }
            _ => unreachable!(),
        };

        Ok(pos.ins().bint(ir::types::I32, bool_is_null))
    }

    fn translate_ref_func(
        &mut self,
        mut pos: cranelift_codegen::cursor::FuncCursor<'_>,
        func_index: FunctionIndex,
    ) -> WasmResult<ir::Value> {
        // TODO: optimize this by storing a pointer to local func_index funcref metadata
        // so that local funcref is just (*global + offset) instead of a function call
        //
        // Actually we can do the above for both local and imported functions because
        // all of those are known statically.
        //
        // prototyping with a function call though

        let (func_sig, func_index_arg, func_idx) =
            self.get_func_ref_func(&mut pos.func, func_index);
        let (vmctx, func_addr) = self.translate_load_builtin_function_address(&mut pos, func_idx);

        let func_index_arg = pos.ins().iconst(I32, func_index_arg as i64);
        let call_inst = pos
            .ins()
            .call_indirect(func_sig, func_addr, &[vmctx, func_index_arg]);

        Ok(*pos.func.dfg.inst_results(call_inst).first().unwrap())
    }

    fn translate_custom_global_get(
        &mut self,
        mut _pos: cranelift_codegen::cursor::FuncCursor<'_>,
        _index: GlobalIndex,
    ) -> WasmResult<ir::Value> {
        unreachable!("we don't make any custom globals")
    }

    fn translate_custom_global_set(
        &mut self,
        mut _pos: cranelift_codegen::cursor::FuncCursor<'_>,
        _index: GlobalIndex,
        _value: ir::Value,
    ) -> WasmResult<()> {
        unreachable!("we don't make any custom globals")
    }

    fn make_heap(&mut self, func: &mut ir::Function, index: MemoryIndex) -> WasmResult<ir::Heap> {
        let pointer_type = self.pointer_type();

        let (ptr, base_offset, current_length_offset) = {
            let vmctx = self.vmctx(func);
            if let Some(def_index) = self.module.local_memory_index(index) {
                let base_offset =
                    i32::try_from(self.offsets.vmctx_vmmemory_definition_base(def_index)).unwrap();
                let current_length_offset = i32::try_from(
                    self.offsets
                        .vmctx_vmmemory_definition_current_length(def_index),
                )
                .unwrap();
                (vmctx, base_offset, current_length_offset)
            } else {
                let from_offset = self.offsets.vmctx_vmmemory_import_definition(index);
                let memory = func.create_global_value(ir::GlobalValueData::Load {
                    base: vmctx,
                    offset: Offset32::new(i32::try_from(from_offset).unwrap()),
                    global_type: pointer_type,
                    readonly: true,
                });
                let base_offset = i32::from(self.offsets.vmmemory_definition_base());
                let current_length_offset =
                    i32::from(self.offsets.vmmemory_definition_current_length());
                (memory, base_offset, current_length_offset)
            }
        };

        // If we have a declared maximum, we can make this a "static" heap, which is
        // allocated up front and never moved.
        let (offset_guard_size, heap_style, readonly_base) = match self.memory_styles[index] {
            MemoryStyle::Dynamic { offset_guard_size } => {
                let heap_bound = func.create_global_value(ir::GlobalValueData::Load {
                    base: ptr,
                    offset: Offset32::new(current_length_offset),
                    global_type: pointer_type,
                    readonly: false,
                });
                (
                    Uimm64::new(offset_guard_size),
                    ir::HeapStyle::Dynamic {
                        bound_gv: heap_bound,
                    },
                    false,
                )
            }
            MemoryStyle::Static {
                bound,
                offset_guard_size,
            } => (
                Uimm64::new(offset_guard_size),
                ir::HeapStyle::Static {
                    bound: Uimm64::new(bound.bytes().0 as u64),
                },
                true,
            ),
        };

        let heap_base = func.create_global_value(ir::GlobalValueData::Load {
            base: ptr,
            offset: Offset32::new(base_offset),
            global_type: pointer_type,
            readonly: readonly_base,
        });
        Ok(func.create_heap(ir::HeapData {
            base: heap_base,
            min_size: 0.into(),
            offset_guard_size,
            style: heap_style,
            index_type: I32,
        }))
    }

    fn make_global(
        &mut self,
        func: &mut ir::Function,
        index: GlobalIndex,
    ) -> WasmResult<GlobalVariable> {
        let pointer_type = self.pointer_type();

        let (ptr, offset) = {
            let vmctx = self.vmctx(func);
            let from_offset = if let Some(def_index) = self.module.local_global_index(index) {
                self.offsets.vmctx_vmglobal_definition(def_index)
            } else {
                self.offsets.vmctx_vmglobal_import_definition(index)
            };
            let global = func.create_global_value(ir::GlobalValueData::Load {
                base: vmctx,
                offset: Offset32::new(i32::try_from(from_offset).unwrap()),
                global_type: pointer_type,
                readonly: true,
            });

            (global, 0)
        };

        Ok(GlobalVariable::Memory {
            gv: ptr,
            offset: offset.into(),
            ty: type_to_irtype(self.module.globals[index].ty, self.target_config())?,
        })
    }

    fn make_indirect_sig(
        &mut self,
        func: &mut ir::Function,
        index: SignatureIndex,
    ) -> WasmResult<ir::SigRef> {
        Ok(func.import_signature(self.signatures[index].clone()))
    }

    fn make_direct_func(
        &mut self,
        func: &mut ir::Function,
        index: FunctionIndex,
    ) -> WasmResult<ir::FuncRef> {
        let sigidx = self.module.functions[index];
        let signature = func.import_signature(self.signatures[sigidx].clone());
        let name = get_function_name(index);
        Ok(func.import_function(ir::ExtFuncData {
            name,
            signature,
            // We currently allocate all code segments independently, so nothing
            // is colocated.
            colocated: false,
        }))
    }

    fn translate_call_indirect(
        &mut self,
        mut pos: FuncCursor<'_>,
        table_index: TableIndex,
        table: ir::Table,
        sig_index: SignatureIndex,
        sig_ref: ir::SigRef,
        callee: ir::Value,
        call_args: &[ir::Value],
    ) -> WasmResult<ir::Inst> {
        let pointer_type = self.pointer_type();

        let table_entry_addr = pos.ins().table_addr(pointer_type, table, callee, 0);

        // Dereference table_entry_addr to get the function address.
        let mem_flags = ir::MemFlags::trusted();
        let table_entry_addr = pos.ins().load(
            pointer_type,
            mem_flags,
            table_entry_addr,
            i32::from(self.offsets.vm_funcref_anyfunc_ptr()),
        );

        // check if the funcref is null
        pos.ins()
            .trapz(table_entry_addr, ir::TrapCode::IndirectCallToNull);

        let func_addr = pos.ins().load(
            pointer_type,
            mem_flags,
            table_entry_addr,
            i32::from(self.offsets.vmcaller_checked_anyfunc_func_ptr()),
        );

        // If necessary, check the signature.
        match self.table_styles[table_index] {
            TableStyle::CallerChecksSignature => {
                let sig_id_size = self.offsets.size_of_vmshared_signature_index();
                let sig_id_type = ir::Type::int(u16::from(sig_id_size) * 8).unwrap();
                let vmctx = self.vmctx(pos.func);
                let base = pos.ins().global_value(pointer_type, vmctx);
                let offset =
                    i32::try_from(self.offsets.vmctx_vmshared_signature_id(sig_index)).unwrap();

                // Load the caller ID.
                let mut mem_flags = ir::MemFlags::trusted();
                mem_flags.set_readonly();
                let caller_sig_id = pos.ins().load(sig_id_type, mem_flags, base, offset);

                // Load the callee ID.
                let mem_flags = ir::MemFlags::trusted();
                let callee_sig_id = pos.ins().load(
                    sig_id_type,
                    mem_flags,
                    table_entry_addr,
                    i32::from(self.offsets.vmcaller_checked_anyfunc_type_index()),
                );

                // Check that they match.
                let cmp = pos.ins().icmp(IntCC::Equal, callee_sig_id, caller_sig_id);
                pos.ins().trapz(cmp, ir::TrapCode::BadSignature);
            }
        }

        let mut real_call_args = Vec::with_capacity(call_args.len() + 2);

        // First append the callee vmctx address.
        let vmctx = pos.ins().load(
            pointer_type,
            mem_flags,
            table_entry_addr,
            i32::from(self.offsets.vmcaller_checked_anyfunc_vmctx()),
        );
        real_call_args.push(vmctx);

        // Then append the regular call arguments.
        real_call_args.extend_from_slice(call_args);

        Ok(pos.ins().call_indirect(sig_ref, func_addr, &real_call_args))
    }

    fn translate_call(
        &mut self,
        mut pos: FuncCursor<'_>,
        callee_index: FunctionIndex,
        callee: ir::FuncRef,
        call_args: &[ir::Value],
    ) -> WasmResult<ir::Inst> {
        let mut real_call_args = Vec::with_capacity(call_args.len() + 2);

        // Handle direct calls to locally-defined functions.
        if !self.module.is_imported_function(callee_index) {
            // Let's get the caller vmctx
            let caller_vmctx = pos.func.special_param(ArgumentPurpose::VMContext).unwrap();
            // First append the callee vmctx address, which is the same as the caller vmctx in
            // this case.
            real_call_args.push(caller_vmctx);

            // Then append the regular call arguments.
            real_call_args.extend_from_slice(call_args);

            return Ok(pos.ins().call(callee, &real_call_args));
        }

        // Handle direct calls to imported functions. We use an indirect call
        // so that we don't have to patch the code at runtime.
        let pointer_type = self.pointer_type();
        let sig_ref = pos.func.dfg.ext_funcs[callee].signature;
        let vmctx = self.vmctx(&mut pos.func);
        let base = pos.ins().global_value(pointer_type, vmctx);

        let mem_flags = ir::MemFlags::trusted();

        // Load the callee address.
        let body_offset =
            i32::try_from(self.offsets.vmctx_vmfunction_import_body(callee_index)).unwrap();
        let func_addr = pos.ins().load(pointer_type, mem_flags, base, body_offset);

        // First append the callee vmctx address.
        let vmctx_offset =
            i32::try_from(self.offsets.vmctx_vmfunction_import_vmctx(callee_index)).unwrap();
        let vmctx = pos.ins().load(pointer_type, mem_flags, base, vmctx_offset);
        real_call_args.push(vmctx);

        // Then append the regular call arguments.
        real_call_args.extend_from_slice(call_args);

        Ok(pos.ins().call_indirect(sig_ref, func_addr, &real_call_args))
    }

    fn translate_memory_grow(
        &mut self,
        mut pos: FuncCursor<'_>,
        index: MemoryIndex,
        _heap: ir::Heap,
        val: ir::Value,
    ) -> WasmResult<ir::Value> {
        let (func_sig, index_arg, func_idx) = self.get_memory_grow_func(&mut pos.func, index);
        let memory_index = pos.ins().iconst(I32, index_arg as i64);
        let (vmctx, func_addr) = self.translate_load_builtin_function_address(&mut pos, func_idx);
        let call_inst = pos
            .ins()
            .call_indirect(func_sig, func_addr, &[vmctx, val, memory_index]);
        Ok(*pos.func.dfg.inst_results(call_inst).first().unwrap())
    }

    fn translate_memory_size(
        &mut self,
        mut pos: FuncCursor<'_>,
        index: MemoryIndex,
        _heap: ir::Heap,
    ) -> WasmResult<ir::Value> {
        let (func_sig, index_arg, func_idx) = self.get_memory_size_func(&mut pos.func, index);
        let memory_index = pos.ins().iconst(I32, index_arg as i64);
        let (vmctx, func_addr) = self.translate_load_builtin_function_address(&mut pos, func_idx);
        let call_inst = pos
            .ins()
            .call_indirect(func_sig, func_addr, &[vmctx, memory_index]);
        Ok(*pos.func.dfg.inst_results(call_inst).first().unwrap())
    }

    fn translate_memory_copy(
        &mut self,
        mut pos: FuncCursor,
        src_index: MemoryIndex,
        _src_heap: ir::Heap,
        _dst_index: MemoryIndex,
        _dst_heap: ir::Heap,
        dst: ir::Value,
        src: ir::Value,
        len: ir::Value,
    ) -> WasmResult<()> {
        let (func_sig, src_index, func_idx) = self.get_memory_copy_func(&mut pos.func, src_index);

        let src_index_arg = pos.ins().iconst(I32, src_index as i64);

        let (vmctx, func_addr) = self.translate_load_builtin_function_address(&mut pos, func_idx);

        pos.ins()
            .call_indirect(func_sig, func_addr, &[vmctx, src_index_arg, dst, src, len]);

        Ok(())
    }

    fn translate_memory_fill(
        &mut self,
        mut pos: FuncCursor,
        memory_index: MemoryIndex,
        _heap: ir::Heap,
        dst: ir::Value,
        val: ir::Value,
        len: ir::Value,
    ) -> WasmResult<()> {
        let (func_sig, memory_index, func_idx) =
            self.get_memory_fill_func(&mut pos.func, memory_index);

        let memory_index_arg = pos.ins().iconst(I32, memory_index as i64);

        let (vmctx, func_addr) = self.translate_load_builtin_function_address(&mut pos, func_idx);

        pos.ins().call_indirect(
            func_sig,
            func_addr,
            &[vmctx, memory_index_arg, dst, val, len],
        );

        Ok(())
    }

    fn translate_memory_init(
        &mut self,
        mut pos: FuncCursor,
        memory_index: MemoryIndex,
        _heap: ir::Heap,
        seg_index: u32,
        dst: ir::Value,
        src: ir::Value,
        len: ir::Value,
    ) -> WasmResult<()> {
        let (func_sig, func_idx) = self.get_memory_init_func(&mut pos.func);

        let memory_index_arg = pos.ins().iconst(I32, memory_index.index() as i64);
        let seg_index_arg = pos.ins().iconst(I32, seg_index as i64);

        let (vmctx, func_addr) = self.translate_load_builtin_function_address(&mut pos, func_idx);

        pos.ins().call_indirect(
            func_sig,
            func_addr,
            &[vmctx, memory_index_arg, seg_index_arg, dst, src, len],
        );

        Ok(())
    }

    fn translate_data_drop(&mut self, mut pos: FuncCursor, seg_index: u32) -> WasmResult<()> {
        let (func_sig, func_idx) = self.get_data_drop_func(&mut pos.func);
        let seg_index_arg = pos.ins().iconst(I32, seg_index as i64);
        let (vmctx, func_addr) = self.translate_load_builtin_function_address(&mut pos, func_idx);
        pos.ins()
            .call_indirect(func_sig, func_addr, &[vmctx, seg_index_arg]);
        Ok(())
    }

    fn translate_table_size(
        &mut self,
        mut pos: FuncCursor,
        table_index: TableIndex,
        _table: ir::Table,
    ) -> WasmResult<ir::Value> {
        let (func_sig, index_arg, func_idx) = self.get_table_size_func(&mut pos.func, table_index);
        let table_index = pos.ins().iconst(I32, index_arg as i64);
        let (vmctx, func_addr) = self.translate_load_builtin_function_address(&mut pos, func_idx);
        let call_inst = pos
            .ins()
            .call_indirect(func_sig, func_addr, &[vmctx, table_index]);
        Ok(*pos.func.dfg.inst_results(call_inst).first().unwrap())
    }

    fn translate_table_copy(
        &mut self,
        mut pos: FuncCursor,
        dst_table_index: TableIndex,
        _dst_table: ir::Table,
        src_table_index: TableIndex,
        _src_table: ir::Table,
        dst: ir::Value,
        src: ir::Value,
        len: ir::Value,
    ) -> WasmResult<()> {
        let (func_sig, dst_table_index_arg, src_table_index_arg, func_idx) =
            self.get_table_copy_func(&mut pos.func, dst_table_index, src_table_index);

        let dst_table_index_arg = pos.ins().iconst(I32, dst_table_index_arg as i64);
        let src_table_index_arg = pos.ins().iconst(I32, src_table_index_arg as i64);

        let (vmctx, func_addr) = self.translate_load_builtin_function_address(&mut pos, func_idx);

        pos.ins().call_indirect(
            func_sig,
            func_addr,
            &[
                vmctx,
                dst_table_index_arg,
                src_table_index_arg,
                dst,
                src,
                len,
            ],
        );

        Ok(())
    }

    fn translate_table_init(
        &mut self,
        mut pos: FuncCursor,
        seg_index: u32,
        table_index: TableIndex,
        _table: ir::Table,
        dst: ir::Value,
        src: ir::Value,
        len: ir::Value,
    ) -> WasmResult<()> {
        let (func_sig, table_index_arg, func_idx) =
            self.get_table_init_func(&mut pos.func, table_index);

        let table_index_arg = pos.ins().iconst(I32, table_index_arg as i64);
        let seg_index_arg = pos.ins().iconst(I32, seg_index as i64);

        let (vmctx, func_addr) = self.translate_load_builtin_function_address(&mut pos, func_idx);

        pos.ins().call_indirect(
            func_sig,
            func_addr,
            &[vmctx, table_index_arg, seg_index_arg, dst, src, len],
        );

        Ok(())
    }

    fn translate_elem_drop(&mut self, mut pos: FuncCursor, elem_index: u32) -> WasmResult<()> {
        let (func_sig, func_idx) = self.get_elem_drop_func(&mut pos.func);

        let elem_index_arg = pos.ins().iconst(I32, elem_index as i64);

        let (vmctx, func_addr) = self.translate_load_builtin_function_address(&mut pos, func_idx);

        pos.ins()
            .call_indirect(func_sig, func_addr, &[vmctx, elem_index_arg]);

        Ok(())
    }

    fn translate_atomic_wait(
        &mut self,
        _pos: FuncCursor,
        _index: MemoryIndex,
        _heap: ir::Heap,
        _addr: ir::Value,
        _expected: ir::Value,
        _timeout: ir::Value,
    ) -> WasmResult<ir::Value> {
        Err(WasmError::Unsupported(
            "wasm atomics (fn translate_atomic_wait)".to_string(),
        ))
    }

    fn translate_atomic_notify(
        &mut self,
        _pos: FuncCursor,
        _index: MemoryIndex,
        _heap: ir::Heap,
        _addr: ir::Value,
        _count: ir::Value,
    ) -> WasmResult<ir::Value> {
        Err(WasmError::Unsupported(
            "wasm atomics (fn translate_atomic_notify)".to_string(),
        ))
    }

    fn get_global_type(&self, global_index: GlobalIndex) -> Option<WasmerType> {
        Some(self.module.globals.get(global_index)?.ty)
    }

    fn push_local_decl_on_stack(&mut self, ty: WasmerType) {
        self.type_stack.push(ty);
    }

    fn push_params_on_stack(&mut self, function_index: LocalFunctionIndex) {
        let func_index = self.module.func_index(function_index);
        let sig_idx = self.module.functions[func_index];
        let signature = &self.module.signatures[sig_idx];
        for param in signature.params() {
            self.type_stack.push(*param);
        }
    }

    fn get_local_type(&self, local_index: u32) -> Option<WasmerType> {
        self.type_stack.get(local_index as usize).cloned()
    }

    fn get_local_types(&self) -> &[WasmerType] {
        &self.type_stack
    }

    fn get_function_type(&self, function_index: FunctionIndex) -> Option<&FunctionType> {
        let sig_idx = self.module.functions.get(function_index)?;
        Some(&self.module.signatures[*sig_idx])
    }

    fn get_function_sig(&self, sig_index: SignatureIndex) -> Option<&FunctionType> {
        self.module.signatures.get(sig_index)
    }

    fn translate_drop_locals(&mut self, builder: &mut FunctionBuilder) -> WasmResult<()> {
        // TODO: this allocation can be removed without too much effort but it will require
        //       maneuvering around the borrow checker
        for (local_index, local_type) in self.type_stack.to_vec().iter().enumerate() {
            if *local_type == WasmerType::ExternRef {
                let val = builder.use_var(Variable::with_u32(local_index as _));
                self.translate_externref_dec(builder.cursor(), val)?;
            }
        }
        Ok(())
    }
}

'''
'''--- lib/compiler-cranelift/src/lib.rs ---
//! A WebAssembly `Compiler` implementation using Cranelift.
//!
//! Cranelift is a fast IR generator created by Mozilla for usage in
//! Firefox as a next JS compiler generator.
//!
//! Compared to LLVM, Cranelift is a bit faster and made entirely in Rust.
#![deny(missing_docs, trivial_numeric_casts, unused_extern_crates)]
#![warn(unused_import_braces)]
#![cfg_attr(
    feature = "cargo-clippy",
    allow(clippy::new_without_default, clippy::new_without_default)
)]
#![cfg_attr(
    feature = "cargo-clippy",
    warn(
        clippy::float_arithmetic,
        clippy::mut_mut,
        clippy::nonminimal_bool,
        clippy::option_map_unwrap_or,
        clippy::option_map_unwrap_or_else,
        clippy::print_stdout,
        clippy::unicode_not_nfc,
        clippy::use_self
    )
)]

#[cfg(not(feature = "std"))]
#[macro_use]
extern crate alloc as std;
#[cfg(feature = "std")]
#[macro_use]
extern crate std;

#[cfg(not(feature = "std"))]
use hashbrown::{
    hash_map,
    hash_map::Entry::{Occupied, Vacant},
    HashMap,
};
#[cfg(feature = "std")]
use std::collections::{
    hash_map,
    hash_map::Entry::{Occupied, Vacant},
    HashMap,
};

mod address_map;
mod compiler;
mod config;
mod debug;
#[cfg(feature = "unwind")]
mod dwarf;
mod func_environ;
mod sink;
mod trampoline;
mod translator;

pub use crate::compiler::CraneliftCompiler;
pub use crate::config::{Cranelift, CraneliftOptLevel};
pub use crate::debug::{ModuleInfoMemoryOffset, ModuleInfoVmctxInfo, ValueLabelsRanges};
pub use crate::trampoline::make_trampoline_function_call;

/// Version number of this crate.
pub const VERSION: &str = env!("CARGO_PKG_VERSION");

'''
'''--- lib/compiler-cranelift/src/sink.rs ---
//! Support for compiling with Cranelift.

use crate::translator::{irlibcall_to_libcall, irreloc_to_relocationkind};
use cranelift_codegen::binemit;
use cranelift_codegen::ir::{self, ExternalName};
use cranelift_entity::EntityRef as CraneliftEntityRef;
use wasmer_compiler::{JumpTable, Relocation, RelocationTarget, TrapInformation};
use wasmer_compiler::{RelocationKind, SectionIndex};
use wasmer_types::entity::EntityRef;
use wasmer_types::{FunctionIndex, LocalFunctionIndex, ModuleInfo};
use wasmer_vm::TrapCode;

/// Implementation of a relocation sink that just saves all the information for later
pub(crate) struct RelocSink<'a> {
    module: &'a ModuleInfo,

    /// Current function index.
    local_func_index: LocalFunctionIndex,

    /// Relocations recorded for the function.
    pub func_relocs: Vec<Relocation>,

    /// The section where the probestack trampoline call is located
    pub probestack_trampoline_relocation_target: Option<SectionIndex>,
}

impl<'a> binemit::RelocSink for RelocSink<'a> {
    fn reloc_external(
        &mut self,
        offset: binemit::CodeOffset,
        _source_loc: ir::SourceLoc,
        reloc: binemit::Reloc,
        name: &ExternalName,
        addend: binemit::Addend,
    ) {
        let reloc_target = if let ExternalName::User { namespace, index } = *name {
            debug_assert_eq!(namespace, 0);
            RelocationTarget::LocalFunc(
                self.module
                    .local_func_index(FunctionIndex::from_u32(index))
                    .expect("The provided function should be local"),
            )
        } else if let ExternalName::LibCall(libcall) = *name {
            match (libcall, self.probestack_trampoline_relocation_target) {
                (ir::LibCall::Probestack, Some(probestack_trampoline_relocation_target)) => {
                    self.func_relocs.push(Relocation {
                        kind: RelocationKind::X86CallPCRel4,
                        reloc_target: RelocationTarget::CustomSection(
                            probestack_trampoline_relocation_target,
                        ),
                        offset: offset,
                        addend: addend,
                    });
                    // Skip the default path
                    return;
                }
                _ => RelocationTarget::LibCall(irlibcall_to_libcall(libcall)),
            }
        } else {
            panic!("unrecognized external name")
        };
        self.func_relocs.push(Relocation {
            kind: irreloc_to_relocationkind(reloc),
            reloc_target,
            offset,
            addend,
        });
    }

    fn reloc_constant(
        &mut self,
        _code_offset: binemit::CodeOffset,
        _reloc: binemit::Reloc,
        _constant_offset: ir::ConstantOffset,
    ) {
        // Do nothing for now: cranelift emits constant data after the function code and also emits
        // function code with correct relative offsets to the constant data.
    }

    fn reloc_jt(&mut self, offset: binemit::CodeOffset, reloc: binemit::Reloc, jt: ir::JumpTable) {
        self.func_relocs.push(Relocation {
            kind: irreloc_to_relocationkind(reloc),
            reloc_target: RelocationTarget::JumpTable(
                self.local_func_index,
                JumpTable::new(jt.index()),
            ),
            offset,
            addend: 0,
        });
    }
}

impl<'a> RelocSink<'a> {
    /// Return a new `RelocSink` instance.
    pub fn new(
        module: &'a ModuleInfo,
        func_index: FunctionIndex,
        probestack_trampoline_relocation_target: Option<SectionIndex>,
    ) -> Self {
        let local_func_index = module
            .local_func_index(func_index)
            .expect("The provided function should be local");
        Self {
            module,
            local_func_index,
            func_relocs: Vec::new(),
            probestack_trampoline_relocation_target,
        }
    }
}

pub(crate) struct TrapSink {
    pub traps: Vec<TrapInformation>,
}

impl TrapSink {
    pub fn new() -> Self {
        Self { traps: Vec::new() }
    }
}

impl binemit::TrapSink for TrapSink {
    fn trap(
        &mut self,
        code_offset: binemit::CodeOffset,
        _source_loc: ir::SourceLoc,
        trap_code: ir::TrapCode,
    ) {
        self.traps.push(TrapInformation {
            code_offset,
            // TODO: Translate properly environment Trapcode into cranelift IR
            trap_code: translate_ir_trapcode(trap_code),
        });
    }
}

/// Translates the Cranelift IR TrapCode into generic Trap Code
fn translate_ir_trapcode(trap: ir::TrapCode) -> TrapCode {
    match trap {
        ir::TrapCode::StackOverflow => TrapCode::StackOverflow,
        ir::TrapCode::HeapOutOfBounds => TrapCode::HeapAccessOutOfBounds,
        ir::TrapCode::HeapMisaligned => TrapCode::HeapMisaligned,
        ir::TrapCode::TableOutOfBounds => TrapCode::TableAccessOutOfBounds,
        ir::TrapCode::IndirectCallToNull => TrapCode::IndirectCallToNull,
        ir::TrapCode::BadSignature => TrapCode::BadSignature,
        ir::TrapCode::IntegerOverflow => TrapCode::IntegerOverflow,
        ir::TrapCode::IntegerDivisionByZero => TrapCode::IntegerDivisionByZero,
        ir::TrapCode::BadConversionToInteger => TrapCode::BadConversionToInteger,
        ir::TrapCode::UnreachableCodeReached => TrapCode::UnreachableCodeReached,
        ir::TrapCode::Interrupt => unimplemented!("Interrupts not supported"),
        ir::TrapCode::User(_user_code) => unimplemented!("User trap code not supported"),
        // ir::TrapCode::Interrupt => TrapCode::Interrupt,
        // ir::TrapCode::User(user_code) => TrapCode::User(user_code),
    }
}

'''
'''--- lib/compiler-cranelift/src/trampoline/dynamic_function.rs ---
// This file contains code from external sources.
// Attributions: https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md

//! A trampoline generator for calling dynamic host functions from Wasm.

use super::binemit::TrampolineRelocSink;
use crate::translator::{compiled_function_unwind_info, signature_to_cranelift_ir};
use cranelift_codegen::ir::{
    ExternalName, Function, InstBuilder, MemFlags, StackSlotData, StackSlotKind,
};
use cranelift_codegen::isa::TargetIsa;
use cranelift_codegen::print_errors::pretty_error;
use cranelift_codegen::Context;
use cranelift_codegen::{binemit, ir};
use std::cmp;
use std::mem;

use cranelift_frontend::{FunctionBuilder, FunctionBuilderContext};
use wasmer_compiler::{CompileError, FunctionBody};
use wasmer_types::FunctionType;
use wasmer_vm::VMOffsets;

/// Create a trampoline for invoking a WebAssembly function.
pub fn make_trampoline_dynamic_function(
    isa: &dyn TargetIsa,
    offsets: &VMOffsets,
    fn_builder_ctx: &mut FunctionBuilderContext,
    func_type: &FunctionType,
) -> Result<FunctionBody, CompileError> {
    let pointer_type = isa.pointer_type();
    let frontend_config = isa.frontend_config();
    let signature = signature_to_cranelift_ir(func_type, frontend_config);
    let mut stub_sig = ir::Signature::new(frontend_config.default_call_conv);
    // Add the caller `vmctx` parameter.
    stub_sig.params.push(ir::AbiParam::special(
        pointer_type,
        ir::ArgumentPurpose::VMContext,
    ));

    // Add the `values_vec` parameter.
    stub_sig.params.push(ir::AbiParam::new(pointer_type));

    // Compute the size of the values vector. The vmctx and caller vmctx are passed separately.
    let value_size = mem::size_of::<u128>();
    let values_vec_len =
        (value_size * cmp::max(signature.params.len() - 1, signature.returns.len())) as u32;

    let mut context = Context::new();
    context.func = Function::with_name_signature(ExternalName::user(0, 0), signature.clone());

    let ss = context.func.create_stack_slot(StackSlotData::new(
        StackSlotKind::ExplicitSlot,
        values_vec_len,
    ));

    {
        let mut builder = FunctionBuilder::new(&mut context.func, fn_builder_ctx);
        let block0 = builder.create_block();

        builder.append_block_params_for_function_params(block0);
        builder.switch_to_block(block0);
        builder.seal_block(block0);

        let values_vec_ptr_val = builder.ins().stack_addr(pointer_type, ss, 0);
        let mflags = MemFlags::trusted();
        // We only get the non-vmctx arguments
        for i in 1..signature.params.len() {
            let val = builder.func.dfg.block_params(block0)[i];
            builder.ins().store(
                mflags,
                val,
                values_vec_ptr_val,
                ((i - 1) * value_size) as i32,
            );
        }

        let block_params = builder.func.dfg.block_params(block0);
        let vmctx_ptr_val = block_params[0];
        let callee_args = vec![vmctx_ptr_val, values_vec_ptr_val];

        let new_sig = builder.import_signature(stub_sig);

        let mem_flags = ir::MemFlags::trusted();
        let callee_value = builder.ins().load(
            pointer_type,
            mem_flags,
            vmctx_ptr_val,
            offsets.vmdynamicfunction_import_context_address() as i32,
        );

        builder
            .ins()
            .call_indirect(new_sig, callee_value, &callee_args);

        let mflags = MemFlags::trusted();
        let mut results = Vec::new();
        for (i, r) in signature.returns.iter().enumerate() {
            let load = builder.ins().load(
                r.value_type,
                mflags,
                values_vec_ptr_val,
                (i * value_size) as i32,
            );
            results.push(load);
        }
        builder.ins().return_(&results);
        builder.finalize()
    }

    let mut code_buf = Vec::new();
    let mut reloc_sink = TrampolineRelocSink {};
    let mut trap_sink = binemit::NullTrapSink {};
    let mut stackmap_sink = binemit::NullStackMapSink {};
    context
        .compile_and_emit(
            isa,
            &mut code_buf,
            &mut reloc_sink,
            &mut trap_sink,
            &mut stackmap_sink,
        )
        .map_err(|error| CompileError::Codegen(pretty_error(&context.func, Some(isa), error)))?;

    let unwind_info = compiled_function_unwind_info(isa, &context)?.maybe_into_to_windows_unwind();

    Ok(FunctionBody {
        body: code_buf,
        unwind_info,
    })
}

'''
'''--- lib/compiler-cranelift/src/trampoline/function_call.rs ---
// This file contains code from external sources.
// Attributions: https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md

//! A trampoline generator for calling Wasm functions easily.
//!
//! That way, you can start calling Wasm functions doing things like:
//! ```ignore
//! let my_func = instance.exports.get("func");
//! my_func.call([1, 2])
//! ```
use super::binemit::TrampolineRelocSink;
use crate::translator::{
    compiled_function_unwind_info, signature_to_cranelift_ir, /*transform_jump_table, */
};
use cranelift_codegen::ir::InstBuilder;
use cranelift_codegen::isa::TargetIsa;
use cranelift_codegen::print_errors::pretty_error;
use cranelift_codegen::Context;
use cranelift_codegen::{binemit, ir};
use cranelift_frontend::{FunctionBuilder, FunctionBuilderContext};
use std::mem;
use wasmer_compiler::{CompileError, FunctionBody};
use wasmer_types::FunctionType;

/// Create a trampoline for invoking a WebAssembly function.
pub fn make_trampoline_function_call(
    isa: &dyn TargetIsa,
    fn_builder_ctx: &mut FunctionBuilderContext,
    func_type: &FunctionType,
) -> Result<FunctionBody, CompileError> {
    let pointer_type = isa.pointer_type();
    let frontend_config = isa.frontend_config();
    let signature = signature_to_cranelift_ir(func_type, frontend_config);
    let mut wrapper_sig = ir::Signature::new(frontend_config.default_call_conv);

    // Add the callee `vmctx` parameter.
    wrapper_sig.params.push(ir::AbiParam::special(
        pointer_type,
        ir::ArgumentPurpose::VMContext,
    ));

    // Add the `callee_address` parameter.
    wrapper_sig.params.push(ir::AbiParam::new(pointer_type));

    // Add the `values_vec` parameter.
    wrapper_sig.params.push(ir::AbiParam::new(pointer_type));

    let mut context = Context::new();
    context.func = ir::Function::with_name_signature(ir::ExternalName::user(0, 0), wrapper_sig);

    let value_size = mem::size_of::<u128>();
    {
        let mut builder = FunctionBuilder::new(&mut context.func, fn_builder_ctx);
        let block0 = builder.create_block();

        builder.append_block_params_for_function_params(block0);
        builder.switch_to_block(block0);
        builder.seal_block(block0);

        let (vmctx_ptr_val, callee_value, values_vec_ptr_val) = {
            let params = builder.func.dfg.block_params(block0);
            (params[0], params[1], params[2])
        };

        // Load the argument values out of `values_vec`.
        let mflags = ir::MemFlags::trusted();
        let callee_args = signature
            .params
            .iter()
            .enumerate()
            .map(|(i, r)| {
                match i {
                    0 => vmctx_ptr_val,
                    _ =>
                    // i - 1 because vmctx is not passed through `values_vec`.
                    {
                        builder.ins().load(
                            r.value_type,
                            mflags,
                            values_vec_ptr_val,
                            ((i - 1) * value_size) as i32,
                        )
                    }
                }
            })
            .collect::<Vec<_>>();

        let new_sig = builder.import_signature(signature);

        let call = builder
            .ins()
            .call_indirect(new_sig, callee_value, &callee_args);

        let results = builder.func.dfg.inst_results(call).to_vec();

        // Store the return values into `values_vec`.
        let mflags = ir::MemFlags::trusted();
        for (i, r) in results.iter().enumerate() {
            builder
                .ins()
                .store(mflags, *r, values_vec_ptr_val, (i * value_size) as i32);
        }

        builder.ins().return_(&[]);
        builder.finalize()
    }

    let mut code_buf = Vec::new();
    let mut reloc_sink = TrampolineRelocSink {};
    let mut trap_sink = binemit::NullTrapSink {};
    let mut stackmap_sink = binemit::NullStackMapSink {};

    context
        .compile_and_emit(
            isa,
            &mut code_buf,
            &mut reloc_sink,
            &mut trap_sink,
            &mut stackmap_sink,
        )
        .map_err(|error| CompileError::Codegen(pretty_error(&context.func, Some(isa), error)))?;

    let unwind_info = compiled_function_unwind_info(isa, &context)?.maybe_into_to_windows_unwind();

    Ok(FunctionBody {
        body: code_buf,
        unwind_info,
        // jt_offsets: transform_jump_table(context.func.jt_offsets),
    })
}

'''
'''--- lib/compiler-cranelift/src/trampoline/mod.rs ---
#![allow(missing_docs)]

mod dynamic_function;
mod function_call;

pub use self::dynamic_function::make_trampoline_dynamic_function;
pub use self::function_call::make_trampoline_function_call;

pub use cranelift_codegen::print_errors::pretty_error;
pub use cranelift_codegen::Context;
pub use cranelift_frontend::{FunctionBuilder, FunctionBuilderContext};

pub mod binemit {
    pub use cranelift_codegen::binemit::NullTrapSink;
    pub use cranelift_codegen::binemit::{CodeOffset, NullStackMapSink, TrapSink};

    use cranelift_codegen::{binemit, ir};

    /// We don't expect trampoline compilation to produce any relocations, so
    /// this `RelocSink` just asserts that it doesn't recieve any.
    pub struct TrampolineRelocSink {}

    impl binemit::RelocSink for TrampolineRelocSink {
        fn reloc_external(
            &mut self,
            _offset: binemit::CodeOffset,
            _source_loc: ir::SourceLoc,
            _reloc: binemit::Reloc,
            _name: &ir::ExternalName,
            _addend: binemit::Addend,
        ) {
            panic!("trampoline compilation should not produce external symbol relocs");
        }
        fn reloc_constant(
            &mut self,
            _code_offset: binemit::CodeOffset,
            _reloc: binemit::Reloc,
            _constant_offset: ir::ConstantOffset,
        ) {
            panic!("trampoline compilation should not produce constant relocs");
        }
        fn reloc_jt(
            &mut self,
            _offset: binemit::CodeOffset,
            _reloc: binemit::Reloc,
            _jt: ir::JumpTable,
        ) {
            panic!("trampoline compilation should not produce jump table relocs");
        }
    }
}

'''
'''--- lib/compiler-cranelift/src/translator/code_translator.rs ---
// This file contains code from external sources.
// Attributions: https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md

//! This module contains the bulk of the interesting code performing the translation between
//! WebAssembly bytecode and Cranelift IR.
//!
//! The translation is done in one pass, opcode by opcode. Two main data structures are used during
//! code translations: the value stack and the control stack. The value stack mimics the execution
//! of the WebAssembly stack machine: each instruction result is pushed onto the stack and
//! instruction arguments are popped off the stack. Similarly, when encountering a control flow
//! block, it is pushed onto the control stack and popped off when encountering the corresponding
//! `End`.
//!
//! Another data structure, the translation state, records information concerning unreachable code
//! status and about if inserting a return at the end of the function is necessary.
//!
//! Some of the WebAssembly instructions need information about the environment for which they
//! are being translated:
//!
//! - the loads and stores need the memory base address;
//! - the `get_global` and `set_global` instructions depend on how the globals are implemented;
//! - `memory.size` and `memory.grow` are runtime functions;
//! - `call_indirect` has to translate the function index into the address of where this
//!    is;
//!
//! That is why `translate_function_body` takes an object having the `WasmRuntime` trait as
//! argument.
//!
//! There is extra complexity associated with translation of 128-bit SIMD instructions.
//! Wasm only considers there to be a single 128-bit vector type.  But CLIF's type system
//! distinguishes different lane configurations, so considers 8X16, 16X8, 32X4 and 64X2 to be
//! different types.  The result is that, in wasm, it's perfectly OK to take the output of (eg)
//! an `add.16x8` and use that as an operand of a `sub.32x4`, without using any cast.  But when
//! translated into CLIF, that will cause a verifier error due to the apparent type mismatch.
//!
//! This file works around that problem by liberally inserting `bitcast` instructions in many
//! places -- mostly, before the use of vector values, either as arguments to CLIF instructions
//! or as block actual parameters.  These are no-op casts which nevertheless have different
//! input and output types, and are used (mostly) to "convert" 16X8, 32X4 and 64X2-typed vectors
//! to the "canonical" type, 8X16.  Hence the functions `optionally_bitcast_vector`,
//! `bitcast_arguments`, `pop*_with_bitcast`, `canonicalise_then_jump`,
//! `canonicalise_then_br{z,nz}`, `is_non_canonical_v128` and `canonicalise_v128_values`.
//! Note that the `bitcast*` functions are occasionally used to convert to some type other than
//! 8X16, but the `canonicalise*` functions always convert to type 8X16.
//!
//! Be careful when adding support for new vector instructions.  And when adding new jumps, even
//! if they are apparently don't have any connection to vectors.  Never generate any kind of
//! (inter-block) jump directly.  Instead use `canonicalise_then_jump` and
//! `canonicalise_then_br{z,nz}`.
//!
//! The use of bitcasts is ugly and inefficient, but currently unavoidable:
//!
//! * they make the logic in this file fragile: miss out a bitcast for any reason, and there is
//!   the risk of the system failing in the verifier.  At least for debug builds.
//!
//! * in the new backends, they potentially interfere with pattern matching on CLIF -- the
//!   patterns need to take into account the presence of bitcast nodes.
//!
//! * in the new backends, they get translated into machine-level vector-register-copy
//!   instructions, none of which are actually necessary.  We then depend on the register
//!   allocator to coalesce them all out.
//!
//! * they increase the total number of CLIF nodes that have to be processed, hence slowing down
//!   the compilation pipeline.  Also, the extra coalescing work generates a slowdown.
//!
//! A better solution which would avoid all four problems would be to remove the 8X16, 16X8,
//! 32X4 and 64X2 types from CLIF and instead have a single V128 type.
//!
//! For further background see also:
//!   <https://github.com/bytecodealliance/wasmtime/issues/1147>
//!     ("Too many raw_bitcasts in SIMD code")
//!   <https://github.com/bytecodealliance/cranelift/pull/1251>
//!     ("Add X128 type to represent WebAssembly's V128 type")
//!   <https://github.com/bytecodealliance/cranelift/pull/1236>
//!     ("Relax verification to allow I8X16 to act as a default vector type")

use super::func_environ::{FuncEnvironment, GlobalVariable, ReturnMode};
use super::func_state::{ControlStackFrame, ElseData, FuncTranslationState, ValueExtraInfo};
use super::translation_utils::{block_with_params, f32_translation, f64_translation};
use crate::{hash_map, HashMap};
use core::cmp;
use core::convert::TryFrom;
use core::{i32, u32};
use cranelift_codegen::ir::condcodes::{FloatCC, IntCC};
use cranelift_codegen::ir::immediates::Offset32;
use cranelift_codegen::ir::types::*;
use cranelift_codegen::ir::{
    self, AtomicRmwOp, ConstantData, InstBuilder, JumpTableData, MemFlags, Value, ValueLabel,
};
use cranelift_codegen::packed_option::ReservedValue;
use cranelift_frontend::{FunctionBuilder, Variable};
use smallvec::SmallVec;
use std::vec::Vec;

use wasmer_compiler::wasmparser::{MemoryImmediate, Operator, Type as WPType};
use wasmer_compiler::WasmResult;
use wasmer_compiler::{wasm_unsupported, ModuleTranslationState};
use wasmer_types::{
    FunctionIndex, GlobalIndex, MemoryIndex, SignatureIndex, TableIndex, Type as WasmerType,
};

// Clippy warns about "align: _" but its important to document that the align field is ignored
#[cfg_attr(
    feature = "cargo-clippy",
    allow(clippy::unneeded_field_pattern, clippy::cognitive_complexity)
)]
/// Translates wasm operators into Cranelift IR instructions. Returns `true` if it inserted
/// a return.
pub fn translate_operator<FE: FuncEnvironment + ?Sized>(
    module_translation_state: &ModuleTranslationState,
    op: &Operator,
    builder: &mut FunctionBuilder,
    state: &mut FuncTranslationState,
    environ: &mut FE,
) -> WasmResult<()> {
    if !state.reachable {
        translate_unreachable_operator(module_translation_state, &op, builder, state, environ)?;
        return Ok(());
    }

    // This big match treats all Wasm code operators.
    match op {
        /********************************** Locals ****************************************
         *  `get_local` and `set_local` are treated as non-SSA variables and will completely
         *  disappear in the Cranelift Code
         ***********************************************************************************/
        Operator::LocalGet { local_index } => {
            let val = builder.use_var(Variable::with_u32(*local_index));
            let local_type = environ.get_local_type(*local_index).unwrap();
            let ref_counted = local_type == WasmerType::ExternRef;
            state.push1_extra((
                val,
                ValueExtraInfo {
                    ref_counted,
                    ..Default::default()
                },
            ));
            let label = ValueLabel::from_u32(*local_index);
            builder.set_val_label(val, label);

            if ref_counted {
                environ.translate_externref_inc(builder.cursor(), val)?;
            }
        }
        Operator::LocalSet { local_index } => {
            let (mut val, _metadata) = state.pop1();

            let local_type = environ.get_local_type(*local_index).unwrap();
            if local_type == WasmerType::ExternRef {
                debug_assert!(_metadata.ref_counted);
                let existing_val = builder.use_var(Variable::with_u32(*local_index));
                environ.translate_externref_dec(builder.cursor(), existing_val)?;
            }
            // Ensure SIMD values are cast to their default Cranelift type, I8x16.
            let ty = builder.func.dfg.value_type(val);
            if ty.is_vector() {
                val = optionally_bitcast_vector(val, I8X16, builder);
            }

            builder.def_var(Variable::with_u32(*local_index), val);
            let label = ValueLabel::from_u32(*local_index);
            builder.set_val_label(val, label);
        }
        Operator::LocalTee { local_index } => {
            let (mut val, _metadata) = state.peek1();

            // ref count if we need to
            let local_type = environ.get_local_type(*local_index).unwrap();
            if local_type == WasmerType::ExternRef {
                debug_assert!(_metadata.ref_counted);
                let existing_val = builder.use_var(Variable::with_u32(*local_index));
                environ.translate_externref_dec(builder.cursor(), existing_val)?;
                environ.translate_externref_inc(builder.cursor(), val)?;
            }
            // Ensure SIMD values are cast to their default Cranelift type, I8x16.
            let ty = builder.func.dfg.value_type(val);
            if ty.is_vector() {
                val = optionally_bitcast_vector(val, I8X16, builder);
            }

            builder.def_var(Variable::with_u32(*local_index), val);
            let label = ValueLabel::from_u32(*local_index);
            builder.set_val_label(val, label);
        }
        /********************************** Globals ****************************************
         *  `get_global` and `set_global` are handled by the environment.
         ***********************************************************************************/
        Operator::GlobalGet { global_index } => {
            let global_index = GlobalIndex::from_u32(*global_index);
            let stack_elem = match state.get_global(builder.func, global_index.as_u32(), environ)? {
                GlobalVariable::Const(val) => (val, ValueExtraInfo::default()),
                GlobalVariable::Memory { gv, offset, ty } => {
                    let global_type = environ.get_global_type(global_index).unwrap();
                    let addr = builder.ins().global_value(environ.pointer_type(), gv);
                    let flags = ir::MemFlags::trusted();
                    let value = builder.ins().load(ty, flags, addr, offset);
                    let ref_counted = global_type == WasmerType::ExternRef;
                    if ref_counted {
                        environ.translate_externref_inc(builder.cursor(), value)?;
                    }

                    (
                        value,
                        ValueExtraInfo {
                            ref_counted,
                            ..Default::default()
                        },
                    )
                }
                GlobalVariable::Custom => (
                    environ.translate_custom_global_get(builder.cursor(), global_index)?,
                    ValueExtraInfo::default(),
                ),
            };
            state.push1_extra(stack_elem);
        }
        Operator::GlobalSet { global_index } => {
            let global_index = GlobalIndex::from_u32(*global_index);
            match state.get_global(builder.func, global_index.as_u32(), environ)? {
                GlobalVariable::Const(_) => {
                    panic!("global #{} is a constant", global_index.as_u32())
                }
                GlobalVariable::Memory { gv, offset, ty } => {
                    let global_type = environ.get_global_type(global_index).unwrap();
                    let addr = builder.ins().global_value(environ.pointer_type(), gv);
                    let flags = ir::MemFlags::trusted();
                    let (mut val, _) = state.pop1();
                    // Ensure SIMD values are cast to their default Cranelift type, I8x16.
                    if ty.is_vector() {
                        val = optionally_bitcast_vector(val, I8X16, builder);
                    }
                    debug_assert_eq!(ty, builder.func.dfg.value_type(val));
                    if global_type == WasmerType::ExternRef {
                        let value = builder.ins().load(ty, flags, addr, offset);
                        environ.translate_externref_dec(builder.cursor(), value)?;
                    }
                    builder.ins().store(flags, val, addr, offset);
                }
                GlobalVariable::Custom => {
                    let (val, _) = state.pop1();
                    environ.translate_custom_global_set(builder.cursor(), global_index, val)?;
                }
            }
        }
        /********************************* Stack misc ***************************************
         *  `drop`, `nop`, `unreachable` and `select`.
         ***********************************************************************************/
        Operator::Drop => {
            let (val, metadata) = state.pop1();
            if metadata.ref_counted {
                environ.translate_externref_dec(builder.cursor(), val)?;
            }
        }
        Operator::Select => {
            // we can ignore metadata because extern ref must use TypedSelect
            let ((arg1, _), (arg2, _), (cond, _)) = state.pop3();
            state.push1(builder.ins().select(cond, arg1, arg2));
        }
        Operator::TypedSelect { ty } => {
            let ((arg1, _), (arg2, _), (cond, _)) = state.pop3();
            let ref_counted = *ty == WPType::ExternRef;
            if ref_counted {
                let selected_ref = builder.ins().select(cond, arg1, arg2);
                let not_selected_ref = builder.ins().select(cond, arg2, arg1);
                state.push1_extra((
                    selected_ref,
                    ValueExtraInfo {
                        ref_counted,
                        ..Default::default()
                    },
                ));
                environ.translate_externref_dec(builder.cursor(), not_selected_ref)?;
            } else {
                state.push1_extra((
                    builder.ins().select(cond, arg1, arg2),
                    ValueExtraInfo::default(),
                ));
            }
        }
        Operator::Nop => {
            // We do nothing
        }
        Operator::Unreachable => {
            builder.ins().trap(ir::TrapCode::UnreachableCodeReached);
            state.reachable = false;
        }
        /***************************** Control flow blocks **********************************
         *  When starting a control flow block, we create a new `Block` that will hold the code
         *  after the block, and we push a frame on the control stack. Depending on the type
         *  of block, we create a new `Block` for the body of the block with an associated
         *  jump instruction.
         *
         *  The `End` instruction pops the last control frame from the control stack, seals
         *  the destination block (since `br` instructions targeting it only appear inside the
         *  block and have already been translated) and modify the value stack to use the
         *  possible `Block`'s arguments values.
         ***********************************************************************************/
        Operator::Block { ty } => {
            let (params, results) = module_translation_state.blocktype_params_results(*ty)?;
            let next = block_with_params(builder, results, environ)?;
            state.push_block(next, params.len(), results.len());
        }
        Operator::Loop { ty } => {
            let (params, results) = module_translation_state.blocktype_params_results(*ty)?;
            let loop_body = block_with_params(builder, params, environ)?;
            let next = block_with_params(builder, results, environ)?;
            canonicalise_then_jump(builder, loop_body, state.peekn(params.len()));
            state.push_loop(loop_body, next, params.len(), results.len());

            // Pop the initial `Block` actuals and replace them with the `Block`'s
            // params since control flow joins at the top of the loop.
            state.popn(params.len());
            state
                .stack
                .extend_from_slice(builder.block_params(loop_body));

            builder.switch_to_block(loop_body);
            environ.translate_loop_header(builder.cursor())?;
        }
        Operator::If { ty } => {
            let (val, _) = state.pop1();

            let (params, results) = module_translation_state.blocktype_params_results(*ty)?;
            let (destination, else_data) = if params == results {
                // It is possible there is no `else` block, so we will only
                // allocate a block for it if/when we find the `else`. For now,
                // we if the condition isn't true, then we jump directly to the
                // destination block following the whole `if...end`. If we do end
                // up discovering an `else`, then we will allocate a block for it
                // and go back and patch the jump.
                let destination = block_with_params(builder, results, environ)?;
                let branch_inst =
                    canonicalise_then_brz(builder, val, destination, state.peekn(params.len()));
                (destination, ElseData::NoElse { branch_inst })
            } else {
                // The `if` type signature is not valid without an `else` block,
                // so we eagerly allocate the `else` block here.
                let destination = block_with_params(builder, results, environ)?;
                let else_block = block_with_params(builder, params, environ)?;
                canonicalise_then_brz(builder, val, else_block, state.peekn(params.len()));
                builder.seal_block(else_block);
                (destination, ElseData::WithElse { else_block })
            };

            let next_block = builder.create_block();
            canonicalise_then_jump(builder, next_block, (&[], &[]));
            builder.seal_block(next_block); // Only predecessor is the current block.
            builder.switch_to_block(next_block);

            // Here we append an argument to a Block targeted by an argumentless jump instruction
            // But in fact there are two cases:
            // - either the If does not have a Else clause, in that case ty = EmptyBlock
            //   and we add nothing;
            // - either the If have an Else clause, in that case the destination of this jump
            //   instruction will be changed later when we translate the Else operator.
            state.push_if(destination, else_data, params.len(), results.len(), *ty);
        }
        Operator::Else => {
            let i = state.control_stack.len() - 1;
            match state.control_stack[i] {
                ControlStackFrame::If {
                    ref else_data,
                    head_is_reachable,
                    ref mut consequent_ends_reachable,
                    num_return_values,
                    blocktype,
                    destination,
                    ..
                } => {
                    // We finished the consequent, so record its final
                    // reachability state.
                    debug_assert!(consequent_ends_reachable.is_none());
                    *consequent_ends_reachable = Some(state.reachable);

                    if head_is_reachable {
                        // We have a branch from the head of the `if` to the `else`.
                        state.reachable = true;

                        // Ensure we have a block for the `else` block (it may have
                        // already been pre-allocated, see `ElseData` for details).
                        let else_block = match *else_data {
                            ElseData::NoElse { branch_inst } => {
                                let (params, _results) =
                                    module_translation_state.blocktype_params_results(blocktype)?;
                                debug_assert_eq!(params.len(), num_return_values);
                                let else_block = block_with_params(builder, params, environ)?;
                                canonicalise_then_jump(
                                    builder,
                                    destination,
                                    state.peekn(params.len()),
                                );
                                state.popn(params.len());

                                builder.change_jump_destination(branch_inst, else_block);
                                builder.seal_block(else_block);
                                else_block
                            }
                            ElseData::WithElse { else_block } => {
                                canonicalise_then_jump(
                                    builder,
                                    destination,
                                    state.peekn(num_return_values),
                                );
                                state.popn(num_return_values);
                                else_block
                            }
                        };

                        // You might be expecting that we push the parameters for this
                        // `else` block here, something like this:
                        //
                        //     state.pushn(&control_stack_frame.params);
                        //
                        // We don't do that because they are already on the top of the stack
                        // for us: we pushed the parameters twice when we saw the initial
                        // `if` so that we wouldn't have to save the parameters in the
                        // `ControlStackFrame` as another `Vec` allocation.

                        builder.switch_to_block(else_block);

                        // We don't bother updating the control frame's `ElseData`
                        // to `WithElse` because nothing else will read it.
                    }
                }
                _ => unreachable!(),
            }
        }
        Operator::End => {
            let frame = state.control_stack.pop().unwrap();
            let next_block = frame.following_code();

            if !builder.is_unreachable() || !builder.is_pristine() {
                let return_count = frame.num_return_values();
                let return_args = state.peekn(return_count);
                canonicalise_then_jump(builder, frame.following_code(), return_args);
                // You might expect that if we just finished an `if` block that
                // didn't have a corresponding `else` block, then we would clean
                // up our duplicate set of parameters that we pushed earlier
                // right here. However, we don't have to explicitly do that,
                // since we truncate the stack back to the original height
                // below.
            }

            builder.switch_to_block(next_block);
            builder.seal_block(next_block);

            // If it is a loop we also have to seal the body loop block
            if let ControlStackFrame::Loop { header, .. } = frame {
                builder.seal_block(header)
            }

            frame.truncate_value_stack_to_original_size(&mut state.stack);
            state
                .stack
                .extend_from_slice(builder.block_params(next_block));
        }
        /**************************** Branch instructions *********************************
         * The branch instructions all have as arguments a target nesting level, which
         * corresponds to how many control stack frames do we have to pop to get the
         * destination `Block`.
         *
         * Once the destination `Block` is found, we sometimes have to declare a certain depth
         * of the stack unreachable, because some branch instructions are terminator.
         *
         * The `br_table` case is much more complicated because Cranelift's `br_table` instruction
         * does not support jump arguments like all the other branch instructions. That is why, in
         * the case where we would use jump arguments for every other branch instruction, we
         * need to split the critical edges leaving the `br_tables` by creating one `Block` per
         * table destination; the `br_table` will point to these newly created `Blocks` and these
         * `Block`s contain only a jump instruction pointing to the final destination, this time with
         * jump arguments.
         *
         * This system is also implemented in Cranelift's SSA construction algorithm, because
         * `use_var` located in a destination `Block` of a `br_table` might trigger the addition
         * of jump arguments in each predecessor branch instruction, one of which might be a
         * `br_table`.
         ***********************************************************************************/
        Operator::Br { relative_depth } => {
            let i = state.control_stack.len() - 1 - (*relative_depth as usize);
            let (return_count, br_destination) = {
                let frame = &mut state.control_stack[i];
                // We signal that all the code that follows until the next End is unreachable
                frame.set_branched_to_exit();
                let return_count = if frame.is_loop() {
                    frame.num_param_values()
                } else {
                    frame.num_return_values()
                };
                (return_count, frame.br_destination())
            };
            let destination_args = state.peekn(return_count);
            canonicalise_then_jump(builder, br_destination, destination_args);
            state.popn(return_count);
            state.reachable = false;
        }
        Operator::BrIf { relative_depth } => translate_br_if(*relative_depth, builder, state),
        Operator::BrTable { table } => {
            let mut depths = table.targets().collect::<Result<Vec<_>, _>>()?;
            let default = depths.pop().unwrap().0;
            let mut min_depth = default;
            for (depth, _) in depths.iter() {
                if *depth < min_depth {
                    min_depth = *depth;
                }
            }
            let jump_args_count = {
                let i = state.control_stack.len() - 1 - (min_depth as usize);
                let min_depth_frame = &state.control_stack[i];
                if min_depth_frame.is_loop() {
                    min_depth_frame.num_param_values()
                } else {
                    min_depth_frame.num_return_values()
                }
            };
            let (val, _) = state.pop1();
            let mut data = JumpTableData::with_capacity(depths.len());
            if jump_args_count == 0 {
                // No jump arguments
                for (depth, _) in depths.iter() {
                    let block = {
                        let i = state.control_stack.len() - 1 - (*depth as usize);
                        let frame = &mut state.control_stack[i];
                        frame.set_branched_to_exit();
                        frame.br_destination()
                    };
                    data.push_entry(block);
                }
                let jt = builder.create_jump_table(data);
                let block = {
                    let i = state.control_stack.len() - 1 - (default as usize);
                    let frame = &mut state.control_stack[i];
                    frame.set_branched_to_exit();
                    frame.br_destination()
                };
                builder.ins().br_table(val, block, jt);
            } else {
                // Here we have jump arguments, but Cranelift's br_table doesn't support them
                // We then proceed to split the edges going out of the br_table
                let return_count = jump_args_count;
                let mut dest_block_sequence = vec![];
                let mut dest_block_map = HashMap::new();
                for (depth, _) in depths.iter() {
                    let branch_block = match dest_block_map.entry(*depth as usize) {
                        hash_map::Entry::Occupied(entry) => *entry.get(),
                        hash_map::Entry::Vacant(entry) => {
                            let block = builder.create_block();
                            dest_block_sequence.push((*depth as usize, block));
                            *entry.insert(block)
                        }
                    };
                    data.push_entry(branch_block);
                }
                let default_branch_block = match dest_block_map.entry(default as usize) {
                    hash_map::Entry::Occupied(entry) => *entry.get(),
                    hash_map::Entry::Vacant(entry) => {
                        let block = builder.create_block();
                        dest_block_sequence.push((default as usize, block));
                        *entry.insert(block)
                    }
                };
                let jt = builder.create_jump_table(data);
                builder.ins().br_table(val, default_branch_block, jt);
                for (depth, dest_block) in dest_block_sequence {
                    builder.switch_to_block(dest_block);
                    builder.seal_block(dest_block);
                    let real_dest_block = {
                        let i = state.control_stack.len() - 1 - depth;
                        let frame = &mut state.control_stack[i];
                        frame.set_branched_to_exit();
                        frame.br_destination()
                    };
                    let destination_args = state.peekn(return_count);
                    canonicalise_then_jump(builder, real_dest_block, destination_args);
                }
                state.popn(return_count);
            }
            state.reachable = false;
        }
        Operator::Return => {
            let (return_count, br_destination) = {
                let frame = &mut state.control_stack[0];
                if environ.return_mode() == ReturnMode::FallthroughReturn {
                    frame.set_branched_to_exit();
                }
                let return_count = frame.num_return_values();
                (return_count, frame.br_destination())
            };
            {
                let (return_args, return_args_metadata) = state.peekn_mut(return_count);
                // TODO(reftypes): maybe ref count here?
                let return_types = wasm_param_types(&builder.func.signature.returns, |i| {
                    environ.is_wasm_return(&builder.func.signature, i)
                });
                bitcast_arguments(return_args, &return_types, builder);
                match environ.return_mode() {
                    ReturnMode::NormalReturns => builder.ins().return_(return_args),
                    ReturnMode::FallthroughReturn => canonicalise_then_jump(
                        builder,
                        br_destination,
                        (&*return_args, &*return_args_metadata),
                    ),
                };
            }
            state.popn(return_count);
            state.reachable = false;
        }
        /********************************** Exception handing **********************************/
        Operator::Try { .. }
        | Operator::Catch { .. }
        | Operator::Throw { .. }
        | Operator::Unwind
        | Operator::Rethrow { .. }
        | Operator::Delegate { .. }
        | Operator::CatchAll => {
            return Err(wasm_unsupported!(
                "proposed exception handling operator {:?}",
                op
            ));
        }
        /************************************ Calls ****************************************
         * The call instructions pop off their arguments from the stack and append their
         * return values to it. `call_indirect` needs environment support because there is an
         * argument referring to an index in the external functions table of the module.
         ************************************************************************************/
        Operator::Call { function_index } => {
            let (fref, num_args) = state.get_direct_func(builder.func, *function_index, environ)?;

            let (args, _args_metadata) = state.peekn_mut(num_args);

            // Bitcast any vector arguments to their default type, I8X16, before calling.
            let callee_signature =
                &builder.func.dfg.signatures[builder.func.dfg.ext_funcs[fref].signature];
            let types = wasm_param_types(&callee_signature.params, |i| {
                environ.is_wasm_parameter(&callee_signature, i)
            });
            bitcast_arguments(args, &types, builder);
            let func_index = FunctionIndex::from_u32(*function_index);

            let call = environ.translate_call(builder.cursor(), func_index, fref, args)?;
            let inst_results = builder.inst_results(call);
            debug_assert_eq!(
                inst_results.len(),
                builder.func.dfg.signatures[builder.func.dfg.ext_funcs[fref].signature]
                    .returns
                    .len(),
                "translate_call results should match the call signature"
            );
            let func_type = environ.get_function_type(func_index).unwrap();
            let mut results_metadata = Vec::with_capacity(func_type.results().len());
            for result in func_type.results() {
                results_metadata.push(if *result == WasmerType::ExternRef {
                    ValueExtraInfo {
                        ref_counted: true,
                        ..Default::default()
                    }
                } else {
                    Default::default()
                });
            }
            state.popn(num_args);
            state.pushn(inst_results, &results_metadata);
        }
        Operator::CallIndirect { index, table_index } => {
            // `index` is the index of the function's signature and `table_index` is the index of
            // the table to search the function in.
            let (sigref, num_args) = state.get_indirect_sig(builder.func, *index, environ)?;
            let table = state.get_or_create_table(builder.func, *table_index, environ)?;
            let (callee, _) = state.pop1();

            // Bitcast any vector arguments to their default type, I8X16, before calling.
            let callee_signature = &builder.func.dfg.signatures[sigref];
            let (args, _) = state.peekn_mut(num_args);
            let types = wasm_param_types(&callee_signature.params, |i| {
                environ.is_wasm_parameter(&callee_signature, i)
            });
            bitcast_arguments(args, &types, builder);

            let (args, _args_metadata) = state.peekn(num_args);
            let sig_idx = SignatureIndex::from_u32(*index);

            let call = environ.translate_call_indirect(
                builder.cursor(),
                TableIndex::from_u32(*table_index),
                table,
                sig_idx,
                sigref,
                callee,
                args,
            )?;
            let inst_results = builder.inst_results(call);
            debug_assert_eq!(
                inst_results.len(),
                builder.func.dfg.signatures[sigref].returns.len(),
                "translate_call_indirect results should match the call signature"
            );
            let func_type = environ.get_function_sig(sig_idx).unwrap();
            let mut results_metadata = Vec::with_capacity(func_type.results().len());
            for result in func_type.results() {
                results_metadata.push(if *result == WasmerType::ExternRef {
                    ValueExtraInfo {
                        ref_counted: true,
                        ..Default::default()
                    }
                } else {
                    Default::default()
                });
            }
            state.popn(num_args);
            state.pushn(inst_results, &results_metadata);
        }
        /******************************* Memory management ***********************************
         * Memory management is handled by environment. It is usually translated into calls to
         * special functions.
         ************************************************************************************/
        Operator::MemoryGrow { mem, mem_byte: _ } => {
            // The WebAssembly MVP only supports one linear memory, but we expect the reserved
            // argument to be a memory index.
            let heap_index = MemoryIndex::from_u32(*mem);
            let heap = state.get_heap(builder.func, *mem, environ)?;
            let (val, _) = state.pop1();
            state.push1(environ.translate_memory_grow(builder.cursor(), heap_index, heap, val)?)
        }
        Operator::MemorySize { mem, mem_byte: _ } => {
            let heap_index = MemoryIndex::from_u32(*mem);
            let heap = state.get_heap(builder.func, *mem, environ)?;
            state.push1(environ.translate_memory_size(builder.cursor(), heap_index, heap)?);
        }
        /******************************* Load instructions ***********************************
         * Wasm specifies an integer alignment flag but we drop it in Cranelift.
         * The memory base address is provided by the environment.
         ************************************************************************************/
        Operator::I32Load8U { memarg } => {
            translate_load(memarg, ir::Opcode::Uload8, I32, builder, state, environ)?;
        }
        Operator::I32Load16U { memarg } => {
            translate_load(memarg, ir::Opcode::Uload16, I32, builder, state, environ)?;
        }
        Operator::I32Load8S { memarg } => {
            translate_load(memarg, ir::Opcode::Sload8, I32, builder, state, environ)?;
        }
        Operator::I32Load16S { memarg } => {
            translate_load(memarg, ir::Opcode::Sload16, I32, builder, state, environ)?;
        }
        Operator::I64Load8U { memarg } => {
            translate_load(memarg, ir::Opcode::Uload8, I64, builder, state, environ)?;
        }
        Operator::I64Load16U { memarg } => {
            translate_load(memarg, ir::Opcode::Uload16, I64, builder, state, environ)?;
        }
        Operator::I64Load8S { memarg } => {
            translate_load(memarg, ir::Opcode::Sload8, I64, builder, state, environ)?;
        }
        Operator::I64Load16S { memarg } => {
            translate_load(memarg, ir::Opcode::Sload16, I64, builder, state, environ)?;
        }
        Operator::I64Load32S { memarg } => {
            translate_load(memarg, ir::Opcode::Sload32, I64, builder, state, environ)?;
        }
        Operator::I64Load32U { memarg } => {
            translate_load(memarg, ir::Opcode::Uload32, I64, builder, state, environ)?;
        }
        Operator::I32Load { memarg } => {
            translate_load(memarg, ir::Opcode::Load, I32, builder, state, environ)?;
        }
        Operator::F32Load { memarg } => {
            translate_load(memarg, ir::Opcode::Load, F32, builder, state, environ)?;
        }
        Operator::I64Load { memarg } => {
            translate_load(memarg, ir::Opcode::Load, I64, builder, state, environ)?;
        }
        Operator::F64Load { memarg } => {
            translate_load(memarg, ir::Opcode::Load, F64, builder, state, environ)?;
        }
        Operator::V128Load { memarg } => {
            translate_load(memarg, ir::Opcode::Load, I8X16, builder, state, environ)?;
        }
        Operator::V128Load8x8S { memarg } => {
            let (flags, base, offset) = prepare_load(memarg, 8, builder, state, environ)?;
            let loaded = builder.ins().sload8x8(flags, base, offset);
            state.push1(loaded);
        }
        Operator::V128Load8x8U { memarg } => {
            let (flags, base, offset) = prepare_load(memarg, 8, builder, state, environ)?;
            let loaded = builder.ins().uload8x8(flags, base, offset);
            state.push1(loaded);
        }
        Operator::V128Load16x4S { memarg } => {
            let (flags, base, offset) = prepare_load(memarg, 8, builder, state, environ)?;
            let loaded = builder.ins().sload16x4(flags, base, offset);
            state.push1(loaded);
        }
        Operator::V128Load16x4U { memarg } => {
            let (flags, base, offset) = prepare_load(memarg, 8, builder, state, environ)?;
            let loaded = builder.ins().uload16x4(flags, base, offset);
            state.push1(loaded);
        }
        Operator::V128Load32x2S { memarg } => {
            let (flags, base, offset) = prepare_load(memarg, 8, builder, state, environ)?;
            let loaded = builder.ins().sload32x2(flags, base, offset);
            state.push1(loaded);
        }
        Operator::V128Load32x2U { memarg } => {
            let (flags, base, offset) = prepare_load(memarg, 8, builder, state, environ)?;
            let loaded = builder.ins().uload32x2(flags, base, offset);
            state.push1(loaded);
        }
        /****************************** Store instructions ***********************************
         * Wasm specifies an integer alignment flag but we drop it in Cranelift.
         * The memory base address is provided by the environment.
         ************************************************************************************/
        Operator::I32Store { memarg }
        | Operator::I64Store { memarg }
        | Operator::F32Store { memarg }
        | Operator::F64Store { memarg } => {
            translate_store(memarg, ir::Opcode::Store, builder, state, environ)?;
        }
        Operator::I32Store8 { memarg } | Operator::I64Store8 { memarg } => {
            translate_store(memarg, ir::Opcode::Istore8, builder, state, environ)?;
        }
        Operator::I32Store16 { memarg } | Operator::I64Store16 { memarg } => {
            translate_store(memarg, ir::Opcode::Istore16, builder, state, environ)?;
        }
        Operator::I64Store32 { memarg } => {
            translate_store(memarg, ir::Opcode::Istore32, builder, state, environ)?;
        }
        Operator::V128Store { memarg } => {
            translate_store(memarg, ir::Opcode::Store, builder, state, environ)?;
        }
        /****************************** Nullary Operators ************************************/
        Operator::I32Const { value } => state.push1(builder.ins().iconst(I32, i64::from(*value))),
        Operator::I64Const { value } => state.push1(builder.ins().iconst(I64, *value)),
        Operator::F32Const { value } => {
            state.push1(builder.ins().f32const(f32_translation(*value)));
        }
        Operator::F64Const { value } => {
            state.push1(builder.ins().f64const(f64_translation(*value)));
        }
        /******************************* Unary Operators *************************************/
        Operator::I32Clz | Operator::I64Clz => {
            let (arg, _) = state.pop1();
            state.push1(builder.ins().clz(arg));
        }
        Operator::I32Ctz | Operator::I64Ctz => {
            let (arg, _) = state.pop1();
            state.push1(builder.ins().ctz(arg));
        }
        Operator::I32Popcnt | Operator::I64Popcnt => {
            let (arg, _) = state.pop1();
            state.push1(builder.ins().popcnt(arg));
        }
        Operator::I64ExtendI32S => {
            let (val, _) = state.pop1();
            state.push1(builder.ins().sextend(I64, val));
        }
        Operator::I64ExtendI32U => {
            let (val, _) = state.pop1();
            state.push1(builder.ins().uextend(I64, val));
        }
        Operator::I32WrapI64 => {
            let (val, _) = state.pop1();
            state.push1(builder.ins().ireduce(I32, val));
        }
        Operator::F32Sqrt | Operator::F64Sqrt => {
            let (arg, _) = state.pop1();
            state.push1(builder.ins().sqrt(arg));
        }
        Operator::F32Ceil | Operator::F64Ceil => {
            let (arg, _) = state.pop1();
            state.push1(builder.ins().ceil(arg));
        }
        Operator::F32Floor | Operator::F64Floor => {
            let (arg, _) = state.pop1();
            state.push1(builder.ins().floor(arg));
        }
        Operator::F32Trunc | Operator::F64Trunc => {
            let (arg, _) = state.pop1();
            state.push1(builder.ins().trunc(arg));
        }
        Operator::F32Nearest | Operator::F64Nearest => {
            let (arg, _) = state.pop1();
            state.push1(builder.ins().nearest(arg));
        }
        Operator::F32Abs | Operator::F64Abs => {
            let (val, _) = state.pop1();
            state.push1(builder.ins().fabs(val));
        }
        Operator::F32Neg | Operator::F64Neg => {
            let (arg, _) = state.pop1();
            state.push1(builder.ins().fneg(arg));
        }
        Operator::F64ConvertI64U | Operator::F64ConvertI32U => {
            let (val, _) = state.pop1();
            state.push1(builder.ins().fcvt_from_uint(F64, val));
        }
        Operator::F64ConvertI64S | Operator::F64ConvertI32S => {
            let (val, _) = state.pop1();
            state.push1(builder.ins().fcvt_from_sint(F64, val));
        }
        Operator::F32ConvertI64S | Operator::F32ConvertI32S => {
            let (val, _) = state.pop1();
            state.push1(builder.ins().fcvt_from_sint(F32, val));
        }
        Operator::F32ConvertI64U | Operator::F32ConvertI32U => {
            let (val, _) = state.pop1();
            state.push1(builder.ins().fcvt_from_uint(F32, val));
        }
        Operator::F64PromoteF32 => {
            let (val, _) = state.pop1();
            state.push1(builder.ins().fpromote(F64, val));
        }
        Operator::F32DemoteF64 => {
            let (val, _) = state.pop1();
            state.push1(builder.ins().fdemote(F32, val));
        }
        Operator::I64TruncF64S | Operator::I64TruncF32S => {
            let (val, _) = state.pop1();
            state.push1(builder.ins().fcvt_to_sint(I64, val));
        }
        Operator::I32TruncF64S | Operator::I32TruncF32S => {
            let (val, _) = state.pop1();
            state.push1(builder.ins().fcvt_to_sint(I32, val));
        }
        Operator::I64TruncF64U | Operator::I64TruncF32U => {
            let (val, _) = state.pop1();
            state.push1(builder.ins().fcvt_to_uint(I64, val));
        }
        Operator::I32TruncF64U | Operator::I32TruncF32U => {
            let (val, _) = state.pop1();
            state.push1(builder.ins().fcvt_to_uint(I32, val));
        }
        Operator::I64TruncSatF64S | Operator::I64TruncSatF32S => {
            let (val, _) = state.pop1();
            state.push1(builder.ins().fcvt_to_sint_sat(I64, val));
        }
        Operator::I32TruncSatF64S | Operator::I32TruncSatF32S => {
            let (val, _) = state.pop1();
            state.push1(builder.ins().fcvt_to_sint_sat(I32, val));
        }
        Operator::I64TruncSatF64U | Operator::I64TruncSatF32U => {
            let (val, _) = state.pop1();
            state.push1(builder.ins().fcvt_to_uint_sat(I64, val));
        }
        Operator::I32TruncSatF64U | Operator::I32TruncSatF32U => {
            let (val, _) = state.pop1();
            state.push1(builder.ins().fcvt_to_uint_sat(I32, val));
        }
        Operator::F32ReinterpretI32 => {
            let (val, _) = state.pop1();
            state.push1(builder.ins().bitcast(F32, val));
        }
        Operator::F64ReinterpretI64 => {
            let (val, _) = state.pop1();
            state.push1(builder.ins().bitcast(F64, val));
        }
        Operator::I32ReinterpretF32 => {
            let (val, _) = state.pop1();
            state.push1(builder.ins().bitcast(I32, val));
        }
        Operator::I64ReinterpretF64 => {
            let (val, _) = state.pop1();
            state.push1(builder.ins().bitcast(I64, val));
        }
        Operator::I32Extend8S => {
            let (val, _) = state.pop1();
            state.push1(builder.ins().ireduce(I8, val));
            let (val, _) = state.pop1();
            state.push1(builder.ins().sextend(I32, val));
        }
        Operator::I32Extend16S => {
            let (val, _) = state.pop1();
            state.push1(builder.ins().ireduce(I16, val));
            let (val, _) = state.pop1();
            state.push1(builder.ins().sextend(I32, val));
        }
        Operator::I64Extend8S => {
            let (val, _) = state.pop1();
            state.push1(builder.ins().ireduce(I8, val));
            let (val, _) = state.pop1();
            state.push1(builder.ins().sextend(I64, val));
        }
        Operator::I64Extend16S => {
            let (val, _) = state.pop1();
            state.push1(builder.ins().ireduce(I16, val));
            let (val, _) = state.pop1();
            state.push1(builder.ins().sextend(I64, val));
        }
        Operator::I64Extend32S => {
            let (val, _) = state.pop1();
            state.push1(builder.ins().ireduce(I32, val));
            let (val, _) = state.pop1();
            state.push1(builder.ins().sextend(I64, val));
        }
        /****************************** Binary Operators ************************************/
        Operator::I32Add | Operator::I64Add => {
            let ((arg1, _), (arg2, _)) = state.pop2();
            state.push1(builder.ins().iadd(arg1, arg2));
        }
        Operator::I32And | Operator::I64And => {
            let ((arg1, _), (arg2, _)) = state.pop2();
            state.push1(builder.ins().band(arg1, arg2));
        }
        Operator::I32Or | Operator::I64Or => {
            let ((arg1, _), (arg2, _)) = state.pop2();
            state.push1(builder.ins().bor(arg1, arg2));
        }
        Operator::I32Xor | Operator::I64Xor => {
            let ((arg1, _), (arg2, _)) = state.pop2();
            state.push1(builder.ins().bxor(arg1, arg2));
        }
        Operator::I32Shl | Operator::I64Shl => {
            let ((arg1, _), (arg2, _)) = state.pop2();
            state.push1(builder.ins().ishl(arg1, arg2));
        }
        Operator::I32ShrS | Operator::I64ShrS => {
            let ((arg1, _), (arg2, _)) = state.pop2();
            state.push1(builder.ins().sshr(arg1, arg2));
        }
        Operator::I32ShrU | Operator::I64ShrU => {
            let ((arg1, _), (arg2, _)) = state.pop2();
            state.push1(builder.ins().ushr(arg1, arg2));
        }
        Operator::I32Rotl | Operator::I64Rotl => {
            let ((arg1, _), (arg2, _)) = state.pop2();
            state.push1(builder.ins().rotl(arg1, arg2));
        }
        Operator::I32Rotr | Operator::I64Rotr => {
            let ((arg1, _), (arg2, _)) = state.pop2();
            state.push1(builder.ins().rotr(arg1, arg2));
        }
        Operator::F32Add | Operator::F64Add => {
            let ((arg1, _), (arg2, _)) = state.pop2();
            state.push1(builder.ins().fadd(arg1, arg2));
        }
        Operator::I32Sub | Operator::I64Sub => {
            let ((arg1, _), (arg2, _)) = state.pop2();
            state.push1(builder.ins().isub(arg1, arg2));
        }
        Operator::F32Sub | Operator::F64Sub => {
            let ((arg1, _), (arg2, _)) = state.pop2();
            state.push1(builder.ins().fsub(arg1, arg2));
        }
        Operator::I32Mul | Operator::I64Mul => {
            let ((arg1, _), (arg2, _)) = state.pop2();
            state.push1(builder.ins().imul(arg1, arg2));
        }
        Operator::F32Mul | Operator::F64Mul => {
            let ((arg1, _), (arg2, _)) = state.pop2();
            state.push1(builder.ins().fmul(arg1, arg2));
        }
        Operator::F32Div | Operator::F64Div => {
            let ((arg1, _), (arg2, _)) = state.pop2();
            state.push1(builder.ins().fdiv(arg1, arg2));
        }
        Operator::I32DivS | Operator::I64DivS => {
            let ((arg1, _), (arg2, _)) = state.pop2();
            state.push1(builder.ins().sdiv(arg1, arg2));
        }
        Operator::I32DivU | Operator::I64DivU => {
            let ((arg1, _), (arg2, _)) = state.pop2();
            state.push1(builder.ins().udiv(arg1, arg2));
        }
        Operator::I32RemS | Operator::I64RemS => {
            let ((arg1, _), (arg2, _)) = state.pop2();
            state.push1(builder.ins().srem(arg1, arg2));
        }
        Operator::I32RemU | Operator::I64RemU => {
            let ((arg1, _), (arg2, _)) = state.pop2();
            state.push1(builder.ins().urem(arg1, arg2));
        }
        Operator::F32Min | Operator::F64Min => {
            let ((arg1, _), (arg2, _)) = state.pop2();
            state.push1(builder.ins().fmin(arg1, arg2));
        }
        Operator::F32Max | Operator::F64Max => {
            let ((arg1, _), (arg2, _)) = state.pop2();
            state.push1(builder.ins().fmax(arg1, arg2));
        }
        Operator::F32Copysign | Operator::F64Copysign => {
            let ((arg1, _), (arg2, _)) = state.pop2();
            state.push1(builder.ins().fcopysign(arg1, arg2));
        }
        /**************************** Comparison Operators **********************************/
        Operator::I32LtS | Operator::I64LtS => {
            translate_icmp(IntCC::SignedLessThan, builder, state)
        }
        Operator::I32LtU | Operator::I64LtU => {
            translate_icmp(IntCC::UnsignedLessThan, builder, state)
        }
        Operator::I32LeS | Operator::I64LeS => {
            translate_icmp(IntCC::SignedLessThanOrEqual, builder, state)
        }
        Operator::I32LeU | Operator::I64LeU => {
            translate_icmp(IntCC::UnsignedLessThanOrEqual, builder, state)
        }
        Operator::I32GtS | Operator::I64GtS => {
            translate_icmp(IntCC::SignedGreaterThan, builder, state)
        }
        Operator::I32GtU | Operator::I64GtU => {
            translate_icmp(IntCC::UnsignedGreaterThan, builder, state)
        }
        Operator::I32GeS | Operator::I64GeS => {
            translate_icmp(IntCC::SignedGreaterThanOrEqual, builder, state)
        }
        Operator::I32GeU | Operator::I64GeU => {
            translate_icmp(IntCC::UnsignedGreaterThanOrEqual, builder, state)
        }
        Operator::I32Eqz | Operator::I64Eqz => {
            let (arg, _) = state.pop1();
            let val = builder.ins().icmp_imm(IntCC::Equal, arg, 0);
            state.push1(builder.ins().bint(I32, val));
        }
        Operator::I32Eq | Operator::I64Eq => translate_icmp(IntCC::Equal, builder, state),
        Operator::F32Eq | Operator::F64Eq => translate_fcmp(FloatCC::Equal, builder, state),
        Operator::I32Ne | Operator::I64Ne => translate_icmp(IntCC::NotEqual, builder, state),
        Operator::F32Ne | Operator::F64Ne => translate_fcmp(FloatCC::NotEqual, builder, state),
        Operator::F32Gt | Operator::F64Gt => translate_fcmp(FloatCC::GreaterThan, builder, state),
        Operator::F32Ge | Operator::F64Ge => {
            translate_fcmp(FloatCC::GreaterThanOrEqual, builder, state)
        }
        Operator::F32Lt | Operator::F64Lt => translate_fcmp(FloatCC::LessThan, builder, state),
        Operator::F32Le | Operator::F64Le => {
            translate_fcmp(FloatCC::LessThanOrEqual, builder, state)
        }
        Operator::RefNull { ty } => state.push1(environ.translate_ref_null(builder.cursor(), *ty)?),
        Operator::RefIsNull => {
            let (value, _) = state.pop1();
            state.push1(environ.translate_ref_is_null(builder.cursor(), value)?);
        }
        Operator::RefFunc { function_index } => {
            let index = FunctionIndex::from_u32(*function_index);
            state.push1(environ.translate_ref_func(builder.cursor(), index)?);
        }
        Operator::MemoryAtomicWait32 { memarg } | Operator::MemoryAtomicWait64 { memarg } => {
            // The WebAssembly MVP only supports one linear memory and
            // wasmparser will ensure that the memory indices specified are
            // zero.
            let implied_ty = match op {
                Operator::MemoryAtomicWait64 { .. } => I64,
                Operator::MemoryAtomicWait32 { .. } => I32,
                _ => unreachable!(),
            };
            let heap_index = MemoryIndex::from_u32(memarg.memory);
            let heap = state.get_heap(builder.func, memarg.memory, environ)?;
            let (timeout, _) = state.pop1(); // 64 (fixed)
            let (expected, _) = state.pop1(); // 32 or 64 (per the `Ixx` in `IxxAtomicWait`)
            let (addr, _) = state.pop1(); // 32 (fixed)
            let addr = fold_atomic_mem_addr(addr, memarg, implied_ty, builder);
            assert!(builder.func.dfg.value_type(expected) == implied_ty);
            // `fn translate_atomic_wait` can inspect the type of `expected` to figure out what
            // code it needs to generate, if it wants.
            let res = environ.translate_atomic_wait(
                builder.cursor(),
                heap_index,
                heap,
                addr,
                expected,
                timeout,
            )?;
            state.push1(res);
        }
        Operator::MemoryAtomicNotify { memarg } => {
            let heap_index = MemoryIndex::from_u32(memarg.memory);
            let heap = state.get_heap(builder.func, memarg.memory, environ)?;
            let (count, _) = state.pop1(); // 32 (fixed)
            let (addr, _) = state.pop1(); // 32 (fixed)
            let addr = fold_atomic_mem_addr(addr, memarg, I32, builder);
            let res =
                environ.translate_atomic_notify(builder.cursor(), heap_index, heap, addr, count)?;
            state.push1(res);
        }
        Operator::I32AtomicLoad { memarg } => {
            translate_atomic_load(I32, I32, memarg, builder, state, environ)?
        }
        Operator::I64AtomicLoad { memarg } => {
            translate_atomic_load(I64, I64, memarg, builder, state, environ)?
        }
        Operator::I32AtomicLoad8U { memarg } => {
            translate_atomic_load(I32, I8, memarg, builder, state, environ)?
        }
        Operator::I32AtomicLoad16U { memarg } => {
            translate_atomic_load(I32, I16, memarg, builder, state, environ)?
        }
        Operator::I64AtomicLoad8U { memarg } => {
            translate_atomic_load(I64, I8, memarg, builder, state, environ)?
        }
        Operator::I64AtomicLoad16U { memarg } => {
            translate_atomic_load(I64, I16, memarg, builder, state, environ)?
        }
        Operator::I64AtomicLoad32U { memarg } => {
            translate_atomic_load(I64, I32, memarg, builder, state, environ)?
        }

        Operator::I32AtomicStore { memarg } => {
            translate_atomic_store(I32, memarg, builder, state, environ)?
        }
        Operator::I64AtomicStore { memarg } => {
            translate_atomic_store(I64, memarg, builder, state, environ)?
        }
        Operator::I32AtomicStore8 { memarg } => {
            translate_atomic_store(I8, memarg, builder, state, environ)?
        }
        Operator::I32AtomicStore16 { memarg } => {
            translate_atomic_store(I16, memarg, builder, state, environ)?
        }
        Operator::I64AtomicStore8 { memarg } => {
            translate_atomic_store(I8, memarg, builder, state, environ)?
        }
        Operator::I64AtomicStore16 { memarg } => {
            translate_atomic_store(I16, memarg, builder, state, environ)?
        }
        Operator::I64AtomicStore32 { memarg } => {
            translate_atomic_store(I32, memarg, builder, state, environ)?
        }

        Operator::I32AtomicRmwAdd { memarg } => {
            translate_atomic_rmw(I32, I32, AtomicRmwOp::Add, memarg, builder, state, environ)?
        }
        Operator::I64AtomicRmwAdd { memarg } => {
            translate_atomic_rmw(I64, I64, AtomicRmwOp::Add, memarg, builder, state, environ)?
        }
        Operator::I32AtomicRmw8AddU { memarg } => {
            translate_atomic_rmw(I32, I8, AtomicRmwOp::Add, memarg, builder, state, environ)?
        }
        Operator::I32AtomicRmw16AddU { memarg } => {
            translate_atomic_rmw(I32, I16, AtomicRmwOp::Add, memarg, builder, state, environ)?
        }
        Operator::I64AtomicRmw8AddU { memarg } => {
            translate_atomic_rmw(I64, I8, AtomicRmwOp::Add, memarg, builder, state, environ)?
        }
        Operator::I64AtomicRmw16AddU { memarg } => {
            translate_atomic_rmw(I64, I16, AtomicRmwOp::Add, memarg, builder, state, environ)?
        }
        Operator::I64AtomicRmw32AddU { memarg } => {
            translate_atomic_rmw(I64, I32, AtomicRmwOp::Add, memarg, builder, state, environ)?
        }

        Operator::I32AtomicRmwSub { memarg } => {
            translate_atomic_rmw(I32, I32, AtomicRmwOp::Sub, memarg, builder, state, environ)?
        }
        Operator::I64AtomicRmwSub { memarg } => {
            translate_atomic_rmw(I64, I64, AtomicRmwOp::Sub, memarg, builder, state, environ)?
        }
        Operator::I32AtomicRmw8SubU { memarg } => {
            translate_atomic_rmw(I32, I8, AtomicRmwOp::Sub, memarg, builder, state, environ)?
        }
        Operator::I32AtomicRmw16SubU { memarg } => {
            translate_atomic_rmw(I32, I16, AtomicRmwOp::Sub, memarg, builder, state, environ)?
        }
        Operator::I64AtomicRmw8SubU { memarg } => {
            translate_atomic_rmw(I64, I8, AtomicRmwOp::Sub, memarg, builder, state, environ)?
        }
        Operator::I64AtomicRmw16SubU { memarg } => {
            translate_atomic_rmw(I64, I16, AtomicRmwOp::Sub, memarg, builder, state, environ)?
        }
        Operator::I64AtomicRmw32SubU { memarg } => {
            translate_atomic_rmw(I64, I32, AtomicRmwOp::Sub, memarg, builder, state, environ)?
        }

        Operator::I32AtomicRmwAnd { memarg } => {
            translate_atomic_rmw(I32, I32, AtomicRmwOp::And, memarg, builder, state, environ)?
        }
        Operator::I64AtomicRmwAnd { memarg } => {
            translate_atomic_rmw(I64, I64, AtomicRmwOp::And, memarg, builder, state, environ)?
        }
        Operator::I32AtomicRmw8AndU { memarg } => {
            translate_atomic_rmw(I32, I8, AtomicRmwOp::And, memarg, builder, state, environ)?
        }
        Operator::I32AtomicRmw16AndU { memarg } => {
            translate_atomic_rmw(I32, I16, AtomicRmwOp::And, memarg, builder, state, environ)?
        }
        Operator::I64AtomicRmw8AndU { memarg } => {
            translate_atomic_rmw(I64, I8, AtomicRmwOp::And, memarg, builder, state, environ)?
        }
        Operator::I64AtomicRmw16AndU { memarg } => {
            translate_atomic_rmw(I64, I16, AtomicRmwOp::And, memarg, builder, state, environ)?
        }
        Operator::I64AtomicRmw32AndU { memarg } => {
            translate_atomic_rmw(I64, I32, AtomicRmwOp::And, memarg, builder, state, environ)?
        }

        Operator::I32AtomicRmwOr { memarg } => {
            translate_atomic_rmw(I32, I32, AtomicRmwOp::Or, memarg, builder, state, environ)?
        }
        Operator::I64AtomicRmwOr { memarg } => {
            translate_atomic_rmw(I64, I64, AtomicRmwOp::Or, memarg, builder, state, environ)?
        }
        Operator::I32AtomicRmw8OrU { memarg } => {
            translate_atomic_rmw(I32, I8, AtomicRmwOp::Or, memarg, builder, state, environ)?
        }
        Operator::I32AtomicRmw16OrU { memarg } => {
            translate_atomic_rmw(I32, I16, AtomicRmwOp::Or, memarg, builder, state, environ)?
        }
        Operator::I64AtomicRmw8OrU { memarg } => {
            translate_atomic_rmw(I64, I8, AtomicRmwOp::Or, memarg, builder, state, environ)?
        }
        Operator::I64AtomicRmw16OrU { memarg } => {
            translate_atomic_rmw(I64, I16, AtomicRmwOp::Or, memarg, builder, state, environ)?
        }
        Operator::I64AtomicRmw32OrU { memarg } => {
            translate_atomic_rmw(I64, I32, AtomicRmwOp::Or, memarg, builder, state, environ)?
        }

        Operator::I32AtomicRmwXor { memarg } => {
            translate_atomic_rmw(I32, I32, AtomicRmwOp::Xor, memarg, builder, state, environ)?
        }
        Operator::I64AtomicRmwXor { memarg } => {
            translate_atomic_rmw(I64, I64, AtomicRmwOp::Xor, memarg, builder, state, environ)?
        }
        Operator::I32AtomicRmw8XorU { memarg } => {
            translate_atomic_rmw(I32, I8, AtomicRmwOp::Xor, memarg, builder, state, environ)?
        }
        Operator::I32AtomicRmw16XorU { memarg } => {
            translate_atomic_rmw(I32, I16, AtomicRmwOp::Xor, memarg, builder, state, environ)?
        }
        Operator::I64AtomicRmw8XorU { memarg } => {
            translate_atomic_rmw(I64, I8, AtomicRmwOp::Xor, memarg, builder, state, environ)?
        }
        Operator::I64AtomicRmw16XorU { memarg } => {
            translate_atomic_rmw(I64, I16, AtomicRmwOp::Xor, memarg, builder, state, environ)?
        }
        Operator::I64AtomicRmw32XorU { memarg } => {
            translate_atomic_rmw(I64, I32, AtomicRmwOp::Xor, memarg, builder, state, environ)?
        }

        Operator::I32AtomicRmwXchg { memarg } => {
            translate_atomic_rmw(I32, I32, AtomicRmwOp::Xchg, memarg, builder, state, environ)?
        }
        Operator::I64AtomicRmwXchg { memarg } => {
            translate_atomic_rmw(I64, I64, AtomicRmwOp::Xchg, memarg, builder, state, environ)?
        }
        Operator::I32AtomicRmw8XchgU { memarg } => {
            translate_atomic_rmw(I32, I8, AtomicRmwOp::Xchg, memarg, builder, state, environ)?
        }
        Operator::I32AtomicRmw16XchgU { memarg } => {
            translate_atomic_rmw(I32, I16, AtomicRmwOp::Xchg, memarg, builder, state, environ)?
        }
        Operator::I64AtomicRmw8XchgU { memarg } => {
            translate_atomic_rmw(I64, I8, AtomicRmwOp::Xchg, memarg, builder, state, environ)?
        }
        Operator::I64AtomicRmw16XchgU { memarg } => {
            translate_atomic_rmw(I64, I16, AtomicRmwOp::Xchg, memarg, builder, state, environ)?
        }
        Operator::I64AtomicRmw32XchgU { memarg } => {
            translate_atomic_rmw(I64, I32, AtomicRmwOp::Xchg, memarg, builder, state, environ)?
        }

        Operator::I32AtomicRmwCmpxchg { memarg } => {
            translate_atomic_cas(I32, I32, memarg, builder, state, environ)?
        }
        Operator::I64AtomicRmwCmpxchg { memarg } => {
            translate_atomic_cas(I64, I64, memarg, builder, state, environ)?
        }
        Operator::I32AtomicRmw8CmpxchgU { memarg } => {
            translate_atomic_cas(I32, I8, memarg, builder, state, environ)?
        }
        Operator::I32AtomicRmw16CmpxchgU { memarg } => {
            translate_atomic_cas(I32, I16, memarg, builder, state, environ)?
        }
        Operator::I64AtomicRmw8CmpxchgU { memarg } => {
            translate_atomic_cas(I64, I8, memarg, builder, state, environ)?
        }
        Operator::I64AtomicRmw16CmpxchgU { memarg } => {
            translate_atomic_cas(I64, I16, memarg, builder, state, environ)?
        }
        Operator::I64AtomicRmw32CmpxchgU { memarg } => {
            translate_atomic_cas(I64, I32, memarg, builder, state, environ)?
        }

        Operator::AtomicFence { .. } => {
            builder.ins().fence();
        }
        Operator::MemoryCopy { src, dst } => {
            let src_index = MemoryIndex::from_u32(*src);
            let dst_index = MemoryIndex::from_u32(*dst);
            let src_heap = state.get_heap(builder.func, *src, environ)?;
            let dst_heap = state.get_heap(builder.func, *dst, environ)?;
            let (len, _) = state.pop1();
            let (src_pos, _) = state.pop1();
            let (dst_pos, _) = state.pop1();
            environ.translate_memory_copy(
                builder.cursor(),
                src_index,
                src_heap,
                dst_index,
                dst_heap,
                dst_pos,
                src_pos,
                len,
            )?;
        }
        Operator::MemoryFill { mem } => {
            let heap_index = MemoryIndex::from_u32(*mem);
            let heap = state.get_heap(builder.func, *mem, environ)?;
            let (len, _) = state.pop1();
            let (val, _) = state.pop1();
            let (dest, _) = state.pop1();
            environ.translate_memory_fill(builder.cursor(), heap_index, heap, dest, val, len)?;
        }
        Operator::MemoryInit { segment, mem } => {
            let heap_index = MemoryIndex::from_u32(*mem);
            let heap = state.get_heap(builder.func, *mem, environ)?;
            let (len, _) = state.pop1();
            let (src, _) = state.pop1();
            let (dest, _) = state.pop1();
            environ.translate_memory_init(
                builder.cursor(),
                heap_index,
                heap,
                *segment,
                dest,
                src,
                len,
            )?;
        }
        Operator::DataDrop { segment } => {
            environ.translate_data_drop(builder.cursor(), *segment)?;
        }
        Operator::TableSize { table: index } => {
            let table = state.get_or_create_table(builder.func, *index, environ)?;
            state.push1(environ.translate_table_size(
                builder.cursor(),
                TableIndex::from_u32(*index),
                table,
            )?);
        }
        Operator::TableGrow { table: index } => {
            let table_index = TableIndex::from_u32(*index);
            let table = state.get_or_create_table(builder.func, *index, environ)?;
            let (delta, _) = state.pop1();
            let (init_value, _) = state.pop1();
            state.push1(environ.translate_table_grow(
                builder.cursor(),
                table_index,
                table,
                delta,
                init_value,
            )?);
        }
        Operator::TableGet { table: index } => {
            let table_index = TableIndex::from_u32(*index);
            let table = state.get_or_create_table(builder.func, *index, environ)?;
            let (index, _) = state.pop1();
            state.push1(environ.translate_table_get(builder, table_index, table, index)?);
        }
        Operator::TableSet { table: index } => {
            let table_index = TableIndex::from_u32(*index);
            let table = state.get_or_create_table(builder.func, *index, environ)?;
            // We don't touch the ref count here because we're passing it to the host
            // then dropping it from the stack. Thus 1 + -1 = 0.
            let (value, _) = state.pop1();
            let (index, _) = state.pop1();
            environ.translate_table_set(builder, table_index, table, value, index)?;
        }
        Operator::TableCopy {
            dst_table: dst_table_index,
            src_table: src_table_index,
        } => {
            let dst_table = state.get_or_create_table(builder.func, *dst_table_index, environ)?;
            let src_table = state.get_or_create_table(builder.func, *src_table_index, environ)?;
            let (len, _) = state.pop1();
            let (src, _) = state.pop1();
            let (dest, _) = state.pop1();
            environ.translate_table_copy(
                builder.cursor(),
                TableIndex::from_u32(*dst_table_index),
                dst_table,
                TableIndex::from_u32(*src_table_index),
                src_table,
                dest,
                src,
                len,
            )?;
        }
        Operator::TableFill { table } => {
            let table_index = TableIndex::from_u32(*table);
            let (len, _) = state.pop1();
            let (val, _) = state.pop1();
            let (dest, _) = state.pop1();
            environ.translate_table_fill(builder.cursor(), table_index, dest, val, len)?;
        }
        Operator::TableInit {
            segment,
            table: table_index,
        } => {
            let table = state.get_or_create_table(builder.func, *table_index, environ)?;
            let (len, _) = state.pop1();
            let (src, _) = state.pop1();
            let (dest, _) = state.pop1();
            environ.translate_table_init(
                builder.cursor(),
                *segment,
                TableIndex::from_u32(*table_index),
                table,
                dest,
                src,
                len,
            )?;
        }
        Operator::ElemDrop { segment } => {
            environ.translate_elem_drop(builder.cursor(), *segment)?;
        }
        Operator::V128Const { value } => {
            let data = value.bytes().to_vec().into();
            let handle = builder.func.dfg.constants.insert(data);
            let value = builder.ins().vconst(I8X16, handle);
            // the v128.const is typed in CLIF as a I8x16 but raw_bitcast to a different type
            // before use
            state.push1(value)
        }
        Operator::I8x16Splat | Operator::I16x8Splat => {
            let reduced = builder
                .ins()
                .ireduce(type_of(op).lane_type(), state.pop1().0);
            let splatted = builder.ins().splat(type_of(op), reduced);
            state.push1(splatted)
        }
        Operator::I32x4Splat
        | Operator::I64x2Splat
        | Operator::F32x4Splat
        | Operator::F64x2Splat => {
            let splatted = builder.ins().splat(type_of(op), state.pop1().0);
            state.push1(splatted)
        }
        Operator::V128Load8Splat { memarg }
        | Operator::V128Load16Splat { memarg }
        | Operator::V128Load32Splat { memarg }
        | Operator::V128Load64Splat { memarg } => {
            translate_load(
                memarg,
                ir::Opcode::Load,
                type_of(op).lane_type(),
                builder,
                state,
                environ,
            )?;
            let splatted = builder.ins().splat(type_of(op), state.pop1().0);
            state.push1(splatted)
        }
        Operator::V128Load32Zero { memarg } | Operator::V128Load64Zero { memarg } => {
            translate_load(
                memarg,
                ir::Opcode::Load,
                type_of(op).lane_type(),
                builder,
                state,
                environ,
            )?;
            let as_vector = builder.ins().scalar_to_vector(type_of(op), state.pop1().0);
            state.push1(as_vector)
        }
        Operator::V128Load8Lane { memarg, lane }
        | Operator::V128Load16Lane { memarg, lane }
        | Operator::V128Load32Lane { memarg, lane }
        | Operator::V128Load64Lane { memarg, lane } => {
            let vector = pop1_with_bitcast(state, type_of(op), builder);
            translate_load(
                memarg,
                ir::Opcode::Load,
                type_of(op).lane_type(),
                builder,
                state,
                environ,
            )?;
            let replacement = state.pop1().0;
            state.push1(builder.ins().insertlane(vector, replacement, *lane))
        }
        Operator::V128Store8Lane { memarg, lane }
        | Operator::V128Store16Lane { memarg, lane }
        | Operator::V128Store32Lane { memarg, lane }
        | Operator::V128Store64Lane { memarg, lane } => {
            let vector = pop1_with_bitcast(state, type_of(op), builder);
            state.push1(builder.ins().extractlane(vector, lane.clone()));
            translate_store(memarg, ir::Opcode::Store, builder, state, environ)?;
        }
        Operator::I8x16ExtractLaneS { lane } | Operator::I16x8ExtractLaneS { lane } => {
            let vector = pop1_with_bitcast(state, type_of(op), builder);
            let extracted = builder.ins().extractlane(vector, lane.clone());
            state.push1(builder.ins().sextend(I32, extracted))
        }
        Operator::I8x16ExtractLaneU { lane } | Operator::I16x8ExtractLaneU { lane } => {
            let vector = pop1_with_bitcast(state, type_of(op), builder);
            let extracted = builder.ins().extractlane(vector, lane.clone());
            state.push1(builder.ins().uextend(I32, extracted));
            // On x86, PEXTRB zeroes the upper bits of the destination register of extractlane so
            // uextend could be elided; for now, uextend is needed for Cranelift's type checks to
            // work.
        }
        Operator::I32x4ExtractLane { lane }
        | Operator::I64x2ExtractLane { lane }
        | Operator::F32x4ExtractLane { lane }
        | Operator::F64x2ExtractLane { lane } => {
            let vector = pop1_with_bitcast(state, type_of(op), builder);
            state.push1(builder.ins().extractlane(vector, lane.clone()))
        }
        Operator::I8x16ReplaceLane { lane } | Operator::I16x8ReplaceLane { lane } => {
            let ((vector, _), (replacement, _)) = state.pop2();
            let ty = type_of(op);
            let reduced = builder.ins().ireduce(ty.lane_type(), replacement);
            let vector = optionally_bitcast_vector(vector, ty, builder);
            state.push1(builder.ins().insertlane(vector, reduced, *lane))
        }
        Operator::I32x4ReplaceLane { lane }
        | Operator::I64x2ReplaceLane { lane }
        | Operator::F32x4ReplaceLane { lane }
        | Operator::F64x2ReplaceLane { lane } => {
            let ((vector, _), (replacement, _)) = state.pop2();
            let vector = optionally_bitcast_vector(vector, type_of(op), builder);
            state.push1(builder.ins().insertlane(vector, replacement, *lane))
        }
        Operator::I8x16Shuffle { lanes, .. } => {
            let (a, b) = pop2_with_bitcast(state, I8X16, builder);
            let lanes = ConstantData::from(lanes.as_ref());
            let mask = builder.func.dfg.immediates.push(lanes);
            let shuffled = builder.ins().shuffle(a, b, mask);
            state.push1(shuffled)
            // At this point the original types of a and b are lost; users of this value (i.e. this
            // WASM-to-CLIF translator) may need to raw_bitcast for type-correctness. This is due
            // to WASM using the less specific v128 type for certain operations and more specific
            // types (e.g. i8x16) for others.
        }
        Operator::I8x16Swizzle => {
            let (a, b) = pop2_with_bitcast(state, I8X16, builder);
            state.push1(builder.ins().swizzle(I8X16, a, b))
        }
        Operator::I8x16Add | Operator::I16x8Add | Operator::I32x4Add | Operator::I64x2Add => {
            let (a, b) = pop2_with_bitcast(state, type_of(op), builder);
            state.push1(builder.ins().iadd(a, b))
        }
        Operator::I8x16AddSatS | Operator::I16x8AddSatS => {
            let (a, b) = pop2_with_bitcast(state, type_of(op), builder);
            state.push1(builder.ins().sadd_sat(a, b))
        }
        Operator::I8x16AddSatU | Operator::I16x8AddSatU => {
            let (a, b) = pop2_with_bitcast(state, type_of(op), builder);
            state.push1(builder.ins().uadd_sat(a, b))
        }
        Operator::I8x16Sub | Operator::I16x8Sub | Operator::I32x4Sub | Operator::I64x2Sub => {
            let (a, b) = pop2_with_bitcast(state, type_of(op), builder);
            state.push1(builder.ins().isub(a, b))
        }
        Operator::I8x16SubSatS | Operator::I16x8SubSatS => {
            let (a, b) = pop2_with_bitcast(state, type_of(op), builder);
            state.push1(builder.ins().ssub_sat(a, b))
        }
        Operator::I8x16SubSatU | Operator::I16x8SubSatU => {
            let (a, b) = pop2_with_bitcast(state, type_of(op), builder);
            state.push1(builder.ins().usub_sat(a, b))
        }
        Operator::I8x16MinS | Operator::I16x8MinS | Operator::I32x4MinS => {
            let (a, b) = pop2_with_bitcast(state, type_of(op), builder);
            state.push1(builder.ins().imin(a, b))
        }
        Operator::I8x16MinU | Operator::I16x8MinU | Operator::I32x4MinU => {
            let (a, b) = pop2_with_bitcast(state, type_of(op), builder);
            state.push1(builder.ins().umin(a, b))
        }
        Operator::I8x16MaxS | Operator::I16x8MaxS | Operator::I32x4MaxS => {
            let (a, b) = pop2_with_bitcast(state, type_of(op), builder);
            state.push1(builder.ins().imax(a, b))
        }
        Operator::I8x16MaxU | Operator::I16x8MaxU | Operator::I32x4MaxU => {
            let (a, b) = pop2_with_bitcast(state, type_of(op), builder);
            state.push1(builder.ins().umax(a, b))
        }
        Operator::I8x16RoundingAverageU | Operator::I16x8RoundingAverageU => {
            let (a, b) = pop2_with_bitcast(state, type_of(op), builder);
            state.push1(builder.ins().avg_round(a, b))
        }
        Operator::I8x16Neg | Operator::I16x8Neg | Operator::I32x4Neg | Operator::I64x2Neg => {
            let a = pop1_with_bitcast(state, type_of(op), builder);
            state.push1(builder.ins().ineg(a))
        }
        Operator::I8x16Abs | Operator::I16x8Abs | Operator::I32x4Abs | Operator::I64x2Abs => {
            let a = pop1_with_bitcast(state, type_of(op), builder);
            state.push1(builder.ins().iabs(a))
        }
        Operator::I16x8Mul | Operator::I32x4Mul | Operator::I64x2Mul => {
            let (a, b) = pop2_with_bitcast(state, type_of(op), builder);
            state.push1(builder.ins().imul(a, b))
        }
        Operator::V128Or => {
            let (a, b) = pop2_with_bitcast(state, type_of(op), builder);
            state.push1(builder.ins().bor(a, b))
        }
        Operator::V128Xor => {
            let (a, b) = pop2_with_bitcast(state, type_of(op), builder);
            state.push1(builder.ins().bxor(a, b))
        }
        Operator::V128And => {
            let (a, b) = pop2_with_bitcast(state, type_of(op), builder);
            state.push1(builder.ins().band(a, b))
        }
        Operator::V128AndNot => {
            let (a, b) = pop2_with_bitcast(state, type_of(op), builder);
            state.push1(builder.ins().band_not(a, b))
        }
        Operator::V128Not => {
            let (a, _) = state.pop1();
            state.push1(builder.ins().bnot(a));
        }
        Operator::I8x16Shl | Operator::I16x8Shl | Operator::I32x4Shl | Operator::I64x2Shl => {
            let ((a, _), (b, _)) = state.pop2();
            let bitcast_a = optionally_bitcast_vector(a, type_of(op), builder);
            let bitwidth = i64::from(type_of(op).lane_bits());
            // The spec expects to shift with `b mod lanewidth`; so, e.g., for 16 bit lane-width
            // we do `b AND 15`; this means fewer instructions than `iconst + urem`.
            let b_mod_bitwidth = builder.ins().band_imm(b, bitwidth - 1);
            state.push1(builder.ins().ishl(bitcast_a, b_mod_bitwidth))
        }
        Operator::I8x16ShrU | Operator::I16x8ShrU | Operator::I32x4ShrU | Operator::I64x2ShrU => {
            let ((a, _), (b, _)) = state.pop2();
            let bitcast_a = optionally_bitcast_vector(a, type_of(op), builder);
            let bitwidth = i64::from(type_of(op).lane_bits());
            // The spec expects to shift with `b mod lanewidth`; so, e.g., for 16 bit lane-width
            // we do `b AND 15`; this means fewer instructions than `iconst + urem`.
            let b_mod_bitwidth = builder.ins().band_imm(b, bitwidth - 1);
            state.push1(builder.ins().ushr(bitcast_a, b_mod_bitwidth))
        }
        Operator::I8x16ShrS | Operator::I16x8ShrS | Operator::I32x4ShrS | Operator::I64x2ShrS => {
            let ((a, _), (b, _)) = state.pop2();
            let bitcast_a = optionally_bitcast_vector(a, type_of(op), builder);
            let bitwidth = i64::from(type_of(op).lane_bits());
            // The spec expects to shift with `b mod lanewidth`; so, e.g., for 16 bit lane-width
            // we do `b AND 15`; this means fewer instructions than `iconst + urem`.
            let b_mod_bitwidth = builder.ins().band_imm(b, bitwidth - 1);
            state.push1(builder.ins().sshr(bitcast_a, b_mod_bitwidth))
        }
        Operator::V128Bitselect => {
            let ((a, _), (b, _), (c, _)) = state.pop3();
            let bitcast_a = optionally_bitcast_vector(a, I8X16, builder);
            let bitcast_b = optionally_bitcast_vector(b, I8X16, builder);
            let bitcast_c = optionally_bitcast_vector(c, I8X16, builder);
            // The CLIF operand ordering is slightly different and the types of all three
            // operands must match (hence the bitcast).
            state.push1(builder.ins().bitselect(bitcast_c, bitcast_a, bitcast_b))
        }
        Operator::V128AnyTrue => {
            let a = pop1_with_bitcast(state, type_of(op), builder);
            let bool_result = builder.ins().vany_true(a);
            state.push1(builder.ins().bint(I32, bool_result))
        }
        Operator::I8x16AllTrue
        | Operator::I16x8AllTrue
        | Operator::I32x4AllTrue
        | Operator::I64x2AllTrue => {
            let a = pop1_with_bitcast(state, type_of(op), builder);
            let bool_result = builder.ins().vall_true(a);
            state.push1(builder.ins().bint(I32, bool_result))
        }
        Operator::I8x16Bitmask
        | Operator::I16x8Bitmask
        | Operator::I32x4Bitmask
        | Operator::I64x2Bitmask => {
            let a = pop1_with_bitcast(state, type_of(op), builder);
            state.push1(builder.ins().vhigh_bits(I32, a));
        }
        Operator::I8x16Eq | Operator::I16x8Eq | Operator::I32x4Eq | Operator::I64x2Eq => {
            translate_vector_icmp(IntCC::Equal, type_of(op), builder, state)
        }
        Operator::I8x16Ne | Operator::I16x8Ne | Operator::I32x4Ne | Operator::I64x2Ne => {
            translate_vector_icmp(IntCC::NotEqual, type_of(op), builder, state)
        }
        Operator::I8x16GtS | Operator::I16x8GtS | Operator::I32x4GtS | Operator::I64x2GtS => {
            translate_vector_icmp(IntCC::SignedGreaterThan, type_of(op), builder, state)
        }
        Operator::I8x16LtS | Operator::I16x8LtS | Operator::I32x4LtS | Operator::I64x2LtS => {
            translate_vector_icmp(IntCC::SignedLessThan, type_of(op), builder, state)
        }
        Operator::I8x16GtU | Operator::I16x8GtU | Operator::I32x4GtU => {
            translate_vector_icmp(IntCC::UnsignedGreaterThan, type_of(op), builder, state)
        }
        Operator::I8x16LtU | Operator::I16x8LtU | Operator::I32x4LtU => {
            translate_vector_icmp(IntCC::UnsignedLessThan, type_of(op), builder, state)
        }
        Operator::I8x16GeS | Operator::I16x8GeS | Operator::I32x4GeS | Operator::I64x2GeS => {
            translate_vector_icmp(IntCC::SignedGreaterThanOrEqual, type_of(op), builder, state)
        }
        Operator::I8x16LeS | Operator::I16x8LeS | Operator::I32x4LeS | Operator::I64x2LeS => {
            translate_vector_icmp(IntCC::SignedLessThanOrEqual, type_of(op), builder, state)
        }
        Operator::I8x16GeU | Operator::I16x8GeU | Operator::I32x4GeU => translate_vector_icmp(
            IntCC::UnsignedGreaterThanOrEqual,
            type_of(op),
            builder,
            state,
        ),
        Operator::I8x16LeU | Operator::I16x8LeU | Operator::I32x4LeU => {
            translate_vector_icmp(IntCC::UnsignedLessThanOrEqual, type_of(op), builder, state)
        }
        Operator::F32x4Eq | Operator::F64x2Eq => {
            translate_vector_fcmp(FloatCC::Equal, type_of(op), builder, state)
        }
        Operator::F32x4Ne | Operator::F64x2Ne => {
            translate_vector_fcmp(FloatCC::NotEqual, type_of(op), builder, state)
        }
        Operator::F32x4Lt | Operator::F64x2Lt => {
            translate_vector_fcmp(FloatCC::LessThan, type_of(op), builder, state)
        }
        Operator::F32x4Gt | Operator::F64x2Gt => {
            translate_vector_fcmp(FloatCC::GreaterThan, type_of(op), builder, state)
        }
        Operator::F32x4Le | Operator::F64x2Le => {
            translate_vector_fcmp(FloatCC::LessThanOrEqual, type_of(op), builder, state)
        }
        Operator::F32x4Ge | Operator::F64x2Ge => {
            translate_vector_fcmp(FloatCC::GreaterThanOrEqual, type_of(op), builder, state)
        }
        Operator::F32x4Add | Operator::F64x2Add => {
            let (a, b) = pop2_with_bitcast(state, type_of(op), builder);
            state.push1(builder.ins().fadd(a, b))
        }
        Operator::F32x4Sub | Operator::F64x2Sub => {
            let (a, b) = pop2_with_bitcast(state, type_of(op), builder);
            state.push1(builder.ins().fsub(a, b))
        }
        Operator::F32x4Mul | Operator::F64x2Mul => {
            let (a, b) = pop2_with_bitcast(state, type_of(op), builder);
            state.push1(builder.ins().fmul(a, b))
        }
        Operator::F32x4Div | Operator::F64x2Div => {
            let (a, b) = pop2_with_bitcast(state, type_of(op), builder);
            state.push1(builder.ins().fdiv(a, b))
        }
        Operator::F32x4Max | Operator::F64x2Max => {
            let (a, b) = pop2_with_bitcast(state, type_of(op), builder);
            state.push1(builder.ins().fmax(a, b))
        }
        Operator::F32x4Min | Operator::F64x2Min => {
            let (a, b) = pop2_with_bitcast(state, type_of(op), builder);
            state.push1(builder.ins().fmin(a, b))
        }
        Operator::F32x4PMax | Operator::F64x2PMax => {
            let (a, b) = pop2_with_bitcast(state, type_of(op), builder);
            state.push1(builder.ins().fmax_pseudo(a, b))
        }
        Operator::F32x4PMin | Operator::F64x2PMin => {
            let (a, b) = pop2_with_bitcast(state, type_of(op), builder);
            state.push1(builder.ins().fmin_pseudo(a, b))
        }
        Operator::F32x4Sqrt | Operator::F64x2Sqrt => {
            let a = pop1_with_bitcast(state, type_of(op), builder);
            state.push1(builder.ins().sqrt(a))
        }
        Operator::F32x4Neg | Operator::F64x2Neg => {
            let a = pop1_with_bitcast(state, type_of(op), builder);
            state.push1(builder.ins().fneg(a))
        }
        Operator::F32x4Abs | Operator::F64x2Abs => {
            let a = pop1_with_bitcast(state, type_of(op), builder);
            state.push1(builder.ins().fabs(a))
        }
        Operator::F32x4ConvertI32x4S => {
            let a = pop1_with_bitcast(state, I32X4, builder);
            state.push1(builder.ins().fcvt_from_sint(F32X4, a))
        }
        Operator::F32x4ConvertI32x4U => {
            let a = pop1_with_bitcast(state, I32X4, builder);
            state.push1(builder.ins().fcvt_from_uint(F32X4, a))
        }
        Operator::F64x2ConvertLowI32x4S => {
            let a = pop1_with_bitcast(state, I32X4, builder);
            state.push1(builder.ins().fcvt_low_from_sint(F64X2, a));
        }
        Operator::I32x4TruncSatF32x4S => {
            let a = pop1_with_bitcast(state, F32X4, builder);
            state.push1(builder.ins().fcvt_to_sint_sat(I32X4, a))
        }
        Operator::I32x4TruncSatF32x4U => {
            let a = pop1_with_bitcast(state, F32X4, builder);
            state.push1(builder.ins().fcvt_to_uint_sat(I32X4, a))
        }
        Operator::I8x16NarrowI16x8S => {
            let (a, b) = pop2_with_bitcast(state, I16X8, builder);
            state.push1(builder.ins().snarrow(a, b))
        }
        Operator::I16x8NarrowI32x4S => {
            let (a, b) = pop2_with_bitcast(state, I32X4, builder);
            state.push1(builder.ins().snarrow(a, b))
        }
        Operator::I8x16NarrowI16x8U => {
            let (a, b) = pop2_with_bitcast(state, I16X8, builder);
            state.push1(builder.ins().unarrow(a, b))
        }
        Operator::I16x8NarrowI32x4U => {
            let (a, b) = pop2_with_bitcast(state, I32X4, builder);
            state.push1(builder.ins().unarrow(a, b))
        }
        Operator::I16x8ExtendLowI8x16S => {
            let a = pop1_with_bitcast(state, I8X16, builder);
            state.push1(builder.ins().swiden_low(a))
        }
        Operator::I16x8ExtendHighI8x16S => {
            let a = pop1_with_bitcast(state, I8X16, builder);
            state.push1(builder.ins().swiden_high(a))
        }
        Operator::I16x8ExtendLowI8x16U => {
            let a = pop1_with_bitcast(state, I8X16, builder);
            state.push1(builder.ins().uwiden_low(a))
        }
        Operator::I16x8ExtendHighI8x16U => {
            let a = pop1_with_bitcast(state, I8X16, builder);
            state.push1(builder.ins().uwiden_high(a))
        }
        Operator::I32x4ExtendLowI16x8S => {
            let a = pop1_with_bitcast(state, I16X8, builder);
            state.push1(builder.ins().swiden_low(a))
        }
        Operator::I32x4ExtendHighI16x8S => {
            let a = pop1_with_bitcast(state, I16X8, builder);
            state.push1(builder.ins().swiden_high(a))
        }
        Operator::I32x4ExtendLowI16x8U => {
            let a = pop1_with_bitcast(state, I16X8, builder);
            state.push1(builder.ins().uwiden_low(a))
        }
        Operator::I32x4ExtendHighI16x8U => {
            let a = pop1_with_bitcast(state, I16X8, builder);
            state.push1(builder.ins().uwiden_high(a))
        }

        Operator::F32x4Ceil | Operator::F64x2Ceil => {
            // This is something of a misuse of `type_of`, because that produces the return type
            // of `op`.  In this case we want the arg type, but we know it's the same as the
            // return type.  Same for the 3 cases below.
            let arg = pop1_with_bitcast(state, type_of(op), builder);
            state.push1(builder.ins().ceil(arg));
        }
        Operator::F32x4Floor | Operator::F64x2Floor => {
            let arg = pop1_with_bitcast(state, type_of(op), builder);
            state.push1(builder.ins().floor(arg));
        }
        Operator::F32x4Trunc | Operator::F64x2Trunc => {
            let arg = pop1_with_bitcast(state, type_of(op), builder);
            state.push1(builder.ins().trunc(arg));
        }
        Operator::F32x4Nearest | Operator::F64x2Nearest => {
            let arg = pop1_with_bitcast(state, type_of(op), builder);
            state.push1(builder.ins().nearest(arg));
        }
        Operator::I32x4DotI16x8S => {
            let (a, b) = pop2_with_bitcast(state, I16X8, builder);
            state.push1(builder.ins().widening_pairwise_dot_product_s(a, b));
        }
        Operator::I64x2ExtendLowI32x4S
        | Operator::I64x2ExtendHighI32x4S
        | Operator::I64x2ExtendLowI32x4U
        | Operator::I64x2ExtendHighI32x4U
        | Operator::I16x8Q15MulrSatS
        | Operator::I16x8ExtMulLowI8x16S
        | Operator::I16x8ExtMulHighI8x16S
        | Operator::I16x8ExtMulLowI8x16U
        | Operator::I16x8ExtMulHighI8x16U
        | Operator::I32x4ExtMulLowI16x8S
        | Operator::I32x4ExtMulHighI16x8S
        | Operator::I32x4ExtMulLowI16x8U
        | Operator::I32x4ExtMulHighI16x8U
        | Operator::I64x2ExtMulLowI32x4S
        | Operator::I64x2ExtMulHighI32x4S
        | Operator::I64x2ExtMulLowI32x4U
        | Operator::I64x2ExtMulHighI32x4U
        | Operator::I16x8ExtAddPairwiseI8x16S
        | Operator::I16x8ExtAddPairwiseI8x16U
        | Operator::I32x4ExtAddPairwiseI16x8S
        | Operator::I32x4ExtAddPairwiseI16x8U
        | Operator::F32x4DemoteF64x2Zero
        | Operator::F64x2PromoteLowF32x4
        | Operator::F64x2ConvertLowI32x4U
        | Operator::I32x4TruncSatF64x2SZero
        | Operator::I32x4TruncSatF64x2UZero
        | Operator::I8x16Popcnt => {
            return Err(wasm_unsupported!("proposed simd operator {:?}", op));
        }
        Operator::ReturnCall { .. } | Operator::ReturnCallIndirect { .. } => {
            return Err(wasm_unsupported!("proposed tail-call operator {:?}", op));
        }
    };
    Ok(())
}

// Clippy warns us of some fields we are deliberately ignoring
#[cfg_attr(feature = "cargo-clippy", allow(clippy::unneeded_field_pattern))]
/// Deals with a Wasm instruction located in an unreachable portion of the code. Most of them
/// are dropped but special ones like `End` or `Else` signal the potential end of the unreachable
/// portion so the translation state must be updated accordingly.
fn translate_unreachable_operator<FE: FuncEnvironment + ?Sized>(
    module_translation_state: &ModuleTranslationState,
    op: &Operator,
    builder: &mut FunctionBuilder,
    state: &mut FuncTranslationState,
    environ: &mut FE,
) -> WasmResult<()> {
    debug_assert!(!state.reachable);
    match *op {
        Operator::If { ty } => {
            // Push a placeholder control stack entry. The if isn't reachable,
            // so we don't have any branches anywhere.
            state.push_if(
                ir::Block::reserved_value(),
                ElseData::NoElse {
                    branch_inst: ir::Inst::reserved_value(),
                },
                0,
                0,
                ty,
            );
        }
        Operator::Loop { ty: _ } | Operator::Block { ty: _ } => {
            state.push_block(ir::Block::reserved_value(), 0, 0);
        }
        Operator::Else => {
            let i = state.control_stack.len() - 1;
            match state.control_stack[i] {
                ControlStackFrame::If {
                    ref else_data,
                    head_is_reachable,
                    ref mut consequent_ends_reachable,
                    blocktype,
                    ..
                } => {
                    debug_assert!(consequent_ends_reachable.is_none());
                    *consequent_ends_reachable = Some(state.reachable);

                    if head_is_reachable {
                        // We have a branch from the head of the `if` to the `else`.
                        state.reachable = true;

                        let else_block = match *else_data {
                            ElseData::NoElse { branch_inst } => {
                                let (params, _results) =
                                    module_translation_state.blocktype_params_results(blocktype)?;
                                let else_block = block_with_params(builder, params, environ)?;
                                let frame = state.control_stack.last().unwrap();
                                frame.truncate_value_stack_to_else_params(&mut state.stack);

                                // We change the target of the branch instruction.
                                builder.change_jump_destination(branch_inst, else_block);
                                builder.seal_block(else_block);
                                else_block
                            }
                            ElseData::WithElse { else_block } => {
                                let frame = state.control_stack.last().unwrap();
                                frame.truncate_value_stack_to_else_params(&mut state.stack);
                                else_block
                            }
                        };

                        builder.switch_to_block(else_block);

                        // Again, no need to push the parameters for the `else`,
                        // since we already did when we saw the original `if`. See
                        // the comment for translating `Operator::Else` in
                        // `translate_operator` for details.
                    }
                }
                _ => unreachable!(),
            }
        }
        Operator::End => {
            let stack = &mut state.stack;
            let control_stack = &mut state.control_stack;
            let frame = control_stack.pop().unwrap();

            // Pop unused parameters from stack.
            frame.truncate_value_stack_to_original_size(stack);

            let reachable_anyway = match frame {
                // If it is a loop we also have to seal the body loop block
                ControlStackFrame::Loop { header, .. } => {
                    builder.seal_block(header);
                    // And loops can't have branches to the end.
                    false
                }
                // If we never set `consequent_ends_reachable` then that means
                // we are finishing the consequent now, and there was no
                // `else`. Whether the following block is reachable depends only
                // on if the head was reachable.
                ControlStackFrame::If {
                    head_is_reachable,
                    consequent_ends_reachable: None,
                    ..
                } => head_is_reachable,
                // Since we are only in this function when in unreachable code,
                // we know that the alternative just ended unreachable. Whether
                // the following block is reachable depends on if the consequent
                // ended reachable or not.
                ControlStackFrame::If {
                    head_is_reachable,
                    consequent_ends_reachable: Some(consequent_ends_reachable),
                    ..
                } => head_is_reachable && consequent_ends_reachable,
                // All other control constructs are already handled.
                _ => false,
            };

            if frame.exit_is_branched_to() || reachable_anyway {
                builder.switch_to_block(frame.following_code());
                builder.seal_block(frame.following_code());

                // And add the return values of the block but only if the next block is reachable
                // (which corresponds to testing if the stack depth is 1)
                stack.extend_from_slice(builder.block_params(frame.following_code()));
                state.reachable = true;
            }
        }
        _ => {
            // We don't translate because this is unreachable code
        }
    }

    Ok(())
}

/// Get the address+offset to use for a heap access.
fn get_heap_addr(
    heap: ir::Heap,
    addr32: ir::Value,
    offset: u32,
    width: u32,
    addr_ty: Type,
    builder: &mut FunctionBuilder,
) -> (ir::Value, i32) {
    let offset_guard_size: u64 = builder.func.heaps[heap].offset_guard_size.into();

    // How exactly the bounds check is performed here and what it's performed
    // on is a bit tricky. Generally we want to rely on access violations (e.g.
    // segfaults) to generate traps since that means we don't have to bounds
    // check anything explicitly.
    //
    // If we don't have a guard page of unmapped memory, though, then we can't
    // rely on this trapping behavior through segfaults. Instead we need to
    // bounds-check the entire memory access here which is everything from
    // `addr32 + offset` to `addr32 + offset + width` (not inclusive). In this
    // scenario our adjusted offset that we're checking is `offset + width`.
    //
    // If we have a guard page, however, then we can perform a further
    // optimization of the generated code by only checking multiples of the
    // offset-guard size to be more CSE-friendly. Knowing that we have at least
    // 1 page of a guard page we're then able to disregard the `width` since we
    // know it's always less than one page. Our bounds check will be for the
    // first byte which will either succeed and be guaranteed to fault if it's
    // actually out of bounds, or the bounds check itself will fail. In any case
    // we assert that the width is reasonably small for now so this assumption
    // can be adjusted in the future if we get larger widths.
    //
    // Put another way we can say, where `y < offset_guard_size`:
    //
    //      n * offset_guard_size + y = offset
    //
    // We'll then pass `n * offset_guard_size` as the bounds check value. If
    // this traps then our `offset` would have trapped anyway. If this check
    // passes we know
    //
    //      addr32 + n * offset_guard_size < bound
    //
    // which means
    //
    //      addr32 + n * offset_guard_size + y < bound + offset_guard_size
    //
    // because `y < offset_guard_size`, which then means:
    //
    //      addr32 + offset < bound + offset_guard_size
    //
    // Since we know that that guard size bytes are all unmapped we're
    // guaranteed that `offset` and the `width` bytes after it are either
    // in-bounds or will hit the guard page, meaning we'll get the desired
    // semantics we want.
    //
    // As one final comment on the bits with the guard size here, another goal
    // of this is to hit an optimization in `heap_addr` where if the heap size
    // minus the offset is >= 4GB then bounds checks are 100% eliminated. This
    // means that with huge guard regions (e.g. our 2GB default) most adjusted
    // offsets we're checking here are zero. This means that we'll hit the fast
    // path and emit zero conditional traps for bounds checks
    let adjusted_offset = if offset_guard_size == 0 {
        u64::from(offset) + u64::from(width)
    } else {
        assert!(width < 1024);
        cmp::max(u64::from(offset) / offset_guard_size * offset_guard_size, 1)
    };
    debug_assert!(adjusted_offset > 0); // want to bounds check at least 1 byte
    let check_size = u32::try_from(adjusted_offset).unwrap_or(u32::MAX);
    let base = builder.ins().heap_addr(addr_ty, heap, addr32, check_size);

    // Native load/store instructions take a signed `Offset32` immediate, so adjust the base
    // pointer if necessary.
    if offset > i32::MAX as u32 {
        // Offset doesn't fit in the load/store instruction.
        let adj = builder.ins().iadd_imm(base, i64::from(i32::MAX) + 1);
        (adj, (offset - (i32::MAX as u32 + 1)) as i32)
    } else {
        (base, offset as i32)
    }
}

/// Prepare for a load; factors out common functionality between load and load_extend operations.
fn prepare_load<FE: FuncEnvironment + ?Sized>(
    memarg: &MemoryImmediate,
    loaded_bytes: u32,
    builder: &mut FunctionBuilder,
    state: &mut FuncTranslationState,
    environ: &mut FE,
) -> WasmResult<(MemFlags, Value, Offset32)> {
    let (addr32, _) = state.pop1();

    let heap = state.get_heap(builder.func, memarg.memory, environ)?;
    let (base, offset) = get_heap_addr(
        heap,
        addr32,
        memarg.offset,
        loaded_bytes,
        environ.pointer_type(),
        builder,
    );

    // Note that we don't set `is_aligned` here, even if the load instruction's
    // alignment immediate says it's aligned, because WebAssembly's immediate
    // field is just a hint, while Cranelift's aligned flag needs a guarantee.
    // WebAssembly memory accesses are always little-endian.
    let mut flags = MemFlags::new();
    flags.set_endianness(ir::Endianness::Little);

    Ok((flags, base, offset.into()))
}

/// Translate a load instruction.
fn translate_load<FE: FuncEnvironment + ?Sized>(
    memarg: &MemoryImmediate,
    opcode: ir::Opcode,
    result_ty: Type,
    builder: &mut FunctionBuilder,
    state: &mut FuncTranslationState,
    environ: &mut FE,
) -> WasmResult<()> {
    let (flags, base, offset) = prepare_load(
        memarg,
        mem_op_size(opcode, result_ty),
        builder,
        state,
        environ,
    )?;
    let (load, dfg) = builder.ins().Load(opcode, result_ty, flags, offset, base);
    state.push1(dfg.first_result(load));
    Ok(())
}

/// Translate a store instruction.
fn translate_store<FE: FuncEnvironment + ?Sized>(
    memarg: &MemoryImmediate,
    opcode: ir::Opcode,
    builder: &mut FunctionBuilder,
    state: &mut FuncTranslationState,
    environ: &mut FE,
) -> WasmResult<()> {
    let ((addr32, _), (val, _)) = state.pop2();
    let val_ty = builder.func.dfg.value_type(val);

    let heap = state.get_heap(builder.func, memarg.memory, environ)?;
    let (base, offset) = get_heap_addr(
        heap,
        addr32,
        memarg.offset,
        mem_op_size(opcode, val_ty),
        environ.pointer_type(),
        builder,
    );
    // See the comments in `prepare_load` about the flags.
    let mut flags = MemFlags::new();
    flags.set_endianness(ir::Endianness::Little);
    builder
        .ins()
        .Store(opcode, val_ty, flags, offset.into(), val, base);
    Ok(())
}

fn mem_op_size(opcode: ir::Opcode, ty: Type) -> u32 {
    match opcode {
        ir::Opcode::Istore8 | ir::Opcode::Sload8 | ir::Opcode::Uload8 => 1,
        ir::Opcode::Istore16 | ir::Opcode::Sload16 | ir::Opcode::Uload16 => 2,
        ir::Opcode::Istore32 | ir::Opcode::Sload32 | ir::Opcode::Uload32 => 4,
        ir::Opcode::Store | ir::Opcode::Load => ty.bytes(),
        _ => panic!("unknown size of mem op for {:?}", opcode),
    }
}

fn translate_icmp(cc: IntCC, builder: &mut FunctionBuilder, state: &mut FuncTranslationState) {
    let ((arg0, _), (arg1, _)) = state.pop2();
    let val = builder.ins().icmp(cc, arg0, arg1);
    state.push1(builder.ins().bint(I32, val));
}

fn fold_atomic_mem_addr(
    linear_mem_addr: Value,
    memarg: &MemoryImmediate,
    access_ty: Type,
    builder: &mut FunctionBuilder,
) -> Value {
    let access_ty_bytes = access_ty.bytes();
    let final_lma = if memarg.offset > 0 {
        assert!(builder.func.dfg.value_type(linear_mem_addr) == I32);
        let linear_mem_addr = builder.ins().uextend(I64, linear_mem_addr);
        let a = builder
            .ins()
            .iadd_imm(linear_mem_addr, i64::from(memarg.offset));
        let cflags = builder.ins().ifcmp_imm(a, 0x1_0000_0000i64);
        builder.ins().trapif(
            IntCC::UnsignedGreaterThanOrEqual,
            cflags,
            ir::TrapCode::HeapOutOfBounds,
        );
        builder.ins().ireduce(I32, a)
    } else {
        linear_mem_addr
    };
    assert!(access_ty_bytes == 4 || access_ty_bytes == 8);
    let final_lma_misalignment = builder
        .ins()
        .band_imm(final_lma, i64::from(access_ty_bytes - 1));
    let f = builder
        .ins()
        .ifcmp_imm(final_lma_misalignment, i64::from(0));
    builder
        .ins()
        .trapif(IntCC::NotEqual, f, ir::TrapCode::HeapMisaligned);
    final_lma
}

// For an atomic memory operation, emit an alignment check for the linear memory address,
// and then compute the final effective address.
fn finalise_atomic_mem_addr<FE: FuncEnvironment + ?Sized>(
    linear_mem_addr: Value,
    memarg: &MemoryImmediate,
    access_ty: Type,
    builder: &mut FunctionBuilder,
    state: &mut FuncTranslationState,
    environ: &mut FE,
) -> WasmResult<Value> {
    // Check the alignment of `linear_mem_addr`.
    let access_ty_bytes = access_ty.bytes();
    let final_lma = builder
        .ins()
        .iadd_imm(linear_mem_addr, i64::from(memarg.offset));
    if access_ty_bytes != 1 {
        assert!(access_ty_bytes == 2 || access_ty_bytes == 4 || access_ty_bytes == 8);
        let final_lma_misalignment = builder
            .ins()
            .band_imm(final_lma, i64::from(access_ty_bytes - 1));
        let f = builder
            .ins()
            .ifcmp_imm(final_lma_misalignment, i64::from(0));
        builder
            .ins()
            .trapif(IntCC::NotEqual, f, ir::TrapCode::HeapMisaligned);
    }

    // Compute the final effective address.
    let heap = state.get_heap(builder.func, memarg.memory, environ)?;
    let (base, offset) = get_heap_addr(
        heap,
        final_lma,
        /*offset=*/ 0,
        access_ty.bytes(),
        environ.pointer_type(),
        builder,
    );

    let final_effective_address = builder.ins().iadd_imm(base, i64::from(offset));
    Ok(final_effective_address)
}

fn translate_atomic_rmw<FE: FuncEnvironment + ?Sized>(
    widened_ty: Type,
    access_ty: Type,
    op: AtomicRmwOp,
    memarg: &MemoryImmediate,
    builder: &mut FunctionBuilder,
    state: &mut FuncTranslationState,
    environ: &mut FE,
) -> WasmResult<()> {
    let ((linear_mem_addr, _), (mut arg2, _)) = state.pop2();
    let arg2_ty = builder.func.dfg.value_type(arg2);

    // The operation is performed at type `access_ty`, and the old value is zero-extended
    // to type `widened_ty`.
    match access_ty {
        I8 | I16 | I32 | I64 => {}
        _ => {
            return Err(wasm_unsupported!(
                "atomic_rmw: unsupported access type {:?}",
                access_ty
            ))
        }
    };
    let w_ty_ok = match widened_ty {
        I32 | I64 => true,
        _ => false,
    };
    assert!(w_ty_ok && widened_ty.bytes() >= access_ty.bytes());

    assert!(arg2_ty.bytes() >= access_ty.bytes());
    if arg2_ty.bytes() > access_ty.bytes() {
        arg2 = builder.ins().ireduce(access_ty, arg2);
    }

    let final_effective_address =
        finalise_atomic_mem_addr(linear_mem_addr, memarg, access_ty, builder, state, environ)?;

    // See the comments in `prepare_load` about the flags.
    let mut flags = MemFlags::new();
    flags.set_endianness(ir::Endianness::Little);
    let mut res = builder
        .ins()
        .atomic_rmw(access_ty, flags, op, final_effective_address, arg2);
    if access_ty != widened_ty {
        res = builder.ins().uextend(widened_ty, res);
    }
    state.push1(res);
    Ok(())
}

fn translate_atomic_cas<FE: FuncEnvironment + ?Sized>(
    widened_ty: Type,
    access_ty: Type,
    memarg: &MemoryImmediate,
    builder: &mut FunctionBuilder,
    state: &mut FuncTranslationState,
    environ: &mut FE,
) -> WasmResult<()> {
    let ((linear_mem_addr, _), (mut expected, _), (mut replacement, _)) = state.pop3();
    let expected_ty = builder.func.dfg.value_type(expected);
    let replacement_ty = builder.func.dfg.value_type(replacement);

    // The compare-and-swap is performed at type `access_ty`, and the old value is zero-extended
    // to type `widened_ty`.
    match access_ty {
        I8 | I16 | I32 | I64 => {}
        _ => {
            return Err(wasm_unsupported!(
                "atomic_cas: unsupported access type {:?}",
                access_ty
            ))
        }
    };
    let w_ty_ok = match widened_ty {
        I32 | I64 => true,
        _ => false,
    };
    assert!(w_ty_ok && widened_ty.bytes() >= access_ty.bytes());

    assert!(expected_ty.bytes() >= access_ty.bytes());
    if expected_ty.bytes() > access_ty.bytes() {
        expected = builder.ins().ireduce(access_ty, expected);
    }
    assert!(replacement_ty.bytes() >= access_ty.bytes());
    if replacement_ty.bytes() > access_ty.bytes() {
        replacement = builder.ins().ireduce(access_ty, replacement);
    }

    let final_effective_address =
        finalise_atomic_mem_addr(linear_mem_addr, memarg, access_ty, builder, state, environ)?;

    // See the comments in `prepare_load` about the flags.
    let mut flags = MemFlags::new();
    flags.set_endianness(ir::Endianness::Little);
    let mut res = builder
        .ins()
        .atomic_cas(flags, final_effective_address, expected, replacement);
    if access_ty != widened_ty {
        res = builder.ins().uextend(widened_ty, res);
    }
    state.push1(res);
    Ok(())
}

fn translate_atomic_load<FE: FuncEnvironment + ?Sized>(
    widened_ty: Type,
    access_ty: Type,
    memarg: &MemoryImmediate,
    builder: &mut FunctionBuilder,
    state: &mut FuncTranslationState,
    environ: &mut FE,
) -> WasmResult<()> {
    let (linear_mem_addr, _) = state.pop1();

    // The load is performed at type `access_ty`, and the loaded value is zero extended
    // to `widened_ty`.
    match access_ty {
        I8 | I16 | I32 | I64 => {}
        _ => {
            return Err(wasm_unsupported!(
                "atomic_load: unsupported access type {:?}",
                access_ty
            ))
        }
    };
    let w_ty_ok = match widened_ty {
        I32 | I64 => true,
        _ => false,
    };
    assert!(w_ty_ok && widened_ty.bytes() >= access_ty.bytes());

    let final_effective_address =
        finalise_atomic_mem_addr(linear_mem_addr, memarg, access_ty, builder, state, environ)?;

    // See the comments in `prepare_load` about the flags.
    let mut flags = MemFlags::new();
    flags.set_endianness(ir::Endianness::Little);
    let mut res = builder
        .ins()
        .atomic_load(access_ty, flags, final_effective_address);
    if access_ty != widened_ty {
        res = builder.ins().uextend(widened_ty, res);
    }
    state.push1(res);
    Ok(())
}

fn translate_atomic_store<FE: FuncEnvironment + ?Sized>(
    access_ty: Type,
    memarg: &MemoryImmediate,
    builder: &mut FunctionBuilder,
    state: &mut FuncTranslationState,
    environ: &mut FE,
) -> WasmResult<()> {
    let ((linear_mem_addr, _), (mut data, _)) = state.pop2();
    let data_ty = builder.func.dfg.value_type(data);

    // The operation is performed at type `access_ty`, and the data to be stored may first
    // need to be narrowed accordingly.
    match access_ty {
        I8 | I16 | I32 | I64 => {}
        _ => {
            return Err(wasm_unsupported!(
                "atomic_store: unsupported access type {:?}",
                access_ty
            ))
        }
    };
    let d_ty_ok = match data_ty {
        I32 | I64 => true,
        _ => false,
    };
    assert!(d_ty_ok && data_ty.bytes() >= access_ty.bytes());

    if data_ty.bytes() > access_ty.bytes() {
        data = builder.ins().ireduce(access_ty, data);
    }

    let final_effective_address =
        finalise_atomic_mem_addr(linear_mem_addr, memarg, access_ty, builder, state, environ)?;

    // See the comments in `prepare_load` about the flags.
    let mut flags = MemFlags::new();
    flags.set_endianness(ir::Endianness::Little);
    builder
        .ins()
        .atomic_store(flags, data, final_effective_address);
    Ok(())
}

fn translate_vector_icmp(
    cc: IntCC,
    needed_type: Type,
    builder: &mut FunctionBuilder,
    state: &mut FuncTranslationState,
) {
    let ((a, _), (b, _)) = state.pop2();
    let bitcast_a = optionally_bitcast_vector(a, needed_type, builder);
    let bitcast_b = optionally_bitcast_vector(b, needed_type, builder);
    state.push1(builder.ins().icmp(cc, bitcast_a, bitcast_b))
}

fn translate_fcmp(cc: FloatCC, builder: &mut FunctionBuilder, state: &mut FuncTranslationState) {
    let ((arg0, _), (arg1, _)) = state.pop2();
    let val = builder.ins().fcmp(cc, arg0, arg1);
    state.push1(builder.ins().bint(I32, val));
}

fn translate_vector_fcmp(
    cc: FloatCC,
    needed_type: Type,
    builder: &mut FunctionBuilder,
    state: &mut FuncTranslationState,
) {
    let ((a, _), (b, _)) = state.pop2();
    let bitcast_a = optionally_bitcast_vector(a, needed_type, builder);
    let bitcast_b = optionally_bitcast_vector(b, needed_type, builder);
    state.push1(builder.ins().fcmp(cc, bitcast_a, bitcast_b))
}

fn translate_br_if(
    relative_depth: u32,
    builder: &mut FunctionBuilder,
    state: &mut FuncTranslationState,
) {
    let (val, _) = state.pop1();
    let (br_destination, inputs) = translate_br_if_args(relative_depth, state);
    let inputs = (&*inputs.0, &*inputs.1);
    canonicalise_then_brnz(builder, val, br_destination, inputs);

    let next_block = builder.create_block();
    canonicalise_then_jump(builder, next_block, (&[], &[]));
    builder.seal_block(next_block); // The only predecessor is the current block.
    builder.switch_to_block(next_block);
}

fn translate_br_if_args(
    relative_depth: u32,
    state: &mut FuncTranslationState,
) -> (ir::Block, (&mut [ir::Value], &mut [ValueExtraInfo])) {
    let i = state.control_stack.len() - 1 - (relative_depth as usize);
    let (return_count, br_destination) = {
        let frame = &mut state.control_stack[i];
        // The values returned by the branch are still available for the reachable
        // code that comes after it
        frame.set_branched_to_exit();
        let return_count = if frame.is_loop() {
            frame.num_param_values()
        } else {
            frame.num_return_values()
        };
        (return_count, frame.br_destination())
    };
    let inputs = state.peekn_mut(return_count);
    (br_destination, inputs)
}

/// Determine the returned value type of a WebAssembly operator
fn type_of(operator: &Operator) -> Type {
    match operator {
        Operator::V128Load { .. }
        | Operator::V128Store { .. }
        | Operator::V128Const { .. }
        | Operator::V128Not
        | Operator::V128And
        | Operator::V128AndNot
        | Operator::V128Or
        | Operator::V128Xor
        | Operator::V128AnyTrue
        | Operator::V128Bitselect => I8X16, // default type representing V128

        Operator::I8x16Shuffle { .. }
        | Operator::I8x16Splat
        | Operator::V128Load8Splat { .. }
        | Operator::V128Load8Lane { .. }
        | Operator::V128Store8Lane { .. }
        | Operator::I8x16ExtractLaneS { .. }
        | Operator::I8x16ExtractLaneU { .. }
        | Operator::I8x16ReplaceLane { .. }
        | Operator::I8x16Eq
        | Operator::I8x16Ne
        | Operator::I8x16LtS
        | Operator::I8x16LtU
        | Operator::I8x16GtS
        | Operator::I8x16GtU
        | Operator::I8x16LeS
        | Operator::I8x16LeU
        | Operator::I8x16GeS
        | Operator::I8x16GeU
        | Operator::I8x16Neg
        | Operator::I8x16Abs
        | Operator::I8x16AllTrue
        | Operator::I8x16Shl
        | Operator::I8x16ShrS
        | Operator::I8x16ShrU
        | Operator::I8x16Add
        | Operator::I8x16AddSatS
        | Operator::I8x16AddSatU
        | Operator::I8x16Sub
        | Operator::I8x16SubSatS
        | Operator::I8x16SubSatU
        | Operator::I8x16MinS
        | Operator::I8x16MinU
        | Operator::I8x16MaxS
        | Operator::I8x16MaxU
        | Operator::I8x16RoundingAverageU
        | Operator::I8x16Bitmask => I8X16,

        Operator::I16x8Splat
        | Operator::V128Load16Splat { .. }
        | Operator::V128Load16Lane { .. }
        | Operator::V128Store16Lane { .. }
        | Operator::I16x8ExtractLaneS { .. }
        | Operator::I16x8ExtractLaneU { .. }
        | Operator::I16x8ReplaceLane { .. }
        | Operator::I16x8Eq
        | Operator::I16x8Ne
        | Operator::I16x8LtS
        | Operator::I16x8LtU
        | Operator::I16x8GtS
        | Operator::I16x8GtU
        | Operator::I16x8LeS
        | Operator::I16x8LeU
        | Operator::I16x8GeS
        | Operator::I16x8GeU
        | Operator::I16x8Neg
        | Operator::I16x8Abs
        | Operator::I16x8AllTrue
        | Operator::I16x8Shl
        | Operator::I16x8ShrS
        | Operator::I16x8ShrU
        | Operator::I16x8Add
        | Operator::I16x8AddSatS
        | Operator::I16x8AddSatU
        | Operator::I16x8Sub
        | Operator::I16x8SubSatS
        | Operator::I16x8SubSatU
        | Operator::I16x8MinS
        | Operator::I16x8MinU
        | Operator::I16x8MaxS
        | Operator::I16x8MaxU
        | Operator::I16x8RoundingAverageU
        | Operator::I16x8Mul
        | Operator::I16x8Bitmask => I16X8,

        Operator::I32x4Splat
        | Operator::V128Load32Splat { .. }
        | Operator::V128Load32Lane { .. }
        | Operator::V128Store32Lane { .. }
        | Operator::I32x4ExtractLane { .. }
        | Operator::I32x4ReplaceLane { .. }
        | Operator::I32x4Eq
        | Operator::I32x4Ne
        | Operator::I32x4LtS
        | Operator::I32x4LtU
        | Operator::I32x4GtS
        | Operator::I32x4GtU
        | Operator::I32x4LeS
        | Operator::I32x4LeU
        | Operator::I32x4GeS
        | Operator::I32x4GeU
        | Operator::I32x4Neg
        | Operator::I32x4Abs
        | Operator::I32x4AllTrue
        | Operator::I32x4Shl
        | Operator::I32x4ShrS
        | Operator::I32x4ShrU
        | Operator::I32x4Add
        | Operator::I32x4Sub
        | Operator::I32x4Mul
        | Operator::I32x4MinS
        | Operator::I32x4MinU
        | Operator::I32x4MaxS
        | Operator::I32x4MaxU
        | Operator::F32x4ConvertI32x4S
        | Operator::F32x4ConvertI32x4U
        | Operator::I32x4Bitmask
        | Operator::V128Load32Zero { .. } => I32X4,

        Operator::I64x2Splat
        | Operator::V128Load64Splat { .. }
        | Operator::V128Load64Lane { .. }
        | Operator::V128Store64Lane { .. }
        | Operator::I64x2ExtractLane { .. }
        | Operator::I64x2ReplaceLane { .. }
        | Operator::I64x2Eq
        | Operator::I64x2Ne
        | Operator::I64x2LtS
        | Operator::I64x2GtS
        | Operator::I64x2LeS
        | Operator::I64x2GeS
        | Operator::I64x2Neg
        | Operator::I64x2Abs
        | Operator::I64x2AllTrue
        | Operator::I64x2Shl
        | Operator::I64x2ShrS
        | Operator::I64x2ShrU
        | Operator::I64x2Add
        | Operator::I64x2Sub
        | Operator::I64x2Mul
        | Operator::I64x2Bitmask
        | Operator::V128Load64Zero { .. } => I64X2,

        Operator::F32x4Splat
        | Operator::F32x4ExtractLane { .. }
        | Operator::F32x4ReplaceLane { .. }
        | Operator::F32x4Eq
        | Operator::F32x4Ne
        | Operator::F32x4Lt
        | Operator::F32x4Gt
        | Operator::F32x4Le
        | Operator::F32x4Ge
        | Operator::F32x4Abs
        | Operator::F32x4Neg
        | Operator::F32x4Sqrt
        | Operator::F32x4Add
        | Operator::F32x4Sub
        | Operator::F32x4Mul
        | Operator::F32x4Div
        | Operator::F32x4Min
        | Operator::F32x4Max
        | Operator::F32x4PMin
        | Operator::F32x4PMax
        | Operator::I32x4TruncSatF32x4S
        | Operator::I32x4TruncSatF32x4U
        | Operator::F32x4Ceil
        | Operator::F32x4Floor
        | Operator::F32x4Trunc
        | Operator::F32x4Nearest => F32X4,

        Operator::F64x2Splat
        | Operator::F64x2ExtractLane { .. }
        | Operator::F64x2ReplaceLane { .. }
        | Operator::F64x2Eq
        | Operator::F64x2Ne
        | Operator::F64x2Lt
        | Operator::F64x2Gt
        | Operator::F64x2Le
        | Operator::F64x2Ge
        | Operator::F64x2Abs
        | Operator::F64x2Neg
        | Operator::F64x2Sqrt
        | Operator::F64x2Add
        | Operator::F64x2Sub
        | Operator::F64x2Mul
        | Operator::F64x2Div
        | Operator::F64x2Min
        | Operator::F64x2Max
        | Operator::F64x2PMin
        | Operator::F64x2PMax
        | Operator::F64x2Ceil
        | Operator::F64x2Floor
        | Operator::F64x2Trunc
        | Operator::F64x2Nearest => F64X2,

        _ => unimplemented!(
            "Currently only SIMD instructions are mapped to their return type; the \
             following instruction is not mapped: {:?}",
            operator
        ),
    }
}

/// Some SIMD operations only operate on I8X16 in CLIF; this will convert them to that type by
/// adding a raw_bitcast if necessary.
fn optionally_bitcast_vector(
    value: Value,
    needed_type: Type,
    builder: &mut FunctionBuilder,
) -> Value {
    if builder.func.dfg.value_type(value) != needed_type {
        builder.ins().raw_bitcast(needed_type, value)
    } else {
        value
    }
}

#[inline(always)]
fn is_non_canonical_v128(ty: ir::Type) -> bool {
    match ty {
        B8X16 | B16X8 | B32X4 | B64X2 | I64X2 | I32X4 | I16X8 | F32X4 | F64X2 => true,
        _ => false,
    }
}

/// Cast to I8X16, any vector values in `values` that are of "non-canonical" type (meaning, not
/// I8X16), and return them in a slice.  A pre-scan is made to determine whether any casts are
/// actually necessary, and if not, the original slice is returned.  Otherwise the cast values
/// are returned in a slice that belongs to the caller-supplied `SmallVec`.
fn canonicalise_v128_values<'a>(
    tmp_canonicalised: &'a mut SmallVec<[ir::Value; 16]>,
    builder: &mut FunctionBuilder,
    values: &'a [ir::Value],
) -> &'a [ir::Value] {
    debug_assert!(tmp_canonicalised.is_empty());
    // First figure out if any of the parameters need to be cast.  Mostly they don't need to be.
    let any_non_canonical = values
        .iter()
        .any(|v| is_non_canonical_v128(builder.func.dfg.value_type(*v)));
    // Hopefully we take this exit most of the time, hence doing no heap allocation.
    if !any_non_canonical {
        return values;
    }
    // Otherwise we'll have to cast, and push the resulting `Value`s into `canonicalised`.
    for v in values {
        tmp_canonicalised.push(if is_non_canonical_v128(builder.func.dfg.value_type(*v)) {
            builder.ins().raw_bitcast(I8X16, *v)
        } else {
            *v
        });
    }
    tmp_canonicalised.as_slice()
}

/// Generate a `jump` instruction, but first cast all 128-bit vector values to I8X16 if they
/// don't have that type.  This is done in somewhat roundabout way so as to ensure that we
/// almost never have to do any heap allocation.
fn canonicalise_then_jump(
    builder: &mut FunctionBuilder,
    destination: ir::Block,
    params: (&[ir::Value], &[ValueExtraInfo]),
) -> ir::Inst {
    let mut tmp_canonicalised = SmallVec::<[ir::Value; 16]>::new();
    let canonicalised = canonicalise_v128_values(&mut tmp_canonicalised, builder, params.0);
    builder.ins().jump(destination, canonicalised)
}

/// The same but for a `brz` instruction.
fn canonicalise_then_brz(
    builder: &mut FunctionBuilder,
    cond: ir::Value,
    destination: ir::Block,
    params: (&[ir::Value], &[ValueExtraInfo]),
) -> ir::Inst {
    let mut tmp_canonicalised = SmallVec::<[ir::Value; 16]>::new();
    let canonicalised = canonicalise_v128_values(&mut tmp_canonicalised, builder, params.0);
    builder.ins().brz(cond, destination, canonicalised)
}

/// The same but for a `brnz` instruction.
fn canonicalise_then_brnz(
    builder: &mut FunctionBuilder,
    cond: ir::Value,
    destination: ir::Block,
    params: (&[ir::Value], &[ValueExtraInfo]),
) -> ir::Inst {
    let mut tmp_canonicalised = SmallVec::<[ir::Value; 16]>::new();
    let canonicalised = canonicalise_v128_values(&mut tmp_canonicalised, builder, params.0);
    builder.ins().brnz(cond, destination, canonicalised)
}

/// A helper for popping and bitcasting a single value; since SIMD values can lose their type by
/// using v128 (i.e. CLIF's I8x16) we must re-type the values using a bitcast to avoid CLIF
/// typing issues.
fn pop1_with_bitcast(
    state: &mut FuncTranslationState,
    needed_type: Type,
    builder: &mut FunctionBuilder,
) -> Value {
    optionally_bitcast_vector(state.pop1().0, needed_type, builder)
}

/// A helper for popping and bitcasting two values; since SIMD values can lose their type by
/// using v128 (i.e. CLIF's I8x16) we must re-type the values using a bitcast to avoid CLIF
/// typing issues.
fn pop2_with_bitcast(
    state: &mut FuncTranslationState,
    needed_type: Type,
    builder: &mut FunctionBuilder,
) -> (Value, Value) {
    let ((a, _), (b, _)) = state.pop2();
    let bitcast_a = optionally_bitcast_vector(a, needed_type, builder);
    let bitcast_b = optionally_bitcast_vector(b, needed_type, builder);
    (bitcast_a, bitcast_b)
}

/// A helper for bitcasting a sequence of values (e.g. function arguments). If a value is a
/// vector type that does not match its expected type, this will modify the value in place to point
/// to the result of a `raw_bitcast`. This conversion is necessary to translate Wasm code that
/// uses `V128` as function parameters (or implicitly in block parameters) and still use specific
/// CLIF types (e.g. `I32X4`) in the function body.
pub fn bitcast_arguments(
    arguments: &mut [Value],
    expected_types: &[Type],
    builder: &mut FunctionBuilder,
) {
    assert_eq!(arguments.len(), expected_types.len());
    for (i, t) in expected_types.iter().enumerate() {
        if t.is_vector() {
            assert!(
                builder.func.dfg.value_type(arguments[i]).is_vector(),
                "unexpected type mismatch: expected {}, argument {} was actually of type {}",
                t,
                arguments[i],
                builder.func.dfg.value_type(arguments[i])
            );
            arguments[i] = optionally_bitcast_vector(arguments[i], *t, builder)
        }
    }
}

/// A helper to extract all the `Type` listings of each variable in `params`
/// for only parameters the return true for `is_wasm`, typically paired with
/// `is_wasm_return` or `is_wasm_parameter`.
pub fn wasm_param_types(params: &[ir::AbiParam], is_wasm: impl Fn(usize) -> bool) -> Vec<Type> {
    let mut ret = Vec::with_capacity(params.len());
    for (i, param) in params.iter().enumerate() {
        if is_wasm(i) {
            ret.push(param.value_type);
        }
    }
    ret
}

'''
'''--- lib/compiler-cranelift/src/translator/func_environ.rs ---
// This file contains code from external sources.
// Attributions: https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md

//! All the runtime support necessary for the wasm to cranelift translation is formalized by the
//! traits `FunctionEnvironment`.

use super::func_state::FuncTranslationState;
use super::translation_utils::reference_type;
use core::convert::From;
use cranelift_codegen::cursor::FuncCursor;
use cranelift_codegen::ir::immediates::Offset32;
use cranelift_codegen::ir::{self, InstBuilder};
use cranelift_codegen::isa::TargetFrontendConfig;
use cranelift_frontend::FunctionBuilder;
use wasmer_compiler::wasmparser::{Operator, Type};
use wasmer_compiler::WasmResult;
use wasmer_types::{
    FunctionIndex, FunctionType, GlobalIndex, LocalFunctionIndex, MemoryIndex, SignatureIndex,
    TableIndex, Type as WasmerType,
};

/// The value of a WebAssembly global variable.
#[derive(Clone, Copy)]
pub enum GlobalVariable {
    #[allow(dead_code)]
    /// This is a constant global with a value known at compile time.
    Const(ir::Value),

    /// This is a variable in memory that should be referenced through a `GlobalValue`.
    Memory {
        /// The address of the global variable storage.
        gv: ir::GlobalValue,
        /// An offset to add to the address.
        offset: Offset32,
        /// The global variable's type.
        ty: ir::Type,
    },

    #[allow(dead_code)]
    /// This is a global variable that needs to be handled by the environment.
    Custom,
}

#[allow(dead_code)]
/// How to return from functions.
#[derive(Copy, Clone, PartialEq, Eq, Debug)]
pub enum ReturnMode {
    /// Use normal return instructions as needed.
    NormalReturns,
    /// Use a single fallthrough return at the end of the function.
    FallthroughReturn,
}

/// Environment affecting the translation of a WebAssembly.
pub trait TargetEnvironment {
    /// Get the information needed to produce Cranelift IR for the given target.
    fn target_config(&self) -> TargetFrontendConfig;

    /// Get the Cranelift integer type to use for native pointers.
    ///
    /// This returns `I64` for 64-bit architectures and `I32` for 32-bit architectures.
    fn pointer_type(&self) -> ir::Type {
        ir::Type::int(u16::from(self.target_config().pointer_bits())).unwrap()
    }

    /// Get the size of a native pointer, in bytes.
    fn pointer_bytes(&self) -> u8 {
        self.target_config().pointer_bytes()
    }

    /// Get the Cranelift reference type to use for native references.
    ///
    /// This returns `R64` for 64-bit architectures and `R32` for 32-bit architectures.
    fn reference_type(&self) -> ir::Type {
        reference_type(self.target_config()).expect("expected reference type")
    }
}

/// Environment affecting the translation of a single WebAssembly function.
///
/// A `FuncEnvironment` trait object is required to translate a WebAssembly function to Cranelift
/// IR. The function environment provides information about the WebAssembly module as well as the
/// runtime environment.
pub trait FuncEnvironment: TargetEnvironment {
    /// Is the given parameter of the given function a wasm-level parameter, as opposed to a hidden
    /// parameter added for use by the implementation?
    fn is_wasm_parameter(&self, signature: &ir::Signature, index: usize) -> bool {
        signature.params[index].purpose == ir::ArgumentPurpose::Normal
    }

    /// Is the given return of the given function a wasm-level parameter, as
    /// opposed to a hidden parameter added for use by the implementation?
    fn is_wasm_return(&self, signature: &ir::Signature, index: usize) -> bool {
        signature.returns[index].purpose == ir::ArgumentPurpose::Normal
    }

    /// Should the code be structured to use a single `fallthrough_return` instruction at the end
    /// of the function body, rather than `return` instructions as needed? This is used by VMs
    /// to append custom epilogues.
    fn return_mode(&self) -> ReturnMode {
        ReturnMode::NormalReturns
    }

    /// Set up the necessary preamble definitions in `func` to access the global variable
    /// identified by `index`.
    ///
    /// The index space covers both imported globals and globals defined by the module.
    ///
    /// Return the global variable reference that should be used to access the global and the
    /// WebAssembly type of the global.
    fn make_global(
        &mut self,
        func: &mut ir::Function,
        index: GlobalIndex,
    ) -> WasmResult<GlobalVariable>;

    /// Set up the necessary preamble definitions in `func` to access the linear memory identified
    /// by `index`.
    ///
    /// The index space covers both imported and locally declared memories.
    fn make_heap(&mut self, func: &mut ir::Function, index: MemoryIndex) -> WasmResult<ir::Heap>;

    /// Set up the necessary preamble definitions in `func` to access the table identified
    /// by `index`.
    ///
    /// The index space covers both imported and locally declared tables.
    fn make_table(&mut self, func: &mut ir::Function, index: TableIndex) -> WasmResult<ir::Table>;

    /// Set up a signature definition in the preamble of `func` that can be used for an indirect
    /// call with signature `index`.
    ///
    /// The signature may contain additional arguments needed for an indirect call, but the
    /// arguments marked as `ArgumentPurpose::Normal` must correspond to the WebAssembly signature
    /// arguments.
    ///
    /// The signature will only be used for indirect calls, even if the module has direct function
    /// calls with the same WebAssembly type.
    fn make_indirect_sig(
        &mut self,
        func: &mut ir::Function,
        index: SignatureIndex,
    ) -> WasmResult<ir::SigRef>;

    /// Set up an external function definition in the preamble of `func` that can be used to
    /// directly call the function `index`.
    ///
    /// The index space covers both imported functions and functions defined in the current module.
    ///
    /// The function's signature may contain additional arguments needed for a direct call, but the
    /// arguments marked as `ArgumentPurpose::Normal` must correspond to the WebAssembly signature
    /// arguments.
    ///
    /// The function's signature will only be used for direct calls, even if the module has
    /// indirect calls with the same WebAssembly type.
    fn make_direct_func(
        &mut self,
        func: &mut ir::Function,
        index: FunctionIndex,
    ) -> WasmResult<ir::FuncRef>;

    /// Translate a `call_indirect` WebAssembly instruction at `pos`.
    ///
    /// Insert instructions at `pos` for an indirect call to the function `callee` in the table
    /// `table_index` with WebAssembly signature `sig_index`. The `callee` value will have type
    /// `i32`.
    ///
    /// The signature `sig_ref` was previously created by `make_indirect_sig()`.
    ///
    /// Return the call instruction whose results are the WebAssembly return values.
    #[cfg_attr(feature = "cargo-clippy", allow(clippy::too_many_arguments))]
    fn translate_call_indirect(
        &mut self,
        pos: FuncCursor,
        table_index: TableIndex,
        table: ir::Table,
        sig_index: SignatureIndex,
        sig_ref: ir::SigRef,
        callee: ir::Value,
        call_args: &[ir::Value],
    ) -> WasmResult<ir::Inst>;

    /// Translate a `call` WebAssembly instruction at `pos`.
    ///
    /// Insert instructions at `pos` for a direct call to the function `callee_index`.
    ///
    /// The function reference `callee` was previously created by `make_direct_func()`.
    ///
    /// Return the call instruction whose results are the WebAssembly return values.
    fn translate_call(
        &mut self,
        mut pos: FuncCursor,
        _callee_index: FunctionIndex,
        callee: ir::FuncRef,
        call_args: &[ir::Value],
    ) -> WasmResult<ir::Inst> {
        Ok(pos.ins().call(callee, call_args))
    }

    /// Translate a `memory.grow` WebAssembly instruction.
    ///
    /// The `index` provided identifies the linear memory to grow, and `heap` is the heap reference
    /// returned by `make_heap` for the same index.
    ///
    /// The `val` value is the requested memory size in pages.
    ///
    /// Returns the old size (in pages) of the memory.
    fn translate_memory_grow(
        &mut self,
        pos: FuncCursor,
        index: MemoryIndex,
        heap: ir::Heap,
        val: ir::Value,
    ) -> WasmResult<ir::Value>;

    /// Translates a `memory.size` WebAssembly instruction.
    ///
    /// The `index` provided identifies the linear memory to query, and `heap` is the heap reference
    /// returned by `make_heap` for the same index.
    ///
    /// Returns the size in pages of the memory.
    fn translate_memory_size(
        &mut self,
        pos: FuncCursor,
        index: MemoryIndex,
        heap: ir::Heap,
    ) -> WasmResult<ir::Value>;

    /// Translate a `memory.copy` WebAssembly instruction.
    ///
    /// The `index` provided identifies the linear memory to query, and `heap` is the heap reference
    /// returned by `make_heap` for the same index.
    fn translate_memory_copy(
        &mut self,
        pos: FuncCursor,
        src_index: MemoryIndex,
        src_heap: ir::Heap,
        dst_index: MemoryIndex,
        dst_heap: ir::Heap,
        dst: ir::Value,
        src: ir::Value,
        len: ir::Value,
    ) -> WasmResult<()>;

    /// Translate a `memory.fill` WebAssembly instruction.
    ///
    /// The `index` provided identifies the linear memory to query, and `heap` is the heap reference
    /// returned by `make_heap` for the same index.
    fn translate_memory_fill(
        &mut self,
        pos: FuncCursor,
        index: MemoryIndex,
        heap: ir::Heap,
        dst: ir::Value,
        val: ir::Value,
        len: ir::Value,
    ) -> WasmResult<()>;

    /// Translate a `memory.init` WebAssembly instruction.
    ///
    /// The `index` provided identifies the linear memory to query, and `heap` is the heap reference
    /// returned by `make_heap` for the same index. `seg_index` is the index of the segment to copy
    /// from.
    #[allow(clippy::too_many_arguments)]
    fn translate_memory_init(
        &mut self,
        pos: FuncCursor,
        index: MemoryIndex,
        heap: ir::Heap,
        seg_index: u32,
        dst: ir::Value,
        src: ir::Value,
        len: ir::Value,
    ) -> WasmResult<()>;

    /// Translate a `data.drop` WebAssembly instruction.
    fn translate_data_drop(&mut self, pos: FuncCursor, seg_index: u32) -> WasmResult<()>;

    /// Translate a `table.size` WebAssembly instruction.
    fn translate_table_size(
        &mut self,
        pos: FuncCursor,
        index: TableIndex,
        table: ir::Table,
    ) -> WasmResult<ir::Value>;

    /// Translate a `table.grow` WebAssembly instruction.
    fn translate_table_grow(
        &mut self,
        pos: FuncCursor,
        table_index: TableIndex,
        table: ir::Table,
        delta: ir::Value,
        init_value: ir::Value,
    ) -> WasmResult<ir::Value>;

    /// Translate a `table.get` WebAssembly instruction.
    fn translate_table_get(
        &mut self,
        builder: &mut FunctionBuilder,
        table_index: TableIndex,
        table: ir::Table,
        index: ir::Value,
    ) -> WasmResult<ir::Value>;

    /// Translate a `table.set` WebAssembly instruction.
    fn translate_table_set(
        &mut self,
        builder: &mut FunctionBuilder,
        table_index: TableIndex,
        table: ir::Table,
        value: ir::Value,
        index: ir::Value,
    ) -> WasmResult<()>;

    /// Translate a `table.copy` WebAssembly instruction.
    #[allow(clippy::too_many_arguments)]
    fn translate_table_copy(
        &mut self,
        pos: FuncCursor,
        dst_table_index: TableIndex,
        dst_table: ir::Table,
        src_table_index: TableIndex,
        src_table: ir::Table,
        dst: ir::Value,
        src: ir::Value,
        len: ir::Value,
    ) -> WasmResult<()>;

    /// Translate a `table.fill` WebAssembly instruction.
    fn translate_table_fill(
        &mut self,
        pos: FuncCursor,
        table_index: TableIndex,
        dst: ir::Value,
        val: ir::Value,
        len: ir::Value,
    ) -> WasmResult<()>;

    /// Translates an externref ref count increment.
    fn translate_externref_inc(
        &mut self,
        pos: cranelift_codegen::cursor::FuncCursor<'_>,
        externref: ir::Value,
    ) -> WasmResult<()>;

    /// Translates an externref ref count decrement.
    fn translate_externref_dec(
        &mut self,
        pos: cranelift_codegen::cursor::FuncCursor<'_>,
        externref: ir::Value,
    ) -> WasmResult<()>;

    /// Translate a `table.init` WebAssembly instruction.
    #[allow(clippy::too_many_arguments)]
    fn translate_table_init(
        &mut self,
        pos: FuncCursor,
        seg_index: u32,
        table_index: TableIndex,
        table: ir::Table,
        dst: ir::Value,
        src: ir::Value,
        len: ir::Value,
    ) -> WasmResult<()>;

    /// Translate a `elem.drop` WebAssembly instruction.
    fn translate_elem_drop(&mut self, pos: FuncCursor, seg_index: u32) -> WasmResult<()>;

    /// Translate a `ref.null T` WebAssembly instruction.
    ///
    /// By default, translates into a null reference type.
    ///
    /// Override this if you don't use Cranelift reference types for all Wasm
    /// reference types (e.g. you use a raw pointer for `funcref`s) or if the
    /// null sentinel is not a null reference type pointer for your type. If you
    /// override this method, then you should also override
    /// `translate_ref_is_null` as well.
    fn translate_ref_null(&mut self, pos: FuncCursor, ty: Type) -> WasmResult<ir::Value>;
    // {
    //     let _ = ty;
    //     Ok(pos.ins().null(self.reference_type(ty)))
    // }

    /// Translate a `ref.is_null` WebAssembly instruction.
    ///
    /// By default, assumes that `value` is a Cranelift reference type, and that
    /// a null Cranelift reference type is the null value for all Wasm reference
    /// types.
    ///
    /// If you override this method, you probably also want to override
    /// `translate_ref_null` as well.
    fn translate_ref_is_null(
        &mut self,
        mut pos: FuncCursor,
        value: ir::Value,
    ) -> WasmResult<ir::Value> {
        let is_null = pos.ins().is_null(value);
        Ok(pos.ins().bint(ir::types::I64, is_null))
    }

    /// Translate a `ref.func` WebAssembly instruction.
    fn translate_ref_func(
        &mut self,
        pos: FuncCursor,
        func_index: FunctionIndex,
    ) -> WasmResult<ir::Value>;

    /// Translate a `global.get` WebAssembly instruction at `pos` for a global
    /// that is custom.
    fn translate_custom_global_get(
        &mut self,
        pos: FuncCursor,
        global_index: GlobalIndex,
    ) -> WasmResult<ir::Value>;

    /// Translate a `global.set` WebAssembly instruction at `pos` for a global
    /// that is custom.
    fn translate_custom_global_set(
        &mut self,
        pos: FuncCursor,
        global_index: GlobalIndex,
        val: ir::Value,
    ) -> WasmResult<()>;

    /// Translate an `i32.atomic.wait` or `i64.atomic.wait` WebAssembly instruction.
    /// The `index` provided identifies the linear memory containing the value
    /// to wait on, and `heap` is the heap reference returned by `make_heap`
    /// for the same index.  Whether the waited-on value is 32- or 64-bit can be
    /// determined by examining the type of `expected`, which must be only I32 or I64.
    ///
    /// Returns an i32, which is negative if the helper call failed.
    fn translate_atomic_wait(
        &mut self,
        pos: FuncCursor,
        index: MemoryIndex,
        heap: ir::Heap,
        addr: ir::Value,
        expected: ir::Value,
        timeout: ir::Value,
    ) -> WasmResult<ir::Value>;

    /// Translate an `atomic.notify` WebAssembly instruction.
    /// The `index` provided identifies the linear memory containing the value
    /// to wait on, and `heap` is the heap reference returned by `make_heap`
    /// for the same index.
    ///
    /// Returns an i64, which is negative if the helper call failed.
    fn translate_atomic_notify(
        &mut self,
        pos: FuncCursor,
        index: MemoryIndex,
        heap: ir::Heap,
        addr: ir::Value,
        count: ir::Value,
    ) -> WasmResult<ir::Value>;

    /// Emit code at the beginning of every wasm loop.
    ///
    /// This can be used to insert explicit interrupt or safepoint checking at
    /// the beginnings of loops.
    fn translate_loop_header(&mut self, _pos: FuncCursor) -> WasmResult<()> {
        // By default, don't emit anything.
        Ok(())
    }

    /// Optional callback for the `FunctionEnvironment` performing this translation to maintain
    /// internal state or prepare custom state for the operator to translate
    fn before_translate_operator(
        &mut self,
        _op: &Operator,
        _builder: &mut FunctionBuilder,
        _state: &FuncTranslationState,
    ) -> WasmResult<()> {
        Ok(())
    }

    /// Optional callback for the `FunctionEnvironment` performing this translation to maintain
    /// internal state or finalize custom state for the operator that was translated
    fn after_translate_operator(
        &mut self,
        _op: &Operator,
        _builder: &mut FunctionBuilder,
        _state: &FuncTranslationState,
    ) -> WasmResult<()> {
        Ok(())
    }

    /// Get the type of the global at the given index.
    fn get_global_type(&self, global_index: GlobalIndex) -> Option<WasmerType>;

    /// Push a local declaration on to the stack to track the type of locals.
    fn push_local_decl_on_stack(&mut self, ty: WasmerType);

    /// Push locals for a the params of a function on to the stack.
    fn push_params_on_stack(&mut self, function_index: LocalFunctionIndex);

    /// Get the type of the local at the given index.
    fn get_local_type(&self, local_index: u32) -> Option<WasmerType>;

    /// Get the types of all the current locals.
    fn get_local_types(&self) -> &[WasmerType];

    /// Get the type of the local at the given index.
    fn get_function_type(&self, function_index: FunctionIndex) -> Option<&FunctionType>;

    /// Get the type of a function with the given signature index.
    fn get_function_sig(&self, sig_index: SignatureIndex) -> Option<&FunctionType>;

    /// Drops all locals that need to be dropped. Useful for returning from functions.
    fn translate_drop_locals(&mut self, builder: &mut FunctionBuilder) -> WasmResult<()>;
}

'''
'''--- lib/compiler-cranelift/src/translator/func_state.rs ---
// This file contains code from external sources.
// Attributions: https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md

//! WebAssembly module and function translation state.
//!
//! The `ModuleTranslationState` struct defined in this module is used to keep track of data about
//! the whole WebAssembly module, such as the decoded type signatures.
//!
//! The `FuncTranslationState` struct defined in this module is used to keep track of the WebAssembly
//! value and control stacks during the translation of a single function.

use super::func_environ::{FuncEnvironment, GlobalVariable};
use crate::{HashMap, Occupied, Vacant};
use cranelift_codegen::ir::{self, Block, Inst, Value};
use std::vec::Vec;
use wasmer_compiler::WasmResult;
use wasmer_types::{FunctionIndex, GlobalIndex, MemoryIndex, SignatureIndex, TableIndex};

/// Information about the presence of an associated `else` for an `if`, or the
/// lack thereof.
#[derive(Debug)]
pub enum ElseData {
    /// The `if` does not already have an `else` block.
    ///
    /// This doesn't mean that it will never have an `else`, just that we
    /// haven't seen it yet.
    NoElse {
        /// If we discover that we need an `else` block, this is the jump
        /// instruction that needs to be fixed up to point to the new `else`
        /// block rather than the destination block after the `if...end`.
        branch_inst: Inst,
    },

    /// We have already allocated an `else` block.
    ///
    /// Usually we don't know whether we will hit an `if .. end` or an `if
    /// .. else .. end`, but sometimes we can tell based on the block's type
    /// signature that the signature is not valid if there isn't an `else`. In
    /// these cases, we pre-allocate the `else` block.
    WithElse {
        /// This is the `else` block.
        else_block: Block,
    },
}

/// A control stack frame can be an `if`, a `block` or a `loop`, each one having the following
/// fields:
///
/// - `destination`: reference to the `Block` that will hold the code after the control block;
/// - `num_return_values`: number of values returned by the control block;
/// - `original_stack_size`: size of the value stack at the beginning of the control block.
///
/// Moreover, the `if` frame has the `branch_inst` field that points to the `brz` instruction
/// separating the `true` and `false` branch. The `loop` frame has a `header` field that references
/// the `Block` that contains the beginning of the body of the loop.
#[derive(Debug)]
pub enum ControlStackFrame {
    If {
        destination: Block,
        else_data: ElseData,
        num_param_values: usize,
        num_return_values: usize,
        original_stack_size: usize,
        exit_is_branched_to: bool,
        blocktype: wasmer_compiler::wasmparser::TypeOrFuncType,
        /// Was the head of the `if` reachable?
        head_is_reachable: bool,
        /// What was the reachability at the end of the consequent?
        ///
        /// This is `None` until we're finished translating the consequent, and
        /// is set to `Some` either by hitting an `else` when we will begin
        /// translating the alternative, or by hitting an `end` in which case
        /// there is no alternative.
        consequent_ends_reachable: Option<bool>,
        // Note: no need for `alternative_ends_reachable` because that is just
        // `state.reachable` when we hit the `end` in the `if .. else .. end`.
    },
    Block {
        destination: Block,
        num_param_values: usize,
        num_return_values: usize,
        original_stack_size: usize,
        exit_is_branched_to: bool,
    },
    Loop {
        destination: Block,
        header: Block,
        num_param_values: usize,
        num_return_values: usize,
        original_stack_size: usize,
    },
}

/// Helper methods for the control stack objects.
impl ControlStackFrame {
    pub fn num_return_values(&self) -> usize {
        match *self {
            Self::If {
                num_return_values, ..
            }
            | Self::Block {
                num_return_values, ..
            }
            | Self::Loop {
                num_return_values, ..
            } => num_return_values,
        }
    }
    pub fn num_param_values(&self) -> usize {
        match *self {
            Self::If {
                num_param_values, ..
            }
            | Self::Block {
                num_param_values, ..
            }
            | Self::Loop {
                num_param_values, ..
            } => num_param_values,
        }
    }
    pub fn following_code(&self) -> Block {
        match *self {
            Self::If { destination, .. }
            | Self::Block { destination, .. }
            | Self::Loop { destination, .. } => destination,
        }
    }
    pub fn br_destination(&self) -> Block {
        match *self {
            Self::If { destination, .. } | Self::Block { destination, .. } => destination,
            Self::Loop { header, .. } => header,
        }
    }
    /// Private helper. Use `truncate_value_stack_to_else_params()` or
    /// `truncate_value_stack_to_original_size()` to restore value-stack state.
    fn original_stack_size(&self) -> usize {
        match *self {
            Self::If {
                original_stack_size,
                ..
            }
            | Self::Block {
                original_stack_size,
                ..
            }
            | Self::Loop {
                original_stack_size,
                ..
            } => original_stack_size,
        }
    }
    pub fn is_loop(&self) -> bool {
        match *self {
            Self::If { .. } | Self::Block { .. } => false,
            Self::Loop { .. } => true,
        }
    }

    pub fn exit_is_branched_to(&self) -> bool {
        match *self {
            Self::If {
                exit_is_branched_to,
                ..
            }
            | Self::Block {
                exit_is_branched_to,
                ..
            } => exit_is_branched_to,
            Self::Loop { .. } => false,
        }
    }

    pub fn set_branched_to_exit(&mut self) {
        match *self {
            Self::If {
                ref mut exit_is_branched_to,
                ..
            }
            | Self::Block {
                ref mut exit_is_branched_to,
                ..
            } => *exit_is_branched_to = true,
            Self::Loop { .. } => {}
        }
    }

    /// Pop values from the value stack so that it is left at the
    /// input-parameters to an else-block.
    pub fn truncate_value_stack_to_else_params(&self, stack: &mut Vec<Value>) {
        debug_assert!(matches!(self, &ControlStackFrame::If { .. }));
        stack.truncate(self.original_stack_size());
    }

    /// Pop values from the value stack so that it is left at the state it was
    /// before this control-flow frame.
    pub fn truncate_value_stack_to_original_size(&self, stack: &mut Vec<Value>) {
        // The "If" frame pushes its parameters twice, so they're available to the else block
        // (see also `FuncTranslationState::push_if`).
        // Yet, the original_stack_size member accounts for them only once, so that the else
        // block can see the same number of parameters as the consequent block. As a matter of
        // fact, we need to substract an extra number of parameter values for if blocks.
        let num_duplicated_params = match self {
            &ControlStackFrame::If {
                num_param_values, ..
            } => {
                debug_assert!(num_param_values <= self.original_stack_size());
                num_param_values
            }
            _ => 0,
        };
        stack.truncate(self.original_stack_size() - num_duplicated_params);
    }
}

/// Extra info about values. For example, on the stack.
#[derive(Debug, Clone, Default)]
pub struct ValueExtraInfo {
    /// Whether or not the value should be ref counted.
    pub ref_counted: bool,
}

/// Contains information passed along during a function's translation and that records:
///
/// - The current value and control stacks.
/// - The depth of the two unreachable control blocks stacks, that are manipulated when translating
///   unreachable code;
pub struct FuncTranslationState {
    /// A stack of values corresponding to the active values in the input wasm function at this
    /// point.
    pub(crate) stack: Vec<Value>,
    /// A stack of active control flow operations at this point in the input wasm function.
    pub(crate) control_stack: Vec<ControlStackFrame>,
    /// Is the current translation state still reachable? This is false when translating operators
    /// like End, Return, or Unreachable.
    pub(crate) reachable: bool,

    // Map of global variables that have already been created by `FuncEnvironment::make_global`.
    globals: HashMap<GlobalIndex, GlobalVariable>,

    // Map of heaps that have been created by `FuncEnvironment::make_heap`.
    heaps: HashMap<MemoryIndex, ir::Heap>,

    // Map of tables that have been created by `FuncEnvironment::make_table`.
    tables: HashMap<TableIndex, ir::Table>,

    // Map of indirect call signatures that have been created by
    // `FuncEnvironment::make_indirect_sig()`.
    // Stores both the signature reference and the number of WebAssembly arguments
    signatures: HashMap<SignatureIndex, (ir::SigRef, usize)>,

    // Imported and local functions that have been created by
    // `FuncEnvironment::make_direct_func()`.
    // Stores both the function reference and the number of WebAssembly arguments
    functions: HashMap<FunctionIndex, (ir::FuncRef, usize)>,
}

// Public methods that are exposed to non-`cranelift_wasm` API consumers.
impl FuncTranslationState {
    /// True if the current translation state expresses reachable code, false if it is unreachable.
    #[inline]
    #[allow(dead_code)]
    pub fn reachable(&self) -> bool {
        self.reachable
    }
}

impl FuncTranslationState {
    /// Construct a new, empty, `FuncTranslationState`
    pub(crate) fn new() -> Self {
        Self {
            stack: Vec::new(),
            // TODO(reftypes):
            //metadata_stack: Vec::new(),
            control_stack: Vec::new(),
            reachable: true,
            globals: HashMap::new(),
            heaps: HashMap::new(),
            tables: HashMap::new(),
            signatures: HashMap::new(),
            functions: HashMap::new(),
        }
    }

    fn clear(&mut self) {
        debug_assert!(self.stack.is_empty());
        debug_assert!(self.control_stack.is_empty());
        self.reachable = true;
        self.globals.clear();
        self.heaps.clear();
        self.tables.clear();
        self.signatures.clear();
        self.functions.clear();
    }

    /// Initialize the state for compiling a function with the given signature.
    ///
    /// This resets the state to containing only a single block representing the whole function.
    /// The exit block is the last block in the function which will contain the return instruction.
    pub(crate) fn initialize(&mut self, sig: &ir::Signature, exit_block: Block) {
        self.clear();
        self.push_block(
            exit_block,
            0,
            sig.returns
                .iter()
                .filter(|arg| arg.purpose == ir::ArgumentPurpose::Normal)
                .count(),
        );
    }

    /// Push a value with extra info attached.
    pub(crate) fn push1_extra(&mut self, val: (Value, ValueExtraInfo)) {
        self.stack.push(val.0);
        // TODO(reftypes):
        //self.metadata_stack.push(val.1);
    }

    /// Push a value with default extra info.
    pub(crate) fn push1(&mut self, val: Value) {
        self.stack.push(val);
        // TODO(reftypes):
        //self.metadata_stack.push(ValueExtraInfo::default());
    }

    /// Push multiple values.
    pub(crate) fn pushn(&mut self, vals: &[Value], _vals_metadata: &[ValueExtraInfo]) {
        assert_eq!(vals.len(), _vals_metadata.len());
        self.stack.extend_from_slice(vals);
        // TODO(reftypes):
        //self.metadata_stack.extend_from_slice(vals_metadata);
    }

    /// Pop one value.
    pub(crate) fn pop1(&mut self) -> (Value, ValueExtraInfo) {
        let val = self
            .stack
            .pop()
            .expect("attempted to pop a value from an empty stack");
        let val_metadata = Default::default();
        (val, val_metadata)
    }

    /// Peek at the top of the stack without popping it.
    pub(crate) fn peek1(&self) -> (Value, ValueExtraInfo) {
        let val = *self
            .stack
            .last()
            .expect("attempted to peek at a value on an empty stack");
        let val_metadata = Default::default();
        (val, val_metadata)
    }

    /// Pop two values. Return them in the order they were pushed.
    pub(crate) fn pop2(&mut self) -> ((Value, ValueExtraInfo), (Value, ValueExtraInfo)) {
        let v2 = self.pop1();
        let v1 = self.pop1();
        (v1, v2)
    }

    /// Pop three values. Return them in the order they were pushed.
    pub(crate) fn pop3(
        &mut self,
    ) -> (
        (Value, ValueExtraInfo),
        (Value, ValueExtraInfo),
        (Value, ValueExtraInfo),
    ) {
        let v3 = self.pop1();
        let v2 = self.pop1();
        let v1 = self.pop1();
        (v1, v2, v3)
    }

    /// Helper to ensure the the stack size is at least as big as `n`; note that due to
    /// `debug_assert` this will not execute in non-optimized builds.
    #[inline]
    fn ensure_length_is_at_least(&self, n: usize) {
        debug_assert!(
            n <= self.stack.len(),
            "attempted to access {} values but stack only has {} values",
            n,
            self.stack.len()
        );
        // TODO(reftypes):
        /*debug_assert!(
            n <= self.metadata_stack.len(),
            "attempted to access {} values but stack only has {} values",
            n,
            self.metadata_stack.len()
        );*/
    }

    /// Pop the top `n` values on the stack.
    ///
    /// The popped values are not returned. Use `peekn` to look at them before popping.
    pub(crate) fn popn(&mut self, n: usize) {
        self.ensure_length_is_at_least(n);
        let new_len = self.stack.len() - n;
        self.stack.truncate(new_len);
    }

    /// Peek at the top `n` values on the stack in the order they were pushed.
    pub(crate) fn peekn(&self, n: usize) -> (&[Value], &[ValueExtraInfo]) {
        self.ensure_length_is_at_least(n);
        let vals = &self.stack[self.stack.len() - n..];
        // TODO(reftypes):
        let vals_metadata = &[]; //&self.metadata_stack[self.metadata_stack.len() - n..];
        (vals, vals_metadata)
    }

    /// Peek at the top `n` values on the stack in the order they were pushed.
    pub(crate) fn peekn_mut(&mut self, n: usize) -> (&mut [Value], &mut [ValueExtraInfo]) {
        self.ensure_length_is_at_least(n);
        let len = self.stack.len();
        // TODO(reftypes):
        //let metadata_len = self.metadata_stack.len();
        //assert_eq!(len, metadata_len);
        let vals = &mut self.stack[len - n..];
        // TODO(reftypes):
        let vals_metadata = &mut []; //&mut self.metadata_stack[metadata_len - n..];
        (vals, vals_metadata)
    }

    /// Push a block on the control stack.
    pub(crate) fn push_block(
        &mut self,
        following_code: Block,
        num_param_types: usize,
        num_result_types: usize,
    ) {
        debug_assert!(num_param_types <= self.stack.len());
        self.control_stack.push(ControlStackFrame::Block {
            destination: following_code,
            original_stack_size: self.stack.len() - num_param_types,
            num_param_values: num_param_types,
            num_return_values: num_result_types,
            exit_is_branched_to: false,
        });
    }

    /// Push a loop on the control stack.
    pub(crate) fn push_loop(
        &mut self,
        header: Block,
        following_code: Block,
        num_param_types: usize,
        num_result_types: usize,
    ) {
        debug_assert!(num_param_types <= self.stack.len());
        self.control_stack.push(ControlStackFrame::Loop {
            header,
            destination: following_code,
            original_stack_size: self.stack.len() - num_param_types,
            num_param_values: num_param_types,
            num_return_values: num_result_types,
        });
    }

    /// Push an if on the control stack.
    pub(crate) fn push_if(
        &mut self,
        destination: Block,
        else_data: ElseData,
        num_param_types: usize,
        num_result_types: usize,
        blocktype: wasmer_compiler::wasmparser::TypeOrFuncType,
    ) {
        debug_assert!(num_param_types <= self.stack.len());

        // Push a second copy of our `if`'s parameters on the stack. This lets
        // us avoid saving them on the side in the `ControlStackFrame` for our
        // `else` block (if it exists), which would require a second heap
        // allocation. See also the comment in `translate_operator` for
        // `Operator::Else`.
        self.stack.reserve(num_param_types);
        for i in (self.stack.len() - num_param_types)..self.stack.len() {
            let val = self.stack[i];
            self.stack.push(val);
        }

        self.control_stack.push(ControlStackFrame::If {
            destination,
            else_data,
            original_stack_size: self.stack.len() - num_param_types,
            num_param_values: num_param_types,
            num_return_values: num_result_types,
            exit_is_branched_to: false,
            head_is_reachable: self.reachable,
            consequent_ends_reachable: None,
            blocktype,
        });
    }
}

/// Methods for handling entity references.
impl FuncTranslationState {
    /// Get the `GlobalVariable` reference that should be used to access the global variable
    /// `index`. Create the reference if necessary.
    /// Also return the WebAssembly type of the global.
    pub(crate) fn get_global<FE: FuncEnvironment + ?Sized>(
        &mut self,
        func: &mut ir::Function,
        index: u32,
        environ: &mut FE,
    ) -> WasmResult<GlobalVariable> {
        let index = GlobalIndex::from_u32(index);
        match self.globals.entry(index) {
            Occupied(entry) => Ok(*entry.get()),
            Vacant(entry) => Ok(*entry.insert(environ.make_global(func, index)?)),
        }
    }

    /// Get the `Heap` reference that should be used to access linear memory `index`.
    /// Create the reference if necessary.
    pub(crate) fn get_heap<FE: FuncEnvironment + ?Sized>(
        &mut self,
        func: &mut ir::Function,
        index: u32,
        environ: &mut FE,
    ) -> WasmResult<ir::Heap> {
        let index = MemoryIndex::from_u32(index);
        match self.heaps.entry(index) {
            Occupied(entry) => Ok(*entry.get()),
            Vacant(entry) => Ok(*entry.insert(environ.make_heap(func, index)?)),
        }
    }

    /// Get the `Table` reference that should be used to access table `index`.
    /// Create the reference if necessary.
    pub(crate) fn get_or_create_table<FE: FuncEnvironment + ?Sized>(
        &mut self,
        func: &mut ir::Function,
        index: u32,
        environ: &mut FE,
    ) -> WasmResult<ir::Table> {
        let index = TableIndex::from_u32(index);
        match self.tables.entry(index) {
            Occupied(entry) => Ok(*entry.get()),
            Vacant(entry) => Ok(*entry.insert(environ.make_table(func, index)?)),
        }
    }

    /// Get the `SigRef` reference that should be used to make an indirect call with signature
    /// `index`. Also return the number of WebAssembly arguments in the signature.
    ///
    /// Create the signature if necessary.
    pub(crate) fn get_indirect_sig<FE: FuncEnvironment + ?Sized>(
        &mut self,
        func: &mut ir::Function,
        index: u32,
        environ: &mut FE,
    ) -> WasmResult<(ir::SigRef, usize)> {
        let index = SignatureIndex::from_u32(index);
        match self.signatures.entry(index) {
            Occupied(entry) => Ok(*entry.get()),
            Vacant(entry) => {
                let sig = environ.make_indirect_sig(func, index)?;
                Ok(*entry.insert((sig, num_wasm_parameters(environ, &func.dfg.signatures[sig]))))
            }
        }
    }

    /// Get the `FuncRef` reference that should be used to make a direct call to function
    /// `index`. Also return the number of WebAssembly arguments in the signature.
    ///
    /// Create the function reference if necessary.
    pub(crate) fn get_direct_func<FE: FuncEnvironment + ?Sized>(
        &mut self,
        func: &mut ir::Function,
        index: u32,
        environ: &mut FE,
    ) -> WasmResult<(ir::FuncRef, usize)> {
        let index = FunctionIndex::from_u32(index);
        match self.functions.entry(index) {
            Occupied(entry) => Ok(*entry.get()),
            Vacant(entry) => {
                let fref = environ.make_direct_func(func, index)?;
                let sig = func.dfg.ext_funcs[fref].signature;
                Ok(*entry.insert((
                    fref,
                    num_wasm_parameters(environ, &func.dfg.signatures[sig]),
                )))
            }
        }
    }
}

fn num_wasm_parameters<FE: FuncEnvironment + ?Sized>(
    environ: &FE,
    signature: &ir::Signature,
) -> usize {
    (0..signature.params.len())
        .filter(|index| environ.is_wasm_parameter(signature, *index))
        .count()
}

'''
'''--- lib/compiler-cranelift/src/translator/func_translator.rs ---
// This file contains code from external sources.
// Attributions: https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md

//! Standalone WebAssembly to Cranelift IR translator.
//!
//! This module defines the `FuncTranslator` type which can translate a single WebAssembly
//! function to Cranelift IR guided by a `FuncEnvironment` which provides information about the
//! WebAssembly module and the runtime environment.

use super::code_translator::{bitcast_arguments, translate_operator, wasm_param_types};
use super::func_environ::{FuncEnvironment, ReturnMode};
use super::func_state::FuncTranslationState;
use super::translation_utils::get_vmctx_value_label;
use cranelift_codegen::entity::EntityRef;
use cranelift_codegen::ir::{self, Block, InstBuilder, ValueLabel};
use cranelift_codegen::timing;
use cranelift_frontend::{FunctionBuilder, FunctionBuilderContext, Variable};
use wasmer_compiler::wasmparser;
use wasmer_compiler::{wasm_unsupported, wptype_to_type, ModuleTranslationState, WasmResult};
use wasmer_types::LocalFunctionIndex;

/// WebAssembly to Cranelift IR function translator.
///
/// A `FuncTranslator` is used to translate a binary WebAssembly function into Cranelift IR guided
/// by a `FuncEnvironment` object. A single translator instance can be reused to translate multiple
/// functions which will reduce heap allocation traffic.
pub struct FuncTranslator {
    func_ctx: FunctionBuilderContext,
    state: FuncTranslationState,
}

impl FuncTranslator {
    /// Create a new translator.
    pub fn new() -> Self {
        Self {
            func_ctx: FunctionBuilderContext::new(),
            state: FuncTranslationState::new(),
        }
    }

    /// Translate a binary WebAssembly function.
    ///
    /// The `code` slice contains the binary WebAssembly *function code* as it appears in the code
    /// section of a WebAssembly module, not including the initial size of the function code. The
    /// slice is expected to contain two parts:
    ///
    /// - The declaration of *locals*, and
    /// - The function *body* as an expression.
    ///
    /// See [the WebAssembly specification][wasm].
    ///
    /// [wasm]: https://webassembly.github.io/spec/core/binary/modules.html#code-section
    ///
    /// The Cranelift IR function `func` should be completely empty except for the `func.signature`
    /// and `func.name` fields. The signature may contain special-purpose arguments which are not
    /// regarded as WebAssembly local variables. Any signature arguments marked as
    /// `ArgumentPurpose::Normal` are made accessible as WebAssembly local variables.
    ///
    pub fn translate<FE: FuncEnvironment + ?Sized>(
        &mut self,
        module_translation_state: &ModuleTranslationState,
        reader: &mut wasmer_compiler::FunctionReader,
        func: &mut ir::Function,
        environ: &mut FE,
        local_function_index: LocalFunctionIndex,
    ) -> WasmResult<()> {
        environ.push_params_on_stack(local_function_index);
        self.translate_from_reader(module_translation_state, reader, func, environ)
    }

    /// Translate a binary WebAssembly function from a `FunctionBinaryReader`.
    pub fn translate_from_reader<FE: FuncEnvironment + ?Sized>(
        &mut self,
        module_translation_state: &ModuleTranslationState,
        reader: &mut wasmer_compiler::FunctionReader,
        func: &mut ir::Function,
        environ: &mut FE,
    ) -> WasmResult<()> {
        let _tt = timing::wasm_translate_function();
        let _span = tracing::info_span!(
            "translate_from_reader",
            bytes = reader.get_binary_reader().bytes_remaining(),
            name = %func.name,
            signature = %func.signature,
        )
        .entered();
        debug_assert_eq!(func.dfg.num_blocks(), 0, "Function must be empty");
        debug_assert_eq!(func.dfg.num_insts(), 0, "Function must be empty");

        // This clears the `FunctionBuilderContext`.
        let mut builder = FunctionBuilder::new(func, &mut self.func_ctx);
        builder.set_srcloc(ir::SourceLoc::new(
            reader.get_binary_reader().original_position() as u32,
        ));
        let entry_block = builder.create_block();
        builder.append_block_params_for_function_params(entry_block);
        builder.switch_to_block(entry_block); // This also creates values for the arguments.
        builder.seal_block(entry_block); // Declare all predecessors known.

        // Make sure the entry block is inserted in the layout before we make any callbacks to
        // `environ`. The callback functions may need to insert things in the entry block.
        builder.ensure_inserted_block();

        let num_params = declare_wasm_parameters(&mut builder, entry_block, environ);

        // Set up the translation state with a single pushed control block representing the whole
        // function and its return values.
        let exit_block = builder.create_block();
        builder.append_block_params_for_function_returns(exit_block);
        self.state.initialize(&builder.func.signature, exit_block);

        parse_local_decls(reader, &mut builder, num_params, environ)?;
        parse_function_body(
            module_translation_state,
            reader,
            &mut builder,
            &mut self.state,
            environ,
        )?;

        builder.finalize();
        Ok(())
    }
}

/// Declare local variables for the signature parameters that correspond to WebAssembly locals.
///
/// Return the number of local variables declared.
fn declare_wasm_parameters<FE: FuncEnvironment + ?Sized>(
    builder: &mut FunctionBuilder,
    entry_block: Block,
    environ: &FE,
) -> usize {
    let sig_len = builder.func.signature.params.len();
    let mut next_local = 0;
    for i in 0..sig_len {
        let param_type = builder.func.signature.params[i];
        // There may be additional special-purpose parameters in addition to the normal WebAssembly
        // signature parameters. For example, a `vmctx` pointer.
        if environ.is_wasm_parameter(&builder.func.signature, i) {
            // This is a normal WebAssembly signature parameter, so create a local for it.
            let local = Variable::new(next_local);
            builder.declare_var(local, param_type.value_type);
            next_local += 1;

            let param_value = builder.block_params(entry_block)[i];
            builder.def_var(local, param_value);
        }
        if param_type.purpose == ir::ArgumentPurpose::VMContext {
            let param_value = builder.block_params(entry_block)[i];
            builder.set_val_label(param_value, get_vmctx_value_label());
        }
    }

    next_local
}

/// Parse the local variable declarations that precede the function body.
///
/// Declare local variables, starting from `num_params`.
fn parse_local_decls<FE: FuncEnvironment + ?Sized>(
    reader: &wasmer_compiler::FunctionReader,
    builder: &mut FunctionBuilder,
    num_params: usize,
    environ: &mut FE,
) -> WasmResult<()> {
    let mut next_local = num_params;
    let mut local_reader = reader.get_locals_reader()?;
    let local_count = local_reader.get_count();
    for _ in 0..local_count {
        builder.set_srcloc(ir::SourceLoc::new(local_reader.original_position() as u32));
        let (count, ty) = local_reader.read()?;
        declare_locals(builder, count, ty, &mut next_local, environ)?;
    }

    Ok(())
}

/// Declare `count` local variables of the same type, starting from `next_local`.
///
/// Fail if the type is not valid for a local.
fn declare_locals<FE: FuncEnvironment + ?Sized>(
    builder: &mut FunctionBuilder,
    count: u32,
    wasm_type: wasmparser::Type,
    next_local: &mut usize,
    environ: &mut FE,
) -> WasmResult<()> {
    // All locals are initialized to 0.
    use wasmparser::Type::*;
    let zeroval = match wasm_type {
        I32 => builder.ins().iconst(ir::types::I32, 0),
        I64 => builder.ins().iconst(ir::types::I64, 0),
        F32 => builder.ins().f32const(ir::immediates::Ieee32::with_bits(0)),
        F64 => builder.ins().f64const(ir::immediates::Ieee64::with_bits(0)),
        V128 => {
            let constant_handle = builder.func.dfg.constants.insert([0; 16].to_vec().into());
            builder.ins().vconst(ir::types::I8X16, constant_handle)
        }
        ExternRef => builder.ins().null(environ.reference_type()),
        FuncRef => builder.ins().null(environ.reference_type()),
        ty => return Err(wasm_unsupported!("unsupported local type {:?}", ty)),
    };

    let wasmer_ty = wptype_to_type(wasm_type).unwrap();
    let ty = builder.func.dfg.value_type(zeroval);
    for _ in 0..count {
        let local = Variable::new(*next_local);
        builder.declare_var(local, ty);
        builder.def_var(local, zeroval);
        builder.set_val_label(zeroval, ValueLabel::new(*next_local));
        environ.push_local_decl_on_stack(wasmer_ty);
        *next_local += 1;
    }
    Ok(())
}

/// Parse the function body in `reader`.
///
/// This assumes that the local variable declarations have already been parsed and function
/// arguments and locals are declared in the builder.
fn parse_function_body<FE: FuncEnvironment + ?Sized>(
    module_translation_state: &ModuleTranslationState,
    reader: &wasmer_compiler::FunctionReader,
    builder: &mut FunctionBuilder,
    state: &mut FuncTranslationState,
    environ: &mut FE,
) -> WasmResult<()> {
    // The control stack is initialized with a single block representing the whole function.
    debug_assert_eq!(state.control_stack.len(), 1, "State not initialized");
    let mut reader = reader.get_operators_reader()?.into_iter_with_offsets();

    // Keep going until the final `End` operator which pops the outermost block.
    while !state.control_stack.is_empty() {
        let (op, pos) = reader.next().unwrap()?;
        builder.set_srcloc(ir::SourceLoc::new(pos as u32));
        environ.before_translate_operator(&op, builder, state)?;
        translate_operator(module_translation_state, &op, builder, state, environ)?;
        environ.after_translate_operator(&op, builder, state)?;
    }

    // When returning we drop all values in locals and on the stack.

    // The final `End` operator left us in the exit block where we need to manually add a return
    // instruction.
    //
    // If the exit block is unreachable, it may not have the correct arguments, so we would
    // generate a return instruction that doesn't match the signature.
    if state.reachable {
        debug_assert!(builder.is_pristine());
        if !builder.is_unreachable() {
            environ.translate_drop_locals(builder)?;

            let _num_elems_to_drop = state.stack.len() - builder.func.signature.returns.len();
            // drop elements on the stack that we're not returning
            /*for val in state
                .stack
                .iter()
                .zip(state.metadata_stack.iter())
                .take(num_elems_to_drop)
                .filter(|(_, metadata)| metadata.ref_counted)
                .map(|(val, _)| val)
            {
                environ.translate_externref_dec(builder.cursor(), *val)?;
            }*/

            // TODO: look into what `state.reachable` check above does as well as `!builder.is_unreachable`, do we need that too for ref counting?

            match environ.return_mode() {
                ReturnMode::NormalReturns => {
                    let return_types = wasm_param_types(&builder.func.signature.returns, |i| {
                        environ.is_wasm_return(&builder.func.signature, i)
                    });
                    bitcast_arguments(&mut state.stack, &return_types, builder);
                    builder.ins().return_(&state.stack)
                }
                ReturnMode::FallthroughReturn => builder.ins().fallthrough_return(&state.stack),
            };
        }
    }

    // Discard any remaining values on the stack. Either we just returned them,
    // or the end of the function is unreachable.
    state.stack.clear();
    //state.metadata_stack.clear();

    Ok(())
}

'''
'''--- lib/compiler-cranelift/src/translator/mod.rs ---
//! Tools for translating wasm function bytecode to Cranelift IR.

mod code_translator;
mod func_environ;
mod func_state;
mod func_translator;
mod translation_utils;
mod unwind;

pub use self::func_environ::{FuncEnvironment, GlobalVariable, ReturnMode, TargetEnvironment};
pub use self::func_state::FuncTranslationState;
pub use self::func_translator::FuncTranslator;
pub use self::translation_utils::{
    get_vmctx_value_label, irlibcall_to_libcall, irreloc_to_relocationkind,
    signature_to_cranelift_ir, transform_jump_table, type_to_irtype,
};
pub(crate) use self::unwind::{compiled_function_unwind_info, CraneliftUnwindInfo};

'''
'''--- lib/compiler-cranelift/src/translator/translation_utils.rs ---
//! Helper functions and structures for the translation.

use super::func_environ::TargetEnvironment;
use crate::std::string::ToString;
use core::u32;
use cranelift_codegen::binemit::Reloc;
use cranelift_codegen::ir::{self, AbiParam};
use cranelift_codegen::isa::TargetFrontendConfig;
use cranelift_entity::{EntityRef as CraneliftEntityRef, SecondaryMap as CraneliftSecondaryMap};
use cranelift_frontend::FunctionBuilder;
use wasmer_compiler::wasm_unsupported;
use wasmer_compiler::wasmparser;
use wasmer_compiler::{JumpTable, RelocationKind};
use wasmer_compiler::{WasmError, WasmResult};
use wasmer_types::entity::{EntityRef, SecondaryMap};
use wasmer_types::{FunctionType, Type};
use wasmer_vm::libcalls::LibCall;

/// Helper function translate a Function signature into Cranelift Ir
pub fn signature_to_cranelift_ir(
    signature: &FunctionType,
    target_config: TargetFrontendConfig,
) -> ir::Signature {
    let mut sig = ir::Signature::new(target_config.default_call_conv);
    sig.params.extend(signature.params().iter().map(|&ty| {
        let cret_arg: ir::Type = type_to_irtype(ty, target_config)
            .expect("only numeric types are supported in function signatures");
        AbiParam::new(cret_arg)
    }));
    sig.returns.extend(signature.results().iter().map(|&ty| {
        let cret_arg: ir::Type = type_to_irtype(ty, target_config)
            .expect("only numeric types are supported in function signatures");
        AbiParam::new(cret_arg)
    }));
    // The Vmctx signature
    sig.params.insert(
        0,
        AbiParam::special(target_config.pointer_type(), ir::ArgumentPurpose::VMContext),
    );
    sig
}

/// Helper function translating wasmparser types to Cranelift types when possible.
pub fn reference_type(target_config: TargetFrontendConfig) -> WasmResult<ir::Type> {
    match target_config.pointer_type() {
        ir::types::I32 => Ok(ir::types::R32),
        ir::types::I64 => Ok(ir::types::R64),
        _ => Err(WasmError::Unsupported(
            "unsupported pointer type".to_string(),
        )),
    }
}

/// Helper function translating wasmparser types to Cranelift types when possible.
pub fn type_to_irtype(ty: Type, target_config: TargetFrontendConfig) -> WasmResult<ir::Type> {
    match ty {
        Type::I32 => Ok(ir::types::I32),
        Type::I64 => Ok(ir::types::I64),
        Type::F32 => Ok(ir::types::F32),
        Type::F64 => Ok(ir::types::F64),
        Type::V128 => Ok(ir::types::I8X16),
        Type::ExternRef | Type::FuncRef => reference_type(target_config),
        // ty => Err(wasm_unsupported!("type_to_type: wasm type {:?}", ty)),
    }
}

/// Transform Cranelift LibCall into runtime LibCall
pub fn irlibcall_to_libcall(libcall: ir::LibCall) -> LibCall {
    match libcall {
        ir::LibCall::Probestack => LibCall::Probestack,
        ir::LibCall::CeilF32 => LibCall::CeilF32,
        ir::LibCall::CeilF64 => LibCall::CeilF64,
        ir::LibCall::FloorF32 => LibCall::FloorF32,
        ir::LibCall::FloorF64 => LibCall::FloorF64,
        ir::LibCall::TruncF32 => LibCall::TruncF32,
        ir::LibCall::TruncF64 => LibCall::TruncF64,
        ir::LibCall::NearestF32 => LibCall::NearestF32,
        ir::LibCall::NearestF64 => LibCall::NearestF64,
        _ => panic!("Unsupported libcall"),
    }
}

/// Transform Cranelift Reloc to compiler Relocation
pub fn irreloc_to_relocationkind(reloc: Reloc) -> RelocationKind {
    match reloc {
        Reloc::Abs4 => RelocationKind::Abs4,
        Reloc::Abs8 => RelocationKind::Abs8,
        Reloc::X86PCRel4 => RelocationKind::X86PCRel4,
        Reloc::X86PCRelRodata4 => RelocationKind::X86PCRelRodata4,
        Reloc::X86CallPCRel4 => RelocationKind::X86CallPCRel4,
        Reloc::X86CallPLTRel4 => RelocationKind::X86CallPLTRel4,
        Reloc::X86GOTPCRel4 => RelocationKind::X86GOTPCRel4,
        _ => panic!("The relocation {} is not yet supported.", reloc),
    }
}

/// Create a `Block` with the given Wasm parameters.
pub fn block_with_params<PE: TargetEnvironment + ?Sized>(
    builder: &mut FunctionBuilder,
    params: &[wasmparser::Type],
    environ: &PE,
) -> WasmResult<ir::Block> {
    let block = builder.create_block();
    for ty in params.iter() {
        match ty {
            wasmparser::Type::I32 => {
                builder.append_block_param(block, ir::types::I32);
            }
            wasmparser::Type::I64 => {
                builder.append_block_param(block, ir::types::I64);
            }
            wasmparser::Type::F32 => {
                builder.append_block_param(block, ir::types::F32);
            }
            wasmparser::Type::F64 => {
                builder.append_block_param(block, ir::types::F64);
            }
            wasmparser::Type::ExternRef | wasmparser::Type::FuncRef => {
                builder.append_block_param(block, environ.reference_type());
            }
            wasmparser::Type::V128 => {
                builder.append_block_param(block, ir::types::I8X16);
            }
            ty => {
                return Err(wasm_unsupported!(
                    "block_with_params: type {:?} in multi-value block's signature",
                    ty
                ))
            }
        }
    }
    Ok(block)
}

/// Turns a `wasmparser` `f32` into a `Cranelift` one.
pub fn f32_translation(x: wasmparser::Ieee32) -> ir::immediates::Ieee32 {
    ir::immediates::Ieee32::with_bits(x.bits())
}

/// Turns a `wasmparser` `f64` into a `Cranelift` one.
pub fn f64_translation(x: wasmparser::Ieee64) -> ir::immediates::Ieee64 {
    ir::immediates::Ieee64::with_bits(x.bits())
}

/// Special VMContext value label. It is tracked as 0xffff_fffe label.
pub fn get_vmctx_value_label() -> ir::ValueLabel {
    const VMCTX_LABEL: u32 = 0xffff_fffe;
    ir::ValueLabel::from_u32(VMCTX_LABEL)
}

/// Transforms Cranelift JumpTable's into runtime JumpTables
pub fn transform_jump_table(
    jt_offsets: CraneliftSecondaryMap<ir::JumpTable, u32>,
) -> SecondaryMap<JumpTable, u32> {
    let mut func_jt_offsets = SecondaryMap::with_capacity(jt_offsets.capacity());

    for (key, value) in jt_offsets.iter() {
        let new_key = JumpTable::new(key.index());
        func_jt_offsets[new_key] = *value;
    }
    func_jt_offsets
}

'''
'''--- lib/compiler-cranelift/src/translator/unwind.rs ---
//! A `Compilation` contains the compiled function bodies for a WebAssembly
//! module.

#[cfg(feature = "unwind")]
use cranelift_codegen::isa::unwind::{systemv::UnwindInfo as DwarfFDE, UnwindInfo};
use cranelift_codegen::print_errors::pretty_error;
use cranelift_codegen::{isa, Context};
use wasmer_compiler::{CompileError, CompiledFunctionUnwindInfo};

/// Cranelift specific unwind info
pub(crate) enum CraneliftUnwindInfo {
    #[cfg(feature = "unwind")]
    /// Windows Unwind info
    WindowsX64(Vec<u8>),
    /// Dwarf FDE
    #[cfg(feature = "unwind")]
    FDE(DwarfFDE),
    /// No Unwind info attached
    None,
}

impl CraneliftUnwindInfo {
    /// Transform the `CraneliftUnwindInfo` to the Windows format.
    ///
    /// We skip the DWARF as it is not needed for trampolines (which are the
    /// main users of this function)
    pub fn maybe_into_to_windows_unwind(self) -> Option<CompiledFunctionUnwindInfo> {
        match self {
            #[cfg(feature = "unwind")]
            Self::WindowsX64(unwind_info) => {
                Some(CompiledFunctionUnwindInfo::WindowsX64(unwind_info))
            }
            _ => None,
        }
    }
}

#[cfg(feature = "unwind")]
/// Constructs unwind info object from Cranelift IR
pub(crate) fn compiled_function_unwind_info(
    isa: &dyn isa::TargetIsa,
    context: &Context,
) -> Result<CraneliftUnwindInfo, CompileError> {
    let unwind_info = context
        .create_unwind_info(isa)
        .map_err(|error| CompileError::Codegen(pretty_error(&context.func, Some(isa), error)))?;

    match unwind_info {
        Some(UnwindInfo::WindowsX64(unwind)) => {
            let size = unwind.emit_size();
            let mut data: Vec<u8> = vec![0; size];
            unwind.emit(&mut data[..]);
            Ok(CraneliftUnwindInfo::WindowsX64(data))
        }
        Some(UnwindInfo::SystemV(unwind)) => Ok(CraneliftUnwindInfo::FDE(unwind)),
        Some(_) | None => Ok(CraneliftUnwindInfo::None),
    }
}

#[cfg(not(feature = "unwind"))]
/// Constructs unwind info object from Cranelift IR
pub(crate) fn compiled_function_unwind_info(
    isa: &dyn isa::TargetIsa,
    context: &Context,
) -> Result<CraneliftUnwindInfo, CompileError> {
    Ok(CraneliftUnwindInfo::None)
}

'''
'''--- lib/compiler-llvm/Cargo.toml ---
[package]
name = "wasmer-compiler-llvm"
version = "2.1.0"
description = "LLVM compiler for Wasmer WebAssembly runtime"
categories = ["wasm"]
keywords = ["wasm", "webassembly", "compiler", "llvm"]
authors = ["Wasmer Engineering Team <engineering@wasmer.io>"]
repository = "https://github.com/wasmerio/wasmer"
documentation = "https://docs.rs/wasmer-compiler-llvm/"
license = "MIT"
readme = "README.md"
edition = "2018"

[dependencies]
wasmer-compiler = { path = "../compiler", version = "=2.4.1", package = "wasmer-compiler-near", features = ["translator"] }
wasmer-vm = { path = "../vm", version = "=2.4.1", package = "wasmer-vm-near" }
wasmer-types = { path = "../types", version = "=2.4.1", package = "wasmer-types-near" }
target-lexicon = { version = "0.12.2", default-features = false }
smallvec = "1.6"
object = { version = "0.27", default-features = false, features = ["read"] }
libc = { version = "^0.2", default-features = false }
byteorder = "1"
itertools = "0.10"
rayon = "1.5"

[dependencies.inkwell]
package = "inkwell"
version = "0.1.0-beta.4"
default-features = false
features = ["llvm12-0", "target-x86", "target-aarch64"]

[build-dependencies]
cc = "1.0"
lazy_static = "1.4"
regex = "1.3"
semver = "1.0"
rustc_version = "0.4"

[features]
test = []

'''
'''--- lib/compiler-llvm/README.md ---
# `wasmer-compiler-llvm` [![Build Status](https://github.com/wasmerio/wasmer/workflows/build/badge.svg?style=flat-square)](https://github.com/wasmerio/wasmer/actions?query=workflow%3Abuild) [![Join Wasmer Slack](https://img.shields.io/static/v1?label=Slack&message=join%20chat&color=brighgreen&style=flat-square)](https://slack.wasmer.io) [![MIT License](https://img.shields.io/github/license/wasmerio/wasmer.svg?style=flat-square)](https://github.com/wasmerio/wasmer/blob/master/LICENSE) [![crates.io](https://img.shields.io/crates/v/wasmer-compiler-llvm.svg)](https://crates.io/crates/wasmer-compiler-llvm)

This crate contains a compiler implementation based on [the LLVM Compiler Infrastructure][LLVM].

## Usage

```rust
use wasmer::{Store, Universal};
use wasmer_compiler_llvm::LLVM;

let compiler = LLVM::new();
// Put it into an engine and add it to the store
let store = Store::new(&Universal::new(compiler).engine());
```

*Note: you can find a [full working example using LLVM compiler here][example].*

## When to use LLVM

We recommend using LLVM as the default compiler when running WebAssembly
files on any **production** system, as it offers maximum peformance near
to native speeds.

## Requirements

The LLVM compiler requires a valid installation of LLVM in your system.
It currently requires **LLVM 12**.

You can install LLVM easily on your Debian-like system via this command:

```bash
wget https://apt.llvm.org/llvm.sh -O /tmp/llvm.sh
sudo bash /tmp/llvm.sh 12
```

Or in macOS:

```bash
brew install llvm
```

Or via any of the [pre-built binaries that LLVM offers][llvm-pre-built].

[LLVM]: https://llvm.org/
[example]: https://github.com/wasmerio/wasmer/blob/master/examples/compiler_llvm.rs
[llvm-pre-built]: https://releases.llvm.org/download.html

'''
'''--- lib/compiler-llvm/src/abi/aarch64_systemv.rs ---
use crate::abi::Abi;
use crate::translator::intrinsics::{type_to_llvm, Intrinsics};
use inkwell::{
    attributes::{Attribute, AttributeLoc},
    builder::Builder,
    context::Context,
    types::{AnyType, BasicMetadataTypeEnum, BasicType, FunctionType, StructType},
    values::{BasicValue, BasicValueEnum, CallSiteValue, FunctionValue, IntValue, PointerValue},
    AddressSpace,
};
use wasmer_compiler::CompileError;
use wasmer_types::{FunctionType as FuncSig, Type};
use wasmer_vm::VMOffsets;

use std::convert::TryInto;

/// Implementation of the [`Abi`] trait for the Aarch64 ABI on Linux.
pub struct Aarch64SystemV {}

impl Abi for Aarch64SystemV {
    // Given a function definition, retrieve the parameter that is the vmctx pointer.
    fn get_vmctx_ptr_param<'ctx>(&self, func_value: &FunctionValue<'ctx>) -> PointerValue<'ctx> {
        func_value
            .get_nth_param(
                if func_value
                    .get_enum_attribute(
                        AttributeLoc::Param(0),
                        Attribute::get_named_enum_kind_id("sret"),
                    )
                    .is_some()
                {
                    1
                } else {
                    0
                },
            )
            .unwrap()
            .into_pointer_value()
    }

    // Given a wasm function type, produce an llvm function declaration.
    fn func_type_to_llvm<'ctx>(
        &self,
        context: &'ctx Context,
        intrinsics: &Intrinsics<'ctx>,
        offsets: Option<&VMOffsets>,
        sig: &FuncSig,
    ) -> Result<(FunctionType<'ctx>, Vec<(Attribute, AttributeLoc)>), CompileError> {
        let user_param_types = sig.params().iter().map(|&ty| type_to_llvm(intrinsics, ty));

        let param_types =
            std::iter::once(Ok(intrinsics.ctx_ptr_ty.as_basic_type_enum())).chain(user_param_types);

        let vmctx_attributes = |i: u32| {
            vec![
                (
                    context.create_enum_attribute(Attribute::get_named_enum_kind_id("nofree"), 0),
                    AttributeLoc::Param(i),
                ),
                (
                    if let Some(offsets) = offsets {
                        context.create_enum_attribute(
                            Attribute::get_named_enum_kind_id("dereferenceable"),
                            offsets.size_of_vmctx().into(),
                        )
                    } else {
                        context
                            .create_enum_attribute(Attribute::get_named_enum_kind_id("nonnull"), 0)
                    },
                    AttributeLoc::Param(i),
                ),
                (
                    context.create_enum_attribute(
                        Attribute::get_named_enum_kind_id("align"),
                        std::mem::align_of::<wasmer_vm::VMContext>()
                            .try_into()
                            .unwrap(),
                    ),
                    AttributeLoc::Param(i),
                ),
            ]
        };

        Ok(match sig.results() {
            [] => (
                intrinsics.void_ty.fn_type(
                    param_types
                        .map(|v| v.map(Into::into))
                        .collect::<Result<Vec<BasicMetadataTypeEnum>, _>>()?
                        .as_slice(),
                    false,
                ),
                vmctx_attributes(0),
            ),
            [_] => {
                let single_value = sig.results()[0];
                (
                    type_to_llvm(intrinsics, single_value)?.fn_type(
                        param_types
                            .map(|v| v.map(Into::into))
                            .collect::<Result<Vec<BasicMetadataTypeEnum>, _>>()?
                            .as_slice(),
                        false,
                    ),
                    vmctx_attributes(0),
                )
            }
            [Type::F32, Type::F32] => {
                let f32_ty = intrinsics.f32_ty.as_basic_type_enum();
                (
                    context.struct_type(&[f32_ty, f32_ty], false).fn_type(
                        param_types
                            .map(|v| v.map(Into::into))
                            .collect::<Result<Vec<BasicMetadataTypeEnum>, _>>()?
                            .as_slice(),
                        false,
                    ),
                    vmctx_attributes(0),
                )
            }
            [Type::F64, Type::F64] => {
                let f64_ty = intrinsics.f64_ty.as_basic_type_enum();
                (
                    context.struct_type(&[f64_ty, f64_ty], false).fn_type(
                        param_types
                            .map(|v| v.map(Into::into))
                            .collect::<Result<Vec<BasicMetadataTypeEnum>, _>>()?
                            .as_slice(),
                        false,
                    ),
                    vmctx_attributes(0),
                )
            }
            [Type::F32, Type::F32, Type::F32] => {
                let f32_ty = intrinsics.f32_ty.as_basic_type_enum();
                (
                    context
                        .struct_type(&[f32_ty, f32_ty, f32_ty], false)
                        .fn_type(
                            param_types
                                .map(|v| v.map(Into::into))
                                .collect::<Result<Vec<BasicMetadataTypeEnum>, _>>()?
                                .as_slice(),
                            false,
                        ),
                    vmctx_attributes(0),
                )
            }
            [Type::F32, Type::F32, Type::F32, Type::F32] => {
                let f32_ty = intrinsics.f32_ty.as_basic_type_enum();
                (
                    context
                        .struct_type(&[f32_ty, f32_ty, f32_ty, f32_ty], false)
                        .fn_type(
                            param_types
                                .map(|v| v.map(Into::into))
                                .collect::<Result<Vec<BasicMetadataTypeEnum>, _>>()?
                                .as_slice(),
                            false,
                        ),
                    vmctx_attributes(0),
                )
            }
            _ => {
                let sig_returns_bitwidths = sig
                    .results()
                    .iter()
                    .map(|ty| match ty {
                        Type::I32 | Type::F32 => 32,
                        Type::I64 | Type::F64 => 64,
                        Type::V128 => 128,
                        Type::ExternRef | Type::FuncRef => 64, /* pointer */
                    })
                    .collect::<Vec<i32>>();
                match sig_returns_bitwidths.as_slice() {
                    [32, 32] => (
                        intrinsics.i64_ty.fn_type(
                            param_types
                                .map(|v| v.map(Into::into))
                                .collect::<Result<Vec<BasicMetadataTypeEnum>, _>>()?
                                .as_slice(),
                            false,
                        ),
                        vmctx_attributes(0),
                    ),
                    [32, 64]
                    | [64, 32]
                    | [64, 64]
                    | [32, 32, 32]
                    | [64, 32, 32]
                    | [32, 32, 64]
                    | [32, 32, 32, 32] => (
                        intrinsics.i64_ty.array_type(2).fn_type(
                            param_types
                                .map(|v| v.map(Into::into))
                                .collect::<Result<Vec<BasicMetadataTypeEnum>, _>>()?
                                .as_slice(),
                            false,
                        ),
                        vmctx_attributes(0),
                    ),
                    _ => {
                        let basic_types: Vec<_> = sig
                            .results()
                            .iter()
                            .map(|&ty| type_to_llvm(intrinsics, ty))
                            .collect::<Result<_, _>>()?;

                        let sret = context.struct_type(&basic_types, false);
                        let sret_ptr = sret.ptr_type(AddressSpace::Generic);

                        let param_types =
                            std::iter::once(Ok(sret_ptr.as_basic_type_enum())).chain(param_types);

                        let mut attributes = vec![(
                            context.create_type_attribute(
                                Attribute::get_named_enum_kind_id("sret"),
                                sret.as_any_type_enum(),
                            ),
                            AttributeLoc::Param(0),
                        )];
                        attributes.append(&mut vmctx_attributes(1));

                        (
                            intrinsics.void_ty.fn_type(
                                param_types
                                    .map(|v| v.map(Into::into))
                                    .collect::<Result<Vec<BasicMetadataTypeEnum>, _>>()?
                                    .as_slice(),
                                false,
                            ),
                            attributes,
                        )
                    }
                }
            }
        })
    }

    // Marshall wasm stack values into function parameters.
    fn args_to_call<'ctx>(
        &self,
        alloca_builder: &Builder<'ctx>,
        func_sig: &FuncSig,
        ctx_ptr: PointerValue<'ctx>,
        llvm_fn_ty: &FunctionType<'ctx>,
        values: &[BasicValueEnum<'ctx>],
    ) -> Vec<BasicValueEnum<'ctx>> {
        // If it's an sret, allocate the return space.
        let sret = if llvm_fn_ty.get_return_type().is_none() && func_sig.results().len() > 1 {
            Some(
                alloca_builder.build_alloca(
                    llvm_fn_ty.get_param_types()[0]
                        .into_pointer_type()
                        .get_element_type()
                        .into_struct_type(),
                    "sret",
                ),
            )
        } else {
            None
        };

        let values = std::iter::once(ctx_ptr.as_basic_value_enum()).chain(values.iter().copied());

        if let Some(sret) = sret {
            std::iter::once(sret.as_basic_value_enum())
                .chain(values)
                .collect()
        } else {
            values.collect()
        }
    }

    // Given a CallSite, extract the returned values and return them in a Vec.
    fn rets_from_call<'ctx>(
        &self,
        builder: &Builder<'ctx>,
        intrinsics: &Intrinsics<'ctx>,
        call_site: CallSiteValue<'ctx>,
        func_sig: &FuncSig,
    ) -> Vec<BasicValueEnum<'ctx>> {
        let split_i64 = |value: IntValue<'ctx>| -> (IntValue<'ctx>, IntValue<'ctx>) {
            assert!(value.get_type() == intrinsics.i64_ty);
            let low = builder.build_int_truncate(value, intrinsics.i32_ty, "");
            let lshr =
                builder.build_right_shift(value, intrinsics.i64_ty.const_int(32, false), false, "");
            let high = builder.build_int_truncate(lshr, intrinsics.i32_ty, "");
            (low, high)
        };

        let casted = |value: BasicValueEnum<'ctx>, ty: Type| -> BasicValueEnum<'ctx> {
            match ty {
                Type::I32 => {
                    assert!(
                        value.get_type() == intrinsics.i32_ty.as_basic_type_enum()
                            || value.get_type() == intrinsics.f32_ty.as_basic_type_enum()
                    );
                    builder.build_bitcast(value, intrinsics.i32_ty, "")
                }
                Type::F32 => {
                    assert!(
                        value.get_type() == intrinsics.i32_ty.as_basic_type_enum()
                            || value.get_type() == intrinsics.f32_ty.as_basic_type_enum()
                    );
                    builder.build_bitcast(value, intrinsics.f32_ty, "")
                }
                Type::I64 => {
                    assert!(
                        value.get_type() == intrinsics.i64_ty.as_basic_type_enum()
                            || value.get_type() == intrinsics.f64_ty.as_basic_type_enum()
                    );
                    builder.build_bitcast(value, intrinsics.i64_ty, "")
                }
                Type::F64 => {
                    assert!(
                        value.get_type() == intrinsics.i64_ty.as_basic_type_enum()
                            || value.get_type() == intrinsics.f64_ty.as_basic_type_enum()
                    );
                    builder.build_bitcast(value, intrinsics.f64_ty, "")
                }
                Type::V128 => {
                    assert!(value.get_type() == intrinsics.i128_ty.as_basic_type_enum());
                    value
                }
                Type::ExternRef | Type::FuncRef => {
                    assert!(value.get_type() == intrinsics.funcref_ty.as_basic_type_enum());
                    value
                }
            }
        };

        if let Some(basic_value) = call_site.try_as_basic_value().left() {
            if func_sig.results().len() > 1 {
                if basic_value.get_type() == intrinsics.i64_ty.as_basic_type_enum() {
                    assert!(func_sig.results().len() == 2);
                    let value = basic_value.into_int_value();
                    let (low, high) = split_i64(value);
                    let low = casted(low.into(), func_sig.results()[0]);
                    let high = casted(high.into(), func_sig.results()[1]);
                    return vec![low, high];
                }
                if basic_value.is_struct_value() {
                    let struct_value = basic_value.into_struct_value();
                    return (0..struct_value.get_type().count_fields())
                        .map(|i| builder.build_extract_value(struct_value, i, "").unwrap())
                        .collect::<Vec<_>>();
                }
                let array_value = basic_value.into_array_value();
                let low = builder
                    .build_extract_value(array_value, 0, "")
                    .unwrap()
                    .into_int_value();
                let high = builder
                    .build_extract_value(array_value, 1, "")
                    .unwrap()
                    .into_int_value();
                let func_sig_returns_bitwidths = func_sig
                    .results()
                    .iter()
                    .map(|ty| match ty {
                        Type::I32 | Type::F32 => 32,
                        Type::I64 | Type::F64 => 64,
                        Type::V128 => 128,
                        Type::ExternRef | Type::FuncRef => 64, /* pointer */
                    })
                    .collect::<Vec<i32>>();

                match func_sig_returns_bitwidths.as_slice() {
                    [32, 64] => {
                        let (low, _) = split_i64(low);
                        let low = casted(low.into(), func_sig.results()[0]);
                        let high = casted(high.into(), func_sig.results()[1]);
                        vec![low, high]
                    }
                    [64, 32] => {
                        let (high, _) = split_i64(high);
                        let low = casted(low.into(), func_sig.results()[0]);
                        let high = casted(high.into(), func_sig.results()[1]);
                        vec![low, high]
                    }
                    [64, 64] => {
                        let low = casted(low.into(), func_sig.results()[0]);
                        let high = casted(high.into(), func_sig.results()[1]);
                        vec![low, high]
                    }
                    [32, 32, 32] => {
                        let (v1, v2) = split_i64(low);
                        let (v3, _) = split_i64(high);
                        let v1 = casted(v1.into(), func_sig.results()[0]);
                        let v2 = casted(v2.into(), func_sig.results()[1]);
                        let v3 = casted(v3.into(), func_sig.results()[2]);
                        vec![v1, v2, v3]
                    }
                    [32, 32, 64] => {
                        let (v1, v2) = split_i64(low);
                        let v1 = casted(v1.into(), func_sig.results()[0]);
                        let v2 = casted(v2.into(), func_sig.results()[1]);
                        let v3 = casted(high.into(), func_sig.results()[2]);
                        vec![v1, v2, v3]
                    }
                    [64, 32, 32] => {
                        let v1 = casted(low.into(), func_sig.results()[0]);
                        let (v2, v3) = split_i64(high);
                        let v2 = casted(v2.into(), func_sig.results()[1]);
                        let v3 = casted(v3.into(), func_sig.results()[2]);
                        vec![v1, v2, v3]
                    }
                    [32, 32, 32, 32] => {
                        let (v1, v2) = split_i64(low);
                        let (v3, v4) = split_i64(high);
                        let v1 = casted(v1.into(), func_sig.results()[0]);
                        let v2 = casted(v2.into(), func_sig.results()[1]);
                        let v3 = casted(v3.into(), func_sig.results()[2]);
                        let v4 = casted(v4.into(), func_sig.results()[3]);
                        vec![v1, v2, v3, v4]
                    }
                    _ => unreachable!("expected an sret for this type"),
                }
            } else {
                assert!(func_sig.results().len() == 1);
                vec![basic_value]
            }
        } else {
            assert!(call_site.count_arguments() > 0); // Either sret or vmctx.
            if call_site
                .get_enum_attribute(
                    AttributeLoc::Param(0),
                    Attribute::get_named_enum_kind_id("sret"),
                )
                .is_some()
            {
                let sret = call_site
                    .try_as_basic_value()
                    .right()
                    .unwrap()
                    .get_operand(0)
                    .unwrap()
                    .left()
                    .unwrap()
                    .into_pointer_value();
                let struct_value = builder.build_load(sret, "").into_struct_value();
                let mut rets: Vec<_> = Vec::new();
                for i in 0..struct_value.get_type().count_fields() {
                    let value = builder.build_extract_value(struct_value, i, "").unwrap();
                    rets.push(value);
                }
                assert!(func_sig.results().len() == rets.len());
                rets
            } else {
                assert!(func_sig.results().is_empty());
                vec![]
            }
        }
    }

    fn is_sret(&self, func_sig: &FuncSig) -> Result<bool, CompileError> {
        let func_sig_returns_bitwidths = func_sig
            .results()
            .iter()
            .map(|ty| match ty {
                Type::I32 | Type::F32 => 32,
                Type::I64 | Type::F64 => 64,
                Type::V128 => 128,
                Type::ExternRef | Type::FuncRef => 64, /* pointer */
            })
            .collect::<Vec<i32>>();

        Ok(!matches!(
            func_sig_returns_bitwidths.as_slice(),
            [] | [_]
                | [32, 32]
                | [32, 64]
                | [64, 32]
                | [64, 64]
                | [32, 32, 32]
                | [32, 32, 64]
                | [64, 32, 32]
                | [32, 32, 32, 32]
        ))
    }

    fn pack_values_for_register_return<'ctx>(
        &self,
        intrinsics: &Intrinsics<'ctx>,
        builder: &Builder<'ctx>,
        values: &[BasicValueEnum<'ctx>],
        func_type: &FunctionType<'ctx>,
    ) -> Result<BasicValueEnum<'ctx>, CompileError> {
        let is_32 = |value: BasicValueEnum| {
            (value.is_int_value() && value.into_int_value().get_type() == intrinsics.i32_ty)
                || (value.is_float_value()
                    && value.into_float_value().get_type() == intrinsics.f32_ty)
        };
        let is_64 = |value: BasicValueEnum| {
            (value.is_int_value() && value.into_int_value().get_type() == intrinsics.i64_ty)
                || (value.is_float_value()
                    && value.into_float_value().get_type() == intrinsics.f64_ty)
        };

        let pack_i32s = |low: BasicValueEnum<'ctx>, high: BasicValueEnum<'ctx>| {
            assert!(low.get_type() == intrinsics.i32_ty.as_basic_type_enum());
            assert!(high.get_type() == intrinsics.i32_ty.as_basic_type_enum());
            let (low, high) = (low.into_int_value(), high.into_int_value());
            let low = builder.build_int_z_extend(low, intrinsics.i64_ty, "");
            let high = builder.build_int_z_extend(high, intrinsics.i64_ty, "");
            let high = builder.build_left_shift(high, intrinsics.i64_ty.const_int(32, false), "");
            builder.build_or(low, high, "").as_basic_value_enum()
        };

        let to_i64 = |v: BasicValueEnum<'ctx>| {
            if v.is_float_value() {
                let v = v.into_float_value();
                if v.get_type() == intrinsics.f32_ty {
                    let v = builder
                        .build_bitcast(v, intrinsics.i32_ty, "")
                        .into_int_value();
                    let v = builder.build_int_z_extend(v, intrinsics.i64_ty, "");
                    v.as_basic_value_enum()
                } else {
                    debug_assert!(v.get_type() == intrinsics.f64_ty);
                    let v = builder.build_bitcast(v, intrinsics.i64_ty, "");
                    v.as_basic_value_enum()
                }
            } else {
                let v = v.into_int_value();
                if v.get_type() == intrinsics.i32_ty {
                    let v = builder.build_int_z_extend(v, intrinsics.i64_ty, "");
                    v.as_basic_value_enum()
                } else {
                    debug_assert!(v.get_type() == intrinsics.i64_ty);
                    v.as_basic_value_enum()
                }
            }
        };

        let build_struct = |ty: StructType<'ctx>, values: &[BasicValueEnum<'ctx>]| {
            let mut struct_value = ty.get_undef();
            for (i, v) in values.iter().enumerate() {
                struct_value = builder
                    .build_insert_value(struct_value, *v, i as u32, "")
                    .unwrap()
                    .into_struct_value();
            }
            struct_value.as_basic_value_enum()
        };

        let build_2xi64 = |low: BasicValueEnum<'ctx>, high: BasicValueEnum<'ctx>| {
            let low = to_i64(low);
            let high = to_i64(high);
            let value = intrinsics.i64_ty.array_type(2).get_undef();
            let value = builder.build_insert_value(value, low, 0, "").unwrap();
            let value = builder.build_insert_value(value, high, 1, "").unwrap();
            value.as_basic_value_enum()
        };

        Ok(match *values {
            [one_value] => one_value,
            [v1, v2]
                if v1.is_float_value()
                    && v2.is_float_value()
                    && v1.into_float_value().get_type() == v2.into_float_value().get_type() =>
            {
                build_struct(
                    func_type.get_return_type().unwrap().into_struct_type(),
                    &[v1, v2],
                )
            }
            [v1, v2] if is_32(v1) && is_32(v2) => {
                let v1 = builder.build_bitcast(v1, intrinsics.i32_ty, "");
                let v2 = builder.build_bitcast(v2, intrinsics.i32_ty, "");
                pack_i32s(v1, v2)
            }
            [v1, v2] => build_2xi64(v1, v2),
            [v1, v2, v3]
                if is_32(v1)
                    && is_32(v2)
                    && is_32(v3)
                    && v1.is_float_value()
                    && v2.is_float_value()
                    && v3.is_float_value() =>
            {
                build_struct(
                    func_type.get_return_type().unwrap().into_struct_type(),
                    &[v1, v2, v3],
                )
            }
            [v1, v2, v3] if is_32(v1) && is_32(v2) => {
                let v1 = builder.build_bitcast(v1, intrinsics.i32_ty, "");
                let v2 = builder.build_bitcast(v2, intrinsics.i32_ty, "");
                let v1v2_pack = pack_i32s(v1, v2);
                build_2xi64(v1v2_pack, v3)
            }
            [v1, v2, v3] if is_64(v1) && is_32(v2) && is_32(v3) => {
                let v2 = builder.build_bitcast(v2, intrinsics.i32_ty, "");
                let v3 = builder.build_bitcast(v3, intrinsics.i32_ty, "");
                let v2v3_pack = pack_i32s(v2, v3);
                build_2xi64(v1, v2v3_pack)
            }
            [v1, v2, v3, v4]
                if is_32(v1)
                    && is_32(v2)
                    && is_32(v3)
                    && is_32(v4)
                    && v1.is_float_value()
                    && v2.is_float_value()
                    && v3.is_float_value()
                    && v4.is_float_value() =>
            {
                build_struct(
                    func_type.get_return_type().unwrap().into_struct_type(),
                    &[v1, v2, v3, v4],
                )
            }
            [v1, v2, v3, v4] if is_32(v1) && is_32(v2) && is_32(v3) && is_32(v4) => {
                let v1 = builder.build_bitcast(v1, intrinsics.i32_ty, "");
                let v2 = builder.build_bitcast(v2, intrinsics.i32_ty, "");
                let v1v2_pack = pack_i32s(v1, v2);
                let v3 = builder.build_bitcast(v3, intrinsics.i32_ty, "");
                let v4 = builder.build_bitcast(v4, intrinsics.i32_ty, "");
                let v3v4_pack = pack_i32s(v3, v4);
                build_2xi64(v1v2_pack, v3v4_pack)
            }
            _ => {
                unreachable!("called to perform register return on struct return or void function")
            }
        })
    }
}

'''
'''--- lib/compiler-llvm/src/abi/mod.rs ---
// LLVM implements part of the ABI lowering internally, but also requires that
// the user pack and unpack values themselves sometimes. This can help the LLVM
// optimizer by exposing operations to the optimizer, but it requires that the
// frontend know exactly what IR to produce in order to get the right ABI.

#![deny(dead_code, missing_docs)]

use crate::translator::intrinsics::Intrinsics;
use inkwell::{
    attributes::{Attribute, AttributeLoc},
    builder::Builder,
    context::Context,
    targets::TargetMachine,
    types::FunctionType,
    values::{BasicValueEnum, CallSiteValue, FunctionValue, PointerValue},
};
use wasmer_compiler::CompileError;
use wasmer_types::FunctionType as FuncSig;
use wasmer_vm::VMOffsets;

mod aarch64_systemv;
mod x86_64_systemv;

use aarch64_systemv::Aarch64SystemV;
use x86_64_systemv::X86_64SystemV;

pub fn get_abi(target_machine: &TargetMachine) -> Box<dyn Abi> {
    if target_machine
        .get_triple()
        .as_str()
        .to_string_lossy()
        .starts_with("aarch64")
    {
        Box::new(Aarch64SystemV {})
    } else {
        Box::new(X86_64SystemV {})
    }
}

/// We need to produce different LLVM IR for different platforms. (Contrary to
/// popular knowledge LLVM IR is not intended to be portable in that way.) This
/// trait deals with differences between function signatures on different
/// targets.
pub trait Abi {
    /// Given a function definition, retrieve the parameter that is the vmctx pointer.
    fn get_vmctx_ptr_param<'ctx>(&self, func_value: &FunctionValue<'ctx>) -> PointerValue<'ctx>;

    /// Given a wasm function type, produce an llvm function declaration.
    fn func_type_to_llvm<'ctx>(
        &self,
        context: &'ctx Context,
        intrinsics: &Intrinsics<'ctx>,
        offsets: Option<&VMOffsets>,
        sig: &FuncSig,
    ) -> Result<(FunctionType<'ctx>, Vec<(Attribute, AttributeLoc)>), CompileError>;

    /// Marshall wasm stack values into function parameters.
    fn args_to_call<'ctx>(
        &self,
        alloca_builder: &Builder<'ctx>,
        func_sig: &FuncSig,
        ctx_ptr: PointerValue<'ctx>,
        llvm_fn_ty: &FunctionType<'ctx>,
        values: &[BasicValueEnum<'ctx>],
    ) -> Vec<BasicValueEnum<'ctx>>;

    /// Given a CallSite, extract the returned values and return them in a Vec.
    fn rets_from_call<'ctx>(
        &self,
        builder: &Builder<'ctx>,
        intrinsics: &Intrinsics<'ctx>,
        call_site: CallSiteValue<'ctx>,
        func_sig: &FuncSig,
    ) -> Vec<BasicValueEnum<'ctx>>;

    /// Whether the llvm equivalent of this wasm function has an `sret` attribute.
    fn is_sret(&self, func_sig: &FuncSig) -> Result<bool, CompileError>;

    /// Pack LLVM IR values representing individual wasm values into the return type for the function.
    fn pack_values_for_register_return<'ctx>(
        &self,
        intrinsics: &Intrinsics<'ctx>,
        builder: &Builder<'ctx>,
        values: &[BasicValueEnum<'ctx>],
        func_type: &FunctionType<'ctx>,
    ) -> Result<BasicValueEnum<'ctx>, CompileError>;
}

'''
'''--- lib/compiler-llvm/src/abi/x86_64_systemv.rs ---
use crate::abi::Abi;
use crate::translator::intrinsics::{type_to_llvm, Intrinsics};
use inkwell::{
    attributes::{Attribute, AttributeLoc},
    builder::Builder,
    context::Context,
    types::{AnyType, BasicMetadataTypeEnum, BasicType, FunctionType, StructType},
    values::{
        BasicValue, BasicValueEnum, CallSiteValue, FloatValue, FunctionValue, IntValue,
        PointerValue, VectorValue,
    },
    AddressSpace,
};
use wasmer_compiler::CompileError;
use wasmer_types::{FunctionType as FuncSig, Type};
use wasmer_vm::VMOffsets;

use std::convert::TryInto;

/// Implementation of the [`Abi`] trait for the AMD64 SystemV ABI.
pub struct X86_64SystemV {}

impl Abi for X86_64SystemV {
    // Given a function definition, retrieve the parameter that is the vmctx pointer.
    fn get_vmctx_ptr_param<'ctx>(&self, func_value: &FunctionValue<'ctx>) -> PointerValue<'ctx> {
        func_value
            .get_nth_param(
                if func_value
                    .get_enum_attribute(
                        AttributeLoc::Param(0),
                        Attribute::get_named_enum_kind_id("sret"),
                    )
                    .is_some()
                {
                    1
                } else {
                    0
                },
            )
            .unwrap()
            .into_pointer_value()
    }

    // Given a wasm function type, produce an llvm function declaration.
    fn func_type_to_llvm<'ctx>(
        &self,
        context: &'ctx Context,
        intrinsics: &Intrinsics<'ctx>,
        offsets: Option<&VMOffsets>,
        sig: &FuncSig,
    ) -> Result<(FunctionType<'ctx>, Vec<(Attribute, AttributeLoc)>), CompileError> {
        let user_param_types = sig.params().iter().map(|&ty| type_to_llvm(intrinsics, ty));

        let param_types =
            std::iter::once(Ok(intrinsics.ctx_ptr_ty.as_basic_type_enum())).chain(user_param_types);

        // TODO: figure out how many bytes long vmctx is, and mark it dereferenceable. (no need to mark it nonnull once we do this.)
        let vmctx_attributes = |i: u32| {
            vec![
                (
                    context.create_enum_attribute(Attribute::get_named_enum_kind_id("nofree"), 0),
                    AttributeLoc::Param(i),
                ),
                (
                    if let Some(offsets) = offsets {
                        context.create_enum_attribute(
                            Attribute::get_named_enum_kind_id("dereferenceable"),
                            offsets.size_of_vmctx().into(),
                        )
                    } else {
                        context
                            .create_enum_attribute(Attribute::get_named_enum_kind_id("nonnull"), 0)
                    },
                    AttributeLoc::Param(i),
                ),
                (
                    context.create_enum_attribute(
                        Attribute::get_named_enum_kind_id("align"),
                        std::mem::align_of::<wasmer_vm::VMContext>()
                            .try_into()
                            .unwrap(),
                    ),
                    AttributeLoc::Param(i),
                ),
            ]
        };

        let sig_returns_bitwidths = sig
            .results()
            .iter()
            .map(|ty| match ty {
                Type::I32 | Type::F32 => 32,
                Type::I64 | Type::F64 => 64,
                Type::V128 => 128,
                Type::ExternRef | Type::FuncRef => 64, /* pointer */
            })
            .collect::<Vec<i32>>();

        Ok(match sig_returns_bitwidths.as_slice() {
            [] => (
                intrinsics.void_ty.fn_type(
                    param_types
                        .map(|v| v.map(Into::into))
                        .collect::<Result<Vec<BasicMetadataTypeEnum>, _>>()?
                        .as_slice(),
                    false,
                ),
                vmctx_attributes(0),
            ),
            [_] => {
                let single_value = sig.results()[0];
                (
                    type_to_llvm(intrinsics, single_value)?.fn_type(
                        param_types
                            .map(|v| v.map(Into::into))
                            .collect::<Result<Vec<BasicMetadataTypeEnum>, _>>()?
                            .as_slice(),
                        false,
                    ),
                    vmctx_attributes(0),
                )
            }
            [32, 64] | [64, 32] | [64, 64] => {
                let basic_types: Vec<_> = sig
                    .results()
                    .iter()
                    .map(|&ty| type_to_llvm(intrinsics, ty))
                    .collect::<Result<_, _>>()?;

                (
                    context.struct_type(&basic_types, false).fn_type(
                        param_types
                            .map(|v| v.map(Into::into))
                            .collect::<Result<Vec<BasicMetadataTypeEnum>, _>>()?
                            .as_slice(),
                        false,
                    ),
                    vmctx_attributes(0),
                )
            }
            [32, 32] if sig.results()[0] == Type::F32 && sig.results()[1] == Type::F32 => (
                intrinsics.f32_ty.vec_type(2).fn_type(
                    param_types
                        .map(|v| v.map(Into::into))
                        .collect::<Result<Vec<BasicMetadataTypeEnum>, _>>()?
                        .as_slice(),
                    false,
                ),
                vmctx_attributes(0),
            ),
            [32, 32] => (
                intrinsics.i64_ty.fn_type(
                    param_types
                        .map(|v| v.map(Into::into))
                        .collect::<Result<Vec<BasicMetadataTypeEnum>, _>>()?
                        .as_slice(),
                    false,
                ),
                vmctx_attributes(0),
            ),
            [32, 32, _] if sig.results()[0] == Type::F32 && sig.results()[1] == Type::F32 => (
                context
                    .struct_type(
                        &[
                            intrinsics.f32_ty.vec_type(2).as_basic_type_enum(),
                            type_to_llvm(intrinsics, sig.results()[2])?,
                        ],
                        false,
                    )
                    .fn_type(
                        param_types
                            .map(|v| v.map(Into::into))
                            .collect::<Result<Vec<BasicMetadataTypeEnum>, _>>()?
                            .as_slice(),
                        false,
                    ),
                vmctx_attributes(0),
            ),
            [32, 32, _] => (
                context
                    .struct_type(
                        &[
                            intrinsics.i64_ty.as_basic_type_enum(),
                            type_to_llvm(intrinsics, sig.results()[2])?,
                        ],
                        false,
                    )
                    .fn_type(
                        param_types
                            .map(|v| v.map(Into::into))
                            .collect::<Result<Vec<BasicMetadataTypeEnum>, _>>()?
                            .as_slice(),
                        false,
                    ),
                vmctx_attributes(0),
            ),
            [64, 32, 32] if sig.results()[1] == Type::F32 && sig.results()[2] == Type::F32 => (
                context
                    .struct_type(
                        &[
                            type_to_llvm(intrinsics, sig.results()[0])?,
                            intrinsics.f32_ty.vec_type(2).as_basic_type_enum(),
                        ],
                        false,
                    )
                    .fn_type(
                        param_types
                            .map(|v| v.map(Into::into))
                            .collect::<Result<Vec<BasicMetadataTypeEnum>, _>>()?
                            .as_slice(),
                        false,
                    ),
                vmctx_attributes(0),
            ),
            [64, 32, 32] => (
                context
                    .struct_type(
                        &[
                            type_to_llvm(intrinsics, sig.results()[0])?,
                            intrinsics.i64_ty.as_basic_type_enum(),
                        ],
                        false,
                    )
                    .fn_type(
                        param_types
                            .map(|v| v.map(Into::into))
                            .collect::<Result<Vec<BasicMetadataTypeEnum>, _>>()?
                            .as_slice(),
                        false,
                    ),
                vmctx_attributes(0),
            ),
            [32, 32, 32, 32] => (
                context
                    .struct_type(
                        &[
                            if sig.results()[0] == Type::F32 && sig.results()[1] == Type::F32 {
                                intrinsics.f32_ty.vec_type(2).as_basic_type_enum()
                            } else {
                                intrinsics.i64_ty.as_basic_type_enum()
                            },
                            if sig.results()[2] == Type::F32 && sig.results()[3] == Type::F32 {
                                intrinsics.f32_ty.vec_type(2).as_basic_type_enum()
                            } else {
                                intrinsics.i64_ty.as_basic_type_enum()
                            },
                        ],
                        false,
                    )
                    .fn_type(
                        param_types
                            .map(|v| v.map(Into::into))
                            .collect::<Result<Vec<BasicMetadataTypeEnum>, _>>()?
                            .as_slice(),
                        false,
                    ),
                vmctx_attributes(0),
            ),
            _ => {
                let basic_types: Vec<_> = sig
                    .results()
                    .iter()
                    .map(|&ty| type_to_llvm(intrinsics, ty))
                    .collect::<Result<_, _>>()?;

                let sret = context.struct_type(&basic_types, false);
                let sret_ptr = sret.ptr_type(AddressSpace::Generic);

                let param_types =
                    std::iter::once(Ok(sret_ptr.as_basic_type_enum())).chain(param_types);

                let mut attributes = vec![(
                    context.create_type_attribute(
                        Attribute::get_named_enum_kind_id("sret"),
                        sret.as_any_type_enum(),
                    ),
                    AttributeLoc::Param(0),
                )];
                attributes.append(&mut vmctx_attributes(1));

                (
                    intrinsics.void_ty.fn_type(
                        param_types
                            .map(|v| v.map(Into::into))
                            .collect::<Result<Vec<BasicMetadataTypeEnum>, _>>()?
                            .as_slice(),
                        false,
                    ),
                    attributes,
                )
            }
        })
    }

    // Marshall wasm stack values into function parameters.
    fn args_to_call<'ctx>(
        &self,
        alloca_builder: &Builder<'ctx>,
        func_sig: &FuncSig,
        ctx_ptr: PointerValue<'ctx>,
        llvm_fn_ty: &FunctionType<'ctx>,
        values: &[BasicValueEnum<'ctx>],
    ) -> Vec<BasicValueEnum<'ctx>> {
        // If it's an sret, allocate the return space.
        let sret = if llvm_fn_ty.get_return_type().is_none() && func_sig.results().len() > 1 {
            Some(
                alloca_builder.build_alloca(
                    llvm_fn_ty.get_param_types()[0]
                        .into_pointer_type()
                        .get_element_type()
                        .into_struct_type(),
                    "sret",
                ),
            )
        } else {
            None
        };

        let values = std::iter::once(ctx_ptr.as_basic_value_enum()).chain(values.iter().copied());

        if let Some(sret) = sret {
            std::iter::once(sret.as_basic_value_enum())
                .chain(values)
                .collect()
        } else {
            values.collect()
        }
    }

    // Given a CallSite, extract the returned values and return them in a Vec.
    fn rets_from_call<'ctx>(
        &self,
        builder: &Builder<'ctx>,
        intrinsics: &Intrinsics<'ctx>,
        call_site: CallSiteValue<'ctx>,
        func_sig: &FuncSig,
    ) -> Vec<BasicValueEnum<'ctx>> {
        let split_i64 = |value: IntValue<'ctx>| -> (IntValue<'ctx>, IntValue<'ctx>) {
            assert!(value.get_type() == intrinsics.i64_ty);
            let low = builder.build_int_truncate(value, intrinsics.i32_ty, "");
            let lshr =
                builder.build_right_shift(value, intrinsics.i64_ty.const_int(32, false), false, "");
            let high = builder.build_int_truncate(lshr, intrinsics.i32_ty, "");
            (low, high)
        };

        let f32x2_ty = intrinsics.f32_ty.vec_type(2).as_basic_type_enum();
        let extract_f32x2 = |value: VectorValue<'ctx>| -> (FloatValue<'ctx>, FloatValue<'ctx>) {
            assert!(value.get_type() == f32x2_ty.into_vector_type());
            let ret0 = builder
                .build_extract_element(value, intrinsics.i32_ty.const_int(0, false), "")
                .into_float_value();
            let ret1 = builder
                .build_extract_element(value, intrinsics.i32_ty.const_int(1, false), "")
                .into_float_value();
            (ret0, ret1)
        };

        let casted = |value: BasicValueEnum<'ctx>, ty: Type| -> BasicValueEnum<'ctx> {
            match ty {
                Type::I32 => {
                    assert!(
                        value.get_type() == intrinsics.i32_ty.as_basic_type_enum()
                            || value.get_type() == intrinsics.f32_ty.as_basic_type_enum()
                    );
                    builder.build_bitcast(value, intrinsics.i32_ty, "")
                }
                Type::F32 => {
                    assert!(
                        value.get_type() == intrinsics.i32_ty.as_basic_type_enum()
                            || value.get_type() == intrinsics.f32_ty.as_basic_type_enum()
                    );
                    builder.build_bitcast(value, intrinsics.f32_ty, "")
                }
                _ => panic!("should only be called to repack 32-bit values"),
            }
        };

        if let Some(basic_value) = call_site.try_as_basic_value().left() {
            if func_sig.results().len() > 1 {
                if basic_value.get_type() == intrinsics.i64_ty.as_basic_type_enum() {
                    assert!(func_sig.results().len() == 2);
                    let value = basic_value.into_int_value();
                    let (low, high) = split_i64(value);
                    let low = casted(low.into(), func_sig.results()[0]);
                    let high = casted(high.into(), func_sig.results()[1]);
                    return vec![low, high];
                }
                if basic_value.get_type() == f32x2_ty {
                    assert!(func_sig.results().len() == 2);
                    let (ret0, ret1) = extract_f32x2(basic_value.into_vector_value());
                    return vec![ret0.into(), ret1.into()];
                }
                let struct_value = basic_value.into_struct_value();
                let rets = (0..struct_value.get_type().count_fields())
                    .map(|i| builder.build_extract_value(struct_value, i, "").unwrap())
                    .collect::<Vec<_>>();
                let func_sig_returns_bitwidths = func_sig
                    .results()
                    .iter()
                    .map(|ty| match ty {
                        Type::I32 | Type::F32 => 32,
                        Type::I64 | Type::F64 => 64,
                        Type::V128 => 128,
                        Type::ExternRef | Type::FuncRef => 64, /* pointer */
                    })
                    .collect::<Vec<i32>>();

                match func_sig_returns_bitwidths.as_slice() {
                    [32, 64] | [64, 32] | [64, 64] => {
                        assert!(func_sig.results().len() == 2);
                        vec![rets[0], rets[1]]
                    }
                    [32, 32, _]
                        if rets[0].get_type()
                            == intrinsics.f32_ty.vec_type(2).as_basic_type_enum() =>
                    {
                        assert!(func_sig.results().len() == 3);
                        let (rets0, rets1) = extract_f32x2(rets[0].into_vector_value());
                        vec![rets0.into(), rets1.into(), rets[1]]
                    }
                    [32, 32, _] => {
                        assert!(func_sig.results().len() == 3);
                        let (low, high) = split_i64(rets[0].into_int_value());
                        let low = casted(low.into(), func_sig.results()[0]);
                        let high = casted(high.into(), func_sig.results()[1]);
                        vec![low, high, rets[1]]
                    }
                    [64, 32, 32]
                        if rets[1].get_type()
                            == intrinsics.f32_ty.vec_type(2).as_basic_type_enum() =>
                    {
                        assert!(func_sig.results().len() == 3);
                        let (rets1, rets2) = extract_f32x2(rets[1].into_vector_value());
                        vec![rets[0], rets1.into(), rets2.into()]
                    }
                    [64, 32, 32] => {
                        assert!(func_sig.results().len() == 3);
                        let (rets1, rets2) = split_i64(rets[1].into_int_value());
                        let rets1 = casted(rets1.into(), func_sig.results()[1]);
                        let rets2 = casted(rets2.into(), func_sig.results()[2]);
                        vec![rets[0], rets1, rets2]
                    }
                    [32, 32, 32, 32] => {
                        assert!(func_sig.results().len() == 4);
                        let (low0, high0) = if rets[0].get_type()
                            == intrinsics.f32_ty.vec_type(2).as_basic_type_enum()
                        {
                            let (x, y) = extract_f32x2(rets[0].into_vector_value());
                            (x.into(), y.into())
                        } else {
                            let (x, y) = split_i64(rets[0].into_int_value());
                            (x.into(), y.into())
                        };
                        let (low1, high1) = if rets[1].get_type()
                            == intrinsics.f32_ty.vec_type(2).as_basic_type_enum()
                        {
                            let (x, y) = extract_f32x2(rets[1].into_vector_value());
                            (x.into(), y.into())
                        } else {
                            let (x, y) = split_i64(rets[1].into_int_value());
                            (x.into(), y.into())
                        };
                        let low0 = casted(low0, func_sig.results()[0]);
                        let high0 = casted(high0, func_sig.results()[1]);
                        let low1 = casted(low1, func_sig.results()[2]);
                        let high1 = casted(high1, func_sig.results()[3]);
                        vec![low0, high0, low1, high1]
                    }
                    _ => unreachable!("expected an sret for this type"),
                }
            } else {
                assert!(func_sig.results().len() == 1);
                vec![basic_value]
            }
        } else {
            assert!(call_site.count_arguments() > 0); // Either sret or vmctx.
            if call_site
                .get_enum_attribute(
                    AttributeLoc::Param(0),
                    Attribute::get_named_enum_kind_id("sret"),
                )
                .is_some()
            {
                let sret = call_site
                    .try_as_basic_value()
                    .right()
                    .unwrap()
                    .get_operand(0)
                    .unwrap()
                    .left()
                    .unwrap()
                    .into_pointer_value();
                let struct_value = builder.build_load(sret, "").into_struct_value();
                let mut rets: Vec<_> = Vec::new();
                for i in 0..struct_value.get_type().count_fields() {
                    let value = builder.build_extract_value(struct_value, i, "").unwrap();
                    rets.push(value);
                }
                assert!(func_sig.results().len() == rets.len());
                rets
            } else {
                assert!(func_sig.results().is_empty());
                vec![]
            }
        }
    }

    fn is_sret(&self, func_sig: &FuncSig) -> Result<bool, CompileError> {
        let func_sig_returns_bitwidths = func_sig
            .results()
            .iter()
            .map(|ty| match ty {
                Type::I32 | Type::F32 => 32,
                Type::I64 | Type::F64 => 64,
                Type::V128 => 128,
                Type::ExternRef | Type::FuncRef => 64, /* pointer */
            })
            .collect::<Vec<i32>>();

        Ok(!matches!(
            func_sig_returns_bitwidths.as_slice(),
            [] | [_]
                | [32, 32]
                | [32, 64]
                | [64, 32]
                | [64, 64]
                | [32, 32, 32]
                | [32, 32, 64]
                | [64, 32, 32]
                | [32, 32, 32, 32]
        ))
    }

    fn pack_values_for_register_return<'ctx>(
        &self,
        intrinsics: &Intrinsics<'ctx>,
        builder: &Builder<'ctx>,
        values: &[BasicValueEnum<'ctx>],
        func_type: &FunctionType<'ctx>,
    ) -> Result<BasicValueEnum<'ctx>, CompileError> {
        let is_32 = |value: BasicValueEnum| {
            (value.is_int_value() && value.into_int_value().get_type() == intrinsics.i32_ty)
                || (value.is_float_value()
                    && value.into_float_value().get_type() == intrinsics.f32_ty)
        };
        let is_64 = |value: BasicValueEnum| {
            (value.is_int_value() && value.into_int_value().get_type() == intrinsics.i64_ty)
                || (value.is_float_value()
                    && value.into_float_value().get_type() == intrinsics.f64_ty)
        };
        let is_f32 = |value: BasicValueEnum| {
            value.is_float_value() && value.into_float_value().get_type() == intrinsics.f32_ty
        };

        let pack_i32s = |low: BasicValueEnum<'ctx>, high: BasicValueEnum<'ctx>| {
            assert!(low.get_type() == intrinsics.i32_ty.as_basic_type_enum());
            assert!(high.get_type() == intrinsics.i32_ty.as_basic_type_enum());
            let (low, high) = (low.into_int_value(), high.into_int_value());
            let low = builder.build_int_z_extend(low, intrinsics.i64_ty, "");
            let high = builder.build_int_z_extend(high, intrinsics.i64_ty, "");
            let high = builder.build_left_shift(high, intrinsics.i64_ty.const_int(32, false), "");
            builder.build_or(low, high, "").as_basic_value_enum()
        };

        let pack_f32s = |first: BasicValueEnum<'ctx>,
                         second: BasicValueEnum<'ctx>|
         -> BasicValueEnum<'ctx> {
            assert!(first.get_type() == intrinsics.f32_ty.as_basic_type_enum());
            assert!(second.get_type() == intrinsics.f32_ty.as_basic_type_enum());
            let (first, second) = (first.into_float_value(), second.into_float_value());
            let vec_ty = intrinsics.f32_ty.vec_type(2);
            let vec =
                builder.build_insert_element(vec_ty.get_undef(), first, intrinsics.i32_zero, "");
            builder
                .build_insert_element(vec, second, intrinsics.i32_ty.const_int(1, false), "")
                .as_basic_value_enum()
        };

        let build_struct = |ty: StructType<'ctx>, values: &[BasicValueEnum<'ctx>]| {
            let mut struct_value = ty.get_undef();
            for (i, v) in values.iter().enumerate() {
                struct_value = builder
                    .build_insert_value(struct_value, *v, i as u32, "")
                    .unwrap()
                    .into_struct_value();
            }
            struct_value.as_basic_value_enum()
        };

        Ok(match *values {
            [one_value] => one_value,
            [v1, v2] if is_f32(v1) && is_f32(v2) => pack_f32s(v1, v2),
            [v1, v2] if is_32(v1) && is_32(v2) => {
                let v1 = builder.build_bitcast(v1, intrinsics.i32_ty, "");
                let v2 = builder.build_bitcast(v2, intrinsics.i32_ty, "");
                pack_i32s(v1, v2)
            }
            [v1, v2] => {
                assert!(!(is_32(v1) && is_32(v2)));
                build_struct(
                    func_type.get_return_type().unwrap().into_struct_type(),
                    &[v1, v2],
                )
            }
            [v1, v2, v3] if is_f32(v1) && is_f32(v2) => build_struct(
                func_type.get_return_type().unwrap().into_struct_type(),
                &[pack_f32s(v1, v2), v3],
            ),
            [v1, v2, v3] if is_32(v1) && is_32(v2) => {
                let v1 = builder.build_bitcast(v1, intrinsics.i32_ty, "");
                let v2 = builder.build_bitcast(v2, intrinsics.i32_ty, "");
                build_struct(
                    func_type.get_return_type().unwrap().into_struct_type(),
                    &[pack_i32s(v1, v2), v3],
                )
            }
            [v1, v2, v3] if is_64(v1) && is_f32(v2) && is_f32(v3) => build_struct(
                func_type.get_return_type().unwrap().into_struct_type(),
                &[v1, pack_f32s(v2, v3)],
            ),
            [v1, v2, v3] if is_64(v1) && is_32(v2) && is_32(v3) => {
                let v2 = builder.build_bitcast(v2, intrinsics.i32_ty, "");
                let v3 = builder.build_bitcast(v3, intrinsics.i32_ty, "");
                build_struct(
                    func_type.get_return_type().unwrap().into_struct_type(),
                    &[v1, pack_i32s(v2, v3)],
                )
            }
            [v1, v2, v3, v4] if is_32(v1) && is_32(v2) && is_32(v3) && is_32(v4) => {
                let v1v2_pack = if is_f32(v1) && is_f32(v2) {
                    pack_f32s(v1, v2)
                } else {
                    let v1 = builder.build_bitcast(v1, intrinsics.i32_ty, "");
                    let v2 = builder.build_bitcast(v2, intrinsics.i32_ty, "");
                    pack_i32s(v1, v2)
                };
                let v3v4_pack = if is_f32(v3) && is_f32(v4) {
                    pack_f32s(v3, v4)
                } else {
                    let v3 = builder.build_bitcast(v3, intrinsics.i32_ty, "");
                    let v4 = builder.build_bitcast(v4, intrinsics.i32_ty, "");
                    pack_i32s(v3, v4)
                };
                build_struct(
                    func_type.get_return_type().unwrap().into_struct_type(),
                    &[v1v2_pack, v3v4_pack],
                )
            }
            _ => {
                unreachable!("called to perform register return on struct return or void function")
            }
        })
    }
}

'''
'''--- lib/compiler-llvm/src/compiler.rs ---
use crate::config::LLVM;
use crate::trampoline::FuncTrampoline;
use crate::translator::FuncTranslator;
use crate::CompiledKind;
use inkwell::context::Context;
use inkwell::memory_buffer::MemoryBuffer;
use inkwell::module::{Linkage, Module};
use inkwell::targets::FileType;
use inkwell::DLLStorageClass;
use rayon::iter::ParallelBridge;
use rayon::prelude::{IntoParallelIterator, IntoParallelRefIterator, ParallelIterator};
use wasmer_compiler::{
    Architecture, Compilation, CompileError, CompileModuleInfo, Compiler, CustomSection,
    CustomSectionProtection, Dwarf, FunctionBodyData, ModuleTranslationState, RelocationTarget,
    SectionBody, SectionIndex, Symbol, SymbolRegistry, Target, TrampolinesSection,
};
use wasmer_types::entity::{EntityRef, PrimaryMap};
use wasmer_types::{FunctionIndex, LocalFunctionIndex, SignatureIndex};

//use std::sync::Mutex;

/// A compiler that compiles a WebAssembly module with LLVM, translating the Wasm to LLVM IR,
/// optimizing it and then translating to assembly.
pub struct LLVMCompiler {
    config: LLVM,
}

impl LLVMCompiler {
    /// Creates a new LLVM compiler
    pub fn new(config: LLVM) -> LLVMCompiler {
        LLVMCompiler { config }
    }

    /// Gets the config for this Compiler
    fn config(&self) -> &LLVM {
        &self.config
    }
}

struct ShortNames {}

impl SymbolRegistry for ShortNames {
    fn symbol_to_name(&self, symbol: Symbol) -> String {
        match symbol {
            Symbol::LocalFunction(index) => format!("f{}", index.index()),
            Symbol::Section(index) => format!("s{}", index.index()),
            Symbol::FunctionCallTrampoline(index) => format!("t{}", index.index()),
            Symbol::DynamicFunctionTrampoline(index) => format!("d{}", index.index()),
        }
    }

    fn name_to_symbol(&self, name: &str) -> Option<Symbol> {
        if name.len() < 2 {
            return None;
        }
        let (ty, idx) = name.split_at(1);
        let idx = idx.parse::<u32>().ok()?;
        match ty.chars().next().unwrap() {
            'f' => Some(Symbol::LocalFunction(LocalFunctionIndex::from_u32(idx))),
            's' => Some(Symbol::Section(SectionIndex::from_u32(idx))),
            't' => Some(Symbol::FunctionCallTrampoline(SignatureIndex::from_u32(
                idx,
            ))),
            'd' => Some(Symbol::DynamicFunctionTrampoline(FunctionIndex::from_u32(
                idx,
            ))),
            _ => None,
        }
    }
}

impl LLVMCompiler {
    fn compile_native_object<'data, 'module>(
        &self,
        target: &Target,
        compile_info: &'module CompileModuleInfo,
        module_translation: &ModuleTranslationState,
        function_body_inputs: &PrimaryMap<LocalFunctionIndex, FunctionBodyData<'data>>,
        symbol_registry: &dyn SymbolRegistry,
        wasmer_metadata: &[u8],
    ) -> Result<Vec<u8>, CompileError> {
        let target_machine = self.config().target_machine(target);
        let ctx = Context::create();

        // TODO: https:/github.com/rayon-rs/rayon/issues/822

        let merged_bitcode = function_body_inputs.into_iter().par_bridge().map_init(
            || {
                let target_machine = self.config().target_machine(target);
                FuncTranslator::new(target_machine)
            },
            |func_translator, (i, input)| {
                let module = func_translator.translate_to_module(
                    &compile_info.module,
                    module_translation,
                    &i,
                    input,
                    self.config(),
                    &compile_info.memory_styles,
                    &compile_info.table_styles,
                    symbol_registry,
                )?;
                Ok(module.write_bitcode_to_memory().as_slice().to_vec())
            },
        );

        let trampolines_bitcode = compile_info.module.signatures.iter().par_bridge().map_init(
            || {
                let target_machine = self.config().target_machine(target);
                FuncTrampoline::new(target_machine)
            },
            |func_trampoline, (i, sig)| {
                let name = symbol_registry.symbol_to_name(Symbol::FunctionCallTrampoline(i));
                let module = func_trampoline.trampoline_to_module(sig, self.config(), &name)?;
                Ok(module.write_bitcode_to_memory().as_slice().to_vec())
            },
        );

        let dynamic_trampolines_bitcode =
            compile_info.module.functions.iter().par_bridge().map_init(
                || {
                    let target_machine = self.config().target_machine(target);
                    (
                        FuncTrampoline::new(target_machine),
                        &compile_info.module.signatures,
                    )
                },
                |(func_trampoline, signatures), (i, sig)| {
                    let sig = &signatures[*sig];
                    let name = symbol_registry.symbol_to_name(Symbol::DynamicFunctionTrampoline(i));
                    let module =
                        func_trampoline.dynamic_trampoline_to_module(sig, self.config(), &name)?;
                    Ok(module.write_bitcode_to_memory().as_slice().to_vec())
                },
            );

        let merged_bitcode = merged_bitcode
            .chain(trampolines_bitcode)
            .chain(dynamic_trampolines_bitcode)
            .collect::<Result<Vec<_>, CompileError>>()?
            .into_par_iter()
            .reduce_with(|bc1, bc2| {
                let ctx = Context::create();
                let membuf = MemoryBuffer::create_from_memory_range(&bc1, "");
                let m1 = Module::parse_bitcode_from_buffer(&membuf, &ctx).unwrap();
                let membuf = MemoryBuffer::create_from_memory_range(&bc2, "");
                let m2 = Module::parse_bitcode_from_buffer(&membuf, &ctx).unwrap();
                m1.link_in_module(m2).unwrap();
                m1.write_bitcode_to_memory().as_slice().to_vec()
            });
        let merged_module = if let Some(bc) = merged_bitcode {
            let membuf = MemoryBuffer::create_from_memory_range(&bc, "");
            Module::parse_bitcode_from_buffer(&membuf, &ctx).unwrap()
        } else {
            ctx.create_module("")
        };

        let i8_ty = ctx.i8_type();
        let metadata_init = i8_ty.const_array(
            wasmer_metadata
                .iter()
                .map(|v| i8_ty.const_int(*v as u64, false))
                .collect::<Vec<_>>()
                .as_slice(),
        );
        let metadata_gv =
            merged_module.add_global(metadata_init.get_type(), None, "WASMER_METADATA");
        metadata_gv.set_initializer(&metadata_init);
        metadata_gv.set_linkage(Linkage::DLLExport);
        metadata_gv.set_dll_storage_class(DLLStorageClass::Export);

        if self.config().enable_verifier {
            merged_module.verify().unwrap();
        }

        let memory_buffer = target_machine
            .write_to_memory_buffer(&merged_module, FileType::Object)
            .unwrap();
        if let Some(ref callbacks) = self.config.callbacks {
            callbacks.obj_memory_buffer(&CompiledKind::Module, &memory_buffer);
        }

        Ok(memory_buffer.as_slice().to_vec())
    }
}

impl Compiler for LLVMCompiler {
    fn experimental_native_compile_module<'data, 'module>(
        &self,
        target: &Target,
        compile_info: &'module CompileModuleInfo,
        module_translation: &ModuleTranslationState,
        // The list of function bodies
        function_body_inputs: &PrimaryMap<LocalFunctionIndex, FunctionBodyData<'data>>,
        symbol_registry: &dyn SymbolRegistry,
        // The metadata to inject into the wasmer_metadata section of the object file.
        wasmer_metadata: &[u8],
    ) -> Option<Result<Vec<u8>, CompileError>> {
        Some(self.compile_native_object(
            target,
            compile_info,
            module_translation,
            function_body_inputs,
            symbol_registry,
            wasmer_metadata,
        ))
    }

    /// Compile the module using LLVM, producing a compilation result with
    /// associated relocations.
    fn compile_module<'data, 'module>(
        &self,
        target: &Target,
        compile_info: &'module CompileModuleInfo,
        module_translation: &ModuleTranslationState,
        function_body_inputs: PrimaryMap<LocalFunctionIndex, FunctionBodyData<'data>>,
    ) -> Result<Compilation, CompileError> {
        //let data = Arc::new(Mutex::new(0));
        let memory_styles = &compile_info.memory_styles;
        let table_styles = &compile_info.table_styles;

        let module = &compile_info.module;

        // TODO: merge constants in sections.

        let mut module_custom_sections = PrimaryMap::new();
        let mut frame_section_bytes = vec![];
        let mut frame_section_relocations = vec![];
        let functions = function_body_inputs
            .iter()
            .collect::<Vec<(LocalFunctionIndex, &FunctionBodyData<'_>)>>()
            .par_iter()
            .map_init(
                || {
                    let target_machine = self.config().target_machine(target);
                    FuncTranslator::new(target_machine)
                },
                |func_translator, (i, input)| {
                    // TODO: remove (to serialize)
                    //let _data = data.lock().unwrap();
                    func_translator.translate(
                        module,
                        module_translation,
                        i,
                        input,
                        self.config(),
                        memory_styles,
                        &table_styles,
                        &ShortNames {},
                    )
                },
            )
            .collect::<Result<Vec<_>, CompileError>>()?
            .into_iter()
            .map(|mut compiled_function| {
                let first_section = module_custom_sections.len() as u32;
                for (section_index, custom_section) in compiled_function.custom_sections.iter() {
                    // TODO: remove this call to clone()
                    let mut custom_section = custom_section.clone();
                    for mut reloc in &mut custom_section.relocations {
                        if let RelocationTarget::CustomSection(index) = reloc.reloc_target {
                            reloc.reloc_target = RelocationTarget::CustomSection(
                                SectionIndex::from_u32(first_section + index.as_u32()),
                            )
                        }
                    }
                    if compiled_function
                        .eh_frame_section_indices
                        .contains(&section_index)
                    {
                        let offset = frame_section_bytes.len() as u32;
                        for mut reloc in &mut custom_section.relocations {
                            reloc.offset += offset;
                        }
                        frame_section_bytes.extend_from_slice(custom_section.bytes.as_slice());
                        frame_section_relocations.extend(custom_section.relocations);
                        // TODO: we do this to keep the count right, remove it.
                        module_custom_sections.push(CustomSection {
                            protection: CustomSectionProtection::Read,
                            bytes: SectionBody::new_with_vec(vec![]),
                            relocations: vec![],
                        });
                    } else {
                        module_custom_sections.push(custom_section);
                    }
                }
                for mut reloc in &mut compiled_function.compiled_function.relocations {
                    if let RelocationTarget::CustomSection(index) = reloc.reloc_target {
                        reloc.reloc_target = RelocationTarget::CustomSection(
                            SectionIndex::from_u32(first_section + index.as_u32()),
                        )
                    }
                }
                compiled_function.compiled_function
            })
            .collect::<PrimaryMap<LocalFunctionIndex, _>>();

        let trampolines = match target.triple().architecture {
            Architecture::Aarch64(_) => {
                let nj = 16;
                // We create a jump to an absolute 64bits address
                // using x17 as a scratch register, SystemV declare both x16 and x17 as Intra-Procedural  scratch register
                // but Apple ask to just not use x16
                // LDR x17, #8      51 00 00 58
                // BR x17           20 02 1f d6
                // JMPADDR          00 00 00 00 00 00 00 00
                let onejump = [
                    0x51, 0x00, 0x00, 0x58, 0x20, 0x02, 0x1f, 0xd6, 0, 0, 0, 0, 0, 0, 0, 0,
                ];
                let trampolines = Some(TrampolinesSection::new(
                    SectionIndex::from_u32(module_custom_sections.len() as u32),
                    nj,
                    onejump.len(),
                ));
                let mut alljmps = vec![];
                for _ in 0..nj {
                    alljmps.extend(onejump.iter().copied());
                }
                module_custom_sections.push(CustomSection {
                    protection: CustomSectionProtection::ReadExecute,
                    bytes: SectionBody::new_with_vec(alljmps),
                    relocations: vec![],
                });
                trampolines
            }
            _ => None,
        };

        let dwarf = if !frame_section_bytes.is_empty() {
            let dwarf = Some(Dwarf::new(SectionIndex::from_u32(
                module_custom_sections.len() as u32,
            )));
            // Terminating zero-length CIE.
            frame_section_bytes.extend(vec![
                0x00, 0x00, 0x00, 0x00, // Length
                0x00, 0x00, 0x00, 0x00, // CIE ID
                0x10, // Version (must be 1)
                0x00, // Augmentation data
                0x00, // Code alignment factor
                0x00, // Data alignment factor
                0x00, // Return address register
                0x00, 0x00, 0x00, // Padding to a multiple of 4 bytes
            ]);
            module_custom_sections.push(CustomSection {
                protection: CustomSectionProtection::Read,
                bytes: SectionBody::new_with_vec(frame_section_bytes),
                relocations: frame_section_relocations,
            });
            dwarf
        } else {
            None
        };

        let function_call_trampolines = module
            .signatures
            .values()
            .collect::<Vec<_>>()
            .par_iter()
            .map_init(
                || {
                    let target_machine = self.config().target_machine(target);
                    FuncTrampoline::new(target_machine)
                },
                |func_trampoline, sig| func_trampoline.trampoline(sig, self.config(), ""),
            )
            .collect::<Vec<_>>()
            .into_iter()
            .collect::<Result<PrimaryMap<_, _>, CompileError>>()?;

        let dynamic_function_trampolines = module
            .imported_function_types()
            .collect::<Vec<_>>()
            .par_iter()
            .map_init(
                || {
                    let target_machine = self.config().target_machine(target);
                    FuncTrampoline::new(target_machine)
                },
                |func_trampoline, func_type| {
                    func_trampoline.dynamic_trampoline(&func_type, self.config(), "")
                },
            )
            .collect::<Result<Vec<_>, CompileError>>()?
            .into_iter()
            .collect::<PrimaryMap<_, _>>();

        Ok(Compilation::new(
            functions,
            module_custom_sections,
            function_call_trampolines,
            dynamic_function_trampolines,
            dwarf,
            trampolines,
        ))
    }
}

'''
'''--- lib/compiler-llvm/src/config.rs ---
use crate::compiler::LLVMCompiler;
use inkwell::targets::{
    CodeModel, InitializationConfig, RelocMode, Target as InkwellTarget, TargetMachine,
    TargetTriple,
};
pub use inkwell::OptimizationLevel as LLVMOptLevel;
use itertools::Itertools;
use std::fmt::Debug;
use std::sync::Arc;
use target_lexicon::Architecture;
use wasmer_compiler::{Compiler, CompilerConfig, Target, Triple};
use wasmer_types::{FunctionType, LocalFunctionIndex};

/// The InkWell ModuleInfo type
pub type InkwellModule<'ctx> = inkwell::module::Module<'ctx>;

/// The InkWell MemoryBuffer type
pub type InkwellMemoryBuffer = inkwell::memory_buffer::MemoryBuffer;

/// The compiled function kind, used for debugging in the `LLVMCallbacks`.
#[derive(Debug, Clone)]
pub enum CompiledKind {
    // A locally-defined function in the Wasm file.
    Local(LocalFunctionIndex),
    // A function call trampoline for a given signature.
    FunctionCallTrampoline(FunctionType),
    // A dynamic function trampoline for a given signature.
    DynamicFunctionTrampoline(FunctionType),
    // An entire Wasm module.
    Module,
}

/// Callbacks to the different LLVM compilation phases.
pub trait LLVMCallbacks: Debug + Send + Sync {
    fn preopt_ir(&self, function: &CompiledKind, module: &InkwellModule);
    fn postopt_ir(&self, function: &CompiledKind, module: &InkwellModule);
    fn obj_memory_buffer(&self, function: &CompiledKind, memory_buffer: &InkwellMemoryBuffer);
}

#[derive(Debug, Clone)]
pub struct LLVM {
    pub(crate) enable_nan_canonicalization: bool,
    pub(crate) enable_verifier: bool,
    pub(crate) opt_level: LLVMOptLevel,
    is_pic: bool,
    pub(crate) callbacks: Option<Arc<dyn LLVMCallbacks>>,
}

impl LLVM {
    /// Creates a new configuration object with the default configuration
    /// specified.
    pub fn new() -> Self {
        Self {
            enable_nan_canonicalization: false,
            enable_verifier: false,
            opt_level: LLVMOptLevel::Aggressive,
            is_pic: false,
            callbacks: None,
        }
    }

    /// The optimization levels when optimizing the IR.
    pub fn opt_level(&mut self, opt_level: LLVMOptLevel) -> &mut Self {
        self.opt_level = opt_level;
        self
    }

    /// Callbacks that will triggered in the different compilation
    /// phases in LLVM.
    pub fn callbacks(&mut self, callbacks: Option<Arc<dyn LLVMCallbacks>>) -> &mut Self {
        self.callbacks = callbacks;
        self
    }

    fn reloc_mode(&self) -> RelocMode {
        if self.is_pic {
            RelocMode::PIC
        } else {
            RelocMode::Static
        }
    }

    fn code_model(&self) -> CodeModel {
        // We normally use the large code model, but when targeting shared
        // objects, we are required to use PIC. If we use PIC anyways, we lose
        // any benefit from large code model and there's some cost on all
        // platforms, plus some platforms (MachO) don't support PIC + large
        // at all.
        if self.is_pic {
            CodeModel::Small
        } else {
            CodeModel::Large
        }
    }

    fn target_triple(&self, target: &Target) -> TargetTriple {
        // Hack: we're using is_pic to determine whether this is a native
        // build or not.
        let operating_system = if target.triple().operating_system
            == wasmer_compiler::OperatingSystem::Darwin
            && !self.is_pic
        {
            // LLVM detects static relocation + darwin + 64-bit and
            // force-enables PIC because MachO doesn't support that
            // combination. They don't check whether they're targeting
            // MachO, they check whether the OS is set to Darwin.
            //
            // Since both linux and darwin use SysV ABI, this should work.
            //  but not in the case of Aarch64, there the ABI is slightly different
            match target.triple().architecture {
                _ => wasmer_compiler::OperatingSystem::Linux,
            }
        } else {
            target.triple().operating_system
        };
        let binary_format = if self.is_pic {
            target.triple().binary_format
        } else {
            target_lexicon::BinaryFormat::Elf
        };
        let triple = Triple {
            architecture: target.triple().architecture,
            vendor: target.triple().vendor.clone(),
            operating_system,
            environment: target.triple().environment,
            binary_format,
        };
        TargetTriple::create(&triple.to_string())
    }

    /// Generates the target machine for the current target
    pub fn target_machine(&self, target: &Target) -> TargetMachine {
        let triple = target.triple();
        let cpu_features = &target.cpu_features();

        match triple.architecture {
            Architecture::X86_64 | Architecture::X86_32(_) => {
                InkwellTarget::initialize_x86(&InitializationConfig {
                    asm_parser: true,
                    asm_printer: true,
                    base: true,
                    disassembler: true,
                    info: true,
                    machine_code: true,
                })
            }
            Architecture::Aarch64(_) => InkwellTarget::initialize_aarch64(&InitializationConfig {
                asm_parser: true,
                asm_printer: true,
                base: true,
                disassembler: true,
                info: true,
                machine_code: true,
            }),
            // Architecture::Arm(_) => InkwellTarget::initialize_arm(&InitializationConfig {
            //     asm_parser: true,
            //     asm_printer: true,
            //     base: true,
            //     disassembler: true,
            //     info: true,
            //     machine_code: true,
            // }),
            _ => unimplemented!("target {} not yet supported in Wasmer", triple),
        }

        // The CPU features formatted as LLVM strings
        // We can safely map to gcc-like features as the CPUFeatures
        // are compliant with the same string representations as gcc.
        let llvm_cpu_features = cpu_features
            .iter()
            .map(|feature| format!("+{}", feature.to_string()))
            .join(",");

        let target_triple = self.target_triple(&target);
        let llvm_target = InkwellTarget::from_triple(&target_triple).unwrap();
        llvm_target
            .create_target_machine(
                &target_triple,
                "generic",
                &llvm_cpu_features,
                self.opt_level,
                self.reloc_mode(),
                self.code_model(),
            )
            .unwrap()
    }
}

impl CompilerConfig for LLVM {
    /// Emit code suitable for dlopen.
    fn enable_pic(&mut self) {
        // TODO: although we can emit PIC, the object file parser does not yet
        // support all the relocations.
        self.is_pic = true;
    }

    /// Whether to verify compiler IR.
    fn enable_verifier(&mut self) {
        self.enable_verifier = true;
    }

    fn enable_nan_canonicalization(&mut self) {
        self.enable_nan_canonicalization = true;
    }

    fn canonicalize_nans(&mut self, enable: bool) {
        self.enable_nan_canonicalization = enable;
    }

    /// Transform it into the compiler.
    fn compiler(self: Box<Self>) -> Box<dyn Compiler> {
        Box::new(LLVMCompiler::new(*self))
    }
}

impl Default for LLVM {
    fn default() -> LLVM {
        Self::new()
    }
}

'''
'''--- lib/compiler-llvm/src/lib.rs ---
#![deny(
    nonstandard_style,
    unused_imports,
    unused_mut,
    unused_variables,
    unused_unsafe,
    unreachable_patterns
)]
#![cfg_attr(
    all(not(target_os = "windows"), not(target_arch = "aarch64")),
    deny(dead_code)
)]
#![doc(html_favicon_url = "https://wasmer.io/images/icons/favicon-32x32.png")]
#![doc(html_logo_url = "https://github.com/wasmerio.png?size=200")]

mod abi;
mod compiler;
mod config;
mod object_file;
mod trampoline;
mod translator;

pub use crate::compiler::LLVMCompiler;
pub use crate::config::{
    CompiledKind, InkwellMemoryBuffer, InkwellModule, LLVMCallbacks, LLVMOptLevel, LLVM,
};

'''
'''--- lib/compiler-llvm/src/object_file.rs ---
use object::{Object, ObjectSection, ObjectSymbol};

use std::collections::{HashMap, HashSet};
use std::convert::TryInto;
use std::num::TryFromIntError;

use wasmer_compiler::{
    CompileError, CompiledFunctionFrameInfo, CustomSection, CustomSectionProtection,
    CustomSections, FunctionAddressMap, FunctionBody, InstructionAddressMap, Relocation,
    RelocationKind, RelocationTarget, SectionBody, SectionIndex, SourceLoc,
};
use wasmer_types::entity::{PrimaryMap, SecondaryMap};
use wasmer_vm::libcalls::LibCall;

fn map_tryfromint_err(error: TryFromIntError) -> CompileError {
    CompileError::Codegen(format!("int doesn't fit: {}", error))
}

fn map_object_err(error: object::read::Error) -> CompileError {
    CompileError::Codegen(format!("error parsing object file: {}", error))
}

pub struct CompiledFunction {
    pub compiled_function: wasmer_compiler::CompiledFunction,
    pub custom_sections: CustomSections,
    pub eh_frame_section_indices: Vec<SectionIndex>,
}

pub fn load_object_file<F>(
    contents: &[u8],
    root_section: &str,
    root_section_reloc_target: RelocationTarget,
    mut symbol_name_to_relocation_target: F,
) -> Result<CompiledFunction, CompileError>
where
    F: FnMut(&str) -> Result<Option<RelocationTarget>, CompileError>,
{
    // TODO: use perfect hash function?
    let mut libcalls = HashMap::new();
    libcalls.insert("ceilf".to_string(), LibCall::CeilF32);
    libcalls.insert("ceil".to_string(), LibCall::CeilF64);
    libcalls.insert("floorf".to_string(), LibCall::FloorF32);
    libcalls.insert("floor".to_string(), LibCall::FloorF64);
    libcalls.insert("nearbyintf".to_string(), LibCall::NearestF32);
    libcalls.insert("nearbyint".to_string(), LibCall::NearestF64);
    libcalls.insert("truncf".to_string(), LibCall::TruncF32);
    libcalls.insert("trunc".to_string(), LibCall::TruncF64);
    libcalls.insert("wasmer_vm_f32_ceil".to_string(), LibCall::CeilF32);
    libcalls.insert("wasmer_vm_f64_ceil".to_string(), LibCall::CeilF64);
    libcalls.insert("wasmer_vm_f32_floor".to_string(), LibCall::FloorF32);
    libcalls.insert("wasmer_vm_f64_floor".to_string(), LibCall::FloorF64);
    libcalls.insert("wasmer_vm_f32_nearest".to_string(), LibCall::NearestF32);
    libcalls.insert("wasmer_vm_f64_nearest".to_string(), LibCall::NearestF64);
    libcalls.insert("wasmer_vm_f32_trunc".to_string(), LibCall::TruncF32);
    libcalls.insert("wasmer_vm_f64_trunc".to_string(), LibCall::TruncF64);
    libcalls.insert("wasmer_vm_memory32_size".to_string(), LibCall::Memory32Size);
    libcalls.insert(
        "wasmer_vm_imported_memory32_size".to_string(),
        LibCall::ImportedMemory32Size,
    );
    libcalls.insert("wasmer_vm_table_copy".to_string(), LibCall::TableCopy);
    libcalls.insert("wasmer_vm_table_init".to_string(), LibCall::TableInit);
    libcalls.insert("wasmer_vm_table_fill".to_string(), LibCall::TableFill);
    libcalls.insert("wasmer_vm_table_size".to_string(), LibCall::TableSize);
    libcalls.insert(
        "wasmer_vm_imported_table_size".to_string(),
        LibCall::ImportedTableSize,
    );
    libcalls.insert("wasmer_vm_table_get".to_string(), LibCall::TableGet);
    libcalls.insert(
        "wasmer_vm_imported_table_get".to_string(),
        LibCall::ImportedTableGet,
    );
    libcalls.insert("wasmer_vm_table_set".to_string(), LibCall::TableSet);
    libcalls.insert(
        "wasmer_vm_imported_table_set".to_string(),
        LibCall::ImportedTableSet,
    );
    libcalls.insert("wasmer_vm_table_grow".to_string(), LibCall::TableGrow);
    libcalls.insert(
        "wasmer_vm_imported_table_grow".to_string(),
        LibCall::ImportedTableGrow,
    );
    libcalls.insert("wasmer_vm_func_ref".to_string(), LibCall::FuncRef);
    libcalls.insert("wasmer_vm_elem_drop".to_string(), LibCall::ElemDrop);
    libcalls.insert("wasmer_vm_memory32_copy".to_string(), LibCall::Memory32Copy);
    libcalls.insert(
        "wasmer_vm_imported_memory32_copy".to_string(),
        LibCall::ImportedMemory32Copy,
    );
    libcalls.insert("wasmer_vm_memory32_fill".to_string(), LibCall::Memory32Fill);
    libcalls.insert(
        "wasmer_vm_imported_memory32_fill".to_string(),
        LibCall::ImportedMemory32Fill,
    );
    libcalls.insert("wasmer_vm_memory32_init".to_string(), LibCall::Memory32Init);
    libcalls.insert("wasmer_vm_data_drop".to_string(), LibCall::DataDrop);
    libcalls.insert("wasmer_vm_raise_trap".to_string(), LibCall::RaiseTrap);
    libcalls.insert("wasmer_vm_probestack".to_string(), LibCall::Probestack);

    let elf = object::File::parse(contents).map_err(map_object_err)?;

    let mut visited: HashSet<object::read::SectionIndex> = HashSet::new();
    let mut worklist: Vec<object::read::SectionIndex> = Vec::new();
    let mut section_targets: HashMap<object::read::SectionIndex, RelocationTarget> = HashMap::new();

    let root_section_index = elf
        .section_by_name(root_section)
        .ok_or_else(|| CompileError::Codegen(format!("no section named {}", root_section)))?
        .index();

    let mut section_to_custom_section = HashMap::new();

    section_targets.insert(root_section_index, root_section_reloc_target);

    let mut next_custom_section: u32 = 0;
    let mut elf_section_to_target = |elf_section_index: object::read::SectionIndex| {
        *section_targets.entry(elf_section_index).or_insert_with(|| {
            let next = SectionIndex::from_u32(next_custom_section);
            section_to_custom_section.insert(elf_section_index, next);
            let target = RelocationTarget::CustomSection(next);
            next_custom_section += 1;
            target
        })
    };

    // From elf section index to list of Relocations. Although we use a Vec,
    // the order of relocations is not important.
    let mut relocations: HashMap<object::read::SectionIndex, Vec<Relocation>> = HashMap::new();

    // Each iteration of this loop pulls a section and the relocations
    // relocations that apply to it. We begin with the ".root_section"
    // section, and then parse all relocation sections that apply to that
    // section. Those relocations may refer to additional sections which we
    // then add to the worklist until we've visited the closure of
    // everything needed to run the code in ".root_section".
    //
    // `worklist` is the list of sections we have yet to visit. It never
    // contains any duplicates or sections we've already visited. `visited`
    // contains all the sections we've ever added to the worklist in a set
    // so that we can quickly check whether a section is new before adding
    // it to worklist. `section_to_custom_section` is filled in with all
    // the sections we want to include.
    worklist.push(root_section_index);
    visited.insert(root_section_index);

    // Also add any .eh_frame sections.
    let mut eh_frame_section_indices = vec![];
    for section in elf.sections() {
        if section.kind() == object::SectionKind::Elf(object::elf::SHT_X86_64_UNWIND) {
            let index = section.index();
            worklist.push(index);
            visited.insert(index);
            eh_frame_section_indices.push(index);
            // This allocates a custom section index for the ELF section.
            elf_section_to_target(index);
        }
    }

    while let Some(section_index) = worklist.pop() {
        for (offset, reloc) in elf
            .section_by_index(section_index)
            .map_err(map_object_err)?
            .relocations()
        {
            let kind = match (reloc.kind(), reloc.size()) {
                (object::RelocationKind::Absolute, 64) => RelocationKind::Abs8,
                (object::RelocationKind::Elf(object::elf::R_X86_64_PC64), 0) => {
                    RelocationKind::X86PCRel8
                }
                (object::RelocationKind::Elf(object::elf::R_AARCH64_MOVW_UABS_G0_NC), 0) => {
                    RelocationKind::Arm64Movw0
                }
                (object::RelocationKind::Elf(object::elf::R_AARCH64_MOVW_UABS_G1_NC), 0) => {
                    RelocationKind::Arm64Movw1
                }
                (object::RelocationKind::Elf(object::elf::R_AARCH64_MOVW_UABS_G2_NC), 0) => {
                    RelocationKind::Arm64Movw2
                }
                (object::RelocationKind::Elf(object::elf::R_AARCH64_MOVW_UABS_G3), 0) => {
                    RelocationKind::Arm64Movw3
                }
                (object::RelocationKind::PltRelative, 26) => RelocationKind::Arm64Call,
                _ => {
                    return Err(CompileError::Codegen(format!(
                        "unknown relocation {:?}",
                        reloc
                    )));
                }
            };
            let addend = reloc.addend();
            let target = match reloc.target() {
                object::read::RelocationTarget::Symbol(index) => {
                    let symbol = elf.symbol_by_index(index).map_err(map_object_err)?;
                    let symbol_name = symbol.name().map_err(map_object_err)?;
                    if symbol.kind() == object::SymbolKind::Section {
                        match symbol.section() {
                            object::SymbolSection::Section(section_index) => {
                                if section_index == root_section_index {
                                    root_section_reloc_target
                                } else {
                                    if visited.insert(section_index) {
                                        worklist.push(section_index);
                                    }
                                    elf_section_to_target(section_index)
                                }
                            }
                            _ => {
                                return Err(CompileError::Codegen(format!(
                                    "relocation targets unknown section {:?}",
                                    reloc
                                )));
                            }
                        }
                        // Maybe a libcall then?
                    } else if let Some(libcall) = libcalls.get(symbol_name) {
                        RelocationTarget::LibCall(*libcall)
                    } else if let Some(reloc_target) =
                        symbol_name_to_relocation_target(symbol_name)?
                    {
                        reloc_target
                    } else {
                        return Err(CompileError::Codegen(format!(
                            "relocation targets unknown symbol {:?}",
                            reloc
                        )));
                    }
                }

                object::read::RelocationTarget::Section(index) => {
                    if index == root_section_index {
                        root_section_reloc_target
                    } else {
                        if visited.insert(index) {
                            worklist.push(index);
                        }
                        elf_section_to_target(index)
                    }
                }

                object::read::RelocationTarget::Absolute => {
                    // Wasm-produced object files should never have absolute
                    // addresses in them because none of the parts of the Wasm
                    // VM, nor the generated code are loaded at fixed addresses.
                    return Err(CompileError::Codegen(format!(
                        "relocation targets absolute address {:?}",
                        reloc
                    )));
                }

                // `object::read::RelocationTarget` is a
                // non-exhaustive enum (`#[non_exhaustive]`), so it
                // could have additional variants added in the
                // future. Therefore, when matching against variants
                // of non-exhaustive enums, an extra wildcard arm must
                // be added to account for any future variants.
                t => {
                    return Err(CompileError::Codegen(format!(
                        "relocation target is unknown `{:?}`",
                        t
                    )));
                }
            };
            relocations
                .entry(section_index)
                .or_default()
                .push(Relocation {
                    kind,
                    reloc_target: target,
                    offset: offset.try_into().map_err(map_tryfromint_err)?,
                    addend,
                });
        }
    }

    let eh_frame_section_indices = eh_frame_section_indices
        .iter()
        .map(|index| {
            section_to_custom_section.get(index).map_or_else(
                || {
                    Err(CompileError::Codegen(format!(
                        ".eh_frame section with index={:?} was never loaded",
                        index
                    )))
                },
                |idx| Ok(*idx),
            )
        })
        .collect::<Result<Vec<SectionIndex>, _>>()?;

    let mut custom_sections = section_to_custom_section
        .iter()
        .map(|(elf_section_index, custom_section_index)| {
            (
                custom_section_index,
                CustomSection {
                    protection: CustomSectionProtection::Read,
                    bytes: SectionBody::new_with_vec(
                        elf.section_by_index(*elf_section_index)
                            .unwrap()
                            .data()
                            .unwrap()
                            .to_vec(),
                    ),
                    relocations: relocations
                        .remove_entry(elf_section_index)
                        .map_or(vec![], |(_, v)| v),
                },
            )
        })
        .collect::<Vec<_>>();
    custom_sections.sort_unstable_by_key(|a| a.0);
    let custom_sections = custom_sections
        .into_iter()
        .map(|(_, v)| v)
        .collect::<PrimaryMap<SectionIndex, _>>();

    let function_body = FunctionBody {
        body: elf
            .section_by_index(root_section_index)
            .unwrap()
            .data()
            .unwrap()
            .to_vec(),
        unwind_info: None,
    };

    let address_map = FunctionAddressMap {
        instructions: vec![InstructionAddressMap {
            srcloc: SourceLoc::default(),
            code_offset: 0,
            code_len: function_body.body.len(),
        }],
        start_srcloc: SourceLoc::default(),
        end_srcloc: SourceLoc::default(),
        body_offset: 0,
        body_len: function_body.body.len(),
    };

    Ok(CompiledFunction {
        compiled_function: wasmer_compiler::CompiledFunction {
            body: function_body,
            jt_offsets: SecondaryMap::new(),
            relocations: relocations
                .remove_entry(&root_section_index)
                .map_or(vec![], |(_, v)| v),
            frame_info: CompiledFunctionFrameInfo {
                address_map,
                traps: vec![],
            },
        },
        custom_sections,
        eh_frame_section_indices,
    })
}

'''
'''--- lib/compiler-llvm/src/trampoline/mod.rs ---
mod wasm;

pub use self::wasm::FuncTrampoline;

'''
'''--- lib/compiler-llvm/src/trampoline/wasm.rs ---
use crate::abi::{get_abi, Abi};
use crate::config::{CompiledKind, LLVM};
use crate::object_file::{load_object_file, CompiledFunction};
use crate::translator::intrinsics::{type_to_llvm, type_to_llvm_ptr, Intrinsics};
use inkwell::values::BasicMetadataValueEnum;
use inkwell::{
    attributes::{Attribute, AttributeLoc},
    context::Context,
    module::{Linkage, Module},
    passes::PassManager,
    targets::{FileType, TargetMachine},
    types::BasicType,
    values::FunctionValue,
    AddressSpace, DLLStorageClass,
};
use std::cmp;
use std::convert::TryFrom;
use std::convert::TryInto;
use wasmer_compiler::{CompileError, FunctionBody, RelocationTarget};
use wasmer_types::{FunctionType, LocalFunctionIndex};

pub struct FuncTrampoline {
    ctx: Context,
    target_machine: TargetMachine,
    abi: Box<dyn Abi>,
}

const FUNCTION_SECTION: &str = "__TEXT,wasmer_trmpl"; // Needs to be between 1 and 16 chars

impl FuncTrampoline {
    pub fn new(target_machine: TargetMachine) -> Self {
        let abi = get_abi(&target_machine);
        Self {
            ctx: Context::create(),
            target_machine,
            abi,
        }
    }

    pub fn trampoline_to_module(
        &self,
        ty: &FunctionType,
        config: &LLVM,
        name: &str,
    ) -> Result<Module, CompileError> {
        // The function type, used for the callbacks.
        let function = CompiledKind::FunctionCallTrampoline(ty.clone());
        let module = self.ctx.create_module("");
        let target_machine = &self.target_machine;
        let target_triple = target_machine.get_triple();
        let target_data = target_machine.get_target_data();
        module.set_triple(&target_triple);
        module.set_data_layout(&target_data.get_data_layout());
        let intrinsics = Intrinsics::declare(&module, &self.ctx, &target_data);

        let (callee_ty, callee_attrs) =
            self.abi
                .func_type_to_llvm(&self.ctx, &intrinsics, None, ty)?;
        let trampoline_ty = intrinsics.void_ty.fn_type(
            &[
                intrinsics.ctx_ptr_ty.into(),                     // vmctx ptr
                callee_ty.ptr_type(AddressSpace::Generic).into(), // callee function address
                intrinsics.i128_ptr_ty.into(),                    // in/out values ptr
            ],
            false,
        );

        let trampoline_func = module.add_function(name, trampoline_ty, Some(Linkage::External));
        trampoline_func
            .as_global_value()
            .set_section(FUNCTION_SECTION);
        trampoline_func
            .as_global_value()
            .set_linkage(Linkage::DLLExport);
        trampoline_func
            .as_global_value()
            .set_dll_storage_class(DLLStorageClass::Export);
        self.generate_trampoline(trampoline_func, ty, &callee_attrs, &self.ctx, &intrinsics)?;

        if let Some(ref callbacks) = config.callbacks {
            callbacks.preopt_ir(&function, &module);
        }

        let pass_manager = PassManager::create(());

        if config.enable_verifier {
            pass_manager.add_verifier_pass();
        }

        pass_manager.add_early_cse_pass();

        pass_manager.run_on(&module);

        if let Some(ref callbacks) = config.callbacks {
            callbacks.postopt_ir(&function, &module);
        }

        Ok(module)
    }

    pub fn trampoline(
        &self,
        ty: &FunctionType,
        config: &LLVM,
        name: &str,
    ) -> Result<FunctionBody, CompileError> {
        let module = self.trampoline_to_module(ty, config, name)?;
        let function = CompiledKind::FunctionCallTrampoline(ty.clone());
        let target_machine = &self.target_machine;

        let memory_buffer = target_machine
            .write_to_memory_buffer(&module, FileType::Object)
            .unwrap();

        if let Some(ref callbacks) = config.callbacks {
            callbacks.obj_memory_buffer(&function, &memory_buffer);
        }

        let mem_buf_slice = memory_buffer.as_slice();
        let CompiledFunction {
            compiled_function,
            custom_sections,
            eh_frame_section_indices,
        } = load_object_file(
            mem_buf_slice,
            FUNCTION_SECTION,
            RelocationTarget::LocalFunc(LocalFunctionIndex::from_u32(0)),
            |name: &str| {
                Err(CompileError::Codegen(format!(
                    "trampoline generation produced reference to unknown function {}",
                    name
                )))
            },
        )?;
        let mut all_sections_are_eh_sections = true;
        if eh_frame_section_indices.len() != custom_sections.len() {
            all_sections_are_eh_sections = false;
        } else {
            let mut eh_frame_section_indices = eh_frame_section_indices;
            eh_frame_section_indices.sort_unstable();
            for (idx, section_idx) in eh_frame_section_indices.iter().enumerate() {
                if idx as u32 != section_idx.as_u32() {
                    all_sections_are_eh_sections = false;
                    break;
                }
            }
        }
        if !all_sections_are_eh_sections {
            return Err(CompileError::Codegen(
                "trampoline generation produced non-eh custom sections".into(),
            ));
        }
        if !compiled_function.relocations.is_empty() {
            return Err(CompileError::Codegen(
                "trampoline generation produced relocations".into(),
            ));
        }
        if !compiled_function.jt_offsets.is_empty() {
            return Err(CompileError::Codegen(
                "trampoline generation produced jump tables".into(),
            ));
        }
        // Ignore CompiledFunctionFrameInfo. Extra frame info isn't a problem.

        Ok(FunctionBody {
            body: compiled_function.body.body,
            unwind_info: compiled_function.body.unwind_info,
        })
    }

    pub fn dynamic_trampoline_to_module(
        &self,
        ty: &FunctionType,
        config: &LLVM,
        name: &str,
    ) -> Result<Module, CompileError> {
        // The function type, used for the callbacks
        let function = CompiledKind::DynamicFunctionTrampoline(ty.clone());
        let module = self.ctx.create_module("");
        let target_machine = &self.target_machine;
        let target_data = target_machine.get_target_data();
        let target_triple = target_machine.get_triple();
        module.set_triple(&target_triple);
        module.set_data_layout(&target_data.get_data_layout());
        let intrinsics = Intrinsics::declare(&module, &self.ctx, &target_data);

        let (trampoline_ty, trampoline_attrs) =
            self.abi
                .func_type_to_llvm(&self.ctx, &intrinsics, None, ty)?;
        let trampoline_func = module.add_function(name, trampoline_ty, Some(Linkage::External));
        for (attr, attr_loc) in trampoline_attrs {
            trampoline_func.add_attribute(attr_loc, attr);
        }
        trampoline_func
            .as_global_value()
            .set_section(FUNCTION_SECTION);
        trampoline_func
            .as_global_value()
            .set_linkage(Linkage::DLLExport);
        trampoline_func
            .as_global_value()
            .set_dll_storage_class(DLLStorageClass::Export);
        self.generate_dynamic_trampoline(trampoline_func, ty, &self.ctx, &intrinsics)?;

        if let Some(ref callbacks) = config.callbacks {
            callbacks.preopt_ir(&function, &module);
        }

        let pass_manager = PassManager::create(());

        if config.enable_verifier {
            pass_manager.add_verifier_pass();
        }

        pass_manager.add_early_cse_pass();

        pass_manager.run_on(&module);

        if let Some(ref callbacks) = config.callbacks {
            callbacks.postopt_ir(&function, &module);
        }

        Ok(module)
    }
    pub fn dynamic_trampoline(
        &self,
        ty: &FunctionType,
        config: &LLVM,
        name: &str,
    ) -> Result<FunctionBody, CompileError> {
        let function = CompiledKind::DynamicFunctionTrampoline(ty.clone());
        let target_machine = &self.target_machine;

        let module = self.dynamic_trampoline_to_module(ty, config, name)?;

        let memory_buffer = target_machine
            .write_to_memory_buffer(&module, FileType::Object)
            .unwrap();

        if let Some(ref callbacks) = config.callbacks {
            callbacks.obj_memory_buffer(&function, &memory_buffer);
        }

        let mem_buf_slice = memory_buffer.as_slice();
        let CompiledFunction {
            compiled_function,
            custom_sections,
            eh_frame_section_indices,
        } = load_object_file(
            mem_buf_slice,
            FUNCTION_SECTION,
            RelocationTarget::LocalFunc(LocalFunctionIndex::from_u32(0)),
            |name: &str| {
                Err(CompileError::Codegen(format!(
                    "trampoline generation produced reference to unknown function {}",
                    name
                )))
            },
        )?;
        let mut all_sections_are_eh_sections = true;
        if eh_frame_section_indices.len() != custom_sections.len() {
            all_sections_are_eh_sections = false;
        } else {
            let mut eh_frame_section_indices = eh_frame_section_indices;
            eh_frame_section_indices.sort_unstable();
            for (idx, section_idx) in eh_frame_section_indices.iter().enumerate() {
                if idx as u32 != section_idx.as_u32() {
                    all_sections_are_eh_sections = false;
                    break;
                }
            }
        }
        if !all_sections_are_eh_sections {
            return Err(CompileError::Codegen(
                "trampoline generation produced non-eh custom sections".into(),
            ));
        }
        if !compiled_function.relocations.is_empty() {
            return Err(CompileError::Codegen(
                "trampoline generation produced relocations".into(),
            ));
        }
        if !compiled_function.jt_offsets.is_empty() {
            return Err(CompileError::Codegen(
                "trampoline generation produced jump tables".into(),
            ));
        }
        // Ignore CompiledFunctionFrameInfo. Extra frame info isn't a problem.

        Ok(FunctionBody {
            body: compiled_function.body.body,
            unwind_info: compiled_function.body.unwind_info,
        })
    }

    fn generate_trampoline<'ctx>(
        &self,
        trampoline_func: FunctionValue,
        func_sig: &FunctionType,
        func_attrs: &[(Attribute, AttributeLoc)],
        context: &'ctx Context,
        intrinsics: &Intrinsics<'ctx>,
    ) -> Result<(), CompileError> {
        let entry_block = context.append_basic_block(trampoline_func, "entry");
        let builder = context.create_builder();
        builder.position_at_end(entry_block);

        let (callee_vmctx_ptr, func_ptr, args_rets_ptr) =
            match *trampoline_func.get_params().as_slice() {
                [callee_vmctx_ptr, func_ptr, args_rets_ptr] => (
                    callee_vmctx_ptr,
                    func_ptr.into_pointer_value(),
                    args_rets_ptr.into_pointer_value(),
                ),
                _ => {
                    return Err(CompileError::Codegen(
                        "trampoline function unimplemented".to_string(),
                    ))
                }
            };

        let mut args_vec: Vec<BasicMetadataValueEnum> =
            Vec::with_capacity(func_sig.params().len() + 1);

        if self.abi.is_sret(func_sig)? {
            let basic_types: Vec<_> = func_sig
                .results()
                .iter()
                .map(|&ty| type_to_llvm(intrinsics, ty))
                .collect::<Result<_, _>>()?;

            let sret_ty = context.struct_type(&basic_types, false);
            args_vec.push(builder.build_alloca(sret_ty, "sret").into());
        }

        args_vec.push(callee_vmctx_ptr.into());

        for (i, param_ty) in func_sig.params().iter().enumerate() {
            let index = intrinsics.i32_ty.const_int(i as _, false);
            let item_pointer =
                unsafe { builder.build_in_bounds_gep(args_rets_ptr, &[index], "arg_ptr") };

            let casted_pointer_type = type_to_llvm_ptr(intrinsics, *param_ty)?;

            let typed_item_pointer =
                builder.build_pointer_cast(item_pointer, casted_pointer_type, "typed_arg_pointer");

            let arg = builder.build_load(typed_item_pointer, "arg");
            args_vec.push(arg.into());
        }

        let callable_func = inkwell::values::CallableValue::try_from(func_ptr).unwrap();
        let call_site = builder.build_call(callable_func, args_vec.as_slice().into(), "call");
        for (attr, attr_loc) in func_attrs {
            call_site.add_attribute(*attr_loc, *attr);
        }

        let rets = self
            .abi
            .rets_from_call(&builder, intrinsics, call_site, func_sig);
        let mut idx = 0;
        rets.iter().for_each(|v| {
            let ptr = unsafe {
                builder.build_gep(
                    args_rets_ptr,
                    &[intrinsics.i32_ty.const_int(idx, false)],
                    "",
                )
            };
            let ptr =
                builder.build_pointer_cast(ptr, v.get_type().ptr_type(AddressSpace::Generic), "");
            builder.build_store(ptr, *v);
            if v.get_type() == intrinsics.i128_ty.as_basic_type_enum() {
                idx += 1;
            }
            idx += 1;
        });

        builder.build_return(None);
        Ok(())
    }

    fn generate_dynamic_trampoline<'ctx>(
        &self,
        trampoline_func: FunctionValue,
        func_sig: &FunctionType,
        context: &'ctx Context,
        intrinsics: &Intrinsics<'ctx>,
    ) -> Result<(), CompileError> {
        let entry_block = context.append_basic_block(trampoline_func, "entry");
        let builder = context.create_builder();
        builder.position_at_end(entry_block);

        // Allocate stack space for the params and results.
        let values = builder.build_alloca(
            intrinsics.i128_ty.array_type(cmp::max(
                func_sig.params().len().try_into().unwrap(),
                func_sig.results().len().try_into().unwrap(),
            )),
            "",
        );

        // Copy params to 'values'.
        let first_user_param = if self.abi.is_sret(func_sig)? { 2 } else { 1 };
        for i in 0..func_sig.params().len() {
            let ptr = unsafe {
                builder.build_in_bounds_gep(
                    values,
                    &[
                        intrinsics.i32_zero,
                        intrinsics.i32_ty.const_int(i.try_into().unwrap(), false),
                    ],
                    "",
                )
            };
            let ptr = builder
                .build_bitcast(ptr, type_to_llvm_ptr(intrinsics, func_sig.params()[i])?, "")
                .into_pointer_value();
            builder.build_store(
                ptr,
                trampoline_func
                    .get_nth_param(i as u32 + first_user_param)
                    .unwrap(),
            );
        }

        let callee_ty = intrinsics
            .void_ty
            .fn_type(
                &[
                    intrinsics.ctx_ptr_ty.into(),  // vmctx ptr
                    intrinsics.i128_ptr_ty.into(), // in/out values ptr
                ],
                false,
            )
            .ptr_type(AddressSpace::Generic);
        let vmctx = self.abi.get_vmctx_ptr_param(&trampoline_func);
        let callee = builder
            .build_load(
                builder
                    .build_bitcast(vmctx, callee_ty.ptr_type(AddressSpace::Generic), "")
                    .into_pointer_value(),
                "",
            )
            .into_pointer_value();

        let values_ptr = builder.build_pointer_cast(values, intrinsics.i128_ptr_ty, "");
        let callable_func = inkwell::values::CallableValue::try_from(callee).unwrap();
        builder.build_call(callable_func, &[vmctx.into(), values_ptr.into()], "");

        if func_sig.results().is_empty() {
            builder.build_return(None);
        } else {
            let results = func_sig
                .results()
                .iter()
                .enumerate()
                .map(|(idx, ty)| {
                    let ptr = unsafe {
                        builder.build_gep(
                            values,
                            &[
                                intrinsics.i32_ty.const_zero(),
                                intrinsics.i32_ty.const_int(idx.try_into().unwrap(), false),
                            ],
                            "",
                        )
                    };
                    let ptr =
                        builder.build_pointer_cast(ptr, type_to_llvm_ptr(intrinsics, *ty)?, "");
                    Ok(builder.build_load(ptr, ""))
                })
                .collect::<Result<Vec<_>, CompileError>>()?;

            if self.abi.is_sret(func_sig)? {
                let sret = trampoline_func
                    .get_first_param()
                    .unwrap()
                    .into_pointer_value();
                let mut struct_value = sret
                    .get_type()
                    .get_element_type()
                    .into_struct_type()
                    .get_undef();
                for (idx, value) in results.iter().enumerate() {
                    let value = builder.build_bitcast(
                        *value,
                        type_to_llvm(&intrinsics, func_sig.results()[idx])?,
                        "",
                    );
                    struct_value = builder
                        .build_insert_value(struct_value, value, idx as u32, "")
                        .unwrap()
                        .into_struct_value();
                }
                builder.build_store(sret, struct_value);
                builder.build_return(None);
            } else {
                builder.build_return(Some(&self.abi.pack_values_for_register_return(
                    &intrinsics,
                    &builder,
                    &results.as_slice(),
                    &trampoline_func.get_type(),
                )?));
            }
        }

        Ok(())
    }
}

'''
'''--- lib/compiler-llvm/src/translator/code.rs ---
use super::{
    intrinsics::{
        tbaa_label, type_to_llvm, CtxType, FunctionCache, GlobalCache, Intrinsics, MemoryCache,
    },
    // stackmap::{StackmapEntry, StackmapEntryKind, StackmapRegistry, ValueSemantic},
    state::{ControlFrame, ExtraInfo, IfElseState, State},
};
use inkwell::{
    attributes::AttributeLoc,
    builder::Builder,
    context::Context,
    module::{Linkage, Module},
    passes::PassManager,
    targets::{FileType, TargetMachine},
    types::{BasicType, FloatMathType, IntType, PointerType, VectorType},
    values::{
        BasicMetadataValueEnum, BasicValue, BasicValueEnum, FloatValue, FunctionValue,
        InstructionOpcode, InstructionValue, IntValue, PhiValue, PointerValue, VectorValue,
    },
    AddressSpace, AtomicOrdering, AtomicRMWBinOp, DLLStorageClass, FloatPredicate, IntPredicate,
};
use smallvec::SmallVec;

use crate::abi::{get_abi, Abi};
use crate::config::{CompiledKind, LLVM};
use crate::object_file::{load_object_file, CompiledFunction};
use std::convert::TryFrom;
use wasmer_compiler::wasmparser::{MemoryImmediate, Operator};
use wasmer_compiler::{
    wptype_to_type, CompileError, FunctionBodyData, ModuleTranslationState, RelocationTarget,
    Symbol, SymbolRegistry,
};
use wasmer_types::entity::PrimaryMap;
use wasmer_types::{
    FunctionIndex, FunctionType, GlobalIndex, LocalFunctionIndex, MemoryIndex, ModuleInfo,
    SignatureIndex, TableIndex, Type,
};
use wasmer_vm::{MemoryStyle, TableStyle, VMOffsets};

const FUNCTION_SECTION: &str = "__TEXT,wasmer_function";

fn to_compile_error(err: impl std::error::Error) -> CompileError {
    CompileError::Codegen(format!("{}", err))
}

pub struct FuncTranslator {
    ctx: Context,
    target_machine: TargetMachine,
    abi: Box<dyn Abi>,
}

impl FuncTranslator {
    pub fn new(target_machine: TargetMachine) -> Self {
        let abi = get_abi(&target_machine);
        Self {
            ctx: Context::create(),
            target_machine,
            abi,
        }
    }

    pub fn translate_to_module(
        &self,
        wasm_module: &ModuleInfo,
        module_translation: &ModuleTranslationState,
        local_func_index: &LocalFunctionIndex,
        function_body: &FunctionBodyData,
        config: &LLVM,
        memory_styles: &PrimaryMap<MemoryIndex, MemoryStyle>,
        _table_styles: &PrimaryMap<TableIndex, TableStyle>,
        symbol_registry: &dyn SymbolRegistry,
    ) -> Result<Module, CompileError> {
        // The function type, used for the callbacks.
        let function = CompiledKind::Local(*local_func_index);
        let func_index = wasm_module.func_index(*local_func_index);
        let function_name =
            symbol_registry.symbol_to_name(Symbol::LocalFunction(*local_func_index));
        let module_name = match wasm_module.name.as_ref() {
            None => format!("<anonymous module> function {}", function_name),
            Some(module_name) => format!("module {} function {}", module_name, function_name),
        };
        let module = self.ctx.create_module(module_name.as_str());

        let target_machine = &self.target_machine;
        let target_triple = target_machine.get_triple();
        let target_data = target_machine.get_target_data();
        module.set_triple(&target_triple);
        module.set_data_layout(&target_data.get_data_layout());
        let wasm_fn_type = wasm_module
            .signatures
            .get(wasm_module.functions[func_index])
            .unwrap();

        // TODO: pointer width
        let offsets = VMOffsets::new(target_data.get_pointer_byte_size(None) as u8)
            .with_module_info(&wasm_module);
        let intrinsics = Intrinsics::declare(&module, &self.ctx, &target_data);
        let (func_type, func_attrs) =
            self.abi
                .func_type_to_llvm(&self.ctx, &intrinsics, Some(&offsets), wasm_fn_type)?;

        let func = module.add_function(&function_name, func_type, Some(Linkage::External));
        for (attr, attr_loc) in &func_attrs {
            func.add_attribute(*attr_loc, *attr);
        }

        func.add_attribute(AttributeLoc::Function, intrinsics.stack_probe);
        func.set_personality_function(intrinsics.personality);
        func.as_global_value().set_section(FUNCTION_SECTION);
        func.set_linkage(Linkage::DLLExport);
        func.as_global_value()
            .set_dll_storage_class(DLLStorageClass::Export);

        let entry = self.ctx.append_basic_block(func, "entry");
        let start_of_code = self.ctx.append_basic_block(func, "start_of_code");
        let return_ = self.ctx.append_basic_block(func, "return");
        let alloca_builder = self.ctx.create_builder();
        let cache_builder = self.ctx.create_builder();
        let builder = self.ctx.create_builder();
        cache_builder.position_at_end(entry);
        let br = cache_builder.build_unconditional_branch(start_of_code);
        alloca_builder.position_before(&br);
        cache_builder.position_before(&br);
        builder.position_at_end(start_of_code);

        let mut state = State::new();
        builder.position_at_end(return_);
        let phis: SmallVec<[PhiValue; 1]> = wasm_fn_type
            .results()
            .iter()
            .map(|&wasm_ty| type_to_llvm(&intrinsics, wasm_ty).map(|ty| builder.build_phi(ty, "")))
            .collect::<Result<_, _>>()?;
        state.push_block(return_, phis);
        builder.position_at_end(start_of_code);

        let reader =
            wasmer_compiler::FunctionReader::new(function_body.module_offset, function_body.data);

        let mut params = vec![];
        let first_param =
            if func_type.get_return_type().is_none() && wasm_fn_type.results().len() > 1 {
                2
            } else {
                1
            };
        let mut is_first_alloca = true;
        let mut insert_alloca = |ty, name| {
            let alloca = alloca_builder.build_alloca(ty, name);
            if is_first_alloca {
                alloca_builder.position_at(entry, &alloca.as_instruction_value().unwrap());
                is_first_alloca = false;
            }
            alloca
        };

        for idx in 0..wasm_fn_type.params().len() {
            let ty = wasm_fn_type.params()[idx];
            let ty = type_to_llvm(&intrinsics, ty)?;
            let value = func
                .get_nth_param((idx as u32).checked_add(first_param).unwrap())
                .unwrap();
            let alloca = insert_alloca(ty, "param");
            cache_builder.build_store(alloca, value);
            params.push(alloca);
        }

        let mut local_reader = reader.get_locals_reader()?;
        let mut locals = vec![];
        let num_locals = local_reader.get_count();
        for _ in 0..num_locals {
            let (count, ty) = local_reader.read()?;
            let ty = wptype_to_type(ty).map_err(to_compile_error)?;
            let ty = type_to_llvm(&intrinsics, ty)?;
            for _ in 0..count {
                let alloca = insert_alloca(ty, "local");
                cache_builder.build_store(alloca, ty.const_zero());
                locals.push(alloca);
            }
        }

        let mut params_locals = params.clone();
        params_locals.extend(locals.iter().cloned());

        let mut fcg = LLVMFunctionCodeGenerator {
            context: &self.ctx,
            builder,
            alloca_builder,
            intrinsics: &intrinsics,
            state,
            function: func,
            locals: params_locals,
            ctx: CtxType::new(wasm_module, &func, &cache_builder, &*self.abi),
            unreachable_depth: 0,
            memory_styles,
            _table_styles,
            module: &module,
            module_translation,
            wasm_module,
            symbol_registry,
            abi: &*self.abi,
            config,
        };
        fcg.ctx.add_func(
            func_index,
            func.as_global_value().as_pointer_value(),
            fcg.ctx.basic(),
            &func_attrs,
        );

        let mut operator_reader = reader.get_operators_reader()?.into_iter_with_offsets();
        while fcg.state.has_control_frames() {
            let (op, pos) = operator_reader.next().unwrap()?;
            fcg.translate_operator(op, pos as u32)?;
        }

        fcg.finalize(wasm_fn_type)?;

        if let Some(ref callbacks) = config.callbacks {
            callbacks.preopt_ir(&function, &module);
        }

        let pass_manager = PassManager::create(());

        if config.enable_verifier {
            pass_manager.add_verifier_pass();
        }

        pass_manager.add_type_based_alias_analysis_pass();
        pass_manager.add_sccp_pass();
        pass_manager.add_prune_eh_pass();
        pass_manager.add_dead_arg_elimination_pass();
        pass_manager.add_lower_expect_intrinsic_pass();
        pass_manager.add_scalar_repl_aggregates_pass();
        pass_manager.add_instruction_combining_pass();
        pass_manager.add_jump_threading_pass();
        pass_manager.add_correlated_value_propagation_pass();
        pass_manager.add_cfg_simplification_pass();
        pass_manager.add_reassociate_pass();
        pass_manager.add_loop_rotate_pass();
        pass_manager.add_loop_unswitch_pass();
        pass_manager.add_ind_var_simplify_pass();
        pass_manager.add_licm_pass();
        pass_manager.add_loop_vectorize_pass();
        pass_manager.add_instruction_combining_pass();
        pass_manager.add_sccp_pass();
        pass_manager.add_reassociate_pass();
        pass_manager.add_cfg_simplification_pass();
        pass_manager.add_gvn_pass();
        pass_manager.add_memcpy_optimize_pass();
        pass_manager.add_dead_store_elimination_pass();
        pass_manager.add_bit_tracking_dce_pass();
        pass_manager.add_instruction_combining_pass();
        pass_manager.add_reassociate_pass();
        pass_manager.add_cfg_simplification_pass();
        pass_manager.add_slp_vectorize_pass();
        pass_manager.add_early_cse_pass();

        pass_manager.run_on(&module);

        if let Some(ref callbacks) = config.callbacks {
            callbacks.postopt_ir(&function, &module);
        }

        Ok(module)
    }

    pub fn translate(
        &self,
        wasm_module: &ModuleInfo,
        module_translation: &ModuleTranslationState,
        local_func_index: &LocalFunctionIndex,
        function_body: &FunctionBodyData,
        config: &LLVM,
        memory_styles: &PrimaryMap<MemoryIndex, MemoryStyle>,
        table_styles: &PrimaryMap<TableIndex, TableStyle>,
        symbol_registry: &dyn SymbolRegistry,
    ) -> Result<CompiledFunction, CompileError> {
        let module = self.translate_to_module(
            wasm_module,
            module_translation,
            local_func_index,
            function_body,
            config,
            memory_styles,
            table_styles,
            symbol_registry,
        )?;
        let function = CompiledKind::Local(*local_func_index);
        let target_machine = &self.target_machine;
        let memory_buffer = target_machine
            .write_to_memory_buffer(&module, FileType::Object)
            .unwrap();

        if let Some(ref callbacks) = config.callbacks {
            callbacks.obj_memory_buffer(&function, &memory_buffer);
        }

        let mem_buf_slice = memory_buffer.as_slice();
        load_object_file(
            mem_buf_slice,
            FUNCTION_SECTION,
            RelocationTarget::LocalFunc(*local_func_index),
            |name: &str| {
                Ok(
                    if let Some(Symbol::LocalFunction(local_func_index)) =
                        symbol_registry.name_to_symbol(name)
                    {
                        Some(RelocationTarget::LocalFunc(local_func_index))
                    } else {
                        None
                    },
                )
            },
        )
    }
}

impl<'ctx, 'a> LLVMFunctionCodeGenerator<'ctx, 'a> {
    // Create a vector where each lane contains the same value.
    fn splat_vector(
        &self,
        value: BasicValueEnum<'ctx>,
        vec_ty: VectorType<'ctx>,
    ) -> VectorValue<'ctx> {
        // Use insert_element to insert the element into an undef vector, then use
        // shuffle vector to copy that lane to all lanes.
        self.builder.build_shuffle_vector(
            self.builder.build_insert_element(
                vec_ty.get_undef(),
                value,
                self.intrinsics.i32_zero,
                "",
            ),
            vec_ty.get_undef(),
            self.intrinsics
                .i32_ty
                .vec_type(vec_ty.get_size())
                .const_zero(),
            "",
        )
    }

    // Convert floating point vector to integer and saturate when out of range.
    // https://github.com/WebAssembly/nontrapping-float-to-int-conversions/blob/master/proposals/nontrapping-float-to-int-conversion/Overview.md
    fn trunc_sat<T: FloatMathType<'ctx>>(
        &self,
        fvec_ty: T,
        ivec_ty: T::MathConvType,
        lower_bound: u64, // Exclusive (least representable value)
        upper_bound: u64, // Exclusive (greatest representable value)
        int_min_value: u64,
        int_max_value: u64,
        value: IntValue<'ctx>,
    ) -> VectorValue<'ctx> {
        // a) Compare vector with itself to identify NaN lanes.
        // b) Compare vector with splat of inttofp(upper_bound) to identify
        //    lanes that need to saturate to max.
        // c) Compare vector with splat of inttofp(lower_bound) to identify
        //    lanes that need to saturate to min.
        // d) Use vector select (not shuffle) to pick from either the
        //    splat vector or the input vector depending on whether the
        //    comparison indicates that we have an unrepresentable value. Replace
        //    unrepresentable values with zero.
        // e) Now that the value is safe, fpto[su]i it.
        // f) Use our previous comparison results to replace certain zeros with
        //    int_min or int_max.

        let fvec_ty = fvec_ty.as_basic_type_enum().into_vector_type();
        let ivec_ty = ivec_ty.as_basic_type_enum().into_vector_type();
        let fvec_element_ty = fvec_ty.get_element_type().into_float_type();
        let ivec_element_ty = ivec_ty.get_element_type().into_int_type();

        let is_signed = int_min_value != 0;
        let int_min_value = self.splat_vector(
            ivec_element_ty
                .const_int(int_min_value, is_signed)
                .as_basic_value_enum(),
            ivec_ty,
        );
        let int_max_value = self.splat_vector(
            ivec_element_ty
                .const_int(int_max_value, is_signed)
                .as_basic_value_enum(),
            ivec_ty,
        );
        let lower_bound = if is_signed {
            self.builder.build_signed_int_to_float(
                ivec_element_ty.const_int(lower_bound, is_signed),
                fvec_element_ty,
                "",
            )
        } else {
            self.builder.build_unsigned_int_to_float(
                ivec_element_ty.const_int(lower_bound, is_signed),
                fvec_element_ty,
                "",
            )
        };
        let upper_bound = if is_signed {
            self.builder.build_signed_int_to_float(
                ivec_element_ty.const_int(upper_bound, is_signed),
                fvec_element_ty,
                "",
            )
        } else {
            self.builder.build_unsigned_int_to_float(
                ivec_element_ty.const_int(upper_bound, is_signed),
                fvec_element_ty,
                "",
            )
        };

        let value = self
            .builder
            .build_bitcast(value, fvec_ty, "")
            .into_vector_value();
        let zero = fvec_ty.const_zero();
        let lower_bound = self.splat_vector(lower_bound.as_basic_value_enum(), fvec_ty);
        let upper_bound = self.splat_vector(upper_bound.as_basic_value_enum(), fvec_ty);
        let nan_cmp = self
            .builder
            .build_float_compare(FloatPredicate::UNO, value, zero, "nan");
        let above_upper_bound_cmp = self.builder.build_float_compare(
            FloatPredicate::OGT,
            value,
            upper_bound,
            "above_upper_bound",
        );
        let below_lower_bound_cmp = self.builder.build_float_compare(
            FloatPredicate::OLT,
            value,
            lower_bound,
            "below_lower_bound",
        );
        let not_representable = self.builder.build_or(
            self.builder.build_or(nan_cmp, above_upper_bound_cmp, ""),
            below_lower_bound_cmp,
            "not_representable_as_int",
        );
        let value = self
            .builder
            .build_select(not_representable, zero, value, "safe_to_convert")
            .into_vector_value();
        let value = if is_signed {
            self.builder
                .build_float_to_signed_int(value, ivec_ty, "as_int")
        } else {
            self.builder
                .build_float_to_unsigned_int(value, ivec_ty, "as_int")
        };
        let value = self
            .builder
            .build_select(above_upper_bound_cmp, int_max_value, value, "")
            .into_vector_value();
        self.builder
            .build_select(below_lower_bound_cmp, int_min_value, value, "")
            .into_vector_value()
    }

    // Convert floating point vector to integer and saturate when out of range.
    // https://github.com/WebAssembly/nontrapping-float-to-int-conversions/blob/master/proposals/nontrapping-float-to-int-conversion/Overview.md
    fn trunc_sat_into_int<T: FloatMathType<'ctx>>(
        &self,
        fvec_ty: T,
        ivec_ty: T::MathConvType,
        lower_bound: u64, // Exclusive (least representable value)
        upper_bound: u64, // Exclusive (greatest representable value)
        int_min_value: u64,
        int_max_value: u64,
        value: IntValue<'ctx>,
    ) -> IntValue<'ctx> {
        let res = self.trunc_sat(
            fvec_ty,
            ivec_ty,
            lower_bound,
            upper_bound,
            int_min_value,
            int_max_value,
            value,
        );
        self.builder
            .build_bitcast(res, self.intrinsics.i128_ty, "")
            .into_int_value()
    }

    // Convert floating point vector to integer and saturate when out of range.
    // https://github.com/WebAssembly/nontrapping-float-to-int-conversions/blob/master/proposals/nontrapping-float-to-int-conversion/Overview.md
    fn trunc_sat_scalar(
        &self,
        int_ty: IntType<'ctx>,
        lower_bound: u64, // Exclusive (least representable value)
        upper_bound: u64, // Exclusive (greatest representable value)
        int_min_value: u64,
        int_max_value: u64,
        value: FloatValue<'ctx>,
    ) -> IntValue<'ctx> {
        // TODO: this is a scalarized version of the process in trunc_sat. Either
        // we should merge with trunc_sat, or we should simplify this function.

        // a) Compare value with itself to identify NaN.
        // b) Compare value inttofp(upper_bound) to identify values that need to
        //    saturate to max.
        // c) Compare value with inttofp(lower_bound) to identify values that need
        //    to saturate to min.
        // d) Use select to pick from either zero or the input vector depending on
        //    whether the comparison indicates that we have an unrepresentable
        //    value.
        // e) Now that the value is safe, fpto[su]i it.
        // f) Use our previous comparison results to replace certain zeros with
        //    int_min or int_max.

        let is_signed = int_min_value != 0;
        let int_min_value = int_ty.const_int(int_min_value, is_signed);
        let int_max_value = int_ty.const_int(int_max_value, is_signed);

        let lower_bound = if is_signed {
            self.builder.build_signed_int_to_float(
                int_ty.const_int(lower_bound, is_signed),
                value.get_type(),
                "",
            )
        } else {
            self.builder.build_unsigned_int_to_float(
                int_ty.const_int(lower_bound, is_signed),
                value.get_type(),
                "",
            )
        };
        let upper_bound = if is_signed {
            self.builder.build_signed_int_to_float(
                int_ty.const_int(upper_bound, is_signed),
                value.get_type(),
                "",
            )
        } else {
            self.builder.build_unsigned_int_to_float(
                int_ty.const_int(upper_bound, is_signed),
                value.get_type(),
                "",
            )
        };

        let zero = value.get_type().const_zero();

        let nan_cmp = self
            .builder
            .build_float_compare(FloatPredicate::UNO, value, zero, "nan");
        let above_upper_bound_cmp = self.builder.build_float_compare(
            FloatPredicate::OGT,
            value,
            upper_bound,
            "above_upper_bound",
        );
        let below_lower_bound_cmp = self.builder.build_float_compare(
            FloatPredicate::OLT,
            value,
            lower_bound,
            "below_lower_bound",
        );
        let not_representable = self.builder.build_or(
            self.builder.build_or(nan_cmp, above_upper_bound_cmp, ""),
            below_lower_bound_cmp,
            "not_representable_as_int",
        );
        let value = self
            .builder
            .build_select(not_representable, zero, value, "safe_to_convert")
            .into_float_value();
        let value = if is_signed {
            self.builder
                .build_float_to_signed_int(value, int_ty, "as_int")
        } else {
            self.builder
                .build_float_to_unsigned_int(value, int_ty, "as_int")
        };
        let value = self
            .builder
            .build_select(above_upper_bound_cmp, int_max_value, value, "")
            .into_int_value();
        let value = self
            .builder
            .build_select(below_lower_bound_cmp, int_min_value, value, "")
            .into_int_value();
        self.builder
            .build_bitcast(value, int_ty, "")
            .into_int_value()
    }

    fn trap_if_not_representable_as_int(
        &self,
        lower_bound: u64, // Inclusive (not a trapping value)
        upper_bound: u64, // Inclusive (not a trapping value)
        value: FloatValue,
    ) {
        let float_ty = value.get_type();
        let int_ty = if float_ty == self.intrinsics.f32_ty {
            self.intrinsics.i32_ty
        } else {
            self.intrinsics.i64_ty
        };

        let lower_bound = self
            .builder
            .build_bitcast(int_ty.const_int(lower_bound, false), float_ty, "")
            .into_float_value();
        let upper_bound = self
            .builder
            .build_bitcast(int_ty.const_int(upper_bound, false), float_ty, "")
            .into_float_value();

        // The 'U' in the float predicate is short for "unordered" which means that
        // the comparison will compare true if either operand is a NaN. Thus, NaNs
        // are out of bounds.
        let above_upper_bound_cmp = self.builder.build_float_compare(
            FloatPredicate::UGT,
            value,
            upper_bound,
            "above_upper_bound",
        );
        let below_lower_bound_cmp = self.builder.build_float_compare(
            FloatPredicate::ULT,
            value,
            lower_bound,
            "below_lower_bound",
        );
        let out_of_bounds = self.builder.build_or(
            above_upper_bound_cmp,
            below_lower_bound_cmp,
            "out_of_bounds",
        );

        let failure_block = self
            .context
            .append_basic_block(self.function, "conversion_failure_block");
        let continue_block = self
            .context
            .append_basic_block(self.function, "conversion_success_block");

        self.builder
            .build_conditional_branch(out_of_bounds, failure_block, continue_block);
        self.builder.position_at_end(failure_block);
        let is_nan = self
            .builder
            .build_float_compare(FloatPredicate::UNO, value, value, "is_nan");
        let trap_code = self.builder.build_select(
            is_nan,
            self.intrinsics.trap_bad_conversion_to_integer,
            self.intrinsics.trap_illegal_arithmetic,
            "",
        );
        self.builder
            .build_call(self.intrinsics.throw_trap, &[trap_code.into()], "throw");
        self.builder.build_unreachable();
        self.builder.position_at_end(continue_block);
    }

    fn trap_if_zero_or_overflow(&self, left: IntValue, right: IntValue) {
        let int_type = left.get_type();

        let (min_value, neg_one_value) = if int_type == self.intrinsics.i32_ty {
            let min_value = int_type.const_int(i32::min_value() as u64, false);
            let neg_one_value = int_type.const_int(-1i32 as u32 as u64, false);
            (min_value, neg_one_value)
        } else if int_type == self.intrinsics.i64_ty {
            let min_value = int_type.const_int(i64::min_value() as u64, false);
            let neg_one_value = int_type.const_int(-1i64 as u64, false);
            (min_value, neg_one_value)
        } else {
            unreachable!()
        };

        let divisor_is_zero = self.builder.build_int_compare(
            IntPredicate::EQ,
            right,
            int_type.const_zero(),
            "divisor_is_zero",
        );
        let should_trap = self.builder.build_or(
            divisor_is_zero,
            self.builder.build_and(
                self.builder
                    .build_int_compare(IntPredicate::EQ, left, min_value, "left_is_min"),
                self.builder.build_int_compare(
                    IntPredicate::EQ,
                    right,
                    neg_one_value,
                    "right_is_neg_one",
                ),
                "div_will_overflow",
            ),
            "div_should_trap",
        );

        let should_trap = self
            .builder
            .build_call(
                self.intrinsics.expect_i1,
                &[
                    should_trap.into(),
                    self.intrinsics.i1_ty.const_zero().into(),
                ],
                "should_trap_expect",
            )
            .try_as_basic_value()
            .left()
            .unwrap()
            .into_int_value();

        let shouldnt_trap_block = self
            .context
            .append_basic_block(self.function, "shouldnt_trap_block");
        let should_trap_block = self
            .context
            .append_basic_block(self.function, "should_trap_block");
        self.builder
            .build_conditional_branch(should_trap, should_trap_block, shouldnt_trap_block);
        self.builder.position_at_end(should_trap_block);
        let trap_code = self.builder.build_select(
            divisor_is_zero,
            self.intrinsics.trap_integer_division_by_zero,
            self.intrinsics.trap_illegal_arithmetic,
            "",
        );
        self.builder
            .build_call(self.intrinsics.throw_trap, &[trap_code.into()], "throw");
        self.builder.build_unreachable();
        self.builder.position_at_end(shouldnt_trap_block);
    }

    fn trap_if_zero(&self, value: IntValue) {
        let int_type = value.get_type();
        let should_trap = self.builder.build_int_compare(
            IntPredicate::EQ,
            value,
            int_type.const_zero(),
            "divisor_is_zero",
        );

        let should_trap = self
            .builder
            .build_call(
                self.intrinsics.expect_i1,
                &[
                    should_trap.into(),
                    self.intrinsics.i1_ty.const_zero().into(),
                ],
                "should_trap_expect",
            )
            .try_as_basic_value()
            .left()
            .unwrap()
            .into_int_value();

        let shouldnt_trap_block = self
            .context
            .append_basic_block(self.function, "shouldnt_trap_block");
        let should_trap_block = self
            .context
            .append_basic_block(self.function, "should_trap_block");
        self.builder
            .build_conditional_branch(should_trap, should_trap_block, shouldnt_trap_block);
        self.builder.position_at_end(should_trap_block);
        self.builder.build_call(
            self.intrinsics.throw_trap,
            &[self.intrinsics.trap_integer_division_by_zero.into()],
            "throw",
        );
        self.builder.build_unreachable();
        self.builder.position_at_end(shouldnt_trap_block);
    }

    fn v128_into_int_vec(
        &self,
        value: BasicValueEnum<'ctx>,
        info: ExtraInfo,
        int_vec_ty: VectorType<'ctx>,
    ) -> (VectorValue<'ctx>, ExtraInfo) {
        let (value, info) = if info.has_pending_f32_nan() {
            let value = self
                .builder
                .build_bitcast(value, self.intrinsics.f32x4_ty, "");
            (self.canonicalize_nans(value), info.strip_pending())
        } else if info.has_pending_f64_nan() {
            let value = self
                .builder
                .build_bitcast(value, self.intrinsics.f64x2_ty, "");
            (self.canonicalize_nans(value), info.strip_pending())
        } else {
            (value, info)
        };
        (
            self.builder
                .build_bitcast(value, int_vec_ty, "")
                .into_vector_value(),
            info,
        )
    }

    fn v128_into_i8x16(
        &self,
        value: BasicValueEnum<'ctx>,
        info: ExtraInfo,
    ) -> (VectorValue<'ctx>, ExtraInfo) {
        self.v128_into_int_vec(value, info, self.intrinsics.i8x16_ty)
    }

    fn v128_into_i16x8(
        &self,
        value: BasicValueEnum<'ctx>,
        info: ExtraInfo,
    ) -> (VectorValue<'ctx>, ExtraInfo) {
        self.v128_into_int_vec(value, info, self.intrinsics.i16x8_ty)
    }

    fn v128_into_i32x4(
        &self,
        value: BasicValueEnum<'ctx>,
        info: ExtraInfo,
    ) -> (VectorValue<'ctx>, ExtraInfo) {
        self.v128_into_int_vec(value, info, self.intrinsics.i32x4_ty)
    }

    fn v128_into_i64x2(
        &self,
        value: BasicValueEnum<'ctx>,
        info: ExtraInfo,
    ) -> (VectorValue<'ctx>, ExtraInfo) {
        self.v128_into_int_vec(value, info, self.intrinsics.i64x2_ty)
    }

    // If the value is pending a 64-bit canonicalization, do it now.
    // Return a f32x4 vector.
    fn v128_into_f32x4(
        &self,
        value: BasicValueEnum<'ctx>,
        info: ExtraInfo,
    ) -> (VectorValue<'ctx>, ExtraInfo) {
        let (value, info) = if info.has_pending_f64_nan() {
            let value = self
                .builder
                .build_bitcast(value, self.intrinsics.f64x2_ty, "");
            (self.canonicalize_nans(value), info.strip_pending())
        } else {
            (value, info)
        };
        (
            self.builder
                .build_bitcast(value, self.intrinsics.f32x4_ty, "")
                .into_vector_value(),
            info,
        )
    }

    // If the value is pending a 32-bit canonicalization, do it now.
    // Return a f64x2 vector.
    fn v128_into_f64x2(
        &self,
        value: BasicValueEnum<'ctx>,
        info: ExtraInfo,
    ) -> (VectorValue<'ctx>, ExtraInfo) {
        let (value, info) = if info.has_pending_f32_nan() {
            let value = self
                .builder
                .build_bitcast(value, self.intrinsics.f32x4_ty, "");
            (self.canonicalize_nans(value), info.strip_pending())
        } else {
            (value, info)
        };
        (
            self.builder
                .build_bitcast(value, self.intrinsics.f64x2_ty, "")
                .into_vector_value(),
            info,
        )
    }

    fn apply_pending_canonicalization(
        &self,
        value: BasicValueEnum<'ctx>,
        info: ExtraInfo,
    ) -> BasicValueEnum<'ctx> {
        if !self.config.enable_nan_canonicalization {
            return value;
        }

        if info.has_pending_f32_nan() {
            if value.get_type().is_vector_type()
                || value.get_type() == self.intrinsics.i128_ty.as_basic_type_enum()
            {
                let ty = value.get_type();
                let value = self
                    .builder
                    .build_bitcast(value, self.intrinsics.f32x4_ty, "");
                let value = self.canonicalize_nans(value);
                self.builder.build_bitcast(value, ty, "")
            } else {
                self.canonicalize_nans(value)
            }
        } else if info.has_pending_f64_nan() {
            if value.get_type().is_vector_type()
                || value.get_type() == self.intrinsics.i128_ty.as_basic_type_enum()
            {
                let ty = value.get_type();
                let value = self
                    .builder
                    .build_bitcast(value, self.intrinsics.f64x2_ty, "");
                let value = self.canonicalize_nans(value);
                self.builder.build_bitcast(value, ty, "")
            } else {
                self.canonicalize_nans(value)
            }
        } else {
            value
        }
    }

    // Replaces any NaN with the canonical QNaN, otherwise leaves the value alone.
    fn canonicalize_nans(&self, value: BasicValueEnum<'ctx>) -> BasicValueEnum<'ctx> {
        if !self.config.enable_nan_canonicalization {
            return value;
        }

        let f_ty = value.get_type();
        if f_ty.is_vector_type() {
            let value = value.into_vector_value();
            let f_ty = f_ty.into_vector_type();
            let zero = f_ty.const_zero();
            let nan_cmp = self
                .builder
                .build_float_compare(FloatPredicate::UNO, value, zero, "nan");
            let canonical_qnan = f_ty
                .get_element_type()
                .into_float_type()
                .const_float(std::f64::NAN);
            let canonical_qnan = self.splat_vector(canonical_qnan.as_basic_value_enum(), f_ty);
            self.builder
                .build_select(nan_cmp, canonical_qnan, value, "")
                .as_basic_value_enum()
        } else {
            let value = value.into_float_value();
            let f_ty = f_ty.into_float_type();
            let zero = f_ty.const_zero();
            let nan_cmp = self
                .builder
                .build_float_compare(FloatPredicate::UNO, value, zero, "nan");
            let canonical_qnan = f_ty.const_float(std::f64::NAN);
            self.builder
                .build_select(nan_cmp, canonical_qnan, value, "")
                .as_basic_value_enum()
        }
    }

    fn quiet_nan(&self, value: BasicValueEnum<'ctx>) -> BasicValueEnum<'ctx> {
        let intrinsic = if value
            .get_type()
            .eq(&self.intrinsics.f32_ty.as_basic_type_enum())
        {
            Some(self.intrinsics.add_f32)
        } else if value
            .get_type()
            .eq(&self.intrinsics.f64_ty.as_basic_type_enum())
        {
            Some(self.intrinsics.add_f64)
        } else if value
            .get_type()
            .eq(&self.intrinsics.f32x4_ty.as_basic_type_enum())
        {
            Some(self.intrinsics.add_f32x4)
        } else if value
            .get_type()
            .eq(&self.intrinsics.f64x2_ty.as_basic_type_enum())
        {
            Some(self.intrinsics.add_f64x2)
        } else {
            None
        };

        match intrinsic {
            Some(intrinsic) => self
                .builder
                .build_call(
                    intrinsic,
                    &[
                        value.into(),
                        value.get_type().const_zero().into(),
                        self.intrinsics.fp_rounding_md,
                        self.intrinsics.fp_exception_md,
                    ],
                    "",
                )
                .try_as_basic_value()
                .left()
                .unwrap(),
            None => value,
        }
    }

    // If this memory access must trap when out of bounds (i.e. it is a memory
    // access written in the user program as opposed to one used by our VM)
    // then mark that it can't be delete.
    fn mark_memaccess_nodelete(
        &mut self,
        memory_index: MemoryIndex,
        memaccess: InstructionValue<'ctx>,
    ) -> Result<(), CompileError> {
        if let MemoryCache::Static { base_ptr: _ } = self.ctx.memory(
            memory_index,
            self.intrinsics,
            self.module,
            self.memory_styles,
        ) {
            // The best we've got is `volatile`.
            // TODO: convert unwrap fail to CompileError
            memaccess.set_volatile(true).unwrap();
        }
        Ok(())
    }

    fn annotate_user_memaccess(
        &mut self,
        memory_index: MemoryIndex,
        _memarg: &MemoryImmediate,
        alignment: u32,
        memaccess: InstructionValue<'ctx>,
    ) -> Result<(), CompileError> {
        match memaccess.get_opcode() {
            InstructionOpcode::Load | InstructionOpcode::Store => {
                memaccess.set_alignment(alignment).unwrap();
            }
            _ => {}
        };
        self.mark_memaccess_nodelete(memory_index, memaccess)?;
        tbaa_label(
            &self.module,
            self.intrinsics,
            format!("memory {}", memory_index.as_u32()),
            memaccess,
        );
        Ok(())
    }

    fn resolve_memory_ptr(
        &mut self,
        memory_index: MemoryIndex,
        memarg: &MemoryImmediate,
        ptr_ty: PointerType<'ctx>,
        var_offset: IntValue<'ctx>,
        value_size: usize,
    ) -> Result<PointerValue<'ctx>, CompileError> {
        let builder = &self.builder;
        let intrinsics = &self.intrinsics;
        let context = &self.context;
        let function = &self.function;

        // Compute the offset into the storage.
        let imm_offset = intrinsics.i64_ty.const_int(memarg.offset as u64, false);
        let var_offset = builder.build_int_z_extend(var_offset, intrinsics.i64_ty, "");
        let offset = builder.build_int_add(var_offset, imm_offset, "");

        // Look up the memory base (as pointer) and bounds (as unsigned integer).
        let base_ptr =
            match self
                .ctx
                .memory(memory_index, intrinsics, self.module, self.memory_styles)
            {
                MemoryCache::Dynamic {
                    ptr_to_base_ptr,
                    ptr_to_current_length,
                } => {
                    // Bounds check it.
                    let minimum = self.wasm_module.memories[memory_index].minimum;
                    let value_size_v = intrinsics.i64_ty.const_int(value_size as u64, false);
                    let ptr_in_bounds = if offset.is_const() {
                        // When the offset is constant, if it's below the minimum
                        // memory size, we've statically shown that it's safe.
                        let load_offset_end = offset.const_add(value_size_v);
                        let ptr_in_bounds = load_offset_end.const_int_compare(
                            IntPredicate::ULE,
                            intrinsics.i64_ty.const_int(minimum.bytes().0 as u64, false),
                        );
                        if ptr_in_bounds.get_zero_extended_constant() == Some(1) {
                            Some(ptr_in_bounds)
                        } else {
                            None
                        }
                    } else {
                        None
                    }
                    .unwrap_or_else(|| {
                        let load_offset_end = builder.build_int_add(offset, value_size_v, "");

                        let current_length = builder
                            .build_load(ptr_to_current_length, "")
                            .into_int_value();
                        tbaa_label(
                            self.module,
                            self.intrinsics,
                            format!("memory {} length", memory_index.as_u32()),
                            current_length.as_instruction_value().unwrap(),
                        );
                        let current_length =
                            builder.build_int_z_extend(current_length, intrinsics.i64_ty, "");

                        builder.build_int_compare(
                            IntPredicate::ULE,
                            load_offset_end,
                            current_length,
                            "",
                        )
                    });
                    if !ptr_in_bounds.is_constant_int()
                        || ptr_in_bounds.get_zero_extended_constant().unwrap() != 1
                    {
                        // LLVM may have folded this into 'i1 true' in which case we know
                        // the pointer is in bounds. LLVM may also have folded it into a
                        // constant expression, not known to be either true or false yet.
                        // If it's false, unknown-but-constant, or not-a-constant, emit a
                        // runtime bounds check. LLVM may yet succeed at optimizing it away.
                        let ptr_in_bounds = builder
                            .build_call(
                                intrinsics.expect_i1,
                                &[
                                    ptr_in_bounds.into(),
                                    intrinsics.i1_ty.const_int(1, true).into(),
                                ],
                                "ptr_in_bounds_expect",
                            )
                            .try_as_basic_value()
                            .left()
                            .unwrap()
                            .into_int_value();

                        let in_bounds_continue_block =
                            context.append_basic_block(*function, "in_bounds_continue_block");
                        let not_in_bounds_block =
                            context.append_basic_block(*function, "not_in_bounds_block");
                        builder.build_conditional_branch(
                            ptr_in_bounds,
                            in_bounds_continue_block,
                            not_in_bounds_block,
                        );
                        builder.position_at_end(not_in_bounds_block);
                        builder.build_call(
                            intrinsics.throw_trap,
                            &[intrinsics.trap_memory_oob.into()],
                            "throw",
                        );
                        builder.build_unreachable();
                        builder.position_at_end(in_bounds_continue_block);
                    }
                    let ptr_to_base = builder.build_load(ptr_to_base_ptr, "").into_pointer_value();
                    tbaa_label(
                        self.module,
                        self.intrinsics,
                        format!("memory base_ptr {}", memory_index.as_u32()),
                        ptr_to_base.as_instruction_value().unwrap(),
                    );
                    ptr_to_base
                }
                MemoryCache::Static { base_ptr } => base_ptr,
            };
        let value_ptr = unsafe { builder.build_gep(base_ptr, &[offset], "") };
        Ok(builder
            .build_bitcast(value_ptr, ptr_ty, "")
            .into_pointer_value())
    }

    fn trap_if_misaligned(&self, memarg: &MemoryImmediate, ptr: PointerValue<'ctx>) {
        let align = memarg.align;
        let value = self
            .builder
            .build_ptr_to_int(ptr, self.intrinsics.i64_ty, "");
        let and = self.builder.build_and(
            value,
            self.intrinsics.i64_ty.const_int((align - 1).into(), false),
            "misaligncheck",
        );
        let aligned =
            self.builder
                .build_int_compare(IntPredicate::EQ, and, self.intrinsics.i64_zero, "");
        let aligned = self
            .builder
            .build_call(
                self.intrinsics.expect_i1,
                &[
                    aligned.into(),
                    self.intrinsics.i1_ty.const_int(1, false).into(),
                ],
                "",
            )
            .try_as_basic_value()
            .left()
            .unwrap()
            .into_int_value();

        let continue_block = self
            .context
            .append_basic_block(self.function, "aligned_access_continue_block");
        let not_aligned_block = self
            .context
            .append_basic_block(self.function, "misaligned_trap_block");
        self.builder
            .build_conditional_branch(aligned, continue_block, not_aligned_block);

        self.builder.position_at_end(not_aligned_block);
        self.builder.build_call(
            self.intrinsics.throw_trap,
            &[self.intrinsics.trap_unaligned_atomic.into()],
            "throw",
        );
        self.builder.build_unreachable();

        self.builder.position_at_end(continue_block);
    }

    fn finalize(&mut self, wasm_fn_type: &FunctionType) -> Result<(), CompileError> {
        let func_type = self.function.get_type();

        let results = self.state.popn_save_extra(wasm_fn_type.results().len())?;
        let results = results
            .into_iter()
            .map(|(v, i)| self.apply_pending_canonicalization(v, i));
        if wasm_fn_type.results().is_empty() {
            self.builder.build_return(None);
        } else if self.abi.is_sret(wasm_fn_type)? {
            let sret = self
                .function
                .get_first_param()
                .unwrap()
                .into_pointer_value();
            let mut struct_value = sret
                .get_type()
                .get_element_type()
                .into_struct_type()
                .get_undef();
            for (idx, value) in results.enumerate() {
                let value = self.builder.build_bitcast(
                    value,
                    type_to_llvm(&self.intrinsics, wasm_fn_type.results()[idx])?,
                    "",
                );
                struct_value = self
                    .builder
                    .build_insert_value(struct_value, value, idx as u32, "")
                    .unwrap()
                    .into_struct_value();
            }
            self.builder.build_store(sret, struct_value);
            self.builder.build_return(None);
        } else {
            self.builder
                .build_return(Some(&self.abi.pack_values_for_register_return(
                    &self.intrinsics,
                    &self.builder,
                    &results.collect::<Vec<_>>(),
                    &func_type,
                )?));
        }
        Ok(())
    }
}

/*
fn emit_stack_map<'ctx>(
    intrinsics: &Intrinsics<'ctx>,
    builder: &Builder<'ctx>,
    local_function_id: usize,
    target: &mut StackmapRegistry,
    kind: StackmapEntryKind,
    locals: &[PointerValue],
    state: &State<'ctx>,
    _ctx: &mut CtxType<'ctx>,
    opcode_offset: usize,
) {
    let stackmap_id = target.entries.len();

    let mut params = Vec::with_capacity(2 + locals.len() + state.stack.len());

    params.push(
        intrinsics
            .i64_ty
            .const_int(stackmap_id as u64, false)
            .as_basic_value_enum(),
    );
    params.push(intrinsics.i32_ty.const_zero().as_basic_value_enum());

    let locals: Vec<_> = locals.iter().map(|x| x.as_basic_value_enum()).collect();
    let mut value_semantics: Vec<ValueSemantic> =
        Vec::with_capacity(locals.len() + state.stack.len());

    params.extend_from_slice(&locals);
    value_semantics.extend((0..locals.len()).map(ValueSemantic::WasmLocal));

    params.extend(state.stack.iter().map(|x| x.0));
    value_semantics.extend((0..state.stack.len()).map(ValueSemantic::WasmStack));

    // FIXME: Information needed for Abstract -> Runtime state transform is not fully preserved
    // to accelerate compilation and reduce memory usage. Check this again when we try to support
    // "full" LLVM OSR.

    assert_eq!(params.len(), value_semantics.len() + 2);

    builder.build_call(intrinsics.experimental_stackmap, &params, "");

    target.entries.push(StackmapEntry {
        kind,
        local_function_id,
        local_count: locals.len(),
        stack_count: state.stack.len(),
        opcode_offset,
        value_semantics,
        is_start: true,
    });
}

fn finalize_opcode_stack_map<'ctx>(
    intrinsics: &Intrinsics<'ctx>,
    builder: &Builder<'ctx>,
    local_function_id: usize,
    target: &mut StackmapRegistry,
    kind: StackmapEntryKind,
    opcode_offset: usize,
) {
    let stackmap_id = target.entries.len();
    builder.build_call(
        intrinsics.experimental_stackmap,
        &[
            intrinsics
                .i64_ty
                .const_int(stackmap_id as u64, false)
                .as_basic_value_enum(),
            intrinsics.i32_ty.const_zero().as_basic_value_enum(),
        ],
        "opcode_stack_map_end",
    );
    target.entries.push(StackmapEntry {
        kind,
        local_function_id,
        local_count: 0,
        stack_count: 0,
        opcode_offset,
        value_semantics: vec![],
        is_start: false,
    });
}
 */

pub struct LLVMFunctionCodeGenerator<'ctx, 'a> {
    context: &'ctx Context,
    builder: Builder<'ctx>,
    alloca_builder: Builder<'ctx>,
    intrinsics: &'a Intrinsics<'ctx>,
    state: State<'ctx>,
    function: FunctionValue<'ctx>,
    locals: Vec<PointerValue<'ctx>>, // Contains params and locals
    ctx: CtxType<'ctx, 'a>,
    unreachable_depth: usize,
    memory_styles: &'a PrimaryMap<MemoryIndex, MemoryStyle>,
    _table_styles: &'a PrimaryMap<TableIndex, TableStyle>,

    // This is support for stackmaps:
    /*
    stackmaps: Rc<RefCell<StackmapRegistry>>,
    index: usize,
    opcode_offset: usize,
    track_state: bool,
    */
    module: &'a Module<'ctx>,
    module_translation: &'a ModuleTranslationState,
    wasm_module: &'a ModuleInfo,
    symbol_registry: &'a dyn SymbolRegistry,
    abi: &'a dyn Abi,
    config: &'a LLVM,
}

impl<'ctx, 'a> LLVMFunctionCodeGenerator<'ctx, 'a> {
    fn translate_operator(&mut self, op: Operator, _source_loc: u32) -> Result<(), CompileError> {
        // TODO: remove this vmctx by moving everything into CtxType. Values
        // computed off vmctx usually benefit from caching.
        let vmctx = &self.ctx.basic().into_pointer_value();

        //let opcode_offset: Option<usize> = None;

        if !self.state.reachable {
            match op {
                Operator::Block { ty: _ } | Operator::Loop { ty: _ } | Operator::If { ty: _ } => {
                    self.unreachable_depth += 1;
                    return Ok(());
                }
                Operator::Else => {
                    if self.unreachable_depth != 0 {
                        return Ok(());
                    }
                }
                Operator::End => {
                    if self.unreachable_depth != 0 {
                        self.unreachable_depth -= 1;
                        return Ok(());
                    }
                }
                _ => {
                    return Ok(());
                }
            }
        }

        match op {
            /***************************
             * Control Flow instructions.
             * https://github.com/sunfishcode/wasm-reference-manual/blob/master/WebAssembly.md#control-flow-instructions
             ***************************/
            Operator::Block { ty } => {
                let current_block = self
                    .builder
                    .get_insert_block()
                    .ok_or_else(|| CompileError::Codegen("not currently in a block".to_string()))?;

                let end_block = self.context.append_basic_block(self.function, "end");
                self.builder.position_at_end(end_block);

                let phis: SmallVec<[PhiValue<'ctx>; 1]> = self
                    .module_translation
                    .blocktype_params_results(ty)?
                    .1
                    .iter()
                    .map(|&wp_ty| {
                        wptype_to_type(wp_ty)
                            .map_err(to_compile_error)
                            .and_then(|wasm_ty| {
                                type_to_llvm(self.intrinsics, wasm_ty)
                                    .map(|ty| self.builder.build_phi(ty, ""))
                            })
                    })
                    .collect::<Result<_, _>>()?;

                self.state.push_block(end_block, phis);
                self.builder.position_at_end(current_block);
            }
            Operator::Loop { ty } => {
                let loop_body = self.context.append_basic_block(self.function, "loop_body");
                let loop_next = self.context.append_basic_block(self.function, "loop_outer");
                let pre_loop_block = self.builder.get_insert_block().unwrap();

                self.builder.build_unconditional_branch(loop_body);

                self.builder.position_at_end(loop_next);
                let blocktypes = self.module_translation.blocktype_params_results(ty)?;
                let phis = blocktypes
                    .1
                    .iter()
                    .map(|&wp_ty| {
                        wptype_to_type(wp_ty)
                            .map_err(to_compile_error)
                            .and_then(|wasm_ty| {
                                type_to_llvm(self.intrinsics, wasm_ty)
                                    .map(|ty| self.builder.build_phi(ty, ""))
                            })
                    })
                    .collect::<Result<_, _>>()?;
                self.builder.position_at_end(loop_body);
                let loop_phis: SmallVec<[PhiValue<'ctx>; 1]> = blocktypes
                    .0
                    .iter()
                    .map(|&wp_ty| {
                        wptype_to_type(wp_ty)
                            .map_err(to_compile_error)
                            .and_then(|wasm_ty| {
                                type_to_llvm(self.intrinsics, wasm_ty)
                                    .map(|ty| self.builder.build_phi(ty, ""))
                            })
                    })
                    .collect::<Result<_, _>>()?;
                for phi in loop_phis.iter().rev() {
                    let (value, info) = self.state.pop1_extra()?;
                    let value = self.apply_pending_canonicalization(value, info);
                    phi.add_incoming(&[(&value, pre_loop_block)]);
                }
                for phi in &loop_phis {
                    self.state.push1(phi.as_basic_value());
                }

                /*
                if self.track_state {
                    if let Some(offset) = opcode_offset {
                        let mut stackmaps = self.stackmaps.borrow_mut();
                        emit_stack_map(
                            self.intrinsics,
                            self.builder,
                            self.index,
                            &mut *stackmaps,
                            StackmapEntryKind::Loop,
                            &self.self.locals,
                            state,
                            ctx,
                            offset,
                        );
                        let signal_mem = ctx.signal_mem();
                        let iv = self.builder
                            .build_store(signal_mem, self.context.i8_type().const_zero());
                        // Any 'store' can be made volatile.
                        iv.set_volatile(true).unwrap();
                        finalize_opcode_stack_map(
                            self.intrinsics,
                            self.builder,
                            self.index,
                            &mut *stackmaps,
                            StackmapEntryKind::Loop,
                            offset,
                        );
                    }
                }
                */

                self.state.push_loop(loop_body, loop_next, loop_phis, phis);
            }
            Operator::Br { relative_depth } => {
                let frame = self.state.frame_at_depth(relative_depth)?;

                let current_block = self
                    .builder
                    .get_insert_block()
                    .ok_or_else(|| CompileError::Codegen("not currently in a block".to_string()))?;

                let phis = if frame.is_loop() {
                    frame.loop_body_phis()
                } else {
                    frame.phis()
                };

                let len = phis.len();
                let values = self.state.peekn_extra(len)?;
                let values = values
                    .iter()
                    .map(|(v, info)| self.apply_pending_canonicalization(*v, *info));

                // For each result of the block we're branching to,
                // pop a value off the value stack and load it into
                // the corresponding phi.
                for (phi, value) in phis.iter().zip(values) {
                    phi.add_incoming(&[(&value, current_block)]);
                }

                self.builder.build_unconditional_branch(*frame.br_dest());

                self.state.popn(len)?;
                self.state.reachable = false;
            }
            Operator::BrIf { relative_depth } => {
                let cond = self.state.pop1()?;
                let frame = self.state.frame_at_depth(relative_depth)?;

                let current_block = self
                    .builder
                    .get_insert_block()
                    .ok_or_else(|| CompileError::Codegen("not currently in a block".to_string()))?;

                let phis = if frame.is_loop() {
                    frame.loop_body_phis()
                } else {
                    frame.phis()
                };

                let param_stack = self.state.peekn_extra(phis.len())?;
                let param_stack = param_stack
                    .iter()
                    .map(|(v, info)| self.apply_pending_canonicalization(*v, *info));

                for (phi, value) in phis.iter().zip(param_stack) {
                    phi.add_incoming(&[(&value, current_block)]);
                }

                let else_block = self.context.append_basic_block(self.function, "else");

                let cond_value = self.builder.build_int_compare(
                    IntPredicate::NE,
                    cond.into_int_value(),
                    self.intrinsics.i32_zero,
                    "",
                );
                self.builder
                    .build_conditional_branch(cond_value, *frame.br_dest(), else_block);
                self.builder.position_at_end(else_block);
            }
            Operator::BrTable { ref table } => {
                let current_block = self
                    .builder
                    .get_insert_block()
                    .ok_or_else(|| CompileError::Codegen("not currently in a block".to_string()))?;

                let mut label_depths = table.targets().collect::<Result<Vec<_>, _>>()?;
                let default_depth = label_depths.pop().unwrap().0;

                let index = self.state.pop1()?;

                let default_frame = self.state.frame_at_depth(default_depth)?;

                let phis = if default_frame.is_loop() {
                    default_frame.loop_body_phis()
                } else {
                    default_frame.phis()
                };
                let args = self.state.peekn(phis.len())?;

                for (phi, value) in phis.iter().zip(args.iter()) {
                    phi.add_incoming(&[(value, current_block)]);
                }

                let cases: Vec<_> = label_depths
                    .iter()
                    .enumerate()
                    .map(|(case_index, &(depth, _))| {
                        let frame_result: Result<&ControlFrame, CompileError> =
                            self.state.frame_at_depth(depth);
                        let frame = match frame_result {
                            Ok(v) => v,
                            Err(e) => return Err(e),
                        };
                        let case_index_literal =
                            self.context.i32_type().const_int(case_index as u64, false);
                        let phis = if frame.is_loop() {
                            frame.loop_body_phis()
                        } else {
                            frame.phis()
                        };
                        for (phi, value) in phis.iter().zip(args.iter()) {
                            phi.add_incoming(&[(value, current_block)]);
                        }

                        Ok((case_index_literal, *frame.br_dest()))
                    })
                    .collect::<Result<_, _>>()?;

                self.builder.build_switch(
                    index.into_int_value(),
                    *default_frame.br_dest(),
                    &cases[..],
                );

                let args_len = args.len();
                self.state.popn(args_len)?;
                self.state.reachable = false;
            }
            Operator::If { ty } => {
                let current_block = self
                    .builder
                    .get_insert_block()
                    .ok_or_else(|| CompileError::Codegen("not currently in a block".to_string()))?;
                let if_then_block = self.context.append_basic_block(self.function, "if_then");
                let if_else_block = self.context.append_basic_block(self.function, "if_else");
                let end_block = self.context.append_basic_block(self.function, "if_end");

                let end_phis = {
                    self.builder.position_at_end(end_block);

                    let phis = self
                        .module_translation
                        .blocktype_params_results(ty)?
                        .1
                        .iter()
                        .map(|&wp_ty| {
                            wptype_to_type(wp_ty)
                                .map_err(to_compile_error)
                                .and_then(|wasm_ty| {
                                    type_to_llvm(self.intrinsics, wasm_ty)
                                        .map(|ty| self.builder.build_phi(ty, ""))
                                })
                        })
                        .collect::<Result<_, _>>()?;

                    self.builder.position_at_end(current_block);
                    phis
                };

                let cond = self.state.pop1()?;

                let cond_value = self.builder.build_int_compare(
                    IntPredicate::NE,
                    cond.into_int_value(),
                    self.intrinsics.i32_zero,
                    "",
                );

                self.builder
                    .build_conditional_branch(cond_value, if_then_block, if_else_block);
                self.builder.position_at_end(if_else_block);
                let block_param_types = self
                    .module_translation
                    .blocktype_params_results(ty)?
                    .0
                    .iter()
                    .map(|&wp_ty| {
                        wptype_to_type(wp_ty)
                            .map_err(to_compile_error)
                            .and_then(|wasm_ty| type_to_llvm(self.intrinsics, wasm_ty))
                    })
                    .collect::<Result<Vec<_>, _>>()?;
                let else_phis: SmallVec<[PhiValue<'ctx>; 1]> = block_param_types
                    .iter()
                    .map(|&ty| self.builder.build_phi(ty, ""))
                    .collect();
                self.builder.position_at_end(if_then_block);
                let then_phis: SmallVec<[PhiValue<'ctx>; 1]> = block_param_types
                    .iter()
                    .map(|&ty| self.builder.build_phi(ty, ""))
                    .collect();
                for (else_phi, then_phi) in else_phis.iter().rev().zip(then_phis.iter().rev()) {
                    let (value, info) = self.state.pop1_extra()?;
                    let value = self.apply_pending_canonicalization(value, info);
                    else_phi.add_incoming(&[(&value, current_block)]);
                    then_phi.add_incoming(&[(&value, current_block)]);
                }
                for phi in then_phis.iter() {
                    self.state.push1(phi.as_basic_value());
                }

                self.state.push_if(
                    if_then_block,
                    if_else_block,
                    end_block,
                    then_phis,
                    else_phis,
                    end_phis,
                );
            }
            Operator::Else => {
                if self.state.reachable {
                    let frame = self.state.frame_at_depth(0)?;
                    let current_block = self.builder.get_insert_block().ok_or_else(|| {
                        CompileError::Codegen("not currently in a block".to_string())
                    })?;

                    for phi in frame.phis().to_vec().iter().rev() {
                        let (value, info) = self.state.pop1_extra()?;
                        let value = self.apply_pending_canonicalization(value, info);
                        phi.add_incoming(&[(&value, current_block)])
                    }

                    let frame = self.state.frame_at_depth(0)?;
                    self.builder.build_unconditional_branch(*frame.code_after());
                }

                let (if_else_block, if_else_state) = if let ControlFrame::IfElse {
                    if_else,
                    if_else_state,
                    ..
                } = self.state.frame_at_depth_mut(0)?
                {
                    (if_else, if_else_state)
                } else {
                    unreachable!()
                };

                *if_else_state = IfElseState::Else;

                self.builder.position_at_end(*if_else_block);
                self.state.reachable = true;

                if let ControlFrame::IfElse { else_phis, .. } = self.state.frame_at_depth(0)? {
                    // Push our own 'else' phi nodes to the stack.
                    for phi in else_phis.clone().iter() {
                        self.state.push1(phi.as_basic_value());
                    }
                };
            }

            Operator::End => {
                let frame = self.state.pop_frame()?;
                let current_block = self
                    .builder
                    .get_insert_block()
                    .ok_or_else(|| CompileError::Codegen("not currently in a block".to_string()))?;

                if self.state.reachable {
                    for phi in frame.phis().iter().rev() {
                        let (value, info) = self.state.pop1_extra()?;
                        let value = self.apply_pending_canonicalization(value, info);
                        phi.add_incoming(&[(&value, current_block)]);
                    }

                    self.builder.build_unconditional_branch(*frame.code_after());
                }

                if let ControlFrame::IfElse {
                    if_else,
                    next,
                    if_else_state,
                    else_phis,
                    ..
                } = &frame
                {
                    if let IfElseState::If = if_else_state {
                        for (phi, else_phi) in frame.phis().iter().zip(else_phis.iter()) {
                            phi.add_incoming(&[(&else_phi.as_basic_value(), *if_else)]);
                        }
                        self.builder.position_at_end(*if_else);
                        self.builder.build_unconditional_branch(*next);
                    }
                }

                self.builder.position_at_end(*frame.code_after());
                self.state.reset_stack(&frame);

                self.state.reachable = true;

                // Push each phi value to the value stack.
                for phi in frame.phis() {
                    if phi.count_incoming() != 0 {
                        self.state.push1(phi.as_basic_value());
                    } else {
                        let basic_ty = phi.as_basic_value().get_type();
                        let placeholder_value = basic_ty.const_zero();
                        self.state.push1(placeholder_value);
                        phi.as_instruction().erase_from_basic_block();
                    }
                }
            }
            Operator::Return => {
                let current_block = self
                    .builder
                    .get_insert_block()
                    .ok_or_else(|| CompileError::Codegen("not currently in a block".to_string()))?;

                let frame = self.state.outermost_frame()?;
                for phi in frame.phis().to_vec().iter().rev() {
                    let (arg, info) = self.state.pop1_extra()?;
                    let arg = self.apply_pending_canonicalization(arg, info);
                    phi.add_incoming(&[(&arg, current_block)]);
                }
                let frame = self.state.outermost_frame()?;
                self.builder.build_unconditional_branch(*frame.br_dest());

                self.state.reachable = false;
            }

            Operator::Unreachable => {
                // Emit an unreachable instruction.
                // If llvm cannot prove that this is never reached,
                // it will emit a `ud2` instruction on x86_64 arches.

                // Comment out this `if` block to allow spectests to pass.
                // TODO: fix this
                /*
                if let Some(offset) = opcode_offset {
                    if self.track_state {
                        let mut stackmaps = self.stackmaps.borrow_mut();
                        emit_stack_map(
                            self.intrinsics,
                            self.builder,
                            self.index,
                            &mut *stackmaps,
                            StackmapEntryKind::Trappable,
                            &self.self.locals,
                            state,
                            ctx,
                            offset,
                        );
                        self.builder.build_call(self.intrinsics.trap, &[], "trap");
                        finalize_opcode_stack_map(
                            self.intrinsics,
                            self.builder,
                            self.index,
                            &mut *stackmaps,
                            StackmapEntryKind::Trappable,
                            offset,
                        );
                    }
                }
                */

                self.builder.build_call(
                    self.intrinsics.throw_trap,
                    &[self.intrinsics.trap_unreachable.into()],
                    "throw",
                );
                self.builder.build_unreachable();

                self.state.reachable = false;
            }

            /***************************
             * Basic instructions.
             * https://github.com/sunfishcode/wasm-reference-manual/blob/master/WebAssembly.md#basic-instructions
             ***************************/
            Operator::Nop => {
                // Do nothing.
            }
            Operator::Drop => {
                self.state.pop1()?;
            }

            // Generate const values.
            Operator::I32Const { value } => {
                let i = self.intrinsics.i32_ty.const_int(value as u64, false);
                let info = if is_f32_arithmetic(value as u32) {
                    ExtraInfo::arithmetic_f32()
                } else {
                    Default::default()
                };
                self.state.push1_extra(i, info);
            }
            Operator::I64Const { value } => {
                let i = self.intrinsics.i64_ty.const_int(value as u64, false);
                let info = if is_f64_arithmetic(value as u64) {
                    ExtraInfo::arithmetic_f64()
                } else {
                    Default::default()
                };
                self.state.push1_extra(i, info);
            }
            Operator::F32Const { value } => {
                let bits = self.intrinsics.i32_ty.const_int(value.bits() as u64, false);
                let info = if is_f32_arithmetic(value.bits()) {
                    ExtraInfo::arithmetic_f32()
                } else {
                    Default::default()
                };
                let f = self
                    .builder
                    .build_bitcast(bits, self.intrinsics.f32_ty, "f");
                self.state.push1_extra(f, info);
            }
            Operator::F64Const { value } => {
                let bits = self.intrinsics.i64_ty.const_int(value.bits(), false);
                let info = if is_f64_arithmetic(value.bits()) {
                    ExtraInfo::arithmetic_f64()
                } else {
                    Default::default()
                };
                let f = self
                    .builder
                    .build_bitcast(bits, self.intrinsics.f64_ty, "f");
                self.state.push1_extra(f, info);
            }
            Operator::V128Const { value } => {
                let mut hi: [u8; 8] = Default::default();
                let mut lo: [u8; 8] = Default::default();
                hi.copy_from_slice(&value.bytes()[0..8]);
                lo.copy_from_slice(&value.bytes()[8..16]);
                let packed = [u64::from_le_bytes(hi), u64::from_le_bytes(lo)];
                let i = self
                    .intrinsics
                    .i128_ty
                    .const_int_arbitrary_precision(&packed);
                let mut quad1: [u8; 4] = Default::default();
                let mut quad2: [u8; 4] = Default::default();
                let mut quad3: [u8; 4] = Default::default();
                let mut quad4: [u8; 4] = Default::default();
                quad1.copy_from_slice(&value.bytes()[0..4]);
                quad2.copy_from_slice(&value.bytes()[4..8]);
                quad3.copy_from_slice(&value.bytes()[8..12]);
                quad4.copy_from_slice(&value.bytes()[12..16]);
                let mut info: ExtraInfo = Default::default();
                if is_f32_arithmetic(u32::from_le_bytes(quad1))
                    && is_f32_arithmetic(u32::from_le_bytes(quad2))
                    && is_f32_arithmetic(u32::from_le_bytes(quad3))
                    && is_f32_arithmetic(u32::from_le_bytes(quad4))
                {
                    info |= ExtraInfo::arithmetic_f32();
                }
                if is_f64_arithmetic(packed[0]) && is_f64_arithmetic(packed[1]) {
                    info |= ExtraInfo::arithmetic_f64();
                }
                self.state.push1_extra(i, info);
            }

            Operator::I8x16Splat => {
                let (v, i) = self.state.pop1_extra()?;
                let v = v.into_int_value();
                let v = self
                    .builder
                    .build_int_truncate(v, self.intrinsics.i8_ty, "");
                let res = self.splat_vector(v.as_basic_value_enum(), self.intrinsics.i8x16_ty);
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1_extra(res, i);
            }
            Operator::I16x8Splat => {
                let (v, i) = self.state.pop1_extra()?;
                let v = v.into_int_value();
                let v = self
                    .builder
                    .build_int_truncate(v, self.intrinsics.i16_ty, "");
                let res = self.splat_vector(v.as_basic_value_enum(), self.intrinsics.i16x8_ty);
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1_extra(res, i);
            }
            Operator::I32x4Splat => {
                let (v, i) = self.state.pop1_extra()?;
                let res = self.splat_vector(v, self.intrinsics.i32x4_ty);
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1_extra(res, i);
            }
            Operator::I64x2Splat => {
                let (v, i) = self.state.pop1_extra()?;
                let res = self.splat_vector(v, self.intrinsics.i64x2_ty);
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1_extra(res, i);
            }
            Operator::F32x4Splat => {
                let (v, i) = self.state.pop1_extra()?;
                let res = self.splat_vector(v, self.intrinsics.f32x4_ty);
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                // The spec is unclear, we interpret splat as preserving NaN
                // payload bits.
                self.state.push1_extra(res, i);
            }
            Operator::F64x2Splat => {
                let (v, i) = self.state.pop1_extra()?;
                let res = self.splat_vector(v, self.intrinsics.f64x2_ty);
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                // The spec is unclear, we interpret splat as preserving NaN
                // payload bits.
                self.state.push1_extra(res, i);
            }

            // Operate on self.locals.
            Operator::LocalGet { local_index } => {
                let pointer_value = self.locals[local_index as usize];
                let v = self.builder.build_load(pointer_value, "");
                tbaa_label(
                    &self.module,
                    self.intrinsics,
                    format!("local {}", local_index),
                    v.as_instruction_value().unwrap(),
                );
                self.state.push1(v);
            }
            Operator::LocalSet { local_index } => {
                let pointer_value = self.locals[local_index as usize];
                let (v, i) = self.state.pop1_extra()?;
                let v = self.apply_pending_canonicalization(v, i);
                let store = self.builder.build_store(pointer_value, v);
                tbaa_label(
                    &self.module,
                    self.intrinsics,
                    format!("local {}", local_index),
                    store,
                );
            }
            Operator::LocalTee { local_index } => {
                let pointer_value = self.locals[local_index as usize];
                let (v, i) = self.state.peek1_extra()?;
                let v = self.apply_pending_canonicalization(v, i);
                let store = self.builder.build_store(pointer_value, v);
                tbaa_label(
                    &self.module,
                    self.intrinsics,
                    format!("local {}", local_index),
                    store,
                );
            }

            Operator::GlobalGet { global_index } => {
                let global_index = GlobalIndex::from_u32(global_index);
                match self
                    .ctx
                    .global(global_index, self.intrinsics, self.module)?
                {
                    GlobalCache::Const { value } => {
                        self.state.push1(*value);
                    }
                    GlobalCache::Mut { ptr_to_value } => {
                        let value = self.builder.build_load(*ptr_to_value, "");
                        tbaa_label(
                            self.module,
                            self.intrinsics,
                            format!("global {}", global_index.as_u32()),
                            value.as_instruction_value().unwrap(),
                        );
                        self.state.push1(value);
                    }
                }
            }
            Operator::GlobalSet { global_index } => {
                let global_index = GlobalIndex::from_u32(global_index);
                match self
                    .ctx
                    .global(global_index, self.intrinsics, self.module)?
                {
                    GlobalCache::Const { value: _ } => {
                        return Err(CompileError::Codegen(format!(
                            "global.set on immutable global index {}",
                            global_index.as_u32()
                        )))
                    }
                    GlobalCache::Mut { ptr_to_value } => {
                        let ptr_to_value = *ptr_to_value;
                        let (value, info) = self.state.pop1_extra()?;
                        let value = self.apply_pending_canonicalization(value, info);
                        let store = self.builder.build_store(ptr_to_value, value);
                        tbaa_label(
                            self.module,
                            self.intrinsics,
                            format!("global {}", global_index.as_u32()),
                            store,
                        );
                    }
                }
            }

            // `TypedSelect` must be used for extern refs so ref counting should
            // be done with TypedSelect. But otherwise they're the same.
            Operator::TypedSelect { .. } | Operator::Select => {
                let ((v1, i1), (v2, i2), (cond, _)) = self.state.pop3_extra()?;
                // We don't bother canonicalizing 'cond' here because we only
                // compare it to zero, and that's invariant under
                // canonicalization.

                // If the pending bits of v1 and v2 are the same, we can pass
                // them along to the result. Otherwise, apply pending
                // canonicalizations now.
                let (v1, i1, v2, i2) = if i1.has_pending_f32_nan() != i2.has_pending_f32_nan()
                    || i1.has_pending_f64_nan() != i2.has_pending_f64_nan()
                {
                    (
                        self.apply_pending_canonicalization(v1, i1),
                        i1.strip_pending(),
                        self.apply_pending_canonicalization(v2, i2),
                        i2.strip_pending(),
                    )
                } else {
                    (v1, i1, v2, i2)
                };
                let cond_value = self.builder.build_int_compare(
                    IntPredicate::NE,
                    cond.into_int_value(),
                    self.intrinsics.i32_zero,
                    "",
                );
                let res = self.builder.build_select(cond_value, v1, v2, "");
                let info = {
                    let mut info = i1.strip_pending() & i2.strip_pending();
                    if i1.has_pending_f32_nan() {
                        debug_assert!(i2.has_pending_f32_nan());
                        info |= ExtraInfo::pending_f32_nan();
                    }
                    if i1.has_pending_f64_nan() {
                        debug_assert!(i2.has_pending_f64_nan());
                        info |= ExtraInfo::pending_f64_nan();
                    }
                    info
                };
                self.state.push1_extra(res, info);
            }
            Operator::Call { function_index } => {
                let func_index = FunctionIndex::from_u32(function_index);
                let sigindex = &self.wasm_module.functions[func_index];
                let func_type = &self.wasm_module.signatures[*sigindex];

                let FunctionCache {
                    func,
                    vmctx: callee_vmctx,
                    attrs,
                } = if let Some(local_func_index) = self.wasm_module.local_func_index(func_index) {
                    let function_name = self
                        .symbol_registry
                        .symbol_to_name(Symbol::LocalFunction(local_func_index));
                    self.ctx.local_func(
                        local_func_index,
                        func_index,
                        self.intrinsics,
                        self.module,
                        self.context,
                        func_type,
                        &function_name,
                    )?
                } else {
                    self.ctx
                        .func(func_index, self.intrinsics, self.context, func_type)?
                };
                let func = *func;
                let callee_vmctx = *callee_vmctx;
                let attrs = attrs.clone();

                /*
                let func_ptr = self.llvm.functions.borrow_mut()[&func_index];

                (params, func_ptr.as_global_value().as_pointer_value())
                */
                let params = self.state.popn_save_extra(func_type.params().len())?;

                // Apply pending canonicalizations.
                let params =
                    params
                        .iter()
                        .zip(func_type.params().iter())
                        .map(|((v, info), wasm_ty)| match wasm_ty {
                            Type::F32 => self.builder.build_bitcast(
                                self.apply_pending_canonicalization(*v, *info),
                                self.intrinsics.f32_ty,
                                "",
                            ),
                            Type::F64 => self.builder.build_bitcast(
                                self.apply_pending_canonicalization(*v, *info),
                                self.intrinsics.f64_ty,
                                "",
                            ),
                            Type::V128 => self.apply_pending_canonicalization(*v, *info),
                            _ => *v,
                        });

                let params = self.abi.args_to_call(
                    &self.alloca_builder,
                    func_type,
                    callee_vmctx.into_pointer_value(),
                    &func.get_type().get_element_type().into_function_type(),
                    params.collect::<Vec<_>>().as_slice(),
                );

                /*
                if self.track_state {
                    if let Some(offset) = opcode_offset {
                        let mut stackmaps = self.stackmaps.borrow_mut();
                        emit_stack_map(
                            &info,
                            self.intrinsics,
                            self.builder,
                            self.index,
                            &mut *stackmaps,
                            StackmapEntryKind::Call,
                            &self.locals,
                            state,
                            ctx,
                            offset,
                        )
                    }
                }
                */

                let callable_func = inkwell::values::CallableValue::try_from(func).unwrap();
                let call_site = self.builder.build_call(
                    callable_func,
                    params
                        .iter()
                        .copied()
                        .map(Into::into)
                        .collect::<Vec<BasicMetadataValueEnum>>()
                        .as_slice(),
                    "",
                );
                for (attr, attr_loc) in attrs {
                    call_site.add_attribute(attr_loc, attr);
                }
                /*
                if self.track_state {
                    if let Some(offset) = opcode_offset {
                        let mut stackmaps = self.stackmaps.borrow_mut();
                        finalize_opcode_stack_map(
                            self.intrinsics,
                            self.builder,
                            self.index,
                            &mut *stackmaps,
                            StackmapEntryKind::Call,
                            offset,
                        )
                    }
                }
                */

                self.abi
                    .rets_from_call(&self.builder, &self.intrinsics, call_site, func_type)
                    .iter()
                    .for_each(|ret| self.state.push1(*ret));
            }
            Operator::CallIndirect { index, table_index } => {
                let sigindex = SignatureIndex::from_u32(index);
                let func_type = &self.wasm_module.signatures[sigindex];
                let expected_dynamic_sigindex =
                    self.ctx
                        .dynamic_sigindex(sigindex, self.intrinsics, self.module);
                let (table_base, table_bound) = self.ctx.table(
                    TableIndex::from_u32(table_index),
                    self.intrinsics,
                    self.module,
                );
                let func_index = self.state.pop1()?.into_int_value();

                let truncated_table_bounds = self.builder.build_int_truncate(
                    table_bound,
                    self.intrinsics.i32_ty,
                    "truncated_table_bounds",
                );

                // First, check if the index is outside of the table bounds.
                let index_in_bounds = self.builder.build_int_compare(
                    IntPredicate::ULT,
                    func_index,
                    truncated_table_bounds,
                    "index_in_bounds",
                );

                let index_in_bounds = self
                    .builder
                    .build_call(
                        self.intrinsics.expect_i1,
                        &[
                            index_in_bounds.into(),
                            self.intrinsics.i1_ty.const_int(1, false).into(),
                        ],
                        "index_in_bounds_expect",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap()
                    .into_int_value();

                let in_bounds_continue_block = self
                    .context
                    .append_basic_block(self.function, "in_bounds_continue_block");
                let not_in_bounds_block = self
                    .context
                    .append_basic_block(self.function, "not_in_bounds_block");
                self.builder.build_conditional_branch(
                    index_in_bounds,
                    in_bounds_continue_block,
                    not_in_bounds_block,
                );
                self.builder.position_at_end(not_in_bounds_block);
                self.builder.build_call(
                    self.intrinsics.throw_trap,
                    &[self.intrinsics.trap_table_access_oob.into()],
                    "throw",
                );
                self.builder.build_unreachable();
                self.builder.position_at_end(in_bounds_continue_block);

                // We assume the table has the `funcref` (pointer to `anyfunc`)
                // element type.
                let casted_table_base = self.builder.build_pointer_cast(
                    table_base,
                    self.intrinsics.funcref_ty.ptr_type(AddressSpace::Generic),
                    "casted_table_base",
                );

                let funcref_ptr = unsafe {
                    self.builder.build_in_bounds_gep(
                        casted_table_base,
                        &[func_index],
                        "funcref_ptr",
                    )
                };

                // a funcref (pointer to `anyfunc`)
                let anyfunc_struct_ptr = self
                    .builder
                    .build_load(funcref_ptr, "anyfunc_struct_ptr")
                    .into_pointer_value();

                // trap if we're trying to call a null funcref
                {
                    let funcref_not_null = self
                        .builder
                        .build_is_not_null(anyfunc_struct_ptr, "null funcref check");

                    let funcref_continue_deref_block = self
                        .context
                        .append_basic_block(self.function, "funcref_continue deref_block");

                    let funcref_is_null_block = self
                        .context
                        .append_basic_block(self.function, "funcref_is_null_block");
                    self.builder.build_conditional_branch(
                        funcref_not_null,
                        funcref_continue_deref_block,
                        funcref_is_null_block,
                    );
                    self.builder.position_at_end(funcref_is_null_block);
                    self.builder.build_call(
                        self.intrinsics.throw_trap,
                        &[self.intrinsics.trap_call_indirect_null.into()],
                        "throw",
                    );
                    self.builder.build_unreachable();
                    self.builder.position_at_end(funcref_continue_deref_block);
                }

                // Load things from the anyfunc data structure.
                let (func_ptr, found_dynamic_sigindex, ctx_ptr) = (
                    self.builder
                        .build_load(
                            self.builder
                                .build_struct_gep(anyfunc_struct_ptr, 0, "func_ptr_ptr")
                                .unwrap(),
                            "func_ptr",
                        )
                        .into_pointer_value(),
                    self.builder
                        .build_load(
                            self.builder
                                .build_struct_gep(anyfunc_struct_ptr, 1, "sigindex_ptr")
                                .unwrap(),
                            "sigindex",
                        )
                        .into_int_value(),
                    self.builder.build_load(
                        self.builder
                            .build_struct_gep(anyfunc_struct_ptr, 2, "ctx_ptr_ptr")
                            .unwrap(),
                        "ctx_ptr",
                    ),
                );

                // Next, check if the table element is initialized.

                // TODO: we may not need this check anymore
                let elem_initialized = self.builder.build_is_not_null(func_ptr, "");

                // Next, check if the signature id is correct.

                let sigindices_equal = self.builder.build_int_compare(
                    IntPredicate::EQ,
                    expected_dynamic_sigindex,
                    found_dynamic_sigindex,
                    "sigindices_equal",
                );

                let initialized_and_sigindices_match =
                    self.builder
                        .build_and(elem_initialized, sigindices_equal, "");

                // Tell llvm that `expected_dynamic_sigindex` should equal `found_dynamic_sigindex`.
                let initialized_and_sigindices_match = self
                    .builder
                    .build_call(
                        self.intrinsics.expect_i1,
                        &[
                            initialized_and_sigindices_match.into(),
                            self.intrinsics.i1_ty.const_int(1, false).into(),
                        ],
                        "initialized_and_sigindices_match_expect",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap()
                    .into_int_value();

                let continue_block = self
                    .context
                    .append_basic_block(self.function, "continue_block");
                let sigindices_notequal_block = self
                    .context
                    .append_basic_block(self.function, "sigindices_notequal_block");
                self.builder.build_conditional_branch(
                    initialized_and_sigindices_match,
                    continue_block,
                    sigindices_notequal_block,
                );

                self.builder.position_at_end(sigindices_notequal_block);
                let trap_code = self.builder.build_select(
                    elem_initialized,
                    self.intrinsics.trap_call_indirect_sig,
                    self.intrinsics.trap_call_indirect_null,
                    "",
                );
                self.builder
                    .build_call(self.intrinsics.throw_trap, &[trap_code.into()], "throw");
                self.builder.build_unreachable();
                self.builder.position_at_end(continue_block);

                let (llvm_func_type, llvm_func_attrs) = self.abi.func_type_to_llvm(
                    &self.context,
                    &self.intrinsics,
                    Some(self.ctx.get_offsets()),
                    func_type,
                )?;

                let params = self.state.popn_save_extra(func_type.params().len())?;

                // Apply pending canonicalizations.
                let params =
                    params
                        .iter()
                        .zip(func_type.params().iter())
                        .map(|((v, info), wasm_ty)| match wasm_ty {
                            Type::F32 => self.builder.build_bitcast(
                                self.apply_pending_canonicalization(*v, *info),
                                self.intrinsics.f32_ty,
                                "",
                            ),
                            Type::F64 => self.builder.build_bitcast(
                                self.apply_pending_canonicalization(*v, *info),
                                self.intrinsics.f64_ty,
                                "",
                            ),
                            Type::V128 => self.apply_pending_canonicalization(*v, *info),
                            _ => *v,
                        });

                let params = self.abi.args_to_call(
                    &self.alloca_builder,
                    func_type,
                    ctx_ptr.into_pointer_value(),
                    &llvm_func_type,
                    params.collect::<Vec<_>>().as_slice(),
                );

                let typed_func_ptr = self.builder.build_pointer_cast(
                    func_ptr,
                    llvm_func_type.ptr_type(AddressSpace::Generic),
                    "typed_func_ptr",
                );

                /*
                if self.track_state {
                    if let Some(offset) = opcode_offset {
                        let mut stackmaps = self.stackmaps.borrow_mut();
                        emit_stack_map(
                            &info,
                            self.intrinsics,
                            self.builder,
                            self.index,
                            &mut *stackmaps,
                            StackmapEntryKind::Call,
                            &self.locals,
                            state,
                            ctx,
                            offset,
                        )
                    }
                }
                */
                let callable_func =
                    inkwell::values::CallableValue::try_from(typed_func_ptr).unwrap();
                let call_site = self.builder.build_call(
                    callable_func,
                    params
                        .iter()
                        .copied()
                        .map(Into::into)
                        .collect::<Vec<BasicMetadataValueEnum>>()
                        .as_slice(),
                    "indirect_call",
                );
                for (attr, attr_loc) in llvm_func_attrs {
                    call_site.add_attribute(attr_loc, attr);
                }
                /*
                if self.track_state {
                    if let Some(offset) = opcode_offset {
                        let mut stackmaps = self.stackmaps.borrow_mut();
                        finalize_opcode_stack_map(
                            self.intrinsics,
                            self.builder,
                            self.index,
                            &mut *stackmaps,
                            StackmapEntryKind::Call,
                            offset,
                        )
                    }
                }
                */

                self.abi
                    .rets_from_call(&self.builder, &self.intrinsics, call_site, func_type)
                    .iter()
                    .for_each(|ret| self.state.push1(*ret));
            }

            /***************************
             * Integer Arithmetic instructions.
             * https://github.com/sunfishcode/wasm-reference-manual/blob/master/WebAssembly.md#integer-arithmetic-instructions
             ***************************/
            Operator::I32Add | Operator::I64Add => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let v1 = self.apply_pending_canonicalization(v1, i1);
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let (v1, v2) = (v1.into_int_value(), v2.into_int_value());
                let res = self.builder.build_int_add(v1, v2, "");
                self.state.push1(res);
            }
            Operator::I8x16Add => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i8x16(v1, i1);
                let (v2, _) = self.v128_into_i8x16(v2, i2);
                let res = self.builder.build_int_add(v1, v2, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I16x8Add => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i16x8(v1, i1);
                let (v2, _) = self.v128_into_i16x8(v2, i2);
                let res = self.builder.build_int_add(v1, v2, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I16x8ExtAddPairwiseI8x16S | Operator::I16x8ExtAddPairwiseI8x16U => {
                let extend_op = match op {
                    Operator::I16x8ExtAddPairwiseI8x16S => {
                        |s: &Self, v| s.builder.build_int_s_extend(v, s.intrinsics.i16x8_ty, "")
                    }
                    Operator::I16x8ExtAddPairwiseI8x16U => {
                        |s: &Self, v| s.builder.build_int_z_extend(v, s.intrinsics.i16x8_ty, "")
                    }
                    _ => unreachable!("Unhandled internal variant"),
                };
                let (v, i) = self.state.pop1_extra()?;
                let (v, _) = self.v128_into_i8x16(v, i);

                let left = self.builder.build_shuffle_vector(
                    v,
                    v.get_type().get_undef(),
                    VectorType::const_vector(&[
                        self.intrinsics.i32_consts[0],
                        self.intrinsics.i32_consts[2],
                        self.intrinsics.i32_consts[4],
                        self.intrinsics.i32_consts[6],
                        self.intrinsics.i32_consts[8],
                        self.intrinsics.i32_consts[10],
                        self.intrinsics.i32_consts[12],
                        self.intrinsics.i32_consts[14],
                    ]),
                    "",
                );
                let left = extend_op(&self, left);
                let right = self.builder.build_shuffle_vector(
                    v,
                    v.get_type().get_undef(),
                    VectorType::const_vector(&[
                        self.intrinsics.i32_consts[1],
                        self.intrinsics.i32_consts[3],
                        self.intrinsics.i32_consts[5],
                        self.intrinsics.i32_consts[7],
                        self.intrinsics.i32_consts[9],
                        self.intrinsics.i32_consts[11],
                        self.intrinsics.i32_consts[13],
                        self.intrinsics.i32_consts[15],
                    ]),
                    "",
                );
                let right = extend_op(&self, right);

                let res = self.builder.build_int_add(left, right, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32x4Add => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i32x4(v1, i1);
                let (v2, _) = self.v128_into_i32x4(v2, i2);
                let res = self.builder.build_int_add(v1, v2, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32x4ExtAddPairwiseI16x8S | Operator::I32x4ExtAddPairwiseI16x8U => {
                let extend_op = match op {
                    Operator::I32x4ExtAddPairwiseI16x8S => {
                        |s: &Self, v| s.builder.build_int_s_extend(v, s.intrinsics.i32x4_ty, "")
                    }
                    Operator::I32x4ExtAddPairwiseI16x8U => {
                        |s: &Self, v| s.builder.build_int_z_extend(v, s.intrinsics.i32x4_ty, "")
                    }
                    _ => unreachable!("Unhandled internal variant"),
                };
                let (v, i) = self.state.pop1_extra()?;
                let (v, _) = self.v128_into_i16x8(v, i);

                let left = self.builder.build_shuffle_vector(
                    v,
                    v.get_type().get_undef(),
                    VectorType::const_vector(&[
                        self.intrinsics.i32_consts[0],
                        self.intrinsics.i32_consts[2],
                        self.intrinsics.i32_consts[4],
                        self.intrinsics.i32_consts[6],
                    ]),
                    "",
                );
                let left = extend_op(&self, left);
                let right = self.builder.build_shuffle_vector(
                    v,
                    v.get_type().get_undef(),
                    VectorType::const_vector(&[
                        self.intrinsics.i32_consts[1],
                        self.intrinsics.i32_consts[3],
                        self.intrinsics.i32_consts[5],
                        self.intrinsics.i32_consts[7],
                    ]),
                    "",
                );
                let right = extend_op(&self, right);

                let res = self.builder.build_int_add(left, right, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I64x2Add => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i64x2(v1, i1);
                let (v2, _) = self.v128_into_i64x2(v2, i2);
                let res = self.builder.build_int_add(v1, v2, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I8x16AddSatS => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i8x16(v1, i1);
                let (v2, _) = self.v128_into_i8x16(v2, i2);
                let res = self
                    .builder
                    .build_call(self.intrinsics.sadd_sat_i8x16, &[v1.into(), v2.into()], "")
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I16x8AddSatS => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i16x8(v1, i1);
                let (v2, _) = self.v128_into_i16x8(v2, i2);
                let res = self
                    .builder
                    .build_call(self.intrinsics.sadd_sat_i16x8, &[v1.into(), v2.into()], "")
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I8x16AddSatU => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i8x16(v1, i1);
                let (v2, _) = self.v128_into_i8x16(v2, i2);
                let res = self
                    .builder
                    .build_call(self.intrinsics.uadd_sat_i8x16, &[v1.into(), v2.into()], "")
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I16x8AddSatU => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i16x8(v1, i1);
                let (v2, _) = self.v128_into_i16x8(v2, i2);
                let res = self
                    .builder
                    .build_call(self.intrinsics.uadd_sat_i16x8, &[v1.into(), v2.into()], "")
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32Sub | Operator::I64Sub => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let v1 = self.apply_pending_canonicalization(v1, i1);
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let (v1, v2) = (v1.into_int_value(), v2.into_int_value());
                let res = self.builder.build_int_sub(v1, v2, "");
                self.state.push1(res);
            }
            Operator::I8x16Sub => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i8x16(v1, i1);
                let (v2, _) = self.v128_into_i8x16(v2, i2);
                let res = self.builder.build_int_sub(v1, v2, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I16x8Sub => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i16x8(v1, i1);
                let (v2, _) = self.v128_into_i16x8(v2, i2);
                let res = self.builder.build_int_sub(v1, v2, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32x4Sub => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i32x4(v1, i1);
                let (v2, _) = self.v128_into_i32x4(v2, i2);
                let res = self.builder.build_int_sub(v1, v2, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I64x2Sub => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i64x2(v1, i1);
                let (v2, _) = self.v128_into_i64x2(v2, i2);
                let res = self.builder.build_int_sub(v1, v2, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I8x16SubSatS => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i8x16(v1, i1);
                let (v2, _) = self.v128_into_i8x16(v2, i2);
                let res = self
                    .builder
                    .build_call(self.intrinsics.ssub_sat_i8x16, &[v1.into(), v2.into()], "")
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I16x8SubSatS => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i16x8(v1, i1);
                let (v2, _) = self.v128_into_i16x8(v2, i2);
                let res = self
                    .builder
                    .build_call(self.intrinsics.ssub_sat_i16x8, &[v1.into(), v2.into()], "")
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I8x16SubSatU => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i8x16(v1, i1);
                let (v2, _) = self.v128_into_i8x16(v2, i2);
                let res = self
                    .builder
                    .build_call(self.intrinsics.usub_sat_i8x16, &[v1.into(), v2.into()], "")
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I16x8SubSatU => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i16x8(v1, i1);
                let (v2, _) = self.v128_into_i16x8(v2, i2);
                let res = self
                    .builder
                    .build_call(self.intrinsics.usub_sat_i16x8, &[v1.into(), v2.into()], "")
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32Mul | Operator::I64Mul => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let v1 = self.apply_pending_canonicalization(v1, i1);
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let (v1, v2) = (v1.into_int_value(), v2.into_int_value());
                let res = self.builder.build_int_mul(v1, v2, "");
                self.state.push1(res);
            }
            Operator::I16x8Mul => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i16x8(v1, i1);
                let (v2, _) = self.v128_into_i16x8(v2, i2);
                let res = self.builder.build_int_mul(v1, v2, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32x4Mul => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i32x4(v1, i1);
                let (v2, _) = self.v128_into_i32x4(v2, i2);
                let res = self.builder.build_int_mul(v1, v2, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I64x2Mul => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i64x2(v1, i1);
                let (v2, _) = self.v128_into_i64x2(v2, i2);
                let res = self.builder.build_int_mul(v1, v2, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I16x8Q15MulrSatS => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i16x8(v1, i1);
                let (v2, _) = self.v128_into_i16x8(v2, i2);

                let max_value = self
                    .intrinsics
                    .i16_ty
                    .const_int(i16::max_value() as u64, false);
                let max_values = VectorType::const_vector(&[max_value; 8]);

                let v1 = self
                    .builder
                    .build_int_s_extend(v1, self.intrinsics.i32x8_ty, "");
                let v2 = self
                    .builder
                    .build_int_s_extend(v2, self.intrinsics.i32x8_ty, "");
                let res = self.builder.build_int_mul(v1, v2, "");

                // magic number specified by the spec
                let bit = self.intrinsics.i32_ty.const_int(0x4000, false);
                let bits = VectorType::const_vector(&[bit; 8]);

                let res = self.builder.build_int_add(res, bits, "");

                let fifteen = self.intrinsics.i32_consts[15];
                let fifteens = VectorType::const_vector(&[fifteen; 8]);

                let res = self.builder.build_right_shift(res, fifteens, true, "");
                let saturate_up = {
                    let max_values =
                        self.builder
                            .build_int_s_extend(max_values, self.intrinsics.i32x8_ty, "");
                    let saturate_up =
                        self.builder
                            .build_int_compare(IntPredicate::SGT, res, max_values, "");
                    saturate_up
                };

                let res = self
                    .builder
                    .build_int_truncate(res, self.intrinsics.i16x8_ty, "");

                let res = self
                    .builder
                    .build_select(saturate_up, max_values, res, "")
                    .into_vector_value();
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I16x8ExtMulLowI8x16S
            | Operator::I16x8ExtMulLowI8x16U
            | Operator::I16x8ExtMulHighI8x16S
            | Operator::I16x8ExtMulHighI8x16U => {
                let extend_op = match op {
                    Operator::I16x8ExtMulLowI8x16S | Operator::I16x8ExtMulHighI8x16S => {
                        |s: &Self, v| s.builder.build_int_s_extend(v, s.intrinsics.i16x8_ty, "")
                    }
                    Operator::I16x8ExtMulLowI8x16U | Operator::I16x8ExtMulHighI8x16U => {
                        |s: &Self, v| s.builder.build_int_z_extend(v, s.intrinsics.i16x8_ty, "")
                    }
                    _ => unreachable!("Unhandled internal variant"),
                };
                let shuffle_array = match op {
                    Operator::I16x8ExtMulLowI8x16S | Operator::I16x8ExtMulLowI8x16U => [
                        self.intrinsics.i32_consts[0],
                        self.intrinsics.i32_consts[2],
                        self.intrinsics.i32_consts[4],
                        self.intrinsics.i32_consts[6],
                        self.intrinsics.i32_consts[8],
                        self.intrinsics.i32_consts[10],
                        self.intrinsics.i32_consts[12],
                        self.intrinsics.i32_consts[14],
                    ],
                    Operator::I16x8ExtMulHighI8x16S | Operator::I16x8ExtMulHighI8x16U => [
                        self.intrinsics.i32_consts[1],
                        self.intrinsics.i32_consts[3],
                        self.intrinsics.i32_consts[5],
                        self.intrinsics.i32_consts[7],
                        self.intrinsics.i32_consts[9],
                        self.intrinsics.i32_consts[11],
                        self.intrinsics.i32_consts[13],
                        self.intrinsics.i32_consts[15],
                    ],
                    _ => unreachable!("Unhandled internal variant"),
                };
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i8x16(v1, i1);
                let (v2, _) = self.v128_into_i8x16(v2, i2);
                let val1 = self.builder.build_shuffle_vector(
                    v1,
                    v1.get_type().get_undef(),
                    VectorType::const_vector(&shuffle_array),
                    "",
                );
                let val1 = extend_op(&self, val1);
                let val2 = self.builder.build_shuffle_vector(
                    v2,
                    v2.get_type().get_undef(),
                    VectorType::const_vector(&shuffle_array),
                    "",
                );
                let val2 = extend_op(&self, val2);
                let res = self.builder.build_int_mul(val1, val2, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32x4ExtMulLowI16x8S
            | Operator::I32x4ExtMulLowI16x8U
            | Operator::I32x4ExtMulHighI16x8S
            | Operator::I32x4ExtMulHighI16x8U => {
                let extend_op = match op {
                    Operator::I32x4ExtMulLowI16x8S | Operator::I32x4ExtMulHighI16x8S => {
                        |s: &Self, v| s.builder.build_int_s_extend(v, s.intrinsics.i32x4_ty, "")
                    }
                    Operator::I32x4ExtMulLowI16x8U | Operator::I32x4ExtMulHighI16x8U => {
                        |s: &Self, v| s.builder.build_int_z_extend(v, s.intrinsics.i32x4_ty, "")
                    }
                    _ => unreachable!("Unhandled internal variant"),
                };
                let shuffle_array = match op {
                    Operator::I32x4ExtMulLowI16x8S | Operator::I32x4ExtMulLowI16x8U => [
                        self.intrinsics.i32_consts[0],
                        self.intrinsics.i32_consts[2],
                        self.intrinsics.i32_consts[4],
                        self.intrinsics.i32_consts[6],
                    ],
                    Operator::I32x4ExtMulHighI16x8S | Operator::I32x4ExtMulHighI16x8U => [
                        self.intrinsics.i32_consts[1],
                        self.intrinsics.i32_consts[3],
                        self.intrinsics.i32_consts[5],
                        self.intrinsics.i32_consts[7],
                    ],
                    _ => unreachable!("Unhandled internal variant"),
                };
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i16x8(v1, i1);
                let (v2, _) = self.v128_into_i16x8(v2, i2);
                let val1 = self.builder.build_shuffle_vector(
                    v1,
                    v1.get_type().get_undef(),
                    VectorType::const_vector(&shuffle_array),
                    "",
                );
                let val1 = extend_op(&self, val1);
                let val2 = self.builder.build_shuffle_vector(
                    v2,
                    v2.get_type().get_undef(),
                    VectorType::const_vector(&shuffle_array),
                    "",
                );
                let val2 = extend_op(&self, val2);
                let res = self.builder.build_int_mul(val1, val2, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I64x2ExtMulLowI32x4S
            | Operator::I64x2ExtMulLowI32x4U
            | Operator::I64x2ExtMulHighI32x4S
            | Operator::I64x2ExtMulHighI32x4U => {
                let extend_op = match op {
                    Operator::I64x2ExtMulLowI32x4S | Operator::I64x2ExtMulHighI32x4S => {
                        |s: &Self, v| s.builder.build_int_s_extend(v, s.intrinsics.i64x2_ty, "")
                    }
                    Operator::I64x2ExtMulLowI32x4U | Operator::I64x2ExtMulHighI32x4U => {
                        |s: &Self, v| s.builder.build_int_z_extend(v, s.intrinsics.i64x2_ty, "")
                    }
                    _ => unreachable!("Unhandled internal variant"),
                };
                let shuffle_array = match op {
                    Operator::I64x2ExtMulLowI32x4S | Operator::I64x2ExtMulLowI32x4U => {
                        [self.intrinsics.i32_consts[0], self.intrinsics.i32_consts[2]]
                    }
                    Operator::I64x2ExtMulHighI32x4S | Operator::I64x2ExtMulHighI32x4U => {
                        [self.intrinsics.i32_consts[1], self.intrinsics.i32_consts[3]]
                    }
                    _ => unreachable!("Unhandled internal variant"),
                };
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i32x4(v1, i1);
                let (v2, _) = self.v128_into_i32x4(v2, i2);
                let val1 = self.builder.build_shuffle_vector(
                    v1,
                    v1.get_type().get_undef(),
                    VectorType::const_vector(&shuffle_array),
                    "",
                );
                let val1 = extend_op(&self, val1);
                let val2 = self.builder.build_shuffle_vector(
                    v2,
                    v2.get_type().get_undef(),
                    VectorType::const_vector(&shuffle_array),
                    "",
                );
                let val2 = extend_op(&self, val2);
                let res = self.builder.build_int_mul(val1, val2, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32x4DotI16x8S => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i16x8(v1, i1);
                let (v2, _) = self.v128_into_i16x8(v2, i2);
                let low_i16 = [
                    self.intrinsics.i32_consts[0],
                    self.intrinsics.i32_consts[2],
                    self.intrinsics.i32_consts[4],
                    self.intrinsics.i32_consts[6],
                ];
                let high_i16 = [
                    self.intrinsics.i32_consts[1],
                    self.intrinsics.i32_consts[3],
                    self.intrinsics.i32_consts[5],
                    self.intrinsics.i32_consts[7],
                ];
                let v1_low = self.builder.build_shuffle_vector(
                    v1,
                    v1.get_type().get_undef(),
                    VectorType::const_vector(&low_i16),
                    "",
                );
                let v1_low = self
                    .builder
                    .build_int_s_extend(v1_low, self.intrinsics.i32x4_ty, "");
                let v1_high = self.builder.build_shuffle_vector(
                    v1,
                    v1.get_type().get_undef(),
                    VectorType::const_vector(&high_i16),
                    "",
                );
                let v1_high =
                    self.builder
                        .build_int_s_extend(v1_high, self.intrinsics.i32x4_ty, "");
                let v2_low = self.builder.build_shuffle_vector(
                    v2,
                    v2.get_type().get_undef(),
                    VectorType::const_vector(&low_i16),
                    "",
                );
                let v2_low = self
                    .builder
                    .build_int_s_extend(v2_low, self.intrinsics.i32x4_ty, "");
                let v2_high = self.builder.build_shuffle_vector(
                    v2,
                    v2.get_type().get_undef(),
                    VectorType::const_vector(&high_i16),
                    "",
                );
                let v2_high =
                    self.builder
                        .build_int_s_extend(v2_high, self.intrinsics.i32x4_ty, "");
                let low_product = self.builder.build_int_mul(v1_low, v2_low, "");
                let high_product = self.builder.build_int_mul(v1_high, v2_high, "");

                let res = self.builder.build_int_add(low_product, high_product, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32DivS | Operator::I64DivS => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let v1 = self.apply_pending_canonicalization(v1, i1);
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let (v1, v2) = (v1.into_int_value(), v2.into_int_value());

                self.trap_if_zero_or_overflow(v1, v2);

                let res = self.builder.build_int_signed_div(v1, v2, "");
                self.state.push1(res);
            }
            Operator::I32DivU | Operator::I64DivU => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let v1 = self.apply_pending_canonicalization(v1, i1);
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let (v1, v2) = (v1.into_int_value(), v2.into_int_value());

                self.trap_if_zero(v2);

                let res = self.builder.build_int_unsigned_div(v1, v2, "");
                self.state.push1(res);
            }
            Operator::I32RemS | Operator::I64RemS => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let v1 = self.apply_pending_canonicalization(v1, i1);
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let (v1, v2) = (v1.into_int_value(), v2.into_int_value());
                let int_type = v1.get_type();
                let (min_value, neg_one_value) = if int_type == self.intrinsics.i32_ty {
                    let min_value = int_type.const_int(i32::min_value() as u64, false);
                    let neg_one_value = int_type.const_int(-1i32 as u32 as u64, false);
                    (min_value, neg_one_value)
                } else if int_type == self.intrinsics.i64_ty {
                    let min_value = int_type.const_int(i64::min_value() as u64, false);
                    let neg_one_value = int_type.const_int(-1i64 as u64, false);
                    (min_value, neg_one_value)
                } else {
                    unreachable!()
                };

                self.trap_if_zero(v2);

                // "Overflow also leads to undefined behavior; this is a rare
                // case, but can occur, for example, by taking the remainder of
                // a 32-bit division of -2147483648 by -1. (The remainder
                // doesn‚Äôt actually overflow, but this rule lets srem be
                // implemented using instructions that return both the result
                // of the division and the remainder.)"
                //   -- https://llvm.org/docs/LangRef.html#srem-instruction
                //
                // In Wasm, the i32.rem_s i32.const -2147483648 i32.const -1 is
                // i32.const 0. We implement this by swapping out the left value
                // for 0 in this case.
                let will_overflow = self.builder.build_and(
                    self.builder
                        .build_int_compare(IntPredicate::EQ, v1, min_value, "left_is_min"),
                    self.builder.build_int_compare(
                        IntPredicate::EQ,
                        v2,
                        neg_one_value,
                        "right_is_neg_one",
                    ),
                    "srem_will_overflow",
                );
                let v1 = self
                    .builder
                    .build_select(will_overflow, int_type.const_zero(), v1, "")
                    .into_int_value();
                let res = self.builder.build_int_signed_rem(v1, v2, "");
                self.state.push1(res);
            }
            Operator::I32RemU | Operator::I64RemU => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let v1 = self.apply_pending_canonicalization(v1, i1);
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let (v1, v2) = (v1.into_int_value(), v2.into_int_value());

                self.trap_if_zero(v2);

                let res = self.builder.build_int_unsigned_rem(v1, v2, "");
                self.state.push1(res);
            }
            Operator::I32And | Operator::I64And | Operator::V128And => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let v1 = self.apply_pending_canonicalization(v1, i1);
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let (v1, v2) = (v1.into_int_value(), v2.into_int_value());
                let res = self.builder.build_and(v1, v2, "");
                self.state.push1(res);
            }
            Operator::I32Or | Operator::I64Or | Operator::V128Or => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let v1 = self.apply_pending_canonicalization(v1, i1);
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let (v1, v2) = (v1.into_int_value(), v2.into_int_value());
                let res = self.builder.build_or(v1, v2, "");
                self.state.push1(res);
            }
            Operator::I32Xor | Operator::I64Xor | Operator::V128Xor => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let v1 = self.apply_pending_canonicalization(v1, i1);
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let (v1, v2) = (v1.into_int_value(), v2.into_int_value());
                let res = self.builder.build_xor(v1, v2, "");
                self.state.push1(res);
            }
            Operator::V128AndNot => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let v1 = self.apply_pending_canonicalization(v1, i1);
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let (v1, v2) = (v1.into_int_value(), v2.into_int_value());
                let v2 = self.builder.build_not(v2, "");
                let res = self.builder.build_and(v1, v2, "");
                self.state.push1(res);
            }
            Operator::V128Bitselect => {
                let ((v1, i1), (v2, i2), (cond, cond_info)) = self.state.pop3_extra()?;
                let v1 = self.apply_pending_canonicalization(v1, i1);
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let cond = self.apply_pending_canonicalization(cond, cond_info);
                let v1 = self
                    .builder
                    .build_bitcast(v1, self.intrinsics.i1x128_ty, "")
                    .into_vector_value();
                let v2 = self
                    .builder
                    .build_bitcast(v2, self.intrinsics.i1x128_ty, "")
                    .into_vector_value();
                let cond = self
                    .builder
                    .build_bitcast(cond, self.intrinsics.i1x128_ty, "")
                    .into_vector_value();
                let res = self.builder.build_select(cond, v1, v2, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I8x16Bitmask => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, _) = self.v128_into_i8x16(v, i);

                let zeros = self.intrinsics.i8x16_ty.const_zero();
                let res = self
                    .builder
                    .build_int_compare(IntPredicate::SLT, v, zeros, "");
                let res = self
                    .builder
                    .build_bitcast(res, self.intrinsics.i16_ty, "")
                    .into_int_value();
                let res = self
                    .builder
                    .build_int_z_extend(res, self.intrinsics.i32_ty, "");
                self.state.push1(res);
            }
            Operator::I16x8Bitmask => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, _) = self.v128_into_i16x8(v, i);

                let zeros = self.intrinsics.i16x8_ty.const_zero();
                let res = self
                    .builder
                    .build_int_compare(IntPredicate::SLT, v, zeros, "");
                let res = self
                    .builder
                    .build_bitcast(res, self.intrinsics.i8_ty, "")
                    .into_int_value();
                let res = self
                    .builder
                    .build_int_z_extend(res, self.intrinsics.i32_ty, "");
                self.state.push1(res);
            }
            Operator::I32x4Bitmask => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, _) = self.v128_into_i32x4(v, i);

                let zeros = self.intrinsics.i32x4_ty.const_zero();
                let res = self
                    .builder
                    .build_int_compare(IntPredicate::SLT, v, zeros, "");
                let res = self
                    .builder
                    .build_bitcast(res, self.intrinsics.i4_ty, "")
                    .into_int_value();
                let res = self
                    .builder
                    .build_int_z_extend(res, self.intrinsics.i32_ty, "");
                self.state.push1(res);
            }
            Operator::I64x2Bitmask => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, _) = self.v128_into_i64x2(v, i);

                let zeros = self.intrinsics.i64x2_ty.const_zero();
                let res = self
                    .builder
                    .build_int_compare(IntPredicate::SLT, v, zeros, "");
                let res = self
                    .builder
                    .build_bitcast(res, self.intrinsics.i2_ty, "")
                    .into_int_value();
                let res = self
                    .builder
                    .build_int_z_extend(res, self.intrinsics.i32_ty, "");
                self.state.push1(res);
            }
            Operator::I32Shl => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let v1 = self.apply_pending_canonicalization(v1, i1);
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let (v1, v2) = (v1.into_int_value(), v2.into_int_value());
                let mask = self.intrinsics.i32_ty.const_int(31u64, false);
                let v2 = self.builder.build_and(v2, mask, "");
                let res = self.builder.build_left_shift(v1, v2, "");
                self.state.push1(res);
            }
            Operator::I64Shl => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let v1 = self.apply_pending_canonicalization(v1, i1);
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let (v1, v2) = (v1.into_int_value(), v2.into_int_value());
                let mask = self.intrinsics.i64_ty.const_int(63u64, false);
                let v2 = self.builder.build_and(v2, mask, "");
                let res = self.builder.build_left_shift(v1, v2, "");
                self.state.push1(res);
            }
            Operator::I8x16Shl => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i8x16(v1, i1);
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let v2 = v2.into_int_value();
                let v2 = self
                    .builder
                    .build_and(v2, self.intrinsics.i32_consts[7], "");
                let v2 = self
                    .builder
                    .build_int_truncate(v2, self.intrinsics.i8_ty, "");
                let v2 = self.splat_vector(v2.as_basic_value_enum(), self.intrinsics.i8x16_ty);
                let res = self.builder.build_left_shift(v1, v2, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I16x8Shl => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i16x8(v1, i1);
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let v2 = v2.into_int_value();
                let v2 = self
                    .builder
                    .build_and(v2, self.intrinsics.i32_consts[15], "");
                let v2 = self
                    .builder
                    .build_int_truncate(v2, self.intrinsics.i16_ty, "");
                let v2 = self.splat_vector(v2.as_basic_value_enum(), self.intrinsics.i16x8_ty);
                let res = self.builder.build_left_shift(v1, v2, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32x4Shl => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i32x4(v1, i1);
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let v2 = v2.into_int_value();
                let v2 =
                    self.builder
                        .build_and(v2, self.intrinsics.i32_ty.const_int(31, false), "");
                let v2 = self.splat_vector(v2.as_basic_value_enum(), self.intrinsics.i32x4_ty);
                let res = self.builder.build_left_shift(v1, v2, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I64x2Shl => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i64x2(v1, i1);
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let v2 = v2.into_int_value();
                let v2 =
                    self.builder
                        .build_and(v2, self.intrinsics.i32_ty.const_int(63, false), "");
                let v2 = self
                    .builder
                    .build_int_z_extend(v2, self.intrinsics.i64_ty, "");
                let v2 = self.splat_vector(v2.as_basic_value_enum(), self.intrinsics.i64x2_ty);
                let res = self.builder.build_left_shift(v1, v2, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32ShrS => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let v1 = self.apply_pending_canonicalization(v1, i1);
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let (v1, v2) = (v1.into_int_value(), v2.into_int_value());
                let mask = self.intrinsics.i32_ty.const_int(31u64, false);
                let v2 = self.builder.build_and(v2, mask, "");
                let res = self.builder.build_right_shift(v1, v2, true, "");
                self.state.push1(res);
            }
            Operator::I64ShrS => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let v1 = self.apply_pending_canonicalization(v1, i1);
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let (v1, v2) = (v1.into_int_value(), v2.into_int_value());
                let mask = self.intrinsics.i64_ty.const_int(63u64, false);
                let v2 = self.builder.build_and(v2, mask, "");
                let res = self.builder.build_right_shift(v1, v2, true, "");
                self.state.push1(res);
            }
            Operator::I8x16ShrS => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i8x16(v1, i1);
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let v2 = v2.into_int_value();
                let v2 = self
                    .builder
                    .build_and(v2, self.intrinsics.i32_consts[7], "");
                let v2 = self
                    .builder
                    .build_int_truncate(v2, self.intrinsics.i8_ty, "");
                let v2 = self.splat_vector(v2.as_basic_value_enum(), self.intrinsics.i8x16_ty);
                let res = self.builder.build_right_shift(v1, v2, true, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I16x8ShrS => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i16x8(v1, i1);
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let v2 = v2.into_int_value();
                let v2 = self
                    .builder
                    .build_and(v2, self.intrinsics.i32_consts[15], "");
                let v2 = self
                    .builder
                    .build_int_truncate(v2, self.intrinsics.i16_ty, "");
                let v2 = self.splat_vector(v2.as_basic_value_enum(), self.intrinsics.i16x8_ty);
                let res = self.builder.build_right_shift(v1, v2, true, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32x4ShrS => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i32x4(v1, i1);
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let v2 = v2.into_int_value();
                let v2 =
                    self.builder
                        .build_and(v2, self.intrinsics.i32_ty.const_int(31, false), "");
                let v2 = self.splat_vector(v2.as_basic_value_enum(), self.intrinsics.i32x4_ty);
                let res = self.builder.build_right_shift(v1, v2, true, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I64x2ShrS => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i64x2(v1, i1);
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let v2 = v2.into_int_value();
                let v2 =
                    self.builder
                        .build_and(v2, self.intrinsics.i32_ty.const_int(63, false), "");
                let v2 = self
                    .builder
                    .build_int_z_extend(v2, self.intrinsics.i64_ty, "");
                let v2 = self.splat_vector(v2.as_basic_value_enum(), self.intrinsics.i64x2_ty);
                let res = self.builder.build_right_shift(v1, v2, true, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32ShrU => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let v1 = self.apply_pending_canonicalization(v1, i1);
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let (v1, v2) = (v1.into_int_value(), v2.into_int_value());
                let mask = self.intrinsics.i32_ty.const_int(31u64, false);
                let v2 = self.builder.build_and(v2, mask, "");
                let res = self.builder.build_right_shift(v1, v2, false, "");
                self.state.push1(res);
            }
            Operator::I64ShrU => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let v1 = self.apply_pending_canonicalization(v1, i1);
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let (v1, v2) = (v1.into_int_value(), v2.into_int_value());
                let mask = self.intrinsics.i64_ty.const_int(63u64, false);
                let v2 = self.builder.build_and(v2, mask, "");
                let res = self.builder.build_right_shift(v1, v2, false, "");
                self.state.push1(res);
            }
            Operator::I8x16ShrU => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i8x16(v1, i1);
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let v2 = v2.into_int_value();
                let v2 = self
                    .builder
                    .build_and(v2, self.intrinsics.i32_consts[7], "");
                let v2 = self
                    .builder
                    .build_int_truncate(v2, self.intrinsics.i8_ty, "");
                let v2 = self.splat_vector(v2.as_basic_value_enum(), self.intrinsics.i8x16_ty);
                let res = self.builder.build_right_shift(v1, v2, false, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I16x8ShrU => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i16x8(v1, i1);
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let v2 = v2.into_int_value();
                let v2 = self
                    .builder
                    .build_and(v2, self.intrinsics.i32_consts[15], "");
                let v2 = self
                    .builder
                    .build_int_truncate(v2, self.intrinsics.i16_ty, "");
                let v2 = self.splat_vector(v2.as_basic_value_enum(), self.intrinsics.i16x8_ty);
                let res = self.builder.build_right_shift(v1, v2, false, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32x4ShrU => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i32x4(v1, i1);
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let v2 = v2.into_int_value();
                let v2 =
                    self.builder
                        .build_and(v2, self.intrinsics.i32_ty.const_int(31, false), "");
                let v2 = self.splat_vector(v2.as_basic_value_enum(), self.intrinsics.i32x4_ty);
                let res = self.builder.build_right_shift(v1, v2, false, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I64x2ShrU => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i64x2(v1, i1);
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let v2 = v2.into_int_value();
                let v2 =
                    self.builder
                        .build_and(v2, self.intrinsics.i32_ty.const_int(63, false), "");
                let v2 = self
                    .builder
                    .build_int_z_extend(v2, self.intrinsics.i64_ty, "");
                let v2 = self.splat_vector(v2.as_basic_value_enum(), self.intrinsics.i64x2_ty);
                let res = self.builder.build_right_shift(v1, v2, false, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32Rotl => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let v1 = self.apply_pending_canonicalization(v1, i1);
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let (v1, v2) = (v1.into_int_value(), v2.into_int_value());
                let mask = self.intrinsics.i32_ty.const_int(31u64, false);
                let v2 = self.builder.build_and(v2, mask, "");
                let lhs = self.builder.build_left_shift(v1, v2, "");
                let rhs = {
                    let negv2 = self.builder.build_int_neg(v2, "");
                    let rhs = self.builder.build_and(negv2, mask, "");
                    self.builder.build_right_shift(v1, rhs, false, "")
                };
                let res = self.builder.build_or(lhs, rhs, "");
                self.state.push1(res);
            }
            Operator::I64Rotl => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let v1 = self.apply_pending_canonicalization(v1, i1);
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let (v1, v2) = (v1.into_int_value(), v2.into_int_value());
                let mask = self.intrinsics.i64_ty.const_int(63u64, false);
                let v2 = self.builder.build_and(v2, mask, "");
                let lhs = self.builder.build_left_shift(v1, v2, "");
                let rhs = {
                    let negv2 = self.builder.build_int_neg(v2, "");
                    let rhs = self.builder.build_and(negv2, mask, "");
                    self.builder.build_right_shift(v1, rhs, false, "")
                };
                let res = self.builder.build_or(lhs, rhs, "");
                self.state.push1(res);
            }
            Operator::I32Rotr => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let v1 = self.apply_pending_canonicalization(v1, i1);
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let (v1, v2) = (v1.into_int_value(), v2.into_int_value());
                let mask = self.intrinsics.i32_ty.const_int(31u64, false);
                let v2 = self.builder.build_and(v2, mask, "");
                let lhs = self.builder.build_right_shift(v1, v2, false, "");
                let rhs = {
                    let negv2 = self.builder.build_int_neg(v2, "");
                    let rhs = self.builder.build_and(negv2, mask, "");
                    self.builder.build_left_shift(v1, rhs, "")
                };
                let res = self.builder.build_or(lhs, rhs, "");
                self.state.push1(res);
            }
            Operator::I64Rotr => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let v1 = self.apply_pending_canonicalization(v1, i1);
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let (v1, v2) = (v1.into_int_value(), v2.into_int_value());
                let mask = self.intrinsics.i64_ty.const_int(63u64, false);
                let v2 = self.builder.build_and(v2, mask, "");
                let lhs = self.builder.build_right_shift(v1, v2, false, "");
                let rhs = {
                    let negv2 = self.builder.build_int_neg(v2, "");
                    let rhs = self.builder.build_and(negv2, mask, "");
                    self.builder.build_left_shift(v1, rhs, "")
                };
                let res = self.builder.build_or(lhs, rhs, "");
                self.state.push1(res);
            }
            Operator::I32Clz => {
                let (input, info) = self.state.pop1_extra()?;
                let input = self.apply_pending_canonicalization(input, info);
                let is_zero_undef = self.intrinsics.i1_zero;
                let res = self
                    .builder
                    .build_call(
                        self.intrinsics.ctlz_i32,
                        &[input.into(), is_zero_undef.into()],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                self.state.push1_extra(res, ExtraInfo::arithmetic_f32());
            }
            Operator::I64Clz => {
                let (input, info) = self.state.pop1_extra()?;
                let input = self.apply_pending_canonicalization(input, info);
                let is_zero_undef = self.intrinsics.i1_zero;
                let res = self
                    .builder
                    .build_call(
                        self.intrinsics.ctlz_i64,
                        &[input.into(), is_zero_undef.into()],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                self.state.push1_extra(res, ExtraInfo::arithmetic_f64());
            }
            Operator::I32Ctz => {
                let (input, info) = self.state.pop1_extra()?;
                let input = self.apply_pending_canonicalization(input, info);
                let is_zero_undef = self.intrinsics.i1_zero;
                let res = self
                    .builder
                    .build_call(
                        self.intrinsics.cttz_i32,
                        &[input.into(), is_zero_undef.into()],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                self.state.push1_extra(res, ExtraInfo::arithmetic_f32());
            }
            Operator::I64Ctz => {
                let (input, info) = self.state.pop1_extra()?;
                let input = self.apply_pending_canonicalization(input, info);
                let is_zero_undef = self.intrinsics.i1_zero;
                let res = self
                    .builder
                    .build_call(
                        self.intrinsics.cttz_i64,
                        &[input.into(), is_zero_undef.into()],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                self.state.push1_extra(res, ExtraInfo::arithmetic_f64());
            }
            Operator::I8x16Popcnt => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, _) = self.v128_into_i8x16(v, i);
                let res = self
                    .builder
                    .build_call(self.intrinsics.ctpop_i8x16, &[v.into()], "")
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32Popcnt => {
                let (input, info) = self.state.pop1_extra()?;
                let input = self.apply_pending_canonicalization(input, info);
                let res = self
                    .builder
                    .build_call(self.intrinsics.ctpop_i32, &[input.into()], "")
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                self.state.push1_extra(res, ExtraInfo::arithmetic_f32());
            }
            Operator::I64Popcnt => {
                let (input, info) = self.state.pop1_extra()?;
                let input = self.apply_pending_canonicalization(input, info);
                let res = self
                    .builder
                    .build_call(self.intrinsics.ctpop_i64, &[input.into()], "")
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                self.state.push1_extra(res, ExtraInfo::arithmetic_f64());
            }
            Operator::I32Eqz => {
                let input = self.state.pop1()?.into_int_value();
                let cond = self.builder.build_int_compare(
                    IntPredicate::EQ,
                    input,
                    self.intrinsics.i32_zero,
                    "",
                );
                let res = self
                    .builder
                    .build_int_z_extend(cond, self.intrinsics.i32_ty, "");
                self.state.push1_extra(res, ExtraInfo::arithmetic_f32());
            }
            Operator::I64Eqz => {
                let input = self.state.pop1()?.into_int_value();
                let cond = self.builder.build_int_compare(
                    IntPredicate::EQ,
                    input,
                    self.intrinsics.i64_zero,
                    "",
                );
                let res = self
                    .builder
                    .build_int_z_extend(cond, self.intrinsics.i32_ty, "");
                self.state.push1_extra(res, ExtraInfo::arithmetic_f64());
            }
            Operator::I8x16Abs => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, _) = self.v128_into_i8x16(v, i);

                let seven = self.intrinsics.i8_ty.const_int(7, false);
                let seven = VectorType::const_vector(&[seven; 16]);
                let all_sign_bits = self.builder.build_right_shift(v, seven, true, "");
                let xor = self.builder.build_xor(v, all_sign_bits, "");
                let res = self.builder.build_int_sub(xor, all_sign_bits, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I16x8Abs => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, _) = self.v128_into_i16x8(v, i);

                let fifteen = self.intrinsics.i16_ty.const_int(15, false);
                let fifteen = VectorType::const_vector(&[fifteen; 8]);
                let all_sign_bits = self.builder.build_right_shift(v, fifteen, true, "");
                let xor = self.builder.build_xor(v, all_sign_bits, "");
                let res = self.builder.build_int_sub(xor, all_sign_bits, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32x4Abs => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, _) = self.v128_into_i32x4(v, i);

                let thirtyone = self.intrinsics.i32_ty.const_int(31, false);
                let thirtyone = VectorType::const_vector(&[thirtyone; 4]);
                let all_sign_bits = self.builder.build_right_shift(v, thirtyone, true, "");
                let xor = self.builder.build_xor(v, all_sign_bits, "");
                let res = self.builder.build_int_sub(xor, all_sign_bits, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I64x2Abs => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, _) = self.v128_into_i64x2(v, i);

                let sixtythree = self.intrinsics.i64_ty.const_int(63, false);
                let sixtythree = VectorType::const_vector(&[sixtythree; 2]);
                let all_sign_bits = self.builder.build_right_shift(v, sixtythree, true, "");
                let xor = self.builder.build_xor(v, all_sign_bits, "");
                let res = self.builder.build_int_sub(xor, all_sign_bits, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I8x16MinS => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i8x16(v1, i1);
                let (v2, _) = self.v128_into_i8x16(v2, i2);
                let cmp = self
                    .builder
                    .build_int_compare(IntPredicate::SLT, v1, v2, "");
                let res = self.builder.build_select(cmp, v1, v2, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I8x16MinU => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i8x16(v1, i1);
                let (v2, _) = self.v128_into_i8x16(v2, i2);
                let cmp = self
                    .builder
                    .build_int_compare(IntPredicate::ULT, v1, v2, "");
                let res = self.builder.build_select(cmp, v1, v2, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I8x16MaxS => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i8x16(v1, i1);
                let (v2, _) = self.v128_into_i8x16(v2, i2);
                let cmp = self
                    .builder
                    .build_int_compare(IntPredicate::SGT, v1, v2, "");
                let res = self.builder.build_select(cmp, v1, v2, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I8x16MaxU => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i8x16(v1, i1);
                let (v2, _) = self.v128_into_i8x16(v2, i2);
                let cmp = self
                    .builder
                    .build_int_compare(IntPredicate::UGT, v1, v2, "");
                let res = self.builder.build_select(cmp, v1, v2, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I16x8MinS => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i16x8(v1, i1);
                let (v2, _) = self.v128_into_i16x8(v2, i2);
                let cmp = self
                    .builder
                    .build_int_compare(IntPredicate::SLT, v1, v2, "");
                let res = self.builder.build_select(cmp, v1, v2, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I16x8MinU => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i16x8(v1, i1);
                let (v2, _) = self.v128_into_i16x8(v2, i2);
                let cmp = self
                    .builder
                    .build_int_compare(IntPredicate::ULT, v1, v2, "");
                let res = self.builder.build_select(cmp, v1, v2, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I16x8MaxS => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i16x8(v1, i1);
                let (v2, _) = self.v128_into_i16x8(v2, i2);
                let cmp = self
                    .builder
                    .build_int_compare(IntPredicate::SGT, v1, v2, "");
                let res = self.builder.build_select(cmp, v1, v2, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I16x8MaxU => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i16x8(v1, i1);
                let (v2, _) = self.v128_into_i16x8(v2, i2);
                let cmp = self
                    .builder
                    .build_int_compare(IntPredicate::UGT, v1, v2, "");
                let res = self.builder.build_select(cmp, v1, v2, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32x4MinS => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i32x4(v1, i1);
                let (v2, _) = self.v128_into_i32x4(v2, i2);
                let cmp = self
                    .builder
                    .build_int_compare(IntPredicate::SLT, v1, v2, "");
                let res = self.builder.build_select(cmp, v1, v2, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32x4MinU => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i32x4(v1, i1);
                let (v2, _) = self.v128_into_i32x4(v2, i2);
                let cmp = self
                    .builder
                    .build_int_compare(IntPredicate::ULT, v1, v2, "");
                let res = self.builder.build_select(cmp, v1, v2, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32x4MaxS => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i32x4(v1, i1);
                let (v2, _) = self.v128_into_i32x4(v2, i2);
                let cmp = self
                    .builder
                    .build_int_compare(IntPredicate::SGT, v1, v2, "");
                let res = self.builder.build_select(cmp, v1, v2, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32x4MaxU => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i32x4(v1, i1);
                let (v2, _) = self.v128_into_i32x4(v2, i2);
                let cmp = self
                    .builder
                    .build_int_compare(IntPredicate::UGT, v1, v2, "");
                let res = self.builder.build_select(cmp, v1, v2, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I8x16RoundingAverageU => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i8x16(v1, i1);
                let (v2, _) = self.v128_into_i8x16(v2, i2);

                // This approach is faster on x86-64 when the PAVG[BW]
                // instructions are available. On other platforms, an alternative
                // implementation appears likely to outperform, described here:
                //   %a = or %v1, %v2
                //   %b = and %a, 1
                //   %v1 = lshr %v1, 1
                //   %v2 = lshr %v2, 1
                //   %sum = add %v1, %v2
                //   %res = add %sum, %b

                let ext_ty = self.intrinsics.i16_ty.vec_type(16);
                let one = self.intrinsics.i16_ty.const_int(1, false);
                let one = VectorType::const_vector(&[one; 16]);

                let v1 = self.builder.build_int_z_extend(v1, ext_ty, "");
                let v2 = self.builder.build_int_z_extend(v2, ext_ty, "");
                let res =
                    self.builder
                        .build_int_add(self.builder.build_int_add(one, v1, ""), v2, "");
                let res = self.builder.build_right_shift(res, one, false, "");
                let res = self
                    .builder
                    .build_int_truncate(res, self.intrinsics.i8x16_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I16x8RoundingAverageU => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i16x8(v1, i1);
                let (v2, _) = self.v128_into_i16x8(v2, i2);

                // This approach is faster on x86-64 when the PAVG[BW]
                // instructions are available. On other platforms, an alternative
                // implementation appears likely to outperform, described here:
                //   %a = or %v1, %v2
                //   %b = and %a, 1
                //   %v1 = lshr %v1, 1
                //   %v2 = lshr %v2, 1
                //   %sum = add %v1, %v2
                //   %res = add %sum, %b

                let ext_ty = self.intrinsics.i32_ty.vec_type(8);
                let one = self.intrinsics.i32_consts[1];
                let one = VectorType::const_vector(&[one; 8]);

                let v1 = self.builder.build_int_z_extend(v1, ext_ty, "");
                let v2 = self.builder.build_int_z_extend(v2, ext_ty, "");
                let res =
                    self.builder
                        .build_int_add(self.builder.build_int_add(one, v1, ""), v2, "");
                let res = self.builder.build_right_shift(res, one, false, "");
                let res = self
                    .builder
                    .build_int_truncate(res, self.intrinsics.i16x8_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }

            /***************************
             * Floating-Point Arithmetic instructions.
             * https://github.com/sunfishcode/wasm-reference-manual/blob/master/WebAssembly.md#floating-point-arithmetic-instructions
             ***************************/
            Operator::F32Add => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let res = self
                    .builder
                    .build_call(
                        self.intrinsics.add_f32,
                        &[
                            v1.into(),
                            v2.into(),
                            self.intrinsics.fp_rounding_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                self.state.push1_extra(
                    res,
                    (i1.strip_pending() & i2.strip_pending()) | ExtraInfo::pending_f32_nan(),
                );
            }
            Operator::F64Add => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, v2) = (v1.into_float_value(), v2.into_float_value());
                let res = self
                    .builder
                    .build_call(
                        self.intrinsics.add_f64,
                        &[
                            v1.into(),
                            v2.into(),
                            self.intrinsics.fp_rounding_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                self.state.push1_extra(
                    res,
                    (i1.strip_pending() & i2.strip_pending()) | ExtraInfo::pending_f64_nan(),
                );
            }
            Operator::F32x4Add => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, i1) = self.v128_into_f32x4(v1, i1);
                let (v2, i2) = self.v128_into_f32x4(v2, i2);
                let res = self
                    .builder
                    .build_call(
                        self.intrinsics.add_f32x4,
                        &[
                            v1.into(),
                            v2.into(),
                            self.intrinsics.fp_rounding_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1_extra(
                    res,
                    (i1.strip_pending() & i2.strip_pending()) | ExtraInfo::pending_f32_nan(),
                );
            }
            Operator::F64x2Add => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, i1) = self.v128_into_f64x2(v1, i1);
                let (v2, i2) = self.v128_into_f64x2(v2, i2);
                let res = self
                    .builder
                    .build_call(
                        self.intrinsics.add_f64x2,
                        &[
                            v1.into(),
                            v2.into(),
                            self.intrinsics.fp_rounding_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1_extra(
                    res,
                    (i1.strip_pending() & i2.strip_pending()) | ExtraInfo::pending_f64_nan(),
                );
            }
            Operator::F32Sub => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, v2) = (v1.into_float_value(), v2.into_float_value());
                let res = self
                    .builder
                    .build_call(
                        self.intrinsics.sub_f32,
                        &[
                            v1.into(),
                            v2.into(),
                            self.intrinsics.fp_rounding_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                self.state.push1_extra(
                    res,
                    (i1.strip_pending() & i2.strip_pending()) | ExtraInfo::pending_f32_nan(),
                );
            }
            Operator::F64Sub => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, v2) = (v1.into_float_value(), v2.into_float_value());
                let res = self
                    .builder
                    .build_call(
                        self.intrinsics.sub_f64,
                        &[
                            v1.into(),
                            v2.into(),
                            self.intrinsics.fp_rounding_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                self.state.push1_extra(
                    res,
                    (i1.strip_pending() & i2.strip_pending()) | ExtraInfo::pending_f64_nan(),
                );
            }
            Operator::F32x4Sub => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, i1) = self.v128_into_f32x4(v1, i1);
                let (v2, i2) = self.v128_into_f32x4(v2, i2);
                let res = self
                    .builder
                    .build_call(
                        self.intrinsics.sub_f32x4,
                        &[
                            v1.into(),
                            v2.into(),
                            self.intrinsics.fp_rounding_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1_extra(
                    res,
                    (i1.strip_pending() & i2.strip_pending()) | ExtraInfo::pending_f32_nan(),
                );
            }
            Operator::F64x2Sub => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, i1) = self.v128_into_f64x2(v1, i1);
                let (v2, i2) = self.v128_into_f64x2(v2, i2);
                let res = self
                    .builder
                    .build_call(
                        self.intrinsics.sub_f64x2,
                        &[
                            v1.into(),
                            v2.into(),
                            self.intrinsics.fp_rounding_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1_extra(
                    res,
                    (i1.strip_pending() & i2.strip_pending()) | ExtraInfo::pending_f64_nan(),
                );
            }
            Operator::F32Mul => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, v2) = (v1.into_float_value(), v2.into_float_value());
                let res = self
                    .builder
                    .build_call(
                        self.intrinsics.mul_f32,
                        &[
                            v1.into(),
                            v2.into(),
                            self.intrinsics.fp_rounding_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                self.state.push1_extra(
                    res,
                    (i1.strip_pending() & i2.strip_pending()) | ExtraInfo::pending_f32_nan(),
                );
            }
            Operator::F64Mul => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, v2) = (v1.into_float_value(), v2.into_float_value());
                let res = self
                    .builder
                    .build_call(
                        self.intrinsics.mul_f64,
                        &[
                            v1.into(),
                            v2.into(),
                            self.intrinsics.fp_rounding_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                self.state.push1_extra(
                    res,
                    (i1.strip_pending() & i2.strip_pending()) | ExtraInfo::pending_f64_nan(),
                );
            }
            Operator::F32x4Mul => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, i1) = self.v128_into_f32x4(v1, i1);
                let (v2, i2) = self.v128_into_f32x4(v2, i2);
                let res = self
                    .builder
                    .build_call(
                        self.intrinsics.mul_f32x4,
                        &[
                            v1.into(),
                            v2.into(),
                            self.intrinsics.fp_rounding_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1_extra(
                    res,
                    (i1.strip_pending() & i2.strip_pending()) | ExtraInfo::pending_f32_nan(),
                );
            }
            Operator::F64x2Mul => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, i1) = self.v128_into_f64x2(v1, i1);
                let (v2, i2) = self.v128_into_f64x2(v2, i2);
                let res = self
                    .builder
                    .build_call(
                        self.intrinsics.mul_f64x2,
                        &[
                            v1.into(),
                            v2.into(),
                            self.intrinsics.fp_rounding_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1_extra(
                    res,
                    (i1.strip_pending() & i2.strip_pending()) | ExtraInfo::pending_f64_nan(),
                );
            }
            Operator::F32Div => {
                let (v1, v2) = self.state.pop2()?;
                let (v1, v2) = (v1.into_float_value(), v2.into_float_value());
                let res = self
                    .builder
                    .build_call(
                        self.intrinsics.div_f32,
                        &[
                            v1.into(),
                            v2.into(),
                            self.intrinsics.fp_rounding_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                self.state.push1_extra(res, ExtraInfo::pending_f32_nan());
            }
            Operator::F64Div => {
                let (v1, v2) = self.state.pop2()?;
                let (v1, v2) = (v1.into_float_value(), v2.into_float_value());
                let res = self
                    .builder
                    .build_call(
                        self.intrinsics.div_f64,
                        &[
                            v1.into(),
                            v2.into(),
                            self.intrinsics.fp_rounding_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                self.state.push1_extra(res, ExtraInfo::pending_f64_nan());
            }
            Operator::F32x4Div => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_f32x4(v1, i1);
                let (v2, _) = self.v128_into_f32x4(v2, i2);
                let res = self
                    .builder
                    .build_call(
                        self.intrinsics.div_f32x4,
                        &[
                            v1.into(),
                            v2.into(),
                            self.intrinsics.fp_rounding_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1_extra(res, ExtraInfo::pending_f32_nan());
            }
            Operator::F64x2Div => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_f64x2(v1, i1);
                let (v2, _) = self.v128_into_f64x2(v2, i2);
                let res = self
                    .builder
                    .build_call(
                        self.intrinsics.div_f64x2,
                        &[
                            v1.into(),
                            v2.into(),
                            self.intrinsics.fp_rounding_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1_extra(res, ExtraInfo::pending_f64_nan());
            }
            Operator::F32Sqrt => {
                let input = self.state.pop1()?;
                let res = self
                    .builder
                    .build_call(self.intrinsics.sqrt_f32, &[input.into()], "")
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                self.state.push1_extra(res, ExtraInfo::pending_f32_nan());
            }
            Operator::F64Sqrt => {
                let input = self.state.pop1()?;
                let res = self
                    .builder
                    .build_call(self.intrinsics.sqrt_f64, &[input.into()], "")
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                self.state.push1_extra(res, ExtraInfo::pending_f64_nan());
            }
            Operator::F32x4Sqrt => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, _) = self.v128_into_f32x4(v, i);
                let res = self
                    .builder
                    .build_call(self.intrinsics.sqrt_f32x4, &[v.into()], "")
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                let bits = self
                    .builder
                    .build_bitcast(res, self.intrinsics.i128_ty, "bits");
                self.state.push1_extra(bits, ExtraInfo::pending_f32_nan());
            }
            Operator::F64x2Sqrt => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, _) = self.v128_into_f64x2(v, i);
                let res = self
                    .builder
                    .build_call(self.intrinsics.sqrt_f64x2, &[v.into()], "")
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                let bits = self
                    .builder
                    .build_bitcast(res, self.intrinsics.i128_ty, "bits");
                self.state.push1(bits);
            }
            Operator::F32Min => {
                // This implements the same logic as LLVM's @llvm.minimum
                // intrinsic would, but x86 lowering of that intrinsic
                // encounters a fatal error in LLVM 11.
                let (v1, v2) = self.state.pop2()?;
                let v1 = self.canonicalize_nans(v1);
                let v2 = self.canonicalize_nans(v2);

                let v1_is_nan = self
                    .builder
                    .build_call(
                        self.intrinsics.cmp_f32,
                        &[
                            v1.into(),
                            self.intrinsics.f32_zero.into(),
                            self.intrinsics.fp_uno_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap()
                    .into_int_value();
                let v2_is_nan = self
                    .builder
                    .build_call(
                        self.intrinsics.cmp_f32,
                        &[
                            v2.into(),
                            self.intrinsics.f32_zero.into(),
                            self.intrinsics.fp_uno_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap()
                    .into_int_value();
                let v1_lt_v2 = self
                    .builder
                    .build_call(
                        self.intrinsics.cmp_f32,
                        &[
                            v1.into(),
                            v2.into(),
                            self.intrinsics.fp_olt_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap()
                    .into_int_value();
                let v1_gt_v2 = self
                    .builder
                    .build_call(
                        self.intrinsics.cmp_f32,
                        &[
                            v1.into(),
                            v2.into(),
                            self.intrinsics.fp_ogt_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap()
                    .into_int_value();

                let res = self.builder.build_select(
                    v1_is_nan,
                    self.quiet_nan(v1),
                    self.builder.build_select(
                        v2_is_nan,
                        self.quiet_nan(v2),
                        self.builder.build_select(
                            v1_lt_v2,
                            v1,
                            self.builder.build_select(
                                v1_gt_v2,
                                v2,
                                self.builder.build_bitcast(
                                    self.builder.build_or(
                                        self.builder
                                            .build_bitcast(v1, self.intrinsics.i32_ty, "")
                                            .into_int_value(),
                                        self.builder
                                            .build_bitcast(v2, self.intrinsics.i32_ty, "")
                                            .into_int_value(),
                                        "",
                                    ),
                                    self.intrinsics.f32_ty,
                                    "",
                                ),
                                "",
                            ),
                            "",
                        ),
                        "",
                    ),
                    "",
                );

                self.state.push1(res);
            }
            Operator::F64Min => {
                // This implements the same logic as LLVM's @llvm.minimum
                // intrinsic would, but x86 lowering of that intrinsic
                // encounters a fatal error in LLVM 11.
                let (v1, v2) = self.state.pop2()?;
                let v1 = self.canonicalize_nans(v1);
                let v2 = self.canonicalize_nans(v2);

                let v1_is_nan = self
                    .builder
                    .build_call(
                        self.intrinsics.cmp_f64,
                        &[
                            v1.into(),
                            self.intrinsics.f64_zero.into(),
                            self.intrinsics.fp_uno_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap()
                    .into_int_value();
                let v2_is_nan = self
                    .builder
                    .build_call(
                        self.intrinsics.cmp_f64,
                        &[
                            v2.into(),
                            self.intrinsics.f64_zero.into(),
                            self.intrinsics.fp_uno_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap()
                    .into_int_value();
                let v1_lt_v2 = self
                    .builder
                    .build_call(
                        self.intrinsics.cmp_f64,
                        &[
                            v1.into(),
                            v2.into(),
                            self.intrinsics.fp_olt_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap()
                    .into_int_value();
                let v1_gt_v2 = self
                    .builder
                    .build_call(
                        self.intrinsics.cmp_f64,
                        &[
                            v1.into(),
                            v2.into(),
                            self.intrinsics.fp_ogt_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap()
                    .into_int_value();

                let res = self.builder.build_select(
                    v1_is_nan,
                    self.quiet_nan(v1),
                    self.builder.build_select(
                        v2_is_nan,
                        self.quiet_nan(v2),
                        self.builder.build_select(
                            v1_lt_v2,
                            v1,
                            self.builder.build_select(
                                v1_gt_v2,
                                v2,
                                self.builder.build_bitcast(
                                    self.builder.build_or(
                                        self.builder
                                            .build_bitcast(v1, self.intrinsics.i64_ty, "")
                                            .into_int_value(),
                                        self.builder
                                            .build_bitcast(v2, self.intrinsics.i64_ty, "")
                                            .into_int_value(),
                                        "",
                                    ),
                                    self.intrinsics.f64_ty,
                                    "",
                                ),
                                "",
                            ),
                            "",
                        ),
                        "",
                    ),
                    "",
                );

                self.state.push1(res);
            }
            Operator::F32x4Min => {
                // This implements the same logic as LLVM's @llvm.minimum
                // intrinsic would, but x86 lowering of that intrinsic
                // encounters a fatal error in LLVM 11.
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_f32x4(v1, i1);
                let (v2, _) = self.v128_into_f32x4(v2, i2);

                let v1_is_nan = self
                    .builder
                    .build_call(
                        self.intrinsics.cmp_f32x4,
                        &[
                            v1.into(),
                            self.intrinsics.f32x4_zero.into(),
                            self.intrinsics.fp_uno_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap()
                    .into_vector_value();
                let v2_is_nan = self
                    .builder
                    .build_call(
                        self.intrinsics.cmp_f32x4,
                        &[
                            v2.into(),
                            self.intrinsics.f32x4_zero.into(),
                            self.intrinsics.fp_uno_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap()
                    .into_vector_value();
                let v1_lt_v2 = self
                    .builder
                    .build_call(
                        self.intrinsics.cmp_f32x4,
                        &[
                            v1.into(),
                            v2.into(),
                            self.intrinsics.fp_olt_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap()
                    .into_vector_value();
                let v1_gt_v2 = self
                    .builder
                    .build_call(
                        self.intrinsics.cmp_f32x4,
                        &[
                            v1.into(),
                            v2.into(),
                            self.intrinsics.fp_ogt_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap()
                    .into_vector_value();

                let res = self.builder.build_select(
                    v1_is_nan,
                    self.quiet_nan(v1.into()).into_vector_value(),
                    self.builder
                        .build_select(
                            v2_is_nan,
                            self.quiet_nan(v2.into()).into_vector_value(),
                            self.builder
                                .build_select(
                                    v1_lt_v2,
                                    v1.into(),
                                    self.builder.build_select(
                                        v1_gt_v2,
                                        v2.into(),
                                        self.builder.build_bitcast(
                                            self.builder.build_or(
                                                self.builder
                                                    .build_bitcast(v1, self.intrinsics.i32x4_ty, "")
                                                    .into_vector_value(),
                                                self.builder
                                                    .build_bitcast(v2, self.intrinsics.i32x4_ty, "")
                                                    .into_vector_value(),
                                                "",
                                            ),
                                            self.intrinsics.f32x4_ty,
                                            "",
                                        ),
                                        "",
                                    ),
                                    "",
                                )
                                .into_vector_value(),
                            "",
                        )
                        .into_vector_value(),
                    "",
                );

                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::F32x4PMin => {
                // Pseudo-min: b < a ? b : a
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _i1) = self.v128_into_f32x4(v1, i1);
                let (v2, _i2) = self.v128_into_f32x4(v2, i2);
                let cmp = self
                    .builder
                    .build_float_compare(FloatPredicate::OLT, v2, v1, "");
                let res = self.builder.build_select(cmp, v2, v1, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::F64x2Min => {
                // This implements the same logic as LLVM's @llvm.minimum
                // intrinsic would, but x86 lowering of that intrinsic
                // encounters a fatal error in LLVM 11.
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_f64x2(v1, i1);
                let (v2, _) = self.v128_into_f64x2(v2, i2);

                let v1_is_nan = self
                    .builder
                    .build_call(
                        self.intrinsics.cmp_f64x2,
                        &[
                            v1.into(),
                            self.intrinsics.f64x2_zero.into(),
                            self.intrinsics.fp_uno_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap()
                    .into_vector_value();
                let v2_is_nan = self
                    .builder
                    .build_call(
                        self.intrinsics.cmp_f64x2,
                        &[
                            v2.into(),
                            self.intrinsics.f64x2_zero.into(),
                            self.intrinsics.fp_uno_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap()
                    .into_vector_value();
                let v1_lt_v2 = self
                    .builder
                    .build_call(
                        self.intrinsics.cmp_f64x2,
                        &[
                            v1.into(),
                            v2.into(),
                            self.intrinsics.fp_olt_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap()
                    .into_vector_value();
                let v1_gt_v2 = self
                    .builder
                    .build_call(
                        self.intrinsics.cmp_f64x2,
                        &[
                            v1.into(),
                            v2.into(),
                            self.intrinsics.fp_ogt_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap()
                    .into_vector_value();

                let res = self.builder.build_select(
                    v1_is_nan,
                    self.quiet_nan(v1.into()).into_vector_value(),
                    self.builder
                        .build_select(
                            v2_is_nan,
                            self.quiet_nan(v2.into()).into_vector_value(),
                            self.builder
                                .build_select(
                                    v1_lt_v2,
                                    v1.into(),
                                    self.builder.build_select(
                                        v1_gt_v2,
                                        v2.into(),
                                        self.builder.build_bitcast(
                                            self.builder.build_or(
                                                self.builder
                                                    .build_bitcast(v1, self.intrinsics.i64x2_ty, "")
                                                    .into_vector_value(),
                                                self.builder
                                                    .build_bitcast(v2, self.intrinsics.i64x2_ty, "")
                                                    .into_vector_value(),
                                                "",
                                            ),
                                            self.intrinsics.f64x2_ty,
                                            "",
                                        ),
                                        "",
                                    ),
                                    "",
                                )
                                .into_vector_value(),
                            "",
                        )
                        .into_vector_value(),
                    "",
                );

                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::F64x2PMin => {
                // Pseudo-min: b < a ? b : a
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _i1) = self.v128_into_f64x2(v1, i1);
                let (v2, _i2) = self.v128_into_f64x2(v2, i2);
                let cmp = self
                    .builder
                    .build_float_compare(FloatPredicate::OLT, v2, v1, "");
                let res = self.builder.build_select(cmp, v2, v1, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::F32Max => {
                // This implements the same logic as LLVM's @llvm.maximum
                // intrinsic would, but x86 lowering of that intrinsic
                // encounters a fatal error in LLVM 11.
                let (v1, v2) = self.state.pop2()?;
                let v1 = self.canonicalize_nans(v1);
                let v2 = self.canonicalize_nans(v2);

                let v1_is_nan = self
                    .builder
                    .build_call(
                        self.intrinsics.cmp_f32,
                        &[
                            v1.into(),
                            self.intrinsics.f32_zero.into(),
                            self.intrinsics.fp_uno_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap()
                    .into_int_value();
                let v2_is_nan = self
                    .builder
                    .build_call(
                        self.intrinsics.cmp_f32,
                        &[
                            v2.into(),
                            self.intrinsics.f32_zero.into(),
                            self.intrinsics.fp_uno_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap()
                    .into_int_value();
                let v1_lt_v2 = self
                    .builder
                    .build_call(
                        self.intrinsics.cmp_f32,
                        &[
                            v1.into(),
                            v2.into(),
                            self.intrinsics.fp_olt_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap()
                    .into_int_value();
                let v1_gt_v2 = self
                    .builder
                    .build_call(
                        self.intrinsics.cmp_f32,
                        &[
                            v1.into(),
                            v2.into(),
                            self.intrinsics.fp_ogt_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap()
                    .into_int_value();

                let res = self.builder.build_select(
                    v1_is_nan,
                    self.quiet_nan(v1),
                    self.builder.build_select(
                        v2_is_nan,
                        self.quiet_nan(v2),
                        self.builder.build_select(
                            v1_lt_v2,
                            v2,
                            self.builder.build_select(
                                v1_gt_v2,
                                v1,
                                self.builder.build_bitcast(
                                    self.builder.build_and(
                                        self.builder
                                            .build_bitcast(v1, self.intrinsics.i32_ty, "")
                                            .into_int_value(),
                                        self.builder
                                            .build_bitcast(v2, self.intrinsics.i32_ty, "")
                                            .into_int_value(),
                                        "",
                                    ),
                                    self.intrinsics.f32_ty,
                                    "",
                                ),
                                "",
                            ),
                            "",
                        ),
                        "",
                    ),
                    "",
                );

                self.state.push1(res);
            }
            Operator::F64Max => {
                // This implements the same logic as LLVM's @llvm.maximum
                // intrinsic would, but x86 lowering of that intrinsic
                // encounters a fatal error in LLVM 11.
                let (v1, v2) = self.state.pop2()?;
                let v1 = self.canonicalize_nans(v1);
                let v2 = self.canonicalize_nans(v2);

                let v1_is_nan = self
                    .builder
                    .build_call(
                        self.intrinsics.cmp_f64,
                        &[
                            v1.into(),
                            self.intrinsics.f64_zero.into(),
                            self.intrinsics.fp_uno_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap()
                    .into_int_value();
                let v2_is_nan = self
                    .builder
                    .build_call(
                        self.intrinsics.cmp_f64,
                        &[
                            v2.into(),
                            self.intrinsics.f64_zero.into(),
                            self.intrinsics.fp_uno_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap()
                    .into_int_value();
                let v1_lt_v2 = self
                    .builder
                    .build_call(
                        self.intrinsics.cmp_f64,
                        &[
                            v1.into(),
                            v2.into(),
                            self.intrinsics.fp_olt_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap()
                    .into_int_value();
                let v1_gt_v2 = self
                    .builder
                    .build_call(
                        self.intrinsics.cmp_f64,
                        &[
                            v1.into(),
                            v2.into(),
                            self.intrinsics.fp_ogt_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap()
                    .into_int_value();

                let res = self.builder.build_select(
                    v1_is_nan,
                    self.quiet_nan(v1),
                    self.builder.build_select(
                        v2_is_nan,
                        self.quiet_nan(v2),
                        self.builder.build_select(
                            v1_lt_v2,
                            v2,
                            self.builder.build_select(
                                v1_gt_v2,
                                v1,
                                self.builder.build_bitcast(
                                    self.builder.build_and(
                                        self.builder
                                            .build_bitcast(v1, self.intrinsics.i64_ty, "")
                                            .into_int_value(),
                                        self.builder
                                            .build_bitcast(v2, self.intrinsics.i64_ty, "")
                                            .into_int_value(),
                                        "",
                                    ),
                                    self.intrinsics.f64_ty,
                                    "",
                                ),
                                "",
                            ),
                            "",
                        ),
                        "",
                    ),
                    "",
                );

                self.state.push1(res);
            }
            Operator::F32x4Max => {
                // This implements the same logic as LLVM's @llvm.maximum
                // intrinsic would, but x86 lowering of that intrinsic
                // encounters a fatal error in LLVM 11.
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_f32x4(v1, i1);
                let (v2, _) = self.v128_into_f32x4(v2, i2);

                let v1_is_nan = self
                    .builder
                    .build_call(
                        self.intrinsics.cmp_f32x4,
                        &[
                            v1.into(),
                            self.intrinsics.f32x4_zero.into(),
                            self.intrinsics.fp_uno_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap()
                    .into_vector_value();
                let v2_is_nan = self
                    .builder
                    .build_call(
                        self.intrinsics.cmp_f32x4,
                        &[
                            v2.into(),
                            self.intrinsics.f32x4_zero.into(),
                            self.intrinsics.fp_uno_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap()
                    .into_vector_value();
                let v1_lt_v2 = self
                    .builder
                    .build_call(
                        self.intrinsics.cmp_f32x4,
                        &[
                            v1.into(),
                            v2.into(),
                            self.intrinsics.fp_olt_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap()
                    .into_vector_value();
                let v1_gt_v2 = self
                    .builder
                    .build_call(
                        self.intrinsics.cmp_f32x4,
                        &[
                            v1.into(),
                            v2.into(),
                            self.intrinsics.fp_ogt_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap()
                    .into_vector_value();

                let res = self.builder.build_select(
                    v1_is_nan,
                    self.quiet_nan(v1.into()).into_vector_value(),
                    self.builder
                        .build_select(
                            v2_is_nan,
                            self.quiet_nan(v2.into()).into_vector_value(),
                            self.builder
                                .build_select(
                                    v1_lt_v2,
                                    v2.into(),
                                    self.builder.build_select(
                                        v1_gt_v2,
                                        v1.into(),
                                        self.builder.build_bitcast(
                                            self.builder.build_and(
                                                self.builder
                                                    .build_bitcast(v1, self.intrinsics.i32x4_ty, "")
                                                    .into_vector_value(),
                                                self.builder
                                                    .build_bitcast(v2, self.intrinsics.i32x4_ty, "")
                                                    .into_vector_value(),
                                                "",
                                            ),
                                            self.intrinsics.f32x4_ty,
                                            "",
                                        ),
                                        "",
                                    ),
                                    "",
                                )
                                .into_vector_value(),
                            "",
                        )
                        .into_vector_value(),
                    "",
                );

                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::F32x4PMax => {
                // Pseudo-max: a < b ? b : a
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _i1) = self.v128_into_f32x4(v1, i1);
                let (v2, _i2) = self.v128_into_f32x4(v2, i2);
                let cmp = self
                    .builder
                    .build_float_compare(FloatPredicate::OLT, v1, v2, "");
                let res = self.builder.build_select(cmp, v2, v1, "");

                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::F64x2Max => {
                // This implements the same logic as LLVM's @llvm.maximum
                // intrinsic would, but x86 lowering of that intrinsic
                // encounters a fatal error in LLVM 11.
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_f64x2(v1, i1);
                let (v2, _) = self.v128_into_f64x2(v2, i2);

                let v1_is_nan = self
                    .builder
                    .build_call(
                        self.intrinsics.cmp_f64x2,
                        &[
                            v1.into(),
                            self.intrinsics.f64x2_zero.into(),
                            self.intrinsics.fp_uno_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap()
                    .into_vector_value();
                let v2_is_nan = self
                    .builder
                    .build_call(
                        self.intrinsics.cmp_f64x2,
                        &[
                            v2.into(),
                            self.intrinsics.f64x2_zero.into(),
                            self.intrinsics.fp_uno_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap()
                    .into_vector_value();
                let v1_lt_v2 = self
                    .builder
                    .build_call(
                        self.intrinsics.cmp_f64x2,
                        &[
                            v1.into(),
                            v2.into(),
                            self.intrinsics.fp_olt_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap()
                    .into_vector_value();
                let v1_gt_v2 = self
                    .builder
                    .build_call(
                        self.intrinsics.cmp_f64x2,
                        &[
                            v1.into(),
                            v2.into(),
                            self.intrinsics.fp_ogt_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap()
                    .into_vector_value();

                let res = self.builder.build_select(
                    v1_is_nan,
                    self.quiet_nan(v1.into()).into_vector_value(),
                    self.builder
                        .build_select(
                            v2_is_nan,
                            self.quiet_nan(v2.into()).into_vector_value(),
                            self.builder
                                .build_select(
                                    v1_lt_v2,
                                    v2.into(),
                                    self.builder.build_select(
                                        v1_gt_v2,
                                        v1.into(),
                                        self.builder.build_bitcast(
                                            self.builder.build_and(
                                                self.builder
                                                    .build_bitcast(v1, self.intrinsics.i64x2_ty, "")
                                                    .into_vector_value(),
                                                self.builder
                                                    .build_bitcast(v2, self.intrinsics.i64x2_ty, "")
                                                    .into_vector_value(),
                                                "",
                                            ),
                                            self.intrinsics.f64x2_ty,
                                            "",
                                        ),
                                        "",
                                    ),
                                    "",
                                )
                                .into_vector_value(),
                            "",
                        )
                        .into_vector_value(),
                    "",
                );

                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::F64x2PMax => {
                // Pseudo-max: a < b ? b : a
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _i1) = self.v128_into_f64x2(v1, i1);
                let (v2, _i2) = self.v128_into_f64x2(v2, i2);
                let cmp = self
                    .builder
                    .build_float_compare(FloatPredicate::OLT, v1, v2, "");
                let res = self.builder.build_select(cmp, v2, v1, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::F32Ceil => {
                let (input, info) = self.state.pop1_extra()?;
                let res = self
                    .builder
                    .build_call(self.intrinsics.ceil_f32, &[input.into()], "")
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                self.state
                    .push1_extra(res, info | ExtraInfo::pending_f32_nan());
            }
            Operator::F32x4Ceil => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, _) = self.v128_into_f32x4(v, i);
                let res = self
                    .builder
                    .build_call(self.intrinsics.ceil_f32x4, &[v.into()], "")
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state
                    .push1_extra(res, i | ExtraInfo::pending_f32_nan());
            }
            Operator::F64Ceil => {
                let (input, info) = self.state.pop1_extra()?;
                let res = self
                    .builder
                    .build_call(self.intrinsics.ceil_f64, &[input.into()], "")
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                self.state
                    .push1_extra(res, info | ExtraInfo::pending_f64_nan());
            }
            Operator::F64x2Ceil => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, _) = self.v128_into_f64x2(v, i);
                let res = self
                    .builder
                    .build_call(self.intrinsics.ceil_f64x2, &[v.into()], "")
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state
                    .push1_extra(res, i | ExtraInfo::pending_f64_nan());
            }
            Operator::F32Floor => {
                let (input, info) = self.state.pop1_extra()?;
                let res = self
                    .builder
                    .build_call(self.intrinsics.floor_f32, &[input.into()], "")
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                self.state
                    .push1_extra(res, info | ExtraInfo::pending_f32_nan());
            }
            Operator::F32x4Floor => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, _) = self.v128_into_f32x4(v, i);
                let res = self
                    .builder
                    .build_call(self.intrinsics.floor_f32x4, &[v.into()], "")
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state
                    .push1_extra(res, i | ExtraInfo::pending_f32_nan());
            }
            Operator::F64Floor => {
                let (input, info) = self.state.pop1_extra()?;
                let res = self
                    .builder
                    .build_call(self.intrinsics.floor_f64, &[input.into()], "")
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                self.state
                    .push1_extra(res, info | ExtraInfo::pending_f64_nan());
            }
            Operator::F64x2Floor => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, _) = self.v128_into_f64x2(v, i);
                let res = self
                    .builder
                    .build_call(self.intrinsics.floor_f64x2, &[v.into()], "")
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state
                    .push1_extra(res, i | ExtraInfo::pending_f64_nan());
            }
            Operator::F32Trunc => {
                let (v, i) = self.state.pop1_extra()?;
                let res = self
                    .builder
                    .build_call(self.intrinsics.trunc_f32, &[v.into()], "")
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                self.state
                    .push1_extra(res, i | ExtraInfo::pending_f32_nan());
            }
            Operator::F32x4Trunc => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, _) = self.v128_into_f32x4(v, i);
                let res = self
                    .builder
                    .build_call(self.intrinsics.trunc_f32x4, &[v.into()], "")
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state
                    .push1_extra(res, i | ExtraInfo::pending_f32_nan());
            }
            Operator::F64Trunc => {
                let (v, i) = self.state.pop1_extra()?;
                let res = self
                    .builder
                    .build_call(self.intrinsics.trunc_f64, &[v.into()], "")
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                self.state
                    .push1_extra(res, i | ExtraInfo::pending_f64_nan());
            }
            Operator::F64x2Trunc => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, _) = self.v128_into_f64x2(v, i);
                let res = self
                    .builder
                    .build_call(self.intrinsics.trunc_f64x2, &[v.into()], "")
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state
                    .push1_extra(res, i | ExtraInfo::pending_f64_nan());
            }
            Operator::F32Nearest => {
                let (v, i) = self.state.pop1_extra()?;
                let res = self
                    .builder
                    .build_call(self.intrinsics.nearbyint_f32, &[v.into()], "")
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                self.state
                    .push1_extra(res, i | ExtraInfo::pending_f32_nan());
            }
            Operator::F32x4Nearest => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, _) = self.v128_into_f32x4(v, i);
                let res = self
                    .builder
                    .build_call(self.intrinsics.nearbyint_f32x4, &[v.into()], "")
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state
                    .push1_extra(res, i | ExtraInfo::pending_f32_nan());
            }
            Operator::F64Nearest => {
                let (v, i) = self.state.pop1_extra()?;
                let res = self
                    .builder
                    .build_call(self.intrinsics.nearbyint_f64, &[v.into()], "")
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                self.state
                    .push1_extra(res, i | ExtraInfo::pending_f64_nan());
            }
            Operator::F64x2Nearest => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, _) = self.v128_into_f64x2(v, i);
                let res = self
                    .builder
                    .build_call(self.intrinsics.nearbyint_f64x2, &[v.into()], "")
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state
                    .push1_extra(res, i | ExtraInfo::pending_f64_nan());
            }
            Operator::F32Abs => {
                let (v, i) = self.state.pop1_extra()?;
                let v = self.apply_pending_canonicalization(v, i);
                let res = self
                    .builder
                    .build_call(self.intrinsics.fabs_f32, &[v.into()], "")
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                // The exact NaN returned by F32Abs is fully defined. Do not
                // adjust.
                self.state.push1_extra(res, i.strip_pending());
            }
            Operator::F64Abs => {
                let (v, i) = self.state.pop1_extra()?;
                let v = self.apply_pending_canonicalization(v, i);
                let res = self
                    .builder
                    .build_call(self.intrinsics.fabs_f64, &[v.into()], "")
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                // The exact NaN returned by F64Abs is fully defined. Do not
                // adjust.
                self.state.push1_extra(res, i.strip_pending());
            }
            Operator::F32x4Abs => {
                let (v, i) = self.state.pop1_extra()?;
                let v =
                    self.builder
                        .build_bitcast(v.into_int_value(), self.intrinsics.f32x4_ty, "");
                let v = self.apply_pending_canonicalization(v, i);
                let res = self
                    .builder
                    .build_call(self.intrinsics.fabs_f32x4, &[v.into()], "")
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                // The exact NaN returned by F32x4Abs is fully defined. Do not
                // adjust.
                self.state.push1_extra(res, i.strip_pending());
            }
            Operator::F64x2Abs => {
                let (v, i) = self.state.pop1_extra()?;
                let v =
                    self.builder
                        .build_bitcast(v.into_int_value(), self.intrinsics.f64x2_ty, "");
                let v = self.apply_pending_canonicalization(v, i);
                let res = self
                    .builder
                    .build_call(self.intrinsics.fabs_f64x2, &[v.into()], "")
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                // The exact NaN returned by F32x4Abs is fully defined. Do not
                // adjust.
                self.state.push1_extra(res, i.strip_pending());
            }
            Operator::F32x4Neg => {
                let (v, i) = self.state.pop1_extra()?;
                let v =
                    self.builder
                        .build_bitcast(v.into_int_value(), self.intrinsics.f32x4_ty, "");
                let v = self
                    .apply_pending_canonicalization(v, i)
                    .into_vector_value();
                let res = self.builder.build_float_neg(v, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                // The exact NaN returned by F32x4Neg is fully defined. Do not
                // adjust.
                self.state.push1_extra(res, i.strip_pending());
            }
            Operator::F64x2Neg => {
                let (v, i) = self.state.pop1_extra()?;
                let v =
                    self.builder
                        .build_bitcast(v.into_int_value(), self.intrinsics.f64x2_ty, "");
                let v = self
                    .apply_pending_canonicalization(v, i)
                    .into_vector_value();
                let res = self.builder.build_float_neg(v, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                // The exact NaN returned by F64x2Neg is fully defined. Do not
                // adjust.
                self.state.push1_extra(res, i.strip_pending());
            }
            Operator::F32Neg | Operator::F64Neg => {
                let (v, i) = self.state.pop1_extra()?;
                let v = self.apply_pending_canonicalization(v, i).into_float_value();
                let res = self.builder.build_float_neg(v, "");
                // The exact NaN returned by F32Neg and F64Neg are fully defined.
                // Do not adjust.
                self.state.push1_extra(res, i.strip_pending());
            }
            Operator::F32Copysign => {
                let ((mag, mag_info), (sgn, sgn_info)) = self.state.pop2_extra()?;
                let mag = self.apply_pending_canonicalization(mag, mag_info);
                let sgn = self.apply_pending_canonicalization(sgn, sgn_info);
                let res = self
                    .builder
                    .build_call(self.intrinsics.copysign_f32, &[mag.into(), sgn.into()], "")
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                // The exact NaN returned by F32Copysign is fully defined.
                // Do not adjust.
                self.state.push1_extra(res, mag_info.strip_pending());
            }
            Operator::F64Copysign => {
                let ((mag, mag_info), (sgn, sgn_info)) = self.state.pop2_extra()?;
                let mag = self.apply_pending_canonicalization(mag, mag_info);
                let sgn = self.apply_pending_canonicalization(sgn, sgn_info);
                let res = self
                    .builder
                    .build_call(self.intrinsics.copysign_f64, &[mag.into(), sgn.into()], "")
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                // The exact NaN returned by F32Copysign is fully defined.
                // Do not adjust.
                self.state.push1_extra(res, mag_info.strip_pending());
            }

            /***************************
             * Integer Comparison instructions.
             * https://github.com/sunfishcode/wasm-reference-manual/blob/master/WebAssembly.md#integer-comparison-instructions
             ***************************/
            Operator::I32Eq | Operator::I64Eq => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let v1 = self.apply_pending_canonicalization(v1, i1);
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let (v1, v2) = (v1.into_int_value(), v2.into_int_value());
                let cond = self.builder.build_int_compare(IntPredicate::EQ, v1, v2, "");
                let res = self
                    .builder
                    .build_int_z_extend(cond, self.intrinsics.i32_ty, "");
                self.state.push1_extra(
                    res,
                    ExtraInfo::arithmetic_f32() | ExtraInfo::arithmetic_f64(),
                );
            }
            Operator::I8x16Eq => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i8x16(v1, i1);
                let (v2, _) = self.v128_into_i8x16(v2, i2);
                let res = self.builder.build_int_compare(IntPredicate::EQ, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i8x16_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I16x8Eq => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i16x8(v1, i1);
                let (v2, _) = self.v128_into_i16x8(v2, i2);
                let res = self.builder.build_int_compare(IntPredicate::EQ, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i16x8_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32x4Eq => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i32x4(v1, i1);
                let (v2, _) = self.v128_into_i32x4(v2, i2);
                let res = self.builder.build_int_compare(IntPredicate::EQ, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i32x4_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I64x2Eq => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i64x2(v1, i1);
                let (v2, _) = self.v128_into_i64x2(v2, i2);
                let res = self.builder.build_int_compare(IntPredicate::EQ, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i64x2_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32Ne | Operator::I64Ne => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let v1 = self.apply_pending_canonicalization(v1, i1);
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let (v1, v2) = (v1.into_int_value(), v2.into_int_value());
                let cond = self.builder.build_int_compare(IntPredicate::NE, v1, v2, "");
                let res = self
                    .builder
                    .build_int_z_extend(cond, self.intrinsics.i32_ty, "");
                self.state.push1_extra(
                    res,
                    ExtraInfo::arithmetic_f32() | ExtraInfo::arithmetic_f64(),
                );
            }
            Operator::I8x16Ne => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i8x16(v1, i1);
                let (v2, _) = self.v128_into_i8x16(v2, i2);
                let res = self.builder.build_int_compare(IntPredicate::NE, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i8x16_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I16x8Ne => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i16x8(v1, i1);
                let (v2, _) = self.v128_into_i16x8(v2, i2);
                let res = self.builder.build_int_compare(IntPredicate::NE, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i16x8_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32x4Ne => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i32x4(v1, i1);
                let (v2, _) = self.v128_into_i32x4(v2, i2);
                let res = self.builder.build_int_compare(IntPredicate::NE, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i32x4_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I64x2Ne => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i64x2(v1, i1);
                let (v2, _) = self.v128_into_i64x2(v2, i2);
                let res = self.builder.build_int_compare(IntPredicate::NE, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i64x2_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32LtS | Operator::I64LtS => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let v1 = self.apply_pending_canonicalization(v1, i1);
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let (v1, v2) = (v1.into_int_value(), v2.into_int_value());
                let cond = self
                    .builder
                    .build_int_compare(IntPredicate::SLT, v1, v2, "");
                let res = self
                    .builder
                    .build_int_z_extend(cond, self.intrinsics.i32_ty, "");
                self.state.push1_extra(
                    res,
                    ExtraInfo::arithmetic_f32() | ExtraInfo::arithmetic_f64(),
                );
            }
            Operator::I8x16LtS => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i8x16(v1, i1);
                let (v2, _) = self.v128_into_i8x16(v2, i2);
                let res = self
                    .builder
                    .build_int_compare(IntPredicate::SLT, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i8x16_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I16x8LtS => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i16x8(v1, i1);
                let (v2, _) = self.v128_into_i16x8(v2, i2);
                let res = self
                    .builder
                    .build_int_compare(IntPredicate::SLT, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i16x8_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32x4LtS => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i32x4(v1, i1);
                let (v2, _) = self.v128_into_i32x4(v2, i2);
                let res = self
                    .builder
                    .build_int_compare(IntPredicate::SLT, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i32x4_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I64x2LtS => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i64x2(v1, i1);
                let (v2, _) = self.v128_into_i64x2(v2, i2);
                let res = self
                    .builder
                    .build_int_compare(IntPredicate::SLT, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i64x2_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32LtU | Operator::I64LtU => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let v1 = self.apply_pending_canonicalization(v1, i1);
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let (v1, v2) = (v1.into_int_value(), v2.into_int_value());
                let cond = self
                    .builder
                    .build_int_compare(IntPredicate::ULT, v1, v2, "");
                let res = self
                    .builder
                    .build_int_z_extend(cond, self.intrinsics.i32_ty, "");
                self.state.push1(res);
            }
            Operator::I8x16LtU => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i8x16(v1, i1);
                let (v2, _) = self.v128_into_i8x16(v2, i2);
                let res = self
                    .builder
                    .build_int_compare(IntPredicate::ULT, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i8x16_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I16x8LtU => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i16x8(v1, i1);
                let (v2, _) = self.v128_into_i16x8(v2, i2);
                let res = self
                    .builder
                    .build_int_compare(IntPredicate::ULT, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i16x8_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32x4LtU => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i32x4(v1, i1);
                let (v2, _) = self.v128_into_i32x4(v2, i2);
                let res = self
                    .builder
                    .build_int_compare(IntPredicate::ULT, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i32x4_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32LeS | Operator::I64LeS => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let v1 = self.apply_pending_canonicalization(v1, i1);
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let (v1, v2) = (v1.into_int_value(), v2.into_int_value());
                let cond = self
                    .builder
                    .build_int_compare(IntPredicate::SLE, v1, v2, "");
                let res = self
                    .builder
                    .build_int_z_extend(cond, self.intrinsics.i32_ty, "");
                self.state.push1_extra(
                    res,
                    ExtraInfo::arithmetic_f32() | ExtraInfo::arithmetic_f64(),
                );
            }
            Operator::I8x16LeS => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i8x16(v1, i1);
                let (v2, _) = self.v128_into_i8x16(v2, i2);
                let res = self
                    .builder
                    .build_int_compare(IntPredicate::SLE, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i8x16_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I16x8LeS => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i16x8(v1, i1);
                let (v2, _) = self.v128_into_i16x8(v2, i2);
                let res = self
                    .builder
                    .build_int_compare(IntPredicate::SLE, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i16x8_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32x4LeS => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i32x4(v1, i1);
                let (v2, _) = self.v128_into_i32x4(v2, i2);
                let res = self
                    .builder
                    .build_int_compare(IntPredicate::SLE, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i32x4_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I64x2LeS => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i64x2(v1, i1);
                let (v2, _) = self.v128_into_i64x2(v2, i2);
                let res = self
                    .builder
                    .build_int_compare(IntPredicate::SLE, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i64x2_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32LeU | Operator::I64LeU => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let v1 = self.apply_pending_canonicalization(v1, i1);
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let (v1, v2) = (v1.into_int_value(), v2.into_int_value());
                let cond = self
                    .builder
                    .build_int_compare(IntPredicate::ULE, v1, v2, "");
                let res = self
                    .builder
                    .build_int_z_extend(cond, self.intrinsics.i32_ty, "");
                self.state.push1_extra(
                    res,
                    ExtraInfo::arithmetic_f32() | ExtraInfo::arithmetic_f64(),
                );
            }
            Operator::I8x16LeU => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i8x16(v1, i1);
                let (v2, _) = self.v128_into_i8x16(v2, i2);
                let res = self
                    .builder
                    .build_int_compare(IntPredicate::ULE, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i8x16_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I16x8LeU => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i16x8(v1, i1);
                let (v2, _) = self.v128_into_i16x8(v2, i2);
                let res = self
                    .builder
                    .build_int_compare(IntPredicate::ULE, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i16x8_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32x4LeU => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i32x4(v1, i1);
                let (v2, _) = self.v128_into_i32x4(v2, i2);
                let res = self
                    .builder
                    .build_int_compare(IntPredicate::ULE, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i32x4_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32GtS | Operator::I64GtS => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let v1 = self.apply_pending_canonicalization(v1, i1);
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let (v1, v2) = (v1.into_int_value(), v2.into_int_value());
                let cond = self
                    .builder
                    .build_int_compare(IntPredicate::SGT, v1, v2, "");
                let res = self
                    .builder
                    .build_int_z_extend(cond, self.intrinsics.i32_ty, "");
                self.state.push1_extra(
                    res,
                    ExtraInfo::arithmetic_f32() | ExtraInfo::arithmetic_f64(),
                );
            }
            Operator::I8x16GtS => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i8x16(v1, i1);
                let (v2, _) = self.v128_into_i8x16(v2, i2);
                let res = self
                    .builder
                    .build_int_compare(IntPredicate::SGT, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i8x16_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I16x8GtS => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i16x8(v1, i1);
                let (v2, _) = self.v128_into_i16x8(v2, i2);
                let res = self
                    .builder
                    .build_int_compare(IntPredicate::SGT, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i16x8_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32x4GtS => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i32x4(v1, i1);
                let (v2, _) = self.v128_into_i32x4(v2, i2);
                let res = self
                    .builder
                    .build_int_compare(IntPredicate::SGT, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i32x4_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I64x2GtS => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i64x2(v1, i1);
                let (v2, _) = self.v128_into_i64x2(v2, i2);
                let res = self
                    .builder
                    .build_int_compare(IntPredicate::SGT, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i64x2_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32GtU | Operator::I64GtU => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let v1 = self.apply_pending_canonicalization(v1, i1);
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let (v1, v2) = (v1.into_int_value(), v2.into_int_value());
                let cond = self
                    .builder
                    .build_int_compare(IntPredicate::UGT, v1, v2, "");
                let res = self
                    .builder
                    .build_int_z_extend(cond, self.intrinsics.i32_ty, "");
                self.state.push1_extra(
                    res,
                    ExtraInfo::arithmetic_f32() | ExtraInfo::arithmetic_f64(),
                );
            }
            Operator::I8x16GtU => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i8x16(v1, i1);
                let (v2, _) = self.v128_into_i8x16(v2, i2);
                let res = self
                    .builder
                    .build_int_compare(IntPredicate::UGT, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i8x16_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I16x8GtU => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i16x8(v1, i1);
                let (v2, _) = self.v128_into_i16x8(v2, i2);
                let res = self
                    .builder
                    .build_int_compare(IntPredicate::UGT, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i16x8_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32x4GtU => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i32x4(v1, i1);
                let (v2, _) = self.v128_into_i32x4(v2, i2);
                let res = self
                    .builder
                    .build_int_compare(IntPredicate::UGT, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i32x4_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32GeS | Operator::I64GeS => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let v1 = self.apply_pending_canonicalization(v1, i1);
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let (v1, v2) = (v1.into_int_value(), v2.into_int_value());
                let cond = self
                    .builder
                    .build_int_compare(IntPredicate::SGE, v1, v2, "");
                let res = self
                    .builder
                    .build_int_z_extend(cond, self.intrinsics.i32_ty, "");
                self.state.push1(res);
            }
            Operator::I8x16GeS => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i8x16(v1, i1);
                let (v2, _) = self.v128_into_i8x16(v2, i2);
                let res = self
                    .builder
                    .build_int_compare(IntPredicate::SGE, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i8x16_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I16x8GeS => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i16x8(v1, i1);
                let (v2, _) = self.v128_into_i16x8(v2, i2);
                let res = self
                    .builder
                    .build_int_compare(IntPredicate::SGE, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i16x8_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32x4GeS => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i32x4(v1, i1);
                let (v2, _) = self.v128_into_i32x4(v2, i2);
                let res = self
                    .builder
                    .build_int_compare(IntPredicate::SGE, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i32x4_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I64x2GeS => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i64x2(v1, i1);
                let (v2, _) = self.v128_into_i64x2(v2, i2);
                let res = self
                    .builder
                    .build_int_compare(IntPredicate::SGE, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i64x2_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32GeU | Operator::I64GeU => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let v1 = self.apply_pending_canonicalization(v1, i1);
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let (v1, v2) = (v1.into_int_value(), v2.into_int_value());
                let cond = self
                    .builder
                    .build_int_compare(IntPredicate::UGE, v1, v2, "");
                let res = self
                    .builder
                    .build_int_z_extend(cond, self.intrinsics.i32_ty, "");
                self.state.push1_extra(
                    res,
                    ExtraInfo::arithmetic_f32() | ExtraInfo::arithmetic_f64(),
                );
            }
            Operator::I8x16GeU => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i8x16(v1, i1);
                let (v2, _) = self.v128_into_i8x16(v2, i2);
                let res = self
                    .builder
                    .build_int_compare(IntPredicate::UGE, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i8x16_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I16x8GeU => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i16x8(v1, i1);
                let (v2, _) = self.v128_into_i16x8(v2, i2);
                let res = self
                    .builder
                    .build_int_compare(IntPredicate::UGE, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i16x8_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32x4GeU => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i32x4(v1, i1);
                let (v2, _) = self.v128_into_i32x4(v2, i2);
                let res = self
                    .builder
                    .build_int_compare(IntPredicate::UGE, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i32x4_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }

            /***************************
             * Floating-Point Comparison instructions.
             * https://github.com/sunfishcode/wasm-reference-manual/blob/master/WebAssembly.md#floating-point-comparison-instructions
             ***************************/
            Operator::F32Eq | Operator::F64Eq => {
                let (v1, v2) = self.state.pop2()?;
                let (v1, v2) = (v1.into_float_value(), v2.into_float_value());
                let cond = self
                    .builder
                    .build_float_compare(FloatPredicate::OEQ, v1, v2, "");
                let res = self
                    .builder
                    .build_int_z_extend(cond, self.intrinsics.i32_ty, "");
                self.state.push1_extra(
                    res,
                    ExtraInfo::arithmetic_f32() | ExtraInfo::arithmetic_f64(),
                );
            }
            Operator::F32x4Eq => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_f32x4(v1, i1);
                let (v2, _) = self.v128_into_f32x4(v2, i2);
                let res = self
                    .builder
                    .build_float_compare(FloatPredicate::OEQ, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i32x4_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::F64x2Eq => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_f64x2(v1, i1);
                let (v2, _) = self.v128_into_f64x2(v2, i2);
                let res = self
                    .builder
                    .build_float_compare(FloatPredicate::OEQ, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i64x2_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::F32Ne | Operator::F64Ne => {
                let (v1, v2) = self.state.pop2()?;
                let (v1, v2) = (v1.into_float_value(), v2.into_float_value());
                let cond = self
                    .builder
                    .build_float_compare(FloatPredicate::UNE, v1, v2, "");
                let res = self
                    .builder
                    .build_int_z_extend(cond, self.intrinsics.i32_ty, "");
                self.state.push1_extra(
                    res,
                    ExtraInfo::arithmetic_f32() | ExtraInfo::arithmetic_f64(),
                );
            }
            Operator::F32x4Ne => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_f32x4(v1, i1);
                let (v2, _) = self.v128_into_f32x4(v2, i2);
                let res = self
                    .builder
                    .build_float_compare(FloatPredicate::UNE, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i32x4_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::F64x2Ne => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_f64x2(v1, i1);
                let (v2, _) = self.v128_into_f64x2(v2, i2);
                let res = self
                    .builder
                    .build_float_compare(FloatPredicate::UNE, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i64x2_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::F32Lt | Operator::F64Lt => {
                let (v1, v2) = self.state.pop2()?;
                let (v1, v2) = (v1.into_float_value(), v2.into_float_value());
                let cond = self
                    .builder
                    .build_float_compare(FloatPredicate::OLT, v1, v2, "");
                let res = self
                    .builder
                    .build_int_z_extend(cond, self.intrinsics.i32_ty, "");
                self.state.push1_extra(
                    res,
                    ExtraInfo::arithmetic_f32() | ExtraInfo::arithmetic_f64(),
                );
            }
            Operator::F32x4Lt => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_f32x4(v1, i1);
                let (v2, _) = self.v128_into_f32x4(v2, i2);
                let res = self
                    .builder
                    .build_float_compare(FloatPredicate::OLT, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i32x4_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::F64x2Lt => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_f64x2(v1, i1);
                let (v2, _) = self.v128_into_f64x2(v2, i2);
                let res = self
                    .builder
                    .build_float_compare(FloatPredicate::OLT, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i64x2_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::F32Le | Operator::F64Le => {
                let (v1, v2) = self.state.pop2()?;
                let (v1, v2) = (v1.into_float_value(), v2.into_float_value());
                let cond = self
                    .builder
                    .build_float_compare(FloatPredicate::OLE, v1, v2, "");
                let res = self
                    .builder
                    .build_int_z_extend(cond, self.intrinsics.i32_ty, "");
                self.state.push1_extra(
                    res,
                    ExtraInfo::arithmetic_f32() | ExtraInfo::arithmetic_f64(),
                );
            }
            Operator::F32x4Le => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_f32x4(v1, i1);
                let (v2, _) = self.v128_into_f32x4(v2, i2);
                let res = self
                    .builder
                    .build_float_compare(FloatPredicate::OLE, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i32x4_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::F64x2Le => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_f64x2(v1, i1);
                let (v2, _) = self.v128_into_f64x2(v2, i2);
                let res = self
                    .builder
                    .build_float_compare(FloatPredicate::OLE, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i64x2_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::F32Gt | Operator::F64Gt => {
                let (v1, v2) = self.state.pop2()?;
                let (v1, v2) = (v1.into_float_value(), v2.into_float_value());
                let cond = self
                    .builder
                    .build_float_compare(FloatPredicate::OGT, v1, v2, "");
                let res = self
                    .builder
                    .build_int_z_extend(cond, self.intrinsics.i32_ty, "");
                self.state.push1_extra(
                    res,
                    ExtraInfo::arithmetic_f32() | ExtraInfo::arithmetic_f64(),
                );
            }
            Operator::F32x4Gt => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_f32x4(v1, i1);
                let (v2, _) = self.v128_into_f32x4(v2, i2);
                let res = self
                    .builder
                    .build_float_compare(FloatPredicate::OGT, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i32x4_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::F64x2Gt => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_f64x2(v1, i1);
                let (v2, _) = self.v128_into_f64x2(v2, i2);
                let res = self
                    .builder
                    .build_float_compare(FloatPredicate::OGT, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i64x2_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::F32Ge | Operator::F64Ge => {
                let (v1, v2) = self.state.pop2()?;
                let (v1, v2) = (v1.into_float_value(), v2.into_float_value());
                let cond = self
                    .builder
                    .build_float_compare(FloatPredicate::OGE, v1, v2, "");
                let res = self
                    .builder
                    .build_int_z_extend(cond, self.intrinsics.i32_ty, "");
                self.state.push1_extra(
                    res,
                    ExtraInfo::arithmetic_f32() | ExtraInfo::arithmetic_f64(),
                );
            }
            Operator::F32x4Ge => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_f32x4(v1, i1);
                let (v2, _) = self.v128_into_f32x4(v2, i2);
                let res = self
                    .builder
                    .build_float_compare(FloatPredicate::OGE, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i32x4_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::F64x2Ge => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_f64x2(v1, i1);
                let (v2, _) = self.v128_into_f64x2(v2, i2);
                let res = self
                    .builder
                    .build_float_compare(FloatPredicate::OGE, v1, v2, "");
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i64x2_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }

            /***************************
             * Conversion instructions.
             * https://github.com/sunfishcode/wasm-reference-manual/blob/master/WebAssembly.md#conversion-instructions
             ***************************/
            Operator::I32WrapI64 => {
                let (v, i) = self.state.pop1_extra()?;
                let v = self.apply_pending_canonicalization(v, i);
                let v = v.into_int_value();
                let res = self
                    .builder
                    .build_int_truncate(v, self.intrinsics.i32_ty, "");
                self.state.push1(res);
            }
            Operator::I64ExtendI32S => {
                let (v, i) = self.state.pop1_extra()?;
                let v = self.apply_pending_canonicalization(v, i);
                let v = v.into_int_value();
                let res = self
                    .builder
                    .build_int_s_extend(v, self.intrinsics.i64_ty, "");
                self.state.push1(res);
            }
            Operator::I64ExtendI32U => {
                let (v, i) = self.state.pop1_extra()?;
                let v = self.apply_pending_canonicalization(v, i);
                let v = v.into_int_value();
                let res = self
                    .builder
                    .build_int_z_extend(v, self.intrinsics.i64_ty, "");
                self.state.push1_extra(res, ExtraInfo::arithmetic_f64());
            }
            Operator::I16x8ExtendLowI8x16S => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, _) = self.v128_into_i8x16(v, i);
                let low = self.builder.build_shuffle_vector(
                    v,
                    v.get_type().get_undef(),
                    VectorType::const_vector(&[
                        self.intrinsics.i32_consts[0],
                        self.intrinsics.i32_consts[1],
                        self.intrinsics.i32_consts[2],
                        self.intrinsics.i32_consts[3],
                        self.intrinsics.i32_consts[4],
                        self.intrinsics.i32_consts[5],
                        self.intrinsics.i32_consts[6],
                        self.intrinsics.i32_consts[7],
                    ]),
                    "",
                );
                let res = self
                    .builder
                    .build_int_s_extend(low, self.intrinsics.i16x8_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I16x8ExtendHighI8x16S => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, _) = self.v128_into_i8x16(v, i);
                let low = self.builder.build_shuffle_vector(
                    v,
                    v.get_type().get_undef(),
                    VectorType::const_vector(&[
                        self.intrinsics.i32_consts[8],
                        self.intrinsics.i32_consts[9],
                        self.intrinsics.i32_consts[10],
                        self.intrinsics.i32_consts[11],
                        self.intrinsics.i32_consts[12],
                        self.intrinsics.i32_consts[13],
                        self.intrinsics.i32_consts[14],
                        self.intrinsics.i32_consts[15],
                    ]),
                    "",
                );
                let res = self
                    .builder
                    .build_int_s_extend(low, self.intrinsics.i16x8_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I16x8ExtendLowI8x16U => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, _) = self.v128_into_i8x16(v, i);
                let low = self.builder.build_shuffle_vector(
                    v,
                    v.get_type().get_undef(),
                    VectorType::const_vector(&[
                        self.intrinsics.i32_consts[0],
                        self.intrinsics.i32_consts[1],
                        self.intrinsics.i32_consts[2],
                        self.intrinsics.i32_consts[3],
                        self.intrinsics.i32_consts[4],
                        self.intrinsics.i32_consts[5],
                        self.intrinsics.i32_consts[6],
                        self.intrinsics.i32_consts[7],
                    ]),
                    "",
                );
                let res = self
                    .builder
                    .build_int_z_extend(low, self.intrinsics.i16x8_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I16x8ExtendHighI8x16U => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, _) = self.v128_into_i8x16(v, i);
                let low = self.builder.build_shuffle_vector(
                    v,
                    v.get_type().get_undef(),
                    VectorType::const_vector(&[
                        self.intrinsics.i32_consts[8],
                        self.intrinsics.i32_consts[9],
                        self.intrinsics.i32_consts[10],
                        self.intrinsics.i32_consts[11],
                        self.intrinsics.i32_consts[12],
                        self.intrinsics.i32_consts[13],
                        self.intrinsics.i32_consts[14],
                        self.intrinsics.i32_consts[15],
                    ]),
                    "",
                );
                let res = self
                    .builder
                    .build_int_z_extend(low, self.intrinsics.i16x8_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32x4ExtendLowI16x8S => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, _) = self.v128_into_i16x8(v, i);
                let low = self.builder.build_shuffle_vector(
                    v,
                    v.get_type().get_undef(),
                    VectorType::const_vector(&[
                        self.intrinsics.i32_consts[0],
                        self.intrinsics.i32_consts[1],
                        self.intrinsics.i32_consts[2],
                        self.intrinsics.i32_consts[3],
                    ]),
                    "",
                );
                let res = self
                    .builder
                    .build_int_s_extend(low, self.intrinsics.i32x4_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32x4ExtendHighI16x8S => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, _) = self.v128_into_i16x8(v, i);
                let low = self.builder.build_shuffle_vector(
                    v,
                    v.get_type().get_undef(),
                    VectorType::const_vector(&[
                        self.intrinsics.i32_consts[4],
                        self.intrinsics.i32_consts[5],
                        self.intrinsics.i32_consts[6],
                        self.intrinsics.i32_consts[7],
                    ]),
                    "",
                );
                let res = self
                    .builder
                    .build_int_s_extend(low, self.intrinsics.i32x4_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32x4ExtendLowI16x8U => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, _) = self.v128_into_i16x8(v, i);
                let low = self.builder.build_shuffle_vector(
                    v,
                    v.get_type().get_undef(),
                    VectorType::const_vector(&[
                        self.intrinsics.i32_consts[0],
                        self.intrinsics.i32_consts[1],
                        self.intrinsics.i32_consts[2],
                        self.intrinsics.i32_consts[3],
                    ]),
                    "",
                );
                let res = self
                    .builder
                    .build_int_z_extend(low, self.intrinsics.i32x4_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32x4ExtendHighI16x8U => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, _) = self.v128_into_i16x8(v, i);
                let low = self.builder.build_shuffle_vector(
                    v,
                    v.get_type().get_undef(),
                    VectorType::const_vector(&[
                        self.intrinsics.i32_consts[4],
                        self.intrinsics.i32_consts[5],
                        self.intrinsics.i32_consts[6],
                        self.intrinsics.i32_consts[7],
                    ]),
                    "",
                );
                let res = self
                    .builder
                    .build_int_z_extend(low, self.intrinsics.i32x4_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I64x2ExtendLowI32x4U
            | Operator::I64x2ExtendLowI32x4S
            | Operator::I64x2ExtendHighI32x4U
            | Operator::I64x2ExtendHighI32x4S => {
                let extend = match op {
                    Operator::I64x2ExtendLowI32x4U | Operator::I64x2ExtendHighI32x4U => {
                        |s: &Self, v| s.builder.build_int_z_extend(v, s.intrinsics.i64x2_ty, "")
                    }
                    Operator::I64x2ExtendLowI32x4S | Operator::I64x2ExtendHighI32x4S => {
                        |s: &Self, v| s.builder.build_int_s_extend(v, s.intrinsics.i64x2_ty, "")
                    }
                    _ => unreachable!("Unhandled inner case"),
                };
                let indices = match op {
                    Operator::I64x2ExtendLowI32x4S | Operator::I64x2ExtendLowI32x4U => {
                        [self.intrinsics.i32_consts[0], self.intrinsics.i32_consts[1]]
                    }
                    Operator::I64x2ExtendHighI32x4S | Operator::I64x2ExtendHighI32x4U => {
                        [self.intrinsics.i32_consts[2], self.intrinsics.i32_consts[3]]
                    }
                    _ => unreachable!("Unhandled inner case"),
                };
                let (v, i) = self.state.pop1_extra()?;
                let (v, _) = self.v128_into_i32x4(v, i);
                let low = self.builder.build_shuffle_vector(
                    v,
                    v.get_type().get_undef(),
                    VectorType::const_vector(&indices),
                    "",
                );
                let res = extend(&self, low);
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I8x16NarrowI16x8S => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i16x8(v1, i1);
                let (v2, _) = self.v128_into_i16x8(v2, i2);
                let min = self.intrinsics.i16_ty.const_int(0xff80, false);
                let max = self.intrinsics.i16_ty.const_int(0x007f, false);
                let min = VectorType::const_vector(&[min; 8]);
                let max = VectorType::const_vector(&[max; 8]);
                let apply_min_clamp_v1 =
                    self.builder
                        .build_int_compare(IntPredicate::SLT, v1, min, "");
                let apply_max_clamp_v1 =
                    self.builder
                        .build_int_compare(IntPredicate::SGT, v1, max, "");
                let apply_min_clamp_v2 =
                    self.builder
                        .build_int_compare(IntPredicate::SLT, v2, min, "");
                let apply_max_clamp_v2 =
                    self.builder
                        .build_int_compare(IntPredicate::SGT, v2, max, "");
                let v1 = self
                    .builder
                    .build_select(apply_min_clamp_v1, min, v1, "")
                    .into_vector_value();
                let v1 = self
                    .builder
                    .build_select(apply_max_clamp_v1, max, v1, "")
                    .into_vector_value();
                let v1 = self
                    .builder
                    .build_int_truncate(v1, self.intrinsics.i8_ty.vec_type(8), "");
                let v2 = self
                    .builder
                    .build_select(apply_min_clamp_v2, min, v2, "")
                    .into_vector_value();
                let v2 = self
                    .builder
                    .build_select(apply_max_clamp_v2, max, v2, "")
                    .into_vector_value();
                let v2 = self
                    .builder
                    .build_int_truncate(v2, self.intrinsics.i8_ty.vec_type(8), "");
                let res = self.builder.build_shuffle_vector(
                    v1,
                    v2,
                    VectorType::const_vector(&[
                        self.intrinsics.i32_consts[0],
                        self.intrinsics.i32_consts[1],
                        self.intrinsics.i32_consts[2],
                        self.intrinsics.i32_consts[3],
                        self.intrinsics.i32_consts[4],
                        self.intrinsics.i32_consts[5],
                        self.intrinsics.i32_consts[6],
                        self.intrinsics.i32_consts[7],
                        self.intrinsics.i32_consts[8],
                        self.intrinsics.i32_consts[9],
                        self.intrinsics.i32_consts[10],
                        self.intrinsics.i32_consts[11],
                        self.intrinsics.i32_consts[12],
                        self.intrinsics.i32_consts[13],
                        self.intrinsics.i32_consts[14],
                        self.intrinsics.i32_consts[15],
                    ]),
                    "",
                );
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I8x16NarrowI16x8U => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i16x8(v1, i1);
                let (v2, _) = self.v128_into_i16x8(v2, i2);
                let min = self.intrinsics.i16x8_ty.const_zero();
                let max = self.intrinsics.i16_ty.const_int(0x00ff, false);
                let max = VectorType::const_vector(&[max; 8]);
                let apply_min_clamp_v1 =
                    self.builder
                        .build_int_compare(IntPredicate::SLT, v1, min, "");
                let apply_max_clamp_v1 =
                    self.builder
                        .build_int_compare(IntPredicate::SGT, v1, max, "");
                let apply_min_clamp_v2 =
                    self.builder
                        .build_int_compare(IntPredicate::SLT, v2, min, "");
                let apply_max_clamp_v2 =
                    self.builder
                        .build_int_compare(IntPredicate::SGT, v2, max, "");
                let v1 = self
                    .builder
                    .build_select(apply_min_clamp_v1, min, v1, "")
                    .into_vector_value();
                let v1 = self
                    .builder
                    .build_select(apply_max_clamp_v1, max, v1, "")
                    .into_vector_value();
                let v1 = self
                    .builder
                    .build_int_truncate(v1, self.intrinsics.i8_ty.vec_type(8), "");
                let v2 = self
                    .builder
                    .build_select(apply_min_clamp_v2, min, v2, "")
                    .into_vector_value();
                let v2 = self
                    .builder
                    .build_select(apply_max_clamp_v2, max, v2, "")
                    .into_vector_value();
                let v2 = self
                    .builder
                    .build_int_truncate(v2, self.intrinsics.i8_ty.vec_type(8), "");
                let res = self.builder.build_shuffle_vector(
                    v1,
                    v2,
                    VectorType::const_vector(&[
                        self.intrinsics.i32_consts[0],
                        self.intrinsics.i32_consts[1],
                        self.intrinsics.i32_consts[2],
                        self.intrinsics.i32_consts[3],
                        self.intrinsics.i32_consts[4],
                        self.intrinsics.i32_consts[5],
                        self.intrinsics.i32_consts[6],
                        self.intrinsics.i32_consts[7],
                        self.intrinsics.i32_consts[8],
                        self.intrinsics.i32_consts[9],
                        self.intrinsics.i32_consts[10],
                        self.intrinsics.i32_consts[11],
                        self.intrinsics.i32_consts[12],
                        self.intrinsics.i32_consts[13],
                        self.intrinsics.i32_consts[14],
                        self.intrinsics.i32_consts[15],
                    ]),
                    "",
                );
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I16x8NarrowI32x4S => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i32x4(v1, i1);
                let (v2, _) = self.v128_into_i32x4(v2, i2);
                let min = self.intrinsics.i32_ty.const_int(0xffff8000, false);
                let max = self.intrinsics.i32_ty.const_int(0x00007fff, false);
                let min = VectorType::const_vector(&[min; 4]);
                let max = VectorType::const_vector(&[max; 4]);
                let apply_min_clamp_v1 =
                    self.builder
                        .build_int_compare(IntPredicate::SLT, v1, min, "");
                let apply_max_clamp_v1 =
                    self.builder
                        .build_int_compare(IntPredicate::SGT, v1, max, "");
                let apply_min_clamp_v2 =
                    self.builder
                        .build_int_compare(IntPredicate::SLT, v2, min, "");
                let apply_max_clamp_v2 =
                    self.builder
                        .build_int_compare(IntPredicate::SGT, v2, max, "");
                let v1 = self
                    .builder
                    .build_select(apply_min_clamp_v1, min, v1, "")
                    .into_vector_value();
                let v1 = self
                    .builder
                    .build_select(apply_max_clamp_v1, max, v1, "")
                    .into_vector_value();
                let v1 =
                    self.builder
                        .build_int_truncate(v1, self.intrinsics.i16_ty.vec_type(4), "");
                let v2 = self
                    .builder
                    .build_select(apply_min_clamp_v2, min, v2, "")
                    .into_vector_value();
                let v2 = self
                    .builder
                    .build_select(apply_max_clamp_v2, max, v2, "")
                    .into_vector_value();
                let v2 =
                    self.builder
                        .build_int_truncate(v2, self.intrinsics.i16_ty.vec_type(4), "");
                let res = self.builder.build_shuffle_vector(
                    v1,
                    v2,
                    VectorType::const_vector(&[
                        self.intrinsics.i32_consts[0],
                        self.intrinsics.i32_consts[1],
                        self.intrinsics.i32_consts[2],
                        self.intrinsics.i32_consts[3],
                        self.intrinsics.i32_consts[4],
                        self.intrinsics.i32_consts[5],
                        self.intrinsics.i32_consts[6],
                        self.intrinsics.i32_consts[7],
                    ]),
                    "",
                );
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I16x8NarrowI32x4U => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i32x4(v1, i1);
                let (v2, _) = self.v128_into_i32x4(v2, i2);
                let min = self.intrinsics.i32x4_ty.const_zero();
                let max = self.intrinsics.i32_ty.const_int(0xffff, false);
                let max = VectorType::const_vector(&[max; 4]);
                let apply_min_clamp_v1 =
                    self.builder
                        .build_int_compare(IntPredicate::SLT, v1, min, "");
                let apply_max_clamp_v1 =
                    self.builder
                        .build_int_compare(IntPredicate::SGT, v1, max, "");
                let apply_min_clamp_v2 =
                    self.builder
                        .build_int_compare(IntPredicate::SLT, v2, min, "");
                let apply_max_clamp_v2 =
                    self.builder
                        .build_int_compare(IntPredicate::SGT, v2, max, "");
                let v1 = self
                    .builder
                    .build_select(apply_min_clamp_v1, min, v1, "")
                    .into_vector_value();
                let v1 = self
                    .builder
                    .build_select(apply_max_clamp_v1, max, v1, "")
                    .into_vector_value();
                let v1 =
                    self.builder
                        .build_int_truncate(v1, self.intrinsics.i16_ty.vec_type(4), "");
                let v2 = self
                    .builder
                    .build_select(apply_min_clamp_v2, min, v2, "")
                    .into_vector_value();
                let v2 = self
                    .builder
                    .build_select(apply_max_clamp_v2, max, v2, "")
                    .into_vector_value();
                let v2 =
                    self.builder
                        .build_int_truncate(v2, self.intrinsics.i16_ty.vec_type(4), "");
                let res = self.builder.build_shuffle_vector(
                    v1,
                    v2,
                    VectorType::const_vector(&[
                        self.intrinsics.i32_consts[0],
                        self.intrinsics.i32_consts[1],
                        self.intrinsics.i32_consts[2],
                        self.intrinsics.i32_consts[3],
                        self.intrinsics.i32_consts[4],
                        self.intrinsics.i32_consts[5],
                        self.intrinsics.i32_consts[6],
                        self.intrinsics.i32_consts[7],
                    ]),
                    "",
                );
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32x4TruncSatF32x4S => {
                let (v, i) = self.state.pop1_extra()?;
                let v = self.apply_pending_canonicalization(v, i);
                let v = v.into_int_value();
                let res = self.trunc_sat_into_int(
                    self.intrinsics.f32x4_ty,
                    self.intrinsics.i32x4_ty,
                    LEF32_GEQ_I32_MIN,
                    GEF32_LEQ_I32_MAX,
                    std::i32::MIN as u64,
                    std::i32::MAX as u64,
                    v,
                );
                self.state.push1(res);
            }
            Operator::I32x4TruncSatF32x4U => {
                let (v, i) = self.state.pop1_extra()?;
                let v = self.apply_pending_canonicalization(v, i);
                let v = v.into_int_value();
                let res = self.trunc_sat_into_int(
                    self.intrinsics.f32x4_ty,
                    self.intrinsics.i32x4_ty,
                    LEF32_GEQ_U32_MIN,
                    GEF32_LEQ_U32_MAX,
                    std::u32::MIN as u64,
                    std::u32::MAX as u64,
                    v,
                );
                self.state.push1(res);
            }
            Operator::I32x4TruncSatF64x2SZero | Operator::I32x4TruncSatF64x2UZero => {
                let ((min, max), (cmp_min, cmp_max)) = match op {
                    Operator::I32x4TruncSatF64x2SZero => (
                        (std::i32::MIN as u64, std::i32::MAX as u64),
                        (LEF64_GEQ_I32_MIN, GEF64_LEQ_I32_MAX),
                    ),
                    Operator::I32x4TruncSatF64x2UZero => (
                        (std::u32::MIN as u64, std::u32::MAX as u64),
                        (LEF64_GEQ_U32_MIN, GEF64_LEQ_U32_MAX),
                    ),
                    _ => unreachable!("Unhandled internal variant"),
                };
                let (v, i) = self.state.pop1_extra()?;
                let v = self.apply_pending_canonicalization(v, i);
                let v = v.into_int_value();
                let res = self.trunc_sat(
                    self.intrinsics.f64x2_ty,
                    self.intrinsics.i32_ty.vec_type(2),
                    cmp_min,
                    cmp_max,
                    min,
                    max,
                    v,
                );

                let zero = self.intrinsics.i32_consts[0];
                let zeros = VectorType::const_vector(&[zero; 2]);
                let res = self.builder.build_shuffle_vector(
                    res,
                    zeros,
                    VectorType::const_vector(&[
                        self.intrinsics.i32_consts[0],
                        self.intrinsics.i32_consts[1],
                        self.intrinsics.i32_consts[2],
                        self.intrinsics.i32_consts[3],
                    ]),
                    "",
                );
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            // Operator::I64x2TruncSatF64x2S => {
            //     let (v, i) = self.state.pop1_extra()?;
            //     let v = self.apply_pending_canonicalization(v, i);
            //     let v = v.into_int_value();
            //     let res = self.trunc_sat_into_int(
            //         self.intrinsics.f64x2_ty,
            //         self.intrinsics.i64x2_ty,
            //         std::i64::MIN as u64,
            //         std::i64::MAX as u64,
            //         std::i64::MIN as u64,
            //         std::i64::MAX as u64,
            //         v,
            //     );
            //     self.state.push1(res);
            // }
            // Operator::I64x2TruncSatF64x2U => {
            //     let (v, i) = self.state.pop1_extra()?;
            //     let v = self.apply_pending_canonicalization(v, i);
            //     let v = v.into_int_value();
            //     let res = self.trunc_sat_into_int(
            //         self.intrinsics.f64x2_ty,
            //         self.intrinsics.i64x2_ty,
            //         std::u64::MIN,
            //         std::u64::MAX,
            //         std::u64::MIN,
            //         std::u64::MAX,
            //         v,
            //     );
            //     self.state.push1(res);
            // }
            Operator::I32TruncF32S => {
                let v1 = self.state.pop1()?.into_float_value();
                self.trap_if_not_representable_as_int(
                    0xcf000000, // -2147483600.0
                    0x4effffff, // 2147483500.0
                    v1,
                );
                let res = self
                    .builder
                    .build_float_to_signed_int(v1, self.intrinsics.i32_ty, "");
                self.state.push1(res);
            }
            Operator::I32TruncF64S => {
                let v1 = self.state.pop1()?.into_float_value();
                self.trap_if_not_representable_as_int(
                    0xc1e00000001fffff, // -2147483648.9999995
                    0x41dfffffffffffff, // 2147483647.9999998
                    v1,
                );
                let res = self
                    .builder
                    .build_float_to_signed_int(v1, self.intrinsics.i32_ty, "");
                self.state.push1(res);
            }
            Operator::I32TruncSatF32S => {
                let (v, i) = self.state.pop1_extra()?;
                let v = self.apply_pending_canonicalization(v, i);
                let v = v.into_float_value();
                let res = self.trunc_sat_scalar(
                    self.intrinsics.i32_ty,
                    LEF32_GEQ_I32_MIN,
                    GEF32_LEQ_I32_MAX,
                    std::i32::MIN as u32 as u64,
                    std::i32::MAX as u32 as u64,
                    v,
                );
                self.state.push1(res);
            }
            Operator::I32TruncSatF64S => {
                let (v, i) = self.state.pop1_extra()?;
                let v = self.apply_pending_canonicalization(v, i);
                let v = v.into_float_value();
                let res = self.trunc_sat_scalar(
                    self.intrinsics.i32_ty,
                    LEF64_GEQ_I32_MIN,
                    GEF64_LEQ_I32_MAX,
                    std::i32::MIN as u64,
                    std::i32::MAX as u64,
                    v,
                );
                self.state.push1(res);
            }
            Operator::I64TruncF32S => {
                let v1 = self.state.pop1()?.into_float_value();
                self.trap_if_not_representable_as_int(
                    0xdf000000, // -9223372000000000000.0
                    0x5effffff, // 9223371500000000000.0
                    v1,
                );
                let res = self
                    .builder
                    .build_float_to_signed_int(v1, self.intrinsics.i64_ty, "");
                self.state.push1(res);
            }
            Operator::I64TruncF64S => {
                let v1 = self.state.pop1()?.into_float_value();
                self.trap_if_not_representable_as_int(
                    0xc3e0000000000000, // -9223372036854776000.0
                    0x43dfffffffffffff, // 9223372036854775000.0
                    v1,
                );
                let res = self
                    .builder
                    .build_float_to_signed_int(v1, self.intrinsics.i64_ty, "");
                self.state.push1(res);
            }
            Operator::I64TruncSatF32S => {
                let (v, i) = self.state.pop1_extra()?;
                let v = self.apply_pending_canonicalization(v, i);
                let v = v.into_float_value();
                let res = self.trunc_sat_scalar(
                    self.intrinsics.i64_ty,
                    LEF32_GEQ_I64_MIN,
                    GEF32_LEQ_I64_MAX,
                    std::i64::MIN as u64,
                    std::i64::MAX as u64,
                    v,
                );
                self.state.push1(res);
            }
            Operator::I64TruncSatF64S => {
                let (v, i) = self.state.pop1_extra()?;
                let v = self.apply_pending_canonicalization(v, i);
                let v = v.into_float_value();
                let res = self.trunc_sat_scalar(
                    self.intrinsics.i64_ty,
                    LEF64_GEQ_I64_MIN,
                    GEF64_LEQ_I64_MAX,
                    std::i64::MIN as u64,
                    std::i64::MAX as u64,
                    v,
                );
                self.state.push1(res);
            }
            Operator::I32TruncF32U => {
                let v1 = self.state.pop1()?.into_float_value();
                self.trap_if_not_representable_as_int(
                    0xbf7fffff, // -0.99999994
                    0x4f7fffff, // 4294967000.0
                    v1,
                );
                let res = self
                    .builder
                    .build_float_to_unsigned_int(v1, self.intrinsics.i32_ty, "");
                self.state.push1(res);
            }
            Operator::I32TruncF64U => {
                let v1 = self.state.pop1()?.into_float_value();
                self.trap_if_not_representable_as_int(
                    0xbfefffffffffffff, // -0.9999999999999999
                    0x41efffffffffffff, // 4294967295.9999995
                    v1,
                );
                let res = self
                    .builder
                    .build_float_to_unsigned_int(v1, self.intrinsics.i32_ty, "");
                self.state.push1(res);
            }
            Operator::I32TruncSatF32U => {
                let (v, i) = self.state.pop1_extra()?;
                let v = self.apply_pending_canonicalization(v, i);
                let v = v.into_float_value();
                let res = self.trunc_sat_scalar(
                    self.intrinsics.i32_ty,
                    LEF32_GEQ_U32_MIN,
                    GEF32_LEQ_U32_MAX,
                    std::u32::MIN as u64,
                    std::u32::MAX as u64,
                    v,
                );
                self.state.push1(res);
            }
            Operator::I32TruncSatF64U => {
                let (v, i) = self.state.pop1_extra()?;
                let v = self.apply_pending_canonicalization(v, i);
                let v = v.into_float_value();
                let res = self.trunc_sat_scalar(
                    self.intrinsics.i32_ty,
                    LEF64_GEQ_U32_MIN,
                    GEF64_LEQ_U32_MAX,
                    std::u32::MIN as u64,
                    std::u32::MAX as u64,
                    v,
                );
                self.state.push1(res);
            }
            Operator::I64TruncF32U => {
                let v1 = self.state.pop1()?.into_float_value();
                self.trap_if_not_representable_as_int(
                    0xbf7fffff, // -0.99999994
                    0x5f7fffff, // 18446743000000000000.0
                    v1,
                );
                let res = self
                    .builder
                    .build_float_to_unsigned_int(v1, self.intrinsics.i64_ty, "");
                self.state.push1(res);
            }
            Operator::I64TruncF64U => {
                let v1 = self.state.pop1()?.into_float_value();
                self.trap_if_not_representable_as_int(
                    0xbfefffffffffffff, // -0.9999999999999999
                    0x43efffffffffffff, // 18446744073709550000.0
                    v1,
                );
                let res = self
                    .builder
                    .build_float_to_unsigned_int(v1, self.intrinsics.i64_ty, "");
                self.state.push1(res);
            }
            Operator::I64TruncSatF32U => {
                let (v, i) = self.state.pop1_extra()?;
                let v = self.apply_pending_canonicalization(v, i);
                let v = v.into_float_value();
                let res = self.trunc_sat_scalar(
                    self.intrinsics.i64_ty,
                    LEF32_GEQ_U64_MIN,
                    GEF32_LEQ_U64_MAX,
                    std::u64::MIN,
                    std::u64::MAX,
                    v,
                );
                self.state.push1(res);
            }
            Operator::I64TruncSatF64U => {
                let (v, i) = self.state.pop1_extra()?;
                let v = self.apply_pending_canonicalization(v, i);
                let v = v.into_float_value();
                let res = self.trunc_sat_scalar(
                    self.intrinsics.i64_ty,
                    LEF64_GEQ_U64_MIN,
                    GEF64_LEQ_U64_MAX,
                    std::u64::MIN,
                    std::u64::MAX,
                    v,
                );
                self.state.push1(res);
            }
            Operator::F32DemoteF64 => {
                let v = self.state.pop1()?;
                let v = v.into_float_value();
                let res = self
                    .builder
                    .build_call(
                        self.intrinsics.fptrunc_f64,
                        &[
                            v.into(),
                            self.intrinsics.fp_rounding_md,
                            self.intrinsics.fp_exception_md,
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                self.state.push1_extra(res, ExtraInfo::pending_f32_nan());
            }
            Operator::F64PromoteF32 => {
                let v = self.state.pop1()?;
                let v = v.into_float_value();
                let res = self
                    .builder
                    .build_call(
                        self.intrinsics.fpext_f32,
                        &[v.into(), self.intrinsics.fp_exception_md],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                self.state.push1_extra(res, ExtraInfo::pending_f64_nan());
            }
            Operator::F32ConvertI32S | Operator::F32ConvertI64S => {
                let (v, i) = self.state.pop1_extra()?;
                let v = self.apply_pending_canonicalization(v, i);
                let v = v.into_int_value();
                let res = self
                    .builder
                    .build_signed_int_to_float(v, self.intrinsics.f32_ty, "");
                self.state.push1(res);
            }
            Operator::F64ConvertI32S | Operator::F64ConvertI64S => {
                let (v, i) = self.state.pop1_extra()?;
                let v = self.apply_pending_canonicalization(v, i);
                let v = v.into_int_value();
                let res = self
                    .builder
                    .build_signed_int_to_float(v, self.intrinsics.f64_ty, "");
                self.state.push1(res);
            }
            Operator::F32ConvertI32U | Operator::F32ConvertI64U => {
                let (v, i) = self.state.pop1_extra()?;
                let v = self.apply_pending_canonicalization(v, i);
                let v = v.into_int_value();
                let res = self
                    .builder
                    .build_unsigned_int_to_float(v, self.intrinsics.f32_ty, "");
                self.state.push1(res);
            }
            Operator::F64ConvertI32U | Operator::F64ConvertI64U => {
                let (v, i) = self.state.pop1_extra()?;
                let v = self.apply_pending_canonicalization(v, i);
                let v = v.into_int_value();
                let res = self
                    .builder
                    .build_unsigned_int_to_float(v, self.intrinsics.f64_ty, "");
                self.state.push1(res);
            }
            Operator::F32x4ConvertI32x4S => {
                let v = self.state.pop1()?;
                let v = self
                    .builder
                    .build_bitcast(v, self.intrinsics.i32x4_ty, "")
                    .into_vector_value();
                let res = self
                    .builder
                    .build_signed_int_to_float(v, self.intrinsics.f32x4_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::F32x4ConvertI32x4U => {
                let v = self.state.pop1()?;
                let v = self
                    .builder
                    .build_bitcast(v, self.intrinsics.i32x4_ty, "")
                    .into_vector_value();
                let res = self
                    .builder
                    .build_unsigned_int_to_float(v, self.intrinsics.f32x4_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::F64x2ConvertLowI32x4S | Operator::F64x2ConvertLowI32x4U => {
                let extend = match op {
                    Operator::F64x2ConvertLowI32x4U => {
                        |s: &Self, v| s.builder.build_int_z_extend(v, s.intrinsics.i64x2_ty, "")
                    }
                    Operator::F64x2ConvertLowI32x4S => {
                        |s: &Self, v| s.builder.build_int_s_extend(v, s.intrinsics.i64x2_ty, "")
                    }
                    _ => unreachable!("Unhandled inner case"),
                };
                let (v, i) = self.state.pop1_extra()?;
                let (v, _) = self.v128_into_i32x4(v, i);
                let low = self.builder.build_shuffle_vector(
                    v,
                    v.get_type().get_undef(),
                    VectorType::const_vector(&[
                        self.intrinsics.i32_consts[0],
                        self.intrinsics.i32_consts[1],
                    ]),
                    "",
                );
                let res = extend(&self, low);
                let res = self
                    .builder
                    .build_signed_int_to_float(res, self.intrinsics.f64x2_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::F64x2PromoteLowF32x4 => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, _) = self.v128_into_f32x4(v, i);
                let low = self.builder.build_shuffle_vector(
                    v,
                    v.get_type().get_undef(),
                    VectorType::const_vector(&[
                        self.intrinsics.i32_consts[0],
                        self.intrinsics.i32_consts[1],
                    ]),
                    "",
                );
                let res = self
                    .builder
                    .build_float_ext(low, self.intrinsics.f64x2_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1_extra(res, ExtraInfo::pending_f64_nan());
            }
            Operator::F32x4DemoteF64x2Zero => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, _) = self.v128_into_f64x2(v, i);
                let f32x2_ty = self.intrinsics.f32_ty.vec_type(2);
                let res = self.builder.build_float_trunc(v, f32x2_ty, "");
                let zeros = f32x2_ty.const_zero();
                let res = self.builder.build_shuffle_vector(
                    res,
                    zeros,
                    VectorType::const_vector(&[
                        self.intrinsics.i32_consts[0],
                        self.intrinsics.i32_consts[1],
                        self.intrinsics.i32_consts[2],
                        self.intrinsics.i32_consts[3],
                    ]),
                    "",
                );
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1_extra(res, ExtraInfo::pending_f32_nan());
            }
            // Operator::F64x2ConvertI64x2S => {
            //     let v = self.state.pop1()?;
            //     let v = self
            //         .builder
            //         .build_bitcast(v, self.intrinsics.i64x2_ty, "")
            //         .into_vector_value();
            //     let res = self
            //         .builder
            //         .build_signed_int_to_float(v, self.intrinsics.f64x2_ty, "");
            //     let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
            //     self.state.push1(res);
            // }
            // Operator::F64x2ConvertI64x2U => {
            //     let v = self.state.pop1()?;
            //     let v = self
            //         .builder
            //         .build_bitcast(v, self.intrinsics.i64x2_ty, "")
            //         .into_vector_value();
            //     let res = self
            //         .builder
            //         .build_unsigned_int_to_float(v, self.intrinsics.f64x2_ty, "");
            //     let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
            //     self.state.push1(res);
            // }
            Operator::I32ReinterpretF32 => {
                let (v, i) = self.state.pop1_extra()?;
                let v = self.apply_pending_canonicalization(v, i);
                let ret = self.builder.build_bitcast(v, self.intrinsics.i32_ty, "");
                self.state.push1_extra(ret, ExtraInfo::arithmetic_f32());
            }
            Operator::I64ReinterpretF64 => {
                let (v, i) = self.state.pop1_extra()?;
                let v = self.apply_pending_canonicalization(v, i);
                let ret = self.builder.build_bitcast(v, self.intrinsics.i64_ty, "");
                self.state.push1_extra(ret, ExtraInfo::arithmetic_f64());
            }
            Operator::F32ReinterpretI32 => {
                let (v, i) = self.state.pop1_extra()?;
                let ret = self.builder.build_bitcast(v, self.intrinsics.f32_ty, "");
                self.state.push1_extra(ret, i);
            }
            Operator::F64ReinterpretI64 => {
                let (v, i) = self.state.pop1_extra()?;
                let ret = self.builder.build_bitcast(v, self.intrinsics.f64_ty, "");
                self.state.push1_extra(ret, i);
            }

            /***************************
             * Sign-extension operators.
             * https://github.com/WebAssembly/sign-extension-ops/blob/master/proposals/sign-extension-ops/Overview.md
             ***************************/
            Operator::I32Extend8S => {
                let value = self.state.pop1()?.into_int_value();
                let narrow_value =
                    self.builder
                        .build_int_truncate(value, self.intrinsics.i8_ty, "");
                let extended_value =
                    self.builder
                        .build_int_s_extend(narrow_value, self.intrinsics.i32_ty, "");
                self.state.push1(extended_value);
            }
            Operator::I32Extend16S => {
                let value = self.state.pop1()?.into_int_value();
                let narrow_value =
                    self.builder
                        .build_int_truncate(value, self.intrinsics.i16_ty, "");
                let extended_value =
                    self.builder
                        .build_int_s_extend(narrow_value, self.intrinsics.i32_ty, "");
                self.state.push1(extended_value);
            }
            Operator::I64Extend8S => {
                let value = self.state.pop1()?.into_int_value();
                let narrow_value =
                    self.builder
                        .build_int_truncate(value, self.intrinsics.i8_ty, "");
                let extended_value =
                    self.builder
                        .build_int_s_extend(narrow_value, self.intrinsics.i64_ty, "");
                self.state.push1(extended_value);
            }
            Operator::I64Extend16S => {
                let value = self.state.pop1()?.into_int_value();
                let narrow_value =
                    self.builder
                        .build_int_truncate(value, self.intrinsics.i16_ty, "");
                let extended_value =
                    self.builder
                        .build_int_s_extend(narrow_value, self.intrinsics.i64_ty, "");
                self.state.push1(extended_value);
            }
            Operator::I64Extend32S => {
                let value = self.state.pop1()?.into_int_value();
                let narrow_value =
                    self.builder
                        .build_int_truncate(value, self.intrinsics.i32_ty, "");
                let extended_value =
                    self.builder
                        .build_int_s_extend(narrow_value, self.intrinsics.i64_ty, "");
                self.state.push1(extended_value);
            }

            /***************************
             * Load and Store instructions.
             * https://github.com/sunfishcode/wasm-reference-manual/blob/master/WebAssembly.md#load-and-store-instructions
             ***************************/
            Operator::I32Load { ref memarg } => {
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i32_ptr_ty,
                    offset,
                    4,
                )?;
                let result = self.builder.build_load(effective_address, "");
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    1,
                    result.as_instruction_value().unwrap(),
                )?;
                self.state.push1(result);
            }
            Operator::I64Load { ref memarg } => {
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i64_ptr_ty,
                    offset,
                    8,
                )?;
                let result = self.builder.build_load(effective_address, "");
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    1,
                    result.as_instruction_value().unwrap(),
                )?;
                self.state.push1(result);
            }
            Operator::F32Load { ref memarg } => {
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.f32_ptr_ty,
                    offset,
                    4,
                )?;
                let result = self.builder.build_load(effective_address, "");
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    1,
                    result.as_instruction_value().unwrap(),
                )?;
                self.state.push1(result);
            }
            Operator::F64Load { ref memarg } => {
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.f64_ptr_ty,
                    offset,
                    8,
                )?;
                let result = self.builder.build_load(effective_address, "");
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    1,
                    result.as_instruction_value().unwrap(),
                )?;
                self.state.push1(result);
            }
            Operator::V128Load { ref memarg } => {
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i128_ptr_ty,
                    offset,
                    16,
                )?;
                let result = self.builder.build_load(effective_address, "");
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    1,
                    result.as_instruction_value().unwrap(),
                )?;
                self.state.push1(result);
            }
            Operator::V128Load8Lane { ref memarg, lane } => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, _i) = self.v128_into_i8x16(v, i);
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(memarg.memory);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i8_ptr_ty,
                    offset,
                    1,
                )?;
                let element = self.builder.build_load(effective_address, "");
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    1,
                    element.as_instruction_value().unwrap(),
                )?;
                let idx = self.intrinsics.i32_ty.const_int(lane.into(), false);
                let res = self.builder.build_insert_element(v, element, idx, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::V128Load16Lane { ref memarg, lane } => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, i) = self.v128_into_i16x8(v, i);
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(memarg.memory);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i16_ptr_ty,
                    offset,
                    2,
                )?;
                let element = self.builder.build_load(effective_address, "");
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    1,
                    element.as_instruction_value().unwrap(),
                )?;
                let idx = self.intrinsics.i32_ty.const_int(lane.into(), false);
                let res = self.builder.build_insert_element(v, element, idx, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1_extra(res, i);
            }
            Operator::V128Load32Lane { ref memarg, lane } => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, i) = self.v128_into_i32x4(v, i);
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(memarg.memory);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i32_ptr_ty,
                    offset,
                    4,
                )?;
                let element = self.builder.build_load(effective_address, "");
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    1,
                    element.as_instruction_value().unwrap(),
                )?;
                let idx = self.intrinsics.i32_ty.const_int(lane.into(), false);
                let res = self.builder.build_insert_element(v, element, idx, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1_extra(res, i);
            }
            Operator::V128Load64Lane { ref memarg, lane } => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, i) = self.v128_into_i64x2(v, i);
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(memarg.memory);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i64_ptr_ty,
                    offset,
                    8,
                )?;
                let element = self.builder.build_load(effective_address, "");
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    1,
                    element.as_instruction_value().unwrap(),
                )?;
                let idx = self.intrinsics.i32_ty.const_int(lane.into(), false);
                let res = self.builder.build_insert_element(v, element, idx, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1_extra(res, i);
            }

            Operator::I32Store { ref memarg } => {
                let value = self.state.pop1()?;
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i32_ptr_ty,
                    offset,
                    4,
                )?;
                let dead_load = self.builder.build_load(effective_address, "");
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    1,
                    dead_load.as_instruction_value().unwrap(),
                )?;
                let store = self.builder.build_store(effective_address, value);
                self.annotate_user_memaccess(memory_index, memarg, 1, store)?;
            }
            Operator::I64Store { ref memarg } => {
                let value = self.state.pop1()?;
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i64_ptr_ty,
                    offset,
                    8,
                )?;
                let dead_load = self.builder.build_load(effective_address, "");
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    1,
                    dead_load.as_instruction_value().unwrap(),
                )?;
                let store = self.builder.build_store(effective_address, value);
                self.annotate_user_memaccess(memory_index, memarg, 1, store)?;
            }
            Operator::F32Store { ref memarg } => {
                let (v, i) = self.state.pop1_extra()?;
                let v = self.apply_pending_canonicalization(v, i);
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.f32_ptr_ty,
                    offset,
                    4,
                )?;
                let dead_load = self.builder.build_load(effective_address, "");
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    1,
                    dead_load.as_instruction_value().unwrap(),
                )?;
                let store = self.builder.build_store(effective_address, v);
                self.annotate_user_memaccess(memory_index, memarg, 1, store)?;
            }
            Operator::F64Store { ref memarg } => {
                let (v, i) = self.state.pop1_extra()?;
                let v = self.apply_pending_canonicalization(v, i);
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.f64_ptr_ty,
                    offset,
                    8,
                )?;
                let dead_load = self.builder.build_load(effective_address, "");
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    1,
                    dead_load.as_instruction_value().unwrap(),
                )?;
                let store = self.builder.build_store(effective_address, v);
                self.annotate_user_memaccess(memory_index, memarg, 1, store)?;
            }
            Operator::V128Store { ref memarg } => {
                let (v, i) = self.state.pop1_extra()?;
                let v = self.apply_pending_canonicalization(v, i);
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i128_ptr_ty,
                    offset,
                    16,
                )?;
                let dead_load = self.builder.build_load(effective_address, "");
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    1,
                    dead_load.as_instruction_value().unwrap(),
                )?;
                let store = self.builder.build_store(effective_address, v);
                self.annotate_user_memaccess(memory_index, memarg, 1, store)?;
            }
            Operator::V128Store8Lane { ref memarg, lane } => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, _i) = self.v128_into_i8x16(v, i);
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(memarg.memory);

                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i8_ptr_ty,
                    offset,
                    1,
                )?;
                let dead_load = self.builder.build_load(effective_address, "");
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    1,
                    dead_load.as_instruction_value().unwrap(),
                )?;
                let idx = self.intrinsics.i32_ty.const_int(lane.into(), false);
                let val = self.builder.build_extract_element(v, idx, "");
                let store = self.builder.build_store(effective_address, val);
                self.annotate_user_memaccess(memory_index, memarg, 1, store)?;
            }
            Operator::V128Store16Lane { ref memarg, lane } => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, _i) = self.v128_into_i16x8(v, i);
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(memarg.memory);

                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i16_ptr_ty,
                    offset,
                    2,
                )?;
                let dead_load = self.builder.build_load(effective_address, "");
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    1,
                    dead_load.as_instruction_value().unwrap(),
                )?;
                let idx = self.intrinsics.i32_ty.const_int(lane.into(), false);
                let val = self.builder.build_extract_element(v, idx, "");
                let store = self.builder.build_store(effective_address, val);
                self.annotate_user_memaccess(memory_index, memarg, 1, store)?;
            }
            Operator::V128Store32Lane { ref memarg, lane } => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, _i) = self.v128_into_i32x4(v, i);
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(memarg.memory);

                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i32_ptr_ty,
                    offset,
                    4,
                )?;
                let dead_load = self.builder.build_load(effective_address, "");
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    1,
                    dead_load.as_instruction_value().unwrap(),
                )?;
                let idx = self.intrinsics.i32_ty.const_int(lane.into(), false);
                let val = self.builder.build_extract_element(v, idx, "");
                let store = self.builder.build_store(effective_address, val);
                self.annotate_user_memaccess(memory_index, memarg, 1, store)?;
            }
            Operator::V128Store64Lane { ref memarg, lane } => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, _i) = self.v128_into_i64x2(v, i);
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(memarg.memory);

                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i64_ptr_ty,
                    offset,
                    8,
                )?;
                let dead_load = self.builder.build_load(effective_address, "");
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    1,
                    dead_load.as_instruction_value().unwrap(),
                )?;
                let idx = self.intrinsics.i32_ty.const_int(lane.into(), false);
                let val = self.builder.build_extract_element(v, idx, "");
                let store = self.builder.build_store(effective_address, val);
                self.annotate_user_memaccess(memory_index, memarg, 1, store)?;
            }
            Operator::I32Load8S { ref memarg } => {
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i8_ptr_ty,
                    offset,
                    1,
                )?;
                let narrow_result = self.builder.build_load(effective_address, "");
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    1,
                    narrow_result.as_instruction_value().unwrap(),
                )?;
                let result = self.builder.build_int_s_extend(
                    narrow_result.into_int_value(),
                    self.intrinsics.i32_ty,
                    "",
                );
                self.state.push1(result);
            }
            Operator::I32Load16S { ref memarg } => {
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i16_ptr_ty,
                    offset,
                    2,
                )?;
                let narrow_result = self.builder.build_load(effective_address, "");
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    1,
                    narrow_result.as_instruction_value().unwrap(),
                )?;
                let result = self.builder.build_int_s_extend(
                    narrow_result.into_int_value(),
                    self.intrinsics.i32_ty,
                    "",
                );
                self.state.push1(result);
            }
            Operator::I64Load8S { ref memarg } => {
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i8_ptr_ty,
                    offset,
                    1,
                )?;
                let narrow_result = self
                    .builder
                    .build_load(effective_address, "")
                    .into_int_value();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    1,
                    narrow_result.as_instruction_value().unwrap(),
                )?;
                let result =
                    self.builder
                        .build_int_s_extend(narrow_result, self.intrinsics.i64_ty, "");
                self.state.push1(result);
            }
            Operator::I64Load16S { ref memarg } => {
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i16_ptr_ty,
                    offset,
                    2,
                )?;
                let narrow_result = self
                    .builder
                    .build_load(effective_address, "")
                    .into_int_value();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    1,
                    narrow_result.as_instruction_value().unwrap(),
                )?;
                let result =
                    self.builder
                        .build_int_s_extend(narrow_result, self.intrinsics.i64_ty, "");
                self.state.push1(result);
            }
            Operator::I64Load32S { ref memarg } => {
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i32_ptr_ty,
                    offset,
                    4,
                )?;
                let narrow_result = self.builder.build_load(effective_address, "");
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    1,
                    narrow_result.as_instruction_value().unwrap(),
                )?;
                let result = self.builder.build_int_s_extend(
                    narrow_result.into_int_value(),
                    self.intrinsics.i64_ty,
                    "",
                );
                self.state.push1(result);
            }

            Operator::I32Load8U { ref memarg } => {
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i8_ptr_ty,
                    offset,
                    1,
                )?;
                let narrow_result = self.builder.build_load(effective_address, "");
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    1,
                    narrow_result.as_instruction_value().unwrap(),
                )?;
                let result = self.builder.build_int_z_extend(
                    narrow_result.into_int_value(),
                    self.intrinsics.i32_ty,
                    "",
                );
                self.state.push1_extra(result, ExtraInfo::arithmetic_f32());
            }
            Operator::I32Load16U { ref memarg } => {
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i16_ptr_ty,
                    offset,
                    2,
                )?;
                let narrow_result = self.builder.build_load(effective_address, "");
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    1,
                    narrow_result.as_instruction_value().unwrap(),
                )?;
                let result = self.builder.build_int_z_extend(
                    narrow_result.into_int_value(),
                    self.intrinsics.i32_ty,
                    "",
                );
                self.state.push1_extra(result, ExtraInfo::arithmetic_f32());
            }
            Operator::I64Load8U { ref memarg } => {
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i8_ptr_ty,
                    offset,
                    1,
                )?;
                let narrow_result = self.builder.build_load(effective_address, "");
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    1,
                    narrow_result.as_instruction_value().unwrap(),
                )?;
                let result = self.builder.build_int_z_extend(
                    narrow_result.into_int_value(),
                    self.intrinsics.i64_ty,
                    "",
                );
                self.state.push1_extra(result, ExtraInfo::arithmetic_f64());
            }
            Operator::I64Load16U { ref memarg } => {
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i16_ptr_ty,
                    offset,
                    2,
                )?;
                let narrow_result = self.builder.build_load(effective_address, "");
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    1,
                    narrow_result.as_instruction_value().unwrap(),
                )?;
                let result = self.builder.build_int_z_extend(
                    narrow_result.into_int_value(),
                    self.intrinsics.i64_ty,
                    "",
                );
                self.state.push1_extra(result, ExtraInfo::arithmetic_f64());
            }
            Operator::I64Load32U { ref memarg } => {
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i32_ptr_ty,
                    offset,
                    4,
                )?;
                let narrow_result = self.builder.build_load(effective_address, "");
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    1,
                    narrow_result.as_instruction_value().unwrap(),
                )?;
                let result = self.builder.build_int_z_extend(
                    narrow_result.into_int_value(),
                    self.intrinsics.i64_ty,
                    "",
                );
                self.state.push1_extra(result, ExtraInfo::arithmetic_f64());
            }

            Operator::I32Store8 { ref memarg } | Operator::I64Store8 { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i8_ptr_ty,
                    offset,
                    1,
                )?;
                let dead_load = self.builder.build_load(effective_address, "");
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    1,
                    dead_load.as_instruction_value().unwrap(),
                )?;
                let narrow_value =
                    self.builder
                        .build_int_truncate(value, self.intrinsics.i8_ty, "");
                let store = self.builder.build_store(effective_address, narrow_value);
                self.annotate_user_memaccess(memory_index, memarg, 1, store)?;
            }
            Operator::I32Store16 { ref memarg } | Operator::I64Store16 { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i16_ptr_ty,
                    offset,
                    2,
                )?;
                let dead_load = self.builder.build_load(effective_address, "");
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    1,
                    dead_load.as_instruction_value().unwrap(),
                )?;
                let narrow_value =
                    self.builder
                        .build_int_truncate(value, self.intrinsics.i16_ty, "");
                let store = self.builder.build_store(effective_address, narrow_value);
                self.annotate_user_memaccess(memory_index, memarg, 1, store)?;
            }
            Operator::I64Store32 { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i32_ptr_ty,
                    offset,
                    4,
                )?;
                let dead_load = self.builder.build_load(effective_address, "");
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    1,
                    dead_load.as_instruction_value().unwrap(),
                )?;
                let narrow_value =
                    self.builder
                        .build_int_truncate(value, self.intrinsics.i32_ty, "");
                let store = self.builder.build_store(effective_address, narrow_value);
                self.annotate_user_memaccess(memory_index, memarg, 1, store)?;
            }
            Operator::I8x16Neg => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, _) = self.v128_into_i8x16(v, i);
                let res = self.builder.build_int_sub(v.get_type().const_zero(), v, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I16x8Neg => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, _) = self.v128_into_i16x8(v, i);
                let res = self.builder.build_int_sub(v.get_type().const_zero(), v, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32x4Neg => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, _) = self.v128_into_i32x4(v, i);
                let res = self.builder.build_int_sub(v.get_type().const_zero(), v, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I64x2Neg => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, _) = self.v128_into_i64x2(v, i);
                let res = self.builder.build_int_sub(v.get_type().const_zero(), v, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::V128Not => {
                let (v, i) = self.state.pop1_extra()?;
                let v = self.apply_pending_canonicalization(v, i).into_int_value();
                let res = self.builder.build_not(v, "");
                self.state.push1(res);
            }
            Operator::V128AnyTrue => {
                // | Operator::I64x2AnyTrue
                // Skip canonicalization, it never changes non-zero values to zero or vice versa.
                let v = self.state.pop1()?.into_int_value();
                let res = self.builder.build_int_compare(
                    IntPredicate::NE,
                    v,
                    v.get_type().const_zero(),
                    "",
                );
                let res = self
                    .builder
                    .build_int_z_extend(res, self.intrinsics.i32_ty, "");
                self.state.push1_extra(
                    res,
                    ExtraInfo::arithmetic_f32() | ExtraInfo::arithmetic_f64(),
                );
            }
            Operator::I8x16AllTrue
            | Operator::I16x8AllTrue
            | Operator::I32x4AllTrue
            | Operator::I64x2AllTrue => {
                let vec_ty = match op {
                    Operator::I8x16AllTrue => self.intrinsics.i8x16_ty,
                    Operator::I16x8AllTrue => self.intrinsics.i16x8_ty,
                    Operator::I32x4AllTrue => self.intrinsics.i32x4_ty,
                    Operator::I64x2AllTrue => self.intrinsics.i64x2_ty,
                    _ => unreachable!(),
                };
                let (v, i) = self.state.pop1_extra()?;
                let v = self.apply_pending_canonicalization(v, i).into_int_value();
                let lane_int_ty = self.context.custom_width_int_type(vec_ty.get_size());
                let vec = self
                    .builder
                    .build_bitcast(v, vec_ty, "vec")
                    .into_vector_value();
                let mask = self.builder.build_int_compare(
                    IntPredicate::NE,
                    vec,
                    vec_ty.const_zero(),
                    "mask",
                );
                let cmask = self
                    .builder
                    .build_bitcast(mask, lane_int_ty, "cmask")
                    .into_int_value();
                let res = self.builder.build_int_compare(
                    IntPredicate::EQ,
                    cmask,
                    lane_int_ty.const_int(std::u64::MAX, true),
                    "",
                );
                let res = self
                    .builder
                    .build_int_z_extend(res, self.intrinsics.i32_ty, "");
                self.state.push1_extra(
                    res,
                    ExtraInfo::arithmetic_f32() | ExtraInfo::arithmetic_f64(),
                );
            }
            Operator::I8x16ExtractLaneS { lane } => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, _) = self.v128_into_i8x16(v, i);
                let idx = self.intrinsics.i32_ty.const_int(lane.into(), false);
                let res = self
                    .builder
                    .build_extract_element(v, idx, "")
                    .into_int_value();
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i32_ty, "");
                self.state.push1(res);
            }
            Operator::I8x16ExtractLaneU { lane } => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, _) = self.v128_into_i8x16(v, i);
                let idx = self.intrinsics.i32_ty.const_int(lane.into(), false);
                let res = self
                    .builder
                    .build_extract_element(v, idx, "")
                    .into_int_value();
                let res = self
                    .builder
                    .build_int_z_extend(res, self.intrinsics.i32_ty, "");
                self.state.push1_extra(res, ExtraInfo::arithmetic_f32());
            }
            Operator::I16x8ExtractLaneS { lane } => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, _) = self.v128_into_i16x8(v, i);
                let idx = self.intrinsics.i32_ty.const_int(lane.into(), false);
                let res = self
                    .builder
                    .build_extract_element(v, idx, "")
                    .into_int_value();
                let res = self
                    .builder
                    .build_int_s_extend(res, self.intrinsics.i32_ty, "");
                self.state.push1(res);
            }
            Operator::I16x8ExtractLaneU { lane } => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, _) = self.v128_into_i16x8(v, i);
                let idx = self.intrinsics.i32_ty.const_int(lane.into(), false);
                let res = self
                    .builder
                    .build_extract_element(v, idx, "")
                    .into_int_value();
                let res = self
                    .builder
                    .build_int_z_extend(res, self.intrinsics.i32_ty, "");
                self.state.push1_extra(res, ExtraInfo::arithmetic_f32());
            }
            Operator::I32x4ExtractLane { lane } => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, i) = self.v128_into_i32x4(v, i);
                let idx = self.intrinsics.i32_ty.const_int(lane.into(), false);
                let res = self.builder.build_extract_element(v, idx, "");
                self.state.push1_extra(res, i);
            }
            Operator::I64x2ExtractLane { lane } => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, i) = self.v128_into_i64x2(v, i);
                let idx = self.intrinsics.i32_ty.const_int(lane.into(), false);
                let res = self.builder.build_extract_element(v, idx, "");
                self.state.push1_extra(res, i);
            }
            Operator::F32x4ExtractLane { lane } => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, i) = self.v128_into_f32x4(v, i);
                let idx = self.intrinsics.i32_ty.const_int(lane.into(), false);
                let res = self.builder.build_extract_element(v, idx, "");
                self.state.push1_extra(res, i);
            }
            Operator::F64x2ExtractLane { lane } => {
                let (v, i) = self.state.pop1_extra()?;
                let (v, i) = self.v128_into_f64x2(v, i);
                let idx = self.intrinsics.i32_ty.const_int(lane.into(), false);
                let res = self.builder.build_extract_element(v, idx, "");
                self.state.push1_extra(res, i);
            }
            Operator::I8x16ReplaceLane { lane } => {
                let ((v1, i1), (v2, _)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i8x16(v1, i1);
                let v2 = v2.into_int_value();
                let v2 = self.builder.build_int_cast(v2, self.intrinsics.i8_ty, "");
                let idx = self.intrinsics.i32_ty.const_int(lane.into(), false);
                let res = self.builder.build_insert_element(v1, v2, idx, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I16x8ReplaceLane { lane } => {
                let ((v1, i1), (v2, _)) = self.state.pop2_extra()?;
                let (v1, _) = self.v128_into_i16x8(v1, i1);
                let v2 = v2.into_int_value();
                let v2 = self.builder.build_int_cast(v2, self.intrinsics.i16_ty, "");
                let idx = self.intrinsics.i32_ty.const_int(lane.into(), false);
                let res = self.builder.build_insert_element(v1, v2, idx, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I32x4ReplaceLane { lane } => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, i1) = self.v128_into_i32x4(v1, i1);
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let v2 = v2.into_int_value();
                let i2 = i2.strip_pending();
                let idx = self.intrinsics.i32_ty.const_int(lane.into(), false);
                let res = self.builder.build_insert_element(v1, v2, idx, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state
                    .push1_extra(res, i1 & i2 & ExtraInfo::arithmetic_f32());
            }
            Operator::I64x2ReplaceLane { lane } => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, i1) = self.v128_into_i64x2(v1, i1);
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let v2 = v2.into_int_value();
                let i2 = i2.strip_pending();
                let idx = self.intrinsics.i32_ty.const_int(lane.into(), false);
                let res = self.builder.build_insert_element(v1, v2, idx, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state
                    .push1_extra(res, i1 & i2 & ExtraInfo::arithmetic_f64());
            }
            Operator::F32x4ReplaceLane { lane } => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, i1) = self.v128_into_f32x4(v1, i1);
                let push_pending_f32_nan_to_result =
                    i1.has_pending_f32_nan() && i2.has_pending_f32_nan();
                let (v1, v2) = if !push_pending_f32_nan_to_result {
                    (
                        self.apply_pending_canonicalization(v1.as_basic_value_enum(), i1)
                            .into_vector_value(),
                        self.apply_pending_canonicalization(v2.as_basic_value_enum(), i2)
                            .into_float_value(),
                    )
                } else {
                    (v1, v2.into_float_value())
                };
                let idx = self.intrinsics.i32_ty.const_int(lane.into(), false);
                let res = self.builder.build_insert_element(v1, v2, idx, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                let info = if push_pending_f32_nan_to_result {
                    ExtraInfo::pending_f32_nan()
                } else {
                    i1.strip_pending() & i2.strip_pending()
                };
                self.state.push1_extra(res, info);
            }
            Operator::F64x2ReplaceLane { lane } => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let (v1, i1) = self.v128_into_f64x2(v1, i1);
                let push_pending_f64_nan_to_result =
                    i1.has_pending_f64_nan() && i2.has_pending_f64_nan();
                let (v1, v2) = if !push_pending_f64_nan_to_result {
                    (
                        self.apply_pending_canonicalization(v1.as_basic_value_enum(), i1)
                            .into_vector_value(),
                        self.apply_pending_canonicalization(v2.as_basic_value_enum(), i2)
                            .into_float_value(),
                    )
                } else {
                    (v1, v2.into_float_value())
                };
                let idx = self.intrinsics.i32_ty.const_int(lane.into(), false);
                let res = self.builder.build_insert_element(v1, v2, idx, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                let info = if push_pending_f64_nan_to_result {
                    ExtraInfo::pending_f64_nan()
                } else {
                    i1.strip_pending() & i2.strip_pending()
                };
                self.state.push1_extra(res, info);
            }
            Operator::I8x16Swizzle => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let v1 = self.apply_pending_canonicalization(v1, i1);
                let v1 = self
                    .builder
                    .build_bitcast(v1, self.intrinsics.i8x16_ty, "")
                    .into_vector_value();
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let v2 = self
                    .builder
                    .build_bitcast(v2, self.intrinsics.i8x16_ty, "")
                    .into_vector_value();
                let lanes = self.intrinsics.i8_ty.const_int(16, false);
                let lanes =
                    self.splat_vector(lanes.as_basic_value_enum(), self.intrinsics.i8x16_ty);
                let mut res = self.intrinsics.i8x16_ty.get_undef();
                let idx_out_of_range = self.builder.build_int_compare(
                    IntPredicate::UGE,
                    v2,
                    lanes,
                    "idx_out_of_range",
                );
                let idx_clamped = self
                    .builder
                    .build_select(
                        idx_out_of_range,
                        self.intrinsics.i8x16_ty.const_zero(),
                        v2,
                        "idx_clamped",
                    )
                    .into_vector_value();
                for i in 0..16 {
                    let idx = self
                        .builder
                        .build_extract_element(
                            idx_clamped,
                            self.intrinsics.i32_ty.const_int(i, false),
                            "idx",
                        )
                        .into_int_value();
                    let replace_with_zero = self
                        .builder
                        .build_extract_element(
                            idx_out_of_range,
                            self.intrinsics.i32_ty.const_int(i, false),
                            "replace_with_zero",
                        )
                        .into_int_value();
                    let elem = self
                        .builder
                        .build_extract_element(v1, idx, "elem")
                        .into_int_value();
                    let elem_or_zero = self.builder.build_select(
                        replace_with_zero,
                        self.intrinsics.i8_zero,
                        elem,
                        "elem_or_zero",
                    );
                    res = self.builder.build_insert_element(
                        res,
                        elem_or_zero,
                        self.intrinsics.i32_ty.const_int(i, false),
                        "",
                    );
                }
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::I8x16Shuffle { lanes } => {
                let ((v1, i1), (v2, i2)) = self.state.pop2_extra()?;
                let v1 = self.apply_pending_canonicalization(v1, i1);
                let v1 = self
                    .builder
                    .build_bitcast(v1, self.intrinsics.i8x16_ty, "")
                    .into_vector_value();
                let v2 = self.apply_pending_canonicalization(v2, i2);
                let v2 = self
                    .builder
                    .build_bitcast(v2, self.intrinsics.i8x16_ty, "")
                    .into_vector_value();
                let mask = VectorType::const_vector(
                    lanes
                        .iter()
                        .map(|l| self.intrinsics.i32_ty.const_int((*l).into(), false))
                        .collect::<Vec<IntValue>>()
                        .as_slice(),
                );
                let res = self.builder.build_shuffle_vector(v1, v2, mask, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::V128Load8x8S { ref memarg } => {
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i64_ptr_ty,
                    offset,
                    8,
                )?;
                let v = self.builder.build_load(effective_address, "");
                let v = self
                    .builder
                    .build_bitcast(v, self.intrinsics.i8_ty.vec_type(8), "")
                    .into_vector_value();
                let res = self
                    .builder
                    .build_int_s_extend(v, self.intrinsics.i16x8_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::V128Load8x8U { ref memarg } => {
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i64_ptr_ty,
                    offset,
                    8,
                )?;
                let v = self.builder.build_load(effective_address, "");
                let v = self
                    .builder
                    .build_bitcast(v, self.intrinsics.i8_ty.vec_type(8), "")
                    .into_vector_value();
                let res = self
                    .builder
                    .build_int_z_extend(v, self.intrinsics.i16x8_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::V128Load16x4S { ref memarg } => {
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i64_ptr_ty,
                    offset,
                    8,
                )?;
                let v = self.builder.build_load(effective_address, "");
                let v = self
                    .builder
                    .build_bitcast(v, self.intrinsics.i16_ty.vec_type(4), "")
                    .into_vector_value();
                let res = self
                    .builder
                    .build_int_s_extend(v, self.intrinsics.i32x4_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::V128Load16x4U { ref memarg } => {
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i64_ptr_ty,
                    offset,
                    8,
                )?;
                let v = self.builder.build_load(effective_address, "");
                let v = self
                    .builder
                    .build_bitcast(v, self.intrinsics.i16_ty.vec_type(4), "")
                    .into_vector_value();
                let res = self
                    .builder
                    .build_int_z_extend(v, self.intrinsics.i32x4_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::V128Load32x2S { ref memarg } => {
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i64_ptr_ty,
                    offset,
                    8,
                )?;
                let v = self.builder.build_load(effective_address, "");
                let v = self
                    .builder
                    .build_bitcast(v, self.intrinsics.i32_ty.vec_type(2), "")
                    .into_vector_value();
                let res = self
                    .builder
                    .build_int_s_extend(v, self.intrinsics.i64x2_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::V128Load32x2U { ref memarg } => {
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i64_ptr_ty,
                    offset,
                    8,
                )?;
                let v = self.builder.build_load(effective_address, "");
                let v = self
                    .builder
                    .build_bitcast(v, self.intrinsics.i32_ty.vec_type(2), "")
                    .into_vector_value();
                let res = self
                    .builder
                    .build_int_z_extend(v, self.intrinsics.i64x2_ty, "");
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::V128Load32Zero { ref memarg } => {
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i32_ptr_ty,
                    offset,
                    4,
                )?;
                let elem = self.builder.build_load(effective_address, "");
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    1,
                    elem.as_instruction_value().unwrap(),
                )?;
                let res = self.builder.build_int_z_extend(
                    elem.into_int_value(),
                    self.intrinsics.i128_ty,
                    "",
                );
                self.state.push1(res);
            }
            Operator::V128Load64Zero { ref memarg } => {
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i64_ptr_ty,
                    offset,
                    8,
                )?;
                let elem = self.builder.build_load(effective_address, "");
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    1,
                    elem.as_instruction_value().unwrap(),
                )?;
                let res = self.builder.build_int_z_extend(
                    elem.into_int_value(),
                    self.intrinsics.i128_ty,
                    "",
                );
                self.state.push1(res);
            }
            Operator::V128Load8Splat { ref memarg } => {
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i8_ptr_ty,
                    offset,
                    1,
                )?;
                let elem = self.builder.build_load(effective_address, "");
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    1,
                    elem.as_instruction_value().unwrap(),
                )?;
                let res = self.splat_vector(elem, self.intrinsics.i8x16_ty);
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::V128Load16Splat { ref memarg } => {
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i16_ptr_ty,
                    offset,
                    2,
                )?;
                let elem = self.builder.build_load(effective_address, "");
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    1,
                    elem.as_instruction_value().unwrap(),
                )?;
                let res = self.splat_vector(elem, self.intrinsics.i16x8_ty);
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::V128Load32Splat { ref memarg } => {
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i32_ptr_ty,
                    offset,
                    4,
                )?;
                let elem = self.builder.build_load(effective_address, "");
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    1,
                    elem.as_instruction_value().unwrap(),
                )?;
                let res = self.splat_vector(elem, self.intrinsics.i32x4_ty);
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::V128Load64Splat { ref memarg } => {
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i64_ptr_ty,
                    offset,
                    8,
                )?;
                let elem = self.builder.build_load(effective_address, "");
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    1,
                    elem.as_instruction_value().unwrap(),
                )?;
                let res = self.splat_vector(elem, self.intrinsics.i64x2_ty);
                let res = self.builder.build_bitcast(res, self.intrinsics.i128_ty, "");
                self.state.push1(res);
            }
            Operator::AtomicFence { flags: _ } => {
                // Fence is a nop.
                //
                // Fence was added to preserve information about fences from
                // source languages. If in the future Wasm extends the memory
                // model, and if we hadn't recorded what fences used to be there,
                // it would lead to data races that weren't present in the
                // original source language.
            }
            Operator::I32AtomicLoad { ref memarg } => {
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i32_ptr_ty,
                    offset,
                    4,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let result = self.builder.build_load(effective_address, "");
                let load = result.as_instruction_value().unwrap();
                self.annotate_user_memaccess(memory_index, memarg, 4, load)?;
                load.set_atomic_ordering(AtomicOrdering::SequentiallyConsistent)
                    .unwrap();
                self.state.push1(result);
            }
            Operator::I64AtomicLoad { ref memarg } => {
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i64_ptr_ty,
                    offset,
                    8,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let result = self.builder.build_load(effective_address, "");
                let load = result.as_instruction_value().unwrap();
                self.annotate_user_memaccess(memory_index, memarg, 8, load)?;
                load.set_atomic_ordering(AtomicOrdering::SequentiallyConsistent)
                    .unwrap();
                self.state.push1(result);
            }
            Operator::I32AtomicLoad8U { ref memarg } => {
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i8_ptr_ty,
                    offset,
                    1,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let narrow_result = self
                    .builder
                    .build_load(effective_address, "")
                    .into_int_value();
                let load = narrow_result.as_instruction_value().unwrap();
                self.annotate_user_memaccess(memory_index, memarg, 1, load)?;
                load.set_atomic_ordering(AtomicOrdering::SequentiallyConsistent)
                    .unwrap();
                let result =
                    self.builder
                        .build_int_z_extend(narrow_result, self.intrinsics.i32_ty, "");
                self.state.push1_extra(result, ExtraInfo::arithmetic_f32());
            }
            Operator::I32AtomicLoad16U { ref memarg } => {
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i16_ptr_ty,
                    offset,
                    2,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let narrow_result = self
                    .builder
                    .build_load(effective_address, "")
                    .into_int_value();
                let load = narrow_result.as_instruction_value().unwrap();
                self.annotate_user_memaccess(memory_index, memarg, 2, load)?;
                load.set_atomic_ordering(AtomicOrdering::SequentiallyConsistent)
                    .unwrap();
                let result =
                    self.builder
                        .build_int_z_extend(narrow_result, self.intrinsics.i32_ty, "");
                self.state.push1_extra(result, ExtraInfo::arithmetic_f32());
            }
            Operator::I64AtomicLoad8U { ref memarg } => {
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i8_ptr_ty,
                    offset,
                    1,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let narrow_result = self
                    .builder
                    .build_load(effective_address, "")
                    .into_int_value();
                let load = narrow_result.as_instruction_value().unwrap();
                self.annotate_user_memaccess(memory_index, memarg, 1, load)?;
                load.set_atomic_ordering(AtomicOrdering::SequentiallyConsistent)
                    .unwrap();
                let result =
                    self.builder
                        .build_int_z_extend(narrow_result, self.intrinsics.i64_ty, "");
                self.state.push1_extra(result, ExtraInfo::arithmetic_f64());
            }
            Operator::I64AtomicLoad16U { ref memarg } => {
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i16_ptr_ty,
                    offset,
                    2,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let narrow_result = self
                    .builder
                    .build_load(effective_address, "")
                    .into_int_value();
                let load = narrow_result.as_instruction_value().unwrap();
                self.annotate_user_memaccess(memory_index, memarg, 2, load)?;
                load.set_atomic_ordering(AtomicOrdering::SequentiallyConsistent)
                    .unwrap();
                let result =
                    self.builder
                        .build_int_z_extend(narrow_result, self.intrinsics.i64_ty, "");
                self.state.push1_extra(result, ExtraInfo::arithmetic_f64());
            }
            Operator::I64AtomicLoad32U { ref memarg } => {
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i32_ptr_ty,
                    offset,
                    4,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let narrow_result = self
                    .builder
                    .build_load(effective_address, "")
                    .into_int_value();
                let load = narrow_result.as_instruction_value().unwrap();
                self.annotate_user_memaccess(memory_index, memarg, 4, load)?;
                load.set_atomic_ordering(AtomicOrdering::SequentiallyConsistent)
                    .unwrap();
                let result =
                    self.builder
                        .build_int_z_extend(narrow_result, self.intrinsics.i64_ty, "");
                self.state.push1_extra(result, ExtraInfo::arithmetic_f64());
            }
            Operator::I32AtomicStore { ref memarg } => {
                let value = self.state.pop1()?;
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i32_ptr_ty,
                    offset,
                    4,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let store = self.builder.build_store(effective_address, value);
                self.annotate_user_memaccess(memory_index, memarg, 4, store)?;
                store
                    .set_atomic_ordering(AtomicOrdering::SequentiallyConsistent)
                    .unwrap();
            }
            Operator::I64AtomicStore { ref memarg } => {
                let value = self.state.pop1()?;
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i64_ptr_ty,
                    offset,
                    8,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let store = self.builder.build_store(effective_address, value);
                self.annotate_user_memaccess(memory_index, memarg, 8, store)?;
                store
                    .set_atomic_ordering(AtomicOrdering::SequentiallyConsistent)
                    .unwrap();
            }
            Operator::I32AtomicStore8 { ref memarg } | Operator::I64AtomicStore8 { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i8_ptr_ty,
                    offset,
                    1,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let narrow_value =
                    self.builder
                        .build_int_truncate(value, self.intrinsics.i8_ty, "");
                let store = self.builder.build_store(effective_address, narrow_value);
                self.annotate_user_memaccess(memory_index, memarg, 1, store)?;
                store
                    .set_atomic_ordering(AtomicOrdering::SequentiallyConsistent)
                    .unwrap();
            }
            Operator::I32AtomicStore16 { ref memarg }
            | Operator::I64AtomicStore16 { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i16_ptr_ty,
                    offset,
                    2,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let narrow_value =
                    self.builder
                        .build_int_truncate(value, self.intrinsics.i16_ty, "");
                let store = self.builder.build_store(effective_address, narrow_value);
                self.annotate_user_memaccess(memory_index, memarg, 2, store)?;
                store
                    .set_atomic_ordering(AtomicOrdering::SequentiallyConsistent)
                    .unwrap();
            }
            Operator::I64AtomicStore32 { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i32_ptr_ty,
                    offset,
                    4,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let narrow_value =
                    self.builder
                        .build_int_truncate(value, self.intrinsics.i32_ty, "");
                let store = self.builder.build_store(effective_address, narrow_value);
                self.annotate_user_memaccess(memory_index, memarg, 4, store)?;
                store
                    .set_atomic_ordering(AtomicOrdering::SequentiallyConsistent)
                    .unwrap();
            }
            Operator::I32AtomicRmw8AddU { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i8_ptr_ty,
                    offset,
                    1,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let narrow_value =
                    self.builder
                        .build_int_truncate(value, self.intrinsics.i8_ty, "");
                let old = self
                    .builder
                    .build_atomicrmw(
                        AtomicRMWBinOp::Add,
                        effective_address,
                        narrow_value,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                tbaa_label(
                    &self.module,
                    self.intrinsics,
                    format!("memory {}", memory_index.as_u32()),
                    old.as_instruction_value().unwrap(),
                );
                let old = self
                    .builder
                    .build_int_z_extend(old, self.intrinsics.i32_ty, "");
                self.state.push1_extra(old, ExtraInfo::arithmetic_f32());
            }
            Operator::I32AtomicRmw16AddU { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i16_ptr_ty,
                    offset,
                    2,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let narrow_value =
                    self.builder
                        .build_int_truncate(value, self.intrinsics.i16_ty, "");
                let old = self
                    .builder
                    .build_atomicrmw(
                        AtomicRMWBinOp::Add,
                        effective_address,
                        narrow_value,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                tbaa_label(
                    &self.module,
                    self.intrinsics,
                    format!("memory {}", memory_index.as_u32()),
                    old.as_instruction_value().unwrap(),
                );
                let old = self
                    .builder
                    .build_int_z_extend(old, self.intrinsics.i32_ty, "");
                self.state.push1_extra(old, ExtraInfo::arithmetic_f32());
            }
            Operator::I32AtomicRmwAdd { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i32_ptr_ty,
                    offset,
                    4,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let old = self
                    .builder
                    .build_atomicrmw(
                        AtomicRMWBinOp::Add,
                        effective_address,
                        value,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                tbaa_label(
                    &self.module,
                    self.intrinsics,
                    format!("memory {}", memory_index.as_u32()),
                    old.as_instruction_value().unwrap(),
                );
                self.state.push1(old);
            }
            Operator::I64AtomicRmw8AddU { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i8_ptr_ty,
                    offset,
                    1,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let narrow_value =
                    self.builder
                        .build_int_truncate(value, self.intrinsics.i8_ty, "");
                let old = self
                    .builder
                    .build_atomicrmw(
                        AtomicRMWBinOp::Add,
                        effective_address,
                        narrow_value,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    0,
                    old.as_instruction_value().unwrap(),
                )?;
                let old = self
                    .builder
                    .build_int_z_extend(old, self.intrinsics.i64_ty, "");
                self.state.push1_extra(old, ExtraInfo::arithmetic_f64());
            }
            Operator::I64AtomicRmw16AddU { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i16_ptr_ty,
                    offset,
                    2,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let narrow_value =
                    self.builder
                        .build_int_truncate(value, self.intrinsics.i16_ty, "");
                let old = self
                    .builder
                    .build_atomicrmw(
                        AtomicRMWBinOp::Add,
                        effective_address,
                        narrow_value,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    0,
                    old.as_instruction_value().unwrap(),
                )?;
                let old = self
                    .builder
                    .build_int_z_extend(old, self.intrinsics.i64_ty, "");
                self.state.push1_extra(old, ExtraInfo::arithmetic_f64());
            }
            Operator::I64AtomicRmw32AddU { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i32_ptr_ty,
                    offset,
                    4,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let narrow_value =
                    self.builder
                        .build_int_truncate(value, self.intrinsics.i32_ty, "");
                let old = self
                    .builder
                    .build_atomicrmw(
                        AtomicRMWBinOp::Add,
                        effective_address,
                        narrow_value,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    0,
                    old.as_instruction_value().unwrap(),
                )?;
                let old = self
                    .builder
                    .build_int_z_extend(old, self.intrinsics.i64_ty, "");
                self.state.push1_extra(old, ExtraInfo::arithmetic_f64());
            }
            Operator::I64AtomicRmwAdd { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i64_ptr_ty,
                    offset,
                    8,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let old = self
                    .builder
                    .build_atomicrmw(
                        AtomicRMWBinOp::Add,
                        effective_address,
                        value,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    0,
                    old.as_instruction_value().unwrap(),
                )?;
                self.state.push1(old);
            }
            Operator::I32AtomicRmw8SubU { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i8_ptr_ty,
                    offset,
                    1,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let narrow_value =
                    self.builder
                        .build_int_truncate(value, self.intrinsics.i8_ty, "");
                let old = self
                    .builder
                    .build_atomicrmw(
                        AtomicRMWBinOp::Sub,
                        effective_address,
                        narrow_value,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    0,
                    old.as_instruction_value().unwrap(),
                )?;
                let old = self
                    .builder
                    .build_int_z_extend(old, self.intrinsics.i32_ty, "");
                self.state.push1_extra(old, ExtraInfo::arithmetic_f32());
            }
            Operator::I32AtomicRmw16SubU { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i16_ptr_ty,
                    offset,
                    2,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let narrow_value =
                    self.builder
                        .build_int_truncate(value, self.intrinsics.i16_ty, "");
                let old = self
                    .builder
                    .build_atomicrmw(
                        AtomicRMWBinOp::Sub,
                        effective_address,
                        narrow_value,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    0,
                    old.as_instruction_value().unwrap(),
                )?;
                let old = self
                    .builder
                    .build_int_z_extend(old, self.intrinsics.i32_ty, "");
                self.state.push1_extra(old, ExtraInfo::arithmetic_f32());
            }
            Operator::I32AtomicRmwSub { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i32_ptr_ty,
                    offset,
                    4,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let old = self
                    .builder
                    .build_atomicrmw(
                        AtomicRMWBinOp::Sub,
                        effective_address,
                        value,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    0,
                    old.as_instruction_value().unwrap(),
                )?;
                self.state.push1(old);
            }
            Operator::I64AtomicRmw8SubU { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i8_ptr_ty,
                    offset,
                    1,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let narrow_value =
                    self.builder
                        .build_int_truncate(value, self.intrinsics.i8_ty, "");
                let old = self
                    .builder
                    .build_atomicrmw(
                        AtomicRMWBinOp::Sub,
                        effective_address,
                        narrow_value,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    0,
                    old.as_instruction_value().unwrap(),
                )?;
                let old = self
                    .builder
                    .build_int_z_extend(old, self.intrinsics.i64_ty, "");
                self.state.push1_extra(old, ExtraInfo::arithmetic_f32());
            }
            Operator::I64AtomicRmw16SubU { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i16_ptr_ty,
                    offset,
                    2,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let narrow_value =
                    self.builder
                        .build_int_truncate(value, self.intrinsics.i16_ty, "");
                let old = self
                    .builder
                    .build_atomicrmw(
                        AtomicRMWBinOp::Sub,
                        effective_address,
                        narrow_value,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    0,
                    old.as_instruction_value().unwrap(),
                )?;
                let old = self
                    .builder
                    .build_int_z_extend(old, self.intrinsics.i64_ty, "");
                self.state.push1_extra(old, ExtraInfo::arithmetic_f64());
            }
            Operator::I64AtomicRmw32SubU { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i32_ptr_ty,
                    offset,
                    4,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let narrow_value =
                    self.builder
                        .build_int_truncate(value, self.intrinsics.i32_ty, "");
                let old = self
                    .builder
                    .build_atomicrmw(
                        AtomicRMWBinOp::Sub,
                        effective_address,
                        narrow_value,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    0,
                    old.as_instruction_value().unwrap(),
                )?;
                let old = self
                    .builder
                    .build_int_z_extend(old, self.intrinsics.i64_ty, "");
                self.state.push1_extra(old, ExtraInfo::arithmetic_f64());
            }
            Operator::I64AtomicRmwSub { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i64_ptr_ty,
                    offset,
                    8,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let old = self
                    .builder
                    .build_atomicrmw(
                        AtomicRMWBinOp::Sub,
                        effective_address,
                        value,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    0,
                    old.as_instruction_value().unwrap(),
                )?;
                self.state.push1(old);
            }
            Operator::I32AtomicRmw8AndU { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i8_ptr_ty,
                    offset,
                    1,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let narrow_value =
                    self.builder
                        .build_int_truncate(value, self.intrinsics.i8_ty, "");
                let old = self
                    .builder
                    .build_atomicrmw(
                        AtomicRMWBinOp::And,
                        effective_address,
                        narrow_value,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    0,
                    old.as_instruction_value().unwrap(),
                )?;
                let old = self
                    .builder
                    .build_int_z_extend(old, self.intrinsics.i32_ty, "");
                self.state.push1_extra(old, ExtraInfo::arithmetic_f32());
            }
            Operator::I32AtomicRmw16AndU { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i16_ptr_ty,
                    offset,
                    2,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let narrow_value =
                    self.builder
                        .build_int_truncate(value, self.intrinsics.i16_ty, "");
                let old = self
                    .builder
                    .build_atomicrmw(
                        AtomicRMWBinOp::And,
                        effective_address,
                        narrow_value,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    0,
                    old.as_instruction_value().unwrap(),
                )?;
                let old = self
                    .builder
                    .build_int_z_extend(old, self.intrinsics.i32_ty, "");
                self.state.push1_extra(old, ExtraInfo::arithmetic_f32());
            }
            Operator::I32AtomicRmwAnd { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i32_ptr_ty,
                    offset,
                    4,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let old = self
                    .builder
                    .build_atomicrmw(
                        AtomicRMWBinOp::And,
                        effective_address,
                        value,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    0,
                    old.as_instruction_value().unwrap(),
                )?;
                self.state.push1(old);
            }
            Operator::I64AtomicRmw8AndU { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i8_ptr_ty,
                    offset,
                    1,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let narrow_value =
                    self.builder
                        .build_int_truncate(value, self.intrinsics.i8_ty, "");
                let old = self
                    .builder
                    .build_atomicrmw(
                        AtomicRMWBinOp::And,
                        effective_address,
                        narrow_value,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    0,
                    old.as_instruction_value().unwrap(),
                )?;
                let old = self
                    .builder
                    .build_int_z_extend(old, self.intrinsics.i64_ty, "");
                self.state.push1_extra(old, ExtraInfo::arithmetic_f64());
            }
            Operator::I64AtomicRmw16AndU { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i16_ptr_ty,
                    offset,
                    2,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let narrow_value =
                    self.builder
                        .build_int_truncate(value, self.intrinsics.i16_ty, "");
                let old = self
                    .builder
                    .build_atomicrmw(
                        AtomicRMWBinOp::And,
                        effective_address,
                        narrow_value,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    0,
                    old.as_instruction_value().unwrap(),
                )?;
                let old = self
                    .builder
                    .build_int_z_extend(old, self.intrinsics.i64_ty, "");
                self.state.push1_extra(old, ExtraInfo::arithmetic_f64());
            }
            Operator::I64AtomicRmw32AndU { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i32_ptr_ty,
                    offset,
                    4,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let narrow_value =
                    self.builder
                        .build_int_truncate(value, self.intrinsics.i32_ty, "");
                let old = self
                    .builder
                    .build_atomicrmw(
                        AtomicRMWBinOp::And,
                        effective_address,
                        narrow_value,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    0,
                    old.as_instruction_value().unwrap(),
                )?;
                let old = self
                    .builder
                    .build_int_z_extend(old, self.intrinsics.i64_ty, "");
                self.state.push1_extra(old, ExtraInfo::arithmetic_f64());
            }
            Operator::I64AtomicRmwAnd { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i64_ptr_ty,
                    offset,
                    8,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let old = self
                    .builder
                    .build_atomicrmw(
                        AtomicRMWBinOp::And,
                        effective_address,
                        value,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    0,
                    old.as_instruction_value().unwrap(),
                )?;
                self.state.push1(old);
            }
            Operator::I32AtomicRmw8OrU { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i8_ptr_ty,
                    offset,
                    1,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let narrow_value =
                    self.builder
                        .build_int_truncate(value, self.intrinsics.i8_ty, "");
                let old = self
                    .builder
                    .build_atomicrmw(
                        AtomicRMWBinOp::Or,
                        effective_address,
                        narrow_value,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    0,
                    old.as_instruction_value().unwrap(),
                )?;
                let old = self
                    .builder
                    .build_int_z_extend(old, self.intrinsics.i32_ty, "");
                self.state.push1_extra(old, ExtraInfo::arithmetic_f32());
            }
            Operator::I32AtomicRmw16OrU { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i16_ptr_ty,
                    offset,
                    2,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let narrow_value =
                    self.builder
                        .build_int_truncate(value, self.intrinsics.i16_ty, "");
                let old = self
                    .builder
                    .build_atomicrmw(
                        AtomicRMWBinOp::Or,
                        effective_address,
                        narrow_value,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    0,
                    old.as_instruction_value().unwrap(),
                )?;
                let old = self
                    .builder
                    .build_int_z_extend(old, self.intrinsics.i32_ty, "");
                self.state.push1_extra(old, ExtraInfo::arithmetic_f32());
            }
            Operator::I32AtomicRmwOr { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i32_ptr_ty,
                    offset,
                    4,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let old = self
                    .builder
                    .build_atomicrmw(
                        AtomicRMWBinOp::Or,
                        effective_address,
                        value,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    0,
                    old.as_instruction_value().unwrap(),
                )?;
                let old = self
                    .builder
                    .build_int_z_extend(old, self.intrinsics.i32_ty, "");
                self.state.push1_extra(old, ExtraInfo::arithmetic_f32());
            }
            Operator::I64AtomicRmw8OrU { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i8_ptr_ty,
                    offset,
                    1,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let narrow_value =
                    self.builder
                        .build_int_truncate(value, self.intrinsics.i8_ty, "");
                let old = self
                    .builder
                    .build_atomicrmw(
                        AtomicRMWBinOp::Or,
                        effective_address,
                        narrow_value,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    0,
                    old.as_instruction_value().unwrap(),
                )?;
                let old = self
                    .builder
                    .build_int_z_extend(old, self.intrinsics.i64_ty, "");
                self.state.push1_extra(old, ExtraInfo::arithmetic_f64());
            }
            Operator::I64AtomicRmw16OrU { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i16_ptr_ty,
                    offset,
                    2,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let narrow_value =
                    self.builder
                        .build_int_truncate(value, self.intrinsics.i16_ty, "");
                let old = self
                    .builder
                    .build_atomicrmw(
                        AtomicRMWBinOp::Or,
                        effective_address,
                        narrow_value,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    0,
                    old.as_instruction_value().unwrap(),
                )?;
                let old = self
                    .builder
                    .build_int_z_extend(old, self.intrinsics.i64_ty, "");
                self.state.push1_extra(old, ExtraInfo::arithmetic_f64());
            }
            Operator::I64AtomicRmw32OrU { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i32_ptr_ty,
                    offset,
                    4,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let narrow_value =
                    self.builder
                        .build_int_truncate(value, self.intrinsics.i32_ty, "");
                let old = self
                    .builder
                    .build_atomicrmw(
                        AtomicRMWBinOp::Or,
                        effective_address,
                        narrow_value,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    0,
                    old.as_instruction_value().unwrap(),
                )?;
                let old = self
                    .builder
                    .build_int_z_extend(old, self.intrinsics.i64_ty, "");
                self.state.push1_extra(old, ExtraInfo::arithmetic_f64());
            }
            Operator::I64AtomicRmwOr { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i64_ptr_ty,
                    offset,
                    8,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let old = self
                    .builder
                    .build_atomicrmw(
                        AtomicRMWBinOp::Or,
                        effective_address,
                        value,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    0,
                    old.as_instruction_value().unwrap(),
                )?;
                self.state.push1(old);
            }
            Operator::I32AtomicRmw8XorU { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i8_ptr_ty,
                    offset,
                    1,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let narrow_value =
                    self.builder
                        .build_int_truncate(value, self.intrinsics.i8_ty, "");
                let old = self
                    .builder
                    .build_atomicrmw(
                        AtomicRMWBinOp::Xor,
                        effective_address,
                        narrow_value,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    0,
                    old.as_instruction_value().unwrap(),
                )?;
                let old = self
                    .builder
                    .build_int_z_extend(old, self.intrinsics.i32_ty, "");
                self.state.push1_extra(old, ExtraInfo::arithmetic_f32());
            }
            Operator::I32AtomicRmw16XorU { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i16_ptr_ty,
                    offset,
                    2,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let narrow_value =
                    self.builder
                        .build_int_truncate(value, self.intrinsics.i16_ty, "");
                let old = self
                    .builder
                    .build_atomicrmw(
                        AtomicRMWBinOp::Xor,
                        effective_address,
                        narrow_value,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    0,
                    old.as_instruction_value().unwrap(),
                )?;
                let old = self
                    .builder
                    .build_int_z_extend(old, self.intrinsics.i32_ty, "");
                self.state.push1_extra(old, ExtraInfo::arithmetic_f32());
            }
            Operator::I32AtomicRmwXor { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i32_ptr_ty,
                    offset,
                    4,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let old = self
                    .builder
                    .build_atomicrmw(
                        AtomicRMWBinOp::Xor,
                        effective_address,
                        value,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    0,
                    old.as_instruction_value().unwrap(),
                )?;
                self.state.push1(old);
            }
            Operator::I64AtomicRmw8XorU { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i8_ptr_ty,
                    offset,
                    1,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let narrow_value =
                    self.builder
                        .build_int_truncate(value, self.intrinsics.i8_ty, "");
                let old = self
                    .builder
                    .build_atomicrmw(
                        AtomicRMWBinOp::Xor,
                        effective_address,
                        narrow_value,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    0,
                    old.as_instruction_value().unwrap(),
                )?;
                let old = self
                    .builder
                    .build_int_z_extend(old, self.intrinsics.i64_ty, "");
                self.state.push1_extra(old, ExtraInfo::arithmetic_f64());
            }
            Operator::I64AtomicRmw16XorU { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i16_ptr_ty,
                    offset,
                    2,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let narrow_value =
                    self.builder
                        .build_int_truncate(value, self.intrinsics.i16_ty, "");
                let old = self
                    .builder
                    .build_atomicrmw(
                        AtomicRMWBinOp::Xor,
                        effective_address,
                        narrow_value,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    0,
                    old.as_instruction_value().unwrap(),
                )?;
                let old = self
                    .builder
                    .build_int_z_extend(old, self.intrinsics.i64_ty, "");
                self.state.push1_extra(old, ExtraInfo::arithmetic_f64());
            }
            Operator::I64AtomicRmw32XorU { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i32_ptr_ty,
                    offset,
                    4,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let narrow_value =
                    self.builder
                        .build_int_truncate(value, self.intrinsics.i32_ty, "");
                let old = self
                    .builder
                    .build_atomicrmw(
                        AtomicRMWBinOp::Xor,
                        effective_address,
                        narrow_value,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    0,
                    old.as_instruction_value().unwrap(),
                )?;
                let old = self
                    .builder
                    .build_int_z_extend(old, self.intrinsics.i64_ty, "");
                self.state.push1_extra(old, ExtraInfo::arithmetic_f64());
            }
            Operator::I64AtomicRmwXor { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i64_ptr_ty,
                    offset,
                    8,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let old = self
                    .builder
                    .build_atomicrmw(
                        AtomicRMWBinOp::Xor,
                        effective_address,
                        value,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    0,
                    old.as_instruction_value().unwrap(),
                )?;
                self.state.push1(old);
            }
            Operator::I32AtomicRmw8XchgU { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i8_ptr_ty,
                    offset,
                    1,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let narrow_value =
                    self.builder
                        .build_int_truncate(value, self.intrinsics.i8_ty, "");
                let old = self
                    .builder
                    .build_atomicrmw(
                        AtomicRMWBinOp::Xchg,
                        effective_address,
                        narrow_value,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    0,
                    old.as_instruction_value().unwrap(),
                )?;
                let old = self
                    .builder
                    .build_int_z_extend(old, self.intrinsics.i32_ty, "");
                self.state.push1_extra(old, ExtraInfo::arithmetic_f32());
            }
            Operator::I32AtomicRmw16XchgU { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i16_ptr_ty,
                    offset,
                    2,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let narrow_value =
                    self.builder
                        .build_int_truncate(value, self.intrinsics.i16_ty, "");
                let old = self
                    .builder
                    .build_atomicrmw(
                        AtomicRMWBinOp::Xchg,
                        effective_address,
                        narrow_value,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    0,
                    old.as_instruction_value().unwrap(),
                )?;
                let old = self
                    .builder
                    .build_int_z_extend(old, self.intrinsics.i32_ty, "");
                self.state.push1_extra(old, ExtraInfo::arithmetic_f32());
            }
            Operator::I32AtomicRmwXchg { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i32_ptr_ty,
                    offset,
                    4,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let old = self
                    .builder
                    .build_atomicrmw(
                        AtomicRMWBinOp::Xchg,
                        effective_address,
                        value,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    0,
                    old.as_instruction_value().unwrap(),
                )?;
                self.state.push1(old);
            }
            Operator::I64AtomicRmw8XchgU { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i8_ptr_ty,
                    offset,
                    1,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let narrow_value =
                    self.builder
                        .build_int_truncate(value, self.intrinsics.i8_ty, "");
                let old = self
                    .builder
                    .build_atomicrmw(
                        AtomicRMWBinOp::Xchg,
                        effective_address,
                        narrow_value,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    0,
                    old.as_instruction_value().unwrap(),
                )?;
                let old = self
                    .builder
                    .build_int_z_extend(old, self.intrinsics.i64_ty, "");
                self.state.push1_extra(old, ExtraInfo::arithmetic_f64());
            }
            Operator::I64AtomicRmw16XchgU { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i16_ptr_ty,
                    offset,
                    2,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let narrow_value =
                    self.builder
                        .build_int_truncate(value, self.intrinsics.i16_ty, "");
                let old = self
                    .builder
                    .build_atomicrmw(
                        AtomicRMWBinOp::Xchg,
                        effective_address,
                        narrow_value,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    0,
                    old.as_instruction_value().unwrap(),
                )?;
                let old = self
                    .builder
                    .build_int_z_extend(old, self.intrinsics.i64_ty, "");
                self.state.push1_extra(old, ExtraInfo::arithmetic_f64());
            }
            Operator::I64AtomicRmw32XchgU { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i32_ptr_ty,
                    offset,
                    4,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let narrow_value =
                    self.builder
                        .build_int_truncate(value, self.intrinsics.i32_ty, "");
                let old = self
                    .builder
                    .build_atomicrmw(
                        AtomicRMWBinOp::Xchg,
                        effective_address,
                        narrow_value,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    0,
                    old.as_instruction_value().unwrap(),
                )?;
                let old = self
                    .builder
                    .build_int_z_extend(old, self.intrinsics.i64_ty, "");
                self.state.push1_extra(old, ExtraInfo::arithmetic_f64());
            }
            Operator::I64AtomicRmwXchg { ref memarg } => {
                let value = self.state.pop1()?.into_int_value();
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i64_ptr_ty,
                    offset,
                    8,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let old = self
                    .builder
                    .build_atomicrmw(
                        AtomicRMWBinOp::Xchg,
                        effective_address,
                        value,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    0,
                    old.as_instruction_value().unwrap(),
                )?;
                self.state.push1(old);
            }
            Operator::I32AtomicRmw8CmpxchgU { ref memarg } => {
                let ((cmp, cmp_info), (new, new_info)) = self.state.pop2_extra()?;
                let cmp = self.apply_pending_canonicalization(cmp, cmp_info);
                let new = self.apply_pending_canonicalization(new, new_info);
                let (cmp, new) = (cmp.into_int_value(), new.into_int_value());
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i8_ptr_ty,
                    offset,
                    1,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let narrow_cmp = self
                    .builder
                    .build_int_truncate(cmp, self.intrinsics.i8_ty, "");
                let narrow_new = self
                    .builder
                    .build_int_truncate(new, self.intrinsics.i8_ty, "");
                let old = self
                    .builder
                    .build_cmpxchg(
                        effective_address,
                        narrow_cmp,
                        narrow_new,
                        AtomicOrdering::SequentiallyConsistent,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    0,
                    old.as_instruction_value().unwrap(),
                )?;
                let old = self
                    .builder
                    .build_extract_value(old, 0, "")
                    .unwrap()
                    .into_int_value();
                let old = self
                    .builder
                    .build_int_z_extend(old, self.intrinsics.i32_ty, "");
                self.state.push1_extra(old, ExtraInfo::arithmetic_f32());
            }
            Operator::I32AtomicRmw16CmpxchgU { ref memarg } => {
                let ((cmp, cmp_info), (new, new_info)) = self.state.pop2_extra()?;
                let cmp = self.apply_pending_canonicalization(cmp, cmp_info);
                let new = self.apply_pending_canonicalization(new, new_info);
                let (cmp, new) = (cmp.into_int_value(), new.into_int_value());
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i16_ptr_ty,
                    offset,
                    2,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let narrow_cmp = self
                    .builder
                    .build_int_truncate(cmp, self.intrinsics.i16_ty, "");
                let narrow_new = self
                    .builder
                    .build_int_truncate(new, self.intrinsics.i16_ty, "");
                let old = self
                    .builder
                    .build_cmpxchg(
                        effective_address,
                        narrow_cmp,
                        narrow_new,
                        AtomicOrdering::SequentiallyConsistent,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    0,
                    old.as_instruction_value().unwrap(),
                )?;
                let old = self
                    .builder
                    .build_extract_value(old, 0, "")
                    .unwrap()
                    .into_int_value();
                let old = self
                    .builder
                    .build_int_z_extend(old, self.intrinsics.i32_ty, "");
                self.state.push1_extra(old, ExtraInfo::arithmetic_f32());
            }
            Operator::I32AtomicRmwCmpxchg { ref memarg } => {
                let ((cmp, cmp_info), (new, new_info)) = self.state.pop2_extra()?;
                let cmp = self.apply_pending_canonicalization(cmp, cmp_info);
                let new = self.apply_pending_canonicalization(new, new_info);
                let (cmp, new) = (cmp.into_int_value(), new.into_int_value());
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i32_ptr_ty,
                    offset,
                    4,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let old = self
                    .builder
                    .build_cmpxchg(
                        effective_address,
                        cmp,
                        new,
                        AtomicOrdering::SequentiallyConsistent,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    0,
                    old.as_instruction_value().unwrap(),
                )?;
                let old = self.builder.build_extract_value(old, 0, "").unwrap();
                self.state.push1(old);
            }
            Operator::I64AtomicRmw8CmpxchgU { ref memarg } => {
                let ((cmp, cmp_info), (new, new_info)) = self.state.pop2_extra()?;
                let cmp = self.apply_pending_canonicalization(cmp, cmp_info);
                let new = self.apply_pending_canonicalization(new, new_info);
                let (cmp, new) = (cmp.into_int_value(), new.into_int_value());
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i8_ptr_ty,
                    offset,
                    1,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let narrow_cmp = self
                    .builder
                    .build_int_truncate(cmp, self.intrinsics.i8_ty, "");
                let narrow_new = self
                    .builder
                    .build_int_truncate(new, self.intrinsics.i8_ty, "");
                let old = self
                    .builder
                    .build_cmpxchg(
                        effective_address,
                        narrow_cmp,
                        narrow_new,
                        AtomicOrdering::SequentiallyConsistent,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    0,
                    old.as_instruction_value().unwrap(),
                )?;
                let old = self
                    .builder
                    .build_extract_value(old, 0, "")
                    .unwrap()
                    .into_int_value();
                let old = self
                    .builder
                    .build_int_z_extend(old, self.intrinsics.i64_ty, "");
                self.state.push1_extra(old, ExtraInfo::arithmetic_f64());
            }
            Operator::I64AtomicRmw16CmpxchgU { ref memarg } => {
                let ((cmp, cmp_info), (new, new_info)) = self.state.pop2_extra()?;
                let cmp = self.apply_pending_canonicalization(cmp, cmp_info);
                let new = self.apply_pending_canonicalization(new, new_info);
                let (cmp, new) = (cmp.into_int_value(), new.into_int_value());
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i16_ptr_ty,
                    offset,
                    2,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let narrow_cmp = self
                    .builder
                    .build_int_truncate(cmp, self.intrinsics.i16_ty, "");
                let narrow_new = self
                    .builder
                    .build_int_truncate(new, self.intrinsics.i16_ty, "");
                let old = self
                    .builder
                    .build_cmpxchg(
                        effective_address,
                        narrow_cmp,
                        narrow_new,
                        AtomicOrdering::SequentiallyConsistent,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    0,
                    old.as_instruction_value().unwrap(),
                )?;
                let old = self
                    .builder
                    .build_extract_value(old, 0, "")
                    .unwrap()
                    .into_int_value();
                let old = self
                    .builder
                    .build_int_z_extend(old, self.intrinsics.i64_ty, "");
                self.state.push1_extra(old, ExtraInfo::arithmetic_f64());
            }
            Operator::I64AtomicRmw32CmpxchgU { ref memarg } => {
                let ((cmp, cmp_info), (new, new_info)) = self.state.pop2_extra()?;
                let cmp = self.apply_pending_canonicalization(cmp, cmp_info);
                let new = self.apply_pending_canonicalization(new, new_info);
                let (cmp, new) = (cmp.into_int_value(), new.into_int_value());
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i32_ptr_ty,
                    offset,
                    4,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let narrow_cmp = self
                    .builder
                    .build_int_truncate(cmp, self.intrinsics.i32_ty, "");
                let narrow_new = self
                    .builder
                    .build_int_truncate(new, self.intrinsics.i32_ty, "");
                let old = self
                    .builder
                    .build_cmpxchg(
                        effective_address,
                        narrow_cmp,
                        narrow_new,
                        AtomicOrdering::SequentiallyConsistent,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    0,
                    old.as_instruction_value().unwrap(),
                )?;
                let old = self
                    .builder
                    .build_extract_value(old, 0, "")
                    .unwrap()
                    .into_int_value();
                let old = self
                    .builder
                    .build_int_z_extend(old, self.intrinsics.i64_ty, "");
                self.state.push1_extra(old, ExtraInfo::arithmetic_f64());
            }
            Operator::I64AtomicRmwCmpxchg { ref memarg } => {
                let ((cmp, cmp_info), (new, new_info)) = self.state.pop2_extra()?;
                let cmp = self.apply_pending_canonicalization(cmp, cmp_info);
                let new = self.apply_pending_canonicalization(new, new_info);
                let (cmp, new) = (cmp.into_int_value(), new.into_int_value());
                let offset = self.state.pop1()?.into_int_value();
                let memory_index = MemoryIndex::from_u32(0);
                let effective_address = self.resolve_memory_ptr(
                    memory_index,
                    memarg,
                    self.intrinsics.i64_ptr_ty,
                    offset,
                    8,
                )?;
                self.trap_if_misaligned(memarg, effective_address);
                let old = self
                    .builder
                    .build_cmpxchg(
                        effective_address,
                        cmp,
                        new,
                        AtomicOrdering::SequentiallyConsistent,
                        AtomicOrdering::SequentiallyConsistent,
                    )
                    .unwrap();
                self.annotate_user_memaccess(
                    memory_index,
                    memarg,
                    0,
                    old.as_instruction_value().unwrap(),
                )?;
                let old = self.builder.build_extract_value(old, 0, "").unwrap();
                self.state.push1(old);
            }

            Operator::MemoryGrow { mem, mem_byte: _ } => {
                let memory_index = MemoryIndex::from_u32(mem);
                let delta = self.state.pop1()?;
                let grow_fn_ptr = self.ctx.memory_grow(memory_index, self.intrinsics);
                let callable_func = inkwell::values::CallableValue::try_from(grow_fn_ptr).unwrap();
                let grow = self.builder.build_call(
                    callable_func,
                    &[
                        vmctx.as_basic_value_enum().into(),
                        delta.into(),
                        self.intrinsics.i32_ty.const_int(mem.into(), false).into(),
                    ],
                    "",
                );
                self.state.push1(grow.try_as_basic_value().left().unwrap());
            }
            Operator::MemorySize { mem, mem_byte: _ } => {
                let memory_index = MemoryIndex::from_u32(mem);
                let size_fn_ptr = self.ctx.memory_size(memory_index, self.intrinsics);
                let callable_func = inkwell::values::CallableValue::try_from(size_fn_ptr).unwrap();
                let size = self.builder.build_call(
                    callable_func,
                    &[
                        vmctx.as_basic_value_enum().into(),
                        self.intrinsics.i32_ty.const_int(mem.into(), false).into(),
                    ],
                    "",
                );
                size.add_attribute(AttributeLoc::Function, self.intrinsics.readonly);
                self.state.push1(size.try_as_basic_value().left().unwrap());
            }
            Operator::MemoryInit { segment, mem } => {
                let (dest, src, len) = self.state.pop3()?;
                let mem = self.intrinsics.i32_ty.const_int(mem.into(), false);
                let segment = self.intrinsics.i32_ty.const_int(segment.into(), false);
                self.builder.build_call(
                    self.intrinsics.memory_init,
                    &[
                        vmctx.as_basic_value_enum().into(),
                        mem.into(),
                        segment.into(),
                        dest.into(),
                        src.into(),
                        len.into(),
                    ],
                    "",
                );
            }
            Operator::DataDrop { segment } => {
                let segment = self.intrinsics.i32_ty.const_int(segment.into(), false);
                self.builder.build_call(
                    self.intrinsics.data_drop,
                    &[vmctx.as_basic_value_enum().into(), segment.into()],
                    "",
                );
            }
            Operator::MemoryCopy { src, dst } => {
                // ignored until we support multiple memories
                let _dst = dst;
                let (memory_copy, src) = if let Some(local_memory_index) = self
                    .wasm_module
                    .local_memory_index(MemoryIndex::from_u32(src))
                {
                    (self.intrinsics.memory_copy, local_memory_index.as_u32())
                } else {
                    (self.intrinsics.imported_memory_copy, src)
                };

                let (dest_pos, src_pos, len) = self.state.pop3()?;
                let src_index = self.intrinsics.i32_ty.const_int(src.into(), false);
                self.builder.build_call(
                    memory_copy,
                    &[
                        vmctx.as_basic_value_enum().into(),
                        src_index.into(),
                        dest_pos.into(),
                        src_pos.into(),
                        len.into(),
                    ],
                    "",
                );
            }
            Operator::MemoryFill { mem } => {
                let (memory_fill, mem) = if let Some(local_memory_index) = self
                    .wasm_module
                    .local_memory_index(MemoryIndex::from_u32(mem))
                {
                    (self.intrinsics.memory_fill, local_memory_index.as_u32())
                } else {
                    (self.intrinsics.imported_memory_fill, mem)
                };

                let (dst, val, len) = self.state.pop3()?;
                let mem_index = self.intrinsics.i32_ty.const_int(mem.into(), false);
                self.builder.build_call(
                    memory_fill,
                    &[
                        vmctx.as_basic_value_enum().into(),
                        mem_index.into(),
                        dst.into(),
                        val.into(),
                        len.into(),
                    ],
                    "",
                );
            }
            /***************************
             * Reference types.
             * https://github.com/WebAssembly/reference-types/blob/master/proposals/reference-types/Overview.md
             ***************************/
            Operator::RefNull { ty } => {
                let ty = wptype_to_type(ty).map_err(to_compile_error)?;
                let ty = type_to_llvm(self.intrinsics, ty)?;
                self.state.push1(ty.const_zero());
            }
            Operator::RefIsNull => {
                let value = self.state.pop1()?.into_pointer_value();
                let is_null = self.builder.build_is_null(value, "");
                let is_null = self
                    .builder
                    .build_int_z_extend(is_null, self.intrinsics.i32_ty, "");
                self.state.push1(is_null);
            }
            Operator::RefFunc { function_index } => {
                let index = self
                    .intrinsics
                    .i32_ty
                    .const_int(function_index.into(), false);
                let value = self
                    .builder
                    .build_call(
                        self.intrinsics.func_ref,
                        &[self.ctx.basic().into(), index.into()],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                self.state.push1(value);
            }
            Operator::TableGet { table } => {
                let table_index = self.intrinsics.i32_ty.const_int(table.into(), false);
                let elem = self.state.pop1()?;
                let table_get = if let Some(_) = self
                    .wasm_module
                    .local_table_index(TableIndex::from_u32(table))
                {
                    self.intrinsics.table_get
                } else {
                    self.intrinsics.imported_table_get
                };
                let value = self
                    .builder
                    .build_call(
                        table_get,
                        &[self.ctx.basic().into(), table_index.into(), elem.into()],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                let value = self.builder.build_bitcast(
                    value,
                    type_to_llvm(
                        self.intrinsics,
                        self.wasm_module
                            .tables
                            .get(TableIndex::from_u32(table))
                            .unwrap()
                            .ty,
                    )?,
                    "",
                );
                self.state.push1(value);
            }
            Operator::TableSet { table } => {
                let table_index = self.intrinsics.i32_ty.const_int(table.into(), false);
                let (elem, value) = self.state.pop2()?;
                let value = self
                    .builder
                    .build_bitcast(value, self.intrinsics.anyref_ty, "");
                let table_set = if let Some(_) = self
                    .wasm_module
                    .local_table_index(TableIndex::from_u32(table))
                {
                    self.intrinsics.table_set
                } else {
                    self.intrinsics.imported_table_set
                };
                self.builder.build_call(
                    table_set,
                    &[
                        self.ctx.basic().into(),
                        table_index.into(),
                        elem.into(),
                        value.into(),
                    ],
                    "",
                );
            }
            Operator::TableCopy {
                dst_table,
                src_table,
            } => {
                let (dst, src, len) = self.state.pop3()?;
                let dst_table = self.intrinsics.i32_ty.const_int(dst_table as u64, false);
                let src_table = self.intrinsics.i32_ty.const_int(src_table as u64, false);
                self.builder.build_call(
                    self.intrinsics.table_copy,
                    &[
                        self.ctx.basic().into(),
                        dst_table.into(),
                        src_table.into(),
                        dst.into(),
                        src.into(),
                        len.into(),
                    ],
                    "",
                );
            }
            Operator::TableInit { segment, table } => {
                let (dst, src, len) = self.state.pop3()?;
                let segment = self.intrinsics.i32_ty.const_int(segment as u64, false);
                let table = self.intrinsics.i32_ty.const_int(table as u64, false);
                self.builder.build_call(
                    self.intrinsics.table_init,
                    &[
                        self.ctx.basic().into(),
                        table.into(),
                        segment.into(),
                        dst.into(),
                        src.into(),
                        len.into(),
                    ],
                    "",
                );
            }
            Operator::ElemDrop { segment } => {
                let segment = self.intrinsics.i32_ty.const_int(segment as u64, false);
                self.builder.build_call(
                    self.intrinsics.elem_drop,
                    &[self.ctx.basic().into(), segment.into()],
                    "",
                );
            }
            Operator::TableFill { table } => {
                let table = self.intrinsics.i32_ty.const_int(table as u64, false);
                let (start, elem, len) = self.state.pop3()?;
                let elem = self
                    .builder
                    .build_bitcast(elem, self.intrinsics.anyref_ty, "");
                self.builder.build_call(
                    self.intrinsics.table_fill,
                    &[
                        self.ctx.basic().into(),
                        table.into(),
                        start.into(),
                        elem.into(),
                        len.into(),
                    ],
                    "",
                );
            }
            Operator::TableGrow { table } => {
                let (elem, delta) = self.state.pop2()?;
                let elem = self
                    .builder
                    .build_bitcast(elem, self.intrinsics.anyref_ty, "");
                let (table_grow, table_index) = if let Some(local_table_index) = self
                    .wasm_module
                    .local_table_index(TableIndex::from_u32(table))
                {
                    (self.intrinsics.table_grow, local_table_index.as_u32())
                } else {
                    (self.intrinsics.imported_table_grow, table)
                };
                let table_index = self.intrinsics.i32_ty.const_int(table_index as u64, false);
                let size = self
                    .builder
                    .build_call(
                        table_grow,
                        &[
                            self.ctx.basic().into(),
                            elem.into(),
                            delta.into(),
                            table_index.into(),
                        ],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                self.state.push1(size);
            }
            Operator::TableSize { table } => {
                let (table_size, table_index) = if let Some(local_table_index) = self
                    .wasm_module
                    .local_table_index(TableIndex::from_u32(table))
                {
                    (self.intrinsics.table_size, local_table_index.as_u32())
                } else {
                    (self.intrinsics.imported_table_size, table)
                };
                let table_index = self.intrinsics.i32_ty.const_int(table_index as u64, false);
                let size = self
                    .builder
                    .build_call(
                        table_size,
                        &[self.ctx.basic().into(), table_index.into()],
                        "",
                    )
                    .try_as_basic_value()
                    .left()
                    .unwrap();
                self.state.push1(size);
            }
            _ => {
                return Err(CompileError::Codegen(format!(
                    "Operator {:?} unimplemented",
                    op
                )));
            }
        }

        Ok(())
    }
}

fn is_f32_arithmetic(bits: u32) -> bool {
    // Mask off sign bit.
    let bits = bits & 0x7FFF_FFFF;
    bits < 0x7FC0_0000
}

fn is_f64_arithmetic(bits: u64) -> bool {
    // Mask off sign bit.
    let bits = bits & 0x7FFF_FFFF_FFFF_FFFF;
    bits < 0x7FF8_0000_0000_0000
}

// Constants for the bounds of truncation operations. These are the least or
// greatest exact floats in either f32 or f64 representation
// greater-than-or-equal-to (for least) or less-than-or-equal-to (for greatest)
// the i32 or i64 or u32 or u64 min (for least) or max (for greatest), when
// rounding towards zero.

/// Least Exact Float (32 bits) greater-than-or-equal-to i32::MIN when rounding towards zero.
const LEF32_GEQ_I32_MIN: u64 = std::i32::MIN as u64;
/// Greatest Exact Float (32 bits) less-than-or-equal-to i32::MAX when rounding towards zero.
const GEF32_LEQ_I32_MAX: u64 = 2147483520; // bits as f32: 0x4eff_ffff
/// Least Exact Float (64 bits) greater-than-or-equal-to i32::MIN when rounding towards zero.
const LEF64_GEQ_I32_MIN: u64 = std::i32::MIN as u64;
/// Greatest Exact Float (64 bits) less-than-or-equal-to i32::MAX when rounding towards zero.
const GEF64_LEQ_I32_MAX: u64 = std::i32::MAX as u64;
/// Least Exact Float (32 bits) greater-than-or-equal-to u32::MIN when rounding towards zero.
const LEF32_GEQ_U32_MIN: u64 = std::u32::MIN as u64;
/// Greatest Exact Float (32 bits) less-than-or-equal-to u32::MAX when rounding towards zero.
const GEF32_LEQ_U32_MAX: u64 = 4294967040; // bits as f32: 0x4f7f_ffff
/// Least Exact Float (64 bits) greater-than-or-equal-to u32::MIN when rounding towards zero.
const LEF64_GEQ_U32_MIN: u64 = std::u32::MIN as u64;
/// Greatest Exact Float (64 bits) less-than-or-equal-to u32::MAX when rounding towards zero.
const GEF64_LEQ_U32_MAX: u64 = 4294967295; // bits as f64: 0x41ef_ffff_ffff_ffff
/// Least Exact Float (32 bits) greater-than-or-equal-to i64::MIN when rounding towards zero.
const LEF32_GEQ_I64_MIN: u64 = std::i64::MIN as u64;
/// Greatest Exact Float (32 bits) less-than-or-equal-to i64::MAX when rounding towards zero.
const GEF32_LEQ_I64_MAX: u64 = 9223371487098961920; // bits as f32: 0x5eff_ffff
/// Least Exact Float (64 bits) greater-than-or-equal-to i64::MIN when rounding towards zero.
const LEF64_GEQ_I64_MIN: u64 = std::i64::MIN as u64;
/// Greatest Exact Float (64 bits) less-than-or-equal-to i64::MAX when rounding towards zero.
const GEF64_LEQ_I64_MAX: u64 = 9223372036854774784; // bits as f64: 0x43df_ffff_ffff_ffff
/// Least Exact Float (32 bits) greater-than-or-equal-to u64::MIN when rounding towards zero.
const LEF32_GEQ_U64_MIN: u64 = std::u64::MIN;
/// Greatest Exact Float (32 bits) less-than-or-equal-to u64::MAX when rounding towards zero.
const GEF32_LEQ_U64_MAX: u64 = 18446742974197923840; // bits as f32: 0x5f7f_ffff
/// Least Exact Float (64 bits) greater-than-or-equal-to u64::MIN when rounding towards zero.
const LEF64_GEQ_U64_MIN: u64 = std::u64::MIN;
/// Greatest Exact Float (64 bits) less-than-or-equal-to u64::MAX when rounding towards zero.
const GEF64_LEQ_U64_MAX: u64 = 18446744073709549568; // bits as f64: 0x43ef_ffff_ffff_ffff

'''
'''--- lib/compiler-llvm/src/translator/intrinsics.rs ---
//! Code for dealing with [LLVM][llvm-intrinsics] and VM intrinsics.
//!
//! VM intrinsics are used to interact with the host VM.
//!
//! [llvm-intrinsics]: https://llvm.org/docs/LangRef.html#intrinsic-functions

use crate::abi::Abi;
use inkwell::values::BasicMetadataValueEnum;
use inkwell::{
    attributes::{Attribute, AttributeLoc},
    builder::Builder,
    context::Context,
    module::{Linkage, Module},
    targets::TargetData,
    types::{
        BasicMetadataTypeEnum, BasicType, BasicTypeEnum, FloatType, IntType, PointerType,
        StructType, VectorType, VoidType,
    },
    values::{
        BasicValue, BasicValueEnum, FloatValue, FunctionValue, InstructionValue, IntValue,
        PointerValue, VectorValue,
    },
    AddressSpace,
};
use std::collections::{hash_map::Entry, HashMap};
use wasmer_compiler::CompileError;
use wasmer_types::entity::{EntityRef, PrimaryMap};
use wasmer_types::{
    FunctionIndex, FunctionType as FuncType, GlobalIndex, LocalFunctionIndex, MemoryIndex,
    ModuleInfo as WasmerCompilerModule, Mutability, SignatureIndex, TableIndex, Type,
};
use wasmer_vm::{MemoryStyle, TrapCode, VMBuiltinFunctionIndex, VMOffsets};

pub fn type_to_llvm_ptr<'ctx>(
    intrinsics: &Intrinsics<'ctx>,
    ty: Type,
) -> Result<PointerType<'ctx>, CompileError> {
    match ty {
        Type::I32 => Ok(intrinsics.i32_ptr_ty),
        Type::I64 => Ok(intrinsics.i64_ptr_ty),
        Type::F32 => Ok(intrinsics.f32_ptr_ty),
        Type::F64 => Ok(intrinsics.f64_ptr_ty),
        Type::V128 => Ok(intrinsics.i128_ptr_ty),
        Type::FuncRef => Ok(intrinsics.funcref_ty.ptr_type(AddressSpace::Generic)),
        Type::ExternRef => Ok(intrinsics.externref_ty.ptr_type(AddressSpace::Generic)),
    }
}

pub fn type_to_llvm<'ctx>(
    intrinsics: &Intrinsics<'ctx>,
    ty: Type,
) -> Result<BasicTypeEnum<'ctx>, CompileError> {
    match ty {
        Type::I32 => Ok(intrinsics.i32_ty.as_basic_type_enum()),
        Type::I64 => Ok(intrinsics.i64_ty.as_basic_type_enum()),
        Type::F32 => Ok(intrinsics.f32_ty.as_basic_type_enum()),
        Type::F64 => Ok(intrinsics.f64_ty.as_basic_type_enum()),
        Type::V128 => Ok(intrinsics.i128_ty.as_basic_type_enum()),
        Type::FuncRef => Ok(intrinsics.funcref_ty.as_basic_type_enum()),
        Type::ExternRef => Ok(intrinsics.externref_ty.as_basic_type_enum()),
    }
}

/// Struct containing LLVM and VM intrinsics.
pub struct Intrinsics<'ctx> {
    pub ctlz_i32: FunctionValue<'ctx>,
    pub ctlz_i64: FunctionValue<'ctx>,

    pub cttz_i32: FunctionValue<'ctx>,
    pub cttz_i64: FunctionValue<'ctx>,

    pub ctpop_i32: FunctionValue<'ctx>,
    pub ctpop_i64: FunctionValue<'ctx>,
    pub ctpop_i8x16: FunctionValue<'ctx>,

    pub fp_rounding_md: BasicMetadataValueEnum<'ctx>,
    pub fp_exception_md: BasicMetadataValueEnum<'ctx>,
    pub fp_ogt_md: BasicMetadataValueEnum<'ctx>,
    pub fp_olt_md: BasicMetadataValueEnum<'ctx>,
    pub fp_uno_md: BasicMetadataValueEnum<'ctx>,

    pub add_f32: FunctionValue<'ctx>,
    pub add_f64: FunctionValue<'ctx>,
    pub add_f32x4: FunctionValue<'ctx>,
    pub add_f64x2: FunctionValue<'ctx>,

    pub sub_f32: FunctionValue<'ctx>,
    pub sub_f64: FunctionValue<'ctx>,
    pub sub_f32x4: FunctionValue<'ctx>,
    pub sub_f64x2: FunctionValue<'ctx>,

    pub mul_f32: FunctionValue<'ctx>,
    pub mul_f64: FunctionValue<'ctx>,
    pub mul_f32x4: FunctionValue<'ctx>,
    pub mul_f64x2: FunctionValue<'ctx>,

    pub div_f32: FunctionValue<'ctx>,
    pub div_f64: FunctionValue<'ctx>,
    pub div_f32x4: FunctionValue<'ctx>,
    pub div_f64x2: FunctionValue<'ctx>,

    pub sqrt_f32: FunctionValue<'ctx>,
    pub sqrt_f64: FunctionValue<'ctx>,
    pub sqrt_f32x4: FunctionValue<'ctx>,
    pub sqrt_f64x2: FunctionValue<'ctx>,

    pub cmp_f32: FunctionValue<'ctx>,
    pub cmp_f64: FunctionValue<'ctx>,
    pub cmp_f32x4: FunctionValue<'ctx>,
    pub cmp_f64x2: FunctionValue<'ctx>,

    pub ceil_f32: FunctionValue<'ctx>,
    pub ceil_f64: FunctionValue<'ctx>,
    pub ceil_f32x4: FunctionValue<'ctx>,
    pub ceil_f64x2: FunctionValue<'ctx>,

    pub floor_f32: FunctionValue<'ctx>,
    pub floor_f64: FunctionValue<'ctx>,
    pub floor_f32x4: FunctionValue<'ctx>,
    pub floor_f64x2: FunctionValue<'ctx>,

    pub trunc_f32: FunctionValue<'ctx>,
    pub trunc_f64: FunctionValue<'ctx>,
    pub trunc_f32x4: FunctionValue<'ctx>,
    pub trunc_f64x2: FunctionValue<'ctx>,

    pub fpext_f32: FunctionValue<'ctx>,
    pub fptrunc_f64: FunctionValue<'ctx>,

    pub nearbyint_f32: FunctionValue<'ctx>,
    pub nearbyint_f64: FunctionValue<'ctx>,
    pub nearbyint_f32x4: FunctionValue<'ctx>,
    pub nearbyint_f64x2: FunctionValue<'ctx>,

    pub fabs_f32: FunctionValue<'ctx>,
    pub fabs_f64: FunctionValue<'ctx>,
    pub fabs_f32x4: FunctionValue<'ctx>,
    pub fabs_f64x2: FunctionValue<'ctx>,

    pub copysign_f32: FunctionValue<'ctx>,
    pub copysign_f64: FunctionValue<'ctx>,
    pub copysign_f32x4: FunctionValue<'ctx>,
    pub copysign_f64x2: FunctionValue<'ctx>,

    pub sadd_sat_i8x16: FunctionValue<'ctx>,
    pub sadd_sat_i16x8: FunctionValue<'ctx>,
    pub uadd_sat_i8x16: FunctionValue<'ctx>,
    pub uadd_sat_i16x8: FunctionValue<'ctx>,

    pub ssub_sat_i8x16: FunctionValue<'ctx>,
    pub ssub_sat_i16x8: FunctionValue<'ctx>,
    pub usub_sat_i8x16: FunctionValue<'ctx>,
    pub usub_sat_i16x8: FunctionValue<'ctx>,

    pub expect_i1: FunctionValue<'ctx>,
    pub trap: FunctionValue<'ctx>,
    pub debug_trap: FunctionValue<'ctx>,

    pub personality: FunctionValue<'ctx>,
    pub readonly: Attribute,
    pub stack_probe: Attribute,

    pub void_ty: VoidType<'ctx>,
    pub i1_ty: IntType<'ctx>,
    pub i2_ty: IntType<'ctx>,
    pub i4_ty: IntType<'ctx>,
    pub i8_ty: IntType<'ctx>,
    pub i16_ty: IntType<'ctx>,
    pub i32_ty: IntType<'ctx>,
    pub i64_ty: IntType<'ctx>,
    pub i128_ty: IntType<'ctx>,
    pub isize_ty: IntType<'ctx>,
    pub f32_ty: FloatType<'ctx>,
    pub f64_ty: FloatType<'ctx>,

    pub i1x128_ty: VectorType<'ctx>,
    pub i8x16_ty: VectorType<'ctx>,
    pub i16x8_ty: VectorType<'ctx>,
    pub i32x4_ty: VectorType<'ctx>,
    pub i64x2_ty: VectorType<'ctx>,
    pub f32x4_ty: VectorType<'ctx>,
    pub f64x2_ty: VectorType<'ctx>,
    pub i32x8_ty: VectorType<'ctx>,

    pub i8_ptr_ty: PointerType<'ctx>,
    pub i16_ptr_ty: PointerType<'ctx>,
    pub i32_ptr_ty: PointerType<'ctx>,
    pub i64_ptr_ty: PointerType<'ctx>,
    pub i128_ptr_ty: PointerType<'ctx>,
    pub isize_ptr_ty: PointerType<'ctx>,
    pub f32_ptr_ty: PointerType<'ctx>,
    pub f64_ptr_ty: PointerType<'ctx>,

    pub anyfunc_ty: StructType<'ctx>,

    pub funcref_ty: PointerType<'ctx>,
    pub externref_ty: PointerType<'ctx>,
    pub anyref_ty: PointerType<'ctx>,

    pub i1_zero: IntValue<'ctx>,
    pub i8_zero: IntValue<'ctx>,
    pub i32_zero: IntValue<'ctx>,
    pub i64_zero: IntValue<'ctx>,
    pub i128_zero: IntValue<'ctx>,
    pub isize_zero: IntValue<'ctx>,
    pub f32_zero: FloatValue<'ctx>,
    pub f64_zero: FloatValue<'ctx>,
    pub f32x4_zero: VectorValue<'ctx>,
    pub f64x2_zero: VectorValue<'ctx>,
    pub i32_consts: [IntValue<'ctx>; 16],

    pub trap_unreachable: BasicValueEnum<'ctx>,
    pub trap_call_indirect_null: BasicValueEnum<'ctx>,
    pub trap_call_indirect_sig: BasicValueEnum<'ctx>,
    pub trap_memory_oob: BasicValueEnum<'ctx>,
    pub trap_illegal_arithmetic: BasicValueEnum<'ctx>,
    pub trap_integer_division_by_zero: BasicValueEnum<'ctx>,
    pub trap_bad_conversion_to_integer: BasicValueEnum<'ctx>,
    pub trap_unaligned_atomic: BasicValueEnum<'ctx>,
    pub trap_table_access_oob: BasicValueEnum<'ctx>,

    pub experimental_stackmap: FunctionValue<'ctx>,

    // VM libcalls.
    pub table_copy: FunctionValue<'ctx>,
    pub table_init: FunctionValue<'ctx>,
    pub table_fill: FunctionValue<'ctx>,
    pub table_size: FunctionValue<'ctx>,
    pub imported_table_size: FunctionValue<'ctx>,
    pub table_get: FunctionValue<'ctx>,
    pub imported_table_get: FunctionValue<'ctx>,
    pub table_set: FunctionValue<'ctx>,
    pub imported_table_set: FunctionValue<'ctx>,
    pub table_grow: FunctionValue<'ctx>,
    pub imported_table_grow: FunctionValue<'ctx>,
    pub memory_init: FunctionValue<'ctx>,
    pub data_drop: FunctionValue<'ctx>,
    pub func_ref: FunctionValue<'ctx>,
    pub elem_drop: FunctionValue<'ctx>,
    pub memory_copy: FunctionValue<'ctx>,
    pub imported_memory_copy: FunctionValue<'ctx>,
    pub memory_fill: FunctionValue<'ctx>,
    pub imported_memory_fill: FunctionValue<'ctx>,

    pub throw_trap: FunctionValue<'ctx>,

    // VM builtins.
    pub vmfunction_import_ptr_ty: PointerType<'ctx>,
    pub vmfunction_import_body_element: u32,
    pub vmfunction_import_vmctx_element: u32,

    pub vmmemory_definition_ptr_ty: PointerType<'ctx>,
    pub vmmemory_definition_base_element: u32,
    pub vmmemory_definition_current_length_element: u32,

    pub memory32_grow_ptr_ty: PointerType<'ctx>,
    pub imported_memory32_grow_ptr_ty: PointerType<'ctx>,
    pub memory32_size_ptr_ty: PointerType<'ctx>,
    pub imported_memory32_size_ptr_ty: PointerType<'ctx>,

    // Pointer to the VM.
    pub ctx_ptr_ty: PointerType<'ctx>,
}

impl<'ctx> Intrinsics<'ctx> {
    /// Create an [`Intrinsics`] for the given [`Context`].
    pub fn declare(
        module: &Module<'ctx>,
        context: &'ctx Context,
        target_data: &TargetData,
    ) -> Self {
        let void_ty = context.void_type();
        let i1_ty = context.bool_type();
        let i2_ty = context.custom_width_int_type(2);
        let i4_ty = context.custom_width_int_type(4);
        let i8_ty = context.i8_type();
        let i16_ty = context.i16_type();
        let i32_ty = context.i32_type();
        let i64_ty = context.i64_type();
        let i128_ty = context.i128_type();
        let isize_ty = context.ptr_sized_int_type(target_data, None);
        let f32_ty = context.f32_type();
        let f64_ty = context.f64_type();

        let i1x4_ty = i1_ty.vec_type(4);
        let i1x2_ty = i1_ty.vec_type(2);
        let i1x128_ty = i1_ty.vec_type(128);
        let i8x16_ty = i8_ty.vec_type(16);
        let i16x8_ty = i16_ty.vec_type(8);
        let i32x4_ty = i32_ty.vec_type(4);
        let i64x2_ty = i64_ty.vec_type(2);
        let f32x4_ty = f32_ty.vec_type(4);
        let f64x2_ty = f64_ty.vec_type(2);
        let i32x8_ty = i32_ty.vec_type(8);

        let i8_ptr_ty = i8_ty.ptr_type(AddressSpace::Generic);
        let i16_ptr_ty = i16_ty.ptr_type(AddressSpace::Generic);
        let i32_ptr_ty = i32_ty.ptr_type(AddressSpace::Generic);
        let i64_ptr_ty = i64_ty.ptr_type(AddressSpace::Generic);
        let i128_ptr_ty = i128_ty.ptr_type(AddressSpace::Generic);
        let isize_ptr_ty = isize_ty.ptr_type(AddressSpace::Generic);
        let f32_ptr_ty = f32_ty.ptr_type(AddressSpace::Generic);
        let f64_ptr_ty = f64_ty.ptr_type(AddressSpace::Generic);

        let i1_zero = i1_ty.const_int(0, false);
        let i8_zero = i8_ty.const_int(0, false);
        let i32_zero = i32_ty.const_int(0, false);
        let i64_zero = i64_ty.const_int(0, false);
        let i128_zero = i128_ty.const_int(0, false);
        let isize_zero = isize_ty.const_int(0, false);
        let f32_zero = f32_ty.const_float(0.0);
        let f64_zero = f64_ty.const_float(0.0);
        let f32x4_zero = f32x4_ty.const_zero();
        let f64x2_zero = f64x2_ty.const_zero();
        let i32_consts = [
            i32_ty.const_int(0, false),
            i32_ty.const_int(1, false),
            i32_ty.const_int(2, false),
            i32_ty.const_int(3, false),
            i32_ty.const_int(4, false),
            i32_ty.const_int(5, false),
            i32_ty.const_int(6, false),
            i32_ty.const_int(7, false),
            i32_ty.const_int(8, false),
            i32_ty.const_int(9, false),
            i32_ty.const_int(10, false),
            i32_ty.const_int(11, false),
            i32_ty.const_int(12, false),
            i32_ty.const_int(13, false),
            i32_ty.const_int(14, false),
            i32_ty.const_int(15, false),
        ];

        let md_ty = context.metadata_type();

        let i8_ptr_ty_basic = i8_ptr_ty.as_basic_type_enum();

        let i1_ty_basic_md: BasicMetadataTypeEnum = i1_ty.into();
        let i32_ty_basic_md: BasicMetadataTypeEnum = i32_ty.into();
        let i64_ty_basic_md: BasicMetadataTypeEnum = i64_ty.into();
        let f32_ty_basic_md: BasicMetadataTypeEnum = f32_ty.into();
        let f64_ty_basic_md: BasicMetadataTypeEnum = f64_ty.into();
        let i8x16_ty_basic_md: BasicMetadataTypeEnum = i8x16_ty.into();
        let i16x8_ty_basic_md: BasicMetadataTypeEnum = i16x8_ty.into();
        let f32x4_ty_basic_md: BasicMetadataTypeEnum = f32x4_ty.into();
        let f64x2_ty_basic_md: BasicMetadataTypeEnum = f64x2_ty.into();
        let md_ty_basic_md: BasicMetadataTypeEnum = md_ty.into();

        let ctx_ty = i8_ty;
        let ctx_ptr_ty = ctx_ty.ptr_type(AddressSpace::Generic);
        let ctx_ptr_ty_basic = ctx_ptr_ty.as_basic_type_enum();
        let ctx_ptr_ty_basic_md: BasicMetadataTypeEnum = ctx_ptr_ty.into();

        let sigindex_ty = i32_ty;

        let anyfunc_ty = context.struct_type(
            &[i8_ptr_ty_basic, sigindex_ty.into(), ctx_ptr_ty_basic],
            false,
        );
        let funcref_ty = anyfunc_ty.ptr_type(AddressSpace::Generic);
        let externref_ty = funcref_ty;
        let anyref_ty = i8_ptr_ty;
        let anyref_ty_basic_md: BasicMetadataTypeEnum = anyref_ty.into();

        let ret_i8x16_take_i8x16 = i8x16_ty.fn_type(&[i8x16_ty_basic_md], false);
        let ret_i8x16_take_i8x16_i8x16 =
            i8x16_ty.fn_type(&[i8x16_ty_basic_md, i8x16_ty_basic_md], false);
        let ret_i16x8_take_i16x8_i16x8 =
            i16x8_ty.fn_type(&[i16x8_ty_basic_md, i16x8_ty_basic_md], false);

        let ret_i32_take_i32_i1 = i32_ty.fn_type(&[i32_ty_basic_md, i1_ty_basic_md], false);
        let ret_i64_take_i64_i1 = i64_ty.fn_type(&[i64_ty_basic_md, i1_ty_basic_md], false);

        let ret_i32_take_i32 = i32_ty.fn_type(&[i32_ty_basic_md], false);
        let ret_i64_take_i64 = i64_ty.fn_type(&[i64_ty_basic_md], false);

        let ret_f32_take_f32 = f32_ty.fn_type(&[f32_ty_basic_md], false);
        let ret_f64_take_f64 = f64_ty.fn_type(&[f64_ty_basic_md], false);
        let ret_f32x4_take_f32x4 = f32x4_ty.fn_type(&[f32x4_ty_basic_md], false);
        let ret_f64x2_take_f64x2 = f64x2_ty.fn_type(&[f64x2_ty_basic_md], false);

        let ret_f32_take_f32_f32 = f32_ty.fn_type(&[f32_ty_basic_md, f32_ty_basic_md], false);
        let ret_f64_take_f64_f64 = f64_ty.fn_type(&[f64_ty_basic_md, f64_ty_basic_md], false);
        let ret_f32x4_take_f32x4_f32x4 =
            f32x4_ty.fn_type(&[f32x4_ty_basic_md, f32x4_ty_basic_md], false);
        let ret_f64x2_take_f64x2_f64x2 =
            f64x2_ty.fn_type(&[f64x2_ty_basic_md, f64x2_ty_basic_md], false);

        let ret_f64_take_f32_md = f64_ty.fn_type(&[f32_ty_basic_md, md_ty_basic_md], false);
        let ret_f32_take_f64_md_md =
            f32_ty.fn_type(&[f64_ty_basic_md, md_ty_basic_md, md_ty_basic_md], false);

        let ret_i1_take_i1_i1 = i1_ty.fn_type(&[i1_ty_basic_md, i1_ty_basic_md], false);

        let ret_i1_take_f32_f32_md_md = i1_ty.fn_type(
            &[
                f32_ty_basic_md,
                f32_ty_basic_md,
                md_ty_basic_md,
                md_ty_basic_md,
            ],
            false,
        );
        let ret_i1_take_f64_f64_md_md = i1_ty.fn_type(
            &[
                f64_ty_basic_md,
                f64_ty_basic_md,
                md_ty_basic_md,
                md_ty_basic_md,
            ],
            false,
        );
        let ret_i1x4_take_f32x4_f32x4_md_md = i1x4_ty.fn_type(
            &[
                f32x4_ty_basic_md,
                f32x4_ty_basic_md,
                md_ty_basic_md,
                md_ty_basic_md,
            ],
            false,
        );
        let ret_i1x2_take_f64x2_f64x2_md_md = i1x2_ty.fn_type(
            &[
                f64x2_ty_basic_md,
                f64x2_ty_basic_md,
                md_ty_basic_md,
                md_ty_basic_md,
            ],
            false,
        );

        let ret_f32_take_f32_f32_md_md = f32_ty.fn_type(
            &[
                f32_ty_basic_md,
                f32_ty_basic_md,
                md_ty_basic_md,
                md_ty_basic_md,
            ],
            false,
        );
        let ret_f64_take_f64_f64_md_md = f64_ty.fn_type(
            &[
                f64_ty_basic_md,
                f64_ty_basic_md,
                md_ty_basic_md,
                md_ty_basic_md,
            ],
            false,
        );
        let ret_f32x4_take_f32x4_f32x4_md_md = f32x4_ty.fn_type(
            &[
                f32x4_ty_basic_md,
                f32x4_ty_basic_md,
                md_ty_basic_md,
                md_ty_basic_md,
            ],
            false,
        );
        let ret_f64x2_take_f64x2_f64x2_md_md = f64x2_ty.fn_type(
            &[
                f64x2_ty_basic_md,
                f64x2_ty_basic_md,
                md_ty_basic_md,
                md_ty_basic_md,
            ],
            false,
        );

        let intrinsics = Self {
            ctlz_i32: module.add_function("llvm.ctlz.i32", ret_i32_take_i32_i1, None),
            ctlz_i64: module.add_function("llvm.ctlz.i64", ret_i64_take_i64_i1, None),

            cttz_i32: module.add_function("llvm.cttz.i32", ret_i32_take_i32_i1, None),
            cttz_i64: module.add_function("llvm.cttz.i64", ret_i64_take_i64_i1, None),

            ctpop_i32: module.add_function("llvm.ctpop.i32", ret_i32_take_i32, None),
            ctpop_i64: module.add_function("llvm.ctpop.i64", ret_i64_take_i64, None),
            ctpop_i8x16: module.add_function("llvm.ctpop.v16i8", ret_i8x16_take_i8x16, None),

            fp_rounding_md: context.metadata_string("round.tonearest").into(),
            fp_exception_md: context.metadata_string("fpexcept.strict").into(),

            fp_ogt_md: context.metadata_string("ogt").into(),
            fp_olt_md: context.metadata_string("olt").into(),
            fp_uno_md: context.metadata_string("uno").into(),

            sqrt_f32: module.add_function("llvm.sqrt.f32", ret_f32_take_f32, None),
            sqrt_f64: module.add_function("llvm.sqrt.f64", ret_f64_take_f64, None),
            sqrt_f32x4: module.add_function("llvm.sqrt.v4f32", ret_f32x4_take_f32x4, None),
            sqrt_f64x2: module.add_function("llvm.sqrt.v2f64", ret_f64x2_take_f64x2, None),

            ceil_f32: module.add_function("llvm.ceil.f32", ret_f32_take_f32, None),
            ceil_f64: module.add_function("llvm.ceil.f64", ret_f64_take_f64, None),
            ceil_f32x4: module.add_function("llvm.ceil.v4f32", ret_f32x4_take_f32x4, None),
            ceil_f64x2: module.add_function("llvm.ceil.v2f64", ret_f64x2_take_f64x2, None),

            floor_f32: module.add_function("llvm.floor.f32", ret_f32_take_f32, None),
            floor_f64: module.add_function("llvm.floor.f64", ret_f64_take_f64, None),
            floor_f32x4: module.add_function("llvm.floor.v4f32", ret_f32x4_take_f32x4, None),
            floor_f64x2: module.add_function("llvm.floor.v2f64", ret_f64x2_take_f64x2, None),

            trunc_f32: module.add_function("llvm.trunc.f32", ret_f32_take_f32, None),
            trunc_f64: module.add_function("llvm.trunc.f64", ret_f64_take_f64, None),
            trunc_f32x4: module.add_function("llvm.trunc.v4f32", ret_f32x4_take_f32x4, None),
            trunc_f64x2: module.add_function("llvm.trunc.v2f64", ret_f64x2_take_f64x2, None),

            nearbyint_f32: module.add_function("llvm.nearbyint.f32", ret_f32_take_f32, None),
            nearbyint_f64: module.add_function("llvm.nearbyint.f64", ret_f64_take_f64, None),
            nearbyint_f32x4: module.add_function(
                "llvm.nearbyint.v4f32",
                ret_f32x4_take_f32x4,
                None,
            ),
            nearbyint_f64x2: module.add_function(
                "llvm.nearbyint.v2f64",
                ret_f64x2_take_f64x2,
                None,
            ),

            add_f32: module.add_function(
                "llvm.experimental.constrained.fadd.f32",
                ret_f32_take_f32_f32_md_md,
                None,
            ),
            add_f64: module.add_function(
                "llvm.experimental.constrained.fadd.f64",
                ret_f64_take_f64_f64_md_md,
                None,
            ),
            add_f32x4: module.add_function(
                "llvm.experimental.constrained.fadd.v4f32",
                ret_f32x4_take_f32x4_f32x4_md_md,
                None,
            ),
            add_f64x2: module.add_function(
                "llvm.experimental.constrained.fadd.v2f64",
                ret_f64x2_take_f64x2_f64x2_md_md,
                None,
            ),

            sub_f32: module.add_function(
                "llvm.experimental.constrained.fsub.f32",
                ret_f32_take_f32_f32_md_md,
                None,
            ),
            sub_f64: module.add_function(
                "llvm.experimental.constrained.fsub.f64",
                ret_f64_take_f64_f64_md_md,
                None,
            ),
            sub_f32x4: module.add_function(
                "llvm.experimental.constrained.fsub.v4f32",
                ret_f32x4_take_f32x4_f32x4_md_md,
                None,
            ),
            sub_f64x2: module.add_function(
                "llvm.experimental.constrained.fsub.v2f64",
                ret_f64x2_take_f64x2_f64x2_md_md,
                None,
            ),

            mul_f32: module.add_function(
                "llvm.experimental.constrained.fmul.f32",
                ret_f32_take_f32_f32_md_md,
                None,
            ),
            mul_f64: module.add_function(
                "llvm.experimental.constrained.fmul.f64",
                ret_f64_take_f64_f64_md_md,
                None,
            ),
            mul_f32x4: module.add_function(
                "llvm.experimental.constrained.fmul.v4f32",
                ret_f32x4_take_f32x4_f32x4_md_md,
                None,
            ),
            mul_f64x2: module.add_function(
                "llvm.experimental.constrained.fmul.v2f64",
                ret_f64x2_take_f64x2_f64x2_md_md,
                None,
            ),

            div_f32: module.add_function(
                "llvm.experimental.constrained.fdiv.f32",
                ret_f32_take_f32_f32_md_md,
                None,
            ),
            div_f64: module.add_function(
                "llvm.experimental.constrained.fdiv.f64",
                ret_f64_take_f64_f64_md_md,
                None,
            ),
            div_f32x4: module.add_function(
                "llvm.experimental.constrained.fdiv.v4f32",
                ret_f32x4_take_f32x4_f32x4_md_md,
                None,
            ),
            div_f64x2: module.add_function(
                "llvm.experimental.constrained.fdiv.v2f64",
                ret_f64x2_take_f64x2_f64x2_md_md,
                None,
            ),

            cmp_f32: module.add_function(
                "llvm.experimental.constrained.fcmp.f32",
                ret_i1_take_f32_f32_md_md,
                None,
            ),
            cmp_f64: module.add_function(
                "llvm.experimental.constrained.fcmp.f64",
                ret_i1_take_f64_f64_md_md,
                None,
            ),
            cmp_f32x4: module.add_function(
                "llvm.experimental.constrained.fcmp.v4f32",
                ret_i1x4_take_f32x4_f32x4_md_md,
                None,
            ),
            cmp_f64x2: module.add_function(
                "llvm.experimental.constrained.fcmp.v2f64",
                ret_i1x2_take_f64x2_f64x2_md_md,
                None,
            ),

            fpext_f32: module.add_function(
                "llvm.experimental.constrained.fpext.f64.f32",
                ret_f64_take_f32_md,
                None,
            ),
            fptrunc_f64: module.add_function(
                "llvm.experimental.constrained.fptrunc.f32.f64",
                ret_f32_take_f64_md_md,
                None,
            ),

            fabs_f32: module.add_function("llvm.fabs.f32", ret_f32_take_f32, None),
            fabs_f64: module.add_function("llvm.fabs.f64", ret_f64_take_f64, None),
            fabs_f32x4: module.add_function("llvm.fabs.v4f32", ret_f32x4_take_f32x4, None),
            fabs_f64x2: module.add_function("llvm.fabs.v2f64", ret_f64x2_take_f64x2, None),

            copysign_f32: module.add_function("llvm.copysign.f32", ret_f32_take_f32_f32, None),
            copysign_f64: module.add_function("llvm.copysign.f64", ret_f64_take_f64_f64, None),
            copysign_f32x4: module.add_function(
                "llvm.copysign.v4f32",
                ret_f32x4_take_f32x4_f32x4,
                None,
            ),
            copysign_f64x2: module.add_function(
                "llvm.copysign.v2f64",
                ret_f64x2_take_f64x2_f64x2,
                None,
            ),

            sadd_sat_i8x16: module.add_function(
                "llvm.sadd.sat.v16i8",
                ret_i8x16_take_i8x16_i8x16,
                None,
            ),
            sadd_sat_i16x8: module.add_function(
                "llvm.sadd.sat.v8i16",
                ret_i16x8_take_i16x8_i16x8,
                None,
            ),
            uadd_sat_i8x16: module.add_function(
                "llvm.uadd.sat.v16i8",
                ret_i8x16_take_i8x16_i8x16,
                None,
            ),
            uadd_sat_i16x8: module.add_function(
                "llvm.uadd.sat.v8i16",
                ret_i16x8_take_i16x8_i16x8,
                None,
            ),

            ssub_sat_i8x16: module.add_function(
                "llvm.ssub.sat.v16i8",
                ret_i8x16_take_i8x16_i8x16,
                None,
            ),
            ssub_sat_i16x8: module.add_function(
                "llvm.ssub.sat.v8i16",
                ret_i16x8_take_i16x8_i16x8,
                None,
            ),
            usub_sat_i8x16: module.add_function(
                "llvm.usub.sat.v16i8",
                ret_i8x16_take_i8x16_i8x16,
                None,
            ),
            usub_sat_i16x8: module.add_function(
                "llvm.usub.sat.v8i16",
                ret_i16x8_take_i16x8_i16x8,
                None,
            ),

            expect_i1: module.add_function("llvm.expect.i1", ret_i1_take_i1_i1, None),
            trap: module.add_function("llvm.trap", void_ty.fn_type(&[], false), None),
            debug_trap: module.add_function("llvm.debugtrap", void_ty.fn_type(&[], false), None),
            personality: module.add_function(
                "__gxx_personality_v0",
                i32_ty.fn_type(&[], false),
                Some(Linkage::External),
            ),
            readonly: context
                .create_enum_attribute(Attribute::get_named_enum_kind_id("readonly"), 0),
            stack_probe: context.create_string_attribute("probe-stack", "wasmer_vm_probestack"),

            void_ty,
            i1_ty,
            i2_ty,
            i4_ty,
            i8_ty,
            i16_ty,
            i32_ty,
            i64_ty,
            i128_ty,
            isize_ty,
            f32_ty,
            f64_ty,

            i1x128_ty,
            i8x16_ty,
            i16x8_ty,
            i32x4_ty,
            i64x2_ty,
            f32x4_ty,
            f64x2_ty,
            i32x8_ty,

            i8_ptr_ty,
            i16_ptr_ty,
            i32_ptr_ty,
            i64_ptr_ty,
            i128_ptr_ty,
            isize_ptr_ty,
            f32_ptr_ty,
            f64_ptr_ty,

            anyfunc_ty,

            funcref_ty,
            externref_ty,
            anyref_ty,

            i1_zero,
            i8_zero,
            i32_zero,
            i64_zero,
            i128_zero,
            isize_zero,
            f32_zero,
            f64_zero,
            f32x4_zero,
            f64x2_zero,
            i32_consts,

            trap_unreachable: i32_ty
                .const_int(TrapCode::UnreachableCodeReached as _, false)
                .as_basic_value_enum(),
            trap_call_indirect_null: i32_ty
                .const_int(TrapCode::IndirectCallToNull as _, false)
                .as_basic_value_enum(),
            trap_call_indirect_sig: i32_ty
                .const_int(TrapCode::BadSignature as _, false)
                .as_basic_value_enum(),
            trap_memory_oob: i32_ty
                .const_int(TrapCode::HeapAccessOutOfBounds as _, false)
                .as_basic_value_enum(),
            trap_illegal_arithmetic: i32_ty
                .const_int(TrapCode::IntegerOverflow as _, false)
                .as_basic_value_enum(),
            trap_integer_division_by_zero: i32_ty
                .const_int(TrapCode::IntegerDivisionByZero as _, false)
                .as_basic_value_enum(),
            trap_bad_conversion_to_integer: i32_ty
                .const_int(TrapCode::BadConversionToInteger as _, false)
                .as_basic_value_enum(),
            trap_unaligned_atomic: i32_ty
                .const_int(TrapCode::UnalignedAtomic as _, false)
                .as_basic_value_enum(),
            trap_table_access_oob: i32_ty
                .const_int(TrapCode::TableAccessOutOfBounds as _, false)
                .as_basic_value_enum(),

            experimental_stackmap: module.add_function(
                "llvm.experimental.stackmap",
                void_ty.fn_type(
                    &[
                        i64_ty_basic_md, /* id */
                        i32_ty_basic_md, /* numShadowBytes */
                    ],
                    true,
                ),
                None,
            ),

            // VM libcalls.
            table_copy: module.add_function(
                "wasmer_vm_table_copy",
                void_ty.fn_type(
                    &[
                        ctx_ptr_ty_basic_md,
                        i32_ty_basic_md,
                        i32_ty_basic_md,
                        i32_ty_basic_md,
                        i32_ty_basic_md,
                        i32_ty_basic_md,
                    ],
                    false,
                ),
                None,
            ),
            table_init: module.add_function(
                "wasmer_vm_table_init",
                void_ty.fn_type(
                    &[
                        ctx_ptr_ty_basic_md,
                        i32_ty_basic_md,
                        i32_ty_basic_md,
                        i32_ty_basic_md,
                        i32_ty_basic_md,
                        i32_ty_basic_md,
                    ],
                    false,
                ),
                None,
            ),
            table_fill: module.add_function(
                "wasmer_vm_table_fill",
                void_ty.fn_type(
                    &[
                        ctx_ptr_ty_basic_md,
                        i32_ty_basic_md,
                        i32_ty_basic_md,
                        anyref_ty_basic_md,
                        i32_ty_basic_md,
                    ],
                    false,
                ),
                None,
            ),
            table_size: module.add_function(
                "wasmer_vm_table_size",
                i32_ty.fn_type(&[ctx_ptr_ty_basic_md, i32_ty_basic_md], false),
                None,
            ),
            imported_table_size: module.add_function(
                "wasmer_vm_imported_table_size",
                i32_ty.fn_type(&[ctx_ptr_ty_basic_md, i32_ty_basic_md], false),
                None,
            ),
            table_get: module.add_function(
                "wasmer_vm_table_get",
                anyref_ty.fn_type(
                    &[ctx_ptr_ty_basic_md, i32_ty_basic_md, i32_ty_basic_md],
                    false,
                ),
                None,
            ),
            imported_table_get: module.add_function(
                "wasmer_vm_imported_table_get",
                anyref_ty.fn_type(
                    &[ctx_ptr_ty_basic_md, i32_ty_basic_md, i32_ty_basic_md],
                    false,
                ),
                None,
            ),
            table_set: module.add_function(
                "wasmer_vm_table_set",
                void_ty.fn_type(
                    &[
                        ctx_ptr_ty_basic_md,
                        i32_ty_basic_md,
                        i32_ty_basic_md,
                        anyref_ty_basic_md,
                    ],
                    false,
                ),
                None,
            ),
            imported_table_set: module.add_function(
                "wasmer_vm_imported_table_set",
                void_ty.fn_type(
                    &[
                        ctx_ptr_ty_basic_md,
                        i32_ty_basic_md,
                        i32_ty_basic_md,
                        anyref_ty_basic_md,
                    ],
                    false,
                ),
                None,
            ),
            table_grow: module.add_function(
                "wasmer_vm_table_grow",
                i32_ty.fn_type(
                    &[
                        ctx_ptr_ty_basic_md,
                        anyref_ty_basic_md,
                        i32_ty_basic_md,
                        i32_ty_basic_md,
                    ],
                    false,
                ),
                None,
            ),
            imported_table_grow: module.add_function(
                "wasmer_vm_imported_table_grow",
                i32_ty.fn_type(
                    &[
                        ctx_ptr_ty_basic_md,
                        anyref_ty_basic_md,
                        i32_ty_basic_md,
                        i32_ty_basic_md,
                    ],
                    false,
                ),
                None,
            ),
            memory_init: module.add_function(
                "wasmer_vm_memory32_init",
                void_ty.fn_type(
                    &[
                        ctx_ptr_ty_basic_md,
                        i32_ty_basic_md,
                        i32_ty_basic_md,
                        i32_ty_basic_md,
                        i32_ty_basic_md,
                        i32_ty_basic_md,
                    ],
                    false,
                ),
                None,
            ),
            memory_copy: module.add_function(
                "wasmer_vm_memory32_copy",
                void_ty.fn_type(
                    &[
                        ctx_ptr_ty_basic_md,
                        i32_ty_basic_md,
                        i32_ty_basic_md,
                        i32_ty_basic_md,
                        i32_ty_basic_md,
                    ],
                    false,
                ),
                None,
            ),
            imported_memory_copy: module.add_function(
                "wasmer_vm_imported_memory32_copy",
                void_ty.fn_type(
                    &[
                        ctx_ptr_ty_basic_md,
                        i32_ty_basic_md,
                        i32_ty_basic_md,
                        i32_ty_basic_md,
                        i32_ty_basic_md,
                    ],
                    false,
                ),
                None,
            ),
            memory_fill: module.add_function(
                "wasmer_vm_memory32_fill",
                void_ty.fn_type(
                    &[
                        ctx_ptr_ty_basic_md,
                        i32_ty_basic_md,
                        i32_ty_basic_md,
                        i32_ty_basic_md,
                        i32_ty_basic_md,
                    ],
                    false,
                ),
                None,
            ),
            imported_memory_fill: module.add_function(
                "wasmer_vm_imported_memory32_fill",
                void_ty.fn_type(
                    &[
                        ctx_ptr_ty_basic_md,
                        i32_ty_basic_md,
                        i32_ty_basic_md,
                        i32_ty_basic_md,
                        i32_ty_basic_md,
                    ],
                    false,
                ),
                None,
            ),
            data_drop: module.add_function(
                "wasmer_vm_data_drop",
                void_ty.fn_type(&[ctx_ptr_ty_basic_md, i32_ty_basic_md], false),
                None,
            ),
            func_ref: module.add_function(
                "wasmer_vm_func_ref",
                funcref_ty.fn_type(&[ctx_ptr_ty_basic_md, i32_ty_basic_md], false),
                None,
            ),
            elem_drop: module.add_function(
                "wasmer_vm_elem_drop",
                void_ty.fn_type(&[ctx_ptr_ty_basic_md, i32_ty_basic_md], false),
                None,
            ),
            throw_trap: module.add_function(
                "wasmer_vm_raise_trap",
                void_ty.fn_type(&[i32_ty_basic_md], false),
                None,
            ),

            vmfunction_import_ptr_ty: context
                .struct_type(&[i8_ptr_ty_basic, i8_ptr_ty_basic], false)
                .ptr_type(AddressSpace::Generic),
            vmfunction_import_body_element: 0,
            vmfunction_import_vmctx_element: 1,

            vmmemory_definition_ptr_ty: context
                .struct_type(&[i8_ptr_ty_basic, isize_ty.into()], false)
                .ptr_type(AddressSpace::Generic),
            vmmemory_definition_base_element: 0,
            vmmemory_definition_current_length_element: 1,

            memory32_grow_ptr_ty: i32_ty
                .fn_type(
                    &[ctx_ptr_ty_basic_md, i32_ty_basic_md, i32_ty_basic_md],
                    false,
                )
                .ptr_type(AddressSpace::Generic),
            imported_memory32_grow_ptr_ty: i32_ty
                .fn_type(
                    &[ctx_ptr_ty_basic_md, i32_ty_basic_md, i32_ty_basic_md],
                    false,
                )
                .ptr_type(AddressSpace::Generic),
            memory32_size_ptr_ty: i32_ty
                .fn_type(&[ctx_ptr_ty_basic_md, i32_ty_basic_md], false)
                .ptr_type(AddressSpace::Generic),
            imported_memory32_size_ptr_ty: i32_ty
                .fn_type(&[ctx_ptr_ty_basic_md, i32_ty_basic_md], false)
                .ptr_type(AddressSpace::Generic),

            ctx_ptr_ty,
        };

        let noreturn =
            context.create_enum_attribute(Attribute::get_named_enum_kind_id("noreturn"), 0);
        intrinsics
            .throw_trap
            .add_attribute(AttributeLoc::Function, noreturn);
        intrinsics
            .func_ref
            .add_attribute(AttributeLoc::Function, intrinsics.readonly);

        intrinsics
    }
}

#[derive(Clone, Copy)]
pub enum MemoryCache<'ctx> {
    /// The memory moves around.
    Dynamic {
        ptr_to_base_ptr: PointerValue<'ctx>,
        ptr_to_current_length: PointerValue<'ctx>,
    },
    /// The memory is always in the same place.
    Static { base_ptr: PointerValue<'ctx> },
}

struct TableCache<'ctx> {
    ptr_to_base_ptr: PointerValue<'ctx>,
    ptr_to_bounds: PointerValue<'ctx>,
}

#[derive(Clone, Copy)]
pub enum GlobalCache<'ctx> {
    Mut { ptr_to_value: PointerValue<'ctx> },
    Const { value: BasicValueEnum<'ctx> },
}

#[derive(Clone)]
pub struct FunctionCache<'ctx> {
    pub func: PointerValue<'ctx>,
    pub vmctx: BasicValueEnum<'ctx>,
    pub attrs: Vec<(Attribute, AttributeLoc)>,
}

pub struct CtxType<'ctx, 'a> {
    ctx_ptr_value: PointerValue<'ctx>,

    wasm_module: &'a WasmerCompilerModule,
    cache_builder: &'a Builder<'ctx>,
    abi: &'a dyn Abi,

    cached_memories: HashMap<MemoryIndex, MemoryCache<'ctx>>,
    cached_tables: HashMap<TableIndex, TableCache<'ctx>>,
    cached_sigindices: HashMap<SignatureIndex, IntValue<'ctx>>,
    cached_globals: HashMap<GlobalIndex, GlobalCache<'ctx>>,
    cached_functions: HashMap<FunctionIndex, FunctionCache<'ctx>>,
    cached_memory_grow: HashMap<MemoryIndex, PointerValue<'ctx>>,
    cached_memory_size: HashMap<MemoryIndex, PointerValue<'ctx>>,

    offsets: VMOffsets,
}

impl<'ctx, 'a> CtxType<'ctx, 'a> {
    pub fn new(
        wasm_module: &'a WasmerCompilerModule,
        func_value: &FunctionValue<'ctx>,
        cache_builder: &'a Builder<'ctx>,
        abi: &'a dyn Abi,
    ) -> CtxType<'ctx, 'a> {
        CtxType {
            ctx_ptr_value: abi.get_vmctx_ptr_param(func_value),

            wasm_module,
            cache_builder,
            abi,

            cached_memories: HashMap::new(),
            cached_tables: HashMap::new(),
            cached_sigindices: HashMap::new(),
            cached_globals: HashMap::new(),
            cached_functions: HashMap::new(),
            cached_memory_grow: HashMap::new(),
            cached_memory_size: HashMap::new(),

            // TODO: pointer width
            offsets: VMOffsets::new(8).with_module_info(&wasm_module),
        }
    }

    pub fn basic(&self) -> BasicValueEnum<'ctx> {
        self.ctx_ptr_value.as_basic_value_enum()
    }

    pub fn memory(
        &mut self,
        index: MemoryIndex,
        intrinsics: &Intrinsics<'ctx>,
        module: &Module<'ctx>,
        memory_styles: &PrimaryMap<MemoryIndex, MemoryStyle>,
    ) -> MemoryCache<'ctx> {
        let (cached_memories, wasm_module, ctx_ptr_value, cache_builder, offsets) = (
            &mut self.cached_memories,
            self.wasm_module,
            self.ctx_ptr_value,
            &self.cache_builder,
            &self.offsets,
        );
        let memory_style = &memory_styles[index];
        *cached_memories.entry(index).or_insert_with(|| {
            let memory_definition_ptr =
                if let Some(local_memory_index) = wasm_module.local_memory_index(index) {
                    let offset = offsets.vmctx_vmmemory_definition(local_memory_index);
                    let offset = intrinsics.i32_ty.const_int(offset.into(), false);
                    unsafe { cache_builder.build_gep(ctx_ptr_value, &[offset], "") }
                } else {
                    let offset = offsets.vmctx_vmmemory_import(index);
                    let offset = intrinsics.i32_ty.const_int(offset.into(), false);
                    let memory_definition_ptr_ptr =
                        unsafe { cache_builder.build_gep(ctx_ptr_value, &[offset], "") };
                    let memory_definition_ptr_ptr = cache_builder
                        .build_bitcast(
                            memory_definition_ptr_ptr,
                            intrinsics.i8_ptr_ty.ptr_type(AddressSpace::Generic),
                            "",
                        )
                        .into_pointer_value();
                    let memory_definition_ptr = cache_builder
                        .build_load(memory_definition_ptr_ptr, "")
                        .into_pointer_value();
                    tbaa_label(
                        module,
                        intrinsics,
                        format!("memory {} definition", index.as_u32()),
                        memory_definition_ptr.as_instruction_value().unwrap(),
                    );
                    memory_definition_ptr
                };
            let memory_definition_ptr = cache_builder
                .build_bitcast(
                    memory_definition_ptr,
                    intrinsics.vmmemory_definition_ptr_ty,
                    "",
                )
                .into_pointer_value();
            let base_ptr = cache_builder
                .build_struct_gep(
                    memory_definition_ptr,
                    intrinsics.vmmemory_definition_base_element,
                    "",
                )
                .unwrap();
            if let MemoryStyle::Dynamic { .. } = memory_style {
                let current_length_ptr = cache_builder
                    .build_struct_gep(
                        memory_definition_ptr,
                        intrinsics.vmmemory_definition_current_length_element,
                        "",
                    )
                    .unwrap();
                MemoryCache::Dynamic {
                    ptr_to_base_ptr: base_ptr,
                    ptr_to_current_length: current_length_ptr,
                }
            } else {
                let base_ptr = cache_builder.build_load(base_ptr, "").into_pointer_value();
                tbaa_label(
                    module,
                    intrinsics,
                    format!("memory base_ptr {}", index.as_u32()),
                    base_ptr.as_instruction_value().unwrap(),
                );
                MemoryCache::Static { base_ptr }
            }
        })
    }

    fn table_prepare(
        &mut self,
        table_index: TableIndex,
        intrinsics: &Intrinsics<'ctx>,
        module: &Module<'ctx>,
    ) -> (PointerValue<'ctx>, PointerValue<'ctx>) {
        let (cached_tables, wasm_module, ctx_ptr_value, cache_builder, offsets) = (
            &mut self.cached_tables,
            self.wasm_module,
            self.ctx_ptr_value,
            &self.cache_builder,
            &self.offsets,
        );
        let TableCache {
            ptr_to_base_ptr,
            ptr_to_bounds,
        } = *cached_tables.entry(table_index).or_insert_with(|| {
            let (ptr_to_base_ptr, ptr_to_bounds) =
                if let Some(local_table_index) = wasm_module.local_table_index(table_index) {
                    let offset = intrinsics.i64_ty.const_int(
                        offsets
                            .vmctx_vmtable_definition_base(local_table_index)
                            .into(),
                        false,
                    );
                    let ptr_to_base_ptr =
                        unsafe { cache_builder.build_gep(ctx_ptr_value, &[offset], "") };
                    let ptr_to_base_ptr = cache_builder
                        .build_bitcast(
                            ptr_to_base_ptr,
                            intrinsics.i8_ptr_ty.ptr_type(AddressSpace::Generic),
                            "",
                        )
                        .into_pointer_value();
                    let offset = intrinsics.i64_ty.const_int(
                        offsets
                            .vmctx_vmtable_definition_current_elements(local_table_index)
                            .into(),
                        false,
                    );
                    let ptr_to_bounds =
                        unsafe { cache_builder.build_gep(ctx_ptr_value, &[offset], "") };
                    let ptr_to_bounds = cache_builder
                        .build_bitcast(ptr_to_bounds, intrinsics.i32_ptr_ty, "")
                        .into_pointer_value();
                    (ptr_to_base_ptr, ptr_to_bounds)
                } else {
                    let offset = intrinsics.i64_ty.const_int(
                        offsets.vmctx_vmtable_import_definition(table_index).into(),
                        false,
                    );
                    let definition_ptr_ptr =
                        unsafe { cache_builder.build_gep(ctx_ptr_value, &[offset], "") };
                    let definition_ptr_ptr = cache_builder
                        .build_bitcast(
                            definition_ptr_ptr,
                            intrinsics.i8_ptr_ty.ptr_type(AddressSpace::Generic),
                            "",
                        )
                        .into_pointer_value();
                    let definition_ptr = cache_builder
                        .build_load(definition_ptr_ptr, "")
                        .into_pointer_value();
                    tbaa_label(
                        module,
                        intrinsics,
                        format!("table {} definition", table_index.as_u32()),
                        definition_ptr.as_instruction_value().unwrap(),
                    );

                    let offset = intrinsics
                        .i64_ty
                        .const_int(offsets.vmtable_definition_base().into(), false);
                    let ptr_to_base_ptr =
                        unsafe { cache_builder.build_gep(definition_ptr, &[offset], "") };
                    let ptr_to_base_ptr = cache_builder
                        .build_bitcast(
                            ptr_to_base_ptr,
                            intrinsics.i8_ptr_ty.ptr_type(AddressSpace::Generic),
                            "",
                        )
                        .into_pointer_value();
                    let offset = intrinsics
                        .i64_ty
                        .const_int(offsets.vmtable_definition_current_elements().into(), false);
                    let ptr_to_bounds =
                        unsafe { cache_builder.build_gep(definition_ptr, &[offset], "") };
                    let ptr_to_bounds = cache_builder
                        .build_bitcast(ptr_to_bounds, intrinsics.i32_ptr_ty, "")
                        .into_pointer_value();
                    (ptr_to_base_ptr, ptr_to_bounds)
                };
            TableCache {
                ptr_to_base_ptr,
                ptr_to_bounds,
            }
        });

        (ptr_to_base_ptr, ptr_to_bounds)
    }

    pub fn table(
        &mut self,
        index: TableIndex,
        intrinsics: &Intrinsics<'ctx>,
        module: &Module<'ctx>,
    ) -> (PointerValue<'ctx>, IntValue<'ctx>) {
        let (ptr_to_base_ptr, ptr_to_bounds) = self.table_prepare(index, intrinsics, module);
        let base_ptr = self
            .cache_builder
            .build_load(ptr_to_base_ptr, "base_ptr")
            .into_pointer_value();
        let bounds = self
            .cache_builder
            .build_load(ptr_to_bounds, "bounds")
            .into_int_value();
        tbaa_label(
            module,
            intrinsics,
            format!("table_base_ptr {}", index.index()),
            base_ptr.as_instruction_value().unwrap(),
        );
        tbaa_label(
            module,
            intrinsics,
            format!("table_bounds {}", index.index()),
            bounds.as_instruction_value().unwrap(),
        );
        (base_ptr, bounds)
    }

    pub fn dynamic_sigindex(
        &mut self,
        index: SignatureIndex,
        intrinsics: &Intrinsics<'ctx>,
        module: &Module<'ctx>,
    ) -> IntValue<'ctx> {
        let (cached_sigindices, ctx_ptr_value, cache_builder, offsets) = (
            &mut self.cached_sigindices,
            self.ctx_ptr_value,
            &self.cache_builder,
            &self.offsets,
        );
        *cached_sigindices.entry(index).or_insert_with(|| {
            let byte_offset = intrinsics
                .i64_ty
                .const_int(offsets.vmctx_vmshared_signature_id(index).into(), false);
            let sigindex_ptr = unsafe {
                cache_builder.build_gep(ctx_ptr_value, &[byte_offset], "dynamic_sigindex")
            };
            let sigindex_ptr = cache_builder
                .build_bitcast(sigindex_ptr, intrinsics.i32_ptr_ty, "")
                .into_pointer_value();

            let sigindex = cache_builder
                .build_load(sigindex_ptr, "sigindex")
                .into_int_value();
            tbaa_label(
                module,
                intrinsics,
                format!("sigindex {}", index.as_u32()),
                sigindex.as_instruction_value().unwrap(),
            );
            sigindex
        })
    }

    pub fn global(
        &mut self,
        index: GlobalIndex,
        intrinsics: &Intrinsics<'ctx>,
        module: &Module<'ctx>,
    ) -> Result<&GlobalCache<'ctx>, CompileError> {
        let (cached_globals, wasm_module, ctx_ptr_value, cache_builder, offsets) = (
            &mut self.cached_globals,
            self.wasm_module,
            self.ctx_ptr_value,
            &self.cache_builder,
            &self.offsets,
        );
        Ok(match cached_globals.entry(index) {
            Entry::Occupied(entry) => entry.into_mut(),
            Entry::Vacant(entry) => {
                let global_type = wasm_module.globals[index];
                let global_value_type = global_type.ty;

                let global_mutability = global_type.mutability;
                let offset = if let Some(local_global_index) = wasm_module.local_global_index(index)
                {
                    offsets.vmctx_vmglobal_definition(local_global_index)
                } else {
                    offsets.vmctx_vmglobal_import(index)
                };
                let offset = intrinsics.i32_ty.const_int(offset.into(), false);
                let global_ptr = {
                    let global_ptr_ptr =
                        unsafe { cache_builder.build_gep(ctx_ptr_value, &[offset], "") };
                    let global_ptr_ptr = cache_builder
                        .build_bitcast(
                            global_ptr_ptr,
                            intrinsics.i32_ptr_ty.ptr_type(AddressSpace::Generic),
                            "",
                        )
                        .into_pointer_value();
                    let global_ptr = cache_builder
                        .build_load(global_ptr_ptr, "")
                        .into_pointer_value();
                    tbaa_label(
                        module,
                        intrinsics,
                        format!("global_ptr {}", index.as_u32()),
                        global_ptr.as_instruction_value().unwrap(),
                    );
                    global_ptr
                };
                let global_ptr = cache_builder
                    .build_bitcast(
                        global_ptr,
                        type_to_llvm_ptr(&intrinsics, global_value_type)?,
                        "",
                    )
                    .into_pointer_value();

                entry.insert(match global_mutability {
                    Mutability::Const => {
                        let value = cache_builder.build_load(global_ptr, "");
                        tbaa_label(
                            module,
                            intrinsics,
                            format!("global {}", index.as_u32()),
                            value.as_instruction_value().unwrap(),
                        );
                        GlobalCache::Const { value }
                    }
                    Mutability::Var => GlobalCache::Mut {
                        ptr_to_value: global_ptr,
                    },
                })
            }
        })
    }

    pub fn add_func(
        &mut self,
        function_index: FunctionIndex,
        func: PointerValue<'ctx>,
        vmctx: BasicValueEnum<'ctx>,
        attrs: &[(Attribute, AttributeLoc)],
    ) {
        match self.cached_functions.entry(function_index) {
            Entry::Occupied(_) => unreachable!("duplicate function"),
            Entry::Vacant(entry) => {
                entry.insert(FunctionCache {
                    func,
                    vmctx,
                    attrs: attrs.to_vec(),
                });
            }
        }
    }

    pub fn local_func(
        &mut self,
        _local_function_index: LocalFunctionIndex,
        function_index: FunctionIndex,
        intrinsics: &Intrinsics<'ctx>,
        module: &Module<'ctx>,
        context: &'ctx Context,
        func_type: &FuncType,
        function_name: &str,
    ) -> Result<&FunctionCache<'ctx>, CompileError> {
        let (cached_functions, ctx_ptr_value, offsets) = (
            &mut self.cached_functions,
            &self.ctx_ptr_value,
            &self.offsets,
        );
        Ok(match cached_functions.entry(function_index) {
            Entry::Occupied(entry) => entry.into_mut(),
            Entry::Vacant(entry) => {
                debug_assert!(module.get_function(function_name).is_none());
                let (llvm_func_type, llvm_func_attrs) =
                    self.abi
                        .func_type_to_llvm(context, intrinsics, Some(offsets), func_type)?;
                let func =
                    module.add_function(function_name, llvm_func_type, Some(Linkage::External));
                for (attr, attr_loc) in &llvm_func_attrs {
                    func.add_attribute(*attr_loc, *attr);
                }
                entry.insert(FunctionCache {
                    func: func.as_global_value().as_pointer_value(),
                    vmctx: ctx_ptr_value.as_basic_value_enum(),
                    attrs: llvm_func_attrs,
                })
            }
        })
    }

    pub fn func(
        &mut self,
        function_index: FunctionIndex,
        intrinsics: &Intrinsics<'ctx>,
        context: &'ctx Context,
        func_type: &FuncType,
    ) -> Result<&FunctionCache<'ctx>, CompileError> {
        let (cached_functions, wasm_module, ctx_ptr_value, cache_builder, offsets) = (
            &mut self.cached_functions,
            self.wasm_module,
            &self.ctx_ptr_value,
            &self.cache_builder,
            &self.offsets,
        );
        Ok(match cached_functions.entry(function_index) {
            Entry::Occupied(entry) => entry.into_mut(),
            Entry::Vacant(entry) => {
                let (llvm_func_type, llvm_func_attrs) =
                    self.abi
                        .func_type_to_llvm(context, intrinsics, Some(offsets), func_type)?;
                debug_assert!(wasm_module.local_func_index(function_index).is_none());
                let offset = offsets.vmctx_vmfunction_import(function_index);
                let offset = intrinsics.i32_ty.const_int(offset.into(), false);
                let vmfunction_import_ptr =
                    unsafe { cache_builder.build_gep(*ctx_ptr_value, &[offset], "") };
                let vmfunction_import_ptr = cache_builder
                    .build_bitcast(
                        vmfunction_import_ptr,
                        intrinsics.vmfunction_import_ptr_ty,
                        "",
                    )
                    .into_pointer_value();

                let body_ptr_ptr = cache_builder
                    .build_struct_gep(
                        vmfunction_import_ptr,
                        intrinsics.vmfunction_import_body_element,
                        "",
                    )
                    .unwrap();
                let body_ptr = cache_builder.build_load(body_ptr_ptr, "");
                let body_ptr = cache_builder
                    .build_bitcast(body_ptr, llvm_func_type.ptr_type(AddressSpace::Generic), "")
                    .into_pointer_value();
                let vmctx_ptr_ptr = cache_builder
                    .build_struct_gep(
                        vmfunction_import_ptr,
                        intrinsics.vmfunction_import_vmctx_element,
                        "",
                    )
                    .unwrap();
                let vmctx_ptr = cache_builder.build_load(vmctx_ptr_ptr, "");
                entry.insert(FunctionCache {
                    func: body_ptr,
                    vmctx: vmctx_ptr,
                    attrs: llvm_func_attrs,
                })
            }
        })
    }

    pub fn memory_grow(
        &mut self,
        memory_index: MemoryIndex,
        intrinsics: &Intrinsics<'ctx>,
    ) -> PointerValue<'ctx> {
        let (cached_memory_grow, wasm_module, offsets, cache_builder, ctx_ptr_value) = (
            &mut self.cached_memory_grow,
            &self.wasm_module,
            &self.offsets,
            &self.cache_builder,
            &self.ctx_ptr_value,
        );
        *cached_memory_grow.entry(memory_index).or_insert_with(|| {
            let (grow_fn, grow_fn_ty) = if wasm_module.local_memory_index(memory_index).is_some() {
                (
                    VMBuiltinFunctionIndex::get_memory32_grow_index(),
                    intrinsics.memory32_grow_ptr_ty,
                )
            } else {
                (
                    VMBuiltinFunctionIndex::get_imported_memory32_grow_index(),
                    intrinsics.imported_memory32_grow_ptr_ty,
                )
            };
            let offset = offsets.vmctx_builtin_function(grow_fn);
            let offset = intrinsics.i32_ty.const_int(offset.into(), false);
            let grow_fn_ptr_ptr = unsafe { cache_builder.build_gep(*ctx_ptr_value, &[offset], "") };

            let grow_fn_ptr_ptr = cache_builder
                .build_bitcast(
                    grow_fn_ptr_ptr,
                    grow_fn_ty.ptr_type(AddressSpace::Generic),
                    "",
                )
                .into_pointer_value();
            cache_builder
                .build_load(grow_fn_ptr_ptr, "")
                .into_pointer_value()
        })
    }

    pub fn memory_size(
        &mut self,
        memory_index: MemoryIndex,
        intrinsics: &Intrinsics<'ctx>,
    ) -> PointerValue<'ctx> {
        let (cached_memory_size, wasm_module, offsets, cache_builder, ctx_ptr_value) = (
            &mut self.cached_memory_size,
            &self.wasm_module,
            &self.offsets,
            &self.cache_builder,
            &self.ctx_ptr_value,
        );
        *cached_memory_size.entry(memory_index).or_insert_with(|| {
            let (size_fn, size_fn_ty) = if wasm_module.local_memory_index(memory_index).is_some() {
                (
                    VMBuiltinFunctionIndex::get_memory32_size_index(),
                    intrinsics.memory32_size_ptr_ty,
                )
            } else {
                (
                    VMBuiltinFunctionIndex::get_imported_memory32_size_index(),
                    intrinsics.imported_memory32_size_ptr_ty,
                )
            };
            let offset = offsets.vmctx_builtin_function(size_fn);
            let offset = intrinsics.i32_ty.const_int(offset.into(), false);
            let size_fn_ptr_ptr = unsafe { cache_builder.build_gep(*ctx_ptr_value, &[offset], "") };

            let size_fn_ptr_ptr = cache_builder
                .build_bitcast(
                    size_fn_ptr_ptr,
                    size_fn_ty.ptr_type(AddressSpace::Generic),
                    "",
                )
                .into_pointer_value();

            cache_builder
                .build_load(size_fn_ptr_ptr, "")
                .into_pointer_value()
        })
    }

    pub fn get_offsets(&self) -> &VMOffsets {
        &self.offsets
    }
}

// Given an instruction that operates on memory, mark the access as not aliasing
// other memory accesses which have a different label.
pub fn tbaa_label<'ctx>(
    module: &Module<'ctx>,
    intrinsics: &Intrinsics<'ctx>,
    label: String,
    instruction: InstructionValue<'ctx>,
) {
    // To convey to LLVM that two pointers must be pointing to distinct memory,
    // we use LLVM's Type Based Aliasing Analysis, or TBAA, to mark the memory
    // operations as having different types whose pointers may not alias.
    //
    // See the LLVM documentation at
    //   https://llvm.org/docs/LangRef.html#tbaa-metadata
    //
    // LLVM TBAA supports many features, but we use it in a simple way, with
    // only scalar types that are children of the root node. Every TBAA type we
    // declare is NoAlias with the others. See NoAlias, PartialAlias,
    // MayAlias and MustAlias in the LLVM documentation:
    //   https://llvm.org/docs/AliasAnalysis.html#must-may-and-no-alias-responses

    let context = module.get_context();

    // TODO: ContextRef can't return us the lifetime from module through Deref.
    // This could be fixed once generic_associated_types is stable.
    let context = {
        let context2 = &*context;
        unsafe { std::mem::transmute::<&Context, &'ctx Context>(context2) }
    };

    // `!wasmer_tbaa_root = {}`, the TBAA root node for wasmer.
    let tbaa_root = module
        .get_global_metadata("wasmer_tbaa_root")
        .pop()
        .unwrap_or_else(|| {
            module
                .add_global_metadata("wasmer_tbaa_root", &context.metadata_node(&[]))
                .unwrap();
            module.get_global_metadata("wasmer_tbaa_root")[0]
        });

    // Construct (or look up) the type descriptor, for example
    //   `!"local 0" = !{!"local 0", !wasmer_tbaa_root}`.
    let type_label = context.metadata_string(label.as_str());
    let type_tbaa = module
        .get_global_metadata(label.as_str())
        .pop()
        .unwrap_or_else(|| {
            module
                .add_global_metadata(
                    label.as_str(),
                    &context.metadata_node(&[type_label.into(), tbaa_root.into()]),
                )
                .unwrap();
            module.get_global_metadata(label.as_str())[0]
        });

    // Construct (or look up) the access tag, which is a struct of the form
    // (base type, access type, offset).
    //
    // "If BaseTy is a scalar type, Offset must be 0 and BaseTy and AccessTy
    // must be the same".
    //   -- https://llvm.org/docs/LangRef.html#tbaa-metadata
    let label = label + "_memop";
    let type_tbaa = module
        .get_global_metadata(label.as_str())
        .pop()
        .unwrap_or_else(|| {
            module
                .add_global_metadata(
                    label.as_str(),
                    &context.metadata_node(&[
                        type_tbaa.into(),
                        type_tbaa.into(),
                        intrinsics.i64_zero.into(),
                    ]),
                )
                .unwrap();
            module.get_global_metadata(label.as_str())[0]
        });

    // Attach the access tag to the instruction.
    let tbaa_kind = context.get_kind_id("tbaa");
    instruction.set_metadata(type_tbaa, tbaa_kind).unwrap();
}

'''
'''--- lib/compiler-llvm/src/translator/mod.rs ---
mod code;
pub mod intrinsics;
//mod stackmap;
mod state;

pub use self::code::FuncTranslator;

'''
'''--- lib/compiler-llvm/src/translator/stackmap.rs ---
// https://llvm.org/docs/StackMaps.html#stackmap-section

use byteorder::{LittleEndian, ReadBytesExt};
use std::io::{self, Cursor};
use wasmer_vm_core::vm::Ctx;
use wasmer_vm_core::{
    module::Module,
    structures::TypedIndex,
    types::{GlobalIndex, LocalOrImport, TableIndex},
};

#[derive(Default, Debug, Clone)]
pub struct StackmapRegistry {
    pub entries: Vec<StackmapEntry>,
}

#[derive(Debug, Clone)]
pub struct StackmapEntry {
    pub kind: StackmapEntryKind,
    pub local_function_id: usize,
    pub opcode_offset: usize,
    pub value_semantics: Vec<ValueSemantic>,
    pub local_count: usize,
    pub stack_count: usize,
    pub is_start: bool,
}

#[derive(Debug, Clone)]
pub enum ValueSemantic {
    WasmLocal(usize),
    WasmStack(usize),
    Ctx,
    SignalMem,
    PointerToMemoryBase,
    PointerToMemoryBound, // 64-bit
    MemoryBase,
    MemoryBound, // 64-bit
    PointerToGlobal(usize),
    Global(usize),
    PointerToTableBase,
    PointerToTableBound,
    ImportedFuncPointer(usize),
    ImportedFuncCtx(usize),
    DynamicSigindice(usize),
}

#[derive(Debug, Clone, Copy, Eq, PartialEq)]
pub enum StackmapEntryKind {
    FunctionHeader,
    Loop,
    Call,
    Trappable,
}

impl StackmapEntry {
    #[cfg(all(
        any(target_os = "freebsd", target_os = "linux", target_vendor = "apple"),
        target_arch = "x86_64"
    ))]
    pub fn populate_msm(
        &self,
        module_info: &ModuleInfo,
        code_addr: usize,
        llvm_map: &StackMap,
        size_record: &StkSizeRecord,
        map_record: &StkMapRecord,
        end: Option<(&StackmapEntry, &StkMapRecord)>,
        msm: &mut wasmer_vm_core::state::ModuleStateMap,
    ) {
        use std::collections::{BTreeMap, HashMap};
        use wasmer_vm_core::state::{
            x64::{new_machine_state, X64Register, GPR},
            FunctionStateMap, MachineStateDiff, MachineValue, OffsetInfo, RegisterIndex,
            SuspendOffset, WasmAbstractValue,
        };
        use wasmer_vm_core::vm;

        let func_base_addr = (size_record.function_address as usize)
            .checked_sub(code_addr)
            .unwrap();
        let target_offset = func_base_addr + map_record.instruction_offset as usize;
        assert!(self.is_start);

        if msm.local_functions.len() == self.local_function_id {
            assert_eq!(self.kind, StackmapEntryKind::FunctionHeader);
            msm.local_functions.insert(
                target_offset,
                FunctionStateMap::new(new_machine_state(), self.local_function_id, 0, vec![]),
            );
        } else if msm.local_functions.len() == self.local_function_id + 1 {
        } else {
            panic!("unordered local functions");
        }

        let (_, fsm) = msm.local_functions.iter_mut().last().unwrap();

        assert_eq!(self.value_semantics.len(), map_record.locations.len());

        // System V requires 16-byte alignment before each call instruction.
        // Considering the saved rbp we need to ensure the stack size % 16 always equals to 8.
        assert!(size_record.stack_size % 16 == 8);

        // Layout begins just below saved rbp. (push rbp; mov rbp, rsp)
        let mut machine_stack_half_layout: Vec<MachineValue> =
            vec![MachineValue::Undefined; (size_record.stack_size - 8) as usize / 4];
        let mut regs: Vec<(RegisterIndex, MachineValue)> = vec![];
        let mut stack_constants: HashMap<usize, u64> = HashMap::new();

        let mut prev_frame_diff: BTreeMap<usize, Option<MachineValue>> = BTreeMap::new();

        let mut wasm_locals: Vec<WasmAbstractValue> = vec![];
        let mut wasm_stack: Vec<WasmAbstractValue> = vec![];

        for (i, loc) in map_record.locations.iter().enumerate() {
            let mv = match self.value_semantics[i] {
                ValueSemantic::WasmLocal(x) => {
                    if x != wasm_locals.len() {
                        panic!("unordered local values");
                    }
                    wasm_locals.push(WasmAbstractValue::Runtime);
                    MachineValue::WasmLocal(x)
                }
                ValueSemantic::WasmStack(x) => {
                    if x != wasm_stack.len() {
                        panic!("unordered stack values");
                    }
                    wasm_stack.push(WasmAbstractValue::Runtime);
                    MachineValue::WasmStack(x)
                }
                ValueSemantic::Ctx => MachineValue::Vmctx,
                ValueSemantic::SignalMem => {
                    MachineValue::VmctxDeref(vec![Ctx::offset_interrupt_signal_mem() as usize, 0])
                }
                ValueSemantic::PointerToMemoryBase => {
                    MachineValue::VmctxDeref(vec![Ctx::offset_memory_base() as usize])
                }
                ValueSemantic::PointerToMemoryBound => {
                    MachineValue::VmctxDeref(vec![Ctx::offset_memory_bound() as usize])
                }
                ValueSemantic::MemoryBase => {
                    MachineValue::VmctxDeref(vec![Ctx::offset_memory_base() as usize, 0])
                }
                ValueSemantic::MemoryBound => {
                    MachineValue::VmctxDeref(vec![Ctx::offset_memory_bound() as usize, 0])
                }
                ValueSemantic::PointerToGlobal(idx) => {
                    MachineValue::VmctxDeref(deref_global(module_info, idx, false))
                }
                ValueSemantic::Global(idx) => {
                    MachineValue::VmctxDeref(deref_global(module_info, idx, true))
                }
                ValueSemantic::PointerToTableBase => {
                    MachineValue::VmctxDeref(deref_table_base(module_info, 0, false))
                }
                ValueSemantic::PointerToTableBound => {
                    MachineValue::VmctxDeref(deref_table_bound(module_info, 0, false))
                }
                ValueSemantic::ImportedFuncPointer(idx) => MachineValue::VmctxDeref(vec![
                    Ctx::offset_imported_funcs() as usize,
                    vm::ImportedFunc::size() as usize * idx
                        + vm::ImportedFunc::offset_func() as usize,
                    0,
                ]),
                ValueSemantic::ImportedFuncCtx(idx) => MachineValue::VmctxDeref(vec![
                    Ctx::offset_imported_funcs() as usize,
                    vm::ImportedFunc::size() as usize * idx
                        + vm::ImportedFunc::offset_func_ctx() as usize,
                    0,
                ]),
                ValueSemantic::DynamicSigindice(idx) => {
                    MachineValue::VmctxDeref(vec![Ctx::offset_signatures() as usize, idx * 4, 0])
                }
            };
            match loc.ty {
                LocationType::Register => {
                    let index = X64Register::from_dwarf_regnum(loc.dwarf_regnum)
                        .expect("invalid regnum")
                        .to_index();
                    regs.push((index, mv));
                }
                LocationType::Constant => {
                    let v = loc.offset_or_small_constant as u32 as u64;
                    match mv {
                        MachineValue::WasmStack(x) => {
                            stack_constants.insert(x, v);
                            *wasm_stack.last_mut().unwrap() = WasmAbstractValue::Const(v);
                        }
                        _ => {} // TODO
                    }
                }
                LocationType::ConstantIndex => {
                    let v =
                        llvm_map.constants[loc.offset_or_small_constant as usize].large_constant;
                    match mv {
                        MachineValue::WasmStack(x) => {
                            stack_constants.insert(x, v);
                            *wasm_stack.last_mut().unwrap() = WasmAbstractValue::Const(v);
                        }
                        _ => {} // TODO
                    }
                }
                LocationType::Direct => match mv {
                    MachineValue::WasmLocal(_) => {
                        assert_eq!(loc.location_size, 8); // the pointer itself
                        assert!(
                            X64Register::from_dwarf_regnum(loc.dwarf_regnum).unwrap()
                                == X64Register::GPR(GPR::RBP)
                        );
                        if loc.offset_or_small_constant >= 0 {
                            assert!(loc.offset_or_small_constant >= 16); // (saved_rbp, return_address)
                            assert!(loc.offset_or_small_constant % 8 == 0);
                            prev_frame_diff
                                .insert((loc.offset_or_small_constant as usize - 16) / 8, Some(mv));
                        } else {
                            let stack_offset = ((-loc.offset_or_small_constant) / 4) as usize;
                            assert!(
                                stack_offset > 0 && stack_offset <= machine_stack_half_layout.len()
                            );
                            machine_stack_half_layout[stack_offset - 1] = mv;
                        }
                    }
                    _ => unreachable!(
                        "Direct location type is not expected for values other than local"
                    ),
                },
                LocationType::Indirect => {
                    assert!(loc.offset_or_small_constant < 0);
                    assert!(
                        X64Register::from_dwarf_regnum(loc.dwarf_regnum).unwrap()
                            == X64Register::GPR(GPR::RBP)
                    );
                    let stack_offset = ((-loc.offset_or_small_constant) / 4) as usize;
                    assert!(stack_offset > 0 && stack_offset <= machine_stack_half_layout.len());
                    machine_stack_half_layout[stack_offset - 1] = mv;
                }
            }
        }

        assert_eq!(wasm_stack.len(), self.stack_count);
        assert_eq!(wasm_locals.len(), self.local_count);

        let mut machine_stack_layout: Vec<MachineValue> =
            Vec::with_capacity(machine_stack_half_layout.len() / 2);

        for i in 0..machine_stack_half_layout.len() / 2 {
            let major = &machine_stack_half_layout[i * 2 + 1]; // mod 8 == 0
            let minor = &machine_stack_half_layout[i * 2]; // mod 8 == 4
            let only_major = match *minor {
                MachineValue::Undefined => true,
                _ => false,
            };
            if only_major {
                machine_stack_layout.push(major.clone());
            } else {
                machine_stack_layout.push(MachineValue::TwoHalves(Box::new((
                    major.clone(),
                    minor.clone(),
                ))));
            }
        }

        let diff = MachineStateDiff {
            last: None,
            stack_push: machine_stack_layout,
            stack_pop: 0,
            prev_frame_diff,
            reg_diff: regs,
            wasm_stack_push: wasm_stack,
            wasm_stack_pop: 0,
            wasm_inst_offset: self.opcode_offset,
        };
        let diff_id = fsm.diffs.len();
        fsm.diffs.push(diff);

        match self.kind {
            StackmapEntryKind::FunctionHeader => {
                fsm.locals = wasm_locals;
            }
            _ => {
                assert_eq!(fsm.locals, wasm_locals);
            }
        }

        let end_offset = {
            if let Some(end) = end {
                let (end_entry, end_record) = end;
                assert_eq!(end_entry.is_start, false);
                assert_eq!(self.opcode_offset, end_entry.opcode_offset);
                let end_offset = func_base_addr + end_record.instruction_offset as usize;
                assert!(end_offset >= target_offset);
                end_offset
            } else {
                target_offset + 1
            }
        };

        match self.kind {
            StackmapEntryKind::Loop => {
                fsm.wasm_offset_to_target_offset
                    .insert(self.opcode_offset, SuspendOffset::Loop(target_offset));
                fsm.loop_offsets.insert(
                    target_offset,
                    OffsetInfo {
                        end_offset,
                        diff_id,
                        activate_offset: target_offset,
                    },
                );
            }
            StackmapEntryKind::Call => {
                fsm.wasm_offset_to_target_offset
                    .insert(self.opcode_offset, SuspendOffset::Call(target_offset));
                fsm.call_offsets.insert(
                    target_offset,
                    OffsetInfo {
                        end_offset: end_offset + 1, // The return address is just after 'call' instruction. Offset by one here.
                        diff_id,
                        activate_offset: target_offset,
                    },
                );
            }
            StackmapEntryKind::Trappable => {
                fsm.wasm_offset_to_target_offset
                    .insert(self.opcode_offset, SuspendOffset::Trappable(target_offset));
                fsm.trappable_offsets.insert(
                    target_offset,
                    OffsetInfo {
                        end_offset,
                        diff_id,
                        activate_offset: target_offset,
                    },
                );
            }
            StackmapEntryKind::FunctionHeader => {
                fsm.wasm_function_header_target_offset = Some(SuspendOffset::Loop(target_offset));
                fsm.loop_offsets.insert(
                    target_offset,
                    OffsetInfo {
                        end_offset,
                        diff_id,
                        activate_offset: target_offset,
                    },
                );
            }
        }
    }
}

#[derive(Clone, Debug, Default)]
pub struct StackMap {
    pub version: u8,
    pub stk_size_records: Vec<StkSizeRecord>,
    pub constants: Vec<Constant>,
    pub stk_map_records: Vec<StkMapRecord>,
}

#[derive(Copy, Clone, Debug, Default)]
pub struct StkSizeRecord {
    pub function_address: u64,
    pub stack_size: u64,
    pub record_count: u64,
}

#[derive(Copy, Clone, Debug, Default)]
pub struct Constant {
    pub large_constant: u64,
}

#[derive(Clone, Debug, Default)]
pub struct StkMapRecord {
    pub patchpoint_id: u64,
    pub instruction_offset: u32,
    pub locations: Vec<Location>,
    pub live_outs: Vec<LiveOut>,
}

#[derive(Copy, Clone, Debug)]
pub struct Location {
    pub ty: LocationType,
    pub location_size: u16,
    pub dwarf_regnum: u16,
    pub offset_or_small_constant: i32,
}

#[derive(Copy, Clone, Debug, Default)]
pub struct LiveOut {
    pub dwarf_regnum: u16,
    pub size_in_bytes: u8,
}

#[derive(Copy, Clone, Debug)]
pub enum LocationType {
    Register,
    Direct,
    Indirect,
    Constant,
    ConstantIndex,
}

impl StackMap {
    pub fn parse(raw: &[u8]) -> io::Result<StackMap> {
        let mut reader = Cursor::new(raw);
        let mut map = StackMap::default();

        let version = reader.read_u8()?;
        if version != 3 {
            return Err(io::Error::new(io::ErrorKind::Other, "version is not 3"));
        }
        map.version = version;
        if reader.read_u8()? != 0 {
            return Err(io::Error::new(
                io::ErrorKind::Other,
                "reserved field is not zero (1)",
            ));
        }
        if reader.read_u16::<LittleEndian>()? != 0 {
            return Err(io::Error::new(
                io::ErrorKind::Other,
                "reserved field is not zero (2)",
            ));
        }
        let num_functions = reader.read_u32::<LittleEndian>()?;
        let num_constants = reader.read_u32::<LittleEndian>()?;
        let num_records = reader.read_u32::<LittleEndian>()?;
        for _ in 0..num_functions {
            let mut record = StkSizeRecord::default();
            record.function_address = reader.read_u64::<LittleEndian>()?;
            record.stack_size = reader.read_u64::<LittleEndian>()?;
            record.record_count = reader.read_u64::<LittleEndian>()?;
            map.stk_size_records.push(record);
        }
        for _ in 0..num_constants {
            map.constants.push(Constant {
                large_constant: reader.read_u64::<LittleEndian>()?,
            });
        }
        for _ in 0..num_records {
            let mut record = StkMapRecord::default();

            record.patchpoint_id = reader.read_u64::<LittleEndian>()?;
            record.instruction_offset = reader.read_u32::<LittleEndian>()?;
            if reader.read_u16::<LittleEndian>()? != 0 {
                return Err(io::Error::new(
                    io::ErrorKind::Other,
                    "reserved field is not zero (3)",
                ));
            }
            let num_locations = reader.read_u16::<LittleEndian>()?;
            for _ in 0..num_locations {
                let ty = reader.read_u8()?;

                let mut location = Location {
                    ty: match ty {
                        1 => LocationType::Register,
                        2 => LocationType::Direct,
                        3 => LocationType::Indirect,
                        4 => LocationType::Constant,
                        5 => LocationType::ConstantIndex,
                        _ => {
                            return Err(io::Error::new(
                                io::ErrorKind::Other,
                                "unknown location type",
                            ))
                        }
                    },
                    location_size: 0,
                    dwarf_regnum: 0,
                    offset_or_small_constant: 0,
                };

                if reader.read_u8()? != 0 {
                    return Err(io::Error::new(
                        io::ErrorKind::Other,
                        "reserved field is not zero (4)",
                    ));
                }
                location.location_size = reader.read_u16::<LittleEndian>()?;
                location.dwarf_regnum = reader.read_u16::<LittleEndian>()?;
                if reader.read_u16::<LittleEndian>()? != 0 {
                    return Err(io::Error::new(
                        io::ErrorKind::Other,
                        "reserved field is not zero (5)",
                    ));
                }
                location.offset_or_small_constant = reader.read_i32::<LittleEndian>()?;

                record.locations.push(location);
            }
            if reader.position() % 8 != 0 {
                if reader.read_u32::<LittleEndian>()? != 0 {
                    return Err(io::Error::new(
                        io::ErrorKind::Other,
                        "reserved field is not zero (6)",
                    ));
                }
            }
            if reader.read_u16::<LittleEndian>()? != 0 {
                return Err(io::Error::new(
                    io::ErrorKind::Other,
                    "reserved field is not zero (7)",
                ));
            }
            let num_live_outs = reader.read_u16::<LittleEndian>()?;
            for _ in 0..num_live_outs {
                let mut liveout = LiveOut::default();

                liveout.dwarf_regnum = reader.read_u16::<LittleEndian>()?;
                if reader.read_u8()? != 0 {
                    return Err(io::Error::new(
                        io::ErrorKind::Other,
                        "reserved field is not zero (8)",
                    ));
                }
                liveout.size_in_bytes = reader.read_u8()?;

                record.live_outs.push(liveout);
            }
            if reader.position() % 8 != 0 {
                if reader.read_u32::<LittleEndian>()? != 0 {
                    return Err(io::Error::new(
                        io::ErrorKind::Other,
                        "reserved field is not zero (9)",
                    ));
                }
            }

            map.stk_map_records.push(record);
        }
        Ok(map)
    }
}

fn deref_global(info: &ModuleInfo, idx: usize, deref_into_value: bool) -> Vec<usize> {
    let mut x: Vec<usize> = match GlobalIndex::new(idx).local_or_import(info) {
        LocalOrImport::Local(idx) => vec![Ctx::offset_globals() as usize, idx.index() * 8, 0],
        LocalOrImport::Import(idx) => {
            vec![Ctx::offset_imported_globals() as usize, idx.index() * 8, 0]
        }
    };
    if deref_into_value {
        x.push(0);
    }
    x
}

fn deref_table_base(info: &ModuleInfo, idx: usize, deref_into_value: bool) -> Vec<usize> {
    let mut x: Vec<usize> = match TableIndex::new(idx).local_or_import(info) {
        LocalOrImport::Local(idx) => vec![Ctx::offset_tables() as usize, idx.index() * 8, 0],
        LocalOrImport::Import(idx) => {
            vec![Ctx::offset_imported_tables() as usize, idx.index() * 8, 0]
        }
    };
    if deref_into_value {
        x.push(0);
    }
    x
}

fn deref_table_bound(info: &ModuleInfo, idx: usize, deref_into_value: bool) -> Vec<usize> {
    let mut x: Vec<usize> = match TableIndex::new(idx).local_or_import(info) {
        LocalOrImport::Local(idx) => vec![Ctx::offset_tables() as usize, idx.index() * 8, 8],
        LocalOrImport::Import(idx) => {
            vec![Ctx::offset_imported_tables() as usize, idx.index() * 8, 8]
        }
    };
    if deref_into_value {
        x.push(0);
    }
    x
}

'''
'''--- lib/compiler-llvm/src/translator/state.rs ---
use inkwell::{
    basic_block::BasicBlock,
    values::{BasicValue, BasicValueEnum, PhiValue},
};
use smallvec::SmallVec;
use std::ops::{BitAnd, BitOr, BitOrAssign};
use wasmer_compiler::CompileError;

#[derive(Debug)]
pub enum ControlFrame<'ctx> {
    Block {
        next: BasicBlock<'ctx>,
        phis: SmallVec<[PhiValue<'ctx>; 1]>,
        stack_size_snapshot: usize,
    },
    Loop {
        body: BasicBlock<'ctx>,
        next: BasicBlock<'ctx>,
        phis: SmallVec<[PhiValue<'ctx>; 1]>,
        loop_body_phis: SmallVec<[PhiValue<'ctx>; 1]>,
        stack_size_snapshot: usize,
    },
    IfElse {
        if_then: BasicBlock<'ctx>,
        if_else: BasicBlock<'ctx>,
        next: BasicBlock<'ctx>,
        then_phis: SmallVec<[PhiValue<'ctx>; 1]>,
        else_phis: SmallVec<[PhiValue<'ctx>; 1]>,
        next_phis: SmallVec<[PhiValue<'ctx>; 1]>,
        stack_size_snapshot: usize,
        if_else_state: IfElseState,
    },
}

#[derive(Debug)]
pub enum IfElseState {
    If,
    Else,
}

impl<'ctx> ControlFrame<'ctx> {
    pub fn code_after(&self) -> &BasicBlock<'ctx> {
        match self {
            ControlFrame::Block { ref next, .. }
            | ControlFrame::Loop { ref next, .. }
            | ControlFrame::IfElse { ref next, .. } => next,
        }
    }

    pub fn br_dest(&self) -> &BasicBlock<'ctx> {
        match self {
            ControlFrame::Block { ref next, .. } | ControlFrame::IfElse { ref next, .. } => next,
            ControlFrame::Loop { ref body, .. } => body,
        }
    }

    pub fn phis(&self) -> &[PhiValue<'ctx>] {
        match self {
            ControlFrame::Block { ref phis, .. } | ControlFrame::Loop { ref phis, .. } => {
                phis.as_slice()
            }
            ControlFrame::IfElse { ref next_phis, .. } => next_phis.as_slice(),
        }
    }

    /// PHI nodes for stack values in the loop body.
    pub fn loop_body_phis(&self) -> &[PhiValue<'ctx>] {
        match self {
            ControlFrame::Block { .. } | ControlFrame::IfElse { .. } => &[],
            ControlFrame::Loop {
                ref loop_body_phis, ..
            } => loop_body_phis.as_slice(),
        }
    }

    pub fn is_loop(&self) -> bool {
        matches!(self, ControlFrame::Loop { .. })
    }
}

#[derive(Debug, Default, Eq, PartialEq, Copy, Clone, Hash)]
pub struct ExtraInfo {
    state: u8,
}
impl ExtraInfo {
    // This value is required to be arithmetic 32-bit NaN (or 32x4) by the WAsm
    // machine, but which might not be in the LLVM value. The conversion to
    // arithmetic NaN is pending. It is required for correctness.
    //
    // When applied to a 64-bit value, this flag has no meaning and must be
    // ignored. It may be set in such cases to allow for common handling of
    // 32 and 64-bit operations.
    pub const fn pending_f32_nan() -> ExtraInfo {
        ExtraInfo { state: 1 }
    }

    // This value is required to be arithmetic 64-bit NaN (or 64x2) by the WAsm
    // machine, but which might not be in the LLVM value. The conversion to
    // arithmetic NaN is pending. It is required for correctness.
    //
    // When applied to a 32-bit value, this flag has no meaning and must be
    // ignored. It may be set in such cases to allow for common handling of
    // 32 and 64-bit operations.
    pub const fn pending_f64_nan() -> ExtraInfo {
        ExtraInfo { state: 2 }
    }

    // This value either does not contain a 32-bit NaN, or it contains an
    // arithmetic NaN. In SIMD, applies to all 4 lanes.
    pub const fn arithmetic_f32() -> ExtraInfo {
        ExtraInfo { state: 4 }
    }

    // This value either does not contain a 64-bit NaN, or it contains an
    // arithmetic NaN. In SIMD, applies to both lanes.
    pub const fn arithmetic_f64() -> ExtraInfo {
        ExtraInfo { state: 8 }
    }

    pub const fn has_pending_f32_nan(&self) -> bool {
        self.state & ExtraInfo::pending_f32_nan().state != 0
    }
    pub const fn has_pending_f64_nan(&self) -> bool {
        self.state & ExtraInfo::pending_f64_nan().state != 0
    }
    pub const fn is_arithmetic_f32(&self) -> bool {
        self.state & ExtraInfo::arithmetic_f32().state != 0
    }
    pub const fn is_arithmetic_f64(&self) -> bool {
        self.state & ExtraInfo::arithmetic_f64().state != 0
    }

    pub const fn strip_pending(&self) -> ExtraInfo {
        ExtraInfo {
            state: self.state
                & !(ExtraInfo::pending_f32_nan().state | ExtraInfo::pending_f64_nan().state),
        }
    }
}

// Union two ExtraInfos.
impl BitOr for ExtraInfo {
    type Output = Self;

    fn bitor(self, other: Self) -> Self {
        debug_assert!(!(self.has_pending_f32_nan() && other.has_pending_f64_nan()));
        debug_assert!(!(self.has_pending_f64_nan() && other.has_pending_f32_nan()));
        ExtraInfo {
            state: if self.is_arithmetic_f32() || other.is_arithmetic_f32() {
                ExtraInfo::arithmetic_f32().state
            } else if self.has_pending_f32_nan() || other.has_pending_f32_nan() {
                ExtraInfo::pending_f32_nan().state
            } else {
                0
            } + if self.is_arithmetic_f64() || other.is_arithmetic_f64() {
                ExtraInfo::arithmetic_f64().state
            } else if self.has_pending_f64_nan() || other.has_pending_f64_nan() {
                ExtraInfo::pending_f64_nan().state
            } else {
                0
            },
        }
    }
}
impl BitOrAssign for ExtraInfo {
    fn bitor_assign(&mut self, other: Self) {
        *self = *self | other;
    }
}

// Intersection for ExtraInfo.
impl BitAnd for ExtraInfo {
    type Output = Self;
    fn bitand(self, other: Self) -> Self {
        // Pending canonicalizations are not safe to discard, or even reorder.
        debug_assert!(
            self.has_pending_f32_nan() == other.has_pending_f32_nan()
                || self.is_arithmetic_f32()
                || other.is_arithmetic_f32()
        );
        debug_assert!(
            self.has_pending_f64_nan() == other.has_pending_f64_nan()
                || self.is_arithmetic_f64()
                || other.is_arithmetic_f64()
        );
        let info = match (
            self.is_arithmetic_f32() && other.is_arithmetic_f32(),
            self.is_arithmetic_f64() && other.is_arithmetic_f64(),
        ) {
            (false, false) => Default::default(),
            (true, false) => ExtraInfo::arithmetic_f32(),
            (false, true) => ExtraInfo::arithmetic_f64(),
            (true, true) => ExtraInfo::arithmetic_f32() | ExtraInfo::arithmetic_f64(),
        };
        match (self.has_pending_f32_nan(), self.has_pending_f64_nan()) {
            (false, false) => info,
            (true, false) => info | ExtraInfo::pending_f32_nan(),
            (false, true) => info | ExtraInfo::pending_f64_nan(),
            (true, true) => unreachable!("Can't form ExtraInfo with two pending canonicalizations"),
        }
    }
}

#[derive(Debug)]
pub struct State<'ctx> {
    pub stack: Vec<(BasicValueEnum<'ctx>, ExtraInfo)>,
    control_stack: Vec<ControlFrame<'ctx>>,

    pub reachable: bool,
}

impl<'ctx> State<'ctx> {
    pub fn new() -> Self {
        Self {
            stack: vec![],
            control_stack: vec![],
            reachable: true,
        }
    }

    pub fn has_control_frames(&self) -> bool {
        !self.control_stack.is_empty()
    }

    pub fn reset_stack(&mut self, frame: &ControlFrame<'ctx>) {
        let stack_size_snapshot = match frame {
            ControlFrame::Block {
                stack_size_snapshot,
                ..
            }
            | ControlFrame::Loop {
                stack_size_snapshot,
                ..
            }
            | ControlFrame::IfElse {
                stack_size_snapshot,
                ..
            } => *stack_size_snapshot,
        };
        self.stack.truncate(stack_size_snapshot);
    }

    pub fn outermost_frame(&self) -> Result<&ControlFrame<'ctx>, CompileError> {
        self.control_stack.get(0).ok_or_else(|| {
            CompileError::Codegen("outermost_frame: invalid control stack depth".to_string())
        })
    }

    pub fn frame_at_depth(&self, depth: u32) -> Result<&ControlFrame<'ctx>, CompileError> {
        let index = self
            .control_stack
            .len()
            .checked_sub(1 + (depth as usize))
            .ok_or_else(|| {
                CompileError::Codegen("frame_at_depth: invalid control stack depth".to_string())
            })?;
        Ok(&self.control_stack[index])
    }

    pub fn frame_at_depth_mut(
        &mut self,
        depth: u32,
    ) -> Result<&mut ControlFrame<'ctx>, CompileError> {
        let index = self
            .control_stack
            .len()
            .checked_sub(1 + (depth as usize))
            .ok_or_else(|| {
                CompileError::Codegen("frame_at_depth_mut: invalid control stack depth".to_string())
            })?;
        Ok(&mut self.control_stack[index])
    }

    pub fn pop_frame(&mut self) -> Result<ControlFrame<'ctx>, CompileError> {
        self.control_stack.pop().ok_or_else(|| {
            CompileError::Codegen("pop_frame: cannot pop from control stack".to_string())
        })
    }

    pub fn push1<T: BasicValue<'ctx>>(&mut self, value: T) {
        self.push1_extra(value, Default::default());
    }

    pub fn push1_extra<T: BasicValue<'ctx>>(&mut self, value: T, info: ExtraInfo) {
        self.stack.push((value.as_basic_value_enum(), info));
    }

    pub fn pop1(&mut self) -> Result<BasicValueEnum<'ctx>, CompileError> {
        Ok(self.pop1_extra()?.0)
    }

    pub fn pop1_extra(&mut self) -> Result<(BasicValueEnum<'ctx>, ExtraInfo), CompileError> {
        self.stack
            .pop()
            .ok_or_else(|| CompileError::Codegen("pop1_extra: invalid value stack".to_string()))
    }

    pub fn pop2(&mut self) -> Result<(BasicValueEnum<'ctx>, BasicValueEnum<'ctx>), CompileError> {
        let v2 = self.pop1()?;
        let v1 = self.pop1()?;
        Ok((v1, v2))
    }

    pub fn pop2_extra(
        &mut self,
    ) -> Result<
        (
            (BasicValueEnum<'ctx>, ExtraInfo),
            (BasicValueEnum<'ctx>, ExtraInfo),
        ),
        CompileError,
    > {
        let v2 = self.pop1_extra()?;
        let v1 = self.pop1_extra()?;
        Ok((v1, v2))
    }

    pub fn pop3(
        &mut self,
    ) -> Result<
        (
            BasicValueEnum<'ctx>,
            BasicValueEnum<'ctx>,
            BasicValueEnum<'ctx>,
        ),
        CompileError,
    > {
        let v3 = self.pop1()?;
        let v2 = self.pop1()?;
        let v1 = self.pop1()?;
        Ok((v1, v2, v3))
    }

    pub fn pop3_extra(
        &mut self,
    ) -> Result<
        (
            (BasicValueEnum<'ctx>, ExtraInfo),
            (BasicValueEnum<'ctx>, ExtraInfo),
            (BasicValueEnum<'ctx>, ExtraInfo),
        ),
        CompileError,
    > {
        let v3 = self.pop1_extra()?;
        let v2 = self.pop1_extra()?;
        let v1 = self.pop1_extra()?;
        Ok((v1, v2, v3))
    }

    pub fn peek1_extra(&self) -> Result<(BasicValueEnum<'ctx>, ExtraInfo), CompileError> {
        let index =
            self.stack.len().checked_sub(1).ok_or_else(|| {
                CompileError::Codegen("peek1_extra: invalid value stack".to_string())
            })?;
        Ok(self.stack[index])
    }

    pub fn peekn(&self, n: usize) -> Result<Vec<BasicValueEnum<'ctx>>, CompileError> {
        Ok(self.peekn_extra(n)?.iter().map(|x| x.0).collect())
    }

    pub fn peekn_extra(
        &self,
        n: usize,
    ) -> Result<&[(BasicValueEnum<'ctx>, ExtraInfo)], CompileError> {
        let index =
            self.stack.len().checked_sub(n).ok_or_else(|| {
                CompileError::Codegen("peekn_extra: invalid value stack".to_string())
            })?;
        Ok(&self.stack[index..])
    }

    pub fn popn_save_extra(
        &mut self,
        n: usize,
    ) -> Result<Vec<(BasicValueEnum<'ctx>, ExtraInfo)>, CompileError> {
        let v = self.peekn_extra(n)?.to_vec();
        self.popn(n)?;
        Ok(v)
    }

    pub fn popn(&mut self, n: usize) -> Result<(), CompileError> {
        let index = self
            .stack
            .len()
            .checked_sub(n)
            .ok_or_else(|| CompileError::Codegen("popn: invalid value stack".to_string()))?;

        self.stack.truncate(index);
        Ok(())
    }

    pub fn push_block(&mut self, next: BasicBlock<'ctx>, phis: SmallVec<[PhiValue<'ctx>; 1]>) {
        self.control_stack.push(ControlFrame::Block {
            next,
            phis,
            stack_size_snapshot: self.stack.len(),
        });
    }

    pub fn push_loop(
        &mut self,
        body: BasicBlock<'ctx>,
        next: BasicBlock<'ctx>,
        loop_body_phis: SmallVec<[PhiValue<'ctx>; 1]>,
        phis: SmallVec<[PhiValue<'ctx>; 1]>,
    ) {
        self.control_stack.push(ControlFrame::Loop {
            body,
            next,
            loop_body_phis,
            phis,
            stack_size_snapshot: self.stack.len(),
        });
    }

    pub fn push_if(
        &mut self,
        if_then: BasicBlock<'ctx>,
        if_else: BasicBlock<'ctx>,
        next: BasicBlock<'ctx>,
        then_phis: SmallVec<[PhiValue<'ctx>; 1]>,
        else_phis: SmallVec<[PhiValue<'ctx>; 1]>,
        next_phis: SmallVec<[PhiValue<'ctx>; 1]>,
    ) {
        self.control_stack.push(ControlFrame::IfElse {
            if_then,
            if_else,
            next,
            then_phis,
            else_phis,
            next_phis,
            stack_size_snapshot: self.stack.len(),
            if_else_state: IfElseState::If,
        });
    }
}

'''
'''--- lib/compiler-singlepass/Cargo.toml ---
[package]
name = "wasmer-compiler-singlepass-near"
version = "2.4.1"
description = "Singlepass compiler for Wasmer WebAssembly runtime"
categories = ["wasm"]
keywords = ["wasm", "webassembly", "compiler", "singlepass"]
authors = ["Wasmer Engineering Team <engineering@wasmer.io>"]
repository = "https://github.com/wasmerio/wasmer"
documentation = "https://docs.rs/wasmer-compiler-singlepass/"
license = "MIT"
readme = "README.md"
edition = "2018"

[lib]
name = "wasmer_compiler_singlepass"

[dependencies]
wasmer-compiler = { path = "../compiler", package = "wasmer-compiler-near", version = "=2.4.1", features = ["translator"], default-features = false }
wasmer-vm = { path = "../vm", package = "wasmer-vm-near", version = "=2.4.1" }
wasmer-types = { path = "../types", package = "wasmer-types-near", version = "=2.4.1", default-features = false, features = ["std"] }
rayon = { version = "1.5", optional = true }
hashbrown = { version = "0.11", optional = true }
more-asserts = "0.2"
dynasm = "1.0"
dynasmrt = "1.0"
lazy_static = "1.4"
byteorder = "1.3"
smallvec = "1.6"
memoffset = "0.6"

[dev-dependencies]
target-lexicon = { version = "0.12.2", default-features = false }

[badges]
maintenance = { status = "actively-developed" }

[features]
default = ["std", "rayon"]
std = ["wasmer-compiler/std", "wasmer-types/std"]
core = ["hashbrown", "wasmer-types/core"]

'''
'''--- lib/compiler-singlepass/README.md ---
# `wasmer-compiler-singlepass` [![Build Status](https://github.com/wasmerio/wasmer/workflows/build/badge.svg?style=flat-square)](https://github.com/wasmerio/wasmer/actions?query=workflow%3Abuild) [![Join Wasmer Slack](https://img.shields.io/static/v1?label=Slack&message=join%20chat&color=brighgreen&style=flat-square)](https://slack.wasmer.io) [![MIT License](https://img.shields.io/github/license/wasmerio/wasmer.svg?style=flat-square)](https://github.com/wasmerio/wasmer/blob/master/LICENSE) [![crates.io](https://img.shields.io/crates/v/wasmer-compiler-singlepass.svg)](https://crates.io/crates/wasmer-compiler-singlepass)

This crate contains a compiler implementation based on the Singlepass linear compiler.

## Usage

```rust
use wasmer::{Store, Universal};
use wasmer_compiler_singlepass::Singlepass;

let compiler = Singlepass::new();
// Put it into an engine and add it to the store
let store = Store::new(&Universal::new(compiler).engine());
```

*Note: you can find a [full working example using Singlepass compiler
here][example].*

## When to use Singlepass

Singlepass is designed to emit compiled code at linear time, as such
is not prone to JIT bombs and also offers great compilation
performance orders of magnitude faster than
[`wasmer-compiler-cranelift`] and [`wasmer-compiler-llvm`], however
with a bit slower runtime speed.

The fact that singlepass is not prone to JIT bombs and offers a very
predictable compilation speed makes it ideal for **blockchains** and other
systems where fast and consistent compilation times are very critical.

[example]: https://github.com/wasmerio/wasmer/blob/master/examples/compiler_singlepass.rs
[`wasmer-compiler-cranelift`]: https://github.com/wasmerio/wasmer/tree/master/lib/compiler-cranelift
[`wasmer-compiler-llvm`]: https://github.com/wasmerio/wasmer/tree/master/lib/compiler-llvm

'''
'''--- lib/compiler-singlepass/src/address_map.rs ---
use wasmer_compiler::{FunctionAddressMap, FunctionBodyData, InstructionAddressMap, SourceLoc};

pub fn get_function_address_map<'data>(
    instructions: Vec<InstructionAddressMap>,
    data: &FunctionBodyData<'data>,
    body_len: usize,
) -> FunctionAddressMap {
    // Generate source loc for a function start/end to identify boundary within module.
    // It will wrap around if byte code is larger than 4 GB.
    let start_srcloc = SourceLoc::new(data.module_offset as u32);
    let end_srcloc = SourceLoc::new((data.module_offset + data.data.len()) as u32);

    FunctionAddressMap {
        instructions,
        start_srcloc,
        end_srcloc,
        body_offset: 0,
        body_len,
    }
}

'''
'''--- lib/compiler-singlepass/src/codegen_x64.rs ---
use crate::address_map::get_function_address_map;
use crate::config::{Intrinsic, IntrinsicKind};
use crate::{config::Singlepass, emitter_x64::*, machine::Machine, x64_decl::*};
use dynasmrt::{x64::X64Relocation, AssemblyOffset, DynamicLabel, DynasmApi, VecAssembler};
use memoffset::offset_of;
use smallvec::{smallvec, SmallVec};
use std::cmp::max;
use std::iter;
use wasmer_compiler::wasmparser::{
    MemoryImmediate, Operator, Type as WpType, TypeOrFuncType as WpTypeOrFuncType,
};
use wasmer_compiler::{
    CallingConvention, CompiledFunction, CompiledFunctionFrameInfo, CustomSection,
    CustomSectionProtection, FunctionBody, FunctionBodyData, InstructionAddressMap,
    ModuleTranslationState, Relocation, RelocationKind, RelocationTarget, SectionBody,
    SectionIndex, SourceLoc,
};
use wasmer_types::{
    entity::{EntityRef, PrimaryMap, SecondaryMap},
    FastGasCounter, FunctionType,
};
use wasmer_types::{
    FunctionIndex, GlobalIndex, LocalFunctionIndex, LocalMemoryIndex, MemoryIndex, ModuleInfo,
    SignatureIndex, TableIndex, Type,
};
use wasmer_vm::{TableStyle, TrapCode, VMBuiltinFunctionIndex, VMOffsets};

type Assembler = VecAssembler<X64Relocation>;

/// The singlepass per-function code generator.
pub(crate) struct FuncGen<'a> {
    // Immutable properties assigned at creation time.
    /// Static module information.
    module: &'a ModuleInfo,

    /// State of module translation.
    module_translation_state: &'a ModuleTranslationState,

    /// ModuleInfo compilation config.
    config: &'a Singlepass,

    /// Offsets of vmctx fields.
    vmoffsets: &'a VMOffsets,

    // // Memory plans.
    // memory_styles: &'a PrimaryMap<MemoryIndex, MemoryStyle>,

    // // Table plans.
    // table_styles: &'a PrimaryMap<TableIndex, TableStyle>,
    /// Function signature.
    signature: FunctionType,

    // Working storage.
    /// The assembler.
    ///
    /// This should be changed to `Vec<u8>` for platform independency, but dynasm doesn't (yet)
    /// support automatic relative relocations for `Vec<u8>`.
    assembler: Assembler,

    /// Memory locations of local variables.
    locals: Vec<Location>,

    /// Types of local variables, including arguments.
    local_types: Vec<WpType>,

    /// Value stack.
    value_stack: Vec<Location>,

    /// Max stack depth.
    max_stack_depth: usize,

    /// Location to patch when we know the max stack depth.
    stack_check_offset: AssemblyOffset,

    /// Metadata about floating point values on the stack.
    fp_stack: Vec<FloatValue>,

    /// A list of frames describing the current control stack.
    control_stack: Vec<ControlFrame>,

    /// Low-level machine state.
    machine: Machine,

    /// Nesting level of unreachable code.
    unreachable_depth: usize,

    /// Relocation information.
    relocations: Vec<Relocation>,

    /// A set of special labels for trapping.
    special_labels: SpecialLabelSet,

    /// The source location for the current operator.
    src_loc: u32,

    /// Map from byte offset into wasm function to range of native instructions.
    ///
    // Ordered by increasing InstructionAddressMap::srcloc.
    instructions_address_map: Vec<InstructionAddressMap>,

    /// Calling convention to use.
    calling_convention: CallingConvention,
}

struct SpecialLabelSet {
    integer_division_by_zero: DynamicLabel,
    integer_overflow: DynamicLabel,
    bad_conversion_to_integer: DynamicLabel,
    heap_access_oob: DynamicLabel,
    table_access_oob: DynamicLabel,
    indirect_call_null: DynamicLabel,
    bad_signature: DynamicLabel,
    gas_limit_exceeded: DynamicLabel,
    stack_overflow: DynamicLabel,
}

/// Metadata about a floating-point value.
#[derive(Copy, Clone, Debug)]
struct FloatValue {
    /// Do we need to canonicalize the value before its bit pattern is next observed? If so, how?
    canonicalization: Option<CanonicalizeType>,

    /// Corresponding depth in the main value stack.
    depth: usize,
}

impl FloatValue {
    fn new(depth: usize) -> Self {
        FloatValue {
            canonicalization: None,
            depth,
        }
    }

    fn cncl_f32(depth: usize) -> Self {
        FloatValue {
            canonicalization: Some(CanonicalizeType::F32),
            depth,
        }
    }

    fn cncl_f64(depth: usize) -> Self {
        FloatValue {
            canonicalization: Some(CanonicalizeType::F64),
            depth,
        }
    }

    fn promote(self, depth: usize) -> FloatValue {
        FloatValue {
            canonicalization: match self.canonicalization {
                Some(CanonicalizeType::F32) => Some(CanonicalizeType::F64),
                Some(CanonicalizeType::F64) => panic!("cannot promote F64"),
                None => None,
            },
            depth,
        }
    }

    fn demote(self, depth: usize) -> FloatValue {
        FloatValue {
            canonicalization: match self.canonicalization {
                Some(CanonicalizeType::F64) => Some(CanonicalizeType::F32),
                Some(CanonicalizeType::F32) => panic!("cannot demote F32"),
                None => None,
            },
            depth,
        }
    }
}

/// Type of a pending canonicalization floating point value.
/// Sometimes we don't have the type information elsewhere and therefore we need to track it here.
#[derive(Copy, Clone, Debug)]
enum CanonicalizeType {
    F32,
    F64,
}

impl CanonicalizeType {
    fn to_size(&self) -> Size {
        match self {
            CanonicalizeType::F32 => Size::S32,
            CanonicalizeType::F64 => Size::S64,
        }
    }
}

trait PopMany<T> {
    fn peek1(&self) -> Result<&T, CodegenError>;
    fn pop1(&mut self) -> Result<T, CodegenError>;
    fn pop2(&mut self) -> Result<(T, T), CodegenError>;
}

impl<T> PopMany<T> for Vec<T> {
    fn peek1(&self) -> Result<&T, CodegenError> {
        self.last().ok_or_else(|| CodegenError {
            message: "peek1() expects at least 1 element".into(),
        })
    }
    fn pop1(&mut self) -> Result<T, CodegenError> {
        self.pop().ok_or_else(|| CodegenError {
            message: "pop1() expects at least 1 element".into(),
        })
    }
    fn pop2(&mut self) -> Result<(T, T), CodegenError> {
        if self.len() < 2 {
            return Err(CodegenError {
                message: "pop2() expects at least 2 elements".into(),
            });
        }

        let right = self.pop().unwrap();
        let left = self.pop().unwrap();
        Ok((left, right))
    }
}

trait WpTypeExt {
    fn is_float(&self) -> bool;
}

impl WpTypeExt for WpType {
    fn is_float(&self) -> bool {
        match self {
            WpType::F32 | WpType::F64 => true,
            _ => false,
        }
    }
}

#[derive(Debug)]
pub(crate) struct ControlFrame {
    pub(crate) label: DynamicLabel,
    pub(crate) loop_like: bool,
    pub(crate) if_else: IfElseState,
    pub(crate) returns: SmallVec<[WpType; 1]>,
    pub(crate) value_stack_depth: usize,
    pub(crate) fp_stack_depth: usize,
}

#[derive(Debug, Copy, Clone)]
pub(crate) enum IfElseState {
    None,
    If(DynamicLabel),
    Else,
}

#[derive(Debug)]
pub(crate) struct CodegenError {
    pub(crate) message: String,
}

/// Abstraction for a 2-input, 1-output operator. Can be an integer/floating-point
/// binop/cmpop.
struct I2O1 {
    loc_a: Location,
    loc_b: Location,
    ret: Location,
}

impl<'a> FuncGen<'a> {
    /// Set the source location of the Wasm to the given offset.
    pub(crate) fn set_srcloc(&mut self, offset: u32) {
        self.src_loc = offset;
    }

    fn get_location_released(&mut self, loc: Location) -> Location {
        self.machine.release_locations(&mut self.assembler, &[loc]);
        loc
    }

    fn update_max_stack_depth(&mut self) {
        self.max_stack_depth = max(
            self.max_stack_depth,
            self.value_stack.len() + self.fp_stack.len(),
        );
    }

    fn pop_value_released(&mut self) -> Location {
        self.update_max_stack_depth();
        let loc = self
            .value_stack
            .pop()
            .expect("pop_value_released: value stack is empty");
        self.get_location_released(loc)
    }

    /// Prepare data for binary operator with 2 inputs and 1 output.
    fn i2o1_prepare(&mut self, ty: WpType) -> I2O1 {
        let loc_b = self.pop_value_released();
        let loc_a = self.pop_value_released();
        let ret = self
            .machine
            .acquire_locations(&mut self.assembler, &[(ty)], false)[0];
        self.value_stack.push(ret);
        I2O1 { loc_a, loc_b, ret }
    }

    fn emit_call(&mut self, function_index: u32) -> Result<(), CodegenError> {
        let function_index = function_index as usize;

        let sig_index = *self
            .module
            .functions
            .get(FunctionIndex::new(function_index))
            .unwrap();
        let sig = self.module.signatures.get(sig_index).unwrap();
        let param_types: SmallVec<[WpType; 8]> =
            sig.params().iter().cloned().map(type_to_wp_type).collect();
        let return_types: SmallVec<[WpType; 1]> =
            sig.results().iter().cloned().map(type_to_wp_type).collect();

        let params: SmallVec<[_; 8]> = self
            .value_stack
            .drain(self.value_stack.len() - param_types.len()..)
            .collect();
        self.machine.release_locations_only_regs(&params);

        // Pop arguments off the FP stack and canonicalize them if needed.
        //
        // Canonicalization state will be lost across function calls, so early canonicalization
        // is necessary here.
        self.update_max_stack_depth();
        while let Some(fp) = self.fp_stack.last() {
            if fp.depth >= self.value_stack.len() {
                let index = fp.depth - self.value_stack.len();
                if self.assembler.arch_supports_canonicalize_nan()
                    && self.config.enable_nan_canonicalization
                    && fp.canonicalization.is_some()
                {
                    let size = fp.canonicalization.unwrap().to_size();
                    self.canonicalize_nan(size, params[index], params[index]);
                }
                self.fp_stack.pop().unwrap();
            } else {
                break;
            }
        }

        if let Some(intrinsic) = self.check_intrinsic(function_index, &params) {
            self.emit_intrinsic(intrinsic, &params)?
        } else {
            let reloc_at = self.assembler.get_offset().0 + self.assembler.arch_mov64_imm_offset();
            // Imported functions are called through trampolines placed as custom sections.
            let imports = self.module.import_counts.functions as usize;
            let reloc_target = if function_index < imports {
                RelocationTarget::CustomSection(SectionIndex::new(function_index))
            } else {
                RelocationTarget::LocalFunc(LocalFunctionIndex::new(function_index - imports))
            };
            self.relocations.push(Relocation {
                kind: RelocationKind::Abs8,
                reloc_target,
                offset: reloc_at as u32,
                addend: 0,
            });

            // RAX is preserved on entry to `emit_call_sysv` callback.
            // The Imm64 value is relocated by the JIT linker.
            self.assembler.emit_mov(
                Size::S64,
                Location::Imm64(std::u64::MAX),
                Location::GPR(GPR::RAX),
            );

            self.emit_call_native(
                |this| {
                    this.assembler.emit_call_location(Location::GPR(GPR::RAX));
                },
                params.iter().copied(),
            )?;

            self.machine
                .release_locations_only_stack(&mut self.assembler, &params);

            if !return_types.is_empty() {
                let ret = self.machine.acquire_locations(
                    &mut self.assembler,
                    &[(return_types[0])],
                    false,
                )[0];
                self.value_stack.push(ret);
                if return_types[0].is_float() {
                    self.assembler
                        .emit_mov(Size::S64, Location::XMM(XMM::XMM0), ret);
                    self.fp_stack
                        .push(FloatValue::new(self.value_stack.len() - 1));
                } else {
                    self.assembler
                        .emit_mov(Size::S64, Location::GPR(GPR::RAX), ret);
                }
            }
        }

        Ok(())
    }

    fn check_intrinsic(
        &mut self,
        index: usize,
        params: &SmallVec<[Location; 8]>,
    ) -> Option<Intrinsic> {
        let function_index = FunctionIndex::new(index);
        let signature_index = self.module.functions[function_index];
        let signature = &self.module.signatures[signature_index];
        // Returns None if not imported.
        let import_name = self
            .module_translation_state
            .import_map
            .get(&function_index)?;
        // TODO: can keep intrinsics in above map, but not sure if we'll have
        //   significant amount of them to make it important.
        for intrinsic in &self.config.intrinsics {
            if intrinsic.name == *import_name
                && intrinsic.signature == *signature
                && intrinsic.is_params_ok(params)
            {
                return Some(intrinsic.clone());
            }
        }
        None
    }

    fn emit_intrinsic(
        &mut self,
        intrinsic: Intrinsic,
        params: &SmallVec<[Location; 8]>,
    ) -> Result<(), CodegenError> {
        match intrinsic.kind {
            IntrinsicKind::Gas => {
                let counter_offset = offset_of!(FastGasCounter, burnt_gas) as i32;
                let gas_limit_offset = offset_of!(FastGasCounter, gas_limit) as i32;
                let opcode_cost_offset = offset_of!(FastGasCounter, opcode_cost) as i32;
                // Recheck offsets, to make sure offsets will never change.
                assert_eq!(counter_offset, 0);
                assert_eq!(gas_limit_offset, 8);
                assert_eq!(opcode_cost_offset, 16);
                assert_eq!(params.len(), 1);
                let count_location = params[0];
                let base_reg = self.machine.acquire_temp_gpr().unwrap();
                // Load gas counter base.
                self.assembler.emit_mov(
                    Size::S64,
                    Location::Memory(
                        Machine::get_vmctx_reg(),
                        self.vmoffsets.vmctx_gas_limiter_pointer() as i32,
                    ),
                    Location::GPR(base_reg),
                );
                let current_burnt_reg = self.machine.acquire_temp_gpr().unwrap();
                // Read current gas counter.
                self.assembler.emit_mov(
                    Size::S64,
                    Location::Memory(base_reg, counter_offset),
                    Location::GPR(current_burnt_reg),
                );
                // Read opcode cost.
                let count_reg = self.machine.acquire_temp_gpr().unwrap();
                self.assembler.emit_mov(
                    Size::S64,
                    Location::Memory(base_reg, opcode_cost_offset),
                    Location::GPR(count_reg),
                );
                // Multiply instruction count by opcode cost.
                match count_location {
                    Location::Imm32(imm) => self.assembler.emit_imul_imm32_gpr64(imm, count_reg),
                    _ => assert!(false),
                }
                // Compute new cost.
                self.assembler.emit_add(
                    Size::S64,
                    Location::GPR(count_reg),
                    Location::GPR(current_burnt_reg),
                );
                self.assembler
                    .emit_jmp(Condition::Overflow, self.special_labels.integer_overflow);
                // Compare with the limit.
                self.assembler.emit_cmp(
                    Size::S64,
                    Location::GPR(current_burnt_reg),
                    Location::Memory(base_reg, gas_limit_offset),
                );
                // Write new gas counter unconditionally, so that runtime can sort out limits case.
                self.assembler.emit_mov(
                    Size::S64,
                    Location::GPR(current_burnt_reg),
                    Location::Memory(base_reg, counter_offset),
                );
                self.assembler.emit_jmp(
                    Condition::BelowEqual,
                    self.special_labels.gas_limit_exceeded,
                );
                self.machine.release_temp_gpr(base_reg);
                self.machine.release_temp_gpr(current_burnt_reg);
                self.machine.release_temp_gpr(count_reg);
            }
        }
        Ok(())
    }

    fn emit_trap(&mut self, code: TrapCode) {
        let label = self.assembler.get_label();
        self.assembler.emit_label(label);
        self.assembler.emit_lea_label(
            label,
            Machine::get_param_location(0, self.calling_convention),
        );
        self.assembler.emit_mov(
            Size::S32,
            Location::Imm32(code as u32),
            Machine::get_param_location(1, self.calling_convention),
        );
        // Align stack.
        self.assembler.emit_and(
            Size::S64,
            Location::Imm32(0xfffffff0),
            Location::GPR(GPR::RSP),
        );
        let offset = self.vmoffsets.vmctx_trap_handler();
        self.assembler
            .emit_call_location(Location::Memory(Machine::get_vmctx_reg(), offset as i32));
    }

    /// Canonicalizes the floating point value at `input` into `output`.
    fn canonicalize_nan(&mut self, sz: Size, input: Location, output: Location) {
        let tmp1 = self.machine.acquire_temp_xmm().unwrap();
        let tmp2 = self.machine.acquire_temp_xmm().unwrap();
        let tmp3 = self.machine.acquire_temp_xmm().unwrap();

        self.emit_relaxed_binop(Assembler::emit_mov, sz, input, Location::XMM(tmp1));
        let tmpg1 = self.machine.acquire_temp_gpr().unwrap();

        match sz {
            Size::S32 => {
                self.assembler
                    .emit_vcmpunordss(tmp1, XMMOrMemory::XMM(tmp1), tmp2);
                self.assembler.emit_mov(
                    Size::S32,
                    Location::Imm32(0x7FC0_0000), // Canonical NaN
                    Location::GPR(tmpg1),
                );
                self.assembler
                    .emit_mov(Size::S64, Location::GPR(tmpg1), Location::XMM(tmp3));
                self.assembler
                    .emit_vblendvps(tmp2, XMMOrMemory::XMM(tmp3), tmp1, tmp1);
            }
            Size::S64 => {
                self.assembler
                    .emit_vcmpunordsd(tmp1, XMMOrMemory::XMM(tmp1), tmp2);
                self.assembler.emit_mov(
                    Size::S64,
                    Location::Imm64(0x7FF8_0000_0000_0000), // Canonical NaN
                    Location::GPR(tmpg1),
                );
                self.assembler
                    .emit_mov(Size::S64, Location::GPR(tmpg1), Location::XMM(tmp3));
                self.assembler
                    .emit_vblendvpd(tmp2, XMMOrMemory::XMM(tmp3), tmp1, tmp1);
            }
            _ => unreachable!(),
        }

        self.emit_relaxed_binop(Assembler::emit_mov, sz, Location::XMM(tmp1), output);

        self.machine.release_temp_gpr(tmpg1);
        self.machine.release_temp_xmm(tmp3);
        self.machine.release_temp_xmm(tmp2);
        self.machine.release_temp_xmm(tmp1);
    }

    /// Moves `loc` to a valid location for `div`/`idiv`.
    fn emit_relaxed_xdiv(&mut self, signed: bool, sz: Size, loc: Location) {
        self.assembler.emit_cmp(sz, Location::Imm32(0), loc);
        self.assembler.emit_jmp(
            Condition::Equal,
            self.special_labels.integer_division_by_zero,
        );

        // Boundary checks for integer overflow. It clearly doesn't make sense for
        // unsigned division, as numerator is of same size as the actual result, and divisor is
        // always >= 1.
        // For signed division we can get out of bound only when divide MIN_INT / -1,
        // as result will be MAX_INT+1 so test for that case explicitly.
        if signed {
            let end = self.assembler.get_label();
            // Check if divisor is -1.
            self.assembler
                .emit_cmp(sz, Location::Imm32(0xffffffff), loc);
            self.assembler.emit_jmp(Condition::NotEqual, end);
            // Check if numerator is MIN_INT.
            match sz {
                Size::S32 => {
                    self.assembler.emit_cmp(
                        sz,
                        Location::Imm32(0x80000000u32),
                        Location::GPR(GPR::RAX),
                    );
                }
                Size::S64 => {
                    self.assembler.emit_mov(
                        sz,
                        Location::Imm64(0x8000000000000000u64),
                        Location::GPR(GPR::RCX),
                    );
                    self.assembler
                        .emit_cmp(sz, Location::GPR(GPR::RCX), Location::GPR(GPR::RAX));
                }
                _ => assert!(false),
            }
            self.assembler.emit_jmp(Condition::NotEqual, end);
            self.assembler
                .emit_jmp(Condition::None, self.special_labels.integer_overflow);
            self.assembler.emit_label(end);
        }

        match loc {
            Location::Imm64(_) | Location::Imm32(_) => {
                self.assembler.emit_mov(sz, loc, Location::GPR(GPR::RCX)); // must not be used during div (rax, rdx)
                if signed {
                    self.assembler.emit_idiv(sz, Location::GPR(GPR::RCX));
                } else {
                    self.assembler.emit_div(sz, Location::GPR(GPR::RCX));
                }
            }
            _ => {
                if signed {
                    self.assembler.emit_idiv(sz, loc);
                } else {
                    self.assembler.emit_div(sz, loc);
                }
            }
        }
    }

    /// Moves `src` and `dst` to valid locations for `movzx`/`movsx`.
    fn emit_relaxed_zx_sx(
        &mut self,
        op: fn(&mut Assembler, Size, Location, Size, Location),
        sz_src: Size,
        mut src: Location,
        sz_dst: Size,
        dst: Location,
    ) -> Result<(), CodegenError> {
        let inner = |m: &mut Machine, a: &mut Assembler, src: Location| match dst {
            Location::Imm32(_) | Location::Imm64(_) => {
                return Err(CodegenError {
                    message: "emit_relaxed_zx_sx dst Imm: unreachable code".to_string(),
                })
            }
            Location::Memory(_, _) => {
                let tmp_dst = m.acquire_temp_gpr().unwrap();
                op(a, sz_src, src, sz_dst, Location::GPR(tmp_dst));
                a.emit_mov(Size::S64, Location::GPR(tmp_dst), dst);

                m.release_temp_gpr(tmp_dst);
                Ok(())
            }
            Location::GPR(_) => {
                op(a, sz_src, src, sz_dst, dst);
                Ok(())
            }
            _ => {
                return Err(CodegenError {
                    message: "emit_relaxed_zx_sx dst: unreachable code".to_string(),
                })
            }
        };

        match src {
            Location::Imm32(_) | Location::Imm64(_) => {
                let tmp_src = self.machine.acquire_temp_gpr().unwrap();
                self.assembler
                    .emit_mov(Size::S64, src, Location::GPR(tmp_src));
                src = Location::GPR(tmp_src);

                inner(&mut self.machine, &mut self.assembler, src)?;

                self.machine.release_temp_gpr(tmp_src);
            }
            Location::GPR(_) | Location::Memory(_, _) => {
                inner(&mut self.machine, &mut self.assembler, src)?
            }
            _ => {
                return Err(CodegenError {
                    message: "emit_relaxed_zx_sx src: unreachable code".to_string(),
                })
            }
        }
        Ok(())
    }

    /// Moves `src` and `dst` to valid locations for generic instructions.
    fn emit_relaxed_binop(
        &mut self,
        op: fn(&mut Assembler, Size, Location, Location),
        sz: Size,
        src: Location,
        dst: Location,
    ) {
        enum RelaxMode {
            Direct,
            SrcToGPR,
            DstToGPR,
            BothToGPR,
        }
        let mode = match (src, dst) {
            (Location::GPR(_), Location::GPR(_))
                if (op as *const u8 == Assembler::emit_imul as *const u8) =>
            {
                RelaxMode::Direct
            }
            _ if (op as *const u8 == Assembler::emit_imul as *const u8) => RelaxMode::BothToGPR,

            (Location::Memory(_, _), Location::Memory(_, _)) => RelaxMode::SrcToGPR,
            (Location::Imm64(_), Location::Imm64(_)) | (Location::Imm64(_), Location::Imm32(_)) => {
                RelaxMode::BothToGPR
            }
            (_, Location::Imm32(_)) | (_, Location::Imm64(_)) => RelaxMode::DstToGPR,
            (Location::Imm64(_), Location::Memory(_, _)) => RelaxMode::SrcToGPR,
            (Location::Imm64(_), Location::GPR(_))
                if (op as *const u8 != Assembler::emit_mov as *const u8) =>
            {
                RelaxMode::SrcToGPR
            }
            (_, Location::XMM(_)) => RelaxMode::SrcToGPR,
            _ => RelaxMode::Direct,
        };

        match mode {
            RelaxMode::SrcToGPR => {
                let temp = self.machine.acquire_temp_gpr().unwrap();
                self.assembler.emit_mov(sz, src, Location::GPR(temp));
                op(&mut self.assembler, sz, Location::GPR(temp), dst);
                self.machine.release_temp_gpr(temp);
            }
            RelaxMode::DstToGPR => {
                let temp = self.machine.acquire_temp_gpr().unwrap();
                self.assembler.emit_mov(sz, dst, Location::GPR(temp));
                op(&mut self.assembler, sz, src, Location::GPR(temp));
                self.machine.release_temp_gpr(temp);
            }
            RelaxMode::BothToGPR => {
                let temp_src = self.machine.acquire_temp_gpr().unwrap();
                let temp_dst = self.machine.acquire_temp_gpr().unwrap();
                self.assembler.emit_mov(sz, src, Location::GPR(temp_src));
                self.assembler.emit_mov(sz, dst, Location::GPR(temp_dst));
                op(
                    &mut self.assembler,
                    sz,
                    Location::GPR(temp_src),
                    Location::GPR(temp_dst),
                );
                match dst {
                    Location::Memory(_, _) | Location::GPR(_) => {
                        self.assembler.emit_mov(sz, Location::GPR(temp_dst), dst);
                    }
                    _ => {}
                }
                self.machine.release_temp_gpr(temp_dst);
                self.machine.release_temp_gpr(temp_src);
            }
            RelaxMode::Direct => {
                op(&mut self.assembler, sz, src, dst);
            }
        }
    }

    /// Moves `src1` and `src2` to valid locations and possibly adds a layer of indirection for `dst` for AVX instructions.
    fn emit_relaxed_avx(
        &mut self,
        op: fn(&mut Assembler, XMM, XMMOrMemory, XMM),
        src1: Location,
        src2: Location,
        dst: Location,
    ) -> Result<(), CodegenError> {
        self.emit_relaxed_avx_base(
            |this, src1, src2, dst| op(&mut this.assembler, src1, src2, dst),
            src1,
            src2,
            dst,
        )?;
        Ok(())
    }

    /// Moves `src1` and `src2` to valid locations and possibly adds a layer of indirection for `dst` for AVX instructions.
    fn emit_relaxed_avx_base<F: FnOnce(&mut Self, XMM, XMMOrMemory, XMM)>(
        &mut self,
        op: F,
        src1: Location,
        src2: Location,
        dst: Location,
    ) -> Result<(), CodegenError> {
        let tmp1 = self.machine.acquire_temp_xmm().unwrap();
        let tmp2 = self.machine.acquire_temp_xmm().unwrap();
        let tmp3 = self.machine.acquire_temp_xmm().unwrap();
        let tmpg = self.machine.acquire_temp_gpr().unwrap();

        let src1 = match src1 {
            Location::XMM(x) => x,
            Location::GPR(_) | Location::Memory(_, _) => {
                self.assembler
                    .emit_mov(Size::S64, src1, Location::XMM(tmp1));
                tmp1
            }
            Location::Imm32(_) => {
                self.assembler
                    .emit_mov(Size::S32, src1, Location::GPR(tmpg));
                self.assembler
                    .emit_mov(Size::S32, Location::GPR(tmpg), Location::XMM(tmp1));
                tmp1
            }
            Location::Imm64(_) => {
                self.assembler
                    .emit_mov(Size::S64, src1, Location::GPR(tmpg));
                self.assembler
                    .emit_mov(Size::S64, Location::GPR(tmpg), Location::XMM(tmp1));
                tmp1
            }
            _ => {
                return Err(CodegenError {
                    message: "emit_relaxed_avx_base src1: unreachable code".to_string(),
                })
            }
        };

        let src2 = match src2 {
            Location::XMM(x) => XMMOrMemory::XMM(x),
            Location::Memory(base, disp) => XMMOrMemory::Memory(base, disp),
            Location::GPR(_) => {
                self.assembler
                    .emit_mov(Size::S64, src2, Location::XMM(tmp2));
                XMMOrMemory::XMM(tmp2)
            }
            Location::Imm32(_) => {
                self.assembler
                    .emit_mov(Size::S32, src2, Location::GPR(tmpg));
                self.assembler
                    .emit_mov(Size::S32, Location::GPR(tmpg), Location::XMM(tmp2));
                XMMOrMemory::XMM(tmp2)
            }
            Location::Imm64(_) => {
                self.assembler
                    .emit_mov(Size::S64, src2, Location::GPR(tmpg));
                self.assembler
                    .emit_mov(Size::S64, Location::GPR(tmpg), Location::XMM(tmp2));
                XMMOrMemory::XMM(tmp2)
            }
            _ => {
                return Err(CodegenError {
                    message: "emit_relaxed_avx_base src2: unreachable code".to_string(),
                })
            }
        };

        match dst {
            Location::XMM(x) => {
                op(self, src1, src2, x);
            }
            Location::Memory(_, _) | Location::GPR(_) => {
                op(self, src1, src2, tmp3);
                self.assembler.emit_mov(Size::S64, Location::XMM(tmp3), dst);
            }
            _ => {
                return Err(CodegenError {
                    message: "emit_relaxed_avx_base dst: unreachable code".to_string(),
                })
            }
        }

        self.machine.release_temp_gpr(tmpg);
        self.machine.release_temp_xmm(tmp3);
        self.machine.release_temp_xmm(tmp2);
        self.machine.release_temp_xmm(tmp1);
        Ok(())
    }

    /// I32 binary operation with both operands popped from the virtual stack.
    fn emit_binop_i32(&mut self, f: fn(&mut Assembler, Size, Location, Location)) {
        // Using Red Zone here.
        let I2O1 { loc_a, loc_b, ret } = self.i2o1_prepare(WpType::I32);
        if loc_a != ret {
            let tmp = self.machine.acquire_temp_gpr().unwrap();
            self.emit_relaxed_binop(Assembler::emit_mov, Size::S32, loc_a, Location::GPR(tmp));
            self.emit_relaxed_binop(f, Size::S32, loc_b, Location::GPR(tmp));
            self.emit_relaxed_binop(Assembler::emit_mov, Size::S32, Location::GPR(tmp), ret);
            self.machine.release_temp_gpr(tmp);
        } else {
            self.emit_relaxed_binop(f, Size::S32, loc_b, ret);
        }
    }

    /// I64 binary operation with both operands popped from the virtual stack.
    fn emit_binop_i64(&mut self, f: fn(&mut Assembler, Size, Location, Location)) {
        // Using Red Zone here.
        let I2O1 { loc_a, loc_b, ret } = self.i2o1_prepare(WpType::I64);

        if loc_a != ret {
            let tmp = self.machine.acquire_temp_gpr().unwrap();
            self.emit_relaxed_binop(Assembler::emit_mov, Size::S64, loc_a, Location::GPR(tmp));
            self.emit_relaxed_binop(f, Size::S64, loc_b, Location::GPR(tmp));
            self.emit_relaxed_binop(Assembler::emit_mov, Size::S64, Location::GPR(tmp), ret);
            self.machine.release_temp_gpr(tmp);
        } else {
            self.emit_relaxed_binop(f, Size::S64, loc_b, ret);
        }
    }

    /// I32 comparison with `loc_b` from input.
    fn emit_cmpop_i32_dynamic_b(
        &mut self,
        c: Condition,
        loc_b: Location,
    ) -> Result<(), CodegenError> {
        // Using Red Zone here.
        let loc_a = self.pop_value_released();

        let ret = self
            .machine
            .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];
        match ret {
            Location::GPR(x) => {
                self.emit_relaxed_binop(Assembler::emit_cmp, Size::S32, loc_b, loc_a);
                self.assembler.emit_set(c, x);
                self.assembler
                    .emit_and(Size::S32, Location::Imm32(0xff), Location::GPR(x));
            }
            Location::Memory(_, _) => {
                let tmp = self.machine.acquire_temp_gpr().unwrap();
                self.emit_relaxed_binop(Assembler::emit_cmp, Size::S32, loc_b, loc_a);
                self.assembler.emit_set(c, tmp);
                self.assembler
                    .emit_and(Size::S32, Location::Imm32(0xff), Location::GPR(tmp));
                self.assembler.emit_mov(Size::S32, Location::GPR(tmp), ret);
                self.machine.release_temp_gpr(tmp);
            }
            _ => {
                return Err(CodegenError {
                    message: "emit_cmpop_i32_dynamic_b ret: unreachable code".to_string(),
                })
            }
        }
        self.value_stack.push(ret);
        Ok(())
    }

    /// I32 comparison with both operands popped from the virtual stack.
    fn emit_cmpop_i32(&mut self, c: Condition) -> Result<(), CodegenError> {
        let loc_b = self.pop_value_released();
        self.emit_cmpop_i32_dynamic_b(c, loc_b)?;
        Ok(())
    }

    /// I64 comparison with `loc_b` from input.
    fn emit_cmpop_i64_dynamic_b(
        &mut self,
        c: Condition,
        loc_b: Location,
    ) -> Result<(), CodegenError> {
        // Using Red Zone here.
        let loc_a = self.pop_value_released();

        let ret = self
            .machine
            .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];
        match ret {
            Location::GPR(x) => {
                self.emit_relaxed_binop(Assembler::emit_cmp, Size::S64, loc_b, loc_a);
                self.assembler.emit_set(c, x);
                self.assembler
                    .emit_and(Size::S32, Location::Imm32(0xff), Location::GPR(x));
            }
            Location::Memory(_, _) => {
                let tmp = self.machine.acquire_temp_gpr().unwrap();
                self.emit_relaxed_binop(Assembler::emit_cmp, Size::S64, loc_b, loc_a);
                self.assembler.emit_set(c, tmp);
                self.assembler
                    .emit_and(Size::S32, Location::Imm32(0xff), Location::GPR(tmp));
                self.assembler.emit_mov(Size::S32, Location::GPR(tmp), ret);
                self.machine.release_temp_gpr(tmp);
            }
            _ => {
                return Err(CodegenError {
                    message: "emit_cmpop_i64_dynamic_b ret: unreachable code".to_string(),
                })
            }
        }
        self.value_stack.push(ret);
        Ok(())
    }

    /// I64 comparison with both operands popped from the virtual stack.
    fn emit_cmpop_i64(&mut self, c: Condition) -> Result<(), CodegenError> {
        let loc_b = self.pop_value_released();
        self.emit_cmpop_i64_dynamic_b(c, loc_b)?;
        Ok(())
    }

    /// I32 `lzcnt`/`tzcnt`/`popcnt` with operand popped from the virtual stack.
    fn emit_xcnt_i32(
        &mut self,
        f: fn(&mut Assembler, Size, Location, Location),
    ) -> Result<(), CodegenError> {
        let loc = self.pop_value_released();
        let ret = self
            .machine
            .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];

        match loc {
            Location::Imm32(_) => {
                let tmp = self.machine.acquire_temp_gpr().unwrap();
                self.assembler.emit_mov(Size::S32, loc, Location::GPR(tmp));
                if let Location::Memory(_, _) = ret {
                    let out_tmp = self.machine.acquire_temp_gpr().unwrap();
                    f(
                        &mut self.assembler,
                        Size::S32,
                        Location::GPR(tmp),
                        Location::GPR(out_tmp),
                    );
                    self.assembler
                        .emit_mov(Size::S32, Location::GPR(out_tmp), ret);
                    self.machine.release_temp_gpr(out_tmp);
                } else {
                    f(&mut self.assembler, Size::S32, Location::GPR(tmp), ret);
                }
                self.machine.release_temp_gpr(tmp);
            }
            Location::Memory(_, _) | Location::GPR(_) => {
                if let Location::Memory(_, _) = ret {
                    let out_tmp = self.machine.acquire_temp_gpr().unwrap();
                    f(&mut self.assembler, Size::S32, loc, Location::GPR(out_tmp));
                    self.assembler
                        .emit_mov(Size::S32, Location::GPR(out_tmp), ret);
                    self.machine.release_temp_gpr(out_tmp);
                } else {
                    f(&mut self.assembler, Size::S32, loc, ret);
                }
            }
            _ => {
                return Err(CodegenError {
                    message: "emit_xcnt_i32 loc: unreachable code".to_string(),
                })
            }
        }
        self.value_stack.push(ret);
        Ok(())
    }

    /// I64 `lzcnt`/`tzcnt`/`popcnt` with operand popped from the virtual stack.
    fn emit_xcnt_i64(
        &mut self,
        f: fn(&mut Assembler, Size, Location, Location),
    ) -> Result<(), CodegenError> {
        let loc = self.pop_value_released();
        let ret = self
            .machine
            .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];

        match loc {
            Location::Imm64(_) | Location::Imm32(_) => {
                let tmp = self.machine.acquire_temp_gpr().unwrap();
                self.assembler.emit_mov(Size::S64, loc, Location::GPR(tmp));
                if let Location::Memory(_, _) = ret {
                    let out_tmp = self.machine.acquire_temp_gpr().unwrap();
                    f(
                        &mut self.assembler,
                        Size::S64,
                        Location::GPR(tmp),
                        Location::GPR(out_tmp),
                    );
                    self.assembler
                        .emit_mov(Size::S64, Location::GPR(out_tmp), ret);
                    self.machine.release_temp_gpr(out_tmp);
                } else {
                    f(&mut self.assembler, Size::S64, Location::GPR(tmp), ret);
                }
                self.machine.release_temp_gpr(tmp);
            }
            Location::Memory(_, _) | Location::GPR(_) => {
                if let Location::Memory(_, _) = ret {
                    let out_tmp = self.machine.acquire_temp_gpr().unwrap();
                    f(&mut self.assembler, Size::S64, loc, Location::GPR(out_tmp));
                    self.assembler
                        .emit_mov(Size::S64, Location::GPR(out_tmp), ret);
                    self.machine.release_temp_gpr(out_tmp);
                } else {
                    f(&mut self.assembler, Size::S64, loc, ret);
                }
            }
            _ => {
                return Err(CodegenError {
                    message: "emit_xcnt_i64 loc: unreachable code".to_string(),
                })
            }
        }
        self.value_stack.push(ret);
        Ok(())
    }

    /// I32 shift with both operands popped from the virtual stack.
    fn emit_shift_i32(&mut self, f: fn(&mut Assembler, Size, Location, Location)) {
        let I2O1 { loc_a, loc_b, ret } = self.i2o1_prepare(WpType::I32);

        self.assembler
            .emit_mov(Size::S32, loc_b, Location::GPR(GPR::RCX));

        if loc_a != ret {
            self.emit_relaxed_binop(Assembler::emit_mov, Size::S32, loc_a, ret);
        }

        f(&mut self.assembler, Size::S32, Location::GPR(GPR::RCX), ret);
    }

    /// I64 shift with both operands popped from the virtual stack.
    fn emit_shift_i64(&mut self, f: fn(&mut Assembler, Size, Location, Location)) {
        let I2O1 { loc_a, loc_b, ret } = self.i2o1_prepare(WpType::I64);
        self.assembler
            .emit_mov(Size::S64, loc_b, Location::GPR(GPR::RCX));

        if loc_a != ret {
            self.emit_relaxed_binop(Assembler::emit_mov, Size::S64, loc_a, ret);
        }

        f(&mut self.assembler, Size::S64, Location::GPR(GPR::RCX), ret);
    }

    /// Floating point (AVX) binary operation with both operands popped from the virtual stack.
    fn emit_fp_binop_avx(
        &mut self,
        f: fn(&mut Assembler, XMM, XMMOrMemory, XMM),
    ) -> Result<(), CodegenError> {
        let I2O1 { loc_a, loc_b, ret } = self.i2o1_prepare(WpType::F64);

        self.emit_relaxed_avx(f, loc_a, loc_b, ret)?;
        Ok(())
    }

    /// Floating point (AVX) comparison with both operands popped from the virtual stack.
    fn emit_fp_cmpop_avx(
        &mut self,
        f: fn(&mut Assembler, XMM, XMMOrMemory, XMM),
    ) -> Result<(), CodegenError> {
        let I2O1 { loc_a, loc_b, ret } = self.i2o1_prepare(WpType::I32);

        self.emit_relaxed_avx(f, loc_a, loc_b, ret)?;

        // Workaround for behavior inconsistency among different backing implementations.
        // (all bits or only the least significant bit are set to one?)
        self.assembler.emit_and(Size::S32, Location::Imm32(1), ret);
        Ok(())
    }

    /// Floating point (AVX) unop with both operands popped from the virtual stack.
    fn emit_fp_unop_avx(
        &mut self,
        f: fn(&mut Assembler, XMM, XMMOrMemory, XMM),
    ) -> Result<(), CodegenError> {
        let loc = self.pop_value_released();
        let ret = self
            .machine
            .acquire_locations(&mut self.assembler, &[(WpType::F64)], false)[0];
        self.value_stack.push(ret);
        self.emit_relaxed_avx(f, loc, loc, ret)?;
        Ok(())
    }

    /// Emits a System V / Windows call sequence.
    ///
    /// This function will not use RAX before `cb` is called.
    ///
    /// The caller MUST NOT hold any temporary registers allocated by `acquire_temp_gpr` when calling
    /// this function.
    fn emit_call_native<I: Iterator<Item = Location>, F: FnOnce(&mut Self)>(
        &mut self,
        cb: F,
        params: I,
    ) -> Result<(), CodegenError> {
        let params: Vec<_> = params.collect();

        // Save used GPRs.
        let used_gprs = self.machine.get_used_gprs();
        for r in used_gprs.iter() {
            self.assembler.emit_push(Size::S64, Location::GPR(*r));
        }

        // Save used XMM registers.
        let used_xmms = self.machine.get_used_xmms();
        if used_xmms.len() > 0 {
            self.assembler.emit_sub(
                Size::S64,
                Location::Imm32((used_xmms.len() * 8) as u32),
                Location::GPR(GPR::RSP),
            );

            for (i, r) in used_xmms.iter().enumerate() {
                self.assembler.emit_mov(
                    Size::S64,
                    Location::XMM(*r),
                    Location::Memory(GPR::RSP, (i * 8) as i32),
                );
            }
        }
        let calling_convention = self.calling_convention;

        let stack_padding: usize = match calling_convention {
            CallingConvention::WindowsFastcall => 32,
            _ => 0,
        };

        let mut stack_offset: usize = 0;

        // Calculate stack offset.
        for (i, _param) in params.iter().enumerate() {
            if let Location::Memory(_, _) = Machine::get_param_location(1 + i, calling_convention) {
                stack_offset += 8;
            }
        }

        // Align stack to 16 bytes.
        if (self.machine.get_stack_offset()
            + used_gprs.len() * 8
            + used_xmms.len() * 8
            + stack_offset)
            % 16
            != 0
        {
            self.assembler
                .emit_sub(Size::S64, Location::Imm32(8), Location::GPR(GPR::RSP));
            stack_offset += 8;
        }

        let mut call_movs: Vec<(Location, GPR)> = vec![];
        // Prepare register & stack parameters.
        for (i, param) in params.iter().enumerate().rev() {
            let loc = Machine::get_param_location(1 + i, calling_convention);
            match loc {
                Location::GPR(x) => {
                    call_movs.push((*param, x));
                }
                Location::Memory(_, _) => {
                    match *param {
                        Location::GPR(_) => {}
                        Location::XMM(_) => {}
                        Location::Memory(reg, _) => {
                            if reg != GPR::RBP {
                                return Err(CodegenError {
                                    message: "emit_call_native loc param: unreachable code"
                                        .to_string(),
                                });
                            }
                            // TODO: Read value at this offset
                        }
                        _ => {}
                    }
                    match *param {
                        Location::Imm64(_) => {
                            // x86_64 does not support `mov imm64, mem`. We must first place the
                            // immdiate value into a register and then write the register to the
                            // memory. Now the problem is that there might not be any registers
                            // available to clobber. In order to make this work out we spill a
                            // register thus retaining both the original value of the
                            // register and producing the required data at the top of the stack.
                            //
                            // FIXME(#2723): figure out how to not require spilling a register
                            // here. It should definitely be possible to `pick_gpr`/`pick_temp_gpr`
                            // to grab an otherwise unused register and just clobber its value
                            // here.
                            self.assembler.emit_push(Size::S64, Location::GPR(GPR::R9));
                            self.assembler
                                .emit_mov(Size::S64, *param, Location::GPR(GPR::R9));
                            self.assembler.emit_xchg(
                                Size::S64,
                                Location::GPR(GPR::R9),
                                Location::Memory(GPR::RSP, 0),
                            );
                        }
                        Location::XMM(_) => {
                            // Dummy value slot to be filled with `mov`.
                            self.assembler.emit_push(Size::S64, Location::GPR(GPR::RAX));

                            // XMM registers can be directly stored to memory.
                            self.assembler.emit_mov(
                                Size::S64,
                                *param,
                                Location::Memory(GPR::RSP, 0),
                            );
                        }
                        _ => self.assembler.emit_push(Size::S64, *param),
                    }
                }
                _ => {
                    return Err(CodegenError {
                        message: "emit_call_native loc: unreachable code".to_string(),
                    })
                }
            }
        }

        // Sort register moves so that register are not overwritten before read.
        sort_call_movs(&mut call_movs);

        // Emit register moves.
        for (loc, gpr) in call_movs {
            if loc != Location::GPR(gpr) {
                self.assembler.emit_mov(Size::S64, loc, Location::GPR(gpr));
            }
        }

        // Put vmctx as the first parameter.
        self.assembler.emit_mov(
            Size::S64,
            Location::GPR(Machine::get_vmctx_reg()),
            Machine::get_param_location(0, calling_convention),
        ); // vmctx

        if stack_padding > 0 {
            self.assembler.emit_sub(
                Size::S64,
                Location::Imm32(stack_padding as u32),
                Location::GPR(GPR::RSP),
            );
        }

        cb(self);

        // Restore stack.
        if stack_offset + stack_padding > 0 {
            self.assembler.emit_add(
                Size::S64,
                Location::Imm32((stack_offset + stack_padding) as u32),
                Location::GPR(GPR::RSP),
            );
            if (stack_offset % 8) != 0 {
                return Err(CodegenError {
                    message: "emit_call_native: Bad restoring stack alignement".to_string(),
                });
            }
        }

        // Restore XMMs.
        if !used_xmms.is_empty() {
            for (i, r) in used_xmms.iter().enumerate() {
                self.assembler.emit_mov(
                    Size::S64,
                    Location::Memory(GPR::RSP, (i * 8) as i32),
                    Location::XMM(*r),
                );
            }
            self.assembler.emit_add(
                Size::S64,
                Location::Imm32((used_xmms.len() * 8) as u32),
                Location::GPR(GPR::RSP),
            );
        }

        // Restore GPRs.
        for r in used_gprs.iter().rev() {
            self.assembler.emit_pop(Size::S64, Location::GPR(*r));
        }

        Ok(())
    }

    /// Emits a System V call sequence, specialized for labels as the call target.
    fn _emit_call_native_label<I: Iterator<Item = Location>>(
        &mut self,
        label: DynamicLabel,
        params: I,
    ) -> Result<(), CodegenError> {
        self.emit_call_native(|this| this.assembler.emit_call_label(label), params)?;
        Ok(())
    }

    /// Emits a memory operation.
    fn emit_memory_op<F: FnOnce(&mut Self, GPR) -> Result<(), CodegenError>>(
        &mut self,
        addr: Location,
        memarg: &MemoryImmediate,
        check_alignment: bool,
        value_size: usize,
        cb: F,
    ) -> Result<(), CodegenError> {
        let need_check = true;
        let tmp_addr = self.machine.acquire_temp_gpr().unwrap();

        // Reusing `tmp_addr` for temporary indirection here, since it's not used before the last reference to `{base,bound}_loc`.
        let (base_loc, bound_loc) = if self.module.import_counts.memories != 0 {
            // Imported memories require one level of indirection.
            let offset = self
                .vmoffsets
                .vmctx_vmmemory_import_definition(MemoryIndex::new(0));
            self.emit_relaxed_binop(
                Assembler::emit_mov,
                Size::S64,
                Location::Memory(Machine::get_vmctx_reg(), offset as i32),
                Location::GPR(tmp_addr),
            );
            (Location::Memory(tmp_addr, 0), Location::Memory(tmp_addr, 8))
        } else {
            let offset = self
                .vmoffsets
                .vmctx_vmmemory_definition(LocalMemoryIndex::new(0));
            (
                Location::Memory(Machine::get_vmctx_reg(), offset as i32),
                Location::Memory(Machine::get_vmctx_reg(), (offset + 8) as i32),
            )
        };

        let tmp_base = self.machine.acquire_temp_gpr().unwrap();
        let tmp_bound = self.machine.acquire_temp_gpr().unwrap();

        // Load base into temporary register.
        self.assembler
            .emit_mov(Size::S64, base_loc, Location::GPR(tmp_base));

        // Load bound into temporary register, if needed.
        if need_check {
            self.assembler
                .emit_mov(Size::S64, bound_loc, Location::GPR(tmp_bound));

            // Wasm -> Effective.
            // Assuming we never underflow - should always be true on Linux/macOS and Windows >=8,
            // since the first page from 0x0 to 0x1000 is not accepted by mmap.

            // This `lea` calculates the upper bound allowed for the beginning of the word.
            // Since the upper bound of the memory is (exclusively) `tmp_bound + tmp_base`,
            // the maximum allowed beginning of word is (inclusively)
            // `tmp_bound + tmp_base - value_size`.
            self.assembler.emit_lea(
                Size::S64,
                Location::MemoryAddTriple(tmp_bound, tmp_base, -(value_size as i32)),
                Location::GPR(tmp_bound),
            );
        }

        // Load effective address.
        // `base_loc` and `bound_loc` becomes INVALID after this line, because `tmp_addr`
        // might be reused.
        self.assembler
            .emit_mov(Size::S32, addr, Location::GPR(tmp_addr));

        // Add offset to memory address.
        if memarg.offset != 0 {
            self.assembler.emit_add(
                Size::S32,
                Location::Imm32(memarg.offset),
                Location::GPR(tmp_addr),
            );

            // Trap if offset calculation overflowed.
            self.assembler
                .emit_jmp(Condition::Carry, self.special_labels.heap_access_oob);
        }

        // Wasm linear memory -> real memory
        self.assembler
            .emit_add(Size::S64, Location::GPR(tmp_base), Location::GPR(tmp_addr));

        if need_check {
            // Trap if the end address of the requested area is above that of the linear memory.
            self.assembler
                .emit_cmp(Size::S64, Location::GPR(tmp_bound), Location::GPR(tmp_addr));

            // `tmp_bound` is inclusive. So trap only if `tmp_addr > tmp_bound`.
            self.assembler
                .emit_jmp(Condition::Above, self.special_labels.heap_access_oob);
        }

        self.machine.release_temp_gpr(tmp_bound);
        self.machine.release_temp_gpr(tmp_base);

        let align = memarg.align;
        if check_alignment && align != 1 {
            let tmp_aligncheck = self.machine.acquire_temp_gpr().unwrap();
            self.assembler.emit_mov(
                Size::S32,
                Location::GPR(tmp_addr),
                Location::GPR(tmp_aligncheck),
            );
            self.assembler.emit_and(
                Size::S64,
                Location::Imm32((align - 1).into()),
                Location::GPR(tmp_aligncheck),
            );
            self.assembler
                .emit_jmp(Condition::NotEqual, self.special_labels.heap_access_oob);
            self.machine.release_temp_gpr(tmp_aligncheck);
        }

        cb(self, tmp_addr).unwrap();

        self.machine.release_temp_gpr(tmp_addr);
        Ok(())
    }

    /// Emits a memory operation.
    fn emit_compare_and_swap<F: FnOnce(&mut Self, GPR, GPR)>(
        &mut self,
        loc: Location,
        target: Location,
        ret: Location,
        memarg: &MemoryImmediate,
        value_size: usize,
        memory_sz: Size,
        stack_sz: Size,
        cb: F,
    ) -> Result<(), CodegenError> {
        if memory_sz > stack_sz {
            return Err(CodegenError {
                message: "emit_compare_and_swap: memory size > stack size".to_string(),
            });
        }

        let compare = self.machine.reserve_unused_temp_gpr(GPR::RAX);
        let value = if loc == Location::GPR(GPR::R14) {
            GPR::R13
        } else {
            GPR::R14
        };
        self.assembler.emit_push(Size::S64, Location::GPR(value));

        self.assembler.emit_mov(stack_sz, loc, Location::GPR(value));

        let retry = self.assembler.get_label();
        self.assembler.emit_label(retry);

        self.emit_memory_op(target, memarg, true, value_size, |this, addr| {
            // Memory moves with size < 32b do not zero upper bits.
            if memory_sz < Size::S32 {
                this.assembler
                    .emit_xor(Size::S32, Location::GPR(compare), Location::GPR(compare));
            }
            this.assembler
                .emit_mov(memory_sz, Location::Memory(addr, 0), Location::GPR(compare));
            this.assembler
                .emit_mov(stack_sz, Location::GPR(compare), ret);
            cb(this, compare, value);
            this.assembler.emit_lock_cmpxchg(
                memory_sz,
                Location::GPR(value),
                Location::Memory(addr, 0),
            );
            Ok(())
        })?;

        self.assembler.emit_jmp(Condition::NotEqual, retry);

        self.assembler.emit_pop(Size::S64, Location::GPR(value));
        self.machine.release_temp_gpr(compare);
        Ok(())
    }

    // Checks for underflow/overflow/nan.
    fn emit_f32_int_conv_check(
        &mut self,
        reg: XMM,
        lower_bound: f32,
        upper_bound: f32,
        underflow_label: <Assembler as Emitter>::Label,
        overflow_label: <Assembler as Emitter>::Label,
        nan_label: <Assembler as Emitter>::Label,
        succeed_label: <Assembler as Emitter>::Label,
    ) {
        let lower_bound = f32::to_bits(lower_bound);
        let upper_bound = f32::to_bits(upper_bound);

        let tmp = self.machine.acquire_temp_gpr().unwrap();
        let tmp_x = self.machine.acquire_temp_xmm().unwrap();

        // Underflow.
        self.assembler
            .emit_mov(Size::S32, Location::Imm32(lower_bound), Location::GPR(tmp));
        self.assembler
            .emit_mov(Size::S32, Location::GPR(tmp), Location::XMM(tmp_x));
        self.assembler
            .emit_vcmpless(reg, XMMOrMemory::XMM(tmp_x), tmp_x);
        self.assembler
            .emit_mov(Size::S32, Location::XMM(tmp_x), Location::GPR(tmp));
        self.assembler
            .emit_cmp(Size::S32, Location::Imm32(0), Location::GPR(tmp));
        self.assembler
            .emit_jmp(Condition::NotEqual, underflow_label);

        // Overflow.
        self.assembler
            .emit_mov(Size::S32, Location::Imm32(upper_bound), Location::GPR(tmp));
        self.assembler
            .emit_mov(Size::S32, Location::GPR(tmp), Location::XMM(tmp_x));
        self.assembler
            .emit_vcmpgess(reg, XMMOrMemory::XMM(tmp_x), tmp_x);
        self.assembler
            .emit_mov(Size::S32, Location::XMM(tmp_x), Location::GPR(tmp));
        self.assembler
            .emit_cmp(Size::S32, Location::Imm32(0), Location::GPR(tmp));
        self.assembler.emit_jmp(Condition::NotEqual, overflow_label);

        // NaN.
        self.assembler
            .emit_vcmpeqss(reg, XMMOrMemory::XMM(reg), tmp_x);
        self.assembler
            .emit_mov(Size::S32, Location::XMM(tmp_x), Location::GPR(tmp));
        self.assembler
            .emit_cmp(Size::S32, Location::Imm32(0), Location::GPR(tmp));
        self.assembler.emit_jmp(Condition::Equal, nan_label);

        self.assembler.emit_jmp(Condition::None, succeed_label);

        self.machine.release_temp_xmm(tmp_x);
        self.machine.release_temp_gpr(tmp);
    }

    // Checks for underflow/overflow/nan before IxxTrunc{U/S}F32.
    fn emit_f32_int_conv_check_trap(&mut self, reg: XMM, lower_bound: f32, upper_bound: f32) {
        let trap_overflow = self.special_labels.integer_overflow;
        let trap_badconv = self.special_labels.bad_conversion_to_integer;
        let end = self.assembler.get_label();

        self.emit_f32_int_conv_check(
            reg,
            lower_bound,
            upper_bound,
            trap_overflow,
            trap_overflow,
            trap_badconv,
            end,
        );

        self.assembler.emit_label(end);
    }

    fn emit_f32_int_conv_check_sat<
        F1: FnOnce(&mut Self),
        F2: FnOnce(&mut Self),
        F3: FnOnce(&mut Self),
        F4: FnOnce(&mut Self),
    >(
        &mut self,
        reg: XMM,
        lower_bound: f32,
        upper_bound: f32,
        underflow_cb: F1,
        overflow_cb: F2,
        nan_cb: Option<F3>,
        convert_cb: F4,
    ) {
        // As an optimization nan_cb is optional, and when set to None we turn
        // use 'underflow' as the 'nan' label. This is useful for callers who
        // set the return value to zero for both underflow and nan.

        let underflow = self.assembler.get_label();
        let overflow = self.assembler.get_label();
        let nan = if nan_cb.is_some() {
            self.assembler.get_label()
        } else {
            underflow
        };
        let convert = self.assembler.get_label();
        let end = self.assembler.get_label();

        self.emit_f32_int_conv_check(
            reg,
            lower_bound,
            upper_bound,
            underflow,
            overflow,
            nan,
            convert,
        );

        self.assembler.emit_label(underflow);
        underflow_cb(self);
        self.assembler.emit_jmp(Condition::None, end);

        self.assembler.emit_label(overflow);
        overflow_cb(self);
        self.assembler.emit_jmp(Condition::None, end);

        if let Some(cb) = nan_cb {
            self.assembler.emit_label(nan);
            cb(self);
            self.assembler.emit_jmp(Condition::None, end);
        }

        self.assembler.emit_label(convert);
        convert_cb(self);
        self.assembler.emit_label(end);
    }

    // Checks for underflow/overflow/nan.
    fn emit_f64_int_conv_check(
        &mut self,
        reg: XMM,
        lower_bound: f64,
        upper_bound: f64,
        underflow_label: <Assembler as Emitter>::Label,
        overflow_label: <Assembler as Emitter>::Label,
        nan_label: <Assembler as Emitter>::Label,
        succeed_label: <Assembler as Emitter>::Label,
    ) {
        let lower_bound = f64::to_bits(lower_bound);
        let upper_bound = f64::to_bits(upper_bound);

        let tmp = self.machine.acquire_temp_gpr().unwrap();
        let tmp_x = self.machine.acquire_temp_xmm().unwrap();

        // Underflow.
        self.assembler
            .emit_mov(Size::S64, Location::Imm64(lower_bound), Location::GPR(tmp));
        self.assembler
            .emit_mov(Size::S64, Location::GPR(tmp), Location::XMM(tmp_x));
        self.assembler
            .emit_vcmplesd(reg, XMMOrMemory::XMM(tmp_x), tmp_x);
        self.assembler
            .emit_mov(Size::S32, Location::XMM(tmp_x), Location::GPR(tmp));
        self.assembler
            .emit_cmp(Size::S32, Location::Imm32(0), Location::GPR(tmp));
        self.assembler
            .emit_jmp(Condition::NotEqual, underflow_label);

        // Overflow.
        self.assembler
            .emit_mov(Size::S64, Location::Imm64(upper_bound), Location::GPR(tmp));
        self.assembler
            .emit_mov(Size::S64, Location::GPR(tmp), Location::XMM(tmp_x));
        self.assembler
            .emit_vcmpgesd(reg, XMMOrMemory::XMM(tmp_x), tmp_x);
        self.assembler
            .emit_mov(Size::S32, Location::XMM(tmp_x), Location::GPR(tmp));
        self.assembler
            .emit_cmp(Size::S32, Location::Imm32(0), Location::GPR(tmp));
        self.assembler.emit_jmp(Condition::NotEqual, overflow_label);

        // NaN.
        self.assembler
            .emit_vcmpeqsd(reg, XMMOrMemory::XMM(reg), tmp_x);
        self.assembler
            .emit_mov(Size::S32, Location::XMM(tmp_x), Location::GPR(tmp));
        self.assembler
            .emit_cmp(Size::S32, Location::Imm32(0), Location::GPR(tmp));
        self.assembler.emit_jmp(Condition::Equal, nan_label);

        self.assembler.emit_jmp(Condition::None, succeed_label);

        self.machine.release_temp_xmm(tmp_x);
        self.machine.release_temp_gpr(tmp);
    }

    // Checks for underflow/overflow/nan before IxxTrunc{U/S}F64.
    fn emit_f64_int_conv_check_trap(&mut self, reg: XMM, lower_bound: f64, upper_bound: f64) {
        let trap_overflow = self.special_labels.integer_overflow;
        let trap_badconv = self.special_labels.bad_conversion_to_integer;
        let end = self.assembler.get_label();

        self.emit_f64_int_conv_check(
            reg,
            lower_bound,
            upper_bound,
            trap_overflow,
            trap_overflow,
            trap_badconv,
            end,
        );

        self.assembler.emit_label(end);
    }

    fn emit_f64_int_conv_check_sat<
        F1: FnOnce(&mut Self),
        F2: FnOnce(&mut Self),
        F3: FnOnce(&mut Self),
        F4: FnOnce(&mut Self),
    >(
        &mut self,
        reg: XMM,
        lower_bound: f64,
        upper_bound: f64,
        underflow_cb: F1,
        overflow_cb: F2,
        nan_cb: Option<F3>,
        convert_cb: F4,
    ) {
        // As an optimization nan_cb is optional, and when set to None we turn
        // use 'underflow' as the 'nan' label. This is useful for callers who
        // set the return value to zero for both underflow and nan.

        let underflow = self.assembler.get_label();
        let overflow = self.assembler.get_label();
        let nan = if nan_cb.is_some() {
            self.assembler.get_label()
        } else {
            underflow
        };
        let convert = self.assembler.get_label();
        let end = self.assembler.get_label();

        self.emit_f64_int_conv_check(
            reg,
            lower_bound,
            upper_bound,
            underflow,
            overflow,
            nan,
            convert,
        );

        self.assembler.emit_label(underflow);
        underflow_cb(self);
        self.assembler.emit_jmp(Condition::None, end);

        self.assembler.emit_label(overflow);
        overflow_cb(self);
        self.assembler.emit_jmp(Condition::None, end);

        if let Some(cb) = nan_cb {
            self.assembler.emit_label(nan);
            cb(self);
            self.assembler.emit_jmp(Condition::None, end);
        }

        self.assembler.emit_label(convert);
        convert_cb(self);
        self.assembler.emit_label(end);
    }

    fn emit_stack_check(&mut self, enter: bool, depth: usize) {
        if enter {
            // Here we must use value we do not yet know, so we write 0x7fff_ffff and patch it later.
            self.assembler.emit_sub(
                Size::S32,
                Location::Imm32(0x7fff_ffff),
                Location::Memory(
                    Machine::get_vmctx_reg(),
                    self.vmoffsets.vmctx_stack_limit_begin() as i32,
                ),
            );
            // TODO: make it cleaner, now we assume instruction with 32-bit immediate at the end.
            // Recheck offsets, if change above instruction to anything else.
            self.stack_check_offset = AssemblyOffset(self.assembler.offset().0 - 4);
            self.assembler
                .emit_jmp(Condition::Signed, self.special_labels.stack_overflow);
        } else {
            {
                // Patch earlier stack checker with now known max stack depth.
                assert!(self.stack_check_offset.0 > 0);
                let mut alter = self.assembler.alter();
                alter.goto(self.stack_check_offset);
                // TODO: check that the value before was 0x7fff_ffff
                alter.push_u32(depth as u32);
            }
            self.assembler.emit_add(
                Size::S32,
                Location::Imm32(depth as u32),
                Location::Memory(
                    Machine::get_vmctx_reg(),
                    self.vmoffsets.vmctx_stack_limit_begin() as i32,
                ),
            );
        }
    }

    fn emit_function_stack_check(&mut self, enter: bool) {
        // `local_types` include parameters as well.
        let depth = self.local_types.len()
            + self.max_stack_depth
            // we add 4 to ensure that deep recursion is prohibited even for local and argument free
            // functions, as they still use stack space for the saved frame base and return address,
            // along with spill area for callee-saved registers.
            + 4;
        self.emit_stack_check(enter, depth);
    }

    fn emit_head(&mut self) -> Result<(), CodegenError> {
        // TODO: Patchpoint is not emitted for now, and ARM trampoline is not prepended.

        // Normal x86 entry prologue.
        self.assembler.emit_push(Size::S64, Location::GPR(GPR::RBP));
        self.assembler
            .emit_mov(Size::S64, Location::GPR(GPR::RSP), Location::GPR(GPR::RBP));
        // Initialize locals.
        self.locals = self.machine.init_locals(
            &mut self.assembler,
            self.local_types.len(),
            self.signature.params().len(),
            self.calling_convention,
        );

        self.emit_function_stack_check(true);

        self.assembler
            .emit_sub(Size::S64, Location::Imm32(32), Location::GPR(GPR::RSP)); // simulate "red zone" if not supported by the platform

        self.control_stack.push(ControlFrame {
            label: self.assembler.get_label(),
            loop_like: false,
            if_else: IfElseState::None,
            returns: self
                .signature
                .results()
                .iter()
                .map(|&x| type_to_wp_type(x))
                .collect(),
            value_stack_depth: 0,
            fp_stack_depth: 0,
        });

        Ok(())
    }

    /// Pushes the instruction to the address map, calculating the offset from a
    /// provided beginning address.
    fn mark_instruction_address_end(&mut self, begin: usize) {
        self.instructions_address_map.push(InstructionAddressMap {
            srcloc: SourceLoc::new(self.src_loc),
            code_offset: begin,
            code_len: self.assembler.get_offset().0 - begin,
        });
    }

    pub(crate) fn new(
        module: &'a ModuleInfo,
        module_translation_state: &'a ModuleTranslationState,
        config: &'a Singlepass,
        vmoffsets: &'a VMOffsets,
        _table_styles: &'a PrimaryMap<TableIndex, TableStyle>,
        local_func_index: LocalFunctionIndex,
        local_types_excluding_arguments: &[WpType],
        calling_convention: CallingConvention,
    ) -> Result<FuncGen<'a>, CodegenError> {
        let func_index = module.func_index(local_func_index);
        let sig_index = module.functions[func_index];
        let signature = module.signatures[sig_index].clone();

        let mut local_types: Vec<_> = signature
            .params()
            .iter()
            .map(|&x| type_to_wp_type(x))
            .collect();
        local_types.extend_from_slice(&local_types_excluding_arguments);

        let mut assembler = Assembler::new(0);
        let special_labels = SpecialLabelSet {
            integer_division_by_zero: assembler.get_label(),
            integer_overflow: assembler.get_label(),
            bad_conversion_to_integer: assembler.get_label(),
            heap_access_oob: assembler.get_label(),
            table_access_oob: assembler.get_label(),
            indirect_call_null: assembler.get_label(),
            bad_signature: assembler.get_label(),
            gas_limit_exceeded: assembler.get_label(),
            stack_overflow: assembler.get_label(),
        };

        let mut fg = FuncGen {
            module,
            module_translation_state,
            config,
            vmoffsets,
            // table_styles,
            signature,
            assembler,
            locals: vec![], // initialization deferred to emit_head
            local_types,
            value_stack: vec![],
            max_stack_depth: 0,
            stack_check_offset: AssemblyOffset(0),
            fp_stack: vec![],
            control_stack: vec![],
            machine: Machine::new(),
            unreachable_depth: 0,
            relocations: vec![],
            special_labels,
            src_loc: 0,
            instructions_address_map: vec![],
            calling_convention,
        };
        fg.emit_head()?;
        Ok(fg)
    }

    pub(crate) fn has_control_frames(&self) -> bool {
        !self.control_stack.is_empty()
    }

    pub(crate) fn feed_operator(&mut self, op: Operator) -> Result<(), CodegenError> {
        assert!(self.fp_stack.len() <= self.value_stack.len());

        let was_unreachable;

        if self.unreachable_depth > 0 {
            was_unreachable = true;

            match op {
                Operator::Block { .. } | Operator::Loop { .. } | Operator::If { .. } => {
                    self.unreachable_depth += 1;
                }
                Operator::End => {
                    self.unreachable_depth -= 1;
                }
                Operator::Else => {
                    // We are in a reachable true branch
                    if self.unreachable_depth == 1 {
                        if let Some(IfElseState::If(_)) =
                            self.control_stack.last().map(|x| x.if_else)
                        {
                            self.unreachable_depth -= 1;
                        }
                    }
                }
                _ => {}
            }
            if self.unreachable_depth > 0 {
                return Ok(());
            }
        } else {
            was_unreachable = false;
        }

        match op {
            Operator::GlobalGet { global_index } => {
                let global_index = GlobalIndex::from_u32(global_index);

                let ty = type_to_wp_type(self.module.globals[global_index].ty);
                if ty.is_float() {
                    self.fp_stack.push(FloatValue::new(self.value_stack.len()));
                }
                let loc = self
                    .machine
                    .acquire_locations(&mut self.assembler, &[(ty)], false)[0];
                self.value_stack.push(loc);

                let tmp = self.machine.acquire_temp_gpr().unwrap();

                let src = if let Some(local_global_index) =
                    self.module.local_global_index(global_index)
                {
                    let offset = self.vmoffsets.vmctx_vmglobal_definition(local_global_index);
                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S64,
                        Location::Memory(Machine::get_vmctx_reg(), offset as i32),
                        Location::GPR(tmp),
                    );
                    Location::Memory(tmp, 0)
                } else {
                    // Imported globals require one level of indirection.
                    let offset = self
                        .vmoffsets
                        .vmctx_vmglobal_import_definition(global_index);
                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S64,
                        Location::Memory(Machine::get_vmctx_reg(), offset as i32),
                        Location::GPR(tmp),
                    );
                    Location::Memory(tmp, 0)
                };

                self.emit_relaxed_binop(Assembler::emit_mov, Size::S64, src, loc);

                self.machine.release_temp_gpr(tmp);
            }
            Operator::GlobalSet { global_index } => {
                let global_index = GlobalIndex::from_u32(global_index);
                let tmp = self.machine.acquire_temp_gpr().unwrap();
                let dst = if let Some(local_global_index) =
                    self.module.local_global_index(global_index)
                {
                    let offset = self.vmoffsets.vmctx_vmglobal_definition(local_global_index);
                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S64,
                        Location::Memory(Machine::get_vmctx_reg(), offset as i32),
                        Location::GPR(tmp),
                    );
                    Location::Memory(tmp, 0)
                } else {
                    // Imported globals require one level of indirection.
                    let offset = self
                        .vmoffsets
                        .vmctx_vmglobal_import_definition(global_index);
                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S64,
                        Location::Memory(Machine::get_vmctx_reg(), offset as i32),
                        Location::GPR(tmp),
                    );
                    Location::Memory(tmp, 0)
                };
                let ty = type_to_wp_type(self.module.globals[global_index].ty);
                let loc = self.pop_value_released();
                if ty.is_float() {
                    let fp = self.fp_stack.pop1()?;
                    if self.assembler.arch_supports_canonicalize_nan()
                        && self.config.enable_nan_canonicalization
                        && fp.canonicalization.is_some()
                    {
                        self.canonicalize_nan(
                            match ty {
                                WpType::F32 => Size::S32,
                                WpType::F64 => Size::S64,
                                _ => unreachable!(),
                            },
                            loc,
                            dst,
                        );
                    } else {
                        self.emit_relaxed_binop(Assembler::emit_mov, Size::S64, loc, dst);
                    }
                } else {
                    self.emit_relaxed_binop(Assembler::emit_mov, Size::S64, loc, dst);
                }
                self.machine.release_temp_gpr(tmp);
            }
            Operator::LocalGet { local_index } => {
                let local_index = local_index as usize;
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.emit_relaxed_binop(
                    Assembler::emit_mov,
                    Size::S64,
                    self.locals[local_index],
                    ret,
                );
                self.value_stack.push(ret);
                if self.local_types[local_index].is_float() {
                    self.fp_stack
                        .push(FloatValue::new(self.value_stack.len() - 1));
                }
            }
            Operator::LocalSet { local_index } => {
                let local_index = local_index as usize;
                let loc = self.pop_value_released();

                if self.local_types[local_index].is_float() {
                    let fp = self.fp_stack.pop1()?;
                    if self.assembler.arch_supports_canonicalize_nan()
                        && self.config.enable_nan_canonicalization
                        && fp.canonicalization.is_some()
                    {
                        self.canonicalize_nan(
                            match self.local_types[local_index] {
                                WpType::F32 => Size::S32,
                                WpType::F64 => Size::S64,
                                _ => unreachable!(),
                            },
                            loc,
                            self.locals[local_index],
                        );
                    } else {
                        self.emit_relaxed_binop(
                            Assembler::emit_mov,
                            Size::S64,
                            loc,
                            self.locals[local_index],
                        );
                    }
                } else {
                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S64,
                        loc,
                        self.locals[local_index],
                    );
                }
            }
            Operator::LocalTee { local_index } => {
                let local_index = local_index as usize;
                let loc = *self.value_stack.last().unwrap();

                if self.local_types[local_index].is_float() {
                    let fp = self.fp_stack.peek1()?;
                    if self.assembler.arch_supports_canonicalize_nan()
                        && self.config.enable_nan_canonicalization
                        && fp.canonicalization.is_some()
                    {
                        self.canonicalize_nan(
                            match self.local_types[local_index] {
                                WpType::F32 => Size::S32,
                                WpType::F64 => Size::S64,
                                _ => unreachable!(),
                            },
                            loc,
                            self.locals[local_index],
                        );
                    } else {
                        self.emit_relaxed_binop(
                            Assembler::emit_mov,
                            Size::S64,
                            loc,
                            self.locals[local_index],
                        );
                    }
                } else {
                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S64,
                        loc,
                        self.locals[local_index],
                    );
                }
            }
            Operator::I32Const { value } => {
                self.value_stack.push(Location::Imm32(value as u32));
            }
            Operator::I32Add => self.emit_binop_i32(Assembler::emit_add),
            Operator::I32Sub => self.emit_binop_i32(Assembler::emit_sub),
            Operator::I32Mul => self.emit_binop_i32(Assembler::emit_imul),
            Operator::I32DivU => {
                // We assume that RAX and RDX are temporary registers here.
                let I2O1 { loc_a, loc_b, ret } = self.i2o1_prepare(WpType::I32);
                self.assembler
                    .emit_mov(Size::S32, loc_a, Location::GPR(GPR::RAX));
                self.assembler.emit_xor(
                    Size::S32,
                    Location::GPR(GPR::RDX),
                    Location::GPR(GPR::RDX),
                );
                self.emit_relaxed_xdiv(false, Size::S32, loc_b);
                self.assembler
                    .emit_mov(Size::S32, Location::GPR(GPR::RAX), ret);
            }
            Operator::I32DivS => {
                // We assume that RAX and RDX are temporary registers here.
                let I2O1 { loc_a, loc_b, ret } = self.i2o1_prepare(WpType::I32);
                self.assembler
                    .emit_mov(Size::S32, loc_a, Location::GPR(GPR::RAX));
                self.assembler.emit_cdq();
                self.emit_relaxed_xdiv(true, Size::S32, loc_b);
                self.assembler
                    .emit_mov(Size::S32, Location::GPR(GPR::RAX), ret);
            }
            Operator::I32RemU => {
                // We assume that RAX and RDX are temporary registers here.
                let I2O1 { loc_a, loc_b, ret } = self.i2o1_prepare(WpType::I32);
                self.assembler
                    .emit_mov(Size::S32, loc_a, Location::GPR(GPR::RAX));
                self.assembler.emit_xor(
                    Size::S32,
                    Location::GPR(GPR::RDX),
                    Location::GPR(GPR::RDX),
                );
                self.emit_relaxed_xdiv(false, Size::S32, loc_b);
                self.assembler
                    .emit_mov(Size::S32, Location::GPR(GPR::RDX), ret);
            }
            Operator::I32RemS => {
                // We assume that RAX and RDX are temporary registers here.
                let I2O1 { loc_a, loc_b, ret } = self.i2o1_prepare(WpType::I32);

                let normal_path = self.assembler.get_label();
                let end = self.assembler.get_label();

                self.emit_relaxed_binop(
                    Assembler::emit_cmp,
                    Size::S32,
                    Location::Imm32(0x80000000),
                    loc_a,
                );
                self.assembler.emit_jmp(Condition::NotEqual, normal_path);
                self.emit_relaxed_binop(
                    Assembler::emit_cmp,
                    Size::S32,
                    Location::Imm32(0xffffffff),
                    loc_b,
                );
                self.assembler.emit_jmp(Condition::NotEqual, normal_path);
                self.assembler.emit_mov(Size::S32, Location::Imm32(0), ret);
                self.assembler.emit_jmp(Condition::None, end);

                self.assembler.emit_label(normal_path);
                self.assembler
                    .emit_mov(Size::S32, loc_a, Location::GPR(GPR::RAX));
                self.assembler.emit_cdq();
                self.emit_relaxed_xdiv(true, Size::S32, loc_b);
                self.assembler
                    .emit_mov(Size::S32, Location::GPR(GPR::RDX), ret);

                self.assembler.emit_label(end);
            }
            Operator::I32And => self.emit_binop_i32(Assembler::emit_and),
            Operator::I32Or => self.emit_binop_i32(Assembler::emit_or),
            Operator::I32Xor => self.emit_binop_i32(Assembler::emit_xor),
            Operator::I32Eq => self.emit_cmpop_i32(Condition::Equal)?,
            Operator::I32Ne => self.emit_cmpop_i32(Condition::NotEqual)?,
            Operator::I32Eqz => {
                self.emit_cmpop_i32_dynamic_b(Condition::Equal, Location::Imm32(0))?
            }
            Operator::I32Clz => {
                let loc = self.pop_value_released();
                let src = match loc {
                    Location::Imm32(_) | Location::Memory(_, _) => {
                        let tmp = self.machine.acquire_temp_gpr().unwrap();
                        self.assembler.emit_mov(Size::S32, loc, Location::GPR(tmp));
                        tmp
                    }
                    Location::GPR(reg) => reg,
                    _ => {
                        return Err(CodegenError {
                            message: "I32Clz src: unreachable code".to_string(),
                        })
                    }
                };

                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];
                self.value_stack.push(ret);

                let dst = match ret {
                    Location::Memory(_, _) => self.machine.acquire_temp_gpr().unwrap(),
                    Location::GPR(reg) => reg,
                    _ => {
                        return Err(CodegenError {
                            message: "I32Clz dst: unreachable code".to_string(),
                        })
                    }
                };

                if self.assembler.arch_has_xzcnt() {
                    self.assembler.arch_emit_lzcnt(
                        Size::S32,
                        Location::GPR(src),
                        Location::GPR(dst),
                    );
                } else {
                    let zero_path = self.assembler.get_label();
                    let end = self.assembler.get_label();

                    self.assembler.emit_test_gpr_64(src);
                    self.assembler.emit_jmp(Condition::Equal, zero_path);
                    self.assembler
                        .emit_bsr(Size::S32, Location::GPR(src), Location::GPR(dst));
                    self.assembler
                        .emit_xor(Size::S32, Location::Imm32(31), Location::GPR(dst));
                    self.assembler.emit_jmp(Condition::None, end);
                    self.assembler.emit_label(zero_path);
                    self.assembler
                        .emit_mov(Size::S32, Location::Imm32(32), Location::GPR(dst));
                    self.assembler.emit_label(end);
                }

                match loc {
                    Location::Imm32(_) | Location::Memory(_, _) => {
                        self.machine.release_temp_gpr(src);
                    }
                    _ => {}
                };
                if let Location::Memory(_, _) = ret {
                    self.assembler.emit_mov(Size::S32, Location::GPR(dst), ret);
                    self.machine.release_temp_gpr(dst);
                };
            }
            Operator::I32Ctz => {
                let loc = self.pop_value_released();
                let src = match loc {
                    Location::Imm32(_) | Location::Memory(_, _) => {
                        let tmp = self.machine.acquire_temp_gpr().unwrap();
                        self.assembler.emit_mov(Size::S32, loc, Location::GPR(tmp));
                        tmp
                    }
                    Location::GPR(reg) => reg,
                    _ => {
                        return Err(CodegenError {
                            message: "I32Ctz src: unreachable code".to_string(),
                        })
                    }
                };

                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];
                self.value_stack.push(ret);

                let dst = match ret {
                    Location::Memory(_, _) => self.machine.acquire_temp_gpr().unwrap(),
                    Location::GPR(reg) => reg,
                    _ => {
                        return Err(CodegenError {
                            message: "I32Ctz dst: unreachable code".to_string(),
                        })
                    }
                };

                if self.assembler.arch_has_xzcnt() {
                    self.assembler.arch_emit_tzcnt(
                        Size::S32,
                        Location::GPR(src),
                        Location::GPR(dst),
                    );
                } else {
                    let zero_path = self.assembler.get_label();
                    let end = self.assembler.get_label();

                    self.assembler.emit_test_gpr_64(src);
                    self.assembler.emit_jmp(Condition::Equal, zero_path);
                    self.assembler
                        .emit_bsf(Size::S32, Location::GPR(src), Location::GPR(dst));
                    self.assembler.emit_jmp(Condition::None, end);
                    self.assembler.emit_label(zero_path);
                    self.assembler
                        .emit_mov(Size::S32, Location::Imm32(32), Location::GPR(dst));
                    self.assembler.emit_label(end);
                }

                match loc {
                    Location::Imm32(_) | Location::Memory(_, _) => {
                        self.machine.release_temp_gpr(src);
                    }
                    _ => {}
                };
                if let Location::Memory(_, _) = ret {
                    self.assembler.emit_mov(Size::S32, Location::GPR(dst), ret);
                    self.machine.release_temp_gpr(dst);
                };
            }
            Operator::I32Popcnt => self.emit_xcnt_i32(Assembler::emit_popcnt)?,
            Operator::I32Shl => self.emit_shift_i32(Assembler::emit_shl),
            Operator::I32ShrU => self.emit_shift_i32(Assembler::emit_shr),
            Operator::I32ShrS => self.emit_shift_i32(Assembler::emit_sar),
            Operator::I32Rotl => self.emit_shift_i32(Assembler::emit_rol),
            Operator::I32Rotr => self.emit_shift_i32(Assembler::emit_ror),
            Operator::I32LtU => self.emit_cmpop_i32(Condition::Below)?,
            Operator::I32LeU => self.emit_cmpop_i32(Condition::BelowEqual)?,
            Operator::I32GtU => self.emit_cmpop_i32(Condition::Above)?,
            Operator::I32GeU => self.emit_cmpop_i32(Condition::AboveEqual)?,
            Operator::I32LtS => {
                self.emit_cmpop_i32(Condition::Less)?;
            }
            Operator::I32LeS => self.emit_cmpop_i32(Condition::LessEqual)?,
            Operator::I32GtS => self.emit_cmpop_i32(Condition::Greater)?,
            Operator::I32GeS => self.emit_cmpop_i32(Condition::GreaterEqual)?,
            Operator::I64Const { value } => {
                let value = value as u64;
                self.value_stack.push(Location::Imm64(value));
            }
            Operator::I64Add => self.emit_binop_i64(Assembler::emit_add),
            Operator::I64Sub => self.emit_binop_i64(Assembler::emit_sub),
            Operator::I64Mul => self.emit_binop_i64(Assembler::emit_imul),
            Operator::I64DivU => {
                // We assume that RAX and RDX are temporary registers here.
                let I2O1 { loc_a, loc_b, ret } = self.i2o1_prepare(WpType::I64);
                self.assembler
                    .emit_mov(Size::S64, loc_a, Location::GPR(GPR::RAX));
                self.assembler.emit_xor(
                    Size::S64,
                    Location::GPR(GPR::RDX),
                    Location::GPR(GPR::RDX),
                );
                self.emit_relaxed_xdiv(false, Size::S64, loc_b);
                self.assembler
                    .emit_mov(Size::S64, Location::GPR(GPR::RAX), ret);
            }
            Operator::I64DivS => {
                // We assume that RAX and RDX are temporary registers here.
                let I2O1 { loc_a, loc_b, ret } = self.i2o1_prepare(WpType::I64);
                self.assembler
                    .emit_mov(Size::S64, loc_a, Location::GPR(GPR::RAX));
                self.assembler.emit_cqo();
                self.emit_relaxed_xdiv(true, Size::S64, loc_b);
                self.assembler
                    .emit_mov(Size::S64, Location::GPR(GPR::RAX), ret);
            }
            Operator::I64RemU => {
                // We assume that RAX and RDX are temporary registers here.
                let I2O1 { loc_a, loc_b, ret } = self.i2o1_prepare(WpType::I64);
                self.assembler
                    .emit_mov(Size::S64, loc_a, Location::GPR(GPR::RAX));
                self.assembler.emit_xor(
                    Size::S64,
                    Location::GPR(GPR::RDX),
                    Location::GPR(GPR::RDX),
                );
                self.emit_relaxed_xdiv(false, Size::S64, loc_b);
                self.assembler
                    .emit_mov(Size::S64, Location::GPR(GPR::RDX), ret);
            }
            Operator::I64RemS => {
                // We assume that RAX and RDX are temporary registers here.
                let I2O1 { loc_a, loc_b, ret } = self.i2o1_prepare(WpType::I64);

                let normal_path = self.assembler.get_label();
                let end = self.assembler.get_label();

                self.emit_relaxed_binop(
                    Assembler::emit_cmp,
                    Size::S64,
                    Location::Imm64(0x8000000000000000u64),
                    loc_a,
                );
                self.assembler.emit_jmp(Condition::NotEqual, normal_path);
                self.emit_relaxed_binop(
                    Assembler::emit_cmp,
                    Size::S64,
                    Location::Imm64(0xffffffffffffffffu64),
                    loc_b,
                );
                self.assembler.emit_jmp(Condition::NotEqual, normal_path);
                self.emit_relaxed_binop(Assembler::emit_mov, Size::S64, Location::Imm64(0), ret);
                self.assembler.emit_jmp(Condition::None, end);

                self.assembler.emit_label(normal_path);

                self.assembler
                    .emit_mov(Size::S64, loc_a, Location::GPR(GPR::RAX));
                self.assembler.emit_cqo();
                self.emit_relaxed_xdiv(true, Size::S64, loc_b);
                self.assembler
                    .emit_mov(Size::S64, Location::GPR(GPR::RDX), ret);
                self.assembler.emit_label(end);
            }
            Operator::I64And => self.emit_binop_i64(Assembler::emit_and),
            Operator::I64Or => self.emit_binop_i64(Assembler::emit_or),
            Operator::I64Xor => self.emit_binop_i64(Assembler::emit_xor),
            Operator::I64Eq => self.emit_cmpop_i64(Condition::Equal)?,
            Operator::I64Ne => self.emit_cmpop_i64(Condition::NotEqual)?,
            Operator::I64Eqz => {
                self.emit_cmpop_i64_dynamic_b(Condition::Equal, Location::Imm64(0))?
            }
            Operator::I64Clz => {
                let loc = self.pop_value_released();
                let src = match loc {
                    Location::Imm64(_) | Location::Imm32(_) | Location::Memory(_, _) => {
                        let tmp = self.machine.acquire_temp_gpr().unwrap();
                        self.assembler.emit_mov(Size::S64, loc, Location::GPR(tmp));
                        tmp
                    }
                    Location::GPR(reg) => reg,
                    _ => {
                        return Err(CodegenError {
                            message: "I64Clz src: unreachable code".to_string(),
                        })
                    }
                };

                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);

                let dst = match ret {
                    Location::Memory(_, _) => self.machine.acquire_temp_gpr().unwrap(),
                    Location::GPR(reg) => reg,
                    _ => {
                        return Err(CodegenError {
                            message: "I64Clz dst: unreachable code".to_string(),
                        })
                    }
                };

                if self.assembler.arch_has_xzcnt() {
                    self.assembler.arch_emit_lzcnt(
                        Size::S64,
                        Location::GPR(src),
                        Location::GPR(dst),
                    );
                } else {
                    let zero_path = self.assembler.get_label();
                    let end = self.assembler.get_label();

                    self.assembler.emit_test_gpr_64(src);
                    self.assembler.emit_jmp(Condition::Equal, zero_path);
                    self.assembler
                        .emit_bsr(Size::S64, Location::GPR(src), Location::GPR(dst));
                    self.assembler
                        .emit_xor(Size::S64, Location::Imm32(63), Location::GPR(dst));
                    self.assembler.emit_jmp(Condition::None, end);
                    self.assembler.emit_label(zero_path);
                    self.assembler
                        .emit_mov(Size::S64, Location::Imm32(64), Location::GPR(dst));
                    self.assembler.emit_label(end);
                }

                match loc {
                    Location::Imm64(_) | Location::Imm32(_) | Location::Memory(_, _) => {
                        self.machine.release_temp_gpr(src);
                    }
                    _ => {}
                };
                if let Location::Memory(_, _) = ret {
                    self.assembler.emit_mov(Size::S64, Location::GPR(dst), ret);
                    self.machine.release_temp_gpr(dst);
                };
            }
            Operator::I64Ctz => {
                let loc = self.pop_value_released();
                let src = match loc {
                    Location::Imm64(_) | Location::Imm32(_) | Location::Memory(_, _) => {
                        let tmp = self.machine.acquire_temp_gpr().unwrap();
                        self.assembler.emit_mov(Size::S64, loc, Location::GPR(tmp));
                        tmp
                    }
                    Location::GPR(reg) => reg,
                    _ => {
                        return Err(CodegenError {
                            message: "I64Ctz src: unreachable code".to_string(),
                        })
                    }
                };

                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);

                let dst = match ret {
                    Location::Memory(_, _) => self.machine.acquire_temp_gpr().unwrap(),
                    Location::GPR(reg) => reg,
                    _ => {
                        return Err(CodegenError {
                            message: "I64Ctz dst: unreachable code".to_string(),
                        })
                    }
                };

                if self.assembler.arch_has_xzcnt() {
                    self.assembler.arch_emit_tzcnt(
                        Size::S64,
                        Location::GPR(src),
                        Location::GPR(dst),
                    );
                } else {
                    let zero_path = self.assembler.get_label();
                    let end = self.assembler.get_label();

                    self.assembler.emit_test_gpr_64(src);
                    self.assembler.emit_jmp(Condition::Equal, zero_path);
                    self.assembler
                        .emit_bsf(Size::S64, Location::GPR(src), Location::GPR(dst));
                    self.assembler.emit_jmp(Condition::None, end);
                    self.assembler.emit_label(zero_path);
                    self.assembler
                        .emit_mov(Size::S64, Location::Imm32(64), Location::GPR(dst));
                    self.assembler.emit_label(end);
                }

                match loc {
                    Location::Imm64(_) | Location::Imm32(_) | Location::Memory(_, _) => {
                        self.machine.release_temp_gpr(src);
                    }
                    _ => {}
                };
                if let Location::Memory(_, _) = ret {
                    self.assembler.emit_mov(Size::S64, Location::GPR(dst), ret);
                    self.machine.release_temp_gpr(dst);
                };
            }
            Operator::I64Popcnt => self.emit_xcnt_i64(Assembler::emit_popcnt)?,
            Operator::I64Shl => self.emit_shift_i64(Assembler::emit_shl),
            Operator::I64ShrU => self.emit_shift_i64(Assembler::emit_shr),
            Operator::I64ShrS => self.emit_shift_i64(Assembler::emit_sar),
            Operator::I64Rotl => self.emit_shift_i64(Assembler::emit_rol),
            Operator::I64Rotr => self.emit_shift_i64(Assembler::emit_ror),
            Operator::I64LtU => self.emit_cmpop_i64(Condition::Below)?,
            Operator::I64LeU => self.emit_cmpop_i64(Condition::BelowEqual)?,
            Operator::I64GtU => self.emit_cmpop_i64(Condition::Above)?,
            Operator::I64GeU => self.emit_cmpop_i64(Condition::AboveEqual)?,
            Operator::I64LtS => {
                self.emit_cmpop_i64(Condition::Less)?;
            }
            Operator::I64LeS => self.emit_cmpop_i64(Condition::LessEqual)?,
            Operator::I64GtS => self.emit_cmpop_i64(Condition::Greater)?,
            Operator::I64GeS => self.emit_cmpop_i64(Condition::GreaterEqual)?,
            Operator::I64ExtendI32U => {
                let loc = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);
                self.emit_relaxed_binop(Assembler::emit_mov, Size::S32, loc, ret);

                // A 32-bit memory write does not automatically clear the upper 32 bits of a 64-bit word.
                // So, we need to explicitly write zero to the upper half here.
                if let Location::Memory(base, off) = ret {
                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S32,
                        Location::Imm32(0),
                        Location::Memory(base, off + 4),
                    );
                }
            }
            Operator::I64ExtendI32S => {
                let loc = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);
                self.emit_relaxed_zx_sx(Assembler::emit_movsx, Size::S32, loc, Size::S64, ret)?;
            }
            Operator::I32Extend8S => {
                let loc = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];
                self.value_stack.push(ret);

                self.emit_relaxed_zx_sx(Assembler::emit_movsx, Size::S8, loc, Size::S32, ret)?;
            }
            Operator::I32Extend16S => {
                let loc = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];
                self.value_stack.push(ret);

                self.emit_relaxed_zx_sx(Assembler::emit_movsx, Size::S16, loc, Size::S32, ret)?;
            }
            Operator::I64Extend8S => {
                let loc = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);

                self.emit_relaxed_zx_sx(Assembler::emit_movsx, Size::S8, loc, Size::S64, ret)?;
            }
            Operator::I64Extend16S => {
                let loc = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);

                self.emit_relaxed_zx_sx(Assembler::emit_movsx, Size::S16, loc, Size::S64, ret)?;
            }
            Operator::I64Extend32S => {
                let loc = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);

                self.emit_relaxed_zx_sx(Assembler::emit_movsx, Size::S32, loc, Size::S64, ret)?;
            }
            Operator::I32WrapI64 => {
                let loc = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];
                self.value_stack.push(ret);
                self.emit_relaxed_binop(Assembler::emit_mov, Size::S32, loc, ret);
            }

            Operator::F32Const { value } => {
                self.value_stack.push(Location::Imm32(value.bits()));
                self.fp_stack
                    .push(FloatValue::new(self.value_stack.len() - 1));
            }
            Operator::F32Add => {
                self.fp_stack.pop2()?;
                self.fp_stack
                    .push(FloatValue::cncl_f32(self.value_stack.len() - 2));
                self.emit_fp_binop_avx(Assembler::emit_vaddss)?;
            }
            Operator::F32Sub => {
                self.fp_stack.pop2()?;
                self.fp_stack
                    .push(FloatValue::cncl_f32(self.value_stack.len() - 2));
                self.emit_fp_binop_avx(Assembler::emit_vsubss)?
            }
            Operator::F32Mul => {
                self.fp_stack.pop2()?;
                self.fp_stack
                    .push(FloatValue::cncl_f32(self.value_stack.len() - 2));
                self.emit_fp_binop_avx(Assembler::emit_vmulss)?
            }
            Operator::F32Div => {
                self.fp_stack.pop2()?;
                self.fp_stack
                    .push(FloatValue::cncl_f32(self.value_stack.len() - 2));
                self.emit_fp_binop_avx(Assembler::emit_vdivss)?
            }
            Operator::F32Max => {
                self.fp_stack.pop2()?;
                self.fp_stack
                    .push(FloatValue::new(self.value_stack.len() - 2));
                if !self.assembler.arch_supports_canonicalize_nan() {
                    self.emit_fp_binop_avx(Assembler::emit_vmaxss)?;
                } else {
                    let I2O1 { loc_a, loc_b, ret } = self.i2o1_prepare(WpType::F64);

                    let tmp1 = self.machine.acquire_temp_xmm().unwrap();
                    let tmp2 = self.machine.acquire_temp_xmm().unwrap();
                    let tmpg1 = self.machine.acquire_temp_gpr().unwrap();
                    let tmpg2 = self.machine.acquire_temp_gpr().unwrap();

                    let src1 = match loc_a {
                        Location::XMM(x) => x,
                        Location::GPR(_) | Location::Memory(_, _) => {
                            self.assembler
                                .emit_mov(Size::S64, loc_a, Location::XMM(tmp1));
                            tmp1
                        }
                        Location::Imm32(_) => {
                            self.assembler
                                .emit_mov(Size::S32, loc_a, Location::GPR(tmpg1));
                            self.assembler.emit_mov(
                                Size::S32,
                                Location::GPR(tmpg1),
                                Location::XMM(tmp1),
                            );
                            tmp1
                        }
                        Location::Imm64(_) => {
                            self.assembler
                                .emit_mov(Size::S64, loc_a, Location::GPR(tmpg1));
                            self.assembler.emit_mov(
                                Size::S64,
                                Location::GPR(tmpg1),
                                Location::XMM(tmp1),
                            );
                            tmp1
                        }
                        _ => {
                            return Err(CodegenError {
                                message: "F32Max src1: unreachable code".to_string(),
                            })
                        }
                    };
                    let src2 = match loc_b {
                        Location::XMM(x) => x,
                        Location::GPR(_) | Location::Memory(_, _) => {
                            self.assembler
                                .emit_mov(Size::S64, loc_b, Location::XMM(tmp2));
                            tmp2
                        }
                        Location::Imm32(_) => {
                            self.assembler
                                .emit_mov(Size::S32, loc_b, Location::GPR(tmpg1));
                            self.assembler.emit_mov(
                                Size::S32,
                                Location::GPR(tmpg1),
                                Location::XMM(tmp2),
                            );
                            tmp2
                        }
                        Location::Imm64(_) => {
                            self.assembler
                                .emit_mov(Size::S64, loc_b, Location::GPR(tmpg1));
                            self.assembler.emit_mov(
                                Size::S64,
                                Location::GPR(tmpg1),
                                Location::XMM(tmp2),
                            );
                            tmp2
                        }
                        _ => {
                            return Err(CodegenError {
                                message: "F32Max src2: unreachable code".to_string(),
                            })
                        }
                    };

                    let tmp_xmm1 = XMM::XMM8;
                    let tmp_xmm2 = XMM::XMM9;
                    let tmp_xmm3 = XMM::XMM10;

                    self.assembler
                        .emit_mov(Size::S32, Location::XMM(src1), Location::GPR(tmpg1));
                    self.assembler
                        .emit_mov(Size::S32, Location::XMM(src2), Location::GPR(tmpg2));
                    self.assembler
                        .emit_cmp(Size::S32, Location::GPR(tmpg2), Location::GPR(tmpg1));
                    self.assembler
                        .emit_vmaxss(src1, XMMOrMemory::XMM(src2), tmp_xmm1);
                    let label1 = self.assembler.get_label();
                    let label2 = self.assembler.get_label();
                    self.assembler.emit_jmp(Condition::NotEqual, label1);
                    self.assembler
                        .emit_vmovaps(XMMOrMemory::XMM(tmp_xmm1), XMMOrMemory::XMM(tmp_xmm2));
                    self.assembler.emit_jmp(Condition::None, label2);
                    self.assembler.emit_label(label1);
                    self.assembler
                        .emit_vxorps(tmp_xmm2, XMMOrMemory::XMM(tmp_xmm2), tmp_xmm2);
                    self.assembler.emit_label(label2);
                    self.assembler
                        .emit_vcmpeqss(src1, XMMOrMemory::XMM(src2), tmp_xmm3);
                    self.assembler.emit_vblendvps(
                        tmp_xmm3,
                        XMMOrMemory::XMM(tmp_xmm2),
                        tmp_xmm1,
                        tmp_xmm1,
                    );
                    self.assembler
                        .emit_vcmpunordss(src1, XMMOrMemory::XMM(src2), src1);
                    // load float canonical nan
                    self.assembler.emit_mov(
                        Size::S64,
                        Location::Imm32(0x7FC0_0000), // Canonical NaN
                        Location::GPR(tmpg1),
                    );
                    self.assembler
                        .emit_mov(Size::S64, Location::GPR(tmpg1), Location::XMM(src2));
                    self.assembler
                        .emit_vblendvps(src1, XMMOrMemory::XMM(src2), tmp_xmm1, src1);
                    match ret {
                        Location::XMM(x) => {
                            self.assembler
                                .emit_vmovaps(XMMOrMemory::XMM(src1), XMMOrMemory::XMM(x));
                        }
                        Location::Memory(_, _) | Location::GPR(_) => {
                            self.assembler.emit_mov(Size::S64, Location::XMM(src1), ret);
                        }
                        _ => {
                            return Err(CodegenError {
                                message: "F32Max ret: unreachable code".to_string(),
                            })
                        }
                    }

                    self.machine.release_temp_gpr(tmpg2);
                    self.machine.release_temp_gpr(tmpg1);
                    self.machine.release_temp_xmm(tmp2);
                    self.machine.release_temp_xmm(tmp1);
                }
            }
            Operator::F32Min => {
                self.fp_stack.pop2()?;
                self.fp_stack
                    .push(FloatValue::new(self.value_stack.len() - 2));
                if !self.assembler.arch_supports_canonicalize_nan() {
                    self.emit_fp_binop_avx(Assembler::emit_vminss)?;
                } else {
                    let I2O1 { loc_a, loc_b, ret } = self.i2o1_prepare(WpType::F64);

                    let tmp1 = self.machine.acquire_temp_xmm().unwrap();
                    let tmp2 = self.machine.acquire_temp_xmm().unwrap();
                    let tmpg1 = self.machine.acquire_temp_gpr().unwrap();
                    let tmpg2 = self.machine.acquire_temp_gpr().unwrap();

                    let src1 = match loc_a {
                        Location::XMM(x) => x,
                        Location::GPR(_) | Location::Memory(_, _) => {
                            self.assembler
                                .emit_mov(Size::S64, loc_a, Location::XMM(tmp1));
                            tmp1
                        }
                        Location::Imm32(_) => {
                            self.assembler
                                .emit_mov(Size::S32, loc_a, Location::GPR(tmpg1));
                            self.assembler.emit_mov(
                                Size::S32,
                                Location::GPR(tmpg1),
                                Location::XMM(tmp1),
                            );
                            tmp1
                        }
                        Location::Imm64(_) => {
                            self.assembler
                                .emit_mov(Size::S64, loc_a, Location::GPR(tmpg1));
                            self.assembler.emit_mov(
                                Size::S64,
                                Location::GPR(tmpg1),
                                Location::XMM(tmp1),
                            );
                            tmp1
                        }
                        _ => {
                            return Err(CodegenError {
                                message: "F32Min src1: unreachable code".to_string(),
                            })
                        }
                    };
                    let src2 = match loc_b {
                        Location::XMM(x) => x,
                        Location::GPR(_) | Location::Memory(_, _) => {
                            self.assembler
                                .emit_mov(Size::S64, loc_b, Location::XMM(tmp2));
                            tmp2
                        }
                        Location::Imm32(_) => {
                            self.assembler
                                .emit_mov(Size::S32, loc_b, Location::GPR(tmpg1));
                            self.assembler.emit_mov(
                                Size::S32,
                                Location::GPR(tmpg1),
                                Location::XMM(tmp2),
                            );
                            tmp2
                        }
                        Location::Imm64(_) => {
                            self.assembler
                                .emit_mov(Size::S64, loc_b, Location::GPR(tmpg1));
                            self.assembler.emit_mov(
                                Size::S64,
                                Location::GPR(tmpg1),
                                Location::XMM(tmp2),
                            );
                            tmp2
                        }
                        _ => {
                            return Err(CodegenError {
                                message: "F32Min src2: unreachable code".to_string(),
                            })
                        }
                    };

                    let tmp_xmm1 = XMM::XMM8;
                    let tmp_xmm2 = XMM::XMM9;
                    let tmp_xmm3 = XMM::XMM10;

                    self.assembler
                        .emit_mov(Size::S32, Location::XMM(src1), Location::GPR(tmpg1));
                    self.assembler
                        .emit_mov(Size::S32, Location::XMM(src2), Location::GPR(tmpg2));
                    self.assembler
                        .emit_cmp(Size::S32, Location::GPR(tmpg2), Location::GPR(tmpg1));
                    self.assembler
                        .emit_vminss(src1, XMMOrMemory::XMM(src2), tmp_xmm1);
                    let label1 = self.assembler.get_label();
                    let label2 = self.assembler.get_label();
                    self.assembler.emit_jmp(Condition::NotEqual, label1);
                    self.assembler
                        .emit_vmovaps(XMMOrMemory::XMM(tmp_xmm1), XMMOrMemory::XMM(tmp_xmm2));
                    self.assembler.emit_jmp(Condition::None, label2);
                    self.assembler.emit_label(label1);
                    // load float -0.0
                    self.assembler.emit_mov(
                        Size::S64,
                        Location::Imm32(0x8000_0000), // Negative zero
                        Location::GPR(tmpg1),
                    );
                    self.assembler.emit_mov(
                        Size::S64,
                        Location::GPR(tmpg1),
                        Location::XMM(tmp_xmm2),
                    );
                    self.assembler.emit_label(label2);
                    self.assembler
                        .emit_vcmpeqss(src1, XMMOrMemory::XMM(src2), tmp_xmm3);
                    self.assembler.emit_vblendvps(
                        tmp_xmm3,
                        XMMOrMemory::XMM(tmp_xmm2),
                        tmp_xmm1,
                        tmp_xmm1,
                    );
                    self.assembler
                        .emit_vcmpunordss(src1, XMMOrMemory::XMM(src2), src1);
                    // load float canonical nan
                    self.assembler.emit_mov(
                        Size::S64,
                        Location::Imm32(0x7FC0_0000), // Canonical NaN
                        Location::GPR(tmpg1),
                    );
                    self.assembler
                        .emit_mov(Size::S64, Location::GPR(tmpg1), Location::XMM(src2));
                    self.assembler
                        .emit_vblendvps(src1, XMMOrMemory::XMM(src2), tmp_xmm1, src1);
                    match ret {
                        Location::XMM(x) => {
                            self.assembler
                                .emit_vmovaps(XMMOrMemory::XMM(src1), XMMOrMemory::XMM(x));
                        }
                        Location::Memory(_, _) | Location::GPR(_) => {
                            self.assembler.emit_mov(Size::S64, Location::XMM(src1), ret);
                        }
                        _ => {
                            return Err(CodegenError {
                                message: "F32Min ret: unreachable code".to_string(),
                            })
                        }
                    }

                    self.machine.release_temp_gpr(tmpg2);
                    self.machine.release_temp_gpr(tmpg1);
                    self.machine.release_temp_xmm(tmp2);
                    self.machine.release_temp_xmm(tmp1);
                }
            }
            Operator::F32Eq => {
                self.fp_stack.pop2()?;
                self.emit_fp_cmpop_avx(Assembler::emit_vcmpeqss)?
            }
            Operator::F32Ne => {
                self.fp_stack.pop2()?;
                self.emit_fp_cmpop_avx(Assembler::emit_vcmpneqss)?
            }
            Operator::F32Lt => {
                self.fp_stack.pop2()?;
                self.emit_fp_cmpop_avx(Assembler::emit_vcmpltss)?
            }
            Operator::F32Le => {
                self.fp_stack.pop2()?;
                self.emit_fp_cmpop_avx(Assembler::emit_vcmpless)?
            }
            Operator::F32Gt => {
                self.fp_stack.pop2()?;
                self.emit_fp_cmpop_avx(Assembler::emit_vcmpgtss)?
            }
            Operator::F32Ge => {
                self.fp_stack.pop2()?;
                self.emit_fp_cmpop_avx(Assembler::emit_vcmpgess)?
            }
            Operator::F32Nearest => {
                self.fp_stack.pop1()?;
                self.fp_stack
                    .push(FloatValue::cncl_f32(self.value_stack.len() - 1));
                self.emit_fp_unop_avx(Assembler::emit_vroundss_nearest)?
            }
            Operator::F32Floor => {
                self.fp_stack.pop1()?;
                self.fp_stack
                    .push(FloatValue::cncl_f32(self.value_stack.len() - 1));
                self.emit_fp_unop_avx(Assembler::emit_vroundss_floor)?
            }
            Operator::F32Ceil => {
                self.fp_stack.pop1()?;
                self.fp_stack
                    .push(FloatValue::cncl_f32(self.value_stack.len() - 1));
                self.emit_fp_unop_avx(Assembler::emit_vroundss_ceil)?
            }
            Operator::F32Trunc => {
                self.fp_stack.pop1()?;
                self.fp_stack
                    .push(FloatValue::cncl_f32(self.value_stack.len() - 1));
                self.emit_fp_unop_avx(Assembler::emit_vroundss_trunc)?
            }
            Operator::F32Sqrt => {
                self.fp_stack.pop1()?;
                self.fp_stack
                    .push(FloatValue::cncl_f32(self.value_stack.len() - 1));
                self.emit_fp_unop_avx(Assembler::emit_vsqrtss)?
            }

            Operator::F32Copysign => {
                let I2O1 { loc_a, loc_b, ret } = self.i2o1_prepare(WpType::F32);

                let (fp_src1, fp_src2) = self.fp_stack.pop2()?;
                self.fp_stack
                    .push(FloatValue::new(self.value_stack.len() - 1));

                let tmp1 = self.machine.acquire_temp_gpr().unwrap();
                let tmp2 = self.machine.acquire_temp_gpr().unwrap();

                if self.assembler.arch_supports_canonicalize_nan()
                    && self.config.enable_nan_canonicalization
                {
                    for (fp, loc, tmp) in [(fp_src1, loc_a, tmp1), (fp_src2, loc_b, tmp2)].iter() {
                        match fp.canonicalization {
                            Some(_) => {
                                self.canonicalize_nan(Size::S32, *loc, Location::GPR(*tmp));
                            }
                            None => {
                                self.assembler
                                    .emit_mov(Size::S32, *loc, Location::GPR(*tmp));
                            }
                        }
                    }
                } else {
                    self.assembler
                        .emit_mov(Size::S32, loc_a, Location::GPR(tmp1));
                    self.assembler
                        .emit_mov(Size::S32, loc_b, Location::GPR(tmp2));
                }
                self.assembler.emit_and(
                    Size::S32,
                    Location::Imm32(0x7fffffffu32),
                    Location::GPR(tmp1),
                );
                self.assembler.emit_and(
                    Size::S32,
                    Location::Imm32(0x80000000u32),
                    Location::GPR(tmp2),
                );
                self.assembler
                    .emit_or(Size::S32, Location::GPR(tmp2), Location::GPR(tmp1));
                self.assembler.emit_mov(Size::S32, Location::GPR(tmp1), ret);
                self.machine.release_temp_gpr(tmp2);
                self.machine.release_temp_gpr(tmp1);
            }

            Operator::F32Abs => {
                // Preserve canonicalization state.

                let loc = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::F32)], false)[0];
                self.value_stack.push(ret);
                let tmp = self.machine.acquire_temp_gpr().unwrap();
                self.assembler.emit_mov(Size::S32, loc, Location::GPR(tmp));
                self.assembler.emit_and(
                    Size::S32,
                    Location::Imm32(0x7fffffffu32),
                    Location::GPR(tmp),
                );
                self.assembler.emit_mov(Size::S32, Location::GPR(tmp), ret);
                self.machine.release_temp_gpr(tmp);
            }

            Operator::F32Neg => {
                // Preserve canonicalization state.

                let loc = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::F32)], false)[0];
                self.value_stack.push(ret);

                if self.assembler.arch_has_fneg() {
                    let tmp = self.machine.acquire_temp_xmm().unwrap();
                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S32,
                        loc,
                        Location::XMM(tmp),
                    );
                    self.assembler.arch_emit_f32_neg(tmp, tmp);
                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S32,
                        Location::XMM(tmp),
                        ret,
                    );
                    self.machine.release_temp_xmm(tmp);
                } else {
                    let tmp = self.machine.acquire_temp_gpr().unwrap();
                    self.assembler.emit_mov(Size::S32, loc, Location::GPR(tmp));
                    self.assembler.emit_btc_gpr_imm8_32(31, tmp);
                    self.assembler.emit_mov(Size::S32, Location::GPR(tmp), ret);
                    self.machine.release_temp_gpr(tmp);
                }
            }

            Operator::F64Const { value } => {
                self.value_stack.push(Location::Imm64(value.bits()));
                self.fp_stack
                    .push(FloatValue::new(self.value_stack.len() - 1));
            }
            Operator::F64Add => {
                self.fp_stack.pop2()?;
                self.fp_stack
                    .push(FloatValue::cncl_f64(self.value_stack.len() - 2));
                self.emit_fp_binop_avx(Assembler::emit_vaddsd)?
            }
            Operator::F64Sub => {
                self.fp_stack.pop2()?;
                self.fp_stack
                    .push(FloatValue::cncl_f64(self.value_stack.len() - 2));
                self.emit_fp_binop_avx(Assembler::emit_vsubsd)?
            }
            Operator::F64Mul => {
                self.fp_stack.pop2()?;
                self.fp_stack
                    .push(FloatValue::cncl_f64(self.value_stack.len() - 2));
                self.emit_fp_binop_avx(Assembler::emit_vmulsd)?
            }
            Operator::F64Div => {
                self.fp_stack.pop2()?;
                self.fp_stack
                    .push(FloatValue::cncl_f64(self.value_stack.len() - 2));
                self.emit_fp_binop_avx(Assembler::emit_vdivsd)?
            }
            Operator::F64Max => {
                self.fp_stack.pop2()?;
                self.fp_stack
                    .push(FloatValue::new(self.value_stack.len() - 2));

                if !self.assembler.arch_supports_canonicalize_nan() {
                    self.emit_fp_binop_avx(Assembler::emit_vmaxsd)?;
                } else {
                    let I2O1 { loc_a, loc_b, ret } = self.i2o1_prepare(WpType::F64);

                    let tmp1 = self.machine.acquire_temp_xmm().unwrap();
                    let tmp2 = self.machine.acquire_temp_xmm().unwrap();
                    let tmpg1 = self.machine.acquire_temp_gpr().unwrap();
                    let tmpg2 = self.machine.acquire_temp_gpr().unwrap();

                    let src1 = match loc_a {
                        Location::XMM(x) => x,
                        Location::GPR(_) | Location::Memory(_, _) => {
                            self.assembler
                                .emit_mov(Size::S64, loc_a, Location::XMM(tmp1));
                            tmp1
                        }
                        Location::Imm32(_) => {
                            self.assembler
                                .emit_mov(Size::S32, loc_a, Location::GPR(tmpg1));
                            self.assembler.emit_mov(
                                Size::S32,
                                Location::GPR(tmpg1),
                                Location::XMM(tmp1),
                            );
                            tmp1
                        }
                        Location::Imm64(_) => {
                            self.assembler
                                .emit_mov(Size::S64, loc_a, Location::GPR(tmpg1));
                            self.assembler.emit_mov(
                                Size::S64,
                                Location::GPR(tmpg1),
                                Location::XMM(tmp1),
                            );
                            tmp1
                        }
                        _ => {
                            return Err(CodegenError {
                                message: "F64Max src1: unreachable code".to_string(),
                            })
                        }
                    };
                    let src2 = match loc_b {
                        Location::XMM(x) => x,
                        Location::GPR(_) | Location::Memory(_, _) => {
                            self.assembler
                                .emit_mov(Size::S64, loc_b, Location::XMM(tmp2));
                            tmp2
                        }
                        Location::Imm32(_) => {
                            self.assembler
                                .emit_mov(Size::S32, loc_b, Location::GPR(tmpg1));
                            self.assembler.emit_mov(
                                Size::S32,
                                Location::GPR(tmpg1),
                                Location::XMM(tmp2),
                            );
                            tmp2
                        }
                        Location::Imm64(_) => {
                            self.assembler
                                .emit_mov(Size::S64, loc_b, Location::GPR(tmpg1));
                            self.assembler.emit_mov(
                                Size::S64,
                                Location::GPR(tmpg1),
                                Location::XMM(tmp2),
                            );
                            tmp2
                        }
                        _ => {
                            return Err(CodegenError {
                                message: "F64Max src2: unreachable code".to_string(),
                            })
                        }
                    };

                    let tmp_xmm1 = XMM::XMM8;
                    let tmp_xmm2 = XMM::XMM9;
                    let tmp_xmm3 = XMM::XMM10;

                    self.assembler
                        .emit_mov(Size::S64, Location::XMM(src1), Location::GPR(tmpg1));
                    self.assembler
                        .emit_mov(Size::S64, Location::XMM(src2), Location::GPR(tmpg2));
                    self.assembler
                        .emit_cmp(Size::S64, Location::GPR(tmpg2), Location::GPR(tmpg1));
                    self.assembler
                        .emit_vmaxsd(src1, XMMOrMemory::XMM(src2), tmp_xmm1);
                    let label1 = self.assembler.get_label();
                    let label2 = self.assembler.get_label();
                    self.assembler.emit_jmp(Condition::NotEqual, label1);
                    self.assembler
                        .emit_vmovapd(XMMOrMemory::XMM(tmp_xmm1), XMMOrMemory::XMM(tmp_xmm2));
                    self.assembler.emit_jmp(Condition::None, label2);
                    self.assembler.emit_label(label1);
                    self.assembler
                        .emit_vxorpd(tmp_xmm2, XMMOrMemory::XMM(tmp_xmm2), tmp_xmm2);
                    self.assembler.emit_label(label2);
                    self.assembler
                        .emit_vcmpeqsd(src1, XMMOrMemory::XMM(src2), tmp_xmm3);
                    self.assembler.emit_vblendvpd(
                        tmp_xmm3,
                        XMMOrMemory::XMM(tmp_xmm2),
                        tmp_xmm1,
                        tmp_xmm1,
                    );
                    self.assembler
                        .emit_vcmpunordsd(src1, XMMOrMemory::XMM(src2), src1);
                    // load float canonical nan
                    self.assembler.emit_mov(
                        Size::S64,
                        Location::Imm64(0x7FF8_0000_0000_0000), // Canonical NaN
                        Location::GPR(tmpg1),
                    );
                    self.assembler
                        .emit_mov(Size::S64, Location::GPR(tmpg1), Location::XMM(src2));
                    self.assembler
                        .emit_vblendvpd(src1, XMMOrMemory::XMM(src2), tmp_xmm1, src1);
                    match ret {
                        Location::XMM(x) => {
                            self.assembler
                                .emit_vmovapd(XMMOrMemory::XMM(src1), XMMOrMemory::XMM(x));
                        }
                        Location::Memory(_, _) | Location::GPR(_) => {
                            self.assembler.emit_mov(Size::S64, Location::XMM(src1), ret);
                        }
                        _ => {
                            return Err(CodegenError {
                                message: "F64Max ret: unreachable code".to_string(),
                            })
                        }
                    }

                    self.machine.release_temp_gpr(tmpg2);
                    self.machine.release_temp_gpr(tmpg1);
                    self.machine.release_temp_xmm(tmp2);
                    self.machine.release_temp_xmm(tmp1);
                }
            }
            Operator::F64Min => {
                self.fp_stack.pop2()?;
                self.fp_stack
                    .push(FloatValue::new(self.value_stack.len() - 2));

                if !self.assembler.arch_supports_canonicalize_nan() {
                    self.emit_fp_binop_avx(Assembler::emit_vminsd)?;
                } else {
                    let I2O1 { loc_a, loc_b, ret } = self.i2o1_prepare(WpType::F64);

                    let tmp1 = self.machine.acquire_temp_xmm().unwrap();
                    let tmp2 = self.machine.acquire_temp_xmm().unwrap();
                    let tmpg1 = self.machine.acquire_temp_gpr().unwrap();
                    let tmpg2 = self.machine.acquire_temp_gpr().unwrap();

                    let src1 = match loc_a {
                        Location::XMM(x) => x,
                        Location::GPR(_) | Location::Memory(_, _) => {
                            self.assembler
                                .emit_mov(Size::S64, loc_a, Location::XMM(tmp1));
                            tmp1
                        }
                        Location::Imm32(_) => {
                            self.assembler
                                .emit_mov(Size::S32, loc_a, Location::GPR(tmpg1));
                            self.assembler.emit_mov(
                                Size::S32,
                                Location::GPR(tmpg1),
                                Location::XMM(tmp1),
                            );
                            tmp1
                        }
                        Location::Imm64(_) => {
                            self.assembler
                                .emit_mov(Size::S64, loc_a, Location::GPR(tmpg1));
                            self.assembler.emit_mov(
                                Size::S64,
                                Location::GPR(tmpg1),
                                Location::XMM(tmp1),
                            );
                            tmp1
                        }
                        _ => {
                            return Err(CodegenError {
                                message: "F64Min src1: unreachable code".to_string(),
                            })
                        }
                    };
                    let src2 = match loc_b {
                        Location::XMM(x) => x,
                        Location::GPR(_) | Location::Memory(_, _) => {
                            self.assembler
                                .emit_mov(Size::S64, loc_b, Location::XMM(tmp2));
                            tmp2
                        }
                        Location::Imm32(_) => {
                            self.assembler
                                .emit_mov(Size::S32, loc_b, Location::GPR(tmpg1));
                            self.assembler.emit_mov(
                                Size::S32,
                                Location::GPR(tmpg1),
                                Location::XMM(tmp2),
                            );
                            tmp2
                        }
                        Location::Imm64(_) => {
                            self.assembler
                                .emit_mov(Size::S64, loc_b, Location::GPR(tmpg1));
                            self.assembler.emit_mov(
                                Size::S64,
                                Location::GPR(tmpg1),
                                Location::XMM(tmp2),
                            );
                            tmp2
                        }
                        _ => {
                            return Err(CodegenError {
                                message: "F64Min src2: unreachable code".to_string(),
                            })
                        }
                    };

                    let tmp_xmm1 = XMM::XMM8;
                    let tmp_xmm2 = XMM::XMM9;
                    let tmp_xmm3 = XMM::XMM10;

                    self.assembler
                        .emit_mov(Size::S64, Location::XMM(src1), Location::GPR(tmpg1));
                    self.assembler
                        .emit_mov(Size::S64, Location::XMM(src2), Location::GPR(tmpg2));
                    self.assembler
                        .emit_cmp(Size::S64, Location::GPR(tmpg2), Location::GPR(tmpg1));
                    self.assembler
                        .emit_vminsd(src1, XMMOrMemory::XMM(src2), tmp_xmm1);
                    let label1 = self.assembler.get_label();
                    let label2 = self.assembler.get_label();
                    self.assembler.emit_jmp(Condition::NotEqual, label1);
                    self.assembler
                        .emit_vmovapd(XMMOrMemory::XMM(tmp_xmm1), XMMOrMemory::XMM(tmp_xmm2));
                    self.assembler.emit_jmp(Condition::None, label2);
                    self.assembler.emit_label(label1);
                    // load float -0.0
                    self.assembler.emit_mov(
                        Size::S64,
                        Location::Imm64(0x8000_0000_0000_0000), // Negative zero
                        Location::GPR(tmpg1),
                    );
                    self.assembler.emit_mov(
                        Size::S64,
                        Location::GPR(tmpg1),
                        Location::XMM(tmp_xmm2),
                    );
                    self.assembler.emit_label(label2);
                    self.assembler
                        .emit_vcmpeqsd(src1, XMMOrMemory::XMM(src2), tmp_xmm3);
                    self.assembler.emit_vblendvpd(
                        tmp_xmm3,
                        XMMOrMemory::XMM(tmp_xmm2),
                        tmp_xmm1,
                        tmp_xmm1,
                    );
                    self.assembler
                        .emit_vcmpunordsd(src1, XMMOrMemory::XMM(src2), src1);
                    // load float canonical nan
                    self.assembler.emit_mov(
                        Size::S64,
                        Location::Imm64(0x7FF8_0000_0000_0000), // Canonical NaN
                        Location::GPR(tmpg1),
                    );
                    self.assembler
                        .emit_mov(Size::S64, Location::GPR(tmpg1), Location::XMM(src2));
                    self.assembler
                        .emit_vblendvpd(src1, XMMOrMemory::XMM(src2), tmp_xmm1, src1);
                    match ret {
                        Location::XMM(x) => {
                            self.assembler
                                .emit_vmovaps(XMMOrMemory::XMM(src1), XMMOrMemory::XMM(x));
                        }
                        Location::Memory(_, _) | Location::GPR(_) => {
                            self.assembler.emit_mov(Size::S64, Location::XMM(src1), ret);
                        }
                        _ => {
                            return Err(CodegenError {
                                message: "F64Min ret: unreachable code".to_string(),
                            })
                        }
                    }

                    self.machine.release_temp_gpr(tmpg2);
                    self.machine.release_temp_gpr(tmpg1);
                    self.machine.release_temp_xmm(tmp2);
                    self.machine.release_temp_xmm(tmp1);
                }
            }
            Operator::F64Eq => {
                self.fp_stack.pop2()?;
                self.emit_fp_cmpop_avx(Assembler::emit_vcmpeqsd)?
            }
            Operator::F64Ne => {
                self.fp_stack.pop2()?;
                self.emit_fp_cmpop_avx(Assembler::emit_vcmpneqsd)?
            }
            Operator::F64Lt => {
                self.fp_stack.pop2()?;
                self.emit_fp_cmpop_avx(Assembler::emit_vcmpltsd)?
            }
            Operator::F64Le => {
                self.fp_stack.pop2()?;
                self.emit_fp_cmpop_avx(Assembler::emit_vcmplesd)?
            }
            Operator::F64Gt => {
                self.fp_stack.pop2()?;
                self.emit_fp_cmpop_avx(Assembler::emit_vcmpgtsd)?
            }
            Operator::F64Ge => {
                self.fp_stack.pop2()?;
                self.emit_fp_cmpop_avx(Assembler::emit_vcmpgesd)?
            }
            Operator::F64Nearest => {
                self.fp_stack.pop1()?;
                self.fp_stack
                    .push(FloatValue::cncl_f64(self.value_stack.len() - 1));
                self.emit_fp_unop_avx(Assembler::emit_vroundsd_nearest)?
            }
            Operator::F64Floor => {
                self.fp_stack.pop1()?;
                self.fp_stack
                    .push(FloatValue::cncl_f64(self.value_stack.len() - 1));
                self.emit_fp_unop_avx(Assembler::emit_vroundsd_floor)?
            }
            Operator::F64Ceil => {
                self.fp_stack.pop1()?;
                self.fp_stack
                    .push(FloatValue::cncl_f64(self.value_stack.len() - 1));
                self.emit_fp_unop_avx(Assembler::emit_vroundsd_ceil)?
            }
            Operator::F64Trunc => {
                self.fp_stack.pop1()?;
                self.fp_stack
                    .push(FloatValue::cncl_f64(self.value_stack.len() - 1));
                self.emit_fp_unop_avx(Assembler::emit_vroundsd_trunc)?
            }
            Operator::F64Sqrt => {
                self.fp_stack.pop1()?;
                self.fp_stack
                    .push(FloatValue::cncl_f64(self.value_stack.len() - 1));
                self.emit_fp_unop_avx(Assembler::emit_vsqrtsd)?
            }

            Operator::F64Copysign => {
                let I2O1 { loc_a, loc_b, ret } = self.i2o1_prepare(WpType::F64);

                let (fp_src1, fp_src2) = self.fp_stack.pop2()?;
                self.fp_stack
                    .push(FloatValue::new(self.value_stack.len() - 1));

                let tmp1 = self.machine.acquire_temp_gpr().unwrap();
                let tmp2 = self.machine.acquire_temp_gpr().unwrap();

                if self.assembler.arch_supports_canonicalize_nan()
                    && self.config.enable_nan_canonicalization
                {
                    for (fp, loc, tmp) in [(fp_src1, loc_a, tmp1), (fp_src2, loc_b, tmp2)].iter() {
                        match fp.canonicalization {
                            Some(_) => {
                                self.canonicalize_nan(Size::S64, *loc, Location::GPR(*tmp));
                            }
                            None => {
                                self.assembler
                                    .emit_mov(Size::S64, *loc, Location::GPR(*tmp));
                            }
                        }
                    }
                } else {
                    self.assembler
                        .emit_mov(Size::S64, loc_a, Location::GPR(tmp1));
                    self.assembler
                        .emit_mov(Size::S64, loc_b, Location::GPR(tmp2));
                }

                let c = self.machine.acquire_temp_gpr().unwrap();

                self.assembler.emit_mov(
                    Size::S64,
                    Location::Imm64(0x7fffffffffffffffu64),
                    Location::GPR(c),
                );
                self.assembler
                    .emit_and(Size::S64, Location::GPR(c), Location::GPR(tmp1));

                self.assembler.emit_mov(
                    Size::S64,
                    Location::Imm64(0x8000000000000000u64),
                    Location::GPR(c),
                );
                self.assembler
                    .emit_and(Size::S64, Location::GPR(c), Location::GPR(tmp2));

                self.assembler
                    .emit_or(Size::S64, Location::GPR(tmp2), Location::GPR(tmp1));
                self.assembler.emit_mov(Size::S64, Location::GPR(tmp1), ret);

                self.machine.release_temp_gpr(c);
                self.machine.release_temp_gpr(tmp2);
                self.machine.release_temp_gpr(tmp1);
            }

            Operator::F64Abs => {
                // Preserve canonicalization state.

                let loc = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::F64)], false)[0];
                self.value_stack.push(ret);

                let tmp = self.machine.acquire_temp_gpr().unwrap();
                let c = self.machine.acquire_temp_gpr().unwrap();

                self.assembler.emit_mov(Size::S64, loc, Location::GPR(tmp));
                self.assembler.emit_mov(
                    Size::S64,
                    Location::Imm64(0x7fffffffffffffffu64),
                    Location::GPR(c),
                );
                self.assembler
                    .emit_and(Size::S64, Location::GPR(c), Location::GPR(tmp));
                self.assembler.emit_mov(Size::S64, Location::GPR(tmp), ret);

                self.machine.release_temp_gpr(c);
                self.machine.release_temp_gpr(tmp);
            }

            Operator::F64Neg => {
                // Preserve canonicalization state.

                let loc = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::F64)], false)[0];
                self.value_stack.push(ret);
                if self.assembler.arch_has_fneg() {
                    let tmp = self.machine.acquire_temp_xmm().unwrap();
                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S64,
                        loc,
                        Location::XMM(tmp),
                    );
                    self.assembler.arch_emit_f64_neg(tmp, tmp);
                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S64,
                        Location::XMM(tmp),
                        ret,
                    );
                    self.machine.release_temp_xmm(tmp);
                } else {
                    let tmp = self.machine.acquire_temp_gpr().unwrap();
                    self.assembler.emit_mov(Size::S64, loc, Location::GPR(tmp));
                    self.assembler.emit_btc_gpr_imm8_64(63, tmp);
                    self.assembler.emit_mov(Size::S64, Location::GPR(tmp), ret);
                    self.machine.release_temp_gpr(tmp);
                }
            }

            Operator::F64PromoteF32 => {
                let fp = self.fp_stack.pop1()?;
                self.fp_stack.push(fp.promote(self.value_stack.len() - 1));
                self.emit_fp_unop_avx(Assembler::emit_vcvtss2sd)?
            }
            Operator::F32DemoteF64 => {
                let fp = self.fp_stack.pop1()?;
                self.fp_stack.push(fp.demote(self.value_stack.len() - 1));
                self.emit_fp_unop_avx(Assembler::emit_vcvtsd2ss)?
            }

            Operator::I32ReinterpretF32 => {
                let loc = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[WpType::I32], false)[0];
                self.value_stack.push(ret);
                let fp = self.fp_stack.pop1()?;

                if !self.assembler.arch_supports_canonicalize_nan()
                    || !self.config.enable_nan_canonicalization
                    || fp.canonicalization.is_none()
                {
                    if loc != ret {
                        self.emit_relaxed_binop(Assembler::emit_mov, Size::S32, loc, ret);
                    }
                } else {
                    self.canonicalize_nan(Size::S32, loc, ret);
                }
            }
            Operator::F32ReinterpretI32 => {
                let loc = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[WpType::F32], false)[0];
                self.value_stack.push(ret);
                self.fp_stack
                    .push(FloatValue::new(self.value_stack.len() - 1));

                if loc != ret {
                    self.emit_relaxed_binop(Assembler::emit_mov, Size::S32, loc, ret);
                }
            }

            Operator::I64ReinterpretF64 => {
                let loc = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);
                let fp = self.fp_stack.pop1()?;

                if !self.assembler.arch_supports_canonicalize_nan()
                    || !self.config.enable_nan_canonicalization
                    || fp.canonicalization.is_none()
                {
                    if loc != ret {
                        self.emit_relaxed_binop(Assembler::emit_mov, Size::S64, loc, ret);
                    }
                } else {
                    self.canonicalize_nan(Size::S64, loc, ret);
                }
            }
            Operator::F64ReinterpretI64 => {
                let loc = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::F64)], false)[0];
                self.value_stack.push(ret);
                self.fp_stack
                    .push(FloatValue::new(self.value_stack.len() - 1));

                if loc != ret {
                    self.emit_relaxed_binop(Assembler::emit_mov, Size::S64, loc, ret);
                }
            }

            Operator::I32TruncF32U => {
                let loc = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];
                self.value_stack.push(ret);
                self.fp_stack.pop1()?;

                if self.assembler.arch_has_itruncf() {
                    let tmp_out = self.machine.acquire_temp_gpr().unwrap();
                    let tmp_in = self.machine.acquire_temp_xmm().unwrap();
                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S32,
                        loc,
                        Location::XMM(tmp_in),
                    );
                    self.assembler.arch_emit_i32_trunc_uf32(tmp_in, tmp_out);
                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S32,
                        Location::GPR(tmp_out),
                        ret,
                    );
                    self.machine.release_temp_xmm(tmp_in);
                    self.machine.release_temp_gpr(tmp_out);
                } else {
                    let tmp_out = self.machine.acquire_temp_gpr().unwrap();
                    let tmp_in = self.machine.acquire_temp_xmm().unwrap();
                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S32,
                        loc,
                        Location::XMM(tmp_in),
                    );
                    self.emit_f32_int_conv_check_trap(tmp_in, GEF32_LT_U32_MIN, LEF32_GT_U32_MAX);

                    self.assembler
                        .emit_cvttss2si_64(XMMOrMemory::XMM(tmp_in), tmp_out);
                    self.assembler
                        .emit_mov(Size::S32, Location::GPR(tmp_out), ret);

                    self.machine.release_temp_xmm(tmp_in);
                    self.machine.release_temp_gpr(tmp_out);
                }
            }

            Operator::I32TruncSatF32U => {
                let loc = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];
                self.value_stack.push(ret);
                self.fp_stack.pop1()?;

                let tmp_out = self.machine.acquire_temp_gpr().unwrap();
                let tmp_in = self.machine.acquire_temp_xmm().unwrap();
                self.emit_relaxed_binop(Assembler::emit_mov, Size::S32, loc, Location::XMM(tmp_in));
                self.emit_f32_int_conv_check_sat(
                    tmp_in,
                    GEF32_LT_U32_MIN,
                    LEF32_GT_U32_MAX,
                    |this| {
                        this.assembler.emit_mov(
                            Size::S32,
                            Location::Imm32(0),
                            Location::GPR(tmp_out),
                        );
                    },
                    |this| {
                        this.assembler.emit_mov(
                            Size::S32,
                            Location::Imm32(std::u32::MAX),
                            Location::GPR(tmp_out),
                        );
                    },
                    None::<fn(this: &mut Self)>,
                    |this| {
                        if this.assembler.arch_has_itruncf() {
                            this.assembler.arch_emit_i32_trunc_uf32(tmp_in, tmp_out);
                        } else {
                            this.assembler
                                .emit_cvttss2si_64(XMMOrMemory::XMM(tmp_in), tmp_out);
                        }
                    },
                );

                self.assembler
                    .emit_mov(Size::S32, Location::GPR(tmp_out), ret);
                self.machine.release_temp_xmm(tmp_in);
                self.machine.release_temp_gpr(tmp_out);
            }

            Operator::I32TruncF32S => {
                let loc = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];
                self.value_stack.push(ret);
                self.fp_stack.pop1()?;

                if self.assembler.arch_has_itruncf() {
                    let tmp_out = self.machine.acquire_temp_gpr().unwrap();
                    let tmp_in = self.machine.acquire_temp_xmm().unwrap();
                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S32,
                        loc,
                        Location::XMM(tmp_in),
                    );
                    self.assembler.arch_emit_i32_trunc_sf32(tmp_in, tmp_out);
                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S32,
                        Location::GPR(tmp_out),
                        ret,
                    );
                    self.machine.release_temp_xmm(tmp_in);
                    self.machine.release_temp_gpr(tmp_out);
                } else {
                    let tmp_out = self.machine.acquire_temp_gpr().unwrap();
                    let tmp_in = self.machine.acquire_temp_xmm().unwrap();

                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S32,
                        loc,
                        Location::XMM(tmp_in),
                    );
                    self.emit_f32_int_conv_check_trap(tmp_in, GEF32_LT_I32_MIN, LEF32_GT_I32_MAX);

                    self.assembler
                        .emit_cvttss2si_32(XMMOrMemory::XMM(tmp_in), tmp_out);
                    self.assembler
                        .emit_mov(Size::S32, Location::GPR(tmp_out), ret);

                    self.machine.release_temp_xmm(tmp_in);
                    self.machine.release_temp_gpr(tmp_out);
                }
            }
            Operator::I32TruncSatF32S => {
                let loc = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];
                self.value_stack.push(ret);
                self.fp_stack.pop1()?;

                let tmp_out = self.machine.acquire_temp_gpr().unwrap();
                let tmp_in = self.machine.acquire_temp_xmm().unwrap();

                self.emit_relaxed_binop(Assembler::emit_mov, Size::S32, loc, Location::XMM(tmp_in));
                self.emit_f32_int_conv_check_sat(
                    tmp_in,
                    GEF32_LT_I32_MIN,
                    LEF32_GT_I32_MAX,
                    |this| {
                        this.assembler.emit_mov(
                            Size::S32,
                            Location::Imm32(std::i32::MIN as u32),
                            Location::GPR(tmp_out),
                        );
                    },
                    |this| {
                        this.assembler.emit_mov(
                            Size::S32,
                            Location::Imm32(std::i32::MAX as u32),
                            Location::GPR(tmp_out),
                        );
                    },
                    Some(|this: &mut Self| {
                        this.assembler.emit_mov(
                            Size::S32,
                            Location::Imm32(0),
                            Location::GPR(tmp_out),
                        );
                    }),
                    |this| {
                        if this.assembler.arch_has_itruncf() {
                            this.assembler.arch_emit_i32_trunc_sf32(tmp_in, tmp_out);
                        } else {
                            this.assembler
                                .emit_cvttss2si_32(XMMOrMemory::XMM(tmp_in), tmp_out);
                        }
                    },
                );

                self.assembler
                    .emit_mov(Size::S32, Location::GPR(tmp_out), ret);
                self.machine.release_temp_xmm(tmp_in);
                self.machine.release_temp_gpr(tmp_out);
            }

            Operator::I64TruncF32S => {
                let loc = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);
                self.fp_stack.pop1()?;

                if self.assembler.arch_has_itruncf() {
                    let tmp_out = self.machine.acquire_temp_gpr().unwrap();
                    let tmp_in = self.machine.acquire_temp_xmm().unwrap();
                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S32,
                        loc,
                        Location::XMM(tmp_in),
                    );
                    self.assembler.arch_emit_i64_trunc_sf32(tmp_in, tmp_out);
                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S64,
                        Location::GPR(tmp_out),
                        ret,
                    );
                    self.machine.release_temp_xmm(tmp_in);
                    self.machine.release_temp_gpr(tmp_out);
                } else {
                    let tmp_out = self.machine.acquire_temp_gpr().unwrap();
                    let tmp_in = self.machine.acquire_temp_xmm().unwrap();

                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S32,
                        loc,
                        Location::XMM(tmp_in),
                    );
                    self.emit_f32_int_conv_check_trap(tmp_in, GEF32_LT_I64_MIN, LEF32_GT_I64_MAX);
                    self.assembler
                        .emit_cvttss2si_64(XMMOrMemory::XMM(tmp_in), tmp_out);
                    self.assembler
                        .emit_mov(Size::S64, Location::GPR(tmp_out), ret);

                    self.machine.release_temp_xmm(tmp_in);
                    self.machine.release_temp_gpr(tmp_out);
                }
            }

            Operator::I64TruncSatF32S => {
                let loc = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);
                self.fp_stack.pop1()?;

                let tmp_out = self.machine.acquire_temp_gpr().unwrap();
                let tmp_in = self.machine.acquire_temp_xmm().unwrap();

                self.emit_relaxed_binop(Assembler::emit_mov, Size::S32, loc, Location::XMM(tmp_in));
                self.emit_f32_int_conv_check_sat(
                    tmp_in,
                    GEF32_LT_I64_MIN,
                    LEF32_GT_I64_MAX,
                    |this| {
                        this.assembler.emit_mov(
                            Size::S64,
                            Location::Imm64(std::i64::MIN as u64),
                            Location::GPR(tmp_out),
                        );
                    },
                    |this| {
                        this.assembler.emit_mov(
                            Size::S64,
                            Location::Imm64(std::i64::MAX as u64),
                            Location::GPR(tmp_out),
                        );
                    },
                    Some(|this: &mut Self| {
                        this.assembler.emit_mov(
                            Size::S64,
                            Location::Imm64(0),
                            Location::GPR(tmp_out),
                        );
                    }),
                    |this| {
                        if this.assembler.arch_has_itruncf() {
                            this.assembler.arch_emit_i64_trunc_sf32(tmp_in, tmp_out);
                        } else {
                            this.assembler
                                .emit_cvttss2si_64(XMMOrMemory::XMM(tmp_in), tmp_out);
                        }
                    },
                );

                self.assembler
                    .emit_mov(Size::S64, Location::GPR(tmp_out), ret);
                self.machine.release_temp_xmm(tmp_in);
                self.machine.release_temp_gpr(tmp_out);
            }

            Operator::I64TruncF32U => {
                let loc = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);
                self.fp_stack.pop1()?;

                if self.assembler.arch_has_itruncf() {
                    let tmp_out = self.machine.acquire_temp_gpr().unwrap();
                    let tmp_in = self.machine.acquire_temp_xmm().unwrap();
                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S32,
                        loc,
                        Location::XMM(tmp_in),
                    );
                    self.assembler.arch_emit_i64_trunc_uf32(tmp_in, tmp_out);
                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S64,
                        Location::GPR(tmp_out),
                        ret,
                    );
                    self.machine.release_temp_xmm(tmp_in);
                    self.machine.release_temp_gpr(tmp_out);
                } else {
                    let tmp_out = self.machine.acquire_temp_gpr().unwrap();
                    let tmp_in = self.machine.acquire_temp_xmm().unwrap(); // xmm2

                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S32,
                        loc,
                        Location::XMM(tmp_in),
                    );
                    self.emit_f32_int_conv_check_trap(tmp_in, GEF32_LT_U64_MIN, LEF32_GT_U64_MAX);

                    let tmp = self.machine.acquire_temp_gpr().unwrap(); // r15
                    let tmp_x1 = self.machine.acquire_temp_xmm().unwrap(); // xmm1
                    let tmp_x2 = self.machine.acquire_temp_xmm().unwrap(); // xmm3

                    self.assembler.emit_mov(
                        Size::S32,
                        Location::Imm32(1593835520u32),
                        Location::GPR(tmp),
                    ); //float 9.22337203E+18
                    self.assembler
                        .emit_mov(Size::S32, Location::GPR(tmp), Location::XMM(tmp_x1));
                    self.assembler.emit_mov(
                        Size::S32,
                        Location::XMM(tmp_in),
                        Location::XMM(tmp_x2),
                    );
                    self.assembler
                        .emit_vsubss(tmp_in, XMMOrMemory::XMM(tmp_x1), tmp_in);
                    self.assembler
                        .emit_cvttss2si_64(XMMOrMemory::XMM(tmp_in), tmp_out);
                    self.assembler.emit_mov(
                        Size::S64,
                        Location::Imm64(0x8000000000000000u64),
                        Location::GPR(tmp),
                    );
                    self.assembler
                        .emit_xor(Size::S64, Location::GPR(tmp_out), Location::GPR(tmp));
                    self.assembler
                        .emit_cvttss2si_64(XMMOrMemory::XMM(tmp_x2), tmp_out);
                    self.assembler
                        .emit_ucomiss(XMMOrMemory::XMM(tmp_x1), tmp_x2);
                    self.assembler.emit_cmovae_gpr_64(tmp, tmp_out);
                    self.assembler
                        .emit_mov(Size::S64, Location::GPR(tmp_out), ret);

                    self.machine.release_temp_xmm(tmp_x2);
                    self.machine.release_temp_xmm(tmp_x1);
                    self.machine.release_temp_gpr(tmp);
                    self.machine.release_temp_xmm(tmp_in);
                    self.machine.release_temp_gpr(tmp_out);
                }
            }
            Operator::I64TruncSatF32U => {
                let loc = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);
                self.fp_stack.pop1()?;

                let tmp_out = self.machine.acquire_temp_gpr().unwrap();
                let tmp_in = self.machine.acquire_temp_xmm().unwrap();

                self.emit_relaxed_binop(Assembler::emit_mov, Size::S32, loc, Location::XMM(tmp_in));
                self.emit_f32_int_conv_check_sat(
                    tmp_in,
                    GEF32_LT_U64_MIN,
                    LEF32_GT_U64_MAX,
                    |this| {
                        this.assembler.emit_mov(
                            Size::S64,
                            Location::Imm64(0),
                            Location::GPR(tmp_out),
                        );
                    },
                    |this| {
                        this.assembler.emit_mov(
                            Size::S64,
                            Location::Imm64(std::u64::MAX),
                            Location::GPR(tmp_out),
                        );
                    },
                    None::<fn(this: &mut Self)>,
                    |this| {
                        if this.assembler.arch_has_itruncf() {
                            this.assembler.arch_emit_i64_trunc_uf32(tmp_in, tmp_out);
                        } else {
                            let tmp = this.machine.acquire_temp_gpr().unwrap();
                            let tmp_x1 = this.machine.acquire_temp_xmm().unwrap();
                            let tmp_x2 = this.machine.acquire_temp_xmm().unwrap();

                            this.assembler.emit_mov(
                                Size::S32,
                                Location::Imm32(1593835520u32),
                                Location::GPR(tmp),
                            ); //float 9.22337203E+18
                            this.assembler.emit_mov(
                                Size::S32,
                                Location::GPR(tmp),
                                Location::XMM(tmp_x1),
                            );
                            this.assembler.emit_mov(
                                Size::S32,
                                Location::XMM(tmp_in),
                                Location::XMM(tmp_x2),
                            );
                            this.assembler
                                .emit_vsubss(tmp_in, XMMOrMemory::XMM(tmp_x1), tmp_in);
                            this.assembler
                                .emit_cvttss2si_64(XMMOrMemory::XMM(tmp_in), tmp_out);
                            this.assembler.emit_mov(
                                Size::S64,
                                Location::Imm64(0x8000000000000000u64),
                                Location::GPR(tmp),
                            );
                            this.assembler.emit_xor(
                                Size::S64,
                                Location::GPR(tmp_out),
                                Location::GPR(tmp),
                            );
                            this.assembler
                                .emit_cvttss2si_64(XMMOrMemory::XMM(tmp_x2), tmp_out);
                            this.assembler
                                .emit_ucomiss(XMMOrMemory::XMM(tmp_x1), tmp_x2);
                            this.assembler.emit_cmovae_gpr_64(tmp, tmp_out);

                            this.machine.release_temp_xmm(tmp_x2);
                            this.machine.release_temp_xmm(tmp_x1);
                            this.machine.release_temp_gpr(tmp);
                        }
                    },
                );

                self.assembler
                    .emit_mov(Size::S64, Location::GPR(tmp_out), ret);
                self.machine.release_temp_xmm(tmp_in);
                self.machine.release_temp_gpr(tmp_out);
            }

            Operator::I32TruncF64U => {
                let loc = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];
                self.value_stack.push(ret);
                self.fp_stack.pop1()?;

                if self.assembler.arch_has_itruncf() {
                    let tmp_out = self.machine.acquire_temp_gpr().unwrap();
                    let tmp_in = self.machine.acquire_temp_xmm().unwrap();
                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S64,
                        loc,
                        Location::XMM(tmp_in),
                    );
                    self.assembler.arch_emit_i32_trunc_uf64(tmp_in, tmp_out);
                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S32,
                        Location::GPR(tmp_out),
                        ret,
                    );
                    self.machine.release_temp_xmm(tmp_in);
                    self.machine.release_temp_gpr(tmp_out);
                } else {
                    let tmp_out = self.machine.acquire_temp_gpr().unwrap();
                    let tmp_in = self.machine.acquire_temp_xmm().unwrap();

                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S64,
                        loc,
                        Location::XMM(tmp_in),
                    );
                    self.emit_f64_int_conv_check_trap(tmp_in, GEF64_LT_U32_MIN, LEF64_GT_U32_MAX);

                    self.assembler
                        .emit_cvttsd2si_64(XMMOrMemory::XMM(tmp_in), tmp_out);
                    self.assembler
                        .emit_mov(Size::S32, Location::GPR(tmp_out), ret);

                    self.machine.release_temp_xmm(tmp_in);
                    self.machine.release_temp_gpr(tmp_out);
                }
            }

            Operator::I32TruncSatF64U => {
                let loc = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];
                self.value_stack.push(ret);
                self.fp_stack.pop1()?;

                let tmp_out = self.machine.acquire_temp_gpr().unwrap();
                let tmp_in = self.machine.acquire_temp_xmm().unwrap();

                self.emit_relaxed_binop(Assembler::emit_mov, Size::S64, loc, Location::XMM(tmp_in));
                self.emit_f64_int_conv_check_sat(
                    tmp_in,
                    GEF64_LT_U32_MIN,
                    LEF64_GT_U32_MAX,
                    |this| {
                        this.assembler.emit_mov(
                            Size::S32,
                            Location::Imm32(0),
                            Location::GPR(tmp_out),
                        );
                    },
                    |this| {
                        this.assembler.emit_mov(
                            Size::S32,
                            Location::Imm32(std::u32::MAX),
                            Location::GPR(tmp_out),
                        );
                    },
                    None::<fn(this: &mut Self)>,
                    |this| {
                        if this.assembler.arch_has_itruncf() {
                            this.assembler.arch_emit_i32_trunc_uf64(tmp_in, tmp_out);
                        } else {
                            this.assembler
                                .emit_cvttsd2si_64(XMMOrMemory::XMM(tmp_in), tmp_out);
                        }
                    },
                );

                self.assembler
                    .emit_mov(Size::S32, Location::GPR(tmp_out), ret);
                self.machine.release_temp_xmm(tmp_in);
                self.machine.release_temp_gpr(tmp_out);
            }

            Operator::I32TruncF64S => {
                let loc = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];
                self.value_stack.push(ret);
                self.fp_stack.pop1()?;

                if self.assembler.arch_has_itruncf() {
                    let tmp_out = self.machine.acquire_temp_gpr().unwrap();
                    let tmp_in = self.machine.acquire_temp_xmm().unwrap();
                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S64,
                        loc,
                        Location::XMM(tmp_in),
                    );
                    self.assembler.arch_emit_i32_trunc_sf64(tmp_in, tmp_out);
                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S32,
                        Location::GPR(tmp_out),
                        ret,
                    );
                    self.machine.release_temp_xmm(tmp_in);
                    self.machine.release_temp_gpr(tmp_out);
                } else {
                    let tmp_out = self.machine.acquire_temp_gpr().unwrap();
                    let tmp_in = self.machine.acquire_temp_xmm().unwrap();

                    let real_in = match loc {
                        Location::Imm32(_) | Location::Imm64(_) => {
                            self.assembler
                                .emit_mov(Size::S64, loc, Location::GPR(tmp_out));
                            self.assembler.emit_mov(
                                Size::S64,
                                Location::GPR(tmp_out),
                                Location::XMM(tmp_in),
                            );
                            tmp_in
                        }
                        Location::XMM(x) => x,
                        _ => {
                            self.assembler
                                .emit_mov(Size::S64, loc, Location::XMM(tmp_in));
                            tmp_in
                        }
                    };

                    self.emit_f64_int_conv_check_trap(real_in, GEF64_LT_I32_MIN, LEF64_GT_I32_MAX);

                    self.assembler
                        .emit_cvttsd2si_32(XMMOrMemory::XMM(real_in), tmp_out);
                    self.assembler
                        .emit_mov(Size::S32, Location::GPR(tmp_out), ret);

                    self.machine.release_temp_xmm(tmp_in);
                    self.machine.release_temp_gpr(tmp_out);
                }
            }

            Operator::I32TruncSatF64S => {
                let loc = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];
                self.value_stack.push(ret);
                self.fp_stack.pop1()?;

                let tmp_out = self.machine.acquire_temp_gpr().unwrap();
                let tmp_in = self.machine.acquire_temp_xmm().unwrap();

                let real_in = match loc {
                    Location::Imm32(_) | Location::Imm64(_) => {
                        self.assembler
                            .emit_mov(Size::S64, loc, Location::GPR(tmp_out));
                        self.assembler.emit_mov(
                            Size::S64,
                            Location::GPR(tmp_out),
                            Location::XMM(tmp_in),
                        );
                        tmp_in
                    }
                    Location::XMM(x) => x,
                    _ => {
                        self.assembler
                            .emit_mov(Size::S64, loc, Location::XMM(tmp_in));
                        tmp_in
                    }
                };

                self.emit_f64_int_conv_check_sat(
                    real_in,
                    GEF64_LT_I32_MIN,
                    LEF64_GT_I32_MAX,
                    |this| {
                        this.assembler.emit_mov(
                            Size::S32,
                            Location::Imm32(std::i32::MIN as u32),
                            Location::GPR(tmp_out),
                        );
                    },
                    |this| {
                        this.assembler.emit_mov(
                            Size::S32,
                            Location::Imm32(std::i32::MAX as u32),
                            Location::GPR(tmp_out),
                        );
                    },
                    Some(|this: &mut Self| {
                        this.assembler.emit_mov(
                            Size::S32,
                            Location::Imm32(0),
                            Location::GPR(tmp_out),
                        );
                    }),
                    |this| {
                        if this.assembler.arch_has_itruncf() {
                            this.assembler.arch_emit_i32_trunc_sf64(tmp_in, tmp_out);
                        } else {
                            this.assembler
                                .emit_cvttsd2si_32(XMMOrMemory::XMM(real_in), tmp_out);
                        }
                    },
                );

                self.assembler
                    .emit_mov(Size::S32, Location::GPR(tmp_out), ret);
                self.machine.release_temp_xmm(tmp_in);
                self.machine.release_temp_gpr(tmp_out);
            }

            Operator::I64TruncF64S => {
                let loc = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);
                self.fp_stack.pop1()?;

                if self.assembler.arch_has_itruncf() {
                    let tmp_out = self.machine.acquire_temp_gpr().unwrap();
                    let tmp_in = self.machine.acquire_temp_xmm().unwrap();
                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S64,
                        loc,
                        Location::XMM(tmp_in),
                    );
                    self.assembler.arch_emit_i64_trunc_sf64(tmp_in, tmp_out);
                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S64,
                        Location::GPR(tmp_out),
                        ret,
                    );
                    self.machine.release_temp_xmm(tmp_in);
                    self.machine.release_temp_gpr(tmp_out);
                } else {
                    let tmp_out = self.machine.acquire_temp_gpr().unwrap();
                    let tmp_in = self.machine.acquire_temp_xmm().unwrap();

                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S64,
                        loc,
                        Location::XMM(tmp_in),
                    );
                    self.emit_f64_int_conv_check_trap(tmp_in, GEF64_LT_I64_MIN, LEF64_GT_I64_MAX);

                    self.assembler
                        .emit_cvttsd2si_64(XMMOrMemory::XMM(tmp_in), tmp_out);
                    self.assembler
                        .emit_mov(Size::S64, Location::GPR(tmp_out), ret);

                    self.machine.release_temp_xmm(tmp_in);
                    self.machine.release_temp_gpr(tmp_out);
                }
            }

            Operator::I64TruncSatF64S => {
                let loc = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);
                self.fp_stack.pop1()?;

                let tmp_out = self.machine.acquire_temp_gpr().unwrap();
                let tmp_in = self.machine.acquire_temp_xmm().unwrap();

                self.emit_relaxed_binop(Assembler::emit_mov, Size::S64, loc, Location::XMM(tmp_in));
                self.emit_f64_int_conv_check_sat(
                    tmp_in,
                    GEF64_LT_I64_MIN,
                    LEF64_GT_I64_MAX,
                    |this| {
                        this.assembler.emit_mov(
                            Size::S64,
                            Location::Imm64(std::i64::MIN as u64),
                            Location::GPR(tmp_out),
                        );
                    },
                    |this| {
                        this.assembler.emit_mov(
                            Size::S64,
                            Location::Imm64(std::i64::MAX as u64),
                            Location::GPR(tmp_out),
                        );
                    },
                    Some(|this: &mut Self| {
                        this.assembler.emit_mov(
                            Size::S64,
                            Location::Imm64(0),
                            Location::GPR(tmp_out),
                        );
                    }),
                    |this| {
                        if this.assembler.arch_has_itruncf() {
                            this.assembler.arch_emit_i64_trunc_sf64(tmp_in, tmp_out);
                        } else {
                            this.assembler
                                .emit_cvttsd2si_64(XMMOrMemory::XMM(tmp_in), tmp_out);
                        }
                    },
                );

                self.assembler
                    .emit_mov(Size::S64, Location::GPR(tmp_out), ret);
                self.machine.release_temp_xmm(tmp_in);
                self.machine.release_temp_gpr(tmp_out);
            }

            Operator::I64TruncF64U => {
                let loc = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);
                self.fp_stack.pop1()?;

                if self.assembler.arch_has_itruncf() {
                    let tmp_out = self.machine.acquire_temp_gpr().unwrap();
                    let tmp_in = self.machine.acquire_temp_xmm().unwrap();
                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S64,
                        loc,
                        Location::XMM(tmp_in),
                    );
                    self.assembler.arch_emit_i64_trunc_uf64(tmp_in, tmp_out);
                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S64,
                        Location::GPR(tmp_out),
                        ret,
                    );
                    self.machine.release_temp_xmm(tmp_in);
                    self.machine.release_temp_gpr(tmp_out);
                } else {
                    let tmp_out = self.machine.acquire_temp_gpr().unwrap();
                    let tmp_in = self.machine.acquire_temp_xmm().unwrap(); // xmm2

                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S64,
                        loc,
                        Location::XMM(tmp_in),
                    );
                    self.emit_f64_int_conv_check_trap(tmp_in, GEF64_LT_U64_MIN, LEF64_GT_U64_MAX);

                    let tmp = self.machine.acquire_temp_gpr().unwrap(); // r15
                    let tmp_x1 = self.machine.acquire_temp_xmm().unwrap(); // xmm1
                    let tmp_x2 = self.machine.acquire_temp_xmm().unwrap(); // xmm3

                    self.assembler.emit_mov(
                        Size::S64,
                        Location::Imm64(4890909195324358656u64),
                        Location::GPR(tmp),
                    ); //double 9.2233720368547758E+18
                    self.assembler
                        .emit_mov(Size::S64, Location::GPR(tmp), Location::XMM(tmp_x1));
                    self.assembler.emit_mov(
                        Size::S64,
                        Location::XMM(tmp_in),
                        Location::XMM(tmp_x2),
                    );
                    self.assembler
                        .emit_vsubsd(tmp_in, XMMOrMemory::XMM(tmp_x1), tmp_in);
                    self.assembler
                        .emit_cvttsd2si_64(XMMOrMemory::XMM(tmp_in), tmp_out);
                    self.assembler.emit_mov(
                        Size::S64,
                        Location::Imm64(0x8000000000000000u64),
                        Location::GPR(tmp),
                    );
                    self.assembler
                        .emit_xor(Size::S64, Location::GPR(tmp_out), Location::GPR(tmp));
                    self.assembler
                        .emit_cvttsd2si_64(XMMOrMemory::XMM(tmp_x2), tmp_out);
                    self.assembler
                        .emit_ucomisd(XMMOrMemory::XMM(tmp_x1), tmp_x2);
                    self.assembler.emit_cmovae_gpr_64(tmp, tmp_out);
                    self.assembler
                        .emit_mov(Size::S64, Location::GPR(tmp_out), ret);

                    self.machine.release_temp_xmm(tmp_x2);
                    self.machine.release_temp_xmm(tmp_x1);
                    self.machine.release_temp_gpr(tmp);
                    self.machine.release_temp_xmm(tmp_in);
                    self.machine.release_temp_gpr(tmp_out);
                }
            }

            Operator::I64TruncSatF64U => {
                let loc = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);
                self.fp_stack.pop1()?;

                let tmp_out = self.machine.acquire_temp_gpr().unwrap();
                let tmp_in = self.machine.acquire_temp_xmm().unwrap();

                self.emit_relaxed_binop(Assembler::emit_mov, Size::S64, loc, Location::XMM(tmp_in));
                self.emit_f64_int_conv_check_sat(
                    tmp_in,
                    GEF64_LT_U64_MIN,
                    LEF64_GT_U64_MAX,
                    |this| {
                        this.assembler.emit_mov(
                            Size::S64,
                            Location::Imm64(0),
                            Location::GPR(tmp_out),
                        );
                    },
                    |this| {
                        this.assembler.emit_mov(
                            Size::S64,
                            Location::Imm64(std::u64::MAX),
                            Location::GPR(tmp_out),
                        );
                    },
                    None::<fn(this: &mut Self)>,
                    |this| {
                        if this.assembler.arch_has_itruncf() {
                            this.assembler.arch_emit_i64_trunc_uf64(tmp_in, tmp_out);
                        } else {
                            let tmp = this.machine.acquire_temp_gpr().unwrap();
                            let tmp_x1 = this.machine.acquire_temp_xmm().unwrap();
                            let tmp_x2 = this.machine.acquire_temp_xmm().unwrap();

                            this.assembler.emit_mov(
                                Size::S64,
                                Location::Imm64(4890909195324358656u64),
                                Location::GPR(tmp),
                            ); //double 9.2233720368547758E+18
                            this.assembler.emit_mov(
                                Size::S64,
                                Location::GPR(tmp),
                                Location::XMM(tmp_x1),
                            );
                            this.assembler.emit_mov(
                                Size::S64,
                                Location::XMM(tmp_in),
                                Location::XMM(tmp_x2),
                            );
                            this.assembler
                                .emit_vsubsd(tmp_in, XMMOrMemory::XMM(tmp_x1), tmp_in);
                            this.assembler
                                .emit_cvttsd2si_64(XMMOrMemory::XMM(tmp_in), tmp_out);
                            this.assembler.emit_mov(
                                Size::S64,
                                Location::Imm64(0x8000000000000000u64),
                                Location::GPR(tmp),
                            );
                            this.assembler.emit_xor(
                                Size::S64,
                                Location::GPR(tmp_out),
                                Location::GPR(tmp),
                            );
                            this.assembler
                                .emit_cvttsd2si_64(XMMOrMemory::XMM(tmp_x2), tmp_out);
                            this.assembler
                                .emit_ucomisd(XMMOrMemory::XMM(tmp_x1), tmp_x2);
                            this.assembler.emit_cmovae_gpr_64(tmp, tmp_out);

                            this.machine.release_temp_xmm(tmp_x2);
                            this.machine.release_temp_xmm(tmp_x1);
                            this.machine.release_temp_gpr(tmp);
                        }
                    },
                );

                self.assembler
                    .emit_mov(Size::S64, Location::GPR(tmp_out), ret);
                self.machine.release_temp_xmm(tmp_in);
                self.machine.release_temp_gpr(tmp_out);
            }

            Operator::F32ConvertI32S => {
                let loc = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::F32)], false)[0];
                self.value_stack.push(ret);
                self.fp_stack
                    .push(FloatValue::new(self.value_stack.len() - 1)); // Converting i32 to f32 never results in NaN.

                if self.assembler.arch_has_fconverti() {
                    let tmp_out = self.machine.acquire_temp_xmm().unwrap();
                    let tmp_in = self.machine.acquire_temp_gpr().unwrap();
                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S32,
                        loc,
                        Location::GPR(tmp_in),
                    );
                    self.assembler.arch_emit_f32_convert_si32(tmp_in, tmp_out);
                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S32,
                        Location::XMM(tmp_out),
                        ret,
                    );
                    self.machine.release_temp_gpr(tmp_in);
                    self.machine.release_temp_xmm(tmp_out);
                } else {
                    let tmp_out = self.machine.acquire_temp_xmm().unwrap();
                    let tmp_in = self.machine.acquire_temp_gpr().unwrap();

                    self.assembler
                        .emit_mov(Size::S32, loc, Location::GPR(tmp_in));
                    self.assembler
                        .emit_vcvtsi2ss_32(tmp_out, GPROrMemory::GPR(tmp_in), tmp_out);
                    self.assembler
                        .emit_mov(Size::S32, Location::XMM(tmp_out), ret);

                    self.machine.release_temp_gpr(tmp_in);
                    self.machine.release_temp_xmm(tmp_out);
                }
            }
            Operator::F32ConvertI32U => {
                let loc = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::F32)], false)[0];
                self.value_stack.push(ret);
                self.fp_stack
                    .push(FloatValue::new(self.value_stack.len() - 1)); // Converting i32 to f32 never results in NaN.

                if self.assembler.arch_has_fconverti() {
                    let tmp_out = self.machine.acquire_temp_xmm().unwrap();
                    let tmp_in = self.machine.acquire_temp_gpr().unwrap();
                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S32,
                        loc,
                        Location::GPR(tmp_in),
                    );
                    self.assembler.arch_emit_f32_convert_ui32(tmp_in, tmp_out);
                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S32,
                        Location::XMM(tmp_out),
                        ret,
                    );
                    self.machine.release_temp_gpr(tmp_in);
                    self.machine.release_temp_xmm(tmp_out);
                } else {
                    let tmp_out = self.machine.acquire_temp_xmm().unwrap();
                    let tmp_in = self.machine.acquire_temp_gpr().unwrap();

                    self.assembler
                        .emit_mov(Size::S32, loc, Location::GPR(tmp_in));
                    self.assembler
                        .emit_vcvtsi2ss_64(tmp_out, GPROrMemory::GPR(tmp_in), tmp_out);
                    self.assembler
                        .emit_mov(Size::S32, Location::XMM(tmp_out), ret);

                    self.machine.release_temp_gpr(tmp_in);
                    self.machine.release_temp_xmm(tmp_out);
                }
            }
            Operator::F32ConvertI64S => {
                let loc = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::F32)], false)[0];
                self.value_stack.push(ret);
                self.fp_stack
                    .push(FloatValue::new(self.value_stack.len() - 1)); // Converting i64 to f32 never results in NaN.

                if self.assembler.arch_has_fconverti() {
                    let tmp_out = self.machine.acquire_temp_xmm().unwrap();
                    let tmp_in = self.machine.acquire_temp_gpr().unwrap();
                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S64,
                        loc,
                        Location::GPR(tmp_in),
                    );
                    self.assembler.arch_emit_f32_convert_si64(tmp_in, tmp_out);
                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S32,
                        Location::XMM(tmp_out),
                        ret,
                    );
                    self.machine.release_temp_gpr(tmp_in);
                    self.machine.release_temp_xmm(tmp_out);
                } else {
                    let tmp_out = self.machine.acquire_temp_xmm().unwrap();
                    let tmp_in = self.machine.acquire_temp_gpr().unwrap();

                    self.assembler
                        .emit_mov(Size::S64, loc, Location::GPR(tmp_in));
                    self.assembler
                        .emit_vcvtsi2ss_64(tmp_out, GPROrMemory::GPR(tmp_in), tmp_out);
                    self.assembler
                        .emit_mov(Size::S32, Location::XMM(tmp_out), ret);

                    self.machine.release_temp_gpr(tmp_in);
                    self.machine.release_temp_xmm(tmp_out);
                }
            }
            Operator::F32ConvertI64U => {
                let loc = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::F32)], false)[0];
                self.value_stack.push(ret);
                self.fp_stack
                    .push(FloatValue::new(self.value_stack.len() - 1)); // Converting i64 to f32 never results in NaN.

                if self.assembler.arch_has_fconverti() {
                    let tmp_out = self.machine.acquire_temp_xmm().unwrap();
                    let tmp_in = self.machine.acquire_temp_gpr().unwrap();
                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S64,
                        loc,
                        Location::GPR(tmp_in),
                    );
                    self.assembler.arch_emit_f32_convert_ui64(tmp_in, tmp_out);
                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S32,
                        Location::XMM(tmp_out),
                        ret,
                    );
                    self.machine.release_temp_gpr(tmp_in);
                    self.machine.release_temp_xmm(tmp_out);
                } else {
                    let tmp_out = self.machine.acquire_temp_xmm().unwrap();
                    let tmp_in = self.machine.acquire_temp_gpr().unwrap();
                    let tmp = self.machine.acquire_temp_gpr().unwrap();

                    let do_convert = self.assembler.get_label();
                    let end_convert = self.assembler.get_label();

                    self.assembler
                        .emit_mov(Size::S64, loc, Location::GPR(tmp_in));
                    self.assembler.emit_test_gpr_64(tmp_in);
                    self.assembler.emit_jmp(Condition::Signed, do_convert);
                    self.assembler
                        .emit_vcvtsi2ss_64(tmp_out, GPROrMemory::GPR(tmp_in), tmp_out);
                    self.assembler.emit_jmp(Condition::None, end_convert);
                    self.assembler.emit_label(do_convert);
                    self.assembler
                        .emit_mov(Size::S64, Location::GPR(tmp_in), Location::GPR(tmp));
                    self.assembler
                        .emit_and(Size::S64, Location::Imm32(1), Location::GPR(tmp));
                    self.assembler
                        .emit_shr(Size::S64, Location::Imm8(1), Location::GPR(tmp_in));
                    self.assembler
                        .emit_or(Size::S64, Location::GPR(tmp), Location::GPR(tmp_in));
                    self.assembler
                        .emit_vcvtsi2ss_64(tmp_out, GPROrMemory::GPR(tmp_in), tmp_out);
                    self.assembler
                        .emit_vaddss(tmp_out, XMMOrMemory::XMM(tmp_out), tmp_out);
                    self.assembler.emit_label(end_convert);
                    self.assembler
                        .emit_mov(Size::S32, Location::XMM(tmp_out), ret);

                    self.machine.release_temp_gpr(tmp);
                    self.machine.release_temp_gpr(tmp_in);
                    self.machine.release_temp_xmm(tmp_out);
                }
            }

            Operator::F64ConvertI32S => {
                let loc = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::F64)], false)[0];
                self.value_stack.push(ret);
                self.fp_stack
                    .push(FloatValue::new(self.value_stack.len() - 1)); // Converting i32 to f64 never results in NaN.

                if self.assembler.arch_has_fconverti() {
                    let tmp_out = self.machine.acquire_temp_xmm().unwrap();
                    let tmp_in = self.machine.acquire_temp_gpr().unwrap();
                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S32,
                        loc,
                        Location::GPR(tmp_in),
                    );
                    self.assembler.arch_emit_f64_convert_si32(tmp_in, tmp_out);
                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S64,
                        Location::XMM(tmp_out),
                        ret,
                    );
                    self.machine.release_temp_gpr(tmp_in);
                    self.machine.release_temp_xmm(tmp_out);
                } else {
                    let tmp_out = self.machine.acquire_temp_xmm().unwrap();
                    let tmp_in = self.machine.acquire_temp_gpr().unwrap();

                    self.assembler
                        .emit_mov(Size::S32, loc, Location::GPR(tmp_in));
                    self.assembler
                        .emit_vcvtsi2sd_32(tmp_out, GPROrMemory::GPR(tmp_in), tmp_out);
                    self.assembler
                        .emit_mov(Size::S64, Location::XMM(tmp_out), ret);

                    self.machine.release_temp_gpr(tmp_in);
                    self.machine.release_temp_xmm(tmp_out);
                }
            }
            Operator::F64ConvertI32U => {
                let loc = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::F64)], false)[0];
                self.value_stack.push(ret);
                self.fp_stack
                    .push(FloatValue::new(self.value_stack.len() - 1)); // Converting i32 to f64 never results in NaN.

                if self.assembler.arch_has_fconverti() {
                    let tmp_out = self.machine.acquire_temp_xmm().unwrap();
                    let tmp_in = self.machine.acquire_temp_gpr().unwrap();
                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S32,
                        loc,
                        Location::GPR(tmp_in),
                    );
                    self.assembler.arch_emit_f64_convert_ui32(tmp_in, tmp_out);
                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S64,
                        Location::XMM(tmp_out),
                        ret,
                    );
                    self.machine.release_temp_gpr(tmp_in);
                    self.machine.release_temp_xmm(tmp_out);
                } else {
                    let tmp_out = self.machine.acquire_temp_xmm().unwrap();
                    let tmp_in = self.machine.acquire_temp_gpr().unwrap();

                    self.assembler
                        .emit_mov(Size::S32, loc, Location::GPR(tmp_in));
                    self.assembler
                        .emit_vcvtsi2sd_64(tmp_out, GPROrMemory::GPR(tmp_in), tmp_out);
                    self.assembler
                        .emit_mov(Size::S64, Location::XMM(tmp_out), ret);

                    self.machine.release_temp_gpr(tmp_in);
                    self.machine.release_temp_xmm(tmp_out);
                }
            }
            Operator::F64ConvertI64S => {
                let loc = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::F64)], false)[0];
                self.value_stack.push(ret);
                self.fp_stack
                    .push(FloatValue::new(self.value_stack.len() - 1)); // Converting i64 to f64 never results in NaN.

                if self.assembler.arch_has_fconverti() {
                    let tmp_out = self.machine.acquire_temp_xmm().unwrap();
                    let tmp_in = self.machine.acquire_temp_gpr().unwrap();
                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S64,
                        loc,
                        Location::GPR(tmp_in),
                    );
                    self.assembler.arch_emit_f64_convert_si64(tmp_in, tmp_out);
                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S64,
                        Location::XMM(tmp_out),
                        ret,
                    );
                    self.machine.release_temp_gpr(tmp_in);
                    self.machine.release_temp_xmm(tmp_out);
                } else {
                    let tmp_out = self.machine.acquire_temp_xmm().unwrap();
                    let tmp_in = self.machine.acquire_temp_gpr().unwrap();

                    self.assembler
                        .emit_mov(Size::S64, loc, Location::GPR(tmp_in));
                    self.assembler
                        .emit_vcvtsi2sd_64(tmp_out, GPROrMemory::GPR(tmp_in), tmp_out);
                    self.assembler
                        .emit_mov(Size::S64, Location::XMM(tmp_out), ret);

                    self.machine.release_temp_gpr(tmp_in);
                    self.machine.release_temp_xmm(tmp_out);
                }
            }
            Operator::F64ConvertI64U => {
                let loc = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::F64)], false)[0];
                self.value_stack.push(ret);
                self.fp_stack
                    .push(FloatValue::new(self.value_stack.len() - 1)); // Converting i64 to f64 never results in NaN.

                if self.assembler.arch_has_fconverti() {
                    let tmp_out = self.machine.acquire_temp_xmm().unwrap();
                    let tmp_in = self.machine.acquire_temp_gpr().unwrap();
                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S64,
                        loc,
                        Location::GPR(tmp_in),
                    );
                    self.assembler.arch_emit_f64_convert_ui64(tmp_in, tmp_out);
                    self.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S64,
                        Location::XMM(tmp_out),
                        ret,
                    );
                    self.machine.release_temp_gpr(tmp_in);
                    self.machine.release_temp_xmm(tmp_out);
                } else {
                    let tmp_out = self.machine.acquire_temp_xmm().unwrap();
                    let tmp_in = self.machine.acquire_temp_gpr().unwrap();
                    let tmp = self.machine.acquire_temp_gpr().unwrap();

                    let do_convert = self.assembler.get_label();
                    let end_convert = self.assembler.get_label();

                    self.assembler
                        .emit_mov(Size::S64, loc, Location::GPR(tmp_in));
                    self.assembler.emit_test_gpr_64(tmp_in);
                    self.assembler.emit_jmp(Condition::Signed, do_convert);
                    self.assembler
                        .emit_vcvtsi2sd_64(tmp_out, GPROrMemory::GPR(tmp_in), tmp_out);
                    self.assembler.emit_jmp(Condition::None, end_convert);
                    self.assembler.emit_label(do_convert);
                    self.assembler
                        .emit_mov(Size::S64, Location::GPR(tmp_in), Location::GPR(tmp));
                    self.assembler
                        .emit_and(Size::S64, Location::Imm32(1), Location::GPR(tmp));
                    self.assembler
                        .emit_shr(Size::S64, Location::Imm8(1), Location::GPR(tmp_in));
                    self.assembler
                        .emit_or(Size::S64, Location::GPR(tmp), Location::GPR(tmp_in));
                    self.assembler
                        .emit_vcvtsi2sd_64(tmp_out, GPROrMemory::GPR(tmp_in), tmp_out);
                    self.assembler
                        .emit_vaddsd(tmp_out, XMMOrMemory::XMM(tmp_out), tmp_out);
                    self.assembler.emit_label(end_convert);
                    self.assembler
                        .emit_mov(Size::S64, Location::XMM(tmp_out), ret);

                    self.machine.release_temp_gpr(tmp);
                    self.machine.release_temp_gpr(tmp_in);
                    self.machine.release_temp_xmm(tmp_out);
                }
            }

            Operator::Call { function_index } => self.emit_call(function_index)?,
            Operator::CallIndirect { index, table_index } => {
                // TODO: removed restriction on always being table idx 0;
                // does any code depend on this?
                let table_index = TableIndex::new(table_index as _);
                let index = SignatureIndex::new(index as usize);
                let sig = self.module.signatures.get(index).unwrap();
                let param_types: SmallVec<[WpType; 8]> =
                    sig.params().iter().cloned().map(type_to_wp_type).collect();
                let return_types: SmallVec<[WpType; 1]> =
                    sig.results().iter().cloned().map(type_to_wp_type).collect();

                let func_index = self.pop_value_released();

                let params: SmallVec<[_; 8]> = self
                    .value_stack
                    .drain(self.value_stack.len() - param_types.len()..)
                    .collect();
                self.machine.release_locations_only_regs(&params);

                // Pop arguments off the FP stack and canonicalize them if needed.
                //
                // Canonicalization state will be lost across function calls, so early canonicalization
                // is necessary here.
                while let Some(fp) = self.fp_stack.last() {
                    if fp.depth >= self.value_stack.len() {
                        let index = fp.depth - self.value_stack.len();
                        if self.assembler.arch_supports_canonicalize_nan()
                            && self.config.enable_nan_canonicalization
                            && fp.canonicalization.is_some()
                        {
                            let size = fp.canonicalization.unwrap().to_size();
                            self.canonicalize_nan(size, params[index], params[index]);
                        }
                        self.fp_stack.pop().unwrap();
                    } else {
                        break;
                    }
                }

                let table_base = self.machine.acquire_temp_gpr().unwrap();
                let table_count = self.machine.acquire_temp_gpr().unwrap();
                let sigidx = self.machine.acquire_temp_gpr().unwrap();

                if let Some(local_table_index) = self.module.local_table_index(table_index) {
                    let (vmctx_offset_base, vmctx_offset_len) = (
                        self.vmoffsets.vmctx_vmtable_definition(local_table_index),
                        self.vmoffsets
                            .vmctx_vmtable_definition_current_elements(local_table_index),
                    );
                    self.assembler.emit_mov(
                        Size::S64,
                        Location::Memory(Machine::get_vmctx_reg(), vmctx_offset_base as i32),
                        Location::GPR(table_base),
                    );
                    self.assembler.emit_mov(
                        Size::S32,
                        Location::Memory(Machine::get_vmctx_reg(), vmctx_offset_len as i32),
                        Location::GPR(table_count),
                    );
                } else {
                    // Do an indirection.
                    let import_offset = self.vmoffsets.vmctx_vmtable_import(table_index);
                    self.assembler.emit_mov(
                        Size::S64,
                        Location::Memory(Machine::get_vmctx_reg(), import_offset as i32),
                        Location::GPR(table_base),
                    );

                    // Load len.
                    self.assembler.emit_mov(
                        Size::S32,
                        Location::Memory(
                            table_base,
                            self.vmoffsets.vmtable_definition_current_elements() as _,
                        ),
                        Location::GPR(table_count),
                    );

                    // Load base.
                    self.assembler.emit_mov(
                        Size::S64,
                        Location::Memory(table_base, self.vmoffsets.vmtable_definition_base() as _),
                        Location::GPR(table_base),
                    );
                }

                self.assembler
                    .emit_cmp(Size::S32, func_index, Location::GPR(table_count));
                self.assembler
                    .emit_jmp(Condition::BelowEqual, self.special_labels.table_access_oob);
                self.assembler
                    .emit_mov(Size::S32, func_index, Location::GPR(table_count));
                self.assembler
                    .emit_imul_imm32_gpr64(self.vmoffsets.size_of_vm_funcref() as u32, table_count);
                self.assembler.emit_add(
                    Size::S64,
                    Location::GPR(table_base),
                    Location::GPR(table_count),
                );

                // deref the table to get a VMFuncRef
                self.assembler.emit_mov(
                    Size::S64,
                    Location::Memory(table_count, self.vmoffsets.vm_funcref_anyfunc_ptr() as i32),
                    Location::GPR(table_count),
                );
                // Trap if the FuncRef is null
                self.assembler
                    .emit_cmp(Size::S64, Location::Imm32(0), Location::GPR(table_count));
                self.assembler
                    .emit_jmp(Condition::Equal, self.special_labels.indirect_call_null);
                self.assembler.emit_mov(
                    Size::S64,
                    Location::Memory(
                        Machine::get_vmctx_reg(),
                        self.vmoffsets.vmctx_vmshared_signature_id(index) as i32,
                    ),
                    Location::GPR(sigidx),
                );

                // Trap if signature mismatches.
                self.assembler.emit_cmp(
                    Size::S32,
                    Location::GPR(sigidx),
                    Location::Memory(
                        table_count,
                        (self.vmoffsets.vmcaller_checked_anyfunc_type_index() as usize) as i32,
                    ),
                );
                self.assembler
                    .emit_jmp(Condition::NotEqual, self.special_labels.bad_signature);

                self.machine.release_temp_gpr(sigidx);
                self.machine.release_temp_gpr(table_count);
                self.machine.release_temp_gpr(table_base);

                if table_count != GPR::RAX {
                    self.assembler.emit_mov(
                        Size::S64,
                        Location::GPR(table_count),
                        Location::GPR(GPR::RAX),
                    );
                }

                let vmcaller_checked_anyfunc_func_ptr =
                    self.vmoffsets.vmcaller_checked_anyfunc_func_ptr() as usize;
                let vmcaller_checked_anyfunc_vmctx =
                    self.vmoffsets.vmcaller_checked_anyfunc_vmctx() as usize;
                let calling_convention = self.calling_convention;

                self.emit_call_native(
                    |this| {
                        if this.assembler.arch_requires_indirect_call_trampoline() {
                            this.assembler.arch_emit_indirect_call_with_trampoline(
                                Location::Memory(
                                    GPR::RAX,
                                    vmcaller_checked_anyfunc_func_ptr as i32,
                                ),
                            );
                        } else {
                            // We set the context pointer
                            this.assembler.emit_mov(
                                Size::S64,
                                Location::Memory(GPR::RAX, vmcaller_checked_anyfunc_vmctx as i32),
                                Machine::get_param_location(0, calling_convention),
                            );

                            this.assembler.emit_call_location(Location::Memory(
                                GPR::RAX,
                                vmcaller_checked_anyfunc_func_ptr as i32,
                            ));
                        }
                    },
                    params.iter().copied(),
                )?;

                self.machine
                    .release_locations_only_stack(&mut self.assembler, &params);

                if !return_types.is_empty() {
                    let ret = self.machine.acquire_locations(
                        &mut self.assembler,
                        &[return_types[0]],
                        false,
                    )[0];
                    self.value_stack.push(ret);
                    if return_types[0].is_float() {
                        self.assembler
                            .emit_mov(Size::S64, Location::XMM(XMM::XMM0), ret);
                        self.fp_stack
                            .push(FloatValue::new(self.value_stack.len() - 1));
                    } else {
                        self.assembler
                            .emit_mov(Size::S64, Location::GPR(GPR::RAX), ret);
                    }
                }
            }
            Operator::If { ty } => {
                let label_end = self.assembler.get_label();
                let label_else = self.assembler.get_label();

                let cond = self.pop_value_released();

                let frame = ControlFrame {
                    label: label_end,
                    loop_like: false,
                    if_else: IfElseState::If(label_else),
                    returns: match ty {
                        WpTypeOrFuncType::Type(WpType::EmptyBlockType) => smallvec![],
                        WpTypeOrFuncType::Type(inner_ty) => smallvec![inner_ty],
                        _ => {
                            return Err(CodegenError {
                                message: "If: multi-value returns not yet implemented".to_string(),
                            })
                        }
                    },
                    value_stack_depth: self.value_stack.len(),
                    fp_stack_depth: self.fp_stack.len(),
                };
                self.control_stack.push(frame);
                self.emit_relaxed_binop(Assembler::emit_cmp, Size::S32, Location::Imm32(0), cond);
                self.assembler.emit_jmp(Condition::Equal, label_else);
            }
            Operator::Else => {
                let frame = self.control_stack.last_mut().unwrap();

                if !was_unreachable && !frame.returns.is_empty() {
                    let first_return = frame.returns[0];
                    let loc = *self.value_stack.last().unwrap();
                    if first_return.is_float() {
                        let fp = self.fp_stack.peek1()?;
                        if self.assembler.arch_supports_canonicalize_nan()
                            && self.config.enable_nan_canonicalization
                            && fp.canonicalization.is_some()
                        {
                            self.canonicalize_nan(
                                match first_return {
                                    WpType::F32 => Size::S32,
                                    WpType::F64 => Size::S64,
                                    _ => unreachable!(),
                                },
                                loc,
                                Location::GPR(GPR::RAX),
                            );
                        } else {
                            self.emit_relaxed_binop(
                                Assembler::emit_mov,
                                Size::S64,
                                loc,
                                Location::GPR(GPR::RAX),
                            );
                        }
                    } else {
                        self.emit_relaxed_binop(
                            Assembler::emit_mov,
                            Size::S64,
                            loc,
                            Location::GPR(GPR::RAX),
                        );
                    }
                }

                self.update_max_stack_depth();

                let mut frame = self.control_stack.last_mut().unwrap();

                let released: &[Location] = &self.value_stack[frame.value_stack_depth..];
                self.machine
                    .release_locations(&mut self.assembler, released);
                self.value_stack.truncate(frame.value_stack_depth);
                self.fp_stack.truncate(frame.fp_stack_depth);

                match frame.if_else {
                    IfElseState::If(label) => {
                        self.assembler.emit_jmp(Condition::None, frame.label);
                        self.assembler.emit_label(label);
                        frame.if_else = IfElseState::Else;
                    }
                    _ => {
                        return Err(CodegenError {
                            message: "Else: frame.if_else unreachable code".to_string(),
                        })
                    }
                }
            }
            // `TypedSelect` must be used for extern refs so ref counting should
            // be done with TypedSelect. But otherwise they're the same.
            Operator::TypedSelect { .. } | Operator::Select => {
                let cond = self.pop_value_released();
                let v_b = self.pop_value_released();
                let v_a = self.pop_value_released();
                let cncl: Option<(Option<CanonicalizeType>, Option<CanonicalizeType>)> =
                    if self.fp_stack.len() >= 2
                        && self.fp_stack[self.fp_stack.len() - 2].depth == self.value_stack.len()
                        && self.fp_stack[self.fp_stack.len() - 1].depth
                            == self.value_stack.len() + 1
                    {
                        let (left, right) = self.fp_stack.pop2()?;
                        self.fp_stack.push(FloatValue::new(self.value_stack.len()));
                        Some((left.canonicalization, right.canonicalization))
                    } else {
                        None
                    };
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);

                let end_label = self.assembler.get_label();
                let zero_label = self.assembler.get_label();

                self.emit_relaxed_binop(Assembler::emit_cmp, Size::S32, Location::Imm32(0), cond);
                self.assembler.emit_jmp(Condition::Equal, zero_label);
                match cncl {
                    Some((Some(fp), _))
                        if self.assembler.arch_supports_canonicalize_nan()
                            && self.config.enable_nan_canonicalization =>
                    {
                        self.canonicalize_nan(fp.to_size(), v_a, ret);
                    }
                    _ => {
                        if v_a != ret {
                            self.emit_relaxed_binop(Assembler::emit_mov, Size::S64, v_a, ret);
                        }
                    }
                }
                self.assembler.emit_jmp(Condition::None, end_label);
                self.assembler.emit_label(zero_label);
                match cncl {
                    Some((_, Some(fp)))
                        if self.assembler.arch_supports_canonicalize_nan()
                            && self.config.enable_nan_canonicalization =>
                    {
                        self.canonicalize_nan(fp.to_size(), v_b, ret);
                    }
                    _ => {
                        if v_b != ret {
                            self.emit_relaxed_binop(Assembler::emit_mov, Size::S64, v_b, ret);
                        }
                    }
                }
                self.assembler.emit_label(end_label);
            }
            Operator::Block { ty } => {
                let frame = ControlFrame {
                    label: self.assembler.get_label(),
                    loop_like: false,
                    if_else: IfElseState::None,
                    returns: match ty {
                        WpTypeOrFuncType::Type(WpType::EmptyBlockType) => smallvec![],
                        WpTypeOrFuncType::Type(inner_ty) => smallvec![inner_ty],
                        _ => {
                            return Err(CodegenError {
                                message: "Block: multi-value returns not yet implemented"
                                    .to_string(),
                            })
                        }
                    },
                    value_stack_depth: self.value_stack.len(),
                    fp_stack_depth: self.fp_stack.len(),
                };
                self.control_stack.push(frame);
            }
            Operator::Loop { ty } => {
                // Pad with NOPs to the next 16-byte boundary.
                // Here we don't use the dynasm `.align 16` attribute because it pads the alignment with single-byte nops
                // which may lead to efficiency problems.
                match self.assembler.get_offset().0 % 16 {
                    0 => {}
                    x => {
                        self.assembler.emit_nop_n(16 - x);
                    }
                }
                assert_eq!(self.assembler.get_offset().0 % 16, 0);

                let label = self.assembler.get_label();
                let _activate_offset = self.assembler.get_offset().0;

                self.control_stack.push(ControlFrame {
                    label,
                    loop_like: true,
                    if_else: IfElseState::None,
                    returns: match ty {
                        WpTypeOrFuncType::Type(WpType::EmptyBlockType) => smallvec![],
                        WpTypeOrFuncType::Type(inner_ty) => smallvec![inner_ty],
                        _ => {
                            return Err(CodegenError {
                                message: "Loop: multi-value returns not yet implemented"
                                    .to_string(),
                            })
                        }
                    },
                    value_stack_depth: self.value_stack.len(),
                    fp_stack_depth: self.fp_stack.len(),
                });
                self.assembler.emit_label(label);

                // TODO: Re-enable interrupt signal check without branching
            }
            Operator::Nop => {}
            Operator::MemorySize { mem, mem_byte: _ } => {
                let memory_index = MemoryIndex::new(mem as usize);
                self.assembler.emit_mov(
                    Size::S64,
                    Location::Memory(
                        Machine::get_vmctx_reg(),
                        self.vmoffsets.vmctx_builtin_function(
                            if self.module.local_memory_index(memory_index).is_some() {
                                VMBuiltinFunctionIndex::get_memory32_size_index()
                            } else {
                                VMBuiltinFunctionIndex::get_imported_memory32_size_index()
                            },
                        ) as i32,
                    ),
                    Location::GPR(GPR::RAX),
                );
                self.emit_call_native(
                    |this| {
                        this.assembler.emit_call_register(GPR::RAX);
                    },
                    // [vmctx, memory_index]
                    iter::once(Location::Imm32(memory_index.index() as u32)),
                )?;
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);
                self.assembler
                    .emit_mov(Size::S64, Location::GPR(GPR::RAX), ret);
            }
            Operator::MemoryInit { segment, mem } => {
                let len = self.value_stack.pop().unwrap();
                let src = self.value_stack.pop().unwrap();
                let dst = self.value_stack.pop().unwrap();
                self.machine.release_locations_only_regs(&[len, src, dst]);

                self.assembler.emit_mov(
                    Size::S64,
                    Location::Memory(
                        Machine::get_vmctx_reg(),
                        self.vmoffsets
                            .vmctx_builtin_function(VMBuiltinFunctionIndex::get_memory_init_index())
                            as i32,
                    ),
                    Location::GPR(GPR::RAX),
                );

                self.emit_call_native(
                    |this| {
                        this.assembler.emit_call_register(GPR::RAX);
                    },
                    // [vmctx, memory_index, segment_index, dst, src, len]
                    [
                        Location::Imm32(mem),
                        Location::Imm32(segment),
                        dst,
                        src,
                        len,
                    ]
                    .iter()
                    .cloned(),
                )?;
                self.machine
                    .release_locations_only_stack(&mut self.assembler, &[dst, src, len]);
            }
            Operator::DataDrop { segment } => {
                self.assembler.emit_mov(
                    Size::S64,
                    Location::Memory(
                        Machine::get_vmctx_reg(),
                        self.vmoffsets
                            .vmctx_builtin_function(VMBuiltinFunctionIndex::get_data_drop_index())
                            as i32,
                    ),
                    Location::GPR(GPR::RAX),
                );

                self.emit_call_native(
                    |this| {
                        this.assembler.emit_call_register(GPR::RAX);
                    },
                    // [vmctx, segment_index]
                    iter::once(Location::Imm32(segment)),
                )?;
            }
            Operator::MemoryCopy { src, dst } => {
                // ignore until we support multiple memories
                let _dst = dst;
                let len = self.value_stack.pop().unwrap();
                let src_pos = self.value_stack.pop().unwrap();
                let dst_pos = self.value_stack.pop().unwrap();
                self.machine
                    .release_locations_only_regs(&[len, src_pos, dst_pos]);

                let memory_index = MemoryIndex::new(src as usize);
                let (memory_copy_index, memory_index) =
                    if self.module.local_memory_index(memory_index).is_some() {
                        (
                            VMBuiltinFunctionIndex::get_memory_copy_index(),
                            memory_index,
                        )
                    } else {
                        (
                            VMBuiltinFunctionIndex::get_imported_memory_copy_index(),
                            memory_index,
                        )
                    };

                self.assembler.emit_mov(
                    Size::S64,
                    Location::Memory(
                        Machine::get_vmctx_reg(),
                        self.vmoffsets.vmctx_builtin_function(memory_copy_index) as i32,
                    ),
                    Location::GPR(GPR::RAX),
                );

                self.emit_call_native(
                    |this| {
                        this.assembler.emit_call_register(GPR::RAX);
                    },
                    // [vmctx, memory_index, dst, src, len]
                    [
                        Location::Imm32(memory_index.index() as u32),
                        dst_pos,
                        src_pos,
                        len,
                    ]
                    .iter()
                    .cloned(),
                )?;
                self.machine
                    .release_locations_only_stack(&mut self.assembler, &[dst_pos, src_pos, len]);
            }
            Operator::MemoryFill { mem } => {
                let len = self.value_stack.pop().unwrap();
                let val = self.value_stack.pop().unwrap();
                let dst = self.value_stack.pop().unwrap();
                self.machine.release_locations_only_regs(&[len, val, dst]);

                let memory_index = MemoryIndex::new(mem as usize);
                let (memory_fill_index, memory_index) =
                    if self.module.local_memory_index(memory_index).is_some() {
                        (
                            VMBuiltinFunctionIndex::get_memory_fill_index(),
                            memory_index,
                        )
                    } else {
                        (
                            VMBuiltinFunctionIndex::get_imported_memory_fill_index(),
                            memory_index,
                        )
                    };

                self.assembler.emit_mov(
                    Size::S64,
                    Location::Memory(
                        Machine::get_vmctx_reg(),
                        self.vmoffsets.vmctx_builtin_function(memory_fill_index) as i32,
                    ),
                    Location::GPR(GPR::RAX),
                );

                self.emit_call_native(
                    |this| {
                        this.assembler.emit_call_register(GPR::RAX);
                    },
                    // [vmctx, memory_index, dst, src, len]
                    [Location::Imm32(memory_index.index() as u32), dst, val, len]
                        .iter()
                        .cloned(),
                )?;
                self.machine
                    .release_locations_only_stack(&mut self.assembler, &[dst, val, len]);
            }
            Operator::MemoryGrow { mem, mem_byte: _ } => {
                let memory_index = MemoryIndex::new(mem as usize);
                let param_pages = self.value_stack.pop().unwrap();

                self.machine.release_locations_only_regs(&[param_pages]);

                self.assembler.emit_mov(
                    Size::S64,
                    Location::Memory(
                        Machine::get_vmctx_reg(),
                        self.vmoffsets.vmctx_builtin_function(
                            if self.module.local_memory_index(memory_index).is_some() {
                                VMBuiltinFunctionIndex::get_memory32_grow_index()
                            } else {
                                VMBuiltinFunctionIndex::get_imported_memory32_grow_index()
                            },
                        ) as i32,
                    ),
                    Location::GPR(GPR::RAX),
                );

                self.emit_call_native(
                    |this| {
                        this.assembler.emit_call_register(GPR::RAX);
                    },
                    // [vmctx, val, memory_index]
                    iter::once(param_pages)
                        .chain(iter::once(Location::Imm32(memory_index.index() as u32))),
                )?;

                self.machine
                    .release_locations_only_stack(&mut self.assembler, &[param_pages]);

                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);
                self.assembler
                    .emit_mov(Size::S64, Location::GPR(GPR::RAX), ret);
            }
            Operator::I32Load { ref memarg } => {
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];
                self.value_stack.push(ret);

                self.emit_memory_op(target, memarg, false, 4, |this, addr| {
                    this.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S32,
                        Location::Memory(addr, 0),
                        ret,
                    );
                    Ok(())
                })?;
            }
            Operator::F32Load { ref memarg } => {
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::F32)], false)[0];
                self.value_stack.push(ret);
                self.fp_stack
                    .push(FloatValue::new(self.value_stack.len() - 1));

                self.emit_memory_op(target, memarg, false, 4, |this, addr| {
                    this.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S32,
                        Location::Memory(addr, 0),
                        ret,
                    );
                    Ok(())
                })?;
            }
            Operator::I32Load8U { ref memarg } => {
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];
                self.value_stack.push(ret);

                self.emit_memory_op(target, memarg, false, 1, |this, addr| {
                    this.emit_relaxed_zx_sx(
                        Assembler::emit_movzx,
                        Size::S8,
                        Location::Memory(addr, 0),
                        Size::S32,
                        ret,
                    )?;
                    Ok(())
                })?;
            }
            Operator::I32Load8S { ref memarg } => {
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];
                self.value_stack.push(ret);

                self.emit_memory_op(target, memarg, false, 1, |this, addr| {
                    this.emit_relaxed_zx_sx(
                        Assembler::emit_movsx,
                        Size::S8,
                        Location::Memory(addr, 0),
                        Size::S32,
                        ret,
                    )?;
                    Ok(())
                })?;
            }
            Operator::I32Load16U { ref memarg } => {
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];
                self.value_stack.push(ret);

                self.emit_memory_op(target, memarg, false, 2, |this, addr| {
                    this.emit_relaxed_zx_sx(
                        Assembler::emit_movzx,
                        Size::S16,
                        Location::Memory(addr, 0),
                        Size::S32,
                        ret,
                    )?;
                    Ok(())
                })?;
            }
            Operator::I32Load16S { ref memarg } => {
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];
                self.value_stack.push(ret);

                self.emit_memory_op(target, memarg, false, 2, |this, addr| {
                    this.emit_relaxed_zx_sx(
                        Assembler::emit_movsx,
                        Size::S16,
                        Location::Memory(addr, 0),
                        Size::S32,
                        ret,
                    )?;
                    Ok(())
                })?;
            }
            Operator::I32Store { ref memarg } => {
                let target_value = self.pop_value_released();
                let target_addr = self.pop_value_released();

                self.emit_memory_op(target_addr, memarg, false, 4, |this, addr| {
                    this.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S32,
                        target_value,
                        Location::Memory(addr, 0),
                    );
                    Ok(())
                })?;
            }
            Operator::F32Store { ref memarg } => {
                let target_value = self.pop_value_released();
                let target_addr = self.pop_value_released();
                let fp = self.fp_stack.pop1()?;
                let config_nan_canonicalization = self.config.enable_nan_canonicalization;

                self.emit_memory_op(target_addr, memarg, false, 4, |this, addr| {
                    if !this.assembler.arch_supports_canonicalize_nan()
                        || !config_nan_canonicalization
                        || fp.canonicalization.is_none()
                    {
                        this.emit_relaxed_binop(
                            Assembler::emit_mov,
                            Size::S32,
                            target_value,
                            Location::Memory(addr, 0),
                        );
                    } else {
                        this.canonicalize_nan(Size::S32, target_value, Location::Memory(addr, 0));
                    }

                    Ok(())
                })?;
            }
            Operator::I32Store8 { ref memarg } => {
                let target_value = self.pop_value_released();
                let target_addr = self.pop_value_released();

                self.emit_memory_op(target_addr, memarg, false, 1, |this, addr| {
                    this.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S8,
                        target_value,
                        Location::Memory(addr, 0),
                    );
                    Ok(())
                })?;
            }
            Operator::I32Store16 { ref memarg } => {
                let target_value = self.pop_value_released();
                let target_addr = self.pop_value_released();

                self.emit_memory_op(target_addr, memarg, false, 2, |this, addr| {
                    this.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S16,
                        target_value,
                        Location::Memory(addr, 0),
                    );
                    Ok(())
                })?;
            }
            Operator::I64Load { ref memarg } => {
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);

                self.emit_memory_op(target, memarg, false, 8, |this, addr| {
                    this.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S64,
                        Location::Memory(addr, 0),
                        ret,
                    );
                    Ok(())
                })?;
            }
            Operator::F64Load { ref memarg } => {
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::F64)], false)[0];
                self.value_stack.push(ret);
                self.fp_stack
                    .push(FloatValue::new(self.value_stack.len() - 1));

                self.emit_memory_op(target, memarg, false, 8, |this, addr| {
                    this.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S64,
                        Location::Memory(addr, 0),
                        ret,
                    );
                    Ok(())
                })?;
            }
            Operator::I64Load8U { ref memarg } => {
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);

                self.emit_memory_op(target, memarg, false, 1, |this, addr| {
                    this.emit_relaxed_zx_sx(
                        Assembler::emit_movzx,
                        Size::S8,
                        Location::Memory(addr, 0),
                        Size::S64,
                        ret,
                    )?;
                    Ok(())
                })?;
            }
            Operator::I64Load8S { ref memarg } => {
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);

                self.emit_memory_op(target, memarg, false, 1, |this, addr| {
                    this.emit_relaxed_zx_sx(
                        Assembler::emit_movsx,
                        Size::S8,
                        Location::Memory(addr, 0),
                        Size::S64,
                        ret,
                    )?;
                    Ok(())
                })?;
            }
            Operator::I64Load16U { ref memarg } => {
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);

                self.emit_memory_op(target, memarg, false, 2, |this, addr| {
                    this.emit_relaxed_zx_sx(
                        Assembler::emit_movzx,
                        Size::S16,
                        Location::Memory(addr, 0),
                        Size::S64,
                        ret,
                    )?;
                    Ok(())
                })?;
            }
            Operator::I64Load16S { ref memarg } => {
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);

                self.emit_memory_op(target, memarg, false, 2, |this, addr| {
                    this.emit_relaxed_zx_sx(
                        Assembler::emit_movsx,
                        Size::S16,
                        Location::Memory(addr, 0),
                        Size::S64,
                        ret,
                    )?;
                    Ok(())
                })?;
            }
            Operator::I64Load32U { ref memarg } => {
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);

                self.emit_memory_op(target, memarg, false, 4, |this, addr| {
                    match ret {
                        Location::GPR(_) => {}
                        Location::Memory(base, offset) => {
                            this.assembler.emit_mov(
                                Size::S32,
                                Location::Imm32(0),
                                Location::Memory(base, offset + 4),
                            ); // clear upper bits
                        }
                        _ => {
                            return Err(CodegenError {
                                message: "I64Load32U ret: unreachable code".to_string(),
                            })
                        }
                    }
                    this.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S32,
                        Location::Memory(addr, 0),
                        ret,
                    );
                    Ok(())
                })?;
            }
            Operator::I64Load32S { ref memarg } => {
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);

                self.emit_memory_op(target, memarg, false, 4, |this, addr| {
                    this.emit_relaxed_zx_sx(
                        Assembler::emit_movsx,
                        Size::S32,
                        Location::Memory(addr, 0),
                        Size::S64,
                        ret,
                    )?;
                    Ok(())
                })?;
            }
            Operator::I64Store { ref memarg } => {
                let target_value = self.pop_value_released();
                let target_addr = self.pop_value_released();

                self.emit_memory_op(target_addr, memarg, false, 8, |this, addr| {
                    this.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S64,
                        target_value,
                        Location::Memory(addr, 0),
                    );
                    Ok(())
                })?;
            }
            Operator::F64Store { ref memarg } => {
                let target_value = self.pop_value_released();
                let target_addr = self.pop_value_released();
                let fp = self.fp_stack.pop1()?;
                let config_nan_canonicalization = self.config.enable_nan_canonicalization;

                self.emit_memory_op(target_addr, memarg, false, 8, |this, addr| {
                    if !this.assembler.arch_supports_canonicalize_nan()
                        || !config_nan_canonicalization
                        || fp.canonicalization.is_none()
                    {
                        this.emit_relaxed_binop(
                            Assembler::emit_mov,
                            Size::S64,
                            target_value,
                            Location::Memory(addr, 0),
                        );
                    } else {
                        this.canonicalize_nan(Size::S64, target_value, Location::Memory(addr, 0));
                    }
                    Ok(())
                })?;
            }
            Operator::I64Store8 { ref memarg } => {
                let target_value = self.pop_value_released();
                let target_addr = self.pop_value_released();

                self.emit_memory_op(target_addr, memarg, false, 1, |this, addr| {
                    this.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S8,
                        target_value,
                        Location::Memory(addr, 0),
                    );
                    Ok(())
                })?;
            }
            Operator::I64Store16 { ref memarg } => {
                let target_value = self.pop_value_released();
                let target_addr = self.pop_value_released();

                self.emit_memory_op(target_addr, memarg, false, 2, |this, addr| {
                    this.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S16,
                        target_value,
                        Location::Memory(addr, 0),
                    );
                    Ok(())
                })?;
            }
            Operator::I64Store32 { ref memarg } => {
                let target_value = self.pop_value_released();
                let target_addr = self.pop_value_released();

                self.emit_memory_op(target_addr, memarg, false, 4, |this, addr| {
                    this.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S32,
                        target_value,
                        Location::Memory(addr, 0),
                    );
                    Ok(())
                })?;
            }
            Operator::Unreachable => {
                let offset = self.assembler.get_offset().0;
                self.emit_trap(TrapCode::UnreachableCodeReached);
                self.mark_instruction_address_end(offset);
                self.unreachable_depth = 1;
            }
            Operator::Return => {
                let frame = &self.control_stack[0];
                if !frame.returns.is_empty() {
                    if frame.returns.len() != 1 {
                        return Err(CodegenError {
                            message: "Return: incorrect frame.returns".to_string(),
                        });
                    }
                    let first_return = frame.returns[0];
                    let loc = *self.value_stack.last().unwrap();
                    if first_return.is_float() {
                        let fp = self.fp_stack.peek1()?;
                        if self.assembler.arch_supports_canonicalize_nan()
                            && self.config.enable_nan_canonicalization
                            && fp.canonicalization.is_some()
                        {
                            self.canonicalize_nan(
                                match first_return {
                                    WpType::F32 => Size::S32,
                                    WpType::F64 => Size::S64,
                                    _ => unreachable!(),
                                },
                                loc,
                                Location::GPR(GPR::RAX),
                            );
                        } else {
                            self.emit_relaxed_binop(
                                Assembler::emit_mov,
                                Size::S64,
                                loc,
                                Location::GPR(GPR::RAX),
                            );
                        }
                    } else {
                        self.emit_relaxed_binop(
                            Assembler::emit_mov,
                            Size::S64,
                            loc,
                            Location::GPR(GPR::RAX),
                        );
                    }
                }
                let frame = &self.control_stack[0];
                let released = &self.value_stack[frame.value_stack_depth..];
                self.machine
                    .release_locations_keep_state(&mut self.assembler, released);
                self.assembler.emit_jmp(Condition::None, frame.label);
                self.unreachable_depth = 1;
            }
            Operator::Br { relative_depth } => {
                let frame =
                    &self.control_stack[self.control_stack.len() - 1 - (relative_depth as usize)];
                if !frame.loop_like && !frame.returns.is_empty() {
                    if frame.returns.len() != 1 {
                        return Err(CodegenError {
                            message: "Br: incorrect frame.returns".to_string(),
                        });
                    }
                    let first_return = frame.returns[0];
                    let loc = *self.value_stack.last().unwrap();

                    if first_return.is_float() {
                        let fp = self.fp_stack.peek1()?;
                        if self.assembler.arch_supports_canonicalize_nan()
                            && self.config.enable_nan_canonicalization
                            && fp.canonicalization.is_some()
                        {
                            self.canonicalize_nan(
                                match first_return {
                                    WpType::F32 => Size::S32,
                                    WpType::F64 => Size::S64,
                                    _ => unreachable!(),
                                },
                                loc,
                                Location::GPR(GPR::RAX),
                            );
                        } else {
                            self.emit_relaxed_binop(
                                Assembler::emit_mov,
                                Size::S64,
                                loc,
                                Location::GPR(GPR::RAX),
                            );
                        }
                    } else {
                        self.assembler
                            .emit_mov(Size::S64, loc, Location::GPR(GPR::RAX));
                    }
                }
                let frame =
                    &self.control_stack[self.control_stack.len() - 1 - (relative_depth as usize)];

                let released = &self.value_stack[frame.value_stack_depth..];
                self.machine
                    .release_locations_keep_state(&mut self.assembler, released);
                self.assembler.emit_jmp(Condition::None, frame.label);
                self.unreachable_depth = 1;
            }
            Operator::BrIf { relative_depth } => {
                let after = self.assembler.get_label();
                let cond = self.pop_value_released();
                self.emit_relaxed_binop(Assembler::emit_cmp, Size::S32, Location::Imm32(0), cond);
                self.assembler.emit_jmp(Condition::Equal, after);

                let frame =
                    &self.control_stack[self.control_stack.len() - 1 - (relative_depth as usize)];
                if !frame.loop_like && !frame.returns.is_empty() {
                    if frame.returns.len() != 1 {
                        return Err(CodegenError {
                            message: "BrIf: incorrect frame.returns".to_string(),
                        });
                    }

                    let first_return = frame.returns[0];
                    let loc = *self.value_stack.last().unwrap();
                    if first_return.is_float() {
                        let fp = self.fp_stack.peek1()?;
                        if self.assembler.arch_supports_canonicalize_nan()
                            && self.config.enable_nan_canonicalization
                            && fp.canonicalization.is_some()
                        {
                            self.canonicalize_nan(
                                match first_return {
                                    WpType::F32 => Size::S32,
                                    WpType::F64 => Size::S64,
                                    _ => unreachable!(),
                                },
                                loc,
                                Location::GPR(GPR::RAX),
                            );
                        } else {
                            self.emit_relaxed_binop(
                                Assembler::emit_mov,
                                Size::S64,
                                loc,
                                Location::GPR(GPR::RAX),
                            );
                        }
                    } else {
                        self.assembler
                            .emit_mov(Size::S64, loc, Location::GPR(GPR::RAX));
                    }
                }
                let frame =
                    &self.control_stack[self.control_stack.len() - 1 - (relative_depth as usize)];
                let released = &self.value_stack[frame.value_stack_depth..];
                self.machine
                    .release_locations_keep_state(&mut self.assembler, released);
                self.assembler.emit_jmp(Condition::None, frame.label);

                self.assembler.emit_label(after);
            }
            Operator::BrTable { ref table } => {
                let mut targets = table
                    .targets()
                    .collect::<Result<Vec<_>, _>>()
                    .map_err(|e| CodegenError {
                        message: format!("BrTable read_table: {:?}", e),
                    })?;
                let default_target = targets.pop().unwrap().0;
                let cond = self.pop_value_released();
                let table_label = self.assembler.get_label();
                let mut table: Vec<DynamicLabel> = vec![];
                let default_br = self.assembler.get_label();
                self.emit_relaxed_binop(
                    Assembler::emit_cmp,
                    Size::S32,
                    Location::Imm32(targets.len() as u32),
                    cond,
                );
                self.assembler.emit_jmp(Condition::AboveEqual, default_br);

                self.assembler
                    .emit_lea_label(table_label, Location::GPR(GPR::RCX));
                self.assembler
                    .emit_mov(Size::S32, cond, Location::GPR(GPR::RDX));

                let instr_size = self.assembler.get_jmp_instr_size();
                self.assembler
                    .emit_imul_imm32_gpr64(instr_size as _, GPR::RDX);
                self.assembler.emit_add(
                    Size::S64,
                    Location::GPR(GPR::RCX),
                    Location::GPR(GPR::RDX),
                );
                self.assembler.emit_jmp_location(Location::GPR(GPR::RDX));

                for (target, _) in targets.iter() {
                    let label = self.assembler.get_label();
                    self.assembler.emit_label(label);
                    table.push(label);
                    let frame =
                        &self.control_stack[self.control_stack.len() - 1 - (*target as usize)];
                    if !frame.loop_like && !frame.returns.is_empty() {
                        if frame.returns.len() != 1 {
                            return Err(CodegenError {
                                message: format!(
                                    "BrTable: incorrect frame.returns for {:?}",
                                    target
                                ),
                            });
                        }

                        let first_return = frame.returns[0];
                        let loc = *self.value_stack.last().unwrap();
                        if first_return.is_float() {
                            let fp = self.fp_stack.peek1()?;
                            if self.assembler.arch_supports_canonicalize_nan()
                                && self.config.enable_nan_canonicalization
                                && fp.canonicalization.is_some()
                            {
                                self.canonicalize_nan(
                                    match first_return {
                                        WpType::F32 => Size::S32,
                                        WpType::F64 => Size::S64,
                                        _ => unreachable!(),
                                    },
                                    loc,
                                    Location::GPR(GPR::RAX),
                                );
                            } else {
                                self.emit_relaxed_binop(
                                    Assembler::emit_mov,
                                    Size::S64,
                                    loc,
                                    Location::GPR(GPR::RAX),
                                );
                            }
                        } else {
                            self.assembler
                                .emit_mov(Size::S64, loc, Location::GPR(GPR::RAX));
                        }
                    }
                    let frame =
                        &self.control_stack[self.control_stack.len() - 1 - (*target as usize)];
                    let released = &self.value_stack[frame.value_stack_depth..];
                    self.machine
                        .release_locations_keep_state(&mut self.assembler, released);
                    self.assembler.emit_jmp(Condition::None, frame.label);
                }
                self.assembler.emit_label(default_br);

                {
                    let frame = &self.control_stack
                        [self.control_stack.len() - 1 - (default_target as usize)];
                    if !frame.loop_like && !frame.returns.is_empty() {
                        if frame.returns.len() != 1 {
                            return Err(CodegenError {
                                message: "BrTable: incorrect frame.returns".to_string(),
                            });
                        }

                        let first_return = frame.returns[0];
                        let loc = *self.value_stack.last().unwrap();
                        if first_return.is_float() {
                            let fp = self.fp_stack.peek1()?;
                            if self.assembler.arch_supports_canonicalize_nan()
                                && self.config.enable_nan_canonicalization
                                && fp.canonicalization.is_some()
                            {
                                self.canonicalize_nan(
                                    match first_return {
                                        WpType::F32 => Size::S32,
                                        WpType::F64 => Size::S64,
                                        _ => unreachable!(),
                                    },
                                    loc,
                                    Location::GPR(GPR::RAX),
                                );
                            } else {
                                self.emit_relaxed_binop(
                                    Assembler::emit_mov,
                                    Size::S64,
                                    loc,
                                    Location::GPR(GPR::RAX),
                                );
                            }
                        } else {
                            self.assembler
                                .emit_mov(Size::S64, loc, Location::GPR(GPR::RAX));
                        }
                    }
                    let frame = &self.control_stack
                        [self.control_stack.len() - 1 - (default_target as usize)];
                    let released = &self.value_stack[frame.value_stack_depth..];
                    self.machine
                        .release_locations_keep_state(&mut self.assembler, released);
                    self.assembler.emit_jmp(Condition::None, frame.label);
                }

                self.assembler.emit_label(table_label);
                for x in table {
                    self.assembler.emit_jmp(Condition::None, x);
                }
                self.unreachable_depth = 1;
            }
            Operator::Drop => {
                self.pop_value_released();
                if let Some(x) = self.fp_stack.last() {
                    if x.depth == self.value_stack.len() {
                        self.fp_stack.pop1()?;
                    }
                }
            }
            Operator::End => {
                let frame = self.control_stack.pop().unwrap();

                if !was_unreachable && !frame.returns.is_empty() {
                    let loc = *self.value_stack.last().unwrap();
                    if frame.returns[0].is_float() {
                        let fp = self.fp_stack.peek1()?;
                        if self.assembler.arch_supports_canonicalize_nan()
                            && self.config.enable_nan_canonicalization
                            && fp.canonicalization.is_some()
                        {
                            self.canonicalize_nan(
                                match frame.returns[0] {
                                    WpType::F32 => Size::S32,
                                    WpType::F64 => Size::S64,
                                    _ => unreachable!(),
                                },
                                loc,
                                Location::GPR(GPR::RAX),
                            );
                        } else {
                            self.emit_relaxed_binop(
                                Assembler::emit_mov,
                                Size::S64,
                                loc,
                                Location::GPR(GPR::RAX),
                            );
                        }
                    } else {
                        self.emit_relaxed_binop(
                            Assembler::emit_mov,
                            Size::S64,
                            loc,
                            Location::GPR(GPR::RAX),
                        );
                    }
                }

                if self.control_stack.is_empty() {
                    self.assembler.emit_label(frame.label);
                    self.update_max_stack_depth();
                    self.emit_function_stack_check(false);
                    self.machine.finalize_locals(
                        &mut self.assembler,
                        &self.locals,
                        self.calling_convention,
                    );
                    self.assembler.emit_mov(
                        Size::S64,
                        Location::GPR(GPR::RBP),
                        Location::GPR(GPR::RSP),
                    );
                    self.assembler.emit_pop(Size::S64, Location::GPR(GPR::RBP));

                    // Make a copy of the return value in XMM0, as required by the SysV CC.
                    match self.signature.results() {
                        [x] if *x == Type::F32 || *x == Type::F64 => {
                            self.assembler.emit_mov(
                                Size::S64,
                                Location::GPR(GPR::RAX),
                                Location::XMM(XMM::XMM0),
                            );
                        }
                        _ => {}
                    }
                    self.assembler.emit_ret();
                } else {
                    let released = &self.value_stack[frame.value_stack_depth..];
                    self.machine
                        .release_locations(&mut self.assembler, released);
                    self.update_max_stack_depth();
                    self.value_stack.truncate(frame.value_stack_depth);
                    self.fp_stack.truncate(frame.fp_stack_depth);

                    if !frame.loop_like {
                        self.assembler.emit_label(frame.label);
                    }

                    if let IfElseState::If(label) = frame.if_else {
                        self.assembler.emit_label(label);
                    }

                    if !frame.returns.is_empty() {
                        if frame.returns.len() != 1 {
                            return Err(CodegenError {
                                message: "End: incorrect frame.returns".to_string(),
                            });
                        }
                        let loc = self.machine.acquire_locations(
                            &mut self.assembler,
                            &[(frame.returns[0])],
                            false,
                        )[0];
                        self.assembler
                            .emit_mov(Size::S64, Location::GPR(GPR::RAX), loc);
                        self.value_stack.push(loc);
                        if frame.returns[0].is_float() {
                            self.fp_stack
                                .push(FloatValue::new(self.value_stack.len() - 1));
                            // we already canonicalized at the `Br*` instruction or here previously.
                        }
                    }
                }
            }
            Operator::AtomicFence { flags: _ } => {
                // Fence is a nop.
                //
                // Fence was added to preserve information about fences from
                // source languages. If in the future Wasm extends the memory
                // model, and if we hadn't recorded what fences used to be there,
                // it would lead to data races that weren't present in the
                // original source language.
            }
            Operator::I32AtomicLoad { ref memarg } => {
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];
                self.value_stack.push(ret);

                self.emit_memory_op(target, memarg, true, 4, |this, addr| {
                    this.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S32,
                        Location::Memory(addr, 0),
                        ret,
                    );
                    Ok(())
                })?;
            }
            Operator::I32AtomicLoad8U { ref memarg } => {
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];
                self.value_stack.push(ret);

                self.emit_memory_op(target, memarg, true, 1, |this, addr| {
                    this.emit_relaxed_zx_sx(
                        Assembler::emit_movzx,
                        Size::S8,
                        Location::Memory(addr, 0),
                        Size::S32,
                        ret,
                    )?;
                    Ok(())
                })?;
            }
            Operator::I32AtomicLoad16U { ref memarg } => {
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];
                self.value_stack.push(ret);

                self.emit_memory_op(target, memarg, true, 2, |this, addr| {
                    this.emit_relaxed_zx_sx(
                        Assembler::emit_movzx,
                        Size::S16,
                        Location::Memory(addr, 0),
                        Size::S32,
                        ret,
                    )?;
                    Ok(())
                })?;
            }
            Operator::I32AtomicStore { ref memarg } => {
                let target_value = self.pop_value_released();
                let target_addr = self.pop_value_released();

                self.emit_memory_op(target_addr, memarg, true, 4, |this, addr| {
                    this.emit_relaxed_binop(
                        Assembler::emit_xchg,
                        Size::S32,
                        target_value,
                        Location::Memory(addr, 0),
                    );
                    Ok(())
                })?;
            }
            Operator::I32AtomicStore8 { ref memarg } => {
                let target_value = self.pop_value_released();
                let target_addr = self.pop_value_released();

                self.emit_memory_op(target_addr, memarg, true, 1, |this, addr| {
                    this.emit_relaxed_binop(
                        Assembler::emit_xchg,
                        Size::S8,
                        target_value,
                        Location::Memory(addr, 0),
                    );
                    Ok(())
                })?;
            }
            Operator::I32AtomicStore16 { ref memarg } => {
                let target_value = self.pop_value_released();
                let target_addr = self.pop_value_released();

                self.emit_memory_op(target_addr, memarg, true, 2, |this, addr| {
                    this.emit_relaxed_binop(
                        Assembler::emit_xchg,
                        Size::S16,
                        target_value,
                        Location::Memory(addr, 0),
                    );
                    Ok(())
                })?;
            }
            Operator::I64AtomicLoad { ref memarg } => {
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);

                self.emit_memory_op(target, memarg, true, 8, |this, addr| {
                    this.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S64,
                        Location::Memory(addr, 0),
                        ret,
                    );
                    Ok(())
                })?;
            }
            Operator::I64AtomicLoad8U { ref memarg } => {
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);

                self.emit_memory_op(target, memarg, true, 1, |this, addr| {
                    this.emit_relaxed_zx_sx(
                        Assembler::emit_movzx,
                        Size::S8,
                        Location::Memory(addr, 0),
                        Size::S64,
                        ret,
                    )?;
                    Ok(())
                })?;
            }
            Operator::I64AtomicLoad16U { ref memarg } => {
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);

                self.emit_memory_op(target, memarg, true, 2, |this, addr| {
                    this.emit_relaxed_zx_sx(
                        Assembler::emit_movzx,
                        Size::S16,
                        Location::Memory(addr, 0),
                        Size::S64,
                        ret,
                    )?;
                    Ok(())
                })?;
            }
            Operator::I64AtomicLoad32U { ref memarg } => {
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);

                self.emit_memory_op(target, memarg, true, 4, |this, addr| {
                    match ret {
                        Location::GPR(_) => {}
                        Location::Memory(base, offset) => {
                            this.assembler.emit_mov(
                                Size::S32,
                                Location::Imm32(0),
                                Location::Memory(base, offset + 4),
                            ); // clear upper bits
                        }
                        _ => {
                            return Err(CodegenError {
                                message: "I64AtomicLoad32U ret: unreachable code".to_string(),
                            })
                        }
                    }
                    this.emit_relaxed_binop(
                        Assembler::emit_mov,
                        Size::S32,
                        Location::Memory(addr, 0),
                        ret,
                    );
                    Ok(())
                })?;
            }
            Operator::I64AtomicStore { ref memarg } => {
                let target_value = self.pop_value_released();
                let target_addr = self.pop_value_released();

                self.emit_memory_op(target_addr, memarg, true, 8, |this, addr| {
                    this.emit_relaxed_binop(
                        Assembler::emit_xchg,
                        Size::S64,
                        target_value,
                        Location::Memory(addr, 0),
                    );
                    Ok(())
                })?;
            }
            Operator::I64AtomicStore8 { ref memarg } => {
                let target_value = self.pop_value_released();
                let target_addr = self.pop_value_released();

                self.emit_memory_op(target_addr, memarg, true, 1, |this, addr| {
                    this.emit_relaxed_binop(
                        Assembler::emit_xchg,
                        Size::S8,
                        target_value,
                        Location::Memory(addr, 0),
                    );
                    Ok(())
                })?;
            }
            Operator::I64AtomicStore16 { ref memarg } => {
                let target_value = self.pop_value_released();
                let target_addr = self.pop_value_released();

                self.emit_memory_op(target_addr, memarg, true, 2, |this, addr| {
                    this.emit_relaxed_binop(
                        Assembler::emit_xchg,
                        Size::S16,
                        target_value,
                        Location::Memory(addr, 0),
                    );
                    Ok(())
                })?;
            }
            Operator::I64AtomicStore32 { ref memarg } => {
                let target_value = self.pop_value_released();
                let target_addr = self.pop_value_released();

                self.emit_memory_op(target_addr, memarg, true, 4, |this, addr| {
                    this.emit_relaxed_binop(
                        Assembler::emit_xchg,
                        Size::S32,
                        target_value,
                        Location::Memory(addr, 0),
                    );
                    Ok(())
                })?;
            }
            Operator::I32AtomicRmwAdd { ref memarg } => {
                let loc = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];
                self.value_stack.push(ret);

                let value = self.machine.acquire_temp_gpr().unwrap();
                self.assembler
                    .emit_mov(Size::S32, loc, Location::GPR(value));
                self.emit_memory_op(target, memarg, true, 4, |this, addr| {
                    this.assembler.emit_lock_xadd(
                        Size::S32,
                        Location::GPR(value),
                        Location::Memory(addr, 0),
                    );
                    Ok(())
                })?;
                self.assembler
                    .emit_mov(Size::S32, Location::GPR(value), ret);
                self.machine.release_temp_gpr(value);
            }
            Operator::I64AtomicRmwAdd { ref memarg } => {
                let loc = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);

                let value = self.machine.acquire_temp_gpr().unwrap();
                self.assembler
                    .emit_mov(Size::S64, loc, Location::GPR(value));
                self.emit_memory_op(target, memarg, true, 8, |this, addr| {
                    this.assembler.emit_lock_xadd(
                        Size::S64,
                        Location::GPR(value),
                        Location::Memory(addr, 0),
                    );
                    Ok(())
                })?;
                self.assembler
                    .emit_mov(Size::S64, Location::GPR(value), ret);
                self.machine.release_temp_gpr(value);
            }
            Operator::I32AtomicRmw8AddU { ref memarg } => {
                let loc = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];
                self.value_stack.push(ret);

                let value = self.machine.acquire_temp_gpr().unwrap();
                self.assembler
                    .emit_movzx(Size::S8, loc, Size::S32, Location::GPR(value));
                self.emit_memory_op(target, memarg, true, 1, |this, addr| {
                    this.assembler.emit_lock_xadd(
                        Size::S8,
                        Location::GPR(value),
                        Location::Memory(addr, 0),
                    );
                    Ok(())
                })?;
                self.assembler
                    .emit_mov(Size::S32, Location::GPR(value), ret);
                self.machine.release_temp_gpr(value);
            }
            Operator::I32AtomicRmw16AddU { ref memarg } => {
                let loc = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];
                self.value_stack.push(ret);

                let value = self.machine.acquire_temp_gpr().unwrap();
                self.assembler
                    .emit_movzx(Size::S16, loc, Size::S32, Location::GPR(value));
                self.emit_memory_op(target, memarg, true, 2, |this, addr| {
                    this.assembler.emit_lock_xadd(
                        Size::S16,
                        Location::GPR(value),
                        Location::Memory(addr, 0),
                    );
                    Ok(())
                })?;
                self.assembler
                    .emit_mov(Size::S32, Location::GPR(value), ret);
                self.machine.release_temp_gpr(value);
            }
            Operator::I64AtomicRmw8AddU { ref memarg } => {
                let loc = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);

                let value = self.machine.acquire_temp_gpr().unwrap();
                self.assembler
                    .emit_movzx(Size::S8, loc, Size::S64, Location::GPR(value));
                self.emit_memory_op(target, memarg, true, 1, |this, addr| {
                    this.assembler.emit_lock_xadd(
                        Size::S8,
                        Location::GPR(value),
                        Location::Memory(addr, 0),
                    );
                    Ok(())
                })?;
                self.assembler
                    .emit_mov(Size::S64, Location::GPR(value), ret);
                self.machine.release_temp_gpr(value);
            }
            Operator::I64AtomicRmw16AddU { ref memarg } => {
                let loc = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);

                let value = self.machine.acquire_temp_gpr().unwrap();
                self.assembler
                    .emit_movzx(Size::S16, loc, Size::S64, Location::GPR(value));
                self.emit_memory_op(target, memarg, true, 2, |this, addr| {
                    this.assembler.emit_lock_xadd(
                        Size::S16,
                        Location::GPR(value),
                        Location::Memory(addr, 0),
                    );
                    Ok(())
                })?;
                self.assembler
                    .emit_mov(Size::S64, Location::GPR(value), ret);
                self.machine.release_temp_gpr(value);
            }
            Operator::I64AtomicRmw32AddU { ref memarg } => {
                let loc = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);

                let value = self.machine.acquire_temp_gpr().unwrap();
                self.assembler
                    .emit_mov(Size::S32, loc, Location::GPR(value));
                self.emit_memory_op(target, memarg, true, 4, |this, addr| {
                    this.assembler.emit_lock_xadd(
                        Size::S32,
                        Location::GPR(value),
                        Location::Memory(addr, 0),
                    );
                    Ok(())
                })?;
                self.assembler
                    .emit_mov(Size::S64, Location::GPR(value), ret);
                self.machine.release_temp_gpr(value);
            }
            Operator::I32AtomicRmwSub { ref memarg } => {
                let loc = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];
                self.value_stack.push(ret);

                let value = self.machine.acquire_temp_gpr().unwrap();
                self.assembler
                    .emit_mov(Size::S32, loc, Location::GPR(value));
                self.assembler.emit_neg(Size::S32, Location::GPR(value));
                self.emit_memory_op(target, memarg, true, 4, |this, addr| {
                    this.assembler.emit_lock_xadd(
                        Size::S32,
                        Location::GPR(value),
                        Location::Memory(addr, 0),
                    );
                    Ok(())
                })?;
                self.assembler
                    .emit_mov(Size::S32, Location::GPR(value), ret);
                self.machine.release_temp_gpr(value);
            }
            Operator::I64AtomicRmwSub { ref memarg } => {
                let loc = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);

                let value = self.machine.acquire_temp_gpr().unwrap();
                self.assembler
                    .emit_mov(Size::S64, loc, Location::GPR(value));
                self.assembler.emit_neg(Size::S64, Location::GPR(value));
                self.emit_memory_op(target, memarg, true, 8, |this, addr| {
                    this.assembler.emit_lock_xadd(
                        Size::S64,
                        Location::GPR(value),
                        Location::Memory(addr, 0),
                    );
                    Ok(())
                })?;
                self.assembler
                    .emit_mov(Size::S64, Location::GPR(value), ret);
                self.machine.release_temp_gpr(value);
            }
            Operator::I32AtomicRmw8SubU { ref memarg } => {
                let loc = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];
                self.value_stack.push(ret);

                let value = self.machine.acquire_temp_gpr().unwrap();
                self.assembler
                    .emit_movzx(Size::S8, loc, Size::S32, Location::GPR(value));
                self.assembler.emit_neg(Size::S8, Location::GPR(value));
                self.emit_memory_op(target, memarg, true, 1, |this, addr| {
                    this.assembler.emit_lock_xadd(
                        Size::S8,
                        Location::GPR(value),
                        Location::Memory(addr, 0),
                    );
                    Ok(())
                })?;
                self.assembler
                    .emit_mov(Size::S32, Location::GPR(value), ret);
                self.machine.release_temp_gpr(value);
            }
            Operator::I32AtomicRmw16SubU { ref memarg } => {
                let loc = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];
                self.value_stack.push(ret);

                let value = self.machine.acquire_temp_gpr().unwrap();
                self.assembler
                    .emit_movzx(Size::S16, loc, Size::S32, Location::GPR(value));
                self.assembler.emit_neg(Size::S16, Location::GPR(value));
                self.emit_memory_op(target, memarg, true, 2, |this, addr| {
                    this.assembler.emit_lock_xadd(
                        Size::S16,
                        Location::GPR(value),
                        Location::Memory(addr, 0),
                    );
                    Ok(())
                })?;
                self.assembler
                    .emit_mov(Size::S32, Location::GPR(value), ret);
                self.machine.release_temp_gpr(value);
            }
            Operator::I64AtomicRmw8SubU { ref memarg } => {
                let loc = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);

                let value = self.machine.acquire_temp_gpr().unwrap();
                self.assembler
                    .emit_movzx(Size::S8, loc, Size::S64, Location::GPR(value));
                self.assembler.emit_neg(Size::S8, Location::GPR(value));
                self.emit_memory_op(target, memarg, true, 1, |this, addr| {
                    this.assembler.emit_lock_xadd(
                        Size::S8,
                        Location::GPR(value),
                        Location::Memory(addr, 0),
                    );
                    Ok(())
                })?;
                self.assembler
                    .emit_mov(Size::S64, Location::GPR(value), ret);
                self.machine.release_temp_gpr(value);
            }
            Operator::I64AtomicRmw16SubU { ref memarg } => {
                let loc = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);

                let value = self.machine.acquire_temp_gpr().unwrap();
                self.assembler
                    .emit_movzx(Size::S16, loc, Size::S64, Location::GPR(value));
                self.assembler.emit_neg(Size::S16, Location::GPR(value));
                self.emit_memory_op(target, memarg, true, 2, |this, addr| {
                    this.assembler.emit_lock_xadd(
                        Size::S16,
                        Location::GPR(value),
                        Location::Memory(addr, 0),
                    );
                    Ok(())
                })?;
                self.assembler
                    .emit_mov(Size::S64, Location::GPR(value), ret);
                self.machine.release_temp_gpr(value);
            }
            Operator::I64AtomicRmw32SubU { ref memarg } => {
                let loc = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);

                let value = self.machine.acquire_temp_gpr().unwrap();
                self.assembler
                    .emit_mov(Size::S32, loc, Location::GPR(value));
                self.assembler.emit_neg(Size::S32, Location::GPR(value));
                self.emit_memory_op(target, memarg, true, 2, |this, addr| {
                    this.assembler.emit_lock_xadd(
                        Size::S32,
                        Location::GPR(value),
                        Location::Memory(addr, 0),
                    );
                    Ok(())
                })?;
                self.assembler
                    .emit_mov(Size::S64, Location::GPR(value), ret);
                self.machine.release_temp_gpr(value);
            }
            Operator::I32AtomicRmwAnd { ref memarg } => {
                let loc = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];
                self.value_stack.push(ret);

                self.emit_compare_and_swap(
                    loc,
                    target,
                    ret,
                    memarg,
                    4,
                    Size::S32,
                    Size::S32,
                    |this, src, dst| {
                        this.assembler
                            .emit_and(Size::S32, Location::GPR(src), Location::GPR(dst));
                    },
                )?;
            }
            Operator::I64AtomicRmwAnd { ref memarg } => {
                let loc = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);

                self.emit_compare_and_swap(
                    loc,
                    target,
                    ret,
                    memarg,
                    8,
                    Size::S64,
                    Size::S64,
                    |this, src, dst| {
                        this.assembler
                            .emit_and(Size::S64, Location::GPR(src), Location::GPR(dst));
                    },
                )?;
            }
            Operator::I32AtomicRmw8AndU { ref memarg } => {
                let loc = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];
                self.value_stack.push(ret);

                self.emit_compare_and_swap(
                    loc,
                    target,
                    ret,
                    memarg,
                    1,
                    Size::S8,
                    Size::S32,
                    |this, src, dst| {
                        this.assembler
                            .emit_and(Size::S32, Location::GPR(src), Location::GPR(dst));
                    },
                )?;
            }
            Operator::I32AtomicRmw16AndU { ref memarg } => {
                let loc = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];
                self.value_stack.push(ret);

                self.emit_compare_and_swap(
                    loc,
                    target,
                    ret,
                    memarg,
                    1,
                    Size::S16,
                    Size::S32,
                    |this, src, dst| {
                        this.assembler
                            .emit_and(Size::S32, Location::GPR(src), Location::GPR(dst));
                    },
                )?;
            }
            Operator::I64AtomicRmw8AndU { ref memarg } => {
                let loc = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);

                self.emit_compare_and_swap(
                    loc,
                    target,
                    ret,
                    memarg,
                    1,
                    Size::S8,
                    Size::S64,
                    |this, src, dst| {
                        this.assembler
                            .emit_and(Size::S64, Location::GPR(src), Location::GPR(dst));
                    },
                )?;
            }
            Operator::I64AtomicRmw16AndU { ref memarg } => {
                let loc = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);

                self.emit_compare_and_swap(
                    loc,
                    target,
                    ret,
                    memarg,
                    1,
                    Size::S16,
                    Size::S64,
                    |this, src, dst| {
                        this.assembler
                            .emit_and(Size::S64, Location::GPR(src), Location::GPR(dst));
                    },
                )?;
            }
            Operator::I64AtomicRmw32AndU { ref memarg } => {
                let loc = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);

                self.emit_compare_and_swap(
                    loc,
                    target,
                    ret,
                    memarg,
                    1,
                    Size::S32,
                    Size::S64,
                    |this, src, dst| {
                        this.assembler
                            .emit_and(Size::S64, Location::GPR(src), Location::GPR(dst));
                    },
                )?;
            }
            Operator::I32AtomicRmwOr { ref memarg } => {
                let loc = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];
                self.value_stack.push(ret);

                self.emit_compare_and_swap(
                    loc,
                    target,
                    ret,
                    memarg,
                    4,
                    Size::S32,
                    Size::S32,
                    |this, src, dst| {
                        this.assembler
                            .emit_or(Size::S32, Location::GPR(src), Location::GPR(dst));
                    },
                )?;
            }
            Operator::I64AtomicRmwOr { ref memarg } => {
                let loc = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);

                self.emit_compare_and_swap(
                    loc,
                    target,
                    ret,
                    memarg,
                    8,
                    Size::S64,
                    Size::S64,
                    |this, src, dst| {
                        this.assembler
                            .emit_or(Size::S64, Location::GPR(src), Location::GPR(dst));
                    },
                )?;
            }
            Operator::I32AtomicRmw8OrU { ref memarg } => {
                let loc = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];
                self.value_stack.push(ret);

                self.emit_compare_and_swap(
                    loc,
                    target,
                    ret,
                    memarg,
                    1,
                    Size::S8,
                    Size::S32,
                    |this, src, dst| {
                        this.assembler
                            .emit_or(Size::S32, Location::GPR(src), Location::GPR(dst));
                    },
                )?;
            }
            Operator::I32AtomicRmw16OrU { ref memarg } => {
                let loc = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];
                self.value_stack.push(ret);

                self.emit_compare_and_swap(
                    loc,
                    target,
                    ret,
                    memarg,
                    1,
                    Size::S16,
                    Size::S32,
                    |this, src, dst| {
                        this.assembler
                            .emit_or(Size::S32, Location::GPR(src), Location::GPR(dst));
                    },
                )?;
            }
            Operator::I64AtomicRmw8OrU { ref memarg } => {
                let loc = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);

                self.emit_compare_and_swap(
                    loc,
                    target,
                    ret,
                    memarg,
                    1,
                    Size::S8,
                    Size::S64,
                    |this, src, dst| {
                        this.assembler
                            .emit_or(Size::S64, Location::GPR(src), Location::GPR(dst));
                    },
                )?;
            }
            Operator::I64AtomicRmw16OrU { ref memarg } => {
                let loc = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);

                self.emit_compare_and_swap(
                    loc,
                    target,
                    ret,
                    memarg,
                    1,
                    Size::S16,
                    Size::S64,
                    |this, src, dst| {
                        this.assembler
                            .emit_or(Size::S64, Location::GPR(src), Location::GPR(dst));
                    },
                )?;
            }
            Operator::I64AtomicRmw32OrU { ref memarg } => {
                let loc = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);

                self.emit_compare_and_swap(
                    loc,
                    target,
                    ret,
                    memarg,
                    1,
                    Size::S32,
                    Size::S64,
                    |this, src, dst| {
                        this.assembler
                            .emit_or(Size::S64, Location::GPR(src), Location::GPR(dst));
                    },
                )?;
            }
            Operator::I32AtomicRmwXor { ref memarg } => {
                let loc = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];
                self.value_stack.push(ret);

                self.emit_compare_and_swap(
                    loc,
                    target,
                    ret,
                    memarg,
                    4,
                    Size::S32,
                    Size::S32,
                    |this, src, dst| {
                        this.assembler
                            .emit_xor(Size::S32, Location::GPR(src), Location::GPR(dst));
                    },
                )?;
            }
            Operator::I64AtomicRmwXor { ref memarg } => {
                let loc = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);

                self.emit_compare_and_swap(
                    loc,
                    target,
                    ret,
                    memarg,
                    8,
                    Size::S64,
                    Size::S64,
                    |this, src, dst| {
                        this.assembler
                            .emit_xor(Size::S64, Location::GPR(src), Location::GPR(dst));
                    },
                )?;
            }
            Operator::I32AtomicRmw8XorU { ref memarg } => {
                let loc = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];
                self.value_stack.push(ret);

                self.emit_compare_and_swap(
                    loc,
                    target,
                    ret,
                    memarg,
                    1,
                    Size::S8,
                    Size::S32,
                    |this, src, dst| {
                        this.assembler
                            .emit_xor(Size::S32, Location::GPR(src), Location::GPR(dst));
                    },
                )?;
            }
            Operator::I32AtomicRmw16XorU { ref memarg } => {
                let loc = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];
                self.value_stack.push(ret);

                self.emit_compare_and_swap(
                    loc,
                    target,
                    ret,
                    memarg,
                    1,
                    Size::S16,
                    Size::S32,
                    |this, src, dst| {
                        this.assembler
                            .emit_xor(Size::S32, Location::GPR(src), Location::GPR(dst));
                    },
                )?;
            }
            Operator::I64AtomicRmw8XorU { ref memarg } => {
                let loc = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);

                self.emit_compare_and_swap(
                    loc,
                    target,
                    ret,
                    memarg,
                    1,
                    Size::S8,
                    Size::S64,
                    |this, src, dst| {
                        this.assembler
                            .emit_xor(Size::S64, Location::GPR(src), Location::GPR(dst));
                    },
                )?;
            }
            Operator::I64AtomicRmw16XorU { ref memarg } => {
                let loc = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);

                self.emit_compare_and_swap(
                    loc,
                    target,
                    ret,
                    memarg,
                    1,
                    Size::S16,
                    Size::S64,
                    |this, src, dst| {
                        this.assembler
                            .emit_xor(Size::S64, Location::GPR(src), Location::GPR(dst));
                    },
                )?;
            }
            Operator::I64AtomicRmw32XorU { ref memarg } => {
                let loc = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);

                self.emit_compare_and_swap(
                    loc,
                    target,
                    ret,
                    memarg,
                    1,
                    Size::S32,
                    Size::S64,
                    |this, src, dst| {
                        this.assembler
                            .emit_xor(Size::S64, Location::GPR(src), Location::GPR(dst));
                    },
                )?;
            }
            Operator::I32AtomicRmwXchg { ref memarg } => {
                let loc = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];
                self.value_stack.push(ret);

                let value = self.machine.acquire_temp_gpr().unwrap();
                self.assembler
                    .emit_mov(Size::S32, loc, Location::GPR(value));
                self.emit_memory_op(target, memarg, true, 4, |this, addr| {
                    this.assembler.emit_xchg(
                        Size::S32,
                        Location::GPR(value),
                        Location::Memory(addr, 0),
                    );
                    Ok(())
                })?;
                self.assembler
                    .emit_mov(Size::S32, Location::GPR(value), ret);
                self.machine.release_temp_gpr(value);
            }
            Operator::I64AtomicRmwXchg { ref memarg } => {
                let loc = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);

                let value = self.machine.acquire_temp_gpr().unwrap();
                self.assembler
                    .emit_mov(Size::S64, loc, Location::GPR(value));
                self.emit_memory_op(target, memarg, true, 8, |this, addr| {
                    this.assembler.emit_xchg(
                        Size::S64,
                        Location::GPR(value),
                        Location::Memory(addr, 0),
                    );
                    Ok(())
                })?;
                self.assembler
                    .emit_mov(Size::S64, Location::GPR(value), ret);
                self.machine.release_temp_gpr(value);
            }
            Operator::I32AtomicRmw8XchgU { ref memarg } => {
                let loc = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];
                self.value_stack.push(ret);

                let value = self.machine.acquire_temp_gpr().unwrap();
                self.assembler
                    .emit_movzx(Size::S8, loc, Size::S32, Location::GPR(value));
                self.emit_memory_op(target, memarg, true, 1, |this, addr| {
                    this.assembler.emit_xchg(
                        Size::S8,
                        Location::GPR(value),
                        Location::Memory(addr, 0),
                    );
                    Ok(())
                })?;
                self.assembler
                    .emit_mov(Size::S32, Location::GPR(value), ret);
                self.machine.release_temp_gpr(value);
            }
            Operator::I32AtomicRmw16XchgU { ref memarg } => {
                let loc = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];
                self.value_stack.push(ret);

                let value = self.machine.acquire_temp_gpr().unwrap();
                self.assembler
                    .emit_movzx(Size::S16, loc, Size::S32, Location::GPR(value));
                self.emit_memory_op(target, memarg, true, 2, |this, addr| {
                    this.assembler.emit_xchg(
                        Size::S16,
                        Location::GPR(value),
                        Location::Memory(addr, 0),
                    );
                    Ok(())
                })?;
                self.assembler
                    .emit_mov(Size::S32, Location::GPR(value), ret);
                self.machine.release_temp_gpr(value);
            }
            Operator::I64AtomicRmw8XchgU { ref memarg } => {
                let loc = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);

                let value = self.machine.acquire_temp_gpr().unwrap();
                self.assembler
                    .emit_movzx(Size::S8, loc, Size::S64, Location::GPR(value));
                self.emit_memory_op(target, memarg, true, 1, |this, addr| {
                    this.assembler.emit_xchg(
                        Size::S8,
                        Location::GPR(value),
                        Location::Memory(addr, 0),
                    );
                    Ok(())
                })?;
                self.assembler
                    .emit_mov(Size::S64, Location::GPR(value), ret);
                self.machine.release_temp_gpr(value);
            }
            Operator::I64AtomicRmw16XchgU { ref memarg } => {
                let loc = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);

                let value = self.machine.acquire_temp_gpr().unwrap();
                self.assembler
                    .emit_movzx(Size::S16, loc, Size::S64, Location::GPR(value));
                self.emit_memory_op(target, memarg, true, 2, |this, addr| {
                    this.assembler.emit_xchg(
                        Size::S16,
                        Location::GPR(value),
                        Location::Memory(addr, 0),
                    );
                    Ok(())
                })?;
                self.assembler
                    .emit_mov(Size::S64, Location::GPR(value), ret);
                self.machine.release_temp_gpr(value);
            }
            Operator::I64AtomicRmw32XchgU { ref memarg } => {
                let loc = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);

                let value = self.machine.acquire_temp_gpr().unwrap();
                self.assembler
                    .emit_mov(Size::S32, loc, Location::GPR(value));
                self.emit_memory_op(target, memarg, true, 4, |this, addr| {
                    this.assembler.emit_xchg(
                        Size::S32,
                        Location::GPR(value),
                        Location::Memory(addr, 0),
                    );
                    Ok(())
                })?;
                self.assembler
                    .emit_mov(Size::S64, Location::GPR(value), ret);
                self.machine.release_temp_gpr(value);
            }
            Operator::I32AtomicRmwCmpxchg { ref memarg } => {
                let new = self.pop_value_released();
                let cmp = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];
                self.value_stack.push(ret);

                let compare = self.machine.reserve_unused_temp_gpr(GPR::RAX);
                let value = if cmp == Location::GPR(GPR::R14) {
                    if new == Location::GPR(GPR::R13) {
                        GPR::R12
                    } else {
                        GPR::R13
                    }
                } else {
                    GPR::R14
                };
                self.assembler.emit_push(Size::S64, Location::GPR(value));
                self.assembler
                    .emit_mov(Size::S32, cmp, Location::GPR(compare));
                self.assembler
                    .emit_mov(Size::S32, new, Location::GPR(value));

                self.emit_memory_op(target, memarg, true, 4, |this, addr| {
                    this.assembler.emit_lock_cmpxchg(
                        Size::S32,
                        Location::GPR(value),
                        Location::Memory(addr, 0),
                    );
                    this.assembler
                        .emit_mov(Size::S32, Location::GPR(compare), ret);
                    Ok(())
                })?;
                self.assembler.emit_pop(Size::S64, Location::GPR(value));
                self.machine.release_temp_gpr(compare);
            }
            Operator::I64AtomicRmwCmpxchg { ref memarg } => {
                let new = self.pop_value_released();
                let cmp = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);

                let compare = self.machine.reserve_unused_temp_gpr(GPR::RAX);
                let value = if cmp == Location::GPR(GPR::R14) {
                    if new == Location::GPR(GPR::R13) {
                        GPR::R12
                    } else {
                        GPR::R13
                    }
                } else {
                    GPR::R14
                };
                self.assembler.emit_push(Size::S64, Location::GPR(value));
                self.assembler
                    .emit_mov(Size::S64, cmp, Location::GPR(compare));
                self.assembler
                    .emit_mov(Size::S64, new, Location::GPR(value));

                self.emit_memory_op(target, memarg, true, 8, |this, addr| {
                    this.assembler.emit_lock_cmpxchg(
                        Size::S64,
                        Location::GPR(value),
                        Location::Memory(addr, 0),
                    );
                    this.assembler
                        .emit_mov(Size::S64, Location::GPR(compare), ret);
                    Ok(())
                })?;
                self.assembler.emit_pop(Size::S64, Location::GPR(value));
                self.machine.release_temp_gpr(compare);
            }
            Operator::I32AtomicRmw8CmpxchgU { ref memarg } => {
                let new = self.pop_value_released();
                let cmp = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];
                self.value_stack.push(ret);

                let compare = self.machine.reserve_unused_temp_gpr(GPR::RAX);
                let value = if cmp == Location::GPR(GPR::R14) {
                    if new == Location::GPR(GPR::R13) {
                        GPR::R12
                    } else {
                        GPR::R13
                    }
                } else {
                    GPR::R14
                };
                self.assembler.emit_push(Size::S64, Location::GPR(value));
                self.assembler
                    .emit_mov(Size::S32, cmp, Location::GPR(compare));
                self.assembler
                    .emit_mov(Size::S32, new, Location::GPR(value));

                self.emit_memory_op(target, memarg, true, 1, |this, addr| {
                    this.assembler.emit_lock_cmpxchg(
                        Size::S8,
                        Location::GPR(value),
                        Location::Memory(addr, 0),
                    );
                    this.assembler
                        .emit_movzx(Size::S8, Location::GPR(compare), Size::S32, ret);
                    Ok(())
                })?;
                self.assembler.emit_pop(Size::S64, Location::GPR(value));
                self.machine.release_temp_gpr(compare);
            }
            Operator::I32AtomicRmw16CmpxchgU { ref memarg } => {
                let new = self.pop_value_released();
                let cmp = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];
                self.value_stack.push(ret);

                let compare = self.machine.reserve_unused_temp_gpr(GPR::RAX);
                let value = if cmp == Location::GPR(GPR::R14) {
                    if new == Location::GPR(GPR::R13) {
                        GPR::R12
                    } else {
                        GPR::R13
                    }
                } else {
                    GPR::R14
                };
                self.assembler.emit_push(Size::S64, Location::GPR(value));
                self.assembler
                    .emit_mov(Size::S32, cmp, Location::GPR(compare));
                self.assembler
                    .emit_mov(Size::S32, new, Location::GPR(value));

                self.emit_memory_op(target, memarg, true, 1, |this, addr| {
                    this.assembler.emit_lock_cmpxchg(
                        Size::S16,
                        Location::GPR(value),
                        Location::Memory(addr, 0),
                    );
                    this.assembler
                        .emit_movzx(Size::S16, Location::GPR(compare), Size::S32, ret);
                    Ok(())
                })?;
                self.assembler.emit_pop(Size::S64, Location::GPR(value));
                self.machine.release_temp_gpr(compare);
            }
            Operator::I64AtomicRmw8CmpxchgU { ref memarg } => {
                let new = self.pop_value_released();
                let cmp = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);

                let compare = self.machine.reserve_unused_temp_gpr(GPR::RAX);
                let value = if cmp == Location::GPR(GPR::R14) {
                    if new == Location::GPR(GPR::R13) {
                        GPR::R12
                    } else {
                        GPR::R13
                    }
                } else {
                    GPR::R14
                };
                self.assembler.emit_push(Size::S64, Location::GPR(value));
                self.assembler
                    .emit_mov(Size::S64, cmp, Location::GPR(compare));
                self.assembler
                    .emit_mov(Size::S64, new, Location::GPR(value));

                self.emit_memory_op(target, memarg, true, 1, |this, addr| {
                    this.assembler.emit_lock_cmpxchg(
                        Size::S8,
                        Location::GPR(value),
                        Location::Memory(addr, 0),
                    );
                    this.assembler
                        .emit_movzx(Size::S8, Location::GPR(compare), Size::S64, ret);
                    Ok(())
                })?;
                self.assembler.emit_pop(Size::S64, Location::GPR(value));
                self.machine.release_temp_gpr(compare);
            }
            Operator::I64AtomicRmw16CmpxchgU { ref memarg } => {
                let new = self.pop_value_released();
                let cmp = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);

                let compare = self.machine.reserve_unused_temp_gpr(GPR::RAX);
                let value = if cmp == Location::GPR(GPR::R14) {
                    if new == Location::GPR(GPR::R13) {
                        GPR::R12
                    } else {
                        GPR::R13
                    }
                } else {
                    GPR::R14
                };
                self.assembler.emit_push(Size::S64, Location::GPR(value));
                self.assembler
                    .emit_mov(Size::S64, cmp, Location::GPR(compare));
                self.assembler
                    .emit_mov(Size::S64, new, Location::GPR(value));

                self.emit_memory_op(target, memarg, true, 1, |this, addr| {
                    this.assembler.emit_lock_cmpxchg(
                        Size::S16,
                        Location::GPR(value),
                        Location::Memory(addr, 0),
                    );
                    this.assembler
                        .emit_movzx(Size::S16, Location::GPR(compare), Size::S64, ret);
                    Ok(())
                })?;
                self.assembler.emit_pop(Size::S64, Location::GPR(value));
                self.machine.release_temp_gpr(compare);
            }
            Operator::I64AtomicRmw32CmpxchgU { ref memarg } => {
                let new = self.pop_value_released();
                let cmp = self.pop_value_released();
                let target = self.pop_value_released();
                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I64)], false)[0];
                self.value_stack.push(ret);

                let compare = self.machine.reserve_unused_temp_gpr(GPR::RAX);
                let value = if cmp == Location::GPR(GPR::R14) {
                    if new == Location::GPR(GPR::R13) {
                        GPR::R12
                    } else {
                        GPR::R13
                    }
                } else {
                    GPR::R14
                };
                self.assembler.emit_push(Size::S64, Location::GPR(value));
                self.assembler
                    .emit_mov(Size::S64, cmp, Location::GPR(compare));
                self.assembler
                    .emit_mov(Size::S64, new, Location::GPR(value));

                self.emit_memory_op(target, memarg, true, 1, |this, addr| {
                    this.assembler.emit_lock_cmpxchg(
                        Size::S32,
                        Location::GPR(value),
                        Location::Memory(addr, 0),
                    );
                    this.assembler
                        .emit_mov(Size::S32, Location::GPR(compare), ret);
                    Ok(())
                })?;
                self.assembler.emit_pop(Size::S64, Location::GPR(value));
                self.machine.release_temp_gpr(compare);
            }

            Operator::RefNull { .. } => {
                self.value_stack.push(Location::Imm64(0));
            }
            Operator::RefFunc { function_index } => {
                self.assembler.emit_mov(
                    Size::S64,
                    Location::Memory(
                        Machine::get_vmctx_reg(),
                        self.vmoffsets
                            .vmctx_builtin_function(VMBuiltinFunctionIndex::get_func_ref_index())
                            as i32,
                    ),
                    Location::GPR(GPR::RAX),
                );

                self.emit_call_native(
                    |this| {
                        this.assembler.emit_call_register(GPR::RAX);
                    },
                    // [vmctx, func_index] -> funcref
                    iter::once(Location::Imm32(function_index as u32)),
                )?;

                let ret = self.machine.acquire_locations(
                    &mut self.assembler,
                    &[(WpType::FuncRef)],
                    false,
                )[0];
                self.value_stack.push(ret);
                self.assembler
                    .emit_mov(Size::S64, Location::GPR(GPR::RAX), ret);
            }
            Operator::RefIsNull => {
                self.emit_cmpop_i64_dynamic_b(Condition::Equal, Location::Imm64(0))?;
            }
            Operator::TableSet { table: index } => {
                let table_index = TableIndex::new(index as _);
                let value = self.value_stack.pop().unwrap();
                let index = self.value_stack.pop().unwrap();
                // double check this does what I think it does
                self.machine.release_locations_only_regs(&[value, index]);

                self.assembler.emit_mov(
                    Size::S64,
                    Location::Memory(
                        Machine::get_vmctx_reg(),
                        self.vmoffsets.vmctx_builtin_function(
                            if self.module.local_table_index(table_index).is_some() {
                                VMBuiltinFunctionIndex::get_table_set_index()
                            } else {
                                VMBuiltinFunctionIndex::get_imported_table_set_index()
                            },
                        ) as i32,
                    ),
                    Location::GPR(GPR::RAX),
                );

                self.emit_call_native(
                    |this| {
                        this.assembler.emit_call_register(GPR::RAX);
                    },
                    // [vmctx, table_index, elem_index, reftype]
                    [Location::Imm32(table_index.index() as u32), index, value]
                        .iter()
                        .cloned(),
                )?;

                self.machine
                    .release_locations_only_stack(&mut self.assembler, &[index, value]);
            }
            Operator::TableGet { table: index } => {
                let table_index = TableIndex::new(index as _);
                let index = self.value_stack.pop().unwrap();
                self.machine.release_locations_only_regs(&[index]);

                self.assembler.emit_mov(
                    Size::S64,
                    Location::Memory(
                        Machine::get_vmctx_reg(),
                        self.vmoffsets.vmctx_builtin_function(
                            if self.module.local_table_index(table_index).is_some() {
                                VMBuiltinFunctionIndex::get_table_get_index()
                            } else {
                                VMBuiltinFunctionIndex::get_imported_table_get_index()
                            },
                        ) as i32,
                    ),
                    Location::GPR(GPR::RAX),
                );

                self.emit_call_native(
                    |this| {
                        this.assembler.emit_call_register(GPR::RAX);
                    },
                    // [vmctx, table_index, elem_index] -> reftype
                    [Location::Imm32(table_index.index() as u32), index]
                        .iter()
                        .cloned(),
                )?;

                self.machine
                    .release_locations_only_stack(&mut self.assembler, &[index]);

                let ret = self.machine.acquire_locations(
                    &mut self.assembler,
                    &[(WpType::FuncRef)],
                    false,
                )[0];
                self.value_stack.push(ret);
                self.assembler
                    .emit_mov(Size::S64, Location::GPR(GPR::RAX), ret);
            }
            Operator::TableSize { table: index } => {
                let table_index = TableIndex::new(index as _);

                self.assembler.emit_mov(
                    Size::S64,
                    Location::Memory(
                        Machine::get_vmctx_reg(),
                        self.vmoffsets.vmctx_builtin_function(
                            if self.module.local_table_index(table_index).is_some() {
                                VMBuiltinFunctionIndex::get_table_size_index()
                            } else {
                                VMBuiltinFunctionIndex::get_imported_table_size_index()
                            },
                        ) as i32,
                    ),
                    Location::GPR(GPR::RAX),
                );

                self.emit_call_native(
                    |this| {
                        this.assembler.emit_call_register(GPR::RAX);
                    },
                    // [vmctx, table_index] -> i32
                    iter::once(Location::Imm32(table_index.index() as u32)),
                )?;

                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];
                self.value_stack.push(ret);
                self.assembler
                    .emit_mov(Size::S32, Location::GPR(GPR::RAX), ret);
            }
            Operator::TableGrow { table: index } => {
                let table_index = TableIndex::new(index as _);
                let delta = self.value_stack.pop().unwrap();
                let init_value = self.value_stack.pop().unwrap();
                self.machine
                    .release_locations_only_regs(&[delta, init_value]);

                self.assembler.emit_mov(
                    Size::S64,
                    Location::Memory(
                        Machine::get_vmctx_reg(),
                        self.vmoffsets.vmctx_builtin_function(
                            if self.module.local_table_index(table_index).is_some() {
                                VMBuiltinFunctionIndex::get_table_grow_index()
                            } else {
                                VMBuiltinFunctionIndex::get_imported_table_get_index()
                            },
                        ) as i32,
                    ),
                    Location::GPR(GPR::RAX),
                );

                // TODO: should this be 2?
                self.emit_call_native(
                    |this| {
                        this.assembler.emit_call_register(GPR::RAX);
                    },
                    // [vmctx, init_value, delta, table_index] -> u32
                    [
                        init_value,
                        delta,
                        Location::Imm32(table_index.index() as u32),
                    ]
                    .iter()
                    .cloned(),
                )?;

                self.machine
                    .release_locations_only_stack(&mut self.assembler, &[init_value, delta]);

                let ret =
                    self.machine
                        .acquire_locations(&mut self.assembler, &[(WpType::I32)], false)[0];
                self.value_stack.push(ret);
                self.assembler
                    .emit_mov(Size::S32, Location::GPR(GPR::RAX), ret);
            }
            Operator::TableCopy {
                dst_table,
                src_table,
            } => {
                let len = self.value_stack.pop().unwrap();
                let src = self.value_stack.pop().unwrap();
                let dest = self.value_stack.pop().unwrap();
                self.machine.release_locations_only_regs(&[len, src, dest]);

                self.assembler.emit_mov(
                    Size::S64,
                    Location::Memory(
                        Machine::get_vmctx_reg(),
                        self.vmoffsets
                            .vmctx_builtin_function(VMBuiltinFunctionIndex::get_table_copy_index())
                            as i32,
                    ),
                    Location::GPR(GPR::RAX),
                );

                // TODO: should this be 3?
                self.emit_call_native(
                    |this| {
                        this.assembler.emit_call_register(GPR::RAX);
                    },
                    // [vmctx, dst_table_index, src_table_index, dst, src, len]
                    [
                        Location::Imm32(dst_table),
                        Location::Imm32(src_table),
                        dest,
                        src,
                        len,
                    ]
                    .iter()
                    .cloned(),
                )?;

                self.machine
                    .release_locations_only_stack(&mut self.assembler, &[dest, src, len]);
            }

            Operator::TableFill { table } => {
                let len = self.value_stack.pop().unwrap();
                let val = self.value_stack.pop().unwrap();
                let dest = self.value_stack.pop().unwrap();
                self.machine.release_locations_only_regs(&[len, val, dest]);

                self.assembler.emit_mov(
                    Size::S64,
                    Location::Memory(
                        Machine::get_vmctx_reg(),
                        self.vmoffsets
                            .vmctx_builtin_function(VMBuiltinFunctionIndex::get_table_fill_index())
                            as i32,
                    ),
                    Location::GPR(GPR::RAX),
                );

                // TODO: should this be 3?
                self.emit_call_native(
                    |this| {
                        this.assembler.emit_call_register(GPR::RAX);
                    },
                    // [vmctx, table_index, start_idx, item, len]
                    [Location::Imm32(table), dest, val, len].iter().cloned(),
                )?;

                self.machine
                    .release_locations_only_stack(&mut self.assembler, &[dest, val, len]);
            }
            Operator::TableInit { segment, table } => {
                let len = self.value_stack.pop().unwrap();
                let src = self.value_stack.pop().unwrap();
                let dest = self.value_stack.pop().unwrap();
                self.machine.release_locations_only_regs(&[len, src, dest]);

                self.assembler.emit_mov(
                    Size::S64,
                    Location::Memory(
                        Machine::get_vmctx_reg(),
                        self.vmoffsets
                            .vmctx_builtin_function(VMBuiltinFunctionIndex::get_table_init_index())
                            as i32,
                    ),
                    Location::GPR(GPR::RAX),
                );

                // TODO: should this be 3?
                self.emit_call_native(
                    |this| {
                        this.assembler.emit_call_register(GPR::RAX);
                    },
                    // [vmctx, table_index, elem_index, dst, src, len]
                    [
                        Location::Imm32(table),
                        Location::Imm32(segment),
                        dest,
                        src,
                        len,
                    ]
                    .iter()
                    .cloned(),
                )?;

                self.machine
                    .release_locations_only_stack(&mut self.assembler, &[dest, src, len]);
            }
            Operator::ElemDrop { segment } => {
                self.assembler.emit_mov(
                    Size::S64,
                    Location::Memory(
                        Machine::get_vmctx_reg(),
                        self.vmoffsets
                            .vmctx_builtin_function(VMBuiltinFunctionIndex::get_elem_drop_index())
                            as i32,
                    ),
                    Location::GPR(GPR::RAX),
                );

                self.emit_call_native(
                    |this| {
                        this.assembler.emit_call_register(GPR::RAX);
                    },
                    // [vmctx, elem_index]
                    [Location::Imm32(segment)].iter().cloned(),
                )?;
            }
            _ => {
                return Err(CodegenError {
                    message: format!("not yet implemented: {:?}", op),
                });
            }
        }

        Ok(())
    }

    pub(crate) fn finalize(mut self, data: &FunctionBodyData) -> CompiledFunction {
        // Generate actual code for special labels.
        self.assembler
            .emit_label(self.special_labels.integer_division_by_zero);
        self.emit_trap(TrapCode::IntegerDivisionByZero);

        self.assembler
            .emit_label(self.special_labels.integer_overflow);
        self.emit_trap(TrapCode::IntegerOverflow);

        self.assembler
            .emit_label(self.special_labels.bad_conversion_to_integer);
        self.emit_trap(TrapCode::BadConversionToInteger);

        self.assembler
            .emit_label(self.special_labels.heap_access_oob);
        self.emit_trap(TrapCode::HeapAccessOutOfBounds);

        self.assembler
            .emit_label(self.special_labels.table_access_oob);
        self.emit_trap(TrapCode::TableAccessOutOfBounds);

        self.assembler
            .emit_label(self.special_labels.indirect_call_null);
        self.emit_trap(TrapCode::IndirectCallToNull);

        self.assembler.emit_label(self.special_labels.bad_signature);
        self.emit_trap(TrapCode::BadSignature);

        self.assembler
            .emit_label(self.special_labels.gas_limit_exceeded);
        self.emit_trap(TrapCode::GasExceeded);

        self.assembler
            .emit_label(self.special_labels.stack_overflow);
        self.emit_trap(TrapCode::StackOverflow);

        // Notify the assembler backend to generate necessary code at end of function.
        self.assembler.finalize_function();

        let body_len = self.assembler.get_offset().0;
        let instructions_address_map = self.instructions_address_map;
        let address_map = get_function_address_map(instructions_address_map, data, body_len);

        CompiledFunction {
            body: FunctionBody {
                body: self.assembler.finalize().unwrap().to_vec(),
                unwind_info: None,
            },
            relocations: self.relocations,
            jt_offsets: SecondaryMap::new(),
            frame_info: CompiledFunctionFrameInfo {
                traps: vec![],
                address_map,
            },
        }
    }
}

fn type_to_wp_type(ty: Type) -> WpType {
    match ty {
        Type::I32 => WpType::I32,
        Type::I64 => WpType::I64,
        Type::F32 => WpType::F32,
        Type::F64 => WpType::F64,
        Type::V128 => WpType::V128,
        Type::ExternRef => WpType::ExternRef,
        Type::FuncRef => WpType::FuncRef, // TODO: FuncRef or Func?
    }
}

// FIXME: This implementation seems to be not enough to resolve all kinds of register dependencies
// at call place.
fn sort_call_movs(movs: &mut [(Location, GPR)]) {
    for i in 0..movs.len() {
        for j in (i + 1)..movs.len() {
            if let Location::GPR(src_gpr) = movs[j].0 {
                if src_gpr == movs[i].1 {
                    movs.swap(i, j);
                }
            }
        }
    }

    // Cycle detector. Uncomment this to debug possibly incorrect call-mov sequences.
    /*
    {
        use std::collections::{HashMap, HashSet, VecDeque};
        let mut mov_map: HashMap<GPR, HashSet<GPR>> = HashMap::new();
        for mov in movs.iter() {
            if let Location::GPR(src_gpr) = mov.0 {
                if src_gpr != mov.1 {
                    mov_map.entry(src_gpr).or_insert_with(|| HashSet::new()).insert(mov.1);
                }
            }
        }

        for (start, _) in mov_map.iter() {
            let mut q: VecDeque<GPR> = VecDeque::new();
            let mut black: HashSet<GPR> = HashSet::new();

            q.push_back(*start);
            black.insert(*start);

            while q.len() > 0 {
                let reg = q.pop_front().unwrap();
                let empty_set = HashSet::new();
                for x in mov_map.get(&reg).unwrap_or(&empty_set).iter() {
                    if black.contains(x) {
                        panic!("cycle detected");
                    }
                    q.push_back(*x);
                    black.insert(*x);
                }
            }
        }
    }
    */
}

// Standard entry trampoline.
pub(crate) fn gen_std_trampoline(
    sig: &FunctionType,
    calling_convention: CallingConvention,
) -> FunctionBody {
    let mut a = Assembler::new(0);

    // Calculate stack offset.
    let mut stack_offset: u32 = 0;
    for (i, _param) in sig.params().iter().enumerate() {
        if let Location::Memory(_, _) = Machine::get_param_location(1 + i, calling_convention) {
            stack_offset += 8;
        }
    }
    let stack_padding: u32 = match calling_convention {
        CallingConvention::WindowsFastcall => 32,
        _ => 0,
    };

    // Align to 16 bytes. We push two 8-byte registers below, so here we need to ensure stack_offset % 16 == 8.
    if stack_offset % 16 != 8 {
        stack_offset += 8;
    }

    // Used callee-saved registers
    a.emit_push(Size::S64, Location::GPR(GPR::R15));
    a.emit_push(Size::S64, Location::GPR(GPR::R14));

    // Prepare stack space.
    a.emit_sub(
        Size::S64,
        Location::Imm32(stack_offset + stack_padding),
        Location::GPR(GPR::RSP),
    );

    // Arguments
    a.emit_mov(
        Size::S64,
        Machine::get_param_location(1, calling_convention),
        Location::GPR(GPR::R15),
    ); // func_ptr
    a.emit_mov(
        Size::S64,
        Machine::get_param_location(2, calling_convention),
        Location::GPR(GPR::R14),
    ); // args_rets

    // Move arguments to their locations.
    // `callee_vmctx` is already in the first argument register, so no need to move.
    {
        let mut n_stack_args: usize = 0;
        for (i, _param) in sig.params().iter().enumerate() {
            let src_loc = Location::Memory(GPR::R14, (i * 16) as _); // args_rets[i]
            let dst_loc = Machine::get_param_location(1 + i, calling_convention);

            match dst_loc {
                Location::GPR(_) => {
                    a.emit_mov(Size::S64, src_loc, dst_loc);
                }
                Location::Memory(_, _) => {
                    // This location is for reading arguments but we are writing arguments here.
                    // So recalculate it.
                    a.emit_mov(Size::S64, src_loc, Location::GPR(GPR::RAX));
                    a.emit_mov(
                        Size::S64,
                        Location::GPR(GPR::RAX),
                        Location::Memory(
                            GPR::RSP,
                            (stack_padding as usize + n_stack_args * 8) as _,
                        ),
                    );
                    n_stack_args += 1;
                }
                _ => unreachable!(),
            }
        }
    }

    // Call.
    a.emit_call_location(Location::GPR(GPR::R15));

    // Restore stack.
    a.emit_add(
        Size::S64,
        Location::Imm32(stack_offset + stack_padding),
        Location::GPR(GPR::RSP),
    );

    // Write return value.
    if !sig.results().is_empty() {
        a.emit_mov(
            Size::S64,
            Location::GPR(GPR::RAX),
            Location::Memory(GPR::R14, 0),
        );
    }

    // Restore callee-saved registers.
    a.emit_pop(Size::S64, Location::GPR(GPR::R14));
    a.emit_pop(Size::S64, Location::GPR(GPR::R15));

    a.emit_ret();

    FunctionBody {
        body: a.finalize().unwrap().to_vec(),
        unwind_info: None,
    }
}

/// Generates dynamic import function call trampoline for a function type.
pub(crate) fn gen_std_dynamic_import_trampoline(
    vmoffsets: &VMOffsets,
    sig: &FunctionType,
    calling_convention: CallingConvention,
) -> FunctionBody {
    let mut a = Assembler::new(0);

    // Allocate argument array.
    let stack_offset: usize = 16 * std::cmp::max(sig.params().len(), sig.results().len()) + 8; // 16 bytes each + 8 bytes sysv call padding
    let stack_padding: usize = match calling_convention {
        CallingConvention::WindowsFastcall => 32,
        _ => 0,
    };
    a.emit_sub(
        Size::S64,
        Location::Imm32((stack_offset + stack_padding) as _),
        Location::GPR(GPR::RSP),
    );

    // Copy arguments.
    if !sig.params().is_empty() {
        let mut argalloc = ArgumentRegisterAllocator::default();
        argalloc.next(Type::I64, calling_convention).unwrap(); // skip VMContext

        let mut stack_param_count: usize = 0;

        for (i, ty) in sig.params().iter().enumerate() {
            let source_loc = match argalloc.next(*ty, calling_convention) {
                Some(X64Register::GPR(gpr)) => Location::GPR(gpr),
                Some(X64Register::XMM(xmm)) => Location::XMM(xmm),
                None => {
                    a.emit_mov(
                        Size::S64,
                        Location::Memory(
                            GPR::RSP,
                            (stack_padding * 2 + stack_offset + 8 + stack_param_count * 8) as _,
                        ),
                        Location::GPR(GPR::RAX),
                    );
                    stack_param_count += 1;
                    Location::GPR(GPR::RAX)
                }
            };
            a.emit_mov(
                Size::S64,
                source_loc,
                Location::Memory(GPR::RSP, (stack_padding + i * 16) as _),
            );

            // Zero upper 64 bits.
            a.emit_mov(
                Size::S64,
                Location::Imm32(0),
                Location::Memory(GPR::RSP, (stack_padding + i * 16 + 8) as _),
            );
        }
    }

    match calling_convention {
        CallingConvention::WindowsFastcall => {
            // Load target address.
            a.emit_mov(
                Size::S64,
                Location::Memory(
                    GPR::RCX,
                    vmoffsets.vmdynamicfunction_import_context_address() as i32,
                ),
                Location::GPR(GPR::RAX),
            );
            // Load values array.
            a.emit_lea(
                Size::S64,
                Location::Memory(GPR::RSP, stack_padding as i32),
                Location::GPR(GPR::RDX),
            );
        }
        _ => {
            // Load target address.
            a.emit_mov(
                Size::S64,
                Location::Memory(
                    GPR::RDI,
                    vmoffsets.vmdynamicfunction_import_context_address() as i32,
                ),
                Location::GPR(GPR::RAX),
            );
            // Load values array.
            a.emit_mov(Size::S64, Location::GPR(GPR::RSP), Location::GPR(GPR::RSI));
        }
    };

    // Call target.
    a.emit_call_location(Location::GPR(GPR::RAX));

    // Fetch return value.
    if !sig.results().is_empty() {
        assert_eq!(sig.results().len(), 1);
        a.emit_mov(
            Size::S64,
            Location::Memory(GPR::RSP, stack_padding as i32),
            Location::GPR(GPR::RAX),
        );
    }

    // Release values array.
    a.emit_add(
        Size::S64,
        Location::Imm32((stack_offset + stack_padding) as _),
        Location::GPR(GPR::RSP),
    );

    // Return.
    a.emit_ret();

    FunctionBody {
        body: a.finalize().unwrap().to_vec(),
        unwind_info: None,
    }
}

// Singlepass calls import functions through a trampoline.
pub(crate) fn gen_import_call_trampoline(
    vmoffsets: &VMOffsets,
    index: FunctionIndex,
    sig: &FunctionType,
    calling_convention: CallingConvention,
) -> CustomSection {
    let mut a = Assembler::new(0);

    // TODO: ARM entry trampoline is not emitted.

    // Singlepass internally treats all arguments as integers
    // For the standard Windows calling convention requires
    //  floating point arguments to be passed in XMM registers for the 4 first arguments only
    //  That's the only change to do, other arguments are not to be changed
    // For the standard System V calling convention requires
    //  floating point arguments to be passed in XMM registers.
    //  Translation is expensive, so only do it if needed.
    if sig
        .params()
        .iter()
        .any(|&x| x == Type::F32 || x == Type::F64)
    {
        match calling_convention {
            CallingConvention::WindowsFastcall => {
                let mut param_locations: Vec<Location> = vec![];
                for i in 0..sig.params().len() {
                    let loc = match i {
                        0..=2 => {
                            static PARAM_REGS: &[GPR] = &[GPR::RDX, GPR::R8, GPR::R9];
                            Location::GPR(PARAM_REGS[i])
                        }
                        _ => Location::Memory(GPR::RSP, 32 + 8 + ((i - 3) * 8) as i32), // will not be used anyway
                    };
                    param_locations.push(loc);
                }
                // Copy Float arguments to XMM from GPR.
                let mut argalloc = ArgumentRegisterAllocator::default();
                for (i, ty) in sig.params().iter().enumerate() {
                    let prev_loc = param_locations[i];
                    match argalloc.next(*ty, calling_convention) {
                        Some(X64Register::GPR(_gpr)) => continue,
                        Some(X64Register::XMM(xmm)) => {
                            a.emit_mov(Size::S64, prev_loc, Location::XMM(xmm))
                        }
                        None => continue,
                    };
                }
            }
            _ => {
                let mut param_locations: Vec<Location> = vec![];

                // Allocate stack space for arguments.
                let stack_offset: i32 = if sig.params().len() > 5 {
                    5 * 8
                } else {
                    (sig.params().len() as i32) * 8
                };
                if stack_offset > 0 {
                    a.emit_sub(
                        Size::S64,
                        Location::Imm32(stack_offset as u32),
                        Location::GPR(GPR::RSP),
                    );
                }

                // Store all arguments to the stack to prevent overwrite.
                for i in 0..sig.params().len() {
                    let loc = match i {
                        0..=4 => {
                            static PARAM_REGS: &[GPR] =
                                &[GPR::RSI, GPR::RDX, GPR::RCX, GPR::R8, GPR::R9];
                            let loc = Location::Memory(GPR::RSP, (i * 8) as i32);
                            a.emit_mov(Size::S64, Location::GPR(PARAM_REGS[i]), loc);
                            loc
                        }
                        _ => Location::Memory(GPR::RSP, stack_offset + 8 + ((i - 5) * 8) as i32),
                    };
                    param_locations.push(loc);
                }

                // Copy arguments.
                let mut argalloc = ArgumentRegisterAllocator::default();
                argalloc.next(Type::I64, calling_convention).unwrap(); // skip VMContext
                let mut caller_stack_offset: i32 = 0;
                for (i, ty) in sig.params().iter().enumerate() {
                    let prev_loc = param_locations[i];
                    let targ = match argalloc.next(*ty, calling_convention) {
                        Some(X64Register::GPR(gpr)) => Location::GPR(gpr),
                        Some(X64Register::XMM(xmm)) => Location::XMM(xmm),
                        None => {
                            // No register can be allocated. Put this argument on the stack.
                            //
                            // Since here we never use fewer registers than by the original call, on the caller's frame
                            // we always have enough space to store the rearranged arguments, and the copy "backward" between different
                            // slots in the caller argument region will always work.
                            a.emit_mov(Size::S64, prev_loc, Location::GPR(GPR::RAX));
                            a.emit_mov(
                                Size::S64,
                                Location::GPR(GPR::RAX),
                                Location::Memory(GPR::RSP, stack_offset + 8 + caller_stack_offset),
                            );
                            caller_stack_offset += 8;
                            continue;
                        }
                    };
                    a.emit_mov(Size::S64, prev_loc, targ);
                }

                // Restore stack pointer.
                if stack_offset > 0 {
                    a.emit_add(
                        Size::S64,
                        Location::Imm32(stack_offset as u32),
                        Location::GPR(GPR::RSP),
                    );
                }
            }
        }
    }

    // Emits a tail call trampoline that loads the address of the target import function
    // from Ctx and jumps to it.

    let body_offset = vmoffsets.vmctx_vmfunction_import_body(index);
    let vmctx_offset = vmoffsets.vmctx_vmfunction_import_vmctx(index);

    match calling_convention {
        CallingConvention::WindowsFastcall => {
            a.emit_mov(
                Size::S64,
                Location::Memory(GPR::RCX, body_offset as i32), // function pointer
                Location::GPR(GPR::RAX),
            );
            a.emit_mov(
                Size::S64,
                Location::Memory(GPR::RCX, vmctx_offset as i32), // target vmctx
                Location::GPR(GPR::RCX),
            );
        }
        _ => {
            a.emit_mov(
                Size::S64,
                Location::Memory(GPR::RDI, body_offset as i32), // function pointer
                Location::GPR(GPR::RAX),
            );
            a.emit_mov(
                Size::S64,
                Location::Memory(GPR::RDI, vmctx_offset as i32), // target vmctx
                Location::GPR(GPR::RDI),
            );
        }
    }
    a.emit_host_redirection(GPR::RAX);

    let section_body = SectionBody::new_with_vec(a.finalize().unwrap().to_vec());

    CustomSection {
        protection: CustomSectionProtection::ReadExecute,
        bytes: section_body,
        relocations: vec![],
    }
}

// Constants for the bounds of truncation operations. These are the least or
// greatest exact floats in either f32 or f64 representation less-than (for
// least) or greater-than (for greatest) the i32 or i64 or u32 or u64
// min (for least) or max (for greatest), when rounding towards zero.

/// Greatest Exact Float (32 bits) less-than i32::MIN when rounding towards zero.
const GEF32_LT_I32_MIN: f32 = -2147483904.0;
/// Least Exact Float (32 bits) greater-than i32::MAX when rounding towards zero.
const LEF32_GT_I32_MAX: f32 = 2147483648.0;
/// Greatest Exact Float (32 bits) less-than i64::MIN when rounding towards zero.
const GEF32_LT_I64_MIN: f32 = -9223373136366403584.0;
/// Least Exact Float (32 bits) greater-than i64::MAX when rounding towards zero.
const LEF32_GT_I64_MAX: f32 = 9223372036854775808.0;
/// Greatest Exact Float (32 bits) less-than u32::MIN when rounding towards zero.
const GEF32_LT_U32_MIN: f32 = -1.0;
/// Least Exact Float (32 bits) greater-than u32::MAX when rounding towards zero.
const LEF32_GT_U32_MAX: f32 = 4294967296.0;
/// Greatest Exact Float (32 bits) less-than u64::MIN when rounding towards zero.
const GEF32_LT_U64_MIN: f32 = -1.0;
/// Least Exact Float (32 bits) greater-than u64::MAX when rounding towards zero.
const LEF32_GT_U64_MAX: f32 = 18446744073709551616.0;

/// Greatest Exact Float (64 bits) less-than i32::MIN when rounding towards zero.
const GEF64_LT_I32_MIN: f64 = -2147483649.0;
/// Least Exact Float (64 bits) greater-than i32::MAX when rounding towards zero.
const LEF64_GT_I32_MAX: f64 = 2147483648.0;
/// Greatest Exact Float (64 bits) less-than i64::MIN when rounding towards zero.
const GEF64_LT_I64_MIN: f64 = -9223372036854777856.0;
/// Least Exact Float (64 bits) greater-than i64::MAX when rounding towards zero.
const LEF64_GT_I64_MAX: f64 = 9223372036854775808.0;
/// Greatest Exact Float (64 bits) less-than u32::MIN when rounding towards zero.
const GEF64_LT_U32_MIN: f64 = -1.0;
/// Least Exact Float (64 bits) greater-than u32::MAX when rounding towards zero.
const LEF64_GT_U32_MAX: f64 = 4294967296.0;
/// Greatest Exact Float (64 bits) less-than u64::MIN when rounding towards zero.
const GEF64_LT_U64_MIN: f64 = -1.0;
/// Least Exact Float (64 bits) greater-than u64::MAX when rounding towards zero.
const LEF64_GT_U64_MAX: f64 = 18446744073709551616.0;

'''
'''--- lib/compiler-singlepass/src/compiler.rs ---
//! Support for compiling with Singlepass.
// Allow unused imports while developing.
#![allow(unused_imports, dead_code)]

use crate::codegen_x64::{
    gen_import_call_trampoline, gen_std_dynamic_import_trampoline, gen_std_trampoline,
    CodegenError, FuncGen,
};
use crate::config::Singlepass;
#[cfg(feature = "rayon")]
use rayon::prelude::{IntoParallelIterator, ParallelIterator};
use std::sync::Arc;
use wasmer_compiler::{
    Architecture, CallingConvention, Compilation, CompileError, CompileModuleInfo,
    CompiledFunction, Compiler, CompilerConfig, CpuFeature, FunctionBody, FunctionBodyData,
    ModuleTranslationState, OperatingSystem, SectionIndex, Target, TrapInformation,
};
use wasmer_types::entity::{EntityRef, PrimaryMap};
use wasmer_types::{
    FunctionIndex, FunctionType, LocalFunctionIndex, MemoryIndex, ModuleInfo, TableIndex,
};
use wasmer_vm::{TrapCode, VMOffsets};

/// A compiler that compiles a WebAssembly module with Singlepass.
/// It does the compilation in one pass
pub struct SinglepassCompiler {
    config: Singlepass,
}

impl SinglepassCompiler {
    /// Creates a new Singlepass compiler
    pub fn new(config: Singlepass) -> Self {
        Self { config }
    }

    /// Gets the config for this Compiler
    fn config(&self) -> &Singlepass {
        &self.config
    }
}

impl Compiler for SinglepassCompiler {
    /// Compile the module using Singlepass, producing a compilation result with
    /// associated relocations.
    fn compile_module(
        &self,
        target: &Target,
        compile_info: &CompileModuleInfo,
        module_translation: &ModuleTranslationState,
        function_body_inputs: PrimaryMap<LocalFunctionIndex, FunctionBodyData<'_>>,
    ) -> Result<Compilation, CompileError> {
        /*if target.triple().operating_system == OperatingSystem::Windows {
            return Err(CompileError::UnsupportedTarget(
                OperatingSystem::Windows.to_string(),
            ));
        }*/
        if target.triple().architecture != Architecture::X86_64 {
            return Err(CompileError::UnsupportedTarget(
                target.triple().architecture.to_string(),
            ));
        }
        if !target.cpu_features().contains(CpuFeature::AVX) {
            return Err(CompileError::UnsupportedTarget(
                "x86_64 without AVX".to_string(),
            ));
        }
        if compile_info.features.multi_value {
            return Err(CompileError::UnsupportedFeature("multivalue".to_string()));
        }
        let calling_convention = match target.triple().default_calling_convention() {
            Ok(CallingConvention::WindowsFastcall) => CallingConvention::WindowsFastcall,
            Ok(CallingConvention::SystemV) => CallingConvention::SystemV,
            //Ok(CallingConvention::AppleAarch64) => AppleAarch64,
            _ => panic!("Unsupported Calling convention for Singlepass compiler"),
        };

        let table_styles = &compile_info.table_styles;
        let module = &compile_info.module;
        let pointer_width = target
            .triple()
            .pointer_width()
            .map_err(|()| {
                CompileError::UnsupportedTarget("target with unknown pointer width".into())
            })?
            .bytes();
        let vmoffsets = VMOffsets::new(pointer_width).with_module_info(&module);
        let import_idxs = 0..module.import_counts.functions as usize;
        let import_trampolines: PrimaryMap<SectionIndex, _> = import_idxs
            .into_par_iter_if_rayon()
            .map(|i| {
                let i = FunctionIndex::new(i);
                gen_import_call_trampoline(
                    &vmoffsets,
                    i,
                    &module.signatures[module.functions[i]],
                    calling_convention,
                )
            })
            .collect::<Vec<_>>()
            .into_iter()
            .collect();
        let functions = function_body_inputs
            .iter()
            .collect::<Vec<(LocalFunctionIndex, &FunctionBodyData<'_>)>>()
            .into_par_iter_if_rayon()
            .map(|(i, input)| {
                let reader = wasmer_compiler::FunctionReader::new(input.module_offset, input.data);

                let mut local_reader = reader.get_locals_reader()?;
                // This local list excludes arguments.
                let mut locals = vec![];
                let num_locals = local_reader.get_count();
                for _ in 0..num_locals {
                    let (count, ty) = local_reader.read()?;
                    for _ in 0..count {
                        locals.push(ty);
                    }
                }

                let mut generator = FuncGen::new(
                    module,
                    module_translation,
                    &self.config,
                    &vmoffsets,
                    &table_styles,
                    i,
                    &locals,
                    calling_convention,
                )
                .map_err(to_compile_error)?;

                let mut operator_reader = reader.get_operators_reader()?.into_iter_with_offsets();
                while generator.has_control_frames() {
                    let (op, pos) = operator_reader.next().unwrap()?;
                    generator.set_srcloc(pos as u32);
                    generator.feed_operator(op).map_err(to_compile_error)?;
                }

                Ok(generator.finalize(&input))
            })
            .collect::<Result<Vec<CompiledFunction>, CompileError>>()?
            .into_iter()
            .collect::<PrimaryMap<LocalFunctionIndex, CompiledFunction>>();

        let function_call_trampolines = module
            .signatures
            .values()
            .collect::<Vec<_>>()
            .into_par_iter_if_rayon()
            .map(|func_type| gen_std_trampoline(&func_type, calling_convention))
            .collect::<Vec<_>>()
            .into_iter()
            .collect::<PrimaryMap<_, _>>();

        let dynamic_function_trampolines = module
            .imported_function_types()
            .collect::<Vec<_>>()
            .into_par_iter_if_rayon()
            .map(|func_type| {
                gen_std_dynamic_import_trampoline(&vmoffsets, &func_type, calling_convention)
            })
            .collect::<Vec<_>>()
            .into_iter()
            .collect::<PrimaryMap<FunctionIndex, FunctionBody>>();

        Ok(Compilation::new(
            functions,
            import_trampolines,
            function_call_trampolines,
            dynamic_function_trampolines,
            None,
            None,
        ))
    }
}

trait ToCompileError {
    fn to_compile_error(self) -> CompileError;
}

impl ToCompileError for CodegenError {
    fn to_compile_error(self) -> CompileError {
        CompileError::Codegen(self.message)
    }
}

fn to_compile_error<T: ToCompileError>(x: T) -> CompileError {
    x.to_compile_error()
}

trait IntoParIterIfRayon {
    type Output;
    fn into_par_iter_if_rayon(self) -> Self::Output;
}

#[cfg(feature = "rayon")]
impl<T: IntoParallelIterator + IntoIterator> IntoParIterIfRayon for T {
    type Output = <T as IntoParallelIterator>::Iter;
    fn into_par_iter_if_rayon(self) -> Self::Output {
        return self.into_par_iter();
    }
}

#[cfg(not(feature = "rayon"))]
impl<T: IntoIterator> IntoParIterIfRayon for T {
    type Output = <T as IntoIterator>::IntoIter;
    fn into_par_iter_if_rayon(self) -> Self::Output {
        return self.into_iter();
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::str::FromStr;
    use target_lexicon::triple;
    use wasmer_compiler::{CpuFeature, Features, Triple};
    use wasmer_vm::{MemoryStyle, TableStyle};

    fn dummy_compilation_ingredients<'a>() -> (
        CompileModuleInfo,
        ModuleTranslationState,
        PrimaryMap<LocalFunctionIndex, FunctionBodyData<'a>>,
    ) {
        let compile_info = CompileModuleInfo {
            features: Features::new(),
            module: Arc::new(ModuleInfo::new()),
            memory_styles: PrimaryMap::<MemoryIndex, MemoryStyle>::new(),
            table_styles: PrimaryMap::<TableIndex, TableStyle>::new(),
        };
        let module_translation = ModuleTranslationState::new();
        let function_body_inputs = PrimaryMap::<LocalFunctionIndex, FunctionBodyData<'_>>::new();
        (compile_info, module_translation, function_body_inputs)
    }

    #[test]
    fn errors_for_unsupported_targets() {
        let compiler = SinglepassCompiler::new(Singlepass::default());

        // Compile for win64
        /*let win64 = Target::new(triple!("x86_64-pc-windows-msvc"), CpuFeature::for_host());
        let (mut info, translation, inputs) = dummy_compilation_ingredients();
        let result = compiler.compile_module(&win64, &mut info, &translation, inputs);
        match result.unwrap_err() {
            CompileError::UnsupportedTarget(name) => assert_eq!(name, "windows"),
            error => panic!("Unexpected error: {:?}", error),
        };*/

        // Compile for 32bit Linux
        let linux32 = Target::new(triple!("i686-unknown-linux-gnu"), CpuFeature::for_host());
        let (mut info, translation, inputs) = dummy_compilation_ingredients();
        let result = compiler.compile_module(&linux32, &mut info, &translation, inputs);
        match result.unwrap_err() {
            CompileError::UnsupportedTarget(name) => assert_eq!(name, "i686"),
            error => panic!("Unexpected error: {:?}", error),
        };

        // Compile for win32
        let win32 = Target::new(triple!("i686-pc-windows-gnu"), CpuFeature::for_host());
        let (mut info, translation, inputs) = dummy_compilation_ingredients();
        let result = compiler.compile_module(&win32, &mut info, &translation, inputs);
        match result.unwrap_err() {
            CompileError::UnsupportedTarget(name) => assert_eq!(name, "i686"), // Windows should be checked before architecture
            error => panic!("Unexpected error: {:?}", error),
        };
    }
}

'''
'''--- lib/compiler-singlepass/src/config.rs ---
// Allow unused imports while developing
#![allow(unused_imports, dead_code)]

use crate::compiler::SinglepassCompiler;
use crate::emitter_x64::Location;
use smallvec::SmallVec;
use std::sync::Arc;
use wasmer_compiler::{Compiler, CompilerConfig, CpuFeature, Target};
use wasmer_types::{Features, FunctionType, Type};

#[derive(Debug, Clone)]
pub(crate) enum IntrinsicKind {
    Gas,
}

#[derive(Debug, Clone)]
pub(crate) struct Intrinsic {
    pub(crate) kind: IntrinsicKind,
    pub(crate) name: String,
    pub(crate) signature: FunctionType,
}

#[derive(Debug, Clone)]
pub struct Singlepass {
    pub(crate) enable_nan_canonicalization: bool,
    pub(crate) enable_stack_check: bool,
    /// Compiler intrinsics.
    pub(crate) intrinsics: Vec<Intrinsic>,
}

impl Singlepass {
    /// Creates a new configuration object with the default configuration
    /// specified.
    pub fn new() -> Self {
        Self {
            enable_nan_canonicalization: true,
            enable_stack_check: false,
            intrinsics: vec![Intrinsic {
                kind: IntrinsicKind::Gas,
                name: "gas".to_string(),
                signature: ([Type::I32], []).into(),
            }],
        }
    }

    /// Enable stack check.
    ///
    /// When enabled, an explicit stack depth check will be performed on entry
    /// to each function to prevent stack overflow.
    ///
    /// Note that this doesn't guarantee deterministic execution across
    /// different platforms.
    pub fn enable_stack_check(&mut self, enable: bool) -> &mut Self {
        self.enable_stack_check = enable;
        self
    }

    fn enable_nan_canonicalization(&mut self) {
        self.enable_nan_canonicalization = true;
    }

    pub fn canonicalize_nans(&mut self, enable: bool) -> &mut Self {
        self.enable_nan_canonicalization = enable;
        self
    }
}

impl CompilerConfig for Singlepass {
    fn enable_pic(&mut self) {
        // Do nothing, since singlepass already emits
        // PIC code.
    }

    /// Transform it into the compiler
    fn compiler(self: Box<Self>) -> Box<dyn Compiler> {
        Box::new(SinglepassCompiler::new(*self))
    }

    /// Gets the default features for this compiler in the given target
    fn default_features_for_target(&self, _target: &Target) -> Features {
        let mut features = Features::default();
        features.multi_value(false);
        features
    }
}

impl Default for Singlepass {
    fn default() -> Singlepass {
        Self::new()
    }
}

impl Intrinsic {
    pub(crate) fn is_params_ok(&self, params: &SmallVec<[Location; 8]>) -> bool {
        match self.kind {
            IntrinsicKind::Gas => match params[0] {
                Location::Imm32(value) => value < i32::MAX as u32,
                _ => false,
            },
        }
    }
}

'''
'''--- lib/compiler-singlepass/src/emitter_x64.rs ---
pub(crate) use crate::x64_decl::{GPR, XMM};
use dynasm::dynasm;
use dynasmrt::{
    x64::X64Relocation, AssemblyOffset, DynamicLabel, DynasmApi, DynasmLabelApi, VecAssembler,
};

type Assembler = VecAssembler<X64Relocation>;

/// Force `dynasm!` to use the correct arch (x64) when cross-compiling.
/// `dynasm!` proc-macro tries to auto-detect it by default by looking at the
/// `target_arch`, but it sees the `target_arch` of the proc-macro itself, which
/// is always equal to host, even when cross-compiling.
macro_rules! dynasm {
    ($a:expr ; $($tt:tt)*) => {
        dynasm::dynasm!(
            $a
            ; .arch x64
            ; $($tt)*
        )
    };
}

#[derive(Copy, Clone, Debug, Eq, PartialEq, Ord, PartialOrd, Hash)]
pub(crate) enum Location {
    Imm8(u8),
    Imm32(u32),
    Imm64(u64),
    // Imm128(u128),
    GPR(GPR),
    XMM(XMM),
    Memory(GPR, i32),
    MemoryAddTriple(GPR, GPR, i32),
}

#[derive(Copy, Clone, Debug, Eq, PartialEq)]
pub(crate) enum Condition {
    None,
    Above,
    AboveEqual,
    Below,
    BelowEqual,
    Greater,
    GreaterEqual,
    Less,
    LessEqual,
    Equal,
    NotEqual,
    Signed,
    Carry,
    Overflow,
}

#[derive(Copy, Clone, Debug, Eq, PartialEq, Ord, PartialOrd)]
pub(crate) enum Size {
    S8,
    S16,
    S32,
    S64,
}

#[derive(Copy, Clone, Debug, Eq, PartialEq)]
#[allow(dead_code)]
pub(crate) enum XMMOrMemory {
    XMM(XMM),
    Memory(GPR, i32),
}

#[derive(Copy, Clone, Debug)]
#[allow(dead_code)]
pub(crate) enum GPROrMemory {
    GPR(GPR),
    Memory(GPR, i32),
}

pub(crate) trait Emitter {
    type Label;
    type Offset;

    fn get_label(&mut self) -> Self::Label;
    fn get_offset(&self) -> Self::Offset;
    fn get_jmp_instr_size(&self) -> u8;

    fn finalize_function(&mut self) {}

    fn emit_u64(&mut self, x: u64);
    fn emit_bytes(&mut self, bytes: &[u8]);

    fn emit_label(&mut self, label: Self::Label);

    fn emit_nop(&mut self);

    /// A high-level assembler method. Emits an instruction sequence of length `n` that is functionally
    /// equivalent to a `nop` instruction, without guarantee about the underlying implementation.
    fn emit_nop_n(&mut self, n: usize);

    fn emit_mov(&mut self, sz: Size, src: Location, dst: Location);
    fn emit_lea(&mut self, sz: Size, src: Location, dst: Location);
    fn emit_lea_label(&mut self, label: Self::Label, dst: Location);
    fn emit_cdq(&mut self);
    fn emit_cqo(&mut self);
    fn emit_xor(&mut self, sz: Size, src: Location, dst: Location);
    fn emit_jmp(&mut self, condition: Condition, label: Self::Label);
    fn emit_jmp_location(&mut self, loc: Location);
    fn emit_set(&mut self, condition: Condition, dst: GPR);
    fn emit_push(&mut self, sz: Size, src: Location);
    fn emit_pop(&mut self, sz: Size, dst: Location);
    fn emit_cmp(&mut self, sz: Size, left: Location, right: Location);
    fn emit_add(&mut self, sz: Size, src: Location, dst: Location);
    fn emit_sub(&mut self, sz: Size, src: Location, dst: Location);
    fn emit_neg(&mut self, sz: Size, value: Location);
    fn emit_imul(&mut self, sz: Size, src: Location, dst: Location);
    fn emit_imul_imm32_gpr64(&mut self, src: u32, dst: GPR);
    fn emit_div(&mut self, sz: Size, divisor: Location);
    fn emit_idiv(&mut self, sz: Size, divisor: Location);
    fn emit_shl(&mut self, sz: Size, src: Location, dst: Location);
    fn emit_shr(&mut self, sz: Size, src: Location, dst: Location);
    fn emit_sar(&mut self, sz: Size, src: Location, dst: Location);
    fn emit_rol(&mut self, sz: Size, src: Location, dst: Location);
    fn emit_ror(&mut self, sz: Size, src: Location, dst: Location);
    fn emit_and(&mut self, sz: Size, src: Location, dst: Location);
    fn emit_or(&mut self, sz: Size, src: Location, dst: Location);
    fn emit_bsr(&mut self, sz: Size, src: Location, dst: Location);
    fn emit_bsf(&mut self, sz: Size, src: Location, dst: Location);
    fn emit_popcnt(&mut self, sz: Size, src: Location, dst: Location);
    fn emit_movzx(&mut self, sz_src: Size, src: Location, sz_dst: Size, dst: Location);
    fn emit_movsx(&mut self, sz_src: Size, src: Location, sz_dst: Size, dst: Location);
    fn emit_xchg(&mut self, sz: Size, src: Location, dst: Location);
    fn emit_lock_xadd(&mut self, sz: Size, src: Location, dst: Location);
    fn emit_lock_cmpxchg(&mut self, sz: Size, src: Location, dst: Location);
    fn emit_rep_stosq(&mut self);

    fn emit_btc_gpr_imm8_32(&mut self, src: u8, dst: GPR);
    fn emit_btc_gpr_imm8_64(&mut self, src: u8, dst: GPR);

    fn emit_cmovae_gpr_32(&mut self, src: GPR, dst: GPR);
    fn emit_cmovae_gpr_64(&mut self, src: GPR, dst: GPR);

    fn emit_vmovaps(&mut self, src: XMMOrMemory, dst: XMMOrMemory);
    fn emit_vmovapd(&mut self, src: XMMOrMemory, dst: XMMOrMemory);
    fn emit_vxorps(&mut self, src1: XMM, src2: XMMOrMemory, dst: XMM);
    fn emit_vxorpd(&mut self, src1: XMM, src2: XMMOrMemory, dst: XMM);

    fn emit_vaddss(&mut self, src1: XMM, src2: XMMOrMemory, dst: XMM);
    fn emit_vaddsd(&mut self, src1: XMM, src2: XMMOrMemory, dst: XMM);
    fn emit_vsubss(&mut self, src1: XMM, src2: XMMOrMemory, dst: XMM);
    fn emit_vsubsd(&mut self, src1: XMM, src2: XMMOrMemory, dst: XMM);
    fn emit_vmulss(&mut self, src1: XMM, src2: XMMOrMemory, dst: XMM);
    fn emit_vmulsd(&mut self, src1: XMM, src2: XMMOrMemory, dst: XMM);
    fn emit_vdivss(&mut self, src1: XMM, src2: XMMOrMemory, dst: XMM);
    fn emit_vdivsd(&mut self, src1: XMM, src2: XMMOrMemory, dst: XMM);
    fn emit_vmaxss(&mut self, src1: XMM, src2: XMMOrMemory, dst: XMM);
    fn emit_vmaxsd(&mut self, src1: XMM, src2: XMMOrMemory, dst: XMM);
    fn emit_vminss(&mut self, src1: XMM, src2: XMMOrMemory, dst: XMM);
    fn emit_vminsd(&mut self, src1: XMM, src2: XMMOrMemory, dst: XMM);

    fn emit_vcmpeqss(&mut self, src1: XMM, src2: XMMOrMemory, dst: XMM);
    fn emit_vcmpeqsd(&mut self, src1: XMM, src2: XMMOrMemory, dst: XMM);

    fn emit_vcmpneqss(&mut self, src1: XMM, src2: XMMOrMemory, dst: XMM);
    fn emit_vcmpneqsd(&mut self, src1: XMM, src2: XMMOrMemory, dst: XMM);

    fn emit_vcmpltss(&mut self, src1: XMM, src2: XMMOrMemory, dst: XMM);
    fn emit_vcmpltsd(&mut self, src1: XMM, src2: XMMOrMemory, dst: XMM);

    fn emit_vcmpless(&mut self, src1: XMM, src2: XMMOrMemory, dst: XMM);
    fn emit_vcmplesd(&mut self, src1: XMM, src2: XMMOrMemory, dst: XMM);

    fn emit_vcmpgtss(&mut self, src1: XMM, src2: XMMOrMemory, dst: XMM);
    fn emit_vcmpgtsd(&mut self, src1: XMM, src2: XMMOrMemory, dst: XMM);

    fn emit_vcmpgess(&mut self, src1: XMM, src2: XMMOrMemory, dst: XMM);
    fn emit_vcmpgesd(&mut self, src1: XMM, src2: XMMOrMemory, dst: XMM);

    fn emit_vcmpunordss(&mut self, src1: XMM, src2: XMMOrMemory, dst: XMM);
    fn emit_vcmpunordsd(&mut self, src1: XMM, src2: XMMOrMemory, dst: XMM);

    fn emit_vcmpordss(&mut self, src1: XMM, src2: XMMOrMemory, dst: XMM);
    fn emit_vcmpordsd(&mut self, src1: XMM, src2: XMMOrMemory, dst: XMM);

    fn emit_vsqrtss(&mut self, src1: XMM, src2: XMMOrMemory, dst: XMM);
    fn emit_vsqrtsd(&mut self, src1: XMM, src2: XMMOrMemory, dst: XMM);

    fn emit_vroundss_nearest(&mut self, src1: XMM, src2: XMMOrMemory, dst: XMM);
    fn emit_vroundss_floor(&mut self, src1: XMM, src2: XMMOrMemory, dst: XMM);
    fn emit_vroundss_ceil(&mut self, src1: XMM, src2: XMMOrMemory, dst: XMM);
    fn emit_vroundss_trunc(&mut self, src1: XMM, src2: XMMOrMemory, dst: XMM);
    fn emit_vroundsd_nearest(&mut self, src1: XMM, src2: XMMOrMemory, dst: XMM);
    fn emit_vroundsd_floor(&mut self, src1: XMM, src2: XMMOrMemory, dst: XMM);
    fn emit_vroundsd_ceil(&mut self, src1: XMM, src2: XMMOrMemory, dst: XMM);
    fn emit_vroundsd_trunc(&mut self, src1: XMM, src2: XMMOrMemory, dst: XMM);

    fn emit_vcvtss2sd(&mut self, src1: XMM, src2: XMMOrMemory, dst: XMM);
    fn emit_vcvtsd2ss(&mut self, src1: XMM, src2: XMMOrMemory, dst: XMM);

    fn emit_ucomiss(&mut self, src: XMMOrMemory, dst: XMM);
    fn emit_ucomisd(&mut self, src: XMMOrMemory, dst: XMM);

    fn emit_cvttss2si_32(&mut self, src: XMMOrMemory, dst: GPR);
    fn emit_cvttss2si_64(&mut self, src: XMMOrMemory, dst: GPR);
    fn emit_cvttsd2si_32(&mut self, src: XMMOrMemory, dst: GPR);
    fn emit_cvttsd2si_64(&mut self, src: XMMOrMemory, dst: GPR);

    fn emit_vcvtsi2ss_32(&mut self, src1: XMM, src2: GPROrMemory, dst: XMM);
    fn emit_vcvtsi2ss_64(&mut self, src1: XMM, src2: GPROrMemory, dst: XMM);
    fn emit_vcvtsi2sd_32(&mut self, src1: XMM, src2: GPROrMemory, dst: XMM);
    fn emit_vcvtsi2sd_64(&mut self, src1: XMM, src2: GPROrMemory, dst: XMM);

    fn emit_vblendvps(&mut self, src1: XMM, src2: XMMOrMemory, mask: XMM, dst: XMM);
    fn emit_vblendvpd(&mut self, src1: XMM, src2: XMMOrMemory, mask: XMM, dst: XMM);

    fn emit_test_gpr_64(&mut self, reg: GPR);

    fn emit_ud2(&mut self);
    fn emit_ret(&mut self);
    fn emit_call_label(&mut self, label: Self::Label);
    fn emit_call_location(&mut self, loc: Location);

    fn emit_call_register(&mut self, reg: GPR);

    fn emit_bkpt(&mut self);

    fn emit_host_redirection(&mut self, target: GPR);

    fn arch_has_itruncf(&self) -> bool {
        false
    }
    fn arch_emit_i32_trunc_sf32(&mut self, _src: XMM, _dst: GPR) {
        unimplemented!()
    }
    fn arch_emit_i32_trunc_sf64(&mut self, _src: XMM, _dst: GPR) {
        unimplemented!()
    }
    fn arch_emit_i32_trunc_uf32(&mut self, _src: XMM, _dst: GPR) {
        unimplemented!()
    }
    fn arch_emit_i32_trunc_uf64(&mut self, _src: XMM, _dst: GPR) {
        unimplemented!()
    }
    fn arch_emit_i64_trunc_sf32(&mut self, _src: XMM, _dst: GPR) {
        unimplemented!()
    }
    fn arch_emit_i64_trunc_sf64(&mut self, _src: XMM, _dst: GPR) {
        unimplemented!()
    }
    fn arch_emit_i64_trunc_uf32(&mut self, _src: XMM, _dst: GPR) {
        unimplemented!()
    }
    fn arch_emit_i64_trunc_uf64(&mut self, _src: XMM, _dst: GPR) {
        unimplemented!()
    }

    fn arch_has_fconverti(&self) -> bool {
        false
    }
    fn arch_emit_f32_convert_si32(&mut self, _src: GPR, _dst: XMM) {
        unimplemented!()
    }
    fn arch_emit_f32_convert_si64(&mut self, _src: GPR, _dst: XMM) {
        unimplemented!()
    }
    fn arch_emit_f32_convert_ui32(&mut self, _src: GPR, _dst: XMM) {
        unimplemented!()
    }
    fn arch_emit_f32_convert_ui64(&mut self, _src: GPR, _dst: XMM) {
        unimplemented!()
    }
    fn arch_emit_f64_convert_si32(&mut self, _src: GPR, _dst: XMM) {
        unimplemented!()
    }
    fn arch_emit_f64_convert_si64(&mut self, _src: GPR, _dst: XMM) {
        unimplemented!()
    }
    fn arch_emit_f64_convert_ui32(&mut self, _src: GPR, _dst: XMM) {
        unimplemented!()
    }
    fn arch_emit_f64_convert_ui64(&mut self, _src: GPR, _dst: XMM) {
        unimplemented!()
    }

    fn arch_has_fneg(&self) -> bool {
        false
    }
    fn arch_emit_f32_neg(&mut self, _src: XMM, _dst: XMM) {
        unimplemented!()
    }
    fn arch_emit_f64_neg(&mut self, _src: XMM, _dst: XMM) {
        unimplemented!()
    }

    fn arch_has_xzcnt(&self) -> bool {
        false
    }
    fn arch_emit_lzcnt(&mut self, _sz: Size, _src: Location, _dst: Location) {
        unimplemented!()
    }
    fn arch_emit_tzcnt(&mut self, _sz: Size, _src: Location, _dst: Location) {
        unimplemented!()
    }

    fn arch_supports_canonicalize_nan(&self) -> bool {
        true
    }

    fn arch_requires_indirect_call_trampoline(&self) -> bool {
        false
    }

    fn arch_emit_indirect_call_with_trampoline(&mut self, _loc: Location) {
        unimplemented!()
    }

    // Emits entry trampoline just before the real function.
    fn arch_emit_entry_trampoline(&mut self) {}

    // Byte offset from the beginning of a `mov Imm64, GPR` instruction to the imm64 value.
    // Required to support emulation on Aarch64.
    fn arch_mov64_imm_offset(&self) -> usize {
        unimplemented!()
    }
}

macro_rules! unop_gpr {
    ($ins:ident, $assembler:tt, $sz:expr, $loc:expr, $otherwise:block) => {
        match ($sz, $loc) {
            (Size::S32, Location::GPR(loc)) => {
                dynasm!($assembler ; $ins Rd(loc as u8));
            },
            (Size::S64, Location::GPR(loc)) => {
                dynasm!($assembler ; $ins Rq(loc as u8));
            },
            _ => $otherwise
        }
    };
}

macro_rules! unop_mem {
    ($ins:ident, $assembler:tt, $sz:expr, $loc:expr, $otherwise:block) => {
        match ($sz, $loc) {
            (Size::S32, Location::Memory(loc, disp)) => {
                dynasm!($assembler ; $ins DWORD [Rq(loc as u8) + disp] );
            },
            (Size::S64, Location::Memory(loc, disp)) => {
                dynasm!($assembler ; $ins QWORD [Rq(loc as u8) + disp] );
            },
            _ => $otherwise
        }
    };
}

macro_rules! unop_gpr_or_mem {
    ($ins:ident, $assembler:tt, $sz:expr, $loc:expr, $otherwise:block) => {
        unop_gpr!($ins, $assembler, $sz, $loc, {
            unop_mem!($ins, $assembler, $sz, $loc, $otherwise)
        })
    };
}

macro_rules! binop_imm32_gpr {
    ($ins:ident, $assembler:tt, $sz:expr, $src:expr, $dst:expr, $otherwise:block) => {
        match ($sz, $src, $dst) {
            (Size::S32, Location::Imm32(src), Location::GPR(dst)) => {
                dynasm!($assembler ; $ins Rd(dst as u8), src as i32); // IMM32_2GPR
            },
            (Size::S64, Location::Imm32(src), Location::GPR(dst)) => {
                dynasm!($assembler ; $ins Rq(dst as u8), src as i32); // IMM32_2GPR
            },
            _ => $otherwise
        }
    };
}

macro_rules! binop_imm32_mem {
    ($ins:ident, $assembler:tt, $sz:expr, $src:expr, $dst:expr, $otherwise:block) => {
        match ($sz, $src, $dst) {
            (Size::S32, Location::Imm32(src), Location::Memory(dst, disp)) => {
                dynasm!($assembler ; $ins DWORD [Rq(dst as u8) + disp], src as i32);
            },
            (Size::S64, Location::Imm32(src), Location::Memory(dst, disp)) => {
                dynasm!($assembler ; $ins QWORD [Rq(dst as u8) + disp], src as i32);
            },
            _ => $otherwise
        }
    };
}

macro_rules! binop_imm64_gpr {
    ($ins:ident, $assembler:tt, $sz:expr, $src:expr, $dst:expr, $otherwise:block) => {
        match ($sz, $src, $dst) {
            (Size::S64, Location::Imm64(src), Location::GPR(dst)) => {
                dynasm!($assembler ; $ins Rq(dst as u8), QWORD src as i64); // IMM32_2GPR
            },
            _ => $otherwise
        }
    };
}

macro_rules! binop_gpr_gpr {
    ($ins:ident, $assembler:tt, $sz:expr, $src:expr, $dst:expr, $otherwise:block) => {
        match ($sz, $src, $dst) {
            (Size::S32, Location::GPR(src), Location::GPR(dst)) => {
                dynasm!($assembler ; $ins Rd(dst as u8), Rd(src as u8)); // GPR2GPR
            },
            (Size::S64, Location::GPR(src), Location::GPR(dst)) => {
                dynasm!($assembler ; $ins Rq(dst as u8), Rq(src as u8)); // GPR2GPR
            },
            _ => $otherwise
        }
    };
}

macro_rules! binop_gpr_mem {
    ($ins:ident, $assembler:tt, $sz:expr, $src:expr, $dst:expr, $otherwise:block) => {
        match ($sz, $src, $dst) {
            (Size::S32, Location::GPR(src), Location::Memory(dst, disp)) => {
                dynasm!($assembler ; $ins [Rq(dst as u8) + disp], Rd(src as u8)); // GPR2MEM
            },
            (Size::S64, Location::GPR(src), Location::Memory(dst, disp)) => {
                dynasm!($assembler ; $ins [Rq(dst as u8) + disp], Rq(src as u8)); // GPR2MEM
            },
            _ => $otherwise
        }
    };
}

macro_rules! binop_mem_gpr {
    ($ins:ident, $assembler:tt, $sz:expr, $src:expr, $dst:expr, $otherwise:block) => {
        match ($sz, $src, $dst) {
            (Size::S32, Location::Memory(src, disp), Location::GPR(dst)) => {
                dynasm!($assembler ; $ins Rd(dst as u8), [Rq(src as u8) + disp]); // MEM2GPR
            },
            (Size::S64, Location::Memory(src, disp), Location::GPR(dst)) => {
                dynasm!($assembler ; $ins Rq(dst as u8), [Rq(src as u8) + disp]); // MEM2GPR
            },
            _ => $otherwise
        }
    };
}

macro_rules! binop_all_nofp {
    ($ins:ident, $assembler:tt, $sz:expr, $src:expr, $dst:expr, $otherwise:block) => {
        binop_imm32_gpr!($ins, $assembler, $sz, $src, $dst, {
            binop_imm32_mem!($ins, $assembler, $sz, $src, $dst, {
                binop_gpr_gpr!($ins, $assembler, $sz, $src, $dst, {
                    binop_gpr_mem!($ins, $assembler, $sz, $src, $dst, {
                        binop_mem_gpr!($ins, $assembler, $sz, $src, $dst, $otherwise)
                    })
                })
            })
        })
    };
}

macro_rules! binop_shift {
    ($ins:ident, $assembler:tt, $sz:expr, $src:expr, $dst:expr, $otherwise:block) => {
        match ($sz, $src, $dst) {
            (Size::S32, Location::GPR(GPR::RCX), Location::GPR(dst)) => {
                dynasm!($assembler ; $ins Rd(dst as u8), cl);
            },
            (Size::S32, Location::GPR(GPR::RCX), Location::Memory(dst, disp)) => {
                dynasm!($assembler ; $ins DWORD [Rq(dst as u8) + disp], cl);
            },
            (Size::S32, Location::Imm8(imm), Location::GPR(dst)) => {
                dynasm!($assembler ; $ins Rd(dst as u8), imm as i8);
            },
            (Size::S32, Location::Imm8(imm), Location::Memory(dst, disp)) => {
                dynasm!($assembler ; $ins DWORD [Rq(dst as u8) + disp], imm as i8);
            },
            (Size::S64, Location::GPR(GPR::RCX), Location::GPR(dst)) => {
                dynasm!($assembler ; $ins Rq(dst as u8), cl);
            },
            (Size::S64, Location::GPR(GPR::RCX), Location::Memory(dst, disp)) => {
                dynasm!($assembler ; $ins QWORD [Rq(dst as u8) + disp], cl);
            },
            (Size::S64, Location::Imm8(imm), Location::GPR(dst)) => {
                dynasm!($assembler ; $ins Rq(dst as u8), imm as i8);
            },
            (Size::S64, Location::Imm8(imm), Location::Memory(dst, disp)) => {
                dynasm!($assembler ; $ins QWORD [Rq(dst as u8) + disp], imm as i8);
            },
            _ => $otherwise
        }
    }
}

macro_rules! jmp_op {
    ($ins:ident, $assembler:tt, $label:ident) => {
        dynasm!($assembler ; $ins =>$label)
    }
}

macro_rules! avx_fn {
    ($ins:ident, $name:ident) => {
        fn $name(&mut self, src1: XMM, src2: XMMOrMemory, dst: XMM) {
            // Dynasm bug: AVX instructions are not encoded correctly.
            match src2 {
                XMMOrMemory::XMM(x) => match src1 {
                    XMM::XMM0 => dynasm!(self ; $ins Rx((dst as u8)), xmm0, Rx((x as u8))),
                    XMM::XMM1 => dynasm!(self ; $ins Rx((dst as u8)), xmm1, Rx((x as u8))),
                    XMM::XMM2 => dynasm!(self ; $ins Rx((dst as u8)), xmm2, Rx((x as u8))),
                    XMM::XMM3 => dynasm!(self ; $ins Rx((dst as u8)), xmm3, Rx((x as u8))),
                    XMM::XMM4 => dynasm!(self ; $ins Rx((dst as u8)), xmm4, Rx((x as u8))),
                    XMM::XMM5 => dynasm!(self ; $ins Rx((dst as u8)), xmm5, Rx((x as u8))),
                    XMM::XMM6 => dynasm!(self ; $ins Rx((dst as u8)), xmm6, Rx((x as u8))),
                    XMM::XMM7 => dynasm!(self ; $ins Rx((dst as u8)), xmm7, Rx((x as u8))),
                    XMM::XMM8 => dynasm!(self ; $ins Rx((dst as u8)), xmm8, Rx((x as u8))),
                    XMM::XMM9 => dynasm!(self ; $ins Rx((dst as u8)), xmm9, Rx((x as u8))),
                    XMM::XMM10 => dynasm!(self ; $ins Rx((dst as u8)), xmm10, Rx((x as u8))),
                    XMM::XMM11 => dynasm!(self ; $ins Rx((dst as u8)), xmm11, Rx((x as u8))),
                    XMM::XMM12 => dynasm!(self ; $ins Rx((dst as u8)), xmm12, Rx((x as u8))),
                    XMM::XMM13 => dynasm!(self ; $ins Rx((dst as u8)), xmm13, Rx((x as u8))),
                    XMM::XMM14 => dynasm!(self ; $ins Rx((dst as u8)), xmm14, Rx((x as u8))),
                    XMM::XMM15 => dynasm!(self ; $ins Rx((dst as u8)), xmm15, Rx((x as u8))),
                },
                XMMOrMemory::Memory(base, disp) => match src1 {
                    XMM::XMM0 => dynasm!(self ; $ins Rx((dst as u8)), xmm0, [Rq((base as u8)) + disp]),
                    XMM::XMM1 => dynasm!(self ; $ins Rx((dst as u8)), xmm1, [Rq((base as u8)) + disp]),
                    XMM::XMM2 => dynasm!(self ; $ins Rx((dst as u8)), xmm2, [Rq((base as u8)) + disp]),
                    XMM::XMM3 => dynasm!(self ; $ins Rx((dst as u8)), xmm3, [Rq((base as u8)) + disp]),
                    XMM::XMM4 => dynasm!(self ; $ins Rx((dst as u8)), xmm4, [Rq((base as u8)) + disp]),
                    XMM::XMM5 => dynasm!(self ; $ins Rx((dst as u8)), xmm5, [Rq((base as u8)) + disp]),
                    XMM::XMM6 => dynasm!(self ; $ins Rx((dst as u8)), xmm6, [Rq((base as u8)) + disp]),
                    XMM::XMM7 => dynasm!(self ; $ins Rx((dst as u8)), xmm7, [Rq((base as u8)) + disp]),
                    XMM::XMM8 => dynasm!(self ; $ins Rx((dst as u8)), xmm8, [Rq((base as u8)) + disp]),
                    XMM::XMM9 => dynasm!(self ; $ins Rx((dst as u8)), xmm9, [Rq((base as u8)) + disp]),
                    XMM::XMM10 => dynasm!(self ; $ins Rx((dst as u8)), xmm10, [Rq((base as u8)) + disp]),
                    XMM::XMM11 => dynasm!(self ; $ins Rx((dst as u8)), xmm11, [Rq((base as u8)) + disp]),
                    XMM::XMM12 => dynasm!(self ; $ins Rx((dst as u8)), xmm12, [Rq((base as u8)) + disp]),
                    XMM::XMM13 => dynasm!(self ; $ins Rx((dst as u8)), xmm13, [Rq((base as u8)) + disp]),
                    XMM::XMM14 => dynasm!(self ; $ins Rx((dst as u8)), xmm14, [Rq((base as u8)) + disp]),
                    XMM::XMM15 => dynasm!(self ; $ins Rx((dst as u8)), xmm15, [Rq((base as u8)) + disp]),
                },
            }
        }
    }
}

macro_rules! avx_i2f_64_fn {
    ($ins:ident, $name:ident) => {
        fn $name(&mut self, src1: XMM, src2: GPROrMemory, dst: XMM) {
            match src2 {
                GPROrMemory::GPR(x) => match src1 {
                    XMM::XMM0 => dynasm!(self ; $ins Rx((dst as u8)), xmm0, Rq((x as u8))),
                    XMM::XMM1 => dynasm!(self ; $ins Rx((dst as u8)), xmm1, Rq((x as u8))),
                    XMM::XMM2 => dynasm!(self ; $ins Rx((dst as u8)), xmm2, Rq((x as u8))),
                    XMM::XMM3 => dynasm!(self ; $ins Rx((dst as u8)), xmm3, Rq((x as u8))),
                    XMM::XMM4 => dynasm!(self ; $ins Rx((dst as u8)), xmm4, Rq((x as u8))),
                    XMM::XMM5 => dynasm!(self ; $ins Rx((dst as u8)), xmm5, Rq((x as u8))),
                    XMM::XMM6 => dynasm!(self ; $ins Rx((dst as u8)), xmm6, Rq((x as u8))),
                    XMM::XMM7 => dynasm!(self ; $ins Rx((dst as u8)), xmm7, Rq((x as u8))),
                    XMM::XMM8 => dynasm!(self ; $ins Rx((dst as u8)), xmm8, Rq((x as u8))),
                    XMM::XMM9 => dynasm!(self ; $ins Rx((dst as u8)), xmm9, Rq((x as u8))),
                    XMM::XMM10 => dynasm!(self ; $ins Rx((dst as u8)), xmm10, Rq((x as u8))),
                    XMM::XMM11 => dynasm!(self ; $ins Rx((dst as u8)), xmm11, Rq((x as u8))),
                    XMM::XMM12 => dynasm!(self ; $ins Rx((dst as u8)), xmm12, Rq((x as u8))),
                    XMM::XMM13 => dynasm!(self ; $ins Rx((dst as u8)), xmm13, Rq((x as u8))),
                    XMM::XMM14 => dynasm!(self ; $ins Rx((dst as u8)), xmm14, Rq((x as u8))),
                    XMM::XMM15 => dynasm!(self ; $ins Rx((dst as u8)), xmm15, Rq((x as u8))),
                },
                GPROrMemory::Memory(base, disp) => match src1 {
                    XMM::XMM0 => dynasm!(self ; $ins Rx((dst as u8)), xmm0, QWORD [Rq((base as u8)) + disp]),
                    XMM::XMM1 => dynasm!(self ; $ins Rx((dst as u8)), xmm1, QWORD [Rq((base as u8)) + disp]),
                    XMM::XMM2 => dynasm!(self ; $ins Rx((dst as u8)), xmm2, QWORD [Rq((base as u8)) + disp]),
                    XMM::XMM3 => dynasm!(self ; $ins Rx((dst as u8)), xmm3, QWORD [Rq((base as u8)) + disp]),
                    XMM::XMM4 => dynasm!(self ; $ins Rx((dst as u8)), xmm4, QWORD [Rq((base as u8)) + disp]),
                    XMM::XMM5 => dynasm!(self ; $ins Rx((dst as u8)), xmm5, QWORD [Rq((base as u8)) + disp]),
                    XMM::XMM6 => dynasm!(self ; $ins Rx((dst as u8)), xmm6, QWORD [Rq((base as u8)) + disp]),
                    XMM::XMM7 => dynasm!(self ; $ins Rx((dst as u8)), xmm7, QWORD [Rq((base as u8)) + disp]),
                    XMM::XMM8 => dynasm!(self ; $ins Rx((dst as u8)), xmm8, QWORD [Rq((base as u8)) + disp]),
                    XMM::XMM9 => dynasm!(self ; $ins Rx((dst as u8)), xmm9, QWORD [Rq((base as u8)) + disp]),
                    XMM::XMM10 => dynasm!(self ; $ins Rx((dst as u8)), xmm10, QWORD [Rq((base as u8)) + disp]),
                    XMM::XMM11 => dynasm!(self ; $ins Rx((dst as u8)), xmm11, QWORD [Rq((base as u8)) + disp]),
                    XMM::XMM12 => dynasm!(self ; $ins Rx((dst as u8)), xmm12, QWORD [Rq((base as u8)) + disp]),
                    XMM::XMM13 => dynasm!(self ; $ins Rx((dst as u8)), xmm13, QWORD [Rq((base as u8)) + disp]),
                    XMM::XMM14 => dynasm!(self ; $ins Rx((dst as u8)), xmm14, QWORD [Rq((base as u8)) + disp]),
                    XMM::XMM15 => dynasm!(self ; $ins Rx((dst as u8)), xmm15, QWORD [Rq((base as u8)) + disp]),
                },
            }
        }
    }
}

macro_rules! avx_i2f_32_fn {
    ($ins:ident, $name:ident) => {
        fn $name(&mut self, src1: XMM, src2: GPROrMemory, dst: XMM) {
            match src2 {
                GPROrMemory::GPR(x) => match src1 {
                    XMM::XMM0 => dynasm!(self ; $ins Rx((dst as u8)), xmm0, Rd((x as u8))),
                    XMM::XMM1 => dynasm!(self ; $ins Rx((dst as u8)), xmm1, Rd((x as u8))),
                    XMM::XMM2 => dynasm!(self ; $ins Rx((dst as u8)), xmm2, Rd((x as u8))),
                    XMM::XMM3 => dynasm!(self ; $ins Rx((dst as u8)), xmm3, Rd((x as u8))),
                    XMM::XMM4 => dynasm!(self ; $ins Rx((dst as u8)), xmm4, Rd((x as u8))),
                    XMM::XMM5 => dynasm!(self ; $ins Rx((dst as u8)), xmm5, Rd((x as u8))),
                    XMM::XMM6 => dynasm!(self ; $ins Rx((dst as u8)), xmm6, Rd((x as u8))),
                    XMM::XMM7 => dynasm!(self ; $ins Rx((dst as u8)), xmm7, Rd((x as u8))),
                    XMM::XMM8 => dynasm!(self ; $ins Rx((dst as u8)), xmm8, Rd((x as u8))),
                    XMM::XMM9 => dynasm!(self ; $ins Rx((dst as u8)), xmm9, Rd((x as u8))),
                    XMM::XMM10 => dynasm!(self ; $ins Rx((dst as u8)), xmm10, Rd((x as u8))),
                    XMM::XMM11 => dynasm!(self ; $ins Rx((dst as u8)), xmm11, Rd((x as u8))),
                    XMM::XMM12 => dynasm!(self ; $ins Rx((dst as u8)), xmm12, Rd((x as u8))),
                    XMM::XMM13 => dynasm!(self ; $ins Rx((dst as u8)), xmm13, Rd((x as u8))),
                    XMM::XMM14 => dynasm!(self ; $ins Rx((dst as u8)), xmm14, Rd((x as u8))),
                    XMM::XMM15 => dynasm!(self ; $ins Rx((dst as u8)), xmm15, Rd((x as u8))),
                },
                GPROrMemory::Memory(base, disp) => match src1 {
                    XMM::XMM0 => dynasm!(self ; $ins Rx((dst as u8)), xmm0, DWORD [Rq((base as u8)) + disp]),
                    XMM::XMM1 => dynasm!(self ; $ins Rx((dst as u8)), xmm1, DWORD [Rq((base as u8)) + disp]),
                    XMM::XMM2 => dynasm!(self ; $ins Rx((dst as u8)), xmm2, DWORD [Rq((base as u8)) + disp]),
                    XMM::XMM3 => dynasm!(self ; $ins Rx((dst as u8)), xmm3, DWORD [Rq((base as u8)) + disp]),
                    XMM::XMM4 => dynasm!(self ; $ins Rx((dst as u8)), xmm4, DWORD [Rq((base as u8)) + disp]),
                    XMM::XMM5 => dynasm!(self ; $ins Rx((dst as u8)), xmm5, DWORD [Rq((base as u8)) + disp]),
                    XMM::XMM6 => dynasm!(self ; $ins Rx((dst as u8)), xmm6, DWORD [Rq((base as u8)) + disp]),
                    XMM::XMM7 => dynasm!(self ; $ins Rx((dst as u8)), xmm7, DWORD [Rq((base as u8)) + disp]),
                    XMM::XMM8 => dynasm!(self ; $ins Rx((dst as u8)), xmm8, DWORD [Rq((base as u8)) + disp]),
                    XMM::XMM9 => dynasm!(self ; $ins Rx((dst as u8)), xmm9, DWORD [Rq((base as u8)) + disp]),
                    XMM::XMM10 => dynasm!(self ; $ins Rx((dst as u8)), xmm10, DWORD [Rq((base as u8)) + disp]),
                    XMM::XMM11 => dynasm!(self ; $ins Rx((dst as u8)), xmm11, DWORD [Rq((base as u8)) + disp]),
                    XMM::XMM12 => dynasm!(self ; $ins Rx((dst as u8)), xmm12, DWORD [Rq((base as u8)) + disp]),
                    XMM::XMM13 => dynasm!(self ; $ins Rx((dst as u8)), xmm13, DWORD [Rq((base as u8)) + disp]),
                    XMM::XMM14 => dynasm!(self ; $ins Rx((dst as u8)), xmm14, DWORD [Rq((base as u8)) + disp]),
                    XMM::XMM15 => dynasm!(self ; $ins Rx((dst as u8)), xmm15, DWORD [Rq((base as u8)) + disp]),
                },
            }
        }
    }
}

macro_rules! avx_round_fn {
    ($ins:ident, $name:ident, $mode:expr) => {
        fn $name(&mut self, src1: XMM, src2: XMMOrMemory, dst: XMM) {
            match src2 {
                XMMOrMemory::XMM(x) => dynasm!(self ; $ins Rx((dst as u8)), Rx((src1 as u8)), Rx((x as u8)), $mode),
                XMMOrMemory::Memory(base, disp) => dynasm!(self ; $ins Rx((dst as u8)), Rx((src1 as u8)), [Rq((base as u8)) + disp], $mode),
            }
        }
    }
}

impl Emitter for Assembler {
    type Label = DynamicLabel;
    type Offset = AssemblyOffset;

    fn get_label(&mut self) -> DynamicLabel {
        self.new_dynamic_label()
    }

    fn get_offset(&self) -> AssemblyOffset {
        self.offset()
    }

    fn get_jmp_instr_size(&self) -> u8 {
        5
    }

    fn finalize_function(&mut self) {
        dynasm!(
            self
            ; const_neg_one_32:
            ; .dword -1
            ; const_zero_32:
            ; .dword 0
            ; const_pos_one_32:
            ; .dword 1
        );
    }

    fn emit_u64(&mut self, x: u64) {
        self.push_u64(x);
    }

    fn emit_bytes(&mut self, bytes: &[u8]) {
        for &b in bytes {
            self.push(b);
        }
    }

    fn emit_label(&mut self, label: Self::Label) {
        dynasm!(self ; => label);
    }

    fn emit_nop(&mut self) {
        dynasm!(self ; nop);
    }

    fn emit_nop_n(&mut self, mut n: usize) {
        /*
            1      90H                            NOP
            2      66 90H                         66 NOP
            3      0F 1F 00H                      NOP DWORD ptr [EAX]
            4      0F 1F 40 00H                   NOP DWORD ptr [EAX + 00H]
            5      0F 1F 44 00 00H                NOP DWORD ptr [EAX + EAX*1 + 00H]
            6      66 0F 1F 44 00 00H             NOP DWORD ptr [AX + AX*1 + 00H]
            7      0F 1F 80 00 00 00 00H          NOP DWORD ptr [EAX + 00000000H]
            8      0F 1F 84 00 00 00 00 00H       NOP DWORD ptr [AX + AX*1 + 00000000H]
            9      66 0F 1F 84 00 00 00 00 00H    NOP DWORD ptr [AX + AX*1 + 00000000H]
        */
        while n >= 9 {
            n -= 9;
            self.emit_bytes(&[0x66, 0x0f, 0x1f, 0x84, 0x00, 0x00, 0x00, 0x00, 0x00]);
            // 9-byte nop
        }
        let seq: &[u8] = match n {
            0 => &[],
            1 => &[0x90],
            2 => &[0x66, 0x90],
            3 => &[0x0f, 0x1f, 0x00],
            4 => &[0x0f, 0x1f, 0x40, 0x00],
            5 => &[0x0f, 0x1f, 0x44, 0x00, 0x00],
            6 => &[0x66, 0x0f, 0x1f, 0x44, 0x00, 0x00],
            7 => &[0x0f, 0x1f, 0x80, 0x00, 0x00, 0x00, 0x00],
            8 => &[0x0f, 0x1f, 0x84, 0x00, 0x00, 0x00, 0x00, 0x00],
            _ => unreachable!(),
        };
        self.emit_bytes(seq);
    }

    fn emit_mov(&mut self, sz: Size, src: Location, dst: Location) {
        // fast path
        if let (Location::Imm32(0), Location::GPR(x)) = (src, dst) {
            dynasm!(self ; xor Rd(x as u8), Rd(x as u8));
            return;
        }

        binop_all_nofp!(mov, self, sz, src, dst, {
            binop_imm64_gpr!(mov, self, sz, src, dst, {
                match (sz, src, dst) {
                    (Size::S8, Location::GPR(src), Location::Memory(dst, disp)) => {
                        dynasm!(self ; mov [Rq(dst as u8) + disp], Rb(src as u8));
                    }
                    (Size::S8, Location::Memory(src, disp), Location::GPR(dst)) => {
                        dynasm!(self ; mov Rb(dst as u8), [Rq(src as u8) + disp]);
                    }
                    (Size::S8, Location::Imm32(src), Location::GPR(dst)) => {
                        dynasm!(self ; mov Rb(dst as u8), src as i8);
                    }
                    (Size::S8, Location::Imm64(src), Location::GPR(dst)) => {
                        dynasm!(self ; mov Rb(dst as u8), src as i8);
                    }
                    (Size::S8, Location::Imm32(src), Location::Memory(dst, disp)) => {
                        dynasm!(self ; mov BYTE [Rq(dst as u8) + disp], src as i8);
                    }
                    (Size::S8, Location::Imm64(src), Location::Memory(dst, disp)) => {
                        dynasm!(self ; mov BYTE [Rq(dst as u8) + disp], src as i8);
                    }
                    (Size::S16, Location::GPR(src), Location::Memory(dst, disp)) => {
                        dynasm!(self ; mov [Rq(dst as u8) + disp], Rw(src as u8));
                    }
                    (Size::S16, Location::Memory(src, disp), Location::GPR(dst)) => {
                        dynasm!(self ; mov Rw(dst as u8), [Rq(src as u8) + disp]);
                    }
                    (Size::S16, Location::Imm32(src), Location::GPR(dst)) => {
                        dynasm!(self ; mov Rw(dst as u8), src as i16);
                    }
                    (Size::S16, Location::Imm64(src), Location::GPR(dst)) => {
                        dynasm!(self ; mov Rw(dst as u8), src as i16);
                    }
                    (Size::S16, Location::Imm32(src), Location::Memory(dst, disp)) => {
                        dynasm!(self ; mov WORD [Rq(dst as u8) + disp], src as i16);
                    }
                    (Size::S16, Location::Imm64(src), Location::Memory(dst, disp)) => {
                        dynasm!(self ; mov WORD [Rq(dst as u8) + disp], src as i16);
                    }
                    (Size::S32, Location::Imm64(src), Location::GPR(dst)) => {
                        dynasm!(self ; mov Rd(dst as u8), src as i32);
                    }
                    (Size::S32, Location::Imm64(src), Location::Memory(dst, disp)) => {
                        dynasm!(self ; mov DWORD [Rq(dst as u8) + disp], src as i32);
                    }
                    (Size::S32, Location::GPR(src), Location::XMM(dst)) => {
                        dynasm!(self ; movd Rx(dst as u8), Rd(src as u8));
                    }
                    (Size::S32, Location::XMM(src), Location::GPR(dst)) => {
                        dynasm!(self ; movd Rd(dst as u8), Rx(src as u8));
                    }
                    (Size::S32, Location::Memory(src, disp), Location::XMM(dst)) => {
                        dynasm!(self ; movd Rx(dst as u8), [Rq(src as u8) + disp]);
                    }
                    (Size::S32, Location::XMM(src), Location::Memory(dst, disp)) => {
                        dynasm!(self ; movd [Rq(dst as u8) + disp], Rx(src as u8));
                    }

                    (Size::S64, Location::GPR(src), Location::XMM(dst)) => {
                        dynasm!(self ; movq Rx(dst as u8), Rq(src as u8));
                    }
                    (Size::S64, Location::XMM(src), Location::GPR(dst)) => {
                        dynasm!(self ; movq Rq(dst as u8), Rx(src as u8));
                    }
                    (Size::S64, Location::Memory(src, disp), Location::XMM(dst)) => {
                        dynasm!(self ; movq Rx(dst as u8), [Rq(src as u8) + disp]);
                    }
                    (Size::S64, Location::XMM(src), Location::Memory(dst, disp)) => {
                        dynasm!(self ; movq [Rq(dst as u8) + disp], Rx(src as u8));
                    }
                    (_, Location::XMM(src), Location::XMM(dst)) => {
                        dynasm!(self ; movq Rx(dst as u8), Rx(src as u8));
                    }

                    _ => panic!("singlepass can't emit MOV {:?} {:?} {:?}", sz, src, dst),
                }
            })
        });
    }
    fn emit_lea(&mut self, sz: Size, src: Location, dst: Location) {
        match (sz, src, dst) {
            (Size::S32, Location::Memory(src, disp), Location::GPR(dst)) => {
                dynasm!(self ; lea Rd(dst as u8), [Rq(src as u8) + disp]);
            }
            (Size::S64, Location::Memory(src, disp), Location::GPR(dst)) => {
                dynasm!(self ; lea Rq(dst as u8), [Rq(src as u8) + disp]);
            }
            (Size::S32, Location::MemoryAddTriple(src1, src2, disp), Location::GPR(dst)) => {
                dynasm!(self ; lea Rd(dst as u8), [Rq(src1 as u8) + Rq(src2 as u8) + disp]);
            }
            (Size::S64, Location::MemoryAddTriple(src1, src2, disp), Location::GPR(dst)) => {
                dynasm!(self ; lea Rq(dst as u8), [Rq(src1 as u8) + Rq(src2 as u8) + disp]);
            }
            _ => panic!("singlepass can't emit LEA {:?} {:?} {:?}", sz, src, dst),
        }
    }
    fn emit_lea_label(&mut self, label: Self::Label, dst: Location) {
        match dst {
            Location::GPR(x) => {
                dynasm!(self ; lea Rq(x as u8), [=>label]);
            }
            _ => panic!("singlepass can't emit LEA label={:?} {:?}", label, dst),
        }
    }
    fn emit_cdq(&mut self) {
        dynasm!(self ; cdq);
    }
    fn emit_cqo(&mut self) {
        dynasm!(self ; cqo);
    }
    fn emit_xor(&mut self, sz: Size, src: Location, dst: Location) {
        binop_all_nofp!(xor, self, sz, src, dst, {
            panic!("singlepass can't emit XOR {:?} {:?} {:?}", sz, src, dst)
        });
    }
    fn emit_jmp(&mut self, condition: Condition, label: Self::Label) {
        match condition {
            Condition::None => jmp_op!(jmp, self, label),
            Condition::Above => jmp_op!(ja, self, label),
            Condition::AboveEqual => jmp_op!(jae, self, label),
            Condition::Below => jmp_op!(jb, self, label),
            Condition::BelowEqual => jmp_op!(jbe, self, label),
            Condition::Greater => jmp_op!(jg, self, label),
            Condition::GreaterEqual => jmp_op!(jge, self, label),
            Condition::Less => jmp_op!(jl, self, label),
            Condition::LessEqual => jmp_op!(jle, self, label),
            Condition::Equal => jmp_op!(je, self, label),
            Condition::NotEqual => jmp_op!(jne, self, label),
            Condition::Signed => jmp_op!(js, self, label),
            Condition::Carry => jmp_op!(jc, self, label),
            Condition::Overflow => jmp_op!(jo, self, label),
        }
    }
    fn emit_jmp_location(&mut self, loc: Location) {
        match loc {
            Location::GPR(x) => dynasm!(self ; jmp Rq(x as u8)),
            Location::Memory(base, disp) => dynasm!(self ; jmp QWORD [Rq(base as u8) + disp]),
            _ => panic!("singlepass can't emit JMP {:?}", loc),
        }
    }
    fn emit_set(&mut self, condition: Condition, dst: GPR) {
        match condition {
            Condition::Above => dynasm!(self ; seta Rb(dst as u8)),
            Condition::AboveEqual => dynasm!(self ; setae Rb(dst as u8)),
            Condition::Below => dynasm!(self ; setb Rb(dst as u8)),
            Condition::BelowEqual => dynasm!(self ; setbe Rb(dst as u8)),
            Condition::Greater => dynasm!(self ; setg Rb(dst as u8)),
            Condition::GreaterEqual => dynasm!(self ; setge Rb(dst as u8)),
            Condition::Less => dynasm!(self ; setl Rb(dst as u8)),
            Condition::LessEqual => dynasm!(self ; setle Rb(dst as u8)),
            Condition::Equal => dynasm!(self ; sete Rb(dst as u8)),
            Condition::NotEqual => dynasm!(self ; setne Rb(dst as u8)),
            Condition::Signed => dynasm!(self ; sets Rb(dst as u8)),
            Condition::Carry => dynasm!(self ; setc Rb(dst as u8)),
            Condition::Overflow => dynasm!(self ; seto Rb(dst as u8)),
            _ => panic!("singlepass can't emit SET {:?} {:?}", condition, dst),
        }
    }
    fn emit_push(&mut self, sz: Size, src: Location) {
        match (sz, src) {
            (Size::S64, Location::Imm32(src)) => dynasm!(self ; push src as i32),
            (Size::S64, Location::GPR(src)) => dynasm!(self ; push Rq(src as u8)),
            (Size::S64, Location::Memory(src, disp)) => {
                dynasm!(self ; push QWORD [Rq(src as u8) + disp])
            }
            _ => panic!("singlepass can't emit PUSH {:?} {:?}", sz, src),
        }
    }
    fn emit_pop(&mut self, sz: Size, dst: Location) {
        match (sz, dst) {
            (Size::S64, Location::GPR(dst)) => dynasm!(self ; pop Rq(dst as u8)),
            (Size::S64, Location::Memory(dst, disp)) => {
                dynasm!(self ; pop QWORD [Rq(dst as u8) + disp])
            }
            _ => panic!("singlepass can't emit POP {:?} {:?}", sz, dst),
        }
    }
    fn emit_cmp(&mut self, sz: Size, left: Location, right: Location) {
        // Constant elimination for comparision between consts.
        //
        // Only needed for `emit_cmp`, since other binary operators actually write to `right` and `right` must
        // be a writable location for them.
        let consts = match (left, right) {
            (Location::Imm32(x), Location::Imm32(y)) => Some((x as i32 as i64, y as i32 as i64)),
            (Location::Imm32(x), Location::Imm64(y)) => Some((x as i32 as i64, y as i64)),
            (Location::Imm64(x), Location::Imm32(y)) => Some((x as i64, y as i32 as i64)),
            (Location::Imm64(x), Location::Imm64(y)) => Some((x as i64, y as i64)),
            _ => None,
        };
        use std::cmp::Ordering;
        match consts {
            Some((x, y)) => match x.cmp(&y) {
                Ordering::Less => dynasm!(self ; cmp DWORD [>const_neg_one_32], 0),
                Ordering::Equal => dynasm!(self ; cmp DWORD [>const_zero_32], 0),
                Ordering::Greater => dynasm!(self ; cmp DWORD [>const_pos_one_32], 0),
            },
            None => binop_all_nofp!(cmp, self, sz, left, right, {
                panic!("singlepass can't emit CMP {:?} {:?} {:?}", sz, left, right);
            }),
        }
    }
    fn emit_add(&mut self, sz: Size, src: Location, dst: Location) {
        // Fast path
        if let Location::Imm32(0) = src {
            return;
        }
        binop_all_nofp!(add, self, sz, src, dst, {
            panic!("singlepass can't emit ADD {:?} {:?} {:?}", sz, src, dst)
        });
    }
    fn emit_sub(&mut self, sz: Size, src: Location, dst: Location) {
        // Fast path
        if let Location::Imm32(0) = src {
            return;
        }
        binop_all_nofp!(sub, self, sz, src, dst, {
            panic!("singlepass can't emit SUB {:?} {:?} {:?}", sz, src, dst)
        });
    }
    fn emit_neg(&mut self, sz: Size, value: Location) {
        match (sz, value) {
            (Size::S8, Location::GPR(value)) => dynasm!(self ; neg Rb(value as u8)),
            (Size::S8, Location::Memory(value, disp)) => {
                dynasm!(self ; neg [Rq(value as u8) + disp])
            }
            (Size::S16, Location::GPR(value)) => dynasm!(self ; neg Rw(value as u8)),
            (Size::S16, Location::Memory(value, disp)) => {
                dynasm!(self ; neg [Rq(value as u8) + disp])
            }
            (Size::S32, Location::GPR(value)) => dynasm!(self ; neg Rd(value as u8)),
            (Size::S32, Location::Memory(value, disp)) => {
                dynasm!(self ; neg [Rq(value as u8) + disp])
            }
            (Size::S64, Location::GPR(value)) => dynasm!(self ; neg Rq(value as u8)),
            (Size::S64, Location::Memory(value, disp)) => {
                dynasm!(self ; neg [Rq(value as u8) + disp])
            }
            _ => panic!("singlepass can't emit NEG {:?} {:?}", sz, value),
        }
    }
    fn emit_imul(&mut self, sz: Size, src: Location, dst: Location) {
        binop_gpr_gpr!(imul, self, sz, src, dst, {
            binop_mem_gpr!(imul, self, sz, src, dst, {
                panic!("singlepass can't emit IMUL {:?} {:?} {:?}", sz, src, dst)
            })
        });
    }
    fn emit_imul_imm32_gpr64(&mut self, src: u32, dst: GPR) {
        dynasm!(self ; imul Rq(dst as u8), Rq(dst as u8), src as i32);
    }
    fn emit_div(&mut self, sz: Size, divisor: Location) {
        unop_gpr_or_mem!(div, self, sz, divisor, {
            panic!("singlepass can't emit DIV {:?} {:?}", sz, divisor)
        });
    }
    fn emit_idiv(&mut self, sz: Size, divisor: Location) {
        unop_gpr_or_mem!(idiv, self, sz, divisor, {
            panic!("singlepass can't emit IDIV {:?} {:?}", sz, divisor)
        });
    }
    fn emit_shl(&mut self, sz: Size, src: Location, dst: Location) {
        binop_shift!(shl, self, sz, src, dst, {
            panic!("singlepass can't emit SHL {:?} {:?} {:?}", sz, src, dst)
        });
    }
    fn emit_shr(&mut self, sz: Size, src: Location, dst: Location) {
        binop_shift!(shr, self, sz, src, dst, {
            panic!("singlepass can't emit SHR {:?} {:?} {:?}", sz, src, dst)
        });
    }
    fn emit_sar(&mut self, sz: Size, src: Location, dst: Location) {
        binop_shift!(sar, self, sz, src, dst, {
            panic!("singlepass can't emit SAR {:?} {:?} {:?}", sz, src, dst)
        });
    }
    fn emit_rol(&mut self, sz: Size, src: Location, dst: Location) {
        binop_shift!(rol, self, sz, src, dst, {
            panic!("singlepass can't emit ROL {:?} {:?} {:?}", sz, src, dst)
        });
    }
    fn emit_ror(&mut self, sz: Size, src: Location, dst: Location) {
        binop_shift!(ror, self, sz, src, dst, {
            panic!("singlepass can't emit ROR {:?} {:?} {:?}", sz, src, dst)
        });
    }
    fn emit_and(&mut self, sz: Size, src: Location, dst: Location) {
        binop_all_nofp!(and, self, sz, src, dst, {
            panic!("singlepass can't emit AND {:?} {:?} {:?}", sz, src, dst)
        });
    }
    fn emit_or(&mut self, sz: Size, src: Location, dst: Location) {
        binop_all_nofp!(or, self, sz, src, dst, {
            panic!("singlepass can't emit OR {:?} {:?} {:?}", sz, src, dst)
        });
    }
    fn emit_bsr(&mut self, sz: Size, src: Location, dst: Location) {
        binop_gpr_gpr!(bsr, self, sz, src, dst, {
            binop_mem_gpr!(bsr, self, sz, src, dst, {
                panic!("singlepass can't emit BSR {:?} {:?} {:?}", sz, src, dst)
            })
        });
    }
    fn emit_bsf(&mut self, sz: Size, src: Location, dst: Location) {
        binop_gpr_gpr!(bsf, self, sz, src, dst, {
            binop_mem_gpr!(bsf, self, sz, src, dst, {
                panic!("singlepass can't emit BSF {:?} {:?} {:?}", sz, src, dst)
            })
        });
    }
    fn emit_popcnt(&mut self, sz: Size, src: Location, dst: Location) {
        binop_gpr_gpr!(popcnt, self, sz, src, dst, {
            binop_mem_gpr!(popcnt, self, sz, src, dst, {
                panic!("singlepass can't emit POPCNT {:?} {:?} {:?}", sz, src, dst)
            })
        });
    }
    fn emit_movzx(&mut self, sz_src: Size, src: Location, sz_dst: Size, dst: Location) {
        match (sz_src, src, sz_dst, dst) {
            (Size::S8, Location::GPR(src), Size::S32, Location::GPR(dst)) => {
                dynasm!(self ; movzx Rd(dst as u8), Rb(src as u8));
            }
            (Size::S16, Location::GPR(src), Size::S32, Location::GPR(dst)) => {
                dynasm!(self ; movzx Rd(dst as u8), Rw(src as u8));
            }
            (Size::S8, Location::Memory(src, disp), Size::S32, Location::GPR(dst)) => {
                dynasm!(self ; movzx Rd(dst as u8), BYTE [Rq(src as u8) + disp]);
            }
            (Size::S16, Location::Memory(src, disp), Size::S32, Location::GPR(dst)) => {
                dynasm!(self ; movzx Rd(dst as u8), WORD [Rq(src as u8) + disp]);
            }
            (Size::S8, Location::GPR(src), Size::S64, Location::GPR(dst)) => {
                dynasm!(self ; movzx Rq(dst as u8), Rb(src as u8));
            }
            (Size::S16, Location::GPR(src), Size::S64, Location::GPR(dst)) => {
                dynasm!(self ; movzx Rq(dst as u8), Rw(src as u8));
            }
            (Size::S8, Location::Memory(src, disp), Size::S64, Location::GPR(dst)) => {
                dynasm!(self ; movzx Rq(dst as u8), BYTE [Rq(src as u8) + disp]);
            }
            (Size::S16, Location::Memory(src, disp), Size::S64, Location::GPR(dst)) => {
                dynasm!(self ; movzx Rq(dst as u8), WORD [Rq(src as u8) + disp]);
            }
            _ => {
                panic!(
                    "singlepass can't emit MOVZX {:?} {:?} {:?} {:?}",
                    sz_src, src, sz_dst, dst
                )
            }
        }
    }
    fn emit_movsx(&mut self, sz_src: Size, src: Location, sz_dst: Size, dst: Location) {
        match (sz_src, src, sz_dst, dst) {
            (Size::S8, Location::GPR(src), Size::S32, Location::GPR(dst)) => {
                dynasm!(self ; movsx Rd(dst as u8), Rb(src as u8));
            }
            (Size::S16, Location::GPR(src), Size::S32, Location::GPR(dst)) => {
                dynasm!(self ; movsx Rd(dst as u8), Rw(src as u8));
            }
            (Size::S8, Location::Memory(src, disp), Size::S32, Location::GPR(dst)) => {
                dynasm!(self ; movsx Rd(dst as u8), BYTE [Rq(src as u8) + disp]);
            }
            (Size::S16, Location::Memory(src, disp), Size::S32, Location::GPR(dst)) => {
                dynasm!(self ; movsx Rd(dst as u8), WORD [Rq(src as u8) + disp]);
            }
            (Size::S8, Location::GPR(src), Size::S64, Location::GPR(dst)) => {
                dynasm!(self ; movsx Rq(dst as u8), Rb(src as u8));
            }
            (Size::S16, Location::GPR(src), Size::S64, Location::GPR(dst)) => {
                dynasm!(self ; movsx Rq(dst as u8), Rw(src as u8));
            }
            (Size::S32, Location::GPR(src), Size::S64, Location::GPR(dst)) => {
                dynasm!(self ; movsx Rq(dst as u8), Rd(src as u8));
            }
            (Size::S8, Location::Memory(src, disp), Size::S64, Location::GPR(dst)) => {
                dynasm!(self ; movsx Rq(dst as u8), BYTE [Rq(src as u8) + disp]);
            }
            (Size::S16, Location::Memory(src, disp), Size::S64, Location::GPR(dst)) => {
                dynasm!(self ; movsx Rq(dst as u8), WORD [Rq(src as u8) + disp]);
            }
            (Size::S32, Location::Memory(src, disp), Size::S64, Location::GPR(dst)) => {
                dynasm!(self ; movsx Rq(dst as u8), DWORD [Rq(src as u8) + disp]);
            }
            _ => {
                panic!(
                    "singlepass can't emit MOVSX {:?} {:?} {:?} {:?}",
                    sz_src, src, sz_dst, dst
                )
            }
        }
    }

    fn emit_xchg(&mut self, sz: Size, src: Location, dst: Location) {
        match (sz, src, dst) {
            (Size::S8, Location::GPR(src), Location::GPR(dst)) => {
                dynasm!(self ; xchg Rb(dst as u8), Rb(src as u8));
            }
            (Size::S16, Location::GPR(src), Location::GPR(dst)) => {
                dynasm!(self ; xchg Rw(dst as u8), Rw(src as u8));
            }
            (Size::S32, Location::GPR(src), Location::GPR(dst)) => {
                dynasm!(self ; xchg Rd(dst as u8), Rd(src as u8));
            }
            (Size::S64, Location::GPR(src), Location::GPR(dst)) => {
                dynasm!(self ; xchg Rq(dst as u8), Rq(src as u8));
            }
            (Size::S8, Location::Memory(src, disp), Location::GPR(dst)) => {
                dynasm!(self ; xchg Rb(dst as u8), [Rq(src as u8) + disp]);
            }
            (Size::S8, Location::GPR(src), Location::Memory(dst, disp)) => {
                dynasm!(self ; xchg [Rq(dst as u8) + disp], Rb(src as u8));
            }
            (Size::S16, Location::Memory(src, disp), Location::GPR(dst)) => {
                dynasm!(self ; xchg Rw(dst as u8), [Rq(src as u8) + disp]);
            }
            (Size::S16, Location::GPR(src), Location::Memory(dst, disp)) => {
                dynasm!(self ; xchg [Rq(dst as u8) + disp], Rw(src as u8));
            }
            (Size::S32, Location::Memory(src, disp), Location::GPR(dst)) => {
                dynasm!(self ; xchg Rd(dst as u8), [Rq(src as u8) + disp]);
            }
            (Size::S32, Location::GPR(src), Location::Memory(dst, disp)) => {
                dynasm!(self ; xchg [Rq(dst as u8) + disp], Rd(src as u8));
            }
            (Size::S64, Location::Memory(src, disp), Location::GPR(dst)) => {
                dynasm!(self ; xchg Rq(dst as u8), [Rq(src as u8) + disp]);
            }
            (Size::S64, Location::GPR(src), Location::Memory(dst, disp)) => {
                dynasm!(self ; xchg [Rq(dst as u8) + disp], Rq(src as u8));
            }
            _ => panic!("singlepass can't emit XCHG {:?} {:?} {:?}", sz, src, dst),
        }
    }

    fn emit_lock_xadd(&mut self, sz: Size, src: Location, dst: Location) {
        match (sz, src, dst) {
            (Size::S8, Location::GPR(src), Location::Memory(dst, disp)) => {
                dynasm!(self ; lock xadd [Rq(dst as u8) + disp], Rb(src as u8));
            }
            (Size::S16, Location::GPR(src), Location::Memory(dst, disp)) => {
                dynasm!(self ; lock xadd [Rq(dst as u8) + disp], Rw(src as u8));
            }
            (Size::S32, Location::GPR(src), Location::Memory(dst, disp)) => {
                dynasm!(self ; lock xadd [Rq(dst as u8) + disp], Rd(src as u8));
            }
            (Size::S64, Location::GPR(src), Location::Memory(dst, disp)) => {
                dynasm!(self ; lock xadd [Rq(dst as u8) + disp], Rq(src as u8));
            }
            _ => panic!(
                "singlepass can't emit LOCK XADD {:?} {:?} {:?}",
                sz, src, dst
            ),
        }
    }

    fn emit_lock_cmpxchg(&mut self, sz: Size, src: Location, dst: Location) {
        match (sz, src, dst) {
            (Size::S8, Location::GPR(src), Location::Memory(dst, disp)) => {
                dynasm!(self ; lock cmpxchg [Rq(dst as u8) + disp], Rb(src as u8));
            }
            (Size::S16, Location::GPR(src), Location::Memory(dst, disp)) => {
                dynasm!(self ; lock cmpxchg [Rq(dst as u8) + disp], Rw(src as u8));
            }
            (Size::S32, Location::GPR(src), Location::Memory(dst, disp)) => {
                dynasm!(self ; lock cmpxchg [Rq(dst as u8) + disp], Rd(src as u8));
            }
            (Size::S64, Location::GPR(src), Location::Memory(dst, disp)) => {
                dynasm!(self ; lock cmpxchg [Rq(dst as u8) + disp], Rq(src as u8));
            }
            _ => panic!(
                "singlepass can't emit LOCK CMPXCHG {:?} {:?} {:?}",
                sz, src, dst
            ),
        }
    }

    fn emit_rep_stosq(&mut self) {
        dynasm!(self ; rep stosq);
    }
    fn emit_btc_gpr_imm8_32(&mut self, src: u8, dst: GPR) {
        dynasm!(self ; btc Rd(dst as u8), BYTE src as i8);
    }

    fn emit_btc_gpr_imm8_64(&mut self, src: u8, dst: GPR) {
        dynasm!(self ; btc Rq(dst as u8), BYTE src as i8);
    }

    fn emit_cmovae_gpr_32(&mut self, src: GPR, dst: GPR) {
        dynasm!(self ; cmovae Rd(dst as u8), Rd(src as u8));
    }

    fn emit_cmovae_gpr_64(&mut self, src: GPR, dst: GPR) {
        dynasm!(self ; cmovae Rq(dst as u8), Rq(src as u8));
    }

    fn emit_vmovaps(&mut self, src: XMMOrMemory, dst: XMMOrMemory) {
        match (src, dst) {
            (XMMOrMemory::XMM(src), XMMOrMemory::XMM(dst)) => {
                dynasm!(self ; movaps Rx(dst as u8), Rx(src as u8))
            }
            (XMMOrMemory::Memory(base, disp), XMMOrMemory::XMM(dst)) => {
                dynasm!(self ; movaps Rx(dst as u8), [Rq(base as u8) + disp])
            }
            (XMMOrMemory::XMM(src), XMMOrMemory::Memory(base, disp)) => {
                dynasm!(self ; movaps [Rq(base as u8) + disp], Rx(src as u8))
            }
            _ => panic!("singlepass can't emit VMOVAPS {:?} {:?}", src, dst),
        };
    }

    fn emit_vmovapd(&mut self, src: XMMOrMemory, dst: XMMOrMemory) {
        match (src, dst) {
            (XMMOrMemory::XMM(src), XMMOrMemory::XMM(dst)) => {
                dynasm!(self ; movapd Rx(dst as u8), Rx(src as u8))
            }
            (XMMOrMemory::Memory(base, disp), XMMOrMemory::XMM(dst)) => {
                dynasm!(self ; movapd Rx(dst as u8), [Rq(base as u8) + disp])
            }
            (XMMOrMemory::XMM(src), XMMOrMemory::Memory(base, disp)) => {
                dynasm!(self ; movapd [Rq(base as u8) + disp], Rx(src as u8))
            }
            _ => panic!("singlepass can't emit VMOVAPD {:?} {:?}", src, dst),
        };
    }

    avx_fn!(vxorps, emit_vxorps);
    avx_fn!(vxorpd, emit_vxorpd);

    avx_fn!(vaddss, emit_vaddss);
    avx_fn!(vaddsd, emit_vaddsd);

    avx_fn!(vsubss, emit_vsubss);
    avx_fn!(vsubsd, emit_vsubsd);

    avx_fn!(vmulss, emit_vmulss);
    avx_fn!(vmulsd, emit_vmulsd);

    avx_fn!(vdivss, emit_vdivss);
    avx_fn!(vdivsd, emit_vdivsd);

    avx_fn!(vmaxss, emit_vmaxss);
    avx_fn!(vmaxsd, emit_vmaxsd);

    avx_fn!(vminss, emit_vminss);
    avx_fn!(vminsd, emit_vminsd);

    avx_fn!(vcmpeqss, emit_vcmpeqss);
    avx_fn!(vcmpeqsd, emit_vcmpeqsd);

    avx_fn!(vcmpneqss, emit_vcmpneqss);
    avx_fn!(vcmpneqsd, emit_vcmpneqsd);

    avx_fn!(vcmpltss, emit_vcmpltss);
    avx_fn!(vcmpltsd, emit_vcmpltsd);

    avx_fn!(vcmpless, emit_vcmpless);
    avx_fn!(vcmplesd, emit_vcmplesd);

    avx_fn!(vcmpgtss, emit_vcmpgtss);
    avx_fn!(vcmpgtsd, emit_vcmpgtsd);

    avx_fn!(vcmpgess, emit_vcmpgess);
    avx_fn!(vcmpgesd, emit_vcmpgesd);

    avx_fn!(vcmpunordss, emit_vcmpunordss);
    avx_fn!(vcmpunordsd, emit_vcmpunordsd);

    avx_fn!(vcmpordss, emit_vcmpordss);
    avx_fn!(vcmpordsd, emit_vcmpordsd);

    avx_fn!(vsqrtss, emit_vsqrtss);
    avx_fn!(vsqrtsd, emit_vsqrtsd);

    avx_fn!(vcvtss2sd, emit_vcvtss2sd);
    avx_fn!(vcvtsd2ss, emit_vcvtsd2ss);

    avx_round_fn!(vroundss, emit_vroundss_nearest, 0);
    avx_round_fn!(vroundss, emit_vroundss_floor, 1);
    avx_round_fn!(vroundss, emit_vroundss_ceil, 2);
    avx_round_fn!(vroundss, emit_vroundss_trunc, 3);
    avx_round_fn!(vroundsd, emit_vroundsd_nearest, 0);
    avx_round_fn!(vroundsd, emit_vroundsd_floor, 1);
    avx_round_fn!(vroundsd, emit_vroundsd_ceil, 2);
    avx_round_fn!(vroundsd, emit_vroundsd_trunc, 3);

    avx_i2f_32_fn!(vcvtsi2ss, emit_vcvtsi2ss_32);
    avx_i2f_32_fn!(vcvtsi2sd, emit_vcvtsi2sd_32);
    avx_i2f_64_fn!(vcvtsi2ss, emit_vcvtsi2ss_64);
    avx_i2f_64_fn!(vcvtsi2sd, emit_vcvtsi2sd_64);

    fn emit_vblendvps(&mut self, src1: XMM, src2: XMMOrMemory, mask: XMM, dst: XMM) {
        match src2 {
            XMMOrMemory::XMM(src2) => {
                dynasm!(self ; vblendvps Rx(dst as u8), Rx(mask as u8), Rx(src2 as u8), Rx(src1 as u8))
            }
            XMMOrMemory::Memory(base, disp) => {
                dynasm!(self ; vblendvps Rx(dst as u8), Rx(mask as u8), [Rq(base as u8) + disp], Rx(src1 as u8))
            }
        }
    }

    fn emit_vblendvpd(&mut self, src1: XMM, src2: XMMOrMemory, mask: XMM, dst: XMM) {
        match src2 {
            XMMOrMemory::XMM(src2) => {
                dynasm!(self ; vblendvpd Rx(dst as u8), Rx(mask as u8), Rx(src2 as u8), Rx(src1 as u8))
            }
            XMMOrMemory::Memory(base, disp) => {
                dynasm!(self ; vblendvpd Rx(dst as u8), Rx(mask as u8), [Rq(base as u8) + disp], Rx(src1 as u8))
            }
        }
    }

    fn emit_ucomiss(&mut self, src: XMMOrMemory, dst: XMM) {
        match src {
            XMMOrMemory::XMM(x) => dynasm!(self ; ucomiss Rx(dst as u8), Rx(x as u8)),
            XMMOrMemory::Memory(base, disp) => {
                dynasm!(self ; ucomiss Rx(dst as u8), [Rq(base as u8) + disp])
            }
        }
    }

    fn emit_ucomisd(&mut self, src: XMMOrMemory, dst: XMM) {
        match src {
            XMMOrMemory::XMM(x) => dynasm!(self ; ucomisd Rx(dst as u8), Rx(x as u8)),
            XMMOrMemory::Memory(base, disp) => {
                dynasm!(self ; ucomisd Rx(dst as u8), [Rq(base as u8) + disp])
            }
        }
    }

    fn emit_cvttss2si_32(&mut self, src: XMMOrMemory, dst: GPR) {
        match src {
            XMMOrMemory::XMM(x) => dynasm!(self ; cvttss2si Rd(dst as u8), Rx(x as u8)),
            XMMOrMemory::Memory(base, disp) => {
                dynasm!(self ; cvttss2si Rd(dst as u8), [Rq(base as u8) + disp])
            }
        }
    }

    fn emit_cvttss2si_64(&mut self, src: XMMOrMemory, dst: GPR) {
        match src {
            XMMOrMemory::XMM(x) => dynasm!(self ; cvttss2si Rq(dst as u8), Rx(x as u8)),
            XMMOrMemory::Memory(base, disp) => {
                dynasm!(self ; cvttss2si Rq(dst as u8), [Rq(base as u8) + disp])
            }
        }
    }

    fn emit_cvttsd2si_32(&mut self, src: XMMOrMemory, dst: GPR) {
        match src {
            XMMOrMemory::XMM(x) => dynasm!(self ; cvttsd2si Rd(dst as u8), Rx(x as u8)),
            XMMOrMemory::Memory(base, disp) => {
                dynasm!(self ; cvttsd2si Rd(dst as u8), [Rq(base as u8) + disp])
            }
        }
    }

    fn emit_cvttsd2si_64(&mut self, src: XMMOrMemory, dst: GPR) {
        match src {
            XMMOrMemory::XMM(x) => dynasm!(self ; cvttsd2si Rq(dst as u8), Rx(x as u8)),
            XMMOrMemory::Memory(base, disp) => {
                dynasm!(self ; cvttsd2si Rq(dst as u8), [Rq(base as u8) + disp])
            }
        }
    }

    fn emit_test_gpr_64(&mut self, reg: GPR) {
        dynasm!(self ; test Rq(reg as u8), Rq(reg as u8));
    }

    fn emit_ud2(&mut self) {
        dynasm!(self ; ud2);
    }
    fn emit_ret(&mut self) {
        dynasm!(self ; ret);
    }

    fn emit_call_label(&mut self, label: Self::Label) {
        dynasm!(self ; call =>label);
    }
    fn emit_call_location(&mut self, loc: Location) {
        match loc {
            Location::GPR(x) => dynasm!(self ; call Rq(x as u8)),
            Location::Memory(base, disp) => dynasm!(self ; call QWORD [Rq(base as u8) + disp]),
            _ => panic!("singlepass can't emit CALL {:?}", loc),
        }
    }

    fn emit_call_register(&mut self, reg: GPR) {
        dynasm!(self ; call Rq(reg as u8));
    }

    fn emit_bkpt(&mut self) {
        dynasm!(self ; int3);
    }

    fn emit_host_redirection(&mut self, target: GPR) {
        self.emit_jmp_location(Location::GPR(target));
    }

    fn arch_mov64_imm_offset(&self) -> usize {
        2
    }
}

'''
'''--- lib/compiler-singlepass/src/lib.rs ---
//! A WebAssembly `Compiler` implementation using Singlepass.
//!
//! Singlepass is a super-fast assembly generator that generates
//! assembly code in just one pass. This is useful for different applications
//! including Blockchains and Edge computing where quick compilation
//! times are a must, and JIT bombs should never happen.
//!
//! Compared to Cranelift and LLVM, Singlepass compiles much faster but has worse
//! runtime performance.

mod address_map;
mod codegen_x64;
mod compiler;
mod config;
mod emitter_x64;
mod machine;
mod x64_decl;

pub use crate::compiler::SinglepassCompiler;
pub use crate::config::Singlepass;

'''
'''--- lib/compiler-singlepass/src/machine.rs ---
use crate::emitter_x64::*;
use smallvec::smallvec;
use smallvec::SmallVec;
use std::cmp;
use std::collections::HashSet;
use wasmer_compiler::wasmparser::Type as WpType;
use wasmer_compiler::CallingConvention;

const NATIVE_PAGE_SIZE: usize = 4096;

struct MachineStackOffset(usize);

pub(crate) struct Machine {
    used_gprs: HashSet<GPR>,
    used_xmms: HashSet<XMM>,
    stack_offset: MachineStackOffset,
    save_area_offset: Option<MachineStackOffset>,
}

impl Machine {
    pub(crate) fn new() -> Self {
        Machine {
            used_gprs: HashSet::new(),
            used_xmms: HashSet::new(),
            stack_offset: MachineStackOffset(0),
            save_area_offset: None,
        }
    }

    pub(crate) fn get_stack_offset(&self) -> usize {
        self.stack_offset.0
    }

    pub(crate) fn get_used_gprs(&self) -> Vec<GPR> {
        let mut result = self.used_gprs.iter().cloned().collect::<Vec<_>>();
        result.sort_unstable();
        result
    }

    pub(crate) fn get_used_xmms(&self) -> Vec<XMM> {
        let mut result = self.used_xmms.iter().cloned().collect::<Vec<_>>();
        result.sort_unstable();
        result
    }

    pub(crate) fn get_vmctx_reg() -> GPR {
        GPR::R15
    }

    /// Picks an unused general purpose register for local/stack/argument use.
    ///
    /// This method does not mark the register as used.
    pub(crate) fn pick_gpr(&self) -> Option<GPR> {
        use GPR::*;
        static REGS: &[GPR] = &[RSI, RDI, R8, R9, R10, R11];
        for r in REGS {
            if !self.used_gprs.contains(r) {
                return Some(*r);
            }
        }
        None
    }

    /// Picks an unused general purpose register for internal temporary use.
    ///
    /// This method does not mark the register as used.
    pub(crate) fn pick_temp_gpr(&self) -> Option<GPR> {
        use GPR::*;
        static REGS: &[GPR] = &[RAX, RCX, RDX];
        for r in REGS {
            if !self.used_gprs.contains(r) {
                return Some(*r);
            }
        }
        None
    }

    /// Acquires a temporary GPR.
    pub(crate) fn acquire_temp_gpr(&mut self) -> Option<GPR> {
        let gpr = self.pick_temp_gpr();
        if let Some(x) = gpr {
            self.used_gprs.insert(x);
        }
        gpr
    }

    /// Releases a temporary GPR.
    pub(crate) fn release_temp_gpr(&mut self, gpr: GPR) {
        assert!(self.used_gprs.remove(&gpr));
    }

    /// Specify that a given register is in use.
    pub(crate) fn reserve_unused_temp_gpr(&mut self, gpr: GPR) -> GPR {
        assert!(!self.used_gprs.contains(&gpr));
        self.used_gprs.insert(gpr);
        gpr
    }

    /// Picks an unused XMM register.
    ///
    /// This method does not mark the register as used.
    pub(crate) fn pick_xmm(&self) -> Option<XMM> {
        use XMM::*;
        static REGS: &[XMM] = &[XMM3, XMM4, XMM5, XMM6, XMM7];
        for r in REGS {
            if !self.used_xmms.contains(r) {
                return Some(*r);
            }
        }
        None
    }

    /// Picks an unused XMM register for internal temporary use.
    ///
    /// This method does not mark the register as used.
    pub(crate) fn pick_temp_xmm(&self) -> Option<XMM> {
        use XMM::*;
        static REGS: &[XMM] = &[XMM0, XMM1, XMM2];
        for r in REGS {
            if !self.used_xmms.contains(r) {
                return Some(*r);
            }
        }
        None
    }

    /// Acquires a temporary XMM register.
    pub(crate) fn acquire_temp_xmm(&mut self) -> Option<XMM> {
        let xmm = self.pick_temp_xmm();
        if let Some(x) = xmm {
            self.used_xmms.insert(x);
        }
        xmm
    }

    /// Releases a temporary XMM register.
    pub(crate) fn release_temp_xmm(&mut self, xmm: XMM) {
        assert_eq!(self.used_xmms.remove(&xmm), true);
    }

    /// Acquires locations from the machine state.
    ///
    /// If the returned locations are used for stack value, `release_location` needs to be called on them;
    /// Otherwise, if the returned locations are used for locals, `release_location` does not need to be called on them.
    pub(crate) fn acquire_locations<E: Emitter>(
        &mut self,
        assembler: &mut E,
        tys: &[WpType],
        zeroed: bool,
    ) -> SmallVec<[Location; 1]> {
        let mut ret = smallvec![];
        let mut delta_stack_offset: usize = 0;

        for ty in tys {
            let loc = match *ty {
                WpType::F32 | WpType::F64 => self.pick_xmm().map(Location::XMM),
                WpType::I32 | WpType::I64 => self.pick_gpr().map(Location::GPR),
                WpType::FuncRef | WpType::ExternRef => self.pick_gpr().map(Location::GPR),
                _ => unreachable!("can't acquire location for type {:?}", ty),
            };

            let loc = if let Some(x) = loc {
                x
            } else {
                self.stack_offset.0 += 8;
                delta_stack_offset += 8;
                Location::Memory(GPR::RBP, -(self.stack_offset.0 as i32))
            };
            if let Location::GPR(x) = loc {
                self.used_gprs.insert(x);
            } else if let Location::XMM(x) = loc {
                self.used_xmms.insert(x);
            }
            ret.push(loc);
        }

        if delta_stack_offset != 0 {
            assembler.emit_sub(
                Size::S64,
                Location::Imm32(delta_stack_offset as u32),
                Location::GPR(GPR::RSP),
            );
        }
        if zeroed {
            for i in 0..tys.len() {
                assembler.emit_mov(Size::S64, Location::Imm32(0), ret[i]);
            }
        }
        ret
    }

    /// Releases locations used for stack value.
    pub(crate) fn release_locations<E: Emitter>(&mut self, assembler: &mut E, locs: &[Location]) {
        let mut delta_stack_offset: usize = 0;

        for loc in locs.iter().rev() {
            match *loc {
                Location::GPR(ref x) => {
                    assert_eq!(self.used_gprs.remove(x), true);
                }
                Location::XMM(ref x) => {
                    assert_eq!(self.used_xmms.remove(x), true);
                }
                Location::Memory(GPR::RBP, x) => {
                    if x >= 0 {
                        unreachable!();
                    }
                    let offset = (-x) as usize;
                    if offset != self.stack_offset.0 {
                        unreachable!();
                    }
                    self.stack_offset.0 -= 8;
                    delta_stack_offset += 8;
                }
                _ => {}
            }
        }

        if delta_stack_offset != 0 {
            assembler.emit_add(
                Size::S64,
                Location::Imm32(delta_stack_offset as u32),
                Location::GPR(GPR::RSP),
            );
        }
    }

    pub(crate) fn release_locations_only_regs(&mut self, locs: &[Location]) {
        for loc in locs.iter().rev() {
            match *loc {
                Location::GPR(ref x) => {
                    assert_eq!(self.used_gprs.remove(x), true);
                }
                Location::XMM(ref x) => {
                    assert_eq!(self.used_xmms.remove(x), true);
                }
                _ => {}
            }
        }
    }

    pub(crate) fn release_locations_only_stack<E: Emitter>(
        &mut self,
        assembler: &mut E,
        locs: &[Location],
    ) {
        let mut delta_stack_offset: usize = 0;

        for loc in locs.iter().rev() {
            if let Location::Memory(GPR::RBP, x) = *loc {
                if x >= 0 {
                    unreachable!();
                }
                let offset = (-x) as usize;
                if offset != self.stack_offset.0 {
                    unreachable!();
                }
                self.stack_offset.0 -= 8;
                delta_stack_offset += 8;
            }
        }

        if delta_stack_offset != 0 {
            assembler.emit_add(
                Size::S64,
                Location::Imm32(delta_stack_offset as u32),
                Location::GPR(GPR::RSP),
            );
        }
    }

    pub(crate) fn release_locations_keep_state<E: Emitter>(
        &self,
        assembler: &mut E,
        locs: &[Location],
    ) {
        let mut delta_stack_offset: usize = 0;
        let mut stack_offset = self.stack_offset.0;

        for loc in locs.iter().rev() {
            if let Location::Memory(GPR::RBP, x) = *loc {
                if x >= 0 {
                    unreachable!();
                }
                let offset = (-x) as usize;
                if offset != stack_offset {
                    unreachable!();
                }
                stack_offset -= 8;
                delta_stack_offset += 8;
            }
        }

        if delta_stack_offset != 0 {
            assembler.emit_add(
                Size::S64,
                Location::Imm32(delta_stack_offset as u32),
                Location::GPR(GPR::RSP),
            );
        }
    }

    pub(crate) fn init_locals<E: Emitter>(
        &mut self,
        a: &mut E,
        n: usize,
        n_params: usize,
        calling_convention: CallingConvention,
    ) -> Vec<Location> {
        // Determine whether a local should be allocated on the stack.
        fn is_local_on_stack(idx: usize) -> bool {
            idx > 3
        }

        // Determine a local's location.
        fn get_local_location(idx: usize, callee_saved_regs_size: usize) -> Location {
            // Use callee-saved registers for the first locals.
            match idx {
                0 => Location::GPR(GPR::R12),
                1 => Location::GPR(GPR::R13),
                2 => Location::GPR(GPR::R14),
                3 => Location::GPR(GPR::RBX),
                _ => Location::Memory(GPR::RBP, -(((idx - 3) * 8 + callee_saved_regs_size) as i32)),
            }
        }

        // How many machine stack slots will all the locals use?
        let num_mem_slots = (0..n).filter(|&x| is_local_on_stack(x)).count();

        // Total size (in bytes) of the pre-allocated "static area" for this function's
        // locals and callee-saved registers.
        let mut static_area_size: usize = 0;

        // Callee-saved registers used for locals.
        // Keep this consistent with the "Save callee-saved registers" code below.
        for i in 0..n {
            // If a local is not stored on stack, then it is allocated to a callee-saved register.
            if !is_local_on_stack(i) {
                static_area_size += 8;
            }
        }

        // Callee-saved R15 for vmctx.
        static_area_size += 8;

        // For Windows ABI, save RDI and RSI
        if calling_convention == CallingConvention::WindowsFastcall {
            static_area_size += 8 * 2;
        }

        // Total size of callee saved registers.
        let callee_saved_regs_size = static_area_size;

        // Now we can determine concrete locations for locals.
        let locations: Vec<Location> = (0..n)
            .map(|i| get_local_location(i, callee_saved_regs_size))
            .collect();

        // Add size of locals on stack.
        static_area_size += num_mem_slots * 8;

        // Allocate save area, without actually writing to it.
        a.emit_sub(
            Size::S64,
            Location::Imm32(static_area_size as _),
            Location::GPR(GPR::RSP),
        );

        // Save callee-saved registers.
        for loc in locations.iter() {
            if let Location::GPR(_) = *loc {
                self.stack_offset.0 += 8;
                a.emit_mov(
                    Size::S64,
                    *loc,
                    Location::Memory(GPR::RBP, -(self.stack_offset.0 as i32)),
                );
            }
        }

        // Save R15 for vmctx use.
        self.stack_offset.0 += 8;
        a.emit_mov(
            Size::S64,
            Location::GPR(GPR::R15),
            Location::Memory(GPR::RBP, -(self.stack_offset.0 as i32)),
        );

        if calling_convention == CallingConvention::WindowsFastcall {
            // Save RDI
            self.stack_offset.0 += 8;
            a.emit_mov(
                Size::S64,
                Location::GPR(GPR::RDI),
                Location::Memory(GPR::RBP, -(self.stack_offset.0 as i32)),
            );
            // Save RSI
            self.stack_offset.0 += 8;
            a.emit_mov(
                Size::S64,
                Location::GPR(GPR::RSI),
                Location::Memory(GPR::RBP, -(self.stack_offset.0 as i32)),
            );
        }

        // Save the offset of register save area.
        self.save_area_offset = Some(MachineStackOffset(self.stack_offset.0));

        // Load in-register parameters into the allocated locations.
        // Locals are allocated on the stack from higher address to lower address,
        // so we won't skip the stack guard page here.
        for i in 0..n_params {
            let loc = Self::get_param_location(i + 1, calling_convention);
            match loc {
                Location::GPR(_) => {
                    a.emit_mov(Size::S64, loc, locations[i]);
                }
                Location::Memory(_, _) => match locations[i] {
                    Location::GPR(_) => {
                        a.emit_mov(Size::S64, loc, locations[i]);
                    }
                    Location::Memory(_, _) => {
                        a.emit_mov(Size::S64, loc, Location::GPR(GPR::RAX));
                        a.emit_mov(Size::S64, Location::GPR(GPR::RAX), locations[i]);
                    }
                    _ => unreachable!(),
                },
                _ => unreachable!(),
            }
        }

        // Load vmctx into R15.
        a.emit_mov(
            Size::S64,
            Self::get_param_location(0, calling_convention),
            Location::GPR(GPR::R15),
        );

        // Stack probe.
        //
        // `rep stosq` writes data from low address to high address and may skip the stack guard page.
        // so here we probe it explicitly when needed.
        for i in (n_params..n).step_by(NATIVE_PAGE_SIZE / 8).skip(1) {
            a.emit_mov(Size::S64, Location::Imm32(0), locations[i]);
        }

        // Initialize all normal locals to zero.
        let mut init_stack_loc_cnt = 0;
        let mut last_stack_loc = Location::Memory(GPR::RBP, i32::MAX);
        for i in n_params..n {
            match locations[i] {
                Location::Memory(_, _) => {
                    init_stack_loc_cnt += 1;
                    last_stack_loc = cmp::min(last_stack_loc, locations[i]);
                }
                Location::GPR(_) => {
                    a.emit_mov(Size::S64, Location::Imm32(0), locations[i]);
                }
                _ => unreachable!(),
            }
        }
        if init_stack_loc_cnt > 0 {
            // Since these assemblies take up to 24 bytes, if more than 2 slots are initialized, then they are smaller.
            a.emit_mov(
                Size::S64,
                Location::Imm64(init_stack_loc_cnt as u64),
                Location::GPR(GPR::RCX),
            );
            a.emit_xor(Size::S64, Location::GPR(GPR::RAX), Location::GPR(GPR::RAX));
            a.emit_lea(Size::S64, last_stack_loc, Location::GPR(GPR::RDI));
            a.emit_rep_stosq();
        }

        // Add the size of all locals allocated to stack.
        self.stack_offset.0 += static_area_size - callee_saved_regs_size;

        locations
    }

    pub(crate) fn finalize_locals<E: Emitter>(
        &mut self,
        a: &mut E,
        locations: &[Location],
        calling_convention: CallingConvention,
    ) {
        // Unwind stack to the "save area".
        a.emit_lea(
            Size::S64,
            Location::Memory(
                GPR::RBP,
                -(self.save_area_offset.as_ref().unwrap().0 as i32),
            ),
            Location::GPR(GPR::RSP),
        );

        if calling_convention == CallingConvention::WindowsFastcall {
            // Restore RSI and RDI
            a.emit_pop(Size::S64, Location::GPR(GPR::RSI));
            a.emit_pop(Size::S64, Location::GPR(GPR::RDI));
        }
        // Restore R15 used by vmctx.
        a.emit_pop(Size::S64, Location::GPR(GPR::R15));

        // Restore callee-saved registers.
        for loc in locations.iter().rev() {
            if let Location::GPR(_) = *loc {
                a.emit_pop(Size::S64, *loc);
            }
        }
    }

    pub(crate) fn get_param_location(
        idx: usize,
        calling_convention: CallingConvention,
    ) -> Location {
        match calling_convention {
            CallingConvention::WindowsFastcall => match idx {
                0 => Location::GPR(GPR::RCX),
                1 => Location::GPR(GPR::RDX),
                2 => Location::GPR(GPR::R8),
                3 => Location::GPR(GPR::R9),
                _ => Location::Memory(GPR::RBP, (16 + 32 + (idx - 4) * 8) as i32),
            },
            _ => match idx {
                0 => Location::GPR(GPR::RDI),
                1 => Location::GPR(GPR::RSI),
                2 => Location::GPR(GPR::RDX),
                3 => Location::GPR(GPR::RCX),
                4 => Location::GPR(GPR::R8),
                5 => Location::GPR(GPR::R9),
                _ => Location::Memory(GPR::RBP, (16 + (idx - 6) * 8) as i32),
            },
        }
    }
}

#[cfg(test)]
mod test {
    use super::*;
    use dynasmrt::x64::X64Relocation;
    use dynasmrt::VecAssembler;
    type Assembler = VecAssembler<X64Relocation>;

    #[test]
    fn test_release_locations_keep_state_nopanic() {
        let mut machine = Machine::new();
        let mut assembler = Assembler::new(0);
        let locs = machine.acquire_locations(
            &mut assembler,
            &(0..10).map(|_| WpType::I32).collect::<Vec<_>>(),
            false,
        );

        machine.release_locations_keep_state(&mut assembler, &locs);
    }
}

'''
'''--- lib/compiler-singlepass/src/x64_decl.rs ---
//! X64 structures.
use wasmer_compiler::CallingConvention;
use wasmer_types::Type;

/// General-purpose registers.
#[repr(u8)]
#[derive(Copy, Clone, Debug, Eq, PartialEq, Ord, PartialOrd, Hash)]
pub(crate) enum GPR {
    /// RAX register
    RAX,
    /// RCX register
    RCX,
    /// RDX register
    RDX,
    /// RBX register
    RBX,
    /// RSP register
    RSP,
    /// RBP register
    RBP,
    /// RSI register
    RSI,
    /// RDI register
    RDI,
    /// R8 register
    R8,
    /// R9 register
    R9,
    /// R10 register
    R10,
    /// R11 register
    R11,
    /// R12 register
    R12,
    /// R13 register
    R13,
    /// R14 register
    R14,
    /// R15 register
    R15,
}

/// XMM registers.
#[repr(u8)]
#[derive(Copy, Clone, Debug, Eq, PartialEq, Ord, PartialOrd, Hash)]
#[allow(dead_code)]
pub(crate) enum XMM {
    /// XMM register 0
    XMM0,
    /// XMM register 1
    XMM1,
    /// XMM register 2
    XMM2,
    /// XMM register 3
    XMM3,
    /// XMM register 4
    XMM4,
    /// XMM register 5
    XMM5,
    /// XMM register 6
    XMM6,
    /// XMM register 7
    XMM7,
    /// XMM register 8
    XMM8,
    /// XMM register 9
    XMM9,
    /// XMM register 10
    XMM10,
    /// XMM register 11
    XMM11,
    /// XMM register 12
    XMM12,
    /// XMM register 13
    XMM13,
    /// XMM register 14
    XMM14,
    /// XMM register 15
    XMM15,
}

/// A machine register under the x86-64 architecture.
#[derive(Copy, Clone, Debug, Eq, PartialEq)]
pub(crate) enum X64Register {
    /// General-purpose registers.
    GPR(GPR),
    /// XMM (floating point/SIMD) registers.
    XMM(XMM),
}

impl X64Register {
    /// Converts a DWARF regnum to X64Register.
    pub(crate) fn _from_dwarf_regnum(x: u16) -> Option<X64Register> {
        Some(match x {
            0 => X64Register::GPR(GPR::RAX),
            1 => X64Register::GPR(GPR::RDX),
            2 => X64Register::GPR(GPR::RCX),
            3 => X64Register::GPR(GPR::RBX),
            4 => X64Register::GPR(GPR::RSI),
            5 => X64Register::GPR(GPR::RDI),
            6 => X64Register::GPR(GPR::RBP),
            7 => X64Register::GPR(GPR::RSP),
            8 => X64Register::GPR(GPR::R8),
            9 => X64Register::GPR(GPR::R9),
            10 => X64Register::GPR(GPR::R10),
            11 => X64Register::GPR(GPR::R11),
            12 => X64Register::GPR(GPR::R12),
            13 => X64Register::GPR(GPR::R13),
            14 => X64Register::GPR(GPR::R14),
            15 => X64Register::GPR(GPR::R15),

            17 => X64Register::XMM(XMM::XMM0),
            18 => X64Register::XMM(XMM::XMM1),
            19 => X64Register::XMM(XMM::XMM2),
            20 => X64Register::XMM(XMM::XMM3),
            21 => X64Register::XMM(XMM::XMM4),
            22 => X64Register::XMM(XMM::XMM5),
            23 => X64Register::XMM(XMM::XMM6),
            24 => X64Register::XMM(XMM::XMM7),
            _ => return None,
        })
    }

    /// Returns the instruction prefix for `movq %this_reg, ?(%rsp)`.
    ///
    /// To build an instruction, append the memory location as a 32-bit
    /// offset to the stack pointer to this prefix.
    pub(crate) fn _prefix_mov_to_stack(&self) -> Option<&'static [u8]> {
        Some(match *self {
            X64Register::GPR(gpr) => match gpr {
                GPR::RDI => &[0x48, 0x89, 0xbc, 0x24],
                GPR::RSI => &[0x48, 0x89, 0xb4, 0x24],
                GPR::RDX => &[0x48, 0x89, 0x94, 0x24],
                GPR::RCX => &[0x48, 0x89, 0x8c, 0x24],
                GPR::R8 => &[0x4c, 0x89, 0x84, 0x24],
                GPR::R9 => &[0x4c, 0x89, 0x8c, 0x24],
                _ => return None,
            },
            X64Register::XMM(xmm) => match xmm {
                XMM::XMM0 => &[0x66, 0x0f, 0xd6, 0x84, 0x24],
                XMM::XMM1 => &[0x66, 0x0f, 0xd6, 0x8c, 0x24],
                XMM::XMM2 => &[0x66, 0x0f, 0xd6, 0x94, 0x24],
                XMM::XMM3 => &[0x66, 0x0f, 0xd6, 0x9c, 0x24],
                XMM::XMM4 => &[0x66, 0x0f, 0xd6, 0xa4, 0x24],
                XMM::XMM5 => &[0x66, 0x0f, 0xd6, 0xac, 0x24],
                XMM::XMM6 => &[0x66, 0x0f, 0xd6, 0xb4, 0x24],
                XMM::XMM7 => &[0x66, 0x0f, 0xd6, 0xbc, 0x24],
                _ => return None,
            },
        })
    }
}

/// An allocator that allocates registers for function arguments according to the System V ABI.
#[derive(Default)]
pub(crate) struct ArgumentRegisterAllocator {
    n_gprs: usize,
    n_xmms: usize,
}

impl ArgumentRegisterAllocator {
    /// Allocates a register for argument type `ty`. Returns `None` if no register is available for this type.
    pub(crate) fn next(
        &mut self,
        ty: Type,
        calling_convention: CallingConvention,
    ) -> Option<X64Register> {
        match calling_convention {
            CallingConvention::WindowsFastcall => {
                static GPR_SEQ: &'static [GPR] = &[GPR::RCX, GPR::RDX, GPR::R8, GPR::R9];
                static XMM_SEQ: &'static [XMM] = &[XMM::XMM0, XMM::XMM1, XMM::XMM2, XMM::XMM3];
                let idx = self.n_gprs + self.n_xmms;
                match ty {
                    Type::I32 | Type::I64 => {
                        if idx < 4 {
                            let gpr = GPR_SEQ[idx];
                            self.n_gprs += 1;
                            Some(X64Register::GPR(gpr))
                        } else {
                            None
                        }
                    }
                    Type::F32 | Type::F64 => {
                        if idx < 4 {
                            let xmm = XMM_SEQ[idx];
                            self.n_xmms += 1;
                            Some(X64Register::XMM(xmm))
                        } else {
                            None
                        }
                    }
                    _ => todo!(
                        "ArgumentRegisterAllocator::next: Unsupported type: {:?}",
                        ty
                    ),
                }
            }
            _ => {
                static GPR_SEQ: &'static [GPR] =
                    &[GPR::RDI, GPR::RSI, GPR::RDX, GPR::RCX, GPR::R8, GPR::R9];
                static XMM_SEQ: &'static [XMM] = &[
                    XMM::XMM0,
                    XMM::XMM1,
                    XMM::XMM2,
                    XMM::XMM3,
                    XMM::XMM4,
                    XMM::XMM5,
                    XMM::XMM6,
                    XMM::XMM7,
                ];
                match ty {
                    Type::I32 | Type::I64 => {
                        if self.n_gprs < GPR_SEQ.len() {
                            let gpr = GPR_SEQ[self.n_gprs];
                            self.n_gprs += 1;
                            Some(X64Register::GPR(gpr))
                        } else {
                            None
                        }
                    }
                    Type::F32 | Type::F64 => {
                        if self.n_xmms < XMM_SEQ.len() {
                            let xmm = XMM_SEQ[self.n_xmms];
                            self.n_xmms += 1;
                            Some(X64Register::XMM(xmm))
                        } else {
                            None
                        }
                    }
                    _ => todo!(
                        "ArgumentRegisterAllocator::next: Unsupported type: {:?}",
                        ty
                    ),
                }
            }
        }
    }
}

'''
'''--- lib/compiler/Cargo.toml ---
[package]
name = "wasmer-compiler-near"
version = "2.4.1"
description = "Base compiler abstraction for Wasmer WebAssembly runtime"
categories = ["wasm", "no-std"]
keywords = ["wasm", "webassembly", "compiler"]
authors = ["Wasmer Engineering Team <engineering@wasmer.io>"]
repository = "https://github.com/wasmerio/wasmer"
license = "MIT OR Apache-2.0 WITH LLVM-exception"
readme = "README.md"
edition = "2018"

[lib]
name = "wasmer_compiler"

[dependencies]
wasmer-vm = { path = "../vm", package = "wasmer-vm-near", version = "=2.4.1" }
wasmer-types = { path = "../types", package = "wasmer-types-near", version = "=2.4.1", default-features = false }
wasmparser = { version = "0.78", optional = true, default-features = false }
target-lexicon = { version = "0.12.2", default-features = false }
enumset = "1.0"
hashbrown = { version = "0.11", optional = true }
thiserror = "1.0"
smallvec = "1.6"
rkyv = { version = "0.7.20" }

[features]
default = ["std" ]
# This feature is for compiler implementors, it enables using `Compiler` and
# `CompilerConfig`, as well as the included wasmparser.
# Disable this feature if you just want a headless engine.
translator = ["wasmparser"]
std = ["wasmer-types/std"]
core = ["hashbrown", "wasmer-types/core"]

[badges]
maintenance = { status = "experimental" }

'''
'''--- lib/compiler/README.md ---
# `wasmer-compiler` [![Build Status](https://github.com/wasmerio/wasmer/workflows/build/badge.svg?style=flat-square)](https://github.com/wasmerio/wasmer/actions?query=workflow%3Abuild) [![Join Wasmer Slack](https://img.shields.io/static/v1?label=Slack&message=join%20chat&color=brighgreen&style=flat-square)](https://slack.wasmer.io) [![MIT License](https://img.shields.io/github/license/wasmerio/wasmer.svg?style=flat-square)](https://github.com/wasmerio/wasmer/blob/master/LICENSE)

This crate is the base for Compiler implementations.

It performs the translation from a Wasm module into a basic
`ModuleInfo`, but leaves the Wasm function bytecode translation to the
compiler implementor.

Here are some of the Compilers provided by Wasmer:

* [Singlepass](https://github.com/wasmerio/wasmer/tree/master/lib/compiler-singlepass),
* [Cranelift](https://github.com/wasmerio/wasmer/tree/master/lib/compiler-cranelift),
* [LLVM](https://github.com/wasmerio/wasmer/tree/master/lib/compiler-llvm).

## How to create a compiler

To create a compiler, one needs to implement two traits:

1. `CompilerConfig`, that configures and creates a new compiler,
2. `Compiler`, the compiler itself that will compile a module.

```rust
/// The compiler configuration options.
pub trait CompilerConfig {
    /// Gets the custom compiler config
    fn compiler(&self) -> Box<dyn Compiler>;
}

/// An implementation of a compiler from parsed WebAssembly module to compiled native code.
pub trait Compiler {
    /// Compiles a parsed module.
    ///
    /// It returns the [`Compilation`] or a [`CompileError`].
    fn compile_module<'data, 'module>(
        &self,
        target: &Target,
        compile_info: &'module CompileModuleInfo,
        module_translation: &ModuleTranslationState,
        // The list of function bodies
        function_body_inputs: PrimaryMap<LocalFunctionIndex, FunctionBodyData<'data>>,
    ) -> Result<Compilation, CompileError>;
}
```

## Acknowledgments

This project borrowed some of the code strucutre from the
[`cranelift-wasm`] crate, however it's been adapted to not depend on
any specific IR and be abstract of any compiler.

Please check [Wasmer `ATTRIBUTIONS`] to further see licenses and other
attributions of the project.

[`cranelift-wasm`]: https://crates.io/crates/cranelift-wasm
[Wasmer `ATTRIBUTIONS`]: https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md

'''
'''--- lib/compiler/src/address_map.rs ---
//! Data structures to provide transformation of the source
// addresses of a WebAssembly module into the native code.

use crate::lib::std::vec::Vec;
use crate::sourceloc::SourceLoc;

/// Single source location to generated address mapping.
#[derive(rkyv::Serialize, rkyv::Deserialize, rkyv::Archive, Debug, Clone, PartialEq, Eq)]
pub struct InstructionAddressMap {
    /// Original source location.
    pub srcloc: SourceLoc,

    /// Generated instructions offset.
    pub code_offset: usize,

    /// Generated instructions length.
    pub code_len: usize,
}

/// Function and its instructions addresses mappings.
#[derive(
    rkyv::Serialize, rkyv::Deserialize, rkyv::Archive, Debug, Clone, PartialEq, Eq, Default,
)]
pub struct FunctionAddressMap {
    /// Instructions maps.
    /// The array is sorted by the InstructionAddressMap::code_offset field.
    pub instructions: Vec<InstructionAddressMap>,

    /// Function start source location (normally declaration).
    pub start_srcloc: SourceLoc,

    /// Function end source location.
    pub end_srcloc: SourceLoc,

    /// Generated function body offset if applicable, otherwise 0.
    pub body_offset: usize,

    /// Generated function body length.
    pub body_len: usize,
}

'''
'''--- lib/compiler/src/compiler.rs ---
//! This module mainly outputs the `Compiler` trait that custom
//! compilers will need to implement.

use crate::error::CompileError;
use crate::function::Compilation;
use crate::lib::std::boxed::Box;
use crate::module::CompileModuleInfo;
use crate::target::Target;
use crate::FunctionBodyData;
use crate::ModuleTranslationState;
use crate::SectionIndex;
use wasmer_types::entity::PrimaryMap;
use wasmer_types::{Features, FunctionIndex, LocalFunctionIndex, SignatureIndex};
use wasmparser::{Validator, WasmFeatures};

/// The compiler configuration options.
pub trait CompilerConfig {
    /// Enable Position Independent Code (PIC).
    ///
    /// This is required for shared object generation (Native Engine),
    /// but will make the JIT Engine to fail, since PIC is not yet
    /// supported in the JIT linking phase.
    fn enable_pic(&mut self) {
        // By default we do nothing, each backend will need to customize this
        // in case they do something special for emitting PIC code.
    }

    /// Enable compiler IR verification.
    ///
    /// For compilers capable of doing so, this enables internal consistency
    /// checking.
    fn enable_verifier(&mut self) {
        // By default we do nothing, each backend will need to customize this
        // in case they create an IR that they can verify.
    }

    /// Enable NaN canonicalization.
    ///
    /// NaN canonicalization is useful when trying to run WebAssembly
    /// deterministically across different architectures.
    #[deprecated(note = "Please use the canonicalize_nans instead")]
    fn enable_nan_canonicalization(&mut self) {
        // By default we do nothing, each backend will need to customize this
        // in case they create an IR that they can verify.
    }

    /// Enable NaN canonicalization.
    ///
    /// NaN canonicalization is useful when trying to run WebAssembly
    /// deterministically across different architectures.
    fn canonicalize_nans(&mut self, _enable: bool) {
        // By default we do nothing, each backend will need to customize this
        // in case they create an IR that they can verify.
    }

    /// Gets the custom compiler config
    fn compiler(self: Box<Self>) -> Box<dyn Compiler>;

    /// Gets the default features for this compiler in the given target
    fn default_features_for_target(&self, _target: &Target) -> Features {
        Features::default()
    }
}

impl<T> From<T> for Box<dyn CompilerConfig + 'static>
where
    T: CompilerConfig + 'static,
{
    fn from(other: T) -> Self {
        Box::new(other)
    }
}

/// An implementation of a Compiler from parsed WebAssembly module to Compiled native code.
pub trait Compiler: Send {
    /// Validates a module.
    ///
    /// It returns the a succesful Result in case is valid, `CompileError` in case is not.
    fn validate_module<'data>(
        &self,
        features: &Features,
        data: &'data [u8],
    ) -> Result<(), CompileError> {
        let mut validator = Validator::new();
        let wasm_features = WasmFeatures {
            bulk_memory: features.bulk_memory,
            threads: features.threads,
            reference_types: features.reference_types,
            multi_value: features.multi_value,
            simd: features.simd,
            tail_call: features.tail_call,
            module_linking: features.module_linking,
            multi_memory: features.multi_memory,
            memory64: features.memory64,
            exceptions: features.exceptions,
            deterministic_only: false,
        };
        validator.wasm_features(wasm_features);
        validator
            .validate_all(data)
            .map_err(|e| CompileError::Validate(format!("{}", e)))?;
        Ok(())
    }

    /// Compiles a parsed module.
    ///
    /// It returns the [`Compilation`] or a [`CompileError`].
    fn compile_module<'data, 'module>(
        &self,
        target: &Target,
        module: &'module CompileModuleInfo,
        module_translation: &ModuleTranslationState,
        // The list of function bodies
        function_body_inputs: PrimaryMap<LocalFunctionIndex, FunctionBodyData<'data>>,
    ) -> Result<Compilation, CompileError>;

    /// Compiles a module into a native object file.
    ///
    /// It returns the bytes as a `&[u8]` or a [`CompileError`].
    fn experimental_native_compile_module<'data, 'module>(
        &self,
        _target: &Target,
        _module: &'module CompileModuleInfo,
        _module_translation: &ModuleTranslationState,
        // The list of function bodies
        _function_body_inputs: &PrimaryMap<LocalFunctionIndex, FunctionBodyData<'data>>,
        _symbol_registry: &dyn SymbolRegistry,
        // The metadata to inject into the wasmer_metadata section of the object file.
        _wasmer_metadata: &[u8],
    ) -> Option<Result<Vec<u8>, CompileError>> {
        None
    }
}

/// The kinds of wasmer_types objects that might be found in a native object file.
#[derive(Clone, Debug, PartialEq, Eq)]
pub enum Symbol {
    /// A function defined in the wasm.
    LocalFunction(LocalFunctionIndex),

    /// A wasm section.
    Section(SectionIndex),

    /// The function call trampoline for a given signature.
    FunctionCallTrampoline(SignatureIndex),

    /// The dynamic function trampoline for a given function.
    DynamicFunctionTrampoline(FunctionIndex),
}

/// This trait facilitates symbol name lookups in a native object file.
pub trait SymbolRegistry: Send + Sync {
    /// Given a `Symbol` it returns the name for that symbol in the object file
    fn symbol_to_name(&self, symbol: Symbol) -> String;

    /// Given a name it returns the `Symbol` for that name in the object file
    ///
    /// This function is the inverse of [`SymbolRegistry::symbol_to_name`]
    fn name_to_symbol(&self, name: &str) -> Option<Symbol>;
}

'''
'''--- lib/compiler/src/error.rs ---
use crate::lib::std::string::String;
#[cfg(feature = "std")]
use thiserror::Error;

// Compilation Errors
//
// If `std` feature is enable, we can't use `thiserror` until
// https://github.com/dtolnay/thiserror/pull/64 is merged.

/// The WebAssembly.CompileError object indicates an error during
/// WebAssembly decoding or validation.
///
/// This is based on the [Wasm Compile Error][compile-error] API.
///
/// [compiler-error]: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/WebAssembly/CompileError
#[derive(Debug)]
#[cfg_attr(feature = "std", derive(Error))]
pub enum CompileError {
    /// A Wasm translation error occured.
    #[cfg_attr(feature = "std", error("WebAssembly translation error: {0}"))]
    Wasm(WasmError),

    /// A compilation error occured.
    #[cfg_attr(feature = "std", error("Compilation error: {0}"))]
    Codegen(String),

    /// The module did not pass validation.
    #[cfg_attr(feature = "std", error("Validation error: {0}"))]
    Validate(String),

    /// The compiler doesn't support a Wasm feature
    #[cfg_attr(feature = "std", error("Feature {0} is not yet supported"))]
    UnsupportedFeature(String),

    /// The compiler cannot compile for the given target.
    /// This can refer to the OS, the chipset or any other aspect of the target system.
    #[cfg_attr(feature = "std", error("The target {0} is not yet supported (see https://docs.wasmer.io/ecosystem/wasmer/wasmer-features)"))]
    UnsupportedTarget(String),

    /// Insufficient resources available for execution.
    #[cfg_attr(feature = "std", error("Insufficient resources: {0}"))]
    Resource(String),

    /// Cannot downcast the engine to a specific type.
    #[cfg_attr(
        feature = "std",
        error("cannot downcast the engine to a specific type")
    )]
    EngineDowncast,
}

impl From<WasmError> for CompileError {
    fn from(original: WasmError) -> Self {
        Self::Wasm(original)
    }
}

/// A error in the middleware.
#[derive(Debug)]
#[cfg_attr(feature = "std", derive(Error))]
#[cfg_attr(feature = "std", error("Error in middleware {name}: {message}"))]
pub struct MiddlewareError {
    /// The name of the middleware where the error was created
    pub name: String,
    /// The error message
    pub message: String,
}

impl MiddlewareError {
    /// Create a new `MiddlewareError`
    pub fn new<A: Into<String>, B: Into<String>>(name: A, message: B) -> Self {
        Self {
            name: name.into(),
            message: message.into(),
        }
    }
}

/// A WebAssembly translation error.
///
/// When a WebAssembly function can't be translated, one of these error codes will be returned
/// to describe the failure.
#[derive(Debug)]
#[cfg_attr(feature = "std", derive(Error))]
pub enum WasmError {
    /// The input WebAssembly code is invalid.
    ///
    /// This error code is used by a WebAssembly translator when it encounters invalid WebAssembly
    /// code. This should never happen for validated WebAssembly code.
    #[cfg_attr(
        feature = "std",
        error("Invalid input WebAssembly code at offset {offset}: {message}")
    )]
    InvalidWebAssembly {
        /// A string describing the validation error.
        message: String,
        /// The bytecode offset where the error occurred.
        offset: usize,
    },

    /// A feature used by the WebAssembly code is not supported by the embedding environment.
    ///
    /// Embedding environments may have their own limitations and feature restrictions.
    #[cfg_attr(feature = "std", error("Unsupported feature: {0}"))]
    Unsupported(String),

    /// An implementation limit was exceeded.
    #[cfg_attr(feature = "std", error("Implementation limit exceeded"))]
    ImplLimitExceeded,

    /// An error from the middleware error.
    #[cfg_attr(feature = "std", error("{0}"))]
    Middleware(MiddlewareError),

    /// A generic error.
    #[cfg_attr(feature = "std", error("{0}"))]
    Generic(String),
}

impl From<MiddlewareError> for WasmError {
    fn from(original: MiddlewareError) -> Self {
        Self::Middleware(original)
    }
}

/// The error that can happen while parsing a `str`
/// to retrieve a [`CpuFeature`](crate::target::CpuFeature).
#[derive(Debug)]
#[cfg_attr(feature = "std", derive(Error))]
pub enum ParseCpuFeatureError {
    /// The provided string feature doesn't exist
    #[cfg_attr(feature = "std", error("CpuFeature {0} not recognized"))]
    Missing(String),
}

/// A convenient alias for a `Result` that uses `WasmError` as the error type.
pub type WasmResult<T> = Result<T, WasmError>;

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn middleware_error_can_be_created() {
        let msg = String::from("Something went wrong");
        let error = MiddlewareError::new("manipulator3000", msg);
        assert_eq!(error.name, "manipulator3000");
        assert_eq!(error.message, "Something went wrong");
    }

    #[test]
    fn middleware_error_be_converted_to_wasm_error() {
        let error = WasmError::from(MiddlewareError::new("manipulator3000", "foo"));
        match error {
            WasmError::Middleware(MiddlewareError { name, message }) => {
                assert_eq!(name, "manipulator3000");
                assert_eq!(message, "foo");
            }
            err => panic!("Unexpected error: {:?}", err),
        }
    }
}

'''
'''--- lib/compiler/src/function.rs ---
// This file contains code from external sources.
// Attributions: https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md

//! A `Compilation` contains the compiled function bodies for a WebAssembly
//! module (`CompiledFunction`).

use crate::lib::std::vec::Vec;
use crate::section::{CustomSection, SectionIndex};
use crate::trap::TrapInformation;
use crate::{
    CompiledFunctionUnwindInfo, CompiledFunctionUnwindInfoRef, FunctionAddressMap,
    JumpTableOffsets, Relocation,
};
use wasmer_types::entity::PrimaryMap;
use wasmer_types::{FunctionIndex, LocalFunctionIndex, SignatureIndex};

/// The frame info for a Compiled function.
///
/// This structure is only used for reconstructing
/// the frame information after a `Trap`.
#[derive(
    rkyv::Serialize, rkyv::Deserialize, rkyv::Archive, Debug, Clone, PartialEq, Eq, Default,
)]
pub struct CompiledFunctionFrameInfo {
    /// The traps (in the function body).
    ///
    /// Code offsets of the traps MUST be in ascending order.
    pub traps: Vec<TrapInformation>,

    /// The address map.
    pub address_map: FunctionAddressMap,
}

/// The function body.
#[derive(rkyv::Serialize, rkyv::Deserialize, rkyv::Archive, Debug, Clone, PartialEq, Eq)]
pub struct FunctionBody {
    /// The function body bytes.
    pub body: Vec<u8>,

    /// The function unwind info
    pub unwind_info: Option<CompiledFunctionUnwindInfo>,
}

/// See [`FunctionBody`].
#[derive(Clone, Copy)]
pub struct FunctionBodyRef<'a> {
    /// Function body bytes.
    pub body: &'a [u8],
    /// The function unwind info.
    pub unwind_info: Option<CompiledFunctionUnwindInfoRef<'a>>,
}

impl<'a> From<&'a FunctionBody> for FunctionBodyRef<'a> {
    fn from(body: &'a FunctionBody) -> Self {
        FunctionBodyRef {
            body: &*body.body,
            unwind_info: body.unwind_info.as_ref().map(Into::into),
        }
    }
}

impl<'a> From<&'a ArchivedFunctionBody> for FunctionBodyRef<'a> {
    fn from(body: &'a ArchivedFunctionBody) -> Self {
        FunctionBodyRef {
            body: &*body.body,
            unwind_info: body.unwind_info.as_ref().map(Into::into),
        }
    }
}

/// The result of compiling a WebAssembly function.
///
/// This structure only have the compiled information data
/// (function bytecode body, relocations, traps, jump tables
/// and unwind information).
#[derive(rkyv::Serialize, rkyv::Deserialize, rkyv::Archive, Debug, Clone, PartialEq, Eq)]
pub struct CompiledFunction {
    /// The function body.
    pub body: FunctionBody,

    /// The relocations (in the body)
    pub relocations: Vec<Relocation>,

    /// The jump tables offsets (in the body).
    pub jt_offsets: JumpTableOffsets,

    /// The frame information.
    pub frame_info: CompiledFunctionFrameInfo,
}

/// The compiled functions map (index in the Wasm -> function)
pub type Functions = PrimaryMap<LocalFunctionIndex, CompiledFunction>;

/// The custom sections for a Compilation.
pub type CustomSections = PrimaryMap<SectionIndex, CustomSection>;

/// The DWARF information for this Compilation.
///
/// It is used for retrieving the unwind information once an exception
/// happens.
/// In the future this structure may also hold other information useful
/// for debugging.
#[derive(rkyv::Serialize, rkyv::Deserialize, rkyv::Archive, Debug, PartialEq, Eq, Clone)]
pub struct Dwarf {
    /// The section index in the [`Compilation`] that corresponds to the exception frames.
    /// [Learn
    /// more](https://refspecs.linuxfoundation.org/LSB_3.0.0/LSB-PDA/LSB-PDA/ehframechpt.html).
    pub eh_frame: SectionIndex,
}

impl Dwarf {
    /// Creates a `Dwarf` struct with the corresponding indices for its sections
    pub fn new(eh_frame: SectionIndex) -> Self {
        Self { eh_frame }
    }
}

/// Trampolines section used by ARM short jump (26bits)
#[derive(rkyv::Serialize, rkyv::Deserialize, rkyv::Archive, Debug, PartialEq, Eq, Clone)]
pub struct TrampolinesSection {
    /// SectionIndex for the actual Trampolines code
    pub section_index: SectionIndex,
    /// Number of jump slots in the section
    pub slots: usize,
    /// Slot size
    pub size: usize,
}

impl TrampolinesSection {
    /// Creates a `Trampolines` struct with the indice for its section, and number of slots and size of slot
    pub fn new(section_index: SectionIndex, slots: usize, size: usize) -> Self {
        Self {
            section_index,
            slots,
            size,
        }
    }
}

/// The result of compiling a WebAssembly module's functions.
#[derive(Debug, PartialEq, Eq)]
pub struct Compilation {
    /// Compiled code for the function bodies.
    functions: Functions,

    /// Custom sections for the module.
    /// It will hold the data, for example, for constants used in a
    /// function, global variables, rodata_64, hot/cold function partitioning, ...
    custom_sections: CustomSections,

    /// Trampolines to call a function defined locally in the wasm via a
    /// provided `Vec` of values.
    ///
    /// This allows us to call easily Wasm functions, such as:
    ///
    /// ```ignore
    /// let func = instance.exports.get_function("my_func");
    /// func.call(&[Value::I32(1)]);
    /// ```
    function_call_trampolines: PrimaryMap<SignatureIndex, FunctionBody>,

    /// Trampolines to call a dynamic function defined in
    /// a host, from a Wasm module.
    ///
    /// This allows us to create dynamic Wasm functions, such as:
    ///
    /// ```ignore
    /// fn my_func(values: &[Val]) -> Result<Vec<Val>, RuntimeError> {
    ///     // do something
    /// }
    ///
    /// let my_func_type = FunctionType::new(vec![Type::I32], vec![Type::I32]);
    /// let imports = imports!{
    ///     "namespace" => {
    ///         "my_func" => Function::new(&store, my_func_type, my_func),
    ///     }
    /// }
    /// ```
    ///
    /// Note: Dynamic function trampolines are only compiled for imported function types.
    dynamic_function_trampolines: PrimaryMap<FunctionIndex, FunctionBody>,

    /// Section ids corresponding to the Dwarf debug info
    debug: Option<Dwarf>,

    /// Trampolines for the arch that needs it
    trampolines: Option<TrampolinesSection>,
}

impl Compilation {
    /// Creates a compilation artifact from a contiguous function buffer and a set of ranges
    pub fn new(
        functions: Functions,
        custom_sections: CustomSections,
        function_call_trampolines: PrimaryMap<SignatureIndex, FunctionBody>,
        dynamic_function_trampolines: PrimaryMap<FunctionIndex, FunctionBody>,
        debug: Option<Dwarf>,
        trampolines: Option<TrampolinesSection>,
    ) -> Self {
        Self {
            functions,
            custom_sections,
            function_call_trampolines,
            dynamic_function_trampolines,
            debug,
            trampolines,
        }
    }

    /// Gets the bytes of a single function
    pub fn get(&self, func: LocalFunctionIndex) -> &CompiledFunction {
        &self.functions[func]
    }

    /// Gets the number of functions defined.
    pub fn len(&self) -> usize {
        self.functions.len()
    }

    /// Returns whether there are no functions defined.
    pub fn is_empty(&self) -> bool {
        self.functions.is_empty()
    }

    /// Gets functions relocations.
    pub fn get_relocations(&self) -> PrimaryMap<LocalFunctionIndex, Vec<Relocation>> {
        self.functions
            .iter()
            .map(|(_, func)| func.relocations.clone())
            .collect::<PrimaryMap<LocalFunctionIndex, _>>()
    }

    /// Gets functions bodies.
    pub fn get_function_bodies(&self) -> PrimaryMap<LocalFunctionIndex, FunctionBody> {
        self.functions
            .iter()
            .map(|(_, func)| func.body.clone())
            .collect::<PrimaryMap<LocalFunctionIndex, _>>()
    }

    /// Gets functions jump table offsets.
    pub fn get_jt_offsets(&self) -> PrimaryMap<LocalFunctionIndex, JumpTableOffsets> {
        self.functions
            .iter()
            .map(|(_, func)| func.jt_offsets.clone())
            .collect::<PrimaryMap<LocalFunctionIndex, _>>()
    }

    /// Gets functions frame info.
    pub fn get_frame_info(&self) -> PrimaryMap<LocalFunctionIndex, CompiledFunctionFrameInfo> {
        self.functions
            .iter()
            .map(|(_, func)| func.frame_info.clone())
            .collect::<PrimaryMap<LocalFunctionIndex, _>>()
    }

    /// Gets function call trampolines.
    pub fn get_function_call_trampolines(&self) -> PrimaryMap<SignatureIndex, FunctionBody> {
        self.function_call_trampolines.clone()
    }

    /// Gets function call trampolines.
    pub fn get_dynamic_function_trampolines(&self) -> PrimaryMap<FunctionIndex, FunctionBody> {
        self.dynamic_function_trampolines.clone()
    }

    /// Gets custom section data.
    pub fn get_custom_sections(&self) -> PrimaryMap<SectionIndex, CustomSection> {
        self.custom_sections.clone()
    }

    /// Gets relocations that apply to custom sections.
    pub fn get_custom_section_relocations(&self) -> PrimaryMap<SectionIndex, Vec<Relocation>> {
        self.custom_sections
            .iter()
            .map(|(_, section)| section.relocations.clone())
            .collect::<PrimaryMap<SectionIndex, _>>()
    }

    /// Returns the Dwarf info.
    pub fn get_debug(&self) -> Option<Dwarf> {
        self.debug.clone()
    }

    /// Returns the Trampolines info.
    pub fn get_trampolines(&self) -> Option<TrampolinesSection> {
        self.trampolines.clone()
    }
}

impl<'a> IntoIterator for &'a Compilation {
    type IntoIter = Iter<'a>;
    type Item = <Self::IntoIter as Iterator>::Item;

    fn into_iter(self) -> Self::IntoIter {
        Iter {
            iterator: self.functions.iter(),
        }
    }
}

pub struct Iter<'a> {
    iterator: <&'a Functions as IntoIterator>::IntoIter,
}

impl<'a> Iterator for Iter<'a> {
    type Item = &'a CompiledFunction;

    fn next(&mut self) -> Option<Self::Item> {
        self.iterator.next().map(|(_, b)| b)
    }
}

'''
'''--- lib/compiler/src/jump_table.rs ---
//! A jump table is a method of transferring program control (branching)
//! to another part of a program (or a different program that may have
//! been dynamically loaded) using a table of branch or jump instructions.
//!
//! [Learn more](https://en.wikipedia.org/wiki/Branch_table).

use super::CodeOffset;
use wasmer_types::entity::{entity_impl, SecondaryMap};

/// An opaque reference to a [jump table](https://en.wikipedia.org/wiki/Branch_table).
///
/// `JumpTable`s are used for indirect branching and are specialized for dense,
/// 0-based jump offsets.
#[derive(rkyv::Serialize, rkyv::Deserialize, rkyv::Archive)]
#[archive_attr(derive(Copy, Clone, PartialEq, Eq, Hash, PartialOrd, Ord))]
#[derive(Copy, Clone, PartialEq, Eq, Hash, PartialOrd, Ord)]
pub struct JumpTable(u32);

entity_impl!(JumpTable, "jt");
entity_impl!(ArchivedJumpTable, "jt");

impl JumpTable {
    /// Create a new jump table reference from its number.
    ///
    /// This method is for use by the parser.
    pub fn with_number(n: u32) -> Option<Self> {
        if n < u32::max_value() {
            Some(Self(n))
        } else {
            None
        }
    }
}

/// Code offsets for Jump Tables.
pub type JumpTableOffsets = SecondaryMap<JumpTable, CodeOffset>;

'''
'''--- lib/compiler/src/lib.rs ---
//! The `wasmer-compiler` crate provides the necessary abstractions
//! to create a compiler.
//!
//! It provides an universal way of parsing a module via `wasmparser`,
//! while giving the responsibility of compiling specific function
//! WebAssembly bodies to the `Compiler` implementation.

#![deny(missing_docs, trivial_numeric_casts, unused_extern_crates)]
#![warn(unused_import_braces)]
#![cfg_attr(feature = "std", deny(unstable_features))]
#![cfg_attr(not(feature = "std"), no_std)]
#![cfg_attr(feature = "cargo-clippy", allow(clippy::new_without_default))]
#![cfg_attr(
    feature = "cargo-clippy",
    warn(
        clippy::float_arithmetic,
        clippy::mut_mut,
        clippy::nonminimal_bool,
        clippy::option_map_unwrap_or,
        clippy::option_map_unwrap_or_else,
        clippy::print_stdout,
        clippy::unicode_not_nfc,
        clippy::use_self
    )
)]

#[cfg(all(feature = "std", feature = "core"))]
compile_error!(
    "The `std` and `core` features are both enabled, which is an error. Please enable only once."
);

#[cfg(all(not(feature = "std"), not(feature = "core")))]
compile_error!("Both the `std` and `core` features are disabled. Please enable one of them.");

#[cfg(feature = "core")]
extern crate alloc;

mod lib {
    #[cfg(feature = "core")]
    pub mod std {
        pub use alloc::{borrow, boxed, str, string, sync, vec};
        pub use core::fmt;
        pub use hashbrown as collections;
    }

    #[cfg(feature = "std")]
    pub mod std {
        pub use std::{borrow, boxed, collections, fmt, str, string, sync, vec};
    }
}

mod address_map;
#[cfg(feature = "translator")]
mod compiler;
mod error;
mod function;
mod jump_table;
mod module;
mod relocation;
mod target;
mod trap;
mod unwind;
#[cfg(feature = "translator")]
#[macro_use]
mod translator;
mod section;
mod sourceloc;

pub use crate::address_map::{FunctionAddressMap, InstructionAddressMap};
#[cfg(feature = "translator")]
pub use crate::compiler::{Compiler, CompilerConfig, Symbol, SymbolRegistry};
pub use crate::error::{
    CompileError, MiddlewareError, ParseCpuFeatureError, WasmError, WasmResult,
};
pub use crate::function::{
    Compilation, CompiledFunction, CompiledFunctionFrameInfo, CustomSections, Dwarf, FunctionBody,
    FunctionBodyRef, Functions, TrampolinesSection,
};
pub use crate::jump_table::{JumpTable, JumpTableOffsets};
pub use crate::module::CompileModuleInfo;
pub use crate::relocation::{Relocation, RelocationKind, RelocationTarget, Relocations};
pub use crate::section::{
    CustomSection, CustomSectionProtection, CustomSectionRef, SectionBody, SectionIndex,
};
pub use crate::sourceloc::SourceLoc;
pub use crate::target::{
    Architecture, BinaryFormat, CallingConvention, CpuFeature, Endianness, OperatingSystem,
    PointerWidth, Target, Triple,
};
#[cfg(feature = "translator")]
pub use crate::translator::{
    translate_module, wptype_to_type, FunctionBodyData, FunctionReader, ModuleEnvironment,
    ModuleTranslationState,
};
pub use crate::trap::TrapInformation;
pub use crate::unwind::{CompiledFunctionUnwindInfo, CompiledFunctionUnwindInfoRef};

pub use wasmer_types::Features;

#[cfg(feature = "translator")]
/// wasmparser is exported as a module to slim compiler dependencies
pub use wasmparser;

/// Offset in bytes from the beginning of the function.
pub type CodeOffset = u32;

/// Addend to add to the symbol value.
pub type Addend = i64;

/// Version number of this crate.
pub const VERSION: &str = env!("CARGO_PKG_VERSION");

'''
'''--- lib/compiler/src/module.rs ---
use crate::lib::std::sync::Arc;
use wasmer_types::entity::PrimaryMap;
use wasmer_types::{Features, MemoryIndex, ModuleInfo, TableIndex};
use wasmer_vm::{MemoryStyle, TableStyle};

/// The required info for compiling a module.
///
/// This differs from [`ModuleInfo`] because it have extra info only
/// possible after translation (such as the features used for compiling,
/// or the `MemoryStyle` and `TableStyle`).
#[derive(Debug, PartialEq, Eq, rkyv::Serialize, rkyv::Deserialize, rkyv::Archive)]
pub struct CompileModuleInfo {
    /// The features used for compiling the module
    pub features: Features,
    /// The module information
    pub module: Arc<ModuleInfo>,
    /// The memory styles used for compiling.
    ///
    /// The compiler will emit the most optimal code based
    /// on the memory style (static or dynamic) chosen.
    pub memory_styles: PrimaryMap<MemoryIndex, MemoryStyle>,
    /// The table plans used for compiling.
    pub table_styles: PrimaryMap<TableIndex, TableStyle>,
}

'''
'''--- lib/compiler/src/relocation.rs ---
//! Relocation is the process of assigning load addresses for position-dependent
//! code and data of a program and adjusting the code and data to reflect the
//! assigned addresses.
//!
//! [Learn more](https://en.wikipedia.org/wiki/Relocation_(computing)).
//!
//! Each time a `Compiler` compiles a WebAssembly function (into machine code),
//! it also attaches if there are any relocations that need to be patched into
//! the generated machine code, so a given frontend (JIT or native) can
//! do the corresponding work to run it.

use crate::lib::std::fmt;
use crate::lib::std::vec::Vec;
use crate::section::SectionIndex;
use crate::{Addend, CodeOffset, JumpTable};
use wasmer_types::entity::PrimaryMap;
use wasmer_types::LocalFunctionIndex;
use wasmer_vm::libcalls::LibCall;

/// Relocation kinds for every ISA.
#[derive(rkyv::Serialize, rkyv::Deserialize, rkyv::Archive, Copy, Clone, Debug, PartialEq, Eq)]
pub enum RelocationKind {
    /// absolute 4-byte
    Abs4,
    /// absolute 8-byte
    Abs8,
    /// x86 PC-relative 4-byte
    X86PCRel4,
    /// x86 PC-relative 8-byte
    X86PCRel8,
    /// x86 PC-relative 4-byte offset to trailing rodata
    X86PCRelRodata4,
    /// x86 call to PC-relative 4-byte
    X86CallPCRel4,
    /// x86 call to PLT-relative 4-byte
    X86CallPLTRel4,
    /// x86 GOT PC-relative 4-byte
    X86GOTPCRel4,
    /// Arm32 call target
    Arm32Call,
    /// Arm64 call target
    Arm64Call,
    /// Arm64 movk/z part 0
    Arm64Movw0,
    /// Arm64 movk/z part 1
    Arm64Movw1,
    /// Arm64 movk/z part 2
    Arm64Movw2,
    /// Arm64 movk/z part 3
    Arm64Movw3,
    // /// RISC-V call target
    // RiscvCall,
    /// Elf x86_64 32 bit signed PC relative offset to two GOT entries for GD symbol.
    ElfX86_64TlsGd,
    // /// Mach-O x86_64 32 bit signed PC relative offset to a `__thread_vars` entry.
    // MachOX86_64Tlv,
}

impl fmt::Display for RelocationKind {
    /// Display trait implementation drops the arch, since its used in contexts where the arch is
    /// already unambiguous, e.g. clif syntax with isa specified. In other contexts, use Debug.
    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
        match *self {
            Self::Abs4 => write!(f, "Abs4"),
            Self::Abs8 => write!(f, "Abs8"),
            Self::X86PCRel4 => write!(f, "PCRel4"),
            Self::X86PCRel8 => write!(f, "PCRel8"),
            Self::X86PCRelRodata4 => write!(f, "PCRelRodata4"),
            Self::X86CallPCRel4 => write!(f, "CallPCRel4"),
            Self::X86CallPLTRel4 => write!(f, "CallPLTRel4"),
            Self::X86GOTPCRel4 => write!(f, "GOTPCRel4"),
            Self::Arm32Call | Self::Arm64Call => write!(f, "Call"),
            Self::Arm64Movw0 => write!(f, "Arm64MovwG0"),
            Self::Arm64Movw1 => write!(f, "Arm64MovwG1"),
            Self::Arm64Movw2 => write!(f, "Arm64MovwG2"),
            Self::Arm64Movw3 => write!(f, "Arm64MovwG3"),
            Self::ElfX86_64TlsGd => write!(f, "ElfX86_64TlsGd"),
            // Self::MachOX86_64Tlv => write!(f, "MachOX86_64Tlv"),
        }
    }
}

/// A record of a relocation to perform.
#[derive(rkyv::Serialize, rkyv::Deserialize, rkyv::Archive, Debug, Clone, PartialEq, Eq)]
pub struct Relocation {
    /// The relocation kind.
    pub kind: RelocationKind,
    /// Relocation target.
    pub reloc_target: RelocationTarget,
    /// The offset where to apply the relocation.
    pub offset: CodeOffset,
    /// The addend to add to the relocation value.
    pub addend: Addend,
}

/// Destination function. Can be either user function or some special one, like `memory.grow`.
#[derive(rkyv::Serialize, rkyv::Deserialize, rkyv::Archive, Debug, Copy, Clone, PartialEq, Eq)]
pub enum RelocationTarget {
    /// A relocation to a function defined locally in the wasm (not an imported one).
    LocalFunc(LocalFunctionIndex),
    /// A compiler-generated libcall.
    LibCall(LibCall),
    /// Jump table index.
    JumpTable(LocalFunctionIndex, JumpTable),
    /// Custom sections generated by the compiler
    CustomSection(SectionIndex),
}

impl Relocation {
    /// Given a function start address, provide the relocation relative
    /// to that address.
    ///
    /// The function returns the relocation address and the delta.
    pub fn for_address(&self, start: usize, target_func_address: u64) -> (usize, u64) {
        match self.kind {
            RelocationKind::Abs8
            | RelocationKind::Arm64Movw0
            | RelocationKind::Arm64Movw1
            | RelocationKind::Arm64Movw2
            | RelocationKind::Arm64Movw3 => {
                let reloc_address = start + self.offset as usize;
                let reloc_addend = self.addend as isize;
                let reloc_abs = target_func_address
                    .checked_add(reloc_addend as u64)
                    .unwrap();
                (reloc_address, reloc_abs)
            }
            RelocationKind::X86PCRel4 => {
                let reloc_address = start + self.offset as usize;
                let reloc_addend = self.addend as isize;
                let reloc_delta_u32 = (target_func_address as u32)
                    .wrapping_sub(reloc_address as u32)
                    .checked_add(reloc_addend as u32)
                    .unwrap();
                (reloc_address, reloc_delta_u32 as u64)
            }
            RelocationKind::X86PCRel8 => {
                let reloc_address = start + self.offset as usize;
                let reloc_addend = self.addend as isize;
                let reloc_delta = target_func_address
                    .wrapping_sub(reloc_address as u64)
                    .checked_add(reloc_addend as u64)
                    .unwrap();
                (reloc_address, reloc_delta)
            }
            RelocationKind::X86CallPCRel4 | RelocationKind::X86CallPLTRel4 => {
                let reloc_address = start + self.offset as usize;
                let reloc_addend = self.addend as isize;
                let reloc_delta_u32 = (target_func_address as u32)
                    .wrapping_sub(reloc_address as u32)
                    .wrapping_add(reloc_addend as u32);
                (reloc_address, reloc_delta_u32 as u64)
            }
            RelocationKind::Arm64Call => {
                let reloc_address = start + self.offset as usize;
                let reloc_addend = self.addend as isize;
                let reloc_delta_u32 = target_func_address
                    .wrapping_sub(reloc_address as u64)
                    .wrapping_add(reloc_addend as u64);
                (reloc_address, reloc_delta_u32)
            }
            // RelocationKind::X86PCRelRodata4 => {
            //     (start, target_func_address)
            // }
            _ => panic!("Relocation kind unsupported"),
        }
    }
}

/// Relocations to apply to function bodies.
pub type Relocations = PrimaryMap<LocalFunctionIndex, Vec<Relocation>>;

'''
'''--- lib/compiler/src/section.rs ---
//! This module define the required structures to emit custom
//! Sections in a `Compilation`.
//!
//! The functions that access a custom [`CustomSection`] would need
//! to emit a custom relocation: `RelocationTarget::CustomSection`, so
//! it can be patched later by the engine (native or JIT).

use crate::lib::std::vec::Vec;
use crate::Relocation;
use wasmer_types::entity::entity_impl;

/// Index type of a Section defined inside a WebAssembly `Compilation`.
#[derive(rkyv::Serialize, rkyv::Deserialize, rkyv::Archive)]
#[archive_attr(derive(Copy, Clone, PartialEq, Eq, Hash, PartialOrd, Ord, Debug))]
#[derive(Copy, Clone, PartialEq, Eq, Hash, PartialOrd, Ord, Debug)]
pub struct SectionIndex(u32);

entity_impl!(SectionIndex);

entity_impl!(ArchivedSectionIndex);

/// Custom section Protection.
///
/// Determines how a custom section may be used.
#[derive(rkyv::Serialize, rkyv::Deserialize, rkyv::Archive, Debug, Copy, Clone, PartialEq, Eq)]
pub enum CustomSectionProtection {
    /// A custom section with read permission.
    Read,

    /// A custom section with read and execute permissions.
    ReadExecute,
}

/// A Section for a `Compilation`.
///
/// This is used so compilers can store arbitrary information
/// in the emitted module.
#[derive(rkyv::Serialize, rkyv::Deserialize, rkyv::Archive, Debug, Clone, PartialEq, Eq)]
pub struct CustomSection {
    /// Memory protection that applies to this section.
    pub protection: CustomSectionProtection,

    /// The bytes corresponding to this section.
    ///
    /// > Note: These bytes have to be at-least 8-byte aligned
    /// > (the start of the memory pointer).
    /// > We might need to create another field for alignment in case it's
    /// > needed in the future.
    pub bytes: SectionBody,

    /// Relocations that apply to this custom section.
    pub relocations: Vec<Relocation>,
}

/// See [`CustomSection`].
///
/// Note that this does not reference the relocation data.
#[derive(Clone, Copy)]
pub struct CustomSectionRef<'a> {
    /// See [`CustomSection::protection`].
    pub protection: CustomSectionProtection,

    /// See [`CustomSection::bytes`].
    pub bytes: &'a [u8],
}

impl<'a> From<&'a CustomSection> for CustomSectionRef<'a> {
    fn from(section: &'a CustomSection) -> Self {
        CustomSectionRef {
            protection: section.protection.clone(),
            bytes: section.bytes.as_slice(),
        }
    }
}

impl<'a> From<&'a ArchivedCustomSection> for CustomSectionRef<'a> {
    fn from(section: &'a ArchivedCustomSection) -> Self {
        CustomSectionRef {
            protection: Result::<_, std::convert::Infallible>::unwrap(
                rkyv::Deserialize::deserialize(&section.protection, &mut rkyv::Infallible),
            ),
            bytes: &section.bytes.0[..],
        }
    }
}

/// The bytes in the section.
#[derive(
    rkyv::Serialize, rkyv::Deserialize, rkyv::Archive, Debug, Clone, PartialEq, Eq, Default,
)]
pub struct SectionBody(Vec<u8>);

impl SectionBody {
    /// Create a new section body with the given contents.
    pub fn new_with_vec(contents: Vec<u8>) -> Self {
        Self(contents)
    }

    /// Returns a raw pointer to the section's buffer.
    pub fn as_ptr(&self) -> *const u8 {
        self.0.as_ptr()
    }

    /// Returns the length of this section in bytes.
    pub fn len(&self) -> usize {
        self.0.len()
    }

    /// Dereferences into the section's buffer.
    pub fn as_slice(&self) -> &[u8] {
        self.0.as_slice()
    }

    /// Returns whether or not the section body is empty.
    pub fn is_empty(&self) -> bool {
        self.0.is_empty()
    }
}

'''
'''--- lib/compiler/src/sourceloc.rs ---
// This file contains code from external sources.
// Attributions: https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md

//! Source locations.
//!
//! A [`SourceLoc`] determines the position of a certain instruction
//! relative to the WebAssembly module. This is used mainly for debugging
//! and tracing errors.

use crate::lib::std::fmt;

/// A source location.
///
/// The default source location uses the all-ones bit pattern `!0`. It is used for instructions
/// that can't be given a real source location.
#[derive(rkyv::Serialize, rkyv::Deserialize, rkyv::Archive, Debug, Clone, Copy, PartialEq, Eq)]
#[archive(as = "Self")]
#[repr(transparent)]
pub struct SourceLoc(u32);

impl SourceLoc {
    /// Create a new source location with the given bits.
    pub fn new(bits: u32) -> Self {
        Self(bits)
    }

    /// Is this the default source location?
    pub fn is_default(self) -> bool {
        self == Default::default()
    }

    /// Read the bits of this source location.
    pub fn bits(self) -> u32 {
        self.0
    }
}

impl Default for SourceLoc {
    fn default() -> Self {
        Self(!0)
    }
}

impl fmt::Display for SourceLoc {
    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
        if self.is_default() {
            write!(f, "0x-")
        } else {
            write!(f, "0x{:04x}", self.0)
        }
    }
}

#[cfg(test)]
mod tests {
    use super::SourceLoc;
    use crate::lib::std::string::ToString;

    #[test]
    fn display() {
        assert_eq!(SourceLoc::default().to_string(), "0x-");
        assert_eq!(SourceLoc::new(0).to_string(), "0x0000");
        assert_eq!(SourceLoc::new(16).to_string(), "0x0010");
        assert_eq!(SourceLoc::new(0xabcdef).to_string(), "0xabcdef");
    }
}

'''
'''--- lib/compiler/src/target.rs ---
//! Target configuration
use crate::error::ParseCpuFeatureError;
use crate::lib::std::str::FromStr;
use crate::lib::std::string::{String, ToString};
use enumset::{EnumSet, EnumSetType};
pub use target_lexicon::{
    Architecture, BinaryFormat, CallingConvention, Endianness, OperatingSystem, PointerWidth,
    Triple,
};

/// The nomenclature is inspired by the [`cpuid` crate].
/// The list of supported features was initially retrieved from
/// [`cranelift-native`].
///
/// The `CpuFeature` enum values are likely to grow closer to the
/// original `cpuid`. However, we prefer to start small and grow from there.
///
/// If you would like to use a flag that doesn't exist yet here, please
/// open a PR.
///
/// [`cpuid` crate]: https://docs.rs/cpuid/0.1.1/cpuid/enum.CpuFeature.html
/// [`cranelift-native`]: https://github.com/bytecodealliance/cranelift/blob/6988545fd20249b084c53f4761b8c861266f5d31/cranelift-native/src/lib.rs#L51-L92
#[allow(missing_docs, clippy::derive_hash_xor_eq)]
#[derive(EnumSetType, Debug, Hash)]
pub enum CpuFeature {
    // X86 features
    SSE2,
    SSE3,
    SSSE3,
    SSE41,
    SSE42,
    POPCNT,
    AVX,
    BMI1,
    BMI2,
    AVX2,
    AVX512DQ,
    AVX512VL,
    AVX512F,
    LZCNT,
    // ARM features
    // Risc-V features
}

impl CpuFeature {
    #[cfg(any(target_arch = "x86", target_arch = "x86_64"))]
    /// Retrieves the features for the current Host
    pub fn for_host() -> EnumSet<Self> {
        let mut features = EnumSet::new();

        if std::is_x86_feature_detected!("sse2") {
            features.insert(Self::SSE2);
        }
        if std::is_x86_feature_detected!("sse3") {
            features.insert(Self::SSE3);
        }
        if std::is_x86_feature_detected!("ssse3") {
            features.insert(Self::SSSE3);
        }
        if std::is_x86_feature_detected!("sse4.1") {
            features.insert(Self::SSE41);
        }
        if std::is_x86_feature_detected!("sse4.2") {
            features.insert(Self::SSE42);
        }
        if std::is_x86_feature_detected!("popcnt") {
            features.insert(Self::POPCNT);
        }
        if std::is_x86_feature_detected!("avx") {
            features.insert(Self::AVX);
        }
        if std::is_x86_feature_detected!("bmi1") {
            features.insert(Self::BMI1);
        }
        if std::is_x86_feature_detected!("bmi2") {
            features.insert(Self::BMI2);
        }
        if std::is_x86_feature_detected!("avx2") {
            features.insert(Self::AVX2);
        }
        if std::is_x86_feature_detected!("avx512dq") {
            features.insert(Self::AVX512DQ);
        }
        if std::is_x86_feature_detected!("avx512vl") {
            features.insert(Self::AVX512VL);
        }
        if std::is_x86_feature_detected!("avx512f") {
            features.insert(Self::AVX512F);
        }
        if std::is_x86_feature_detected!("lzcnt") {
            features.insert(Self::LZCNT);
        }
        features
    }
    #[cfg(not(any(target_arch = "x86", target_arch = "x86_64")))]
    /// Retrieves the features for the current Host
    pub fn for_host() -> EnumSet<Self> {
        // We default to an empty hash set
        EnumSet::new()
    }

    /// Retrieves an empty set of `CpuFeature`s.
    pub fn set() -> EnumSet<Self> {
        // We default to an empty hash set
        EnumSet::new()
    }
}

// This options should map exactly the GCC options indicated
// here by architectures:
//
// X86: https://gcc.gnu.org/onlinedocs/gcc/x86-Options.html
// ARM: https://gcc.gnu.org/onlinedocs/gcc/ARM-Options.html
// Aarch64: https://gcc.gnu.org/onlinedocs/gcc/AArch64-Options.html
impl FromStr for CpuFeature {
    type Err = ParseCpuFeatureError;

    fn from_str(s: &str) -> Result<Self, Self::Err> {
        match s {
            "sse2" => Ok(Self::SSE2),
            "sse3" => Ok(Self::SSE3),
            "ssse3" => Ok(Self::SSSE3),
            "sse4.1" => Ok(Self::SSE41),
            "sse4.2" => Ok(Self::SSE42),
            "popcnt" => Ok(Self::POPCNT),
            "avx" => Ok(Self::AVX),
            "bmi" => Ok(Self::BMI1),
            "bmi2" => Ok(Self::BMI2),
            "avx2" => Ok(Self::AVX2),
            "avx512dq" => Ok(Self::AVX512DQ),
            "avx512vl" => Ok(Self::AVX512VL),
            "avx512f" => Ok(Self::AVX512F),
            "lzcnt" => Ok(Self::LZCNT),
            _ => Err(ParseCpuFeatureError::Missing(s.to_string())),
        }
    }
}

impl ToString for CpuFeature {
    fn to_string(&self) -> String {
        match self {
            Self::SSE2 => "sse2",
            Self::SSE3 => "sse3",
            Self::SSSE3 => "ssse3",
            Self::SSE41 => "sse4.1",
            Self::SSE42 => "sse4.2",
            Self::POPCNT => "popcnt",
            Self::AVX => "avx",
            Self::BMI1 => "bmi",
            Self::BMI2 => "bmi2",
            Self::AVX2 => "avx2",
            Self::AVX512DQ => "avx512dq",
            Self::AVX512VL => "avx512vl",
            Self::AVX512F => "avx512f",
            Self::LZCNT => "lzcnt",
        }
        .to_string()
    }
}

/// This is the target that we will use for compiling
/// the WebAssembly ModuleInfo, and then run it.
#[derive(Clone, Debug, PartialEq, Eq, Hash)]
pub struct Target {
    triple: Triple,
    cpu_features: EnumSet<CpuFeature>,
}

impl Target {
    /// Creates a new target given a triple
    pub fn new(triple: Triple, cpu_features: EnumSet<CpuFeature>) -> Self {
        Self {
            triple,
            cpu_features,
        }
    }

    /// The triple associated for the target.
    pub fn triple(&self) -> &Triple {
        &self.triple
    }

    /// The triple associated for the target.
    pub fn cpu_features(&self) -> &EnumSet<CpuFeature> {
        &self.cpu_features
    }
}

/// The default for the Target will use the HOST as the triple
impl Default for Target {
    fn default() -> Self {
        Self {
            triple: Triple::host(),
            cpu_features: CpuFeature::for_host(),
        }
    }
}

'''
'''--- lib/compiler/src/translator/environ.rs ---
// This file contains code from external sources.
// Attributions: https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md
use super::state::ModuleTranslationState;
use crate::lib::std::borrow::ToOwned;
use crate::lib::std::string::ToString;
use crate::lib::std::{boxed::Box, string::String, vec::Vec};
use crate::translate_module;
use crate::{WasmError, WasmResult};
use std::convert::{TryFrom, TryInto};
use std::sync::Arc;
use wasmer_types::entity::PrimaryMap;
use wasmer_types::FunctionType;
use wasmer_types::{
    CustomSectionIndex, DataIndex, DataInitializer, DataInitializerLocation, ElemIndex,
    ExportIndex, FunctionIndex, GlobalIndex, GlobalInit, GlobalType, ImportIndex,
    LocalFunctionIndex, MemoryIndex, MemoryType, ModuleInfo, OwnedTableInitializer, SignatureIndex,
    TableIndex, TableType,
};
pub use wasmparser::FunctionBody as FunctionReader;

/// Contains function data: bytecode and its offset in the module.
#[derive(Hash)]
pub struct FunctionBodyData<'a> {
    /// Function body bytecode.
    pub data: &'a [u8],

    /// Body offset relative to the module file.
    pub module_offset: usize,
}

/// The result of translating via `ModuleEnvironment`. Function bodies are not
/// yet translated, and data initializers have not yet been copied out of the
/// original buffer.
/// The function bodies will be translated by a specific compiler backend.
pub struct ModuleEnvironment<'data> {
    /// ModuleInfo information.
    pub module: ModuleInfo,

    /// References to the function bodies.
    pub function_body_inputs: PrimaryMap<LocalFunctionIndex, FunctionBodyData<'data>>,

    /// References to the data initializers.
    pub data_initializers: Vec<DataInitializer<'data>>,

    /// The decoded Wasm types for the module.
    pub module_translation_state: Option<ModuleTranslationState>,
}

impl<'data> ModuleEnvironment<'data> {
    /// Allocates the environment data structures.
    pub fn new() -> Self {
        Self {
            module: ModuleInfo::new(),
            function_body_inputs: PrimaryMap::new(),
            data_initializers: Vec::new(),
            module_translation_state: None,
        }
    }

    /// Translate a wasm module using this environment. This consumes the
    /// `ModuleEnvironment` and produces a `ModuleInfoTranslation`.
    pub fn translate(mut self, data: &'data [u8]) -> WasmResult<ModuleEnvironment<'data>> {
        assert!(self.module_translation_state.is_none());
        let module_translation_state = translate_module(data, &mut self)?;
        self.module_translation_state = Some(module_translation_state);
        Ok(self)
    }

    pub(crate) fn declare_export(&mut self, export: ExportIndex, name: &str) -> WasmResult<()> {
        self.module.exports.insert(String::from(name), export);
        Ok(())
    }

    pub(crate) fn declare_import(
        &mut self,
        import: ImportIndex,
        module: &str,
        field: &str,
    ) -> WasmResult<()> {
        self.module.imports.insert(
            (
                String::from(module),
                String::from(field),
                self.module.imports.len().try_into().unwrap(),
            ),
            import,
        );
        Ok(())
    }

    pub(crate) fn reserve_signatures(&mut self, num: u32) -> WasmResult<()> {
        self.module
            .signatures
            .reserve_exact(usize::try_from(num).unwrap());
        Ok(())
    }

    pub(crate) fn declare_signature(&mut self, sig: FunctionType) -> WasmResult<()> {
        // TODO: Deduplicate signatures.
        self.module.signatures.push(sig);
        Ok(())
    }

    pub(crate) fn declare_func_import(
        &mut self,
        sig_index: SignatureIndex,
        module: &str,
        field: &str,
    ) -> WasmResult<()> {
        debug_assert_eq!(
            self.module.functions.len(),
            self.module.import_counts.functions as usize,
            "Imported functions must be declared first"
        );
        self.declare_import(
            ImportIndex::Function(FunctionIndex::from_u32(self.module.import_counts.functions)),
            module,
            field,
        )?;
        self.module.functions.push(sig_index);
        self.module.import_counts.functions += 1;
        Ok(())
    }

    pub(crate) fn declare_table_import(
        &mut self,
        table: TableType,
        module: &str,
        field: &str,
    ) -> WasmResult<()> {
        debug_assert_eq!(
            self.module.tables.len(),
            self.module.import_counts.tables as usize,
            "Imported tables must be declared first"
        );
        self.declare_import(
            ImportIndex::Table(TableIndex::from_u32(self.module.import_counts.tables)),
            module,
            field,
        )?;
        self.module.tables.push(table);
        self.module.import_counts.tables += 1;
        Ok(())
    }

    pub(crate) fn declare_memory_import(
        &mut self,
        memory: MemoryType,
        module: &str,
        field: &str,
    ) -> WasmResult<()> {
        debug_assert_eq!(
            self.module.memories.len(),
            self.module.import_counts.memories as usize,
            "Imported memories must be declared first"
        );
        self.declare_import(
            ImportIndex::Memory(MemoryIndex::from_u32(self.module.import_counts.memories)),
            module,
            field,
        )?;
        self.module.memories.push(memory);
        self.module.import_counts.memories += 1;
        Ok(())
    }

    pub(crate) fn declare_global_import(
        &mut self,
        global: GlobalType,
        module: &str,
        field: &str,
    ) -> WasmResult<()> {
        debug_assert_eq!(
            self.module.globals.len(),
            self.module.import_counts.globals as usize,
            "Imported globals must be declared first"
        );
        self.declare_import(
            ImportIndex::Global(GlobalIndex::from_u32(self.module.import_counts.globals)),
            module,
            field,
        )?;
        self.module.globals.push(global);
        self.module.import_counts.globals += 1;
        Ok(())
    }

    pub(crate) fn finish_imports(&mut self) -> WasmResult<()> {
        Ok(())
    }

    pub(crate) fn reserve_func_types(&mut self, num: u32) -> WasmResult<()> {
        self.module
            .functions
            .reserve_exact(usize::try_from(num).unwrap());
        self.function_body_inputs
            .reserve_exact(usize::try_from(num).unwrap());
        Ok(())
    }

    pub(crate) fn declare_func_type(&mut self, sig_index: SignatureIndex) -> WasmResult<()> {
        self.module.functions.push(sig_index);
        Ok(())
    }

    pub(crate) fn reserve_tables(&mut self, num: u32) -> WasmResult<()> {
        self.module
            .tables
            .reserve_exact(usize::try_from(num).unwrap());
        Ok(())
    }

    pub(crate) fn declare_table(&mut self, table: TableType) -> WasmResult<()> {
        self.module.tables.push(table);
        Ok(())
    }

    pub(crate) fn reserve_memories(&mut self, num: u32) -> WasmResult<()> {
        self.module
            .memories
            .reserve_exact(usize::try_from(num).unwrap());
        Ok(())
    }

    pub(crate) fn declare_memory(&mut self, memory: MemoryType) -> WasmResult<()> {
        if memory.shared {
            return Err(WasmError::Unsupported(
                "shared memories are not supported yet".to_owned(),
            ));
        }
        self.module.memories.push(memory);
        Ok(())
    }

    pub(crate) fn reserve_globals(&mut self, num: u32) -> WasmResult<()> {
        self.module
            .globals
            .reserve_exact(usize::try_from(num).unwrap());
        Ok(())
    }

    pub(crate) fn declare_global(
        &mut self,
        global: GlobalType,
        initializer: GlobalInit,
    ) -> WasmResult<()> {
        self.module.globals.push(global);
        self.module.global_initializers.push(initializer);
        Ok(())
    }

    pub(crate) fn reserve_exports(&mut self, num: u32) -> WasmResult<()> {
        self.module.exports.reserve(usize::try_from(num).unwrap());
        Ok(())
    }

    pub(crate) fn declare_func_export(
        &mut self,
        func_index: FunctionIndex,
        name: &str,
    ) -> WasmResult<()> {
        self.declare_export(ExportIndex::Function(func_index), name)
    }

    pub(crate) fn declare_table_export(
        &mut self,
        table_index: TableIndex,
        name: &str,
    ) -> WasmResult<()> {
        self.declare_export(ExportIndex::Table(table_index), name)
    }

    pub(crate) fn declare_memory_export(
        &mut self,
        memory_index: MemoryIndex,
        name: &str,
    ) -> WasmResult<()> {
        self.declare_export(ExportIndex::Memory(memory_index), name)
    }

    pub(crate) fn declare_global_export(
        &mut self,
        global_index: GlobalIndex,
        name: &str,
    ) -> WasmResult<()> {
        self.declare_export(ExportIndex::Global(global_index), name)
    }

    pub(crate) fn declare_start_function(&mut self, func_index: FunctionIndex) -> WasmResult<()> {
        debug_assert!(self.module.start_function.is_none());
        self.module.start_function = Some(func_index);
        Ok(())
    }

    pub(crate) fn reserve_table_initializers(&mut self, num: u32) -> WasmResult<()> {
        self.module
            .table_initializers
            .reserve_exact(usize::try_from(num).unwrap());
        Ok(())
    }

    pub(crate) fn declare_table_initializers(
        &mut self,
        table_index: TableIndex,
        base: Option<GlobalIndex>,
        offset: usize,
        elements: Box<[FunctionIndex]>,
    ) -> WasmResult<()> {
        self.module.table_initializers.push(OwnedTableInitializer {
            table_index,
            base,
            offset,
            elements,
        });
        Ok(())
    }

    pub(crate) fn declare_passive_element(
        &mut self,
        elem_index: ElemIndex,
        segments: Box<[FunctionIndex]>,
    ) -> WasmResult<()> {
        let old = self.module.passive_elements.insert(elem_index, segments);
        debug_assert!(
            old.is_none(),
            "should never get duplicate element indices, that would be a bug in `wasmer_compiler`'s \
             translation"
        );
        Ok(())
    }

    pub(crate) fn define_function_body(
        &mut self,
        _module_translation_state: &ModuleTranslationState,
        body_bytes: &'data [u8],
        body_offset: usize,
    ) -> WasmResult<()> {
        self.function_body_inputs.push(FunctionBodyData {
            data: body_bytes,
            module_offset: body_offset,
        });
        Ok(())
    }

    pub(crate) fn reserve_data_initializers(&mut self, num: u32) -> WasmResult<()> {
        self.data_initializers
            .reserve_exact(usize::try_from(num).unwrap());
        Ok(())
    }

    pub(crate) fn declare_data_initialization(
        &mut self,
        memory_index: MemoryIndex,
        base: Option<GlobalIndex>,
        offset: usize,
        data: &'data [u8],
    ) -> WasmResult<()> {
        self.data_initializers.push(DataInitializer {
            location: DataInitializerLocation {
                memory_index,
                base,
                offset,
            },
            data,
        });
        Ok(())
    }

    pub(crate) fn reserve_passive_data(&mut self, _count: u32) -> WasmResult<()> {
        // TODO(0-copy): consider finding a more appropriate data structure for this?
        Ok(())
    }

    pub(crate) fn declare_passive_data(
        &mut self,
        data_index: DataIndex,
        data: &'data [u8],
    ) -> WasmResult<()> {
        let old = self.module.passive_data.insert(data_index, Arc::from(data));
        debug_assert!(
            old.is_none(),
            "a module can't have duplicate indices, this would be a wasmer-compiler bug"
        );
        Ok(())
    }

    pub(crate) fn declare_module_name(&mut self, name: &'data str) -> WasmResult<()> {
        self.module.name = Some(name.to_string());
        Ok(())
    }

    pub(crate) fn declare_function_name(
        &mut self,
        func_index: FunctionIndex,
        name: &'data str,
    ) -> WasmResult<()> {
        self.module
            .function_names
            .insert(func_index, name.to_string());
        Ok(())
    }

    /// Provides the number of imports up front. By default this does nothing, but
    /// implementations can use this to preallocate memory if desired.
    pub(crate) fn reserve_imports(&mut self, _num: u32) -> WasmResult<()> {
        Ok(())
    }

    /// Notifies the implementation that all exports have been declared.
    pub(crate) fn finish_exports(&mut self) -> WasmResult<()> {
        Ok(())
    }

    /// Indicates that a custom section has been found in the wasm file
    pub(crate) fn custom_section(&mut self, name: &'data str, data: &'data [u8]) -> WasmResult<()> {
        let custom_section = CustomSectionIndex::from_u32(
            self.module.custom_sections_data.len().try_into().unwrap(),
        );
        self.module
            .custom_sections
            .insert(String::from(name), custom_section);
        self.module.custom_sections_data.push(Arc::from(data));
        Ok(())
    }
}

'''
'''--- lib/compiler/src/translator/error.rs ---
use crate::{CompileError, WasmError};
use wasmparser::BinaryReaderError;

/// Return an `Err(WasmError::Unsupported(msg))` where `msg` the string built by calling `format!`
/// on the arguments to this macro.
#[macro_export]
macro_rules! wasm_unsupported {
    ($($arg:tt)*) => { $crate::WasmError::Unsupported(format!($($arg)*)) }
}

impl From<BinaryReaderError> for WasmError {
    fn from(original: BinaryReaderError) -> Self {
        Self::InvalidWebAssembly {
            message: original.message().into(),
            offset: original.offset(),
        }
    }
}

impl From<BinaryReaderError> for CompileError {
    fn from(original: BinaryReaderError) -> Self {
        // `From` does not seem to be transitive by default, so we convert
        // BinaryReaderError -> WasmError -> CompileError
        Self::from(WasmError::from(original))
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use wasmparser::BinaryReader;

    #[test]
    fn can_convert_binary_reader_error_to_wasm_error() {
        let mut reader = BinaryReader::new(b"\0\0\0\0");
        let binary_reader_error = reader.read_bytes(10).unwrap_err();
        match WasmError::from(binary_reader_error) {
            WasmError::InvalidWebAssembly { message, offset } => {
                assert_eq!(message, "Unexpected EOF");
                assert_eq!(offset, 0);
            }
            err => panic!("Unexpected error: {:?}", err),
        }
    }

    #[test]
    fn can_convert_binary_reader_error_to_compile_error() {
        let mut reader = BinaryReader::new(b"\0\0\0\0");
        let binary_reader_error = reader.read_bytes(10).unwrap_err();
        match CompileError::from(binary_reader_error) {
            CompileError::Wasm(WasmError::InvalidWebAssembly { message, offset }) => {
                assert_eq!(message, "Unexpected EOF");
                assert_eq!(offset, 0);
            }
            err => panic!("Unexpected error: {:?}", err),
        }
    }
}

'''
'''--- lib/compiler/src/translator/mod.rs ---
//! This module defines the parser and translator from wasmparser
//! to a common structure `ModuleInfo`.
//!
//! It's derived from [cranelift-wasm] but architected for multiple
//! compilers rather than just Cranelift.
//!
//! [cranelift-wasm]: https://crates.io/crates/cranelift-wasm/
mod environ;
mod module;
mod state;
#[macro_use]
mod error;
mod sections;

pub use self::environ::{FunctionBodyData, FunctionReader, ModuleEnvironment};
pub use self::module::translate_module;
pub use self::sections::wptype_to_type;
pub use self::state::ModuleTranslationState;

'''
'''--- lib/compiler/src/translator/module.rs ---
// This file contains code from external sources.
// Attributions: https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md

//! Translation skeleton that traverses the whole WebAssembly module and call helper functions
//! to deal with each part of it.
use super::environ::ModuleEnvironment;
use super::sections::{
    parse_data_section, parse_element_section, parse_export_section, parse_function_section,
    parse_global_section, parse_import_section, parse_memory_section, parse_name_section,
    parse_start_section, parse_table_section, parse_type_section,
};
use super::state::ModuleTranslationState;
use crate::WasmResult;
use wasmparser::{NameSectionReader, Parser, Payload};

/// Translate a sequence of bytes forming a valid Wasm binary into a
/// parsed ModuleInfo `ModuleTranslationState`.
pub fn translate_module<'data>(
    data: &'data [u8],
    environ: &mut ModuleEnvironment<'data>,
) -> WasmResult<ModuleTranslationState> {
    let mut module_translation_state = ModuleTranslationState::new();

    for payload in Parser::new(0).parse_all(data) {
        match payload? {
            Payload::Version { .. } | Payload::End => {}

            Payload::TypeSection(types) => {
                parse_type_section(types, &mut module_translation_state, environ)?;
            }

            Payload::ImportSection(imports) => {
                parse_import_section(imports, environ)?;
            }

            Payload::FunctionSection(functions) => {
                parse_function_section(functions, environ)?;
            }

            Payload::TableSection(tables) => {
                parse_table_section(tables, environ)?;
            }

            Payload::MemorySection(memories) => {
                parse_memory_section(memories, environ)?;
            }

            Payload::GlobalSection(globals) => {
                parse_global_section(globals, environ)?;
            }

            Payload::ExportSection(exports) => {
                parse_export_section(exports, environ)?;
            }

            Payload::StartSection { func, .. } => {
                parse_start_section(func, environ)?;
            }

            Payload::ElementSection(elements) => {
                parse_element_section(elements, environ)?;
            }

            Payload::CodeSectionStart { .. } => {}
            Payload::CodeSectionEntry(code) => {
                let mut code = code.get_binary_reader();
                let size = code.bytes_remaining();
                let offset = code.original_position();
                environ.define_function_body(
                    &module_translation_state,
                    code.read_bytes(size)?,
                    offset,
                )?;
            }

            Payload::DataSection(data) => {
                parse_data_section(data, environ)?;
            }

            Payload::DataCountSection { count, .. } => {
                environ.reserve_passive_data(count)?;
            }

            Payload::InstanceSection(_)
            | Payload::AliasSection(_)
            | Payload::EventSection(_)
            | Payload::ModuleSectionStart { .. }
            | Payload::ModuleSectionEntry { .. } => {
                unimplemented!("module linking not implemented yet")
            }

            Payload::CustomSection {
                name: "name",
                data,
                data_offset,
                ..
            } => parse_name_section(NameSectionReader::new(data, data_offset)?, environ)?,

            Payload::CustomSection { name, data, .. } => environ.custom_section(name, data)?,

            Payload::UnknownSection { .. } => unreachable!(),
        }
    }

    module_translation_state.build_import_map(&environ.module);

    Ok(module_translation_state)
}

'''
'''--- lib/compiler/src/translator/sections.rs ---
// This file contains code from external sources.
// Attributions: https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md

//! Helper functions to gather information for each of the non-function sections of a
//! WebAssembly module.
//!
//! The code of these helper functions is straightforward since they only read metadata
//! about linear memories, tables, globals, etc. and store them for later use.
//!
//! The special case of the initialize expressions for table elements offsets or global variables
//! is handled, according to the semantics of WebAssembly, to only specific expressions that are
//! interpreted on the fly.
use super::environ::ModuleEnvironment;
use super::state::ModuleTranslationState;
use crate::wasm_unsupported;
use crate::{WasmError, WasmResult};
use core::convert::TryFrom;
use std::boxed::Box;
use std::collections::HashMap;
use std::vec::Vec;
use wasmer_types::entity::packed_option::ReservedValue;
use wasmer_types::entity::EntityRef;
use wasmer_types::{
    DataIndex, ElemIndex, FunctionIndex, FunctionType, GlobalIndex, GlobalInit, GlobalType,
    MemoryIndex, MemoryType, Mutability, Pages, SignatureIndex, TableIndex, TableType, Type, V128,
};
use wasmparser::{
    self, Data, DataKind, DataSectionReader, Element, ElementItem, ElementItems, ElementKind,
    ElementSectionReader, Export, ExportSectionReader, ExternalKind, FuncType as WPFunctionType,
    FunctionSectionReader, GlobalSectionReader, GlobalType as WPGlobalType, ImportSectionEntryType,
    ImportSectionReader, MemorySectionReader, MemoryType as WPMemoryType, NameSectionReader,
    Naming, NamingReader, Operator, TableSectionReader, TypeDef, TypeSectionReader,
};

/// Helper function translating wasmparser types to Wasm Type.
pub fn wptype_to_type(ty: wasmparser::Type) -> WasmResult<Type> {
    match ty {
        wasmparser::Type::I32 => Ok(Type::I32),
        wasmparser::Type::I64 => Ok(Type::I64),
        wasmparser::Type::F32 => Ok(Type::F32),
        wasmparser::Type::F64 => Ok(Type::F64),
        wasmparser::Type::V128 => Ok(Type::V128),
        wasmparser::Type::ExternRef => Ok(Type::ExternRef),
        wasmparser::Type::FuncRef => Ok(Type::FuncRef),
        ty => Err(wasm_unsupported!(
            "wptype_to_type: wasmparser type {:?}",
            ty
        )),
    }
}

/// Parses the Type section of the wasm module.
pub fn parse_type_section(
    types: TypeSectionReader,
    module_translation_state: &mut ModuleTranslationState,
    environ: &mut ModuleEnvironment,
) -> WasmResult<()> {
    let count = types.get_count();
    environ.reserve_signatures(count)?;

    for entry in types {
        if let Ok(TypeDef::Func(WPFunctionType { params, returns })) = entry {
            let sig_params: Vec<Type> = params
                .iter()
                .map(|ty| {
                    wptype_to_type(*ty)
                        .expect("only numeric types are supported in function signatures")
                })
                .collect();
            let sig_returns: Vec<Type> = returns
                .iter()
                .map(|ty| {
                    wptype_to_type(*ty)
                        .expect("only numeric types are supported in function signatures")
                })
                .collect();
            let sig = FunctionType::new(sig_params, sig_returns);
            environ.declare_signature(sig)?;
            module_translation_state.wasm_types.push((params, returns));
        } else {
            unimplemented!("module linking not implemented yet")
        }
    }

    Ok(())
}

/// Parses the Import section of the wasm module.
pub fn parse_import_section<'data>(
    imports: ImportSectionReader<'data>,
    environ: &mut ModuleEnvironment<'data>,
) -> WasmResult<()> {
    environ.reserve_imports(imports.get_count())?;

    for entry in imports {
        let import = entry?;
        let module_name = import.module;
        let field_name = import.field;

        match import.ty {
            ImportSectionEntryType::Function(sig) => {
                environ.declare_func_import(
                    SignatureIndex::from_u32(sig),
                    module_name,
                    field_name.unwrap_or_default(),
                )?;
            }
            ImportSectionEntryType::Module(_)
            | ImportSectionEntryType::Instance(_)
            | ImportSectionEntryType::Event(_) => {
                unimplemented!("module linking not implemented yet")
            }
            ImportSectionEntryType::Memory(WPMemoryType::M32 {
                limits: ref memlimits,
                shared,
            }) => {
                environ.declare_memory_import(
                    MemoryType {
                        minimum: Pages(memlimits.initial),
                        maximum: memlimits.maximum.map(Pages),
                        shared,
                    },
                    module_name,
                    field_name.unwrap_or_default(),
                )?;
            }
            ImportSectionEntryType::Memory(WPMemoryType::M64 { .. }) => {
                unimplemented!("64bit memory not implemented yet")
            }
            ImportSectionEntryType::Global(ref ty) => {
                environ.declare_global_import(
                    GlobalType {
                        ty: wptype_to_type(ty.content_type).unwrap(),
                        mutability: if ty.mutable {
                            Mutability::Var
                        } else {
                            Mutability::Const
                        },
                    },
                    module_name,
                    field_name.unwrap_or_default(),
                )?;
            }
            ImportSectionEntryType::Table(ref tab) => {
                environ.declare_table_import(
                    TableType {
                        ty: wptype_to_type(tab.element_type).unwrap(),
                        minimum: tab.limits.initial,
                        maximum: tab.limits.maximum,
                    },
                    module_name,
                    field_name.unwrap_or_default(),
                )?;
            }
        }
    }

    environ.finish_imports()?;
    Ok(())
}

/// Parses the Function section of the wasm module.
pub fn parse_function_section(
    functions: FunctionSectionReader,
    environ: &mut ModuleEnvironment,
) -> WasmResult<()> {
    let num_functions = functions.get_count();
    if num_functions == std::u32::MAX {
        // We reserve `u32::MAX` for our own use.
        return Err(WasmError::ImplLimitExceeded);
    }

    environ.reserve_func_types(num_functions)?;

    for entry in functions {
        let sigindex = entry?;
        environ.declare_func_type(SignatureIndex::from_u32(sigindex))?;
    }

    Ok(())
}

/// Parses the Table section of the wasm module.
pub fn parse_table_section(
    tables: TableSectionReader,
    environ: &mut ModuleEnvironment,
) -> WasmResult<()> {
    environ.reserve_tables(tables.get_count())?;

    for entry in tables {
        let table = entry?;
        environ.declare_table(TableType {
            ty: wptype_to_type(table.element_type).unwrap(),
            minimum: table.limits.initial,
            maximum: table.limits.maximum,
        })?;
    }

    Ok(())
}

/// Parses the Memory section of the wasm module.
pub fn parse_memory_section(
    memories: MemorySectionReader,
    environ: &mut ModuleEnvironment,
) -> WasmResult<()> {
    environ.reserve_memories(memories.get_count())?;

    for entry in memories {
        let memory = entry?;
        match memory {
            WPMemoryType::M32 { limits, shared } => {
                environ.declare_memory(MemoryType {
                    minimum: Pages(limits.initial),
                    maximum: limits.maximum.map(Pages),
                    shared,
                })?;
            }
            WPMemoryType::M64 { .. } => unimplemented!("64bit memory not implemented yet"),
        }
    }

    Ok(())
}

/// Parses the Global section of the wasm module.
pub fn parse_global_section(
    globals: GlobalSectionReader,
    environ: &mut ModuleEnvironment,
) -> WasmResult<()> {
    environ.reserve_globals(globals.get_count())?;

    for entry in globals {
        let wasmparser::Global {
            ty: WPGlobalType {
                content_type,
                mutable,
            },
            init_expr,
        } = entry?;
        let mut init_expr_reader = init_expr.get_binary_reader();
        let initializer = match init_expr_reader.read_operator()? {
            Operator::I32Const { value } => GlobalInit::I32Const(value),
            Operator::I64Const { value } => GlobalInit::I64Const(value),
            Operator::F32Const { value } => GlobalInit::F32Const(f32::from_bits(value.bits())),
            Operator::F64Const { value } => GlobalInit::F64Const(f64::from_bits(value.bits())),
            Operator::V128Const { value } => GlobalInit::V128Const(V128::from(*value.bytes())),
            Operator::RefNull { ty: _ } => GlobalInit::RefNullConst,
            Operator::RefFunc { function_index } => {
                GlobalInit::RefFunc(FunctionIndex::from_u32(function_index))
            }
            Operator::GlobalGet { global_index } => {
                GlobalInit::GetGlobal(GlobalIndex::from_u32(global_index))
            }
            ref s => {
                return Err(wasm_unsupported!(
                    "unsupported init expr in global section: {:?}",
                    s
                ));
            }
        };
        let global = GlobalType {
            ty: wptype_to_type(content_type).unwrap(),
            mutability: if mutable {
                Mutability::Var
            } else {
                Mutability::Const
            },
        };
        environ.declare_global(global, initializer)?;
    }

    Ok(())
}

/// Parses the Export section of the wasm module.
pub fn parse_export_section<'data>(
    exports: ExportSectionReader<'data>,
    environ: &mut ModuleEnvironment<'data>,
) -> WasmResult<()> {
    environ.reserve_exports(exports.get_count())?;

    for entry in exports {
        let Export {
            field,
            ref kind,
            index,
        } = entry?;

        // The input has already been validated, so we should be able to
        // assume valid UTF-8 and use `from_utf8_unchecked` if performance
        // becomes a concern here.
        let index = index as usize;
        match *kind {
            ExternalKind::Function => {
                environ.declare_func_export(FunctionIndex::new(index), field)?
            }
            ExternalKind::Table => environ.declare_table_export(TableIndex::new(index), field)?,
            ExternalKind::Memory => {
                environ.declare_memory_export(MemoryIndex::new(index), field)?
            }
            ExternalKind::Global => {
                environ.declare_global_export(GlobalIndex::new(index), field)?
            }
            ExternalKind::Type
            | ExternalKind::Module
            | ExternalKind::Instance
            | ExternalKind::Event => {
                unimplemented!("module linking not implemented yet")
            }
        }
    }

    environ.finish_exports()?;
    Ok(())
}

/// Parses the Start section of the wasm module.
pub fn parse_start_section(index: u32, environ: &mut ModuleEnvironment) -> WasmResult<()> {
    environ.declare_start_function(FunctionIndex::from_u32(index))?;
    Ok(())
}

fn read_elems(items: &ElementItems) -> WasmResult<Box<[FunctionIndex]>> {
    let items_reader = items.get_items_reader()?;
    let mut elems = Vec::with_capacity(usize::try_from(items_reader.get_count()).unwrap());
    for item in items_reader {
        let elem = match item? {
            ElementItem::Null(_ty) => FunctionIndex::reserved_value(),
            ElementItem::Func(index) => FunctionIndex::from_u32(index),
        };
        elems.push(elem);
    }
    Ok(elems.into_boxed_slice())
}

/// Parses the Element section of the wasm module.
pub fn parse_element_section<'data>(
    elements: ElementSectionReader<'data>,
    environ: &mut ModuleEnvironment,
) -> WasmResult<()> {
    environ.reserve_table_initializers(elements.get_count())?;

    for (index, entry) in elements.into_iter().enumerate() {
        let Element { kind, items, ty } = entry?;
        if ty != wasmparser::Type::FuncRef {
            return Err(wasm_unsupported!(
                "unsupported table element type: {:?}",
                ty
            ));
        }
        let segments = read_elems(&items)?;
        match kind {
            ElementKind::Active {
                table_index,
                init_expr,
            } => {
                let mut init_expr_reader = init_expr.get_binary_reader();
                let (base, offset) = match init_expr_reader.read_operator()? {
                    Operator::I32Const { value } => (None, value as u32 as usize),
                    Operator::GlobalGet { global_index } => {
                        (Some(GlobalIndex::from_u32(global_index)), 0)
                    }
                    ref s => {
                        return Err(wasm_unsupported!(
                            "unsupported init expr in element section: {:?}",
                            s
                        ));
                    }
                };
                environ.declare_table_initializers(
                    TableIndex::from_u32(table_index),
                    base,
                    offset,
                    segments,
                )?
            }
            ElementKind::Passive => {
                let index = ElemIndex::from_u32(index as u32);
                environ.declare_passive_element(index, segments)?;
            }
            ElementKind::Declared => (),
        }
    }
    Ok(())
}

/// Parses the Data section of the wasm module.
pub fn parse_data_section<'data>(
    data: DataSectionReader<'data>,
    environ: &mut ModuleEnvironment<'data>,
) -> WasmResult<()> {
    environ.reserve_data_initializers(data.get_count())?;

    for (index, entry) in data.into_iter().enumerate() {
        let Data { kind, data } = entry?;
        match kind {
            DataKind::Active {
                memory_index,
                init_expr,
            } => {
                let mut init_expr_reader = init_expr.get_binary_reader();
                let (base, offset) = match init_expr_reader.read_operator()? {
                    Operator::I32Const { value } => (None, value as u32 as usize),
                    Operator::GlobalGet { global_index } => {
                        (Some(GlobalIndex::from_u32(global_index)), 0)
                    }
                    ref s => {
                        return Err(wasm_unsupported!(
                            "unsupported init expr in data section: {:?}",
                            s
                        ))
                    }
                };
                environ.declare_data_initialization(
                    MemoryIndex::from_u32(memory_index),
                    base,
                    offset,
                    data,
                )?;
            }
            DataKind::Passive => {
                let index = DataIndex::from_u32(index as u32);
                environ.declare_passive_data(index, data)?;
            }
        }
    }

    Ok(())
}

/// Parses the Name section of the wasm module.
pub fn parse_name_section<'data>(
    mut names: NameSectionReader<'data>,
    environ: &mut ModuleEnvironment<'data>,
) -> WasmResult<()> {
    while let Ok(subsection) = names.read() {
        match subsection {
            wasmparser::Name::Function(function_subsection) => {
                if let Some(function_names) = function_subsection
                    .get_map()
                    .ok()
                    .and_then(parse_function_name_subsection)
                {
                    for (index, name) in function_names {
                        environ.declare_function_name(index, name)?;
                    }
                }
            }
            wasmparser::Name::Module(module) => {
                if let Ok(name) = module.get_name() {
                    environ.declare_module_name(name)?;
                }
            }
            wasmparser::Name::Local(_) => {}
            wasmparser::Name::Unknown { .. } => {}
        };
    }
    Ok(())
}

fn parse_function_name_subsection(
    mut naming_reader: NamingReader<'_>,
) -> Option<HashMap<FunctionIndex, &str>> {
    let mut function_names = HashMap::new();
    for _ in 0..naming_reader.get_count() {
        let Naming { index, name } = naming_reader.read().ok()?;
        if index == std::u32::MAX {
            // We reserve `u32::MAX` for our own use.
            return None;
        }

        if function_names
            .insert(FunctionIndex::from_u32(index), name)
            .is_some()
        {
            // If the function index has been previously seen, then we
            // break out of the loop and early return `None`, because these
            // should be unique.
            return None;
        }
    }
    Some(function_names)
}

'''
'''--- lib/compiler/src/translator/state.rs ---
// This file contains code from external sources.
// Attributions: https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md

use crate::{wasm_unsupported, WasmResult};
use std::boxed::Box;
use std::collections::HashMap;
use wasmer_types::entity::PrimaryMap;
use wasmer_types::{FunctionIndex, ImportIndex, ModuleInfo, SignatureIndex};

/// Map of signatures to a function's parameter and return types.
pub(crate) type WasmTypes =
    PrimaryMap<SignatureIndex, (Box<[wasmparser::Type]>, Box<[wasmparser::Type]>)>;

/// Contains information decoded from the Wasm module that must be referenced
/// during each Wasm function's translation.
///
/// This is only for data that is maintained by `wasmer-compiler` itself, as
/// opposed to being maintained by the embedder. Data that is maintained by the
/// embedder is represented with `ModuleEnvironment`.
#[derive(Debug)]
pub struct ModuleTranslationState {
    /// A map containing a Wasm module's original, raw signatures.
    ///
    /// This is used for translating multi-value Wasm blocks inside functions,
    /// which are encoded to refer to their type signature via index.
    pub(crate) wasm_types: WasmTypes,

    /// Imported functions names map.
    pub import_map: HashMap<FunctionIndex, String>,
}

impl ModuleTranslationState {
    /// Creates a new empty ModuleTranslationState.
    pub fn new() -> Self {
        Self {
            wasm_types: PrimaryMap::new(),
            import_map: HashMap::new(),
        }
    }

    /// Build map of imported functions names for intrinsification.
    pub fn build_import_map(&mut self, module: &ModuleInfo) {
        for key in module.imports.keys() {
            let value = &module.imports[key];
            match value {
                ImportIndex::Function(index) => {
                    self.import_map.insert(*index, key.1.clone());
                }
                _ => {
                    // Non-function import.
                }
            }
        }
    }

    /// Get the parameter and result types for the given Wasm blocktype.
    pub fn blocktype_params_results(
        &self,
        ty_or_ft: wasmparser::TypeOrFuncType,
    ) -> WasmResult<(&[wasmparser::Type], &[wasmparser::Type])> {
        Ok(match ty_or_ft {
            wasmparser::TypeOrFuncType::Type(ty) => match ty {
                wasmparser::Type::I32 => (&[], &[wasmparser::Type::I32]),
                wasmparser::Type::I64 => (&[], &[wasmparser::Type::I64]),
                wasmparser::Type::F32 => (&[], &[wasmparser::Type::F32]),
                wasmparser::Type::F64 => (&[], &[wasmparser::Type::F64]),
                wasmparser::Type::V128 => (&[], &[wasmparser::Type::V128]),
                wasmparser::Type::ExternRef => (&[], &[wasmparser::Type::ExternRef]),
                wasmparser::Type::FuncRef => (&[], &[wasmparser::Type::FuncRef]),
                wasmparser::Type::EmptyBlockType => (&[], &[]),
                ty => return Err(wasm_unsupported!("blocktype_params_results: type {:?}", ty)),
            },
            wasmparser::TypeOrFuncType::FuncType(ty_index) => {
                let sig_idx = SignatureIndex::from_u32(ty_index);
                let (ref params, ref results) = self.wasm_types[sig_idx];
                (&*params, &*results)
            }
        })
    }
}

'''
'''--- lib/compiler/src/trap.rs ---
use crate::CodeOffset;
use wasmer_vm::TrapCode;

/// Information about trap.
#[derive(rkyv::Serialize, rkyv::Deserialize, rkyv::Archive, Clone, Debug, PartialEq, Eq)]
pub struct TrapInformation {
    /// The offset of the trapping instruction in native code. It is relative to the beginning of the function.
    pub code_offset: CodeOffset,
    /// Code of the trap.
    pub trap_code: TrapCode,
}

'''
'''--- lib/compiler/src/unwind.rs ---
//! A `CompiledFunctionUnwindInfo` contains the function unwind information.
//!
//! The unwind information is used to determine which function
//! called the function that threw the exception, and which
//! function called that one, and so forth.
//!
//! [Learn more](https://en.wikipedia.org/wiki/Call_stack).
use crate::lib::std::vec::Vec;

/// Compiled function unwind information.
///
/// > Note: Windows OS have a different way of representing the [unwind info],
/// > That's why we keep the Windows data and the Unix frame layout in different
/// > fields.
///
/// [unwind info]: https://docs.microsoft.com/en-us/cpp/build/exception-handling-x64?view=vs-2019
#[derive(rkyv::Serialize, rkyv::Deserialize, rkyv::Archive, Debug, Clone, PartialEq, Eq)]
pub enum CompiledFunctionUnwindInfo {
    /// Windows UNWIND_INFO.
    WindowsX64(Vec<u8>),

    /// The unwind info is added to the Dwarf section in `Compilation`.
    Dwarf,
}

/// See [`CompiledFunctionUnwindInfo`].
#[derive(Clone, Copy)]
pub enum CompiledFunctionUnwindInfoRef<'a> {
    /// Windows UNWIND_INFO.
    WindowsX64(&'a [u8]),
    /// Unwind info is added to the Dwarf section in `Compilation`.
    Dwarf,
}

impl<'a> From<&'a CompiledFunctionUnwindInfo> for CompiledFunctionUnwindInfoRef<'a> {
    fn from(uw: &'a CompiledFunctionUnwindInfo) -> Self {
        match uw {
            CompiledFunctionUnwindInfo::WindowsX64(d) => {
                CompiledFunctionUnwindInfoRef::WindowsX64(d)
            }
            CompiledFunctionUnwindInfo::Dwarf => CompiledFunctionUnwindInfoRef::Dwarf,
        }
    }
}

impl<'a> From<&'a ArchivedCompiledFunctionUnwindInfo> for CompiledFunctionUnwindInfoRef<'a> {
    fn from(uw: &'a ArchivedCompiledFunctionUnwindInfo) -> Self {
        match uw {
            ArchivedCompiledFunctionUnwindInfo::WindowsX64(d) => {
                CompiledFunctionUnwindInfoRef::WindowsX64(d)
            }
            ArchivedCompiledFunctionUnwindInfo::Dwarf => CompiledFunctionUnwindInfoRef::Dwarf,
        }
    }
}

'''
'''--- lib/derive/Cargo.toml ---
[package]
name = "wasmer-derive-near"
version = "2.4.1"
description = "Wasmer derive macros"
authors = ["Wasmer Engineering Team <engineering@wasmer.io>"]
repository = "https://github.com/wasmerio/wasmer"
license = "MIT"
edition = "2018"

[lib]
proc-macro = true
name = "wasmer_derive"

[dependencies]
syn = { version = "1.0.72", features = ["full", "extra-traits"] }
quote = "1"
proc-macro2 = "1"
proc-macro-error = "1.0.0"

[dev-dependencies]
wasmer = { path = "../api", version = "=2.4.1", package = "wasmer-near" }
compiletest_rs = "0.6"

'''
'''--- lib/derive/src/lib.rs ---
extern crate proc_macro;

use proc_macro2::TokenStream;
use proc_macro_error::{abort, proc_macro_error, set_dummy};
use quote::{quote, quote_spanned, ToTokens};
use syn::{spanned::Spanned, *};

mod parse;

use crate::parse::WasmerAttr;

#[proc_macro_error]
#[proc_macro_derive(WasmerEnv, attributes(wasmer))]
pub fn derive_wasmer_env(input: proc_macro::TokenStream) -> proc_macro::TokenStream {
    let input: DeriveInput = syn::parse(input).unwrap();
    let gen = impl_wasmer_env(&input);
    gen.into()
}

fn impl_wasmer_env_for_struct(
    name: &Ident,
    data: &DataStruct,
    generics: &Generics,
    _attrs: &[Attribute],
) -> TokenStream {
    let (trait_methods, helper_methods) = derive_struct_fields(data);
    let lifetimes_and_generics = generics.params.clone();
    let where_clause = generics.where_clause.clone();
    quote! {
        impl < #lifetimes_and_generics > ::wasmer::WasmerEnv for #name < #lifetimes_and_generics > #where_clause{
            #trait_methods
        }

        #[allow(dead_code)]
        impl < #lifetimes_and_generics > #name < #lifetimes_and_generics > #where_clause {
            #helper_methods
        }
    }
}

fn impl_wasmer_env(input: &DeriveInput) -> TokenStream {
    let struct_name = &input.ident;

    set_dummy(quote! {
        impl ::wasmer::WasmerEnv for #struct_name {
            fn init_with_instance(&mut self, instance: &::wasmer::Instance) -> ::core::result::Result<(), ::wasmer::HostEnvInitError> {
                Ok(())
            }
        }
    });

    match &input.data {
        Data::Struct(ds) => {
            impl_wasmer_env_for_struct(struct_name, ds, &input.generics, &input.attrs)
        }
        _ => todo!(),
    }
    /*match input.data {
        Struct(ds /*DataStruct {
            fields: syn::Fields::Named(ref fields),
            ..
        }*/) => ,
        Enum(ref e) => impl_wasmer_env_for_enum(struct_name, &e.variants, &input.attrs),
        _ => abort_call_site!("Clap only supports non-tuple structs and enums"),
    }*/
}

fn derive_struct_fields(data: &DataStruct) -> (TokenStream, TokenStream) {
    let mut finish = vec![];
    let mut helpers = vec![];
    //let mut assign_tokens = vec![];
    let mut touched_fields = vec![];
    let fields: Vec<Field> = match &data.fields {
        Fields::Named(ref fields) => fields.named.iter().cloned().collect(),
        Fields::Unit => vec![],
        Fields::Unnamed(fields) => fields.unnamed.iter().cloned().collect(),
    };
    for (field_num, f) in fields.into_iter().enumerate() {
        let field_idx = syn::Index::from(field_num);
        let name = f.ident.clone();
        let top_level_ty: &Type = &f.ty;
        touched_fields.push(name.clone());
        let mut wasmer_attr = None;
        for attr in &f.attrs {
            // if / filter
            if attr.path.is_ident(&Ident::new("wasmer", attr.span())) {
                let tokens = attr.tokens.clone();
                match syn::parse2(tokens) {
                    Ok(attr) => {
                        wasmer_attr = Some(attr);
                        break;
                    }
                    Err(e) => {
                        abort!(attr, "Failed to parse `wasmer` attribute: {}", e);
                    }
                }
            }
        }

        if let Some(wasmer_attr) = wasmer_attr {
            let inner_type = get_identifier(top_level_ty);
            if let Some(name) = &name {
                let name_ref_str = format!("{}_ref", name);
                let name_ref = syn::Ident::new(&name_ref_str, name.span());
                let name_ref_unchecked_str = format!("{}_ref_unchecked", name);
                let name_ref_unchecked = syn::Ident::new(&name_ref_unchecked_str, name.span());
                let helper_tokens = quote_spanned! {f.span()=>
                    /// Get access to the underlying data.
                    ///
                    /// If `WasmerEnv::finish` has been called, this function will never
                    /// return `None` unless the underlying data has been mutated manually.
                    pub fn #name_ref(&self) -> Option<&#inner_type> {
                        self.#name.get_ref()
                    }
                    /// Gets the item without checking if it's been initialized.
                    ///
                    /// # Safety
                    /// `WasmerEnv::finish` must have been called on this function or
                    /// this type manually initialized.
                    pub unsafe fn #name_ref_unchecked(&self) -> &#inner_type {
                        self.#name.get_unchecked()
                    }
                };
                helpers.push(helper_tokens);
            }
            match wasmer_attr {
                WasmerAttr::Export {
                    identifier,
                    optional,
                    aliases,
                    span,
                } => {
                    let finish_tokens = if let Some(name) = name {
                        let name_str = name.to_string();
                        let item_name =
                            identifier.unwrap_or_else(|| LitStr::new(&name_str, name.span()));
                        let mut access_expr = quote_spanned! {
                            f.span() =>
                                instance.get_with_generics_weak::<#inner_type, _, _>(#item_name)
                        };
                        for alias in aliases {
                            access_expr = quote_spanned! {
                                f.span()=>
                                    #access_expr .or_else(|_| instance.get_with_generics_weak::<#inner_type, _, _>(#alias))
                            };
                        }
                        if optional {
                            quote_spanned! {
                                f.span()=>
                                    match #access_expr {
                                        Ok(#name) => { self.#name.initialize(#name); },
                                        Err(_) => (),
                                    };
                            }
                        } else {
                            quote_spanned! {
                                f.span()=>
                                    let #name: #inner_type = #access_expr?;
                                    self.#name.initialize(#name);
                            }
                        }
                    } else if let Some(identifier) = identifier {
                        let mut access_expr = quote_spanned! {
                            f.span() =>
                                instance.get_with_generics_weak::<#inner_type, _, _>(#identifier)
                        };
                        for alias in aliases {
                            access_expr = quote_spanned! {
                                f.span()=>
                                    #access_expr .or_else(|_| instance.get_with_generics_weak::<#inner_type, _, _>(#alias))
                            };
                        }
                        let local_var =
                            Ident::new(&format!("field_{}", field_num), identifier.span());
                        if optional {
                            quote_spanned! {
                                f.span()=>
                                    match #access_expr {
                                        Ok(#local_var) => {
                                            self.#field_idx.initialize(#local_var);
                                        },
                                        Err(_) => (),
                                    }
                            }
                        } else {
                            quote_spanned! {
                                f.span()=>
                                    let #local_var: #inner_type = #access_expr?;
                                self.#field_idx.initialize(#local_var);
                            }
                        }
                    } else {
                        abort!(
                            span,
                            "Expected `name` field on export attribute because field does not have a name. For example: `#[wasmer(export(name = \"wasm_ident\"))]`.",
                        );
                    };

                    finish.push(finish_tokens);
                }
            }
        }
    }

    let trait_methods = quote! {
        fn init_with_instance(&mut self, instance: &::wasmer::Instance) -> ::core::result::Result<(), ::wasmer::HostEnvInitError> {
            #(#finish)*
            Ok(())
        }
    };

    let helper_methods = quote! {
        #(#helpers)*
    };

    (trait_methods, helper_methods)
}

// TODO: name this something that makes sense
fn get_identifier(ty: &Type) -> TokenStream {
    match ty {
        Type::Path(TypePath {
            path: Path { segments, .. },
            ..
        }) => {
            if let Some(PathSegment { ident, arguments }) = segments.last() {
                if ident != "LazyInit" {
                    abort!(
                        ident,
                        "WasmerEnv derive expects all `export`s to be wrapped in `LazyInit`"
                    );
                }
                if let PathArguments::AngleBracketed(AngleBracketedGenericArguments {
                    args, ..
                }) = arguments
                {
                    // TODO: proper error handling
                    assert_eq!(args.len(), 1);
                    if let GenericArgument::Type(Type::Path(TypePath {
                        path: Path { segments, .. },
                        ..
                    })) = &args[0]
                    {
                        segments
                            .last()
                            .expect("there must be at least one segment; TODO: error handling")
                            .to_token_stream()
                    } else {
                        abort!(
                            &args[0],
                            "unrecognized type in first generic position on `LazyInit`"
                        );
                    }
                } else {
                    abort!(arguments, "Expected a generic parameter on `LazyInit`");
                }
            } else {
                abort!(segments, "Unknown type found");
            }
        }
        _ => abort!(ty, "Unrecognized/unsupported type"),
    }
}

'''
'''--- lib/derive/src/parse.rs ---
use proc_macro2::Span;
use proc_macro_error::abort;
use syn::{
    parenthesized,
    parse::{Parse, ParseStream},
    token, Ident, LitBool, LitStr, Token,
};

pub enum WasmerAttr {
    Export {
        /// The identifier is an override, otherwise we use the field name as the name
        /// to lookup in `instance.exports`.
        identifier: Option<LitStr>,
        optional: bool,
        aliases: Vec<LitStr>,
        span: Span,
    },
}

#[derive(Debug)]
struct ExportExpr {
    name: Option<LitStr>,
    optional: bool,
    aliases: Vec<LitStr>,
}

#[derive(Debug)]
struct ExportOptions {
    name: Option<LitStr>,
    optional: bool,
    aliases: Vec<LitStr>,
}
impl Parse for ExportOptions {
    fn parse(input: ParseStream<'_>) -> syn::Result<Self> {
        let mut name = None;
        let mut optional: bool = false;
        let mut aliases: Vec<LitStr> = vec![];
        loop {
            let ident = input.parse::<Ident>()?;
            let _ = input.parse::<Token![=]>()?;
            let ident_str = ident.to_string();

            match ident_str.as_str() {
                "name" => {
                    name = Some(input.parse::<LitStr>()?);
                }
                "optional" => {
                    optional = input.parse::<LitBool>()?.value;
                }
                "alias" => {
                    let alias = input.parse::<LitStr>()?;
                    aliases.push(alias);
                }
                otherwise => {
                    abort!(
                        ident,
                        "Unrecognized argument in export options: expected `name = \"string\"`, `optional = bool`, or `alias = \"string\"` found `{}`",
                        otherwise
                    );
                }
            }

            match input.parse::<Token![,]>() {
                Ok(_) => continue,
                Err(_) => break,
            }
        }

        Ok(ExportOptions {
            name,
            optional,
            aliases,
        })
    }
}

impl Parse for ExportExpr {
    fn parse(input: ParseStream<'_>) -> syn::Result<Self> {
        let name;
        let optional;
        let aliases;
        if input.peek(Ident) {
            let options = input.parse::<ExportOptions>()?;
            name = options.name;
            optional = options.optional;
            aliases = options.aliases;
        } else {
            name = None;
            optional = false;
            aliases = vec![];
        }
        Ok(Self {
            name,
            optional,
            aliases,
        })
    }
}

// allows us to handle parens more cleanly
struct WasmerAttrInner(WasmerAttr);

impl Parse for WasmerAttrInner {
    fn parse(input: ParseStream<'_>) -> syn::Result<Self> {
        let ident: Ident = input.parse()?;
        let ident_str = ident.to_string();
        let span = ident.span();
        let out = match ident_str.as_str() {
            "export" => {
                let export_expr;
                let (name, optional, aliases) = if input.peek(token::Paren) {
                    let _: token::Paren = parenthesized!(export_expr in input);

                    let expr = export_expr.parse::<ExportExpr>()?;
                    (expr.name, expr.optional, expr.aliases)
                } else {
                    (None, false, vec![])
                };

                WasmerAttr::Export {
                    identifier: name,
                    optional,
                    aliases,
                    span,
                }
            }
            otherwise => abort!(
                ident,
                "Unexpected identifier `{}`. Expected `export`.",
                otherwise
            ),
        };
        Ok(WasmerAttrInner(out))
    }
}

impl Parse for WasmerAttr {
    fn parse(input: ParseStream<'_>) -> syn::Result<Self> {
        let attr_inner;
        parenthesized!(attr_inner in input);
        Ok(attr_inner.parse::<WasmerAttrInner>()?.0)
    }
}

'''
'''--- lib/derive/tests/basic.rs ---
#![allow(dead_code)]

use wasmer::{Function, Global, LazyInit, Memory, NativeFunc, Table, WasmerEnv};

#[derive(WasmerEnv, Clone)]
struct MyEnv {
    num: u32,
    nums: Vec<i32>,
}

fn impls_wasmer_env<T: WasmerEnv>() -> bool {
    true
}

#[test]
fn test_derive() {
    let _my_env = MyEnv {
        num: 3,
        nums: vec![1, 2, 3],
    };
    assert!(impls_wasmer_env::<MyEnv>());
}

#[derive(WasmerEnv, Clone)]
struct MyEnvWithMemory {
    num: u32,
    nums: Vec<i32>,
    #[wasmer(export)]
    memory: LazyInit<Memory>,
}

#[derive(WasmerEnv, Clone)]
struct MyEnvWithFuncs {
    num: u32,
    nums: Vec<i32>,
    #[wasmer(export)]
    memory: LazyInit<Memory>,
    #[wasmer(export)]
    sum: LazyInit<NativeFunc<(i32, i32), i32>>,
}

#[derive(WasmerEnv, Clone)]
struct MyEnvWithEverything {
    num: u32,
    nums: Vec<i32>,
    #[wasmer(export)]
    memory: LazyInit<Memory>,
    #[wasmer(export)]
    sum: LazyInit<NativeFunc<(), i32>>,
    #[wasmer(export)]
    multiply: LazyInit<Function>,
    #[wasmer(export)]
    counter: LazyInit<Global>,
    #[wasmer(export)]
    functions: LazyInit<Table>,
}

#[derive(WasmerEnv, Clone)]
struct MyEnvWithLifetime<'a> {
    name: &'a str,
    #[wasmer(export(name = "memory"))]
    memory: LazyInit<Memory>,
}

#[derive(WasmerEnv, Clone)]
struct MyUnitStruct;

#[derive(WasmerEnv, Clone)]
struct MyTupleStruct(u32);

#[derive(WasmerEnv, Clone)]
struct MyTupleStruct2(u32, u32);

#[derive(WasmerEnv, Clone)]
struct MyTupleStructWithAttribute(#[wasmer(export(name = "memory"))] LazyInit<Memory>, u32);

#[test]
fn test_derive_with_attribute() {
    assert!(impls_wasmer_env::<MyEnvWithMemory>());
    assert!(impls_wasmer_env::<MyEnvWithFuncs>());
    assert!(impls_wasmer_env::<MyEnvWithEverything>());
    assert!(impls_wasmer_env::<MyEnvWithLifetime>());
    assert!(impls_wasmer_env::<MyUnitStruct>());
    assert!(impls_wasmer_env::<MyTupleStruct>());
    assert!(impls_wasmer_env::<MyTupleStruct2>());
    assert!(impls_wasmer_env::<MyTupleStructWithAttribute>());
}

#[derive(WasmerEnv, Clone)]
struct StructWithOptionalField {
    #[wasmer(export(optional = true))]
    memory: LazyInit<Memory>,
    #[wasmer(export(optional = true, name = "real_memory"))]
    memory2: LazyInit<Memory>,
    #[wasmer(export(optional = false))]
    memory3: LazyInit<Memory>,
}

#[test]
fn test_derive_with_optional() {
    assert!(impls_wasmer_env::<StructWithOptionalField>());
}

#[derive(WasmerEnv, Clone)]
struct StructWithAliases {
    #[wasmer(export(alias = "_memory"))]
    memory: LazyInit<Memory>,
    #[wasmer(export(alias = "_real_memory", optional = true, name = "real_memory"))]
    memory2: LazyInit<Memory>,
    #[wasmer(export(alias = "_memory3", alias = "__memory3"))]
    memory3: LazyInit<Memory>,
    #[wasmer(export(alias = "_memory3", name = "memory4", alias = "__memory3"))]
    memory4: LazyInit<Memory>,
}

#[test]
fn test_derive_with_aliases() {
    assert!(impls_wasmer_env::<StructWithAliases>());
}

'''
'''--- lib/derive/tests/compile-fail/bad-attribute.rs ---
extern crate wasmer;

use wasmer::{LazyInit, Memory, WasmerEnv};

#[derive(WasmerEnv)]
struct BadAttribute {
    #[wasmer(extraport)] //~ Unexpected identifier `extraport`. Expected `export`.
    memory: LazyInit<Memory>,
}

fn main() {}

'''
'''--- lib/derive/tests/compile-fail/bad-export-arg.rs ---
extern crate wasmer;

use wasmer::{LazyInit, Memory, WasmerEnv};

#[derive(WasmerEnv)]
struct BadExportArg {
    #[wasmer(export(this_is_not_a_real_argument = "hello, world"))]
    //~ Unrecognized argument in export options: expected `name` found `this_is_not_a_real_argument
    memory: LazyInit<Memory>,
}

#[derive(WasmerEnv)]
struct BadExportArgRawString {
    #[wasmer(export("hello"))] //~ Failed to parse `wasmer` attribute: unexpected token
    memory: LazyInit<Memory>,
}

fn main() {}

'''
'''--- lib/derive/tests/compile-fail/no-lazy-init.rs ---
extern crate wasmer;

use wasmer::{LazyInit, Memory, WasmerEnv};

#[derive(WasmerEnv)]
struct ExportNotWrappedInLazyInit {
    #[wasmer(export)]
    memory: Memory, //~ WasmerEnv derive expects all `export`s to be wrapped in `LazyInit`
}

fn main() {}

'''
'''--- lib/derive/tests/compiletest.rs ---
// file is a modified version of  https://github.com/AltSysrq/proptest/blob/proptest-derive/proptest-derive/tests/compiletest.rs

// Original copyright and license:
// Copyright 2018 The proptest developers
//
// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or
// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license
// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your
// option. This file may not be copied, modified, or distributed
// except according to those terms.

// Modifications copyright 2020 Wasmer
// Licensed under the MIT license

extern crate compiletest_rs as ct;

use std::env;

fn run_mode(src: &'static str, mode: &'static str) {
    let mut config = ct::Config::default();

    config.mode = mode.parse().expect("invalid mode");
    config.target_rustcflags = Some("-L ../../target/debug/deps".to_owned());
    if let Ok(name) = env::var("TESTNAME") {
        config.filters.push(name);
    }
    config.src_base = format!("tests/{}", src).into();

    // hack to make this work on OSX: we probably don't need it though
    /*if std::env::var("DYLD_LIBRARY_PATH").is_err() {
        let val =    std::env::var("DYLD_FALLBACK_LIBRARY_PATH").unwrap();
        std::env::set_var("DYLD_LIBRARY_PATH", val);
    }
    config.link_deps();*/

    // Uncomment this if you have the "multiple crates named `wasmer` issue". Massively slows
    // down test iteration though...
    config.clean_rmeta();

    ct::run_tests(&config);
}

#[test]
#[ignore] // ignored by default because it needs to essentially run `cargo clean` to work correctly
          // and that's really, really slow
fn compile_test() {
    run_mode("compile-fail", "compile-fail");
}

'''
'''--- lib/engine-universal/Cargo.toml ---
[package]
name = "wasmer-engine-universal-near"
version = "2.4.1"
description = "Wasmer Universal Engine"
categories = ["wasm"]
keywords = ["wasm", "webassembly", "engine", "universal"]
authors = ["Wasmer Engineering Team <engineering@wasmer.io>"]
repository = "https://github.com/wasmerio/wasmer"
license = "MIT OR Apache-2.0 WITH LLVM-exception "
readme = "README.md"
edition = "2018"

[lib]
name = "wasmer_engine_universal"

[dependencies]
wasmer-types = { path = "../types", version = "=2.4.1", package = "wasmer-types-near" }
wasmer-compiler = { path = "../compiler", version = "=2.4.1", package = "wasmer-compiler-near", features = ["translator"] }
wasmer-vm = { path = "../vm", version = "=2.4.1", package = "wasmer-vm-near" }
wasmer-engine = { path = "../engine", package = "wasmer-engine-near", version = "=2.4.1" }
# flexbuffers = { path = "../../../flatbuffers/rust/flexbuffers", version = "0.1.0" }
region = "3.0"
cfg-if = "1.0"
leb128 = "0.2"
rkyv = "0.7.31"
enumset = "1.0"
thiserror = "1"

[target.'cfg(target_os = "windows")'.dependencies]
winapi = { version = "0.3", features = ["winnt", "impl-default"] }

[features]
# Enable the `compiler` feature if you want the engine to compile
# and not be only on headless mode.
compiler = ["wasmer-compiler/translator"]

[badges]
maintenance = { status = "actively-developed" }

'''
'''--- lib/engine-universal/README.md ---
# `wasmer-engine-universal` [![Build Status](https://github.com/wasmerio/wasmer/workflows/build/badge.svg?style=flat-square)](https://github.com/wasmerio/wasmer/actions?query=workflow%3Abuild) [![Join Wasmer Slack](https://img.shields.io/static/v1?label=Slack&message=join%20chat&color=brighgreen&style=flat-square)](https://slack.wasmer.io) [![MIT License](https://img.shields.io/github/license/wasmerio/wasmer.svg?style=flat-square)](https://github.com/wasmerio/wasmer/blob/master/LICENSE)

The Wasmer Universal engine is usable with any compiler implementation based
on [`wasmer-compiler`]. After the compiler process the result, the Universal
pushes it into memory and links its contents so it can be usable by
the [`wasmer`] API.

*Note: you can find a [full working example using the Universal engine
here][example].*

### Acknowledgments

This project borrowed some of the code of the code memory and unwind
tables from the [`wasmtime-jit`], the code since then has evolved
significantly.

Please check [Wasmer `ATTRIBUTIONS`] to further see licenses and other
attributions of the project.

[`wasmer-compiler`]: https://github.com/wasmerio/wasmer/tree/master/lib/compiler
[`wasmer`]: https://github.com/wasmerio/wasmer/tree/master/lib/api
[example]: https://github.com/wasmerio/wasmer/blob/master/examples/engine_universal.rs
[`wasmtime-jit`]: https://crates.io/crates/wasmtime-jit
[Wasmer `ATTRIBUTIONS`]: https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md

'''
'''--- lib/engine-universal/src/artifact.rs ---
//! Define `UniversalArtifact` to allow compiling and instantiating to be
//! done as separate steps.

use std::collections::BTreeMap;
use std::convert::TryFrom;
use std::sync::Arc;
use wasmer_engine::InstantiationError;
use wasmer_types::entity::{BoxedSlice, EntityRef, PrimaryMap};
use wasmer_types::{
    DataIndex, ElemIndex, FunctionIndex, GlobalInit, GlobalType, ImportCounts, LocalFunctionIndex,
    LocalGlobalIndex, MemoryType, OwnedDataInitializer, OwnedTableInitializer, SignatureIndex,
    TableType,
};
use wasmer_vm::{
    Artifact, FunctionBodyPtr, FunctionExtent, InstanceHandle, Instantiatable, MemoryStyle,
    Resolver, TableStyle, Tunables, VMImport, VMImportType, VMLocalFunction, VMOffsets,
    VMSharedSignatureIndex,
};

/// A compiled wasm module, containing everything necessary for instantiation.
pub struct UniversalArtifact {
    // TODO: figure out how to allocate fewer distinct structures onto heap. Maybe have an arena‚Ä¶?
    pub(crate) engine: crate::UniversalEngine,
    pub(crate) import_counts: ImportCounts,
    pub(crate) start_function: Option<FunctionIndex>,
    pub(crate) vmoffsets: VMOffsets,
    pub(crate) imports: Vec<VMImport>,
    pub(crate) dynamic_function_trampolines: BoxedSlice<FunctionIndex, FunctionBodyPtr>,
    pub(crate) functions: BoxedSlice<LocalFunctionIndex, VMLocalFunction>,
    pub(crate) exports: BTreeMap<String, wasmer_types::ExportIndex>,
    pub(crate) signatures: BoxedSlice<SignatureIndex, VMSharedSignatureIndex>,
    pub(crate) local_memories: Vec<(MemoryType, MemoryStyle)>,
    pub(crate) data_segments: Vec<OwnedDataInitializer>,
    pub(crate) passive_data: BTreeMap<DataIndex, Arc<[u8]>>,
    pub(crate) local_tables: Vec<(TableType, TableStyle)>,
    pub(crate) element_segments: Vec<OwnedTableInitializer>,
    // TODO: does this need to be a BTreeMap? Can it be a plain vector?
    pub(crate) passive_elements: BTreeMap<ElemIndex, Box<[FunctionIndex]>>,
    pub(crate) local_globals: Vec<(GlobalType, GlobalInit)>,
}

impl UniversalArtifact {
    /// Return the extents of the specified local function.
    pub fn function_extent(&self, index: LocalFunctionIndex) -> Option<FunctionExtent> {
        let func = self.functions.get(index)?;
        Some(FunctionExtent {
            address: func.body,
            length: usize::try_from(func.length).unwrap(),
        })
    }

    /// Return the engine instance this artifact is loaded into.
    pub fn engine(&self) -> &crate::UniversalEngine {
        &self.engine
    }
}

impl Instantiatable for UniversalArtifact {
    type Error = InstantiationError;

    unsafe fn instantiate(
        self: Arc<Self>,
        tunables: &dyn Tunables,
        resolver: &dyn Resolver,
        host_state: Box<dyn std::any::Any>,
        config: wasmer_types::InstanceConfig,
    ) -> Result<InstanceHandle, Self::Error> {
        let (imports, import_function_envs) = {
            let mut imports = wasmer_engine::resolve_imports(
                &self.engine,
                resolver,
                &self.import_counts,
                &self.imports,
                &self.dynamic_function_trampolines,
            )
            .map_err(InstantiationError::Link)?;

            // Get the `WasmerEnv::init_with_instance` function pointers and the pointers
            // to the envs to call it on.
            let import_function_envs = imports.get_imported_function_envs();

            (imports, import_function_envs)
        };

        let (allocator, memory_definition_locations, table_definition_locations) =
            wasmer_vm::InstanceAllocator::new(self.vmoffsets.clone());

        // Memories
        let mut memories: PrimaryMap<wasmer_types::LocalMemoryIndex, _> =
            PrimaryMap::with_capacity(self.local_memories.len());
        for (idx, (ty, style)) in (self.import_counts.memories..).zip(self.local_memories.iter()) {
            let memory = tunables
                .create_vm_memory(&ty, &style, memory_definition_locations[idx as usize])
                .map_err(|e| {
                    InstantiationError::Link(wasmer_engine::LinkError::Resource(format!(
                        "Failed to create memory: {}",
                        e
                    )))
                })?;
            memories.push(memory);
        }

        // Tables
        let mut tables: PrimaryMap<wasmer_types::LocalTableIndex, _> =
            PrimaryMap::with_capacity(self.local_tables.len());
        for (idx, (ty, style)) in (self.import_counts.tables..).zip(self.local_tables.iter()) {
            let table = tunables
                .create_vm_table(ty, style, table_definition_locations[idx as usize])
                .map_err(|e| InstantiationError::Link(wasmer_engine::LinkError::Resource(e)))?;
            tables.push(table);
        }

        // Globals
        let mut globals =
            PrimaryMap::<LocalGlobalIndex, _>::with_capacity(self.local_globals.len());
        for (ty, _) in self.local_globals.iter() {
            globals.push(Arc::new(wasmer_vm::Global::new(*ty)));
        }

        let passive_data = self.passive_data.clone();
        Ok(InstanceHandle::new(
            self,
            allocator,
            memories.into_boxed_slice(),
            tables.into_boxed_slice(),
            globals.into_boxed_slice(),
            imports,
            passive_data,
            host_state,
            import_function_envs,
            config,
        ))
    }
}

impl Artifact for UniversalArtifact {
    fn offsets(&self) -> &wasmer_vm::VMOffsets {
        &self.vmoffsets
    }

    fn import_counts(&self) -> &ImportCounts {
        &self.import_counts
    }

    fn functions(&self) -> &BoxedSlice<LocalFunctionIndex, VMLocalFunction> {
        &self.functions
    }

    fn passive_elements(&self) -> &BTreeMap<ElemIndex, Box<[FunctionIndex]>> {
        &self.passive_elements
    }

    fn element_segments(&self) -> &[OwnedTableInitializer] {
        &self.element_segments[..]
    }

    fn data_segments(&self) -> &[OwnedDataInitializer] {
        &self.data_segments[..]
    }

    fn globals(&self) -> &[(GlobalType, GlobalInit)] {
        &self.local_globals[..]
    }

    fn start_function(&self) -> Option<FunctionIndex> {
        self.start_function
    }

    fn export_field(&self, name: &str) -> Option<wasmer_types::ExportIndex> {
        self.exports.get(name).cloned()
    }

    fn signatures(&self) -> &[wasmer_vm::VMSharedSignatureIndex] {
        self.signatures.values().as_slice()
    }

    fn function_signature(&self, index: FunctionIndex) -> Option<VMSharedSignatureIndex> {
        match self.import_counts().local_function_index(index) {
            Ok(local) => Some(self.functions[local].signature),
            Err(import) => self
                .imports
                .iter()
                .filter_map(|im| {
                    if let VMImportType::Function { sig, .. } = im.ty {
                        Some(sig)
                    } else {
                        None
                    }
                })
                .nth(import.index()),
        }
    }
}

'''
'''--- lib/engine-universal/src/builder.rs ---
use crate::UniversalEngine;
use wasmer_compiler::{CompilerConfig, Features, Target};

/// The Universal builder
pub struct Universal {
    #[allow(dead_code)]
    compiler_config: Option<Box<dyn CompilerConfig>>,
    target: Option<Target>,
    features: Option<Features>,
}

impl Universal {
    /// Create a new Universal
    pub fn new<T>(compiler_config: T) -> Self
    where
        T: Into<Box<dyn CompilerConfig>>,
    {
        Self {
            compiler_config: Some(compiler_config.into()),
            target: None,
            features: None,
        }
    }

    /// Create a new headless Universal
    pub fn headless() -> Self {
        Self {
            compiler_config: None,
            target: None,
            features: None,
        }
    }

    /// Set the target
    pub fn target(mut self, target: Target) -> Self {
        self.target = Some(target);
        self
    }

    /// Set the features
    pub fn features(mut self, features: Features) -> Self {
        self.features = Some(features);
        self
    }

    /// Build the `UniversalEngine` for this configuration
    #[cfg(feature = "compiler")]
    pub fn engine(self) -> UniversalEngine {
        let target = self.target.unwrap_or_default();
        if let Some(compiler_config) = self.compiler_config {
            let features = self
                .features
                .unwrap_or_else(|| compiler_config.default_features_for_target(&target));
            let compiler = compiler_config.compiler();
            UniversalEngine::new(compiler, target, features)
        } else {
            UniversalEngine::headless()
        }
    }

    /// Build the `UniversalEngine` for this configuration
    #[cfg(not(feature = "compiler"))]
    pub fn engine(self) -> UniversalEngine {
        UniversalEngine::headless()
    }
}

'''
'''--- lib/engine-universal/src/code_memory.rs ---
// This file contains code from external sources.
// Attributions: https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md

//! Memory management for executable code.
use crate::unwind::UnwindRegistry;
use wasmer_compiler::{CompiledFunctionUnwindInfoRef, CustomSectionRef, FunctionBodyRef};
use wasmer_vm::{Mmap, VMFunctionBody};

/// The optimal alignment for functions.
///
/// On x86-64, this is 16 since it's what the optimizations assume.
/// When we add support for other architectures, we should also figure out their
/// optimal alignment values.
const ARCH_FUNCTION_ALIGNMENT: usize = 16;

/// The optimal alignment for data.
///
const DATA_SECTION_ALIGNMENT: usize = 64;

/// Memory manager for executable code.
pub struct CodeMemory {
    unwind_registry: UnwindRegistry,
    mmap: Mmap,
    start_of_nonexecutable_pages: usize,
}

impl CodeMemory {
    /// Create a new `CodeMemory` instance.
    pub fn new() -> Self {
        Self {
            unwind_registry: UnwindRegistry::new(),
            mmap: Mmap::new(),
            start_of_nonexecutable_pages: 0,
        }
    }

    /// Mutably get the UnwindRegistry.
    pub fn unwind_registry_mut(&mut self) -> &mut UnwindRegistry {
        &mut self.unwind_registry
    }

    /// Allocate a single contiguous block of memory for the functions and custom sections, and copy the data in place.
    pub fn allocate(
        &mut self,
        functions: &[FunctionBodyRef<'_>],
        executable_sections: &[CustomSectionRef<'_>],
        data_sections: &[CustomSectionRef<'_>],
    ) -> Result<(Vec<&mut [VMFunctionBody]>, Vec<&mut [u8]>, Vec<&mut [u8]>), String> {
        let mut function_result = vec![];
        let mut data_section_result = vec![];
        let mut executable_section_result = vec![];

        let page_size = region::page::size();

        // 1. Calculate the total size, that is:
        // - function body size, including all trampolines
        // -- windows unwind info
        // -- padding between functions
        // - executable section body
        // -- padding between executable sections
        // - padding until a new page to change page permissions
        // - data section body size
        // -- padding between data sections

        let total_len = round_up(
            functions.iter().fold(0, |acc, func| {
                round_up(
                    acc + Self::function_allocation_size(*func),
                    ARCH_FUNCTION_ALIGNMENT,
                )
            }) + executable_sections.iter().fold(0, |acc, exec| {
                round_up(acc + exec.bytes.len(), ARCH_FUNCTION_ALIGNMENT)
            }),
            page_size,
        ) + data_sections.iter().fold(0, |acc, data| {
            round_up(acc + data.bytes.len(), DATA_SECTION_ALIGNMENT)
        });

        // 2. Allocate the pages. Mark them all read-write.

        self.mmap = Mmap::with_at_least(total_len)?;

        // 3. Determine where the pointers to each function, executable section
        // or data section are. Copy the functions. Collect the addresses of each and return them.

        let mut bytes = 0;
        let mut buf = self.mmap.as_mut_slice();
        for func in functions {
            let len = round_up(
                Self::function_allocation_size(*func),
                ARCH_FUNCTION_ALIGNMENT,
            );
            let (func_buf, next_buf) = buf.split_at_mut(len);
            buf = next_buf;
            bytes += len;

            let vmfunc = Self::copy_function(&mut self.unwind_registry, *func, func_buf);
            assert_eq!(vmfunc.as_ptr() as usize % ARCH_FUNCTION_ALIGNMENT, 0);
            function_result.push(vmfunc);
        }
        for section in executable_sections {
            let section = &section.bytes;
            assert_eq!(buf.as_mut_ptr() as usize % ARCH_FUNCTION_ALIGNMENT, 0);
            let len = round_up(section.len(), ARCH_FUNCTION_ALIGNMENT);
            let (s, next_buf) = buf.split_at_mut(len);
            buf = next_buf;
            bytes += len;
            s[..section.len()].copy_from_slice(*section);
            executable_section_result.push(s);
        }

        self.start_of_nonexecutable_pages = bytes;

        if !data_sections.is_empty() {
            // Data sections have different page permissions from the executable
            // code that came before it, so they need to be on different pages.
            let padding = round_up(bytes, page_size) - bytes;
            buf = buf.split_at_mut(padding).1;

            for section in data_sections {
                let section = &section.bytes;
                assert_eq!(buf.as_mut_ptr() as usize % DATA_SECTION_ALIGNMENT, 0);
                let len = round_up(section.len(), DATA_SECTION_ALIGNMENT);
                let (s, next_buf) = buf.split_at_mut(len);
                buf = next_buf;
                s[..section.len()].copy_from_slice(*section);
                data_section_result.push(s);
            }
        }

        Ok((
            function_result,
            executable_section_result,
            data_section_result,
        ))
    }

    /// Apply the page permissions.
    pub fn publish(&mut self) {
        if self.mmap.is_empty() || self.start_of_nonexecutable_pages == 0 {
            return;
        }
        assert!(self.mmap.len() >= self.start_of_nonexecutable_pages);
        unsafe {
            region::protect(
                self.mmap.as_mut_ptr(),
                self.start_of_nonexecutable_pages,
                region::Protection::READ_EXECUTE,
            )
        }
        .expect("unable to make memory readonly and executable");
    }

    /// Calculates the allocation size of the given compiled function.
    fn function_allocation_size(func: FunctionBodyRef<'_>) -> usize {
        match &func.unwind_info {
            Some(CompiledFunctionUnwindInfoRef::WindowsX64(info)) => {
                // Windows unwind information is required to be emitted into code memory
                // This is because it must be a positive relative offset from the start of the memory
                // Account for necessary unwind information alignment padding (32-bit alignment)
                ((func.body.len() + 3) & !3) + info.len()
            }
            _ => func.body.len(),
        }
    }

    /// Copies the data of the compiled function to the given buffer.
    ///
    /// This will also add the function to the current function table.
    fn copy_function<'a>(
        registry: &mut UnwindRegistry,
        func: FunctionBodyRef<'_>,
        buf: &'a mut [u8],
    ) -> &'a mut [VMFunctionBody] {
        assert_eq!(buf.as_ptr() as usize % ARCH_FUNCTION_ALIGNMENT, 0);

        let func_len = func.body.len();

        let (body, remainder) = buf.split_at_mut(func_len);
        body.copy_from_slice(&func.body);
        let vmfunc = Self::view_as_mut_vmfunc_slice(body);

        if let Some(CompiledFunctionUnwindInfoRef::WindowsX64(info)) = &func.unwind_info {
            // Windows unwind information is written following the function body
            // Keep unwind information 32-bit aligned (round up to the nearest 4 byte boundary)
            let unwind_start = (func_len + 3) & !3;
            let unwind_size = info.len();
            let padding = unwind_start - func_len;
            assert_eq!((func_len + padding) % 4, 0);
            let slice = remainder.split_at_mut(padding + unwind_size).0;
            slice[padding..].copy_from_slice(&info);
        }

        if let Some(info) = &func.unwind_info {
            registry
                .register(vmfunc.as_ptr() as usize, 0, func_len as u32, *info)
                .expect("failed to register unwind information");
        }

        vmfunc
    }

    /// Convert mut a slice from u8 to VMFunctionBody.
    fn view_as_mut_vmfunc_slice(slice: &mut [u8]) -> &mut [VMFunctionBody] {
        let byte_ptr: *mut [u8] = slice;
        let body_ptr = byte_ptr as *mut [VMFunctionBody];
        unsafe { &mut *body_ptr }
    }
}

fn round_up(size: usize, multiple: usize) -> usize {
    debug_assert!(multiple.is_power_of_two());
    (size + (multiple - 1)) & !(multiple - 1)
}

#[cfg(test)]
mod tests {
    use super::CodeMemory;
    fn _assert() {
        fn _assert_send_sync<T: Send + Sync>() {}
        _assert_send_sync::<CodeMemory>();
    }
}

'''
'''--- lib/engine-universal/src/engine.rs ---
//! Universal compilation.

use crate::executable::{unrkyv, UniversalExecutableRef};
use crate::{CodeMemory, UniversalArtifact, UniversalExecutable};
use rkyv::de::deserializers::SharedDeserializeMap;
use std::collections::BTreeMap;
use std::convert::TryFrom;
use std::sync::{Arc, Mutex};
#[cfg(feature = "compiler")]
use wasmer_compiler::Compiler;
use wasmer_compiler::{
    CompileError, CustomSectionProtection, CustomSectionRef, FunctionBodyRef, JumpTable,
    SectionIndex, Target,
};
use wasmer_engine::{Engine, EngineId};
use wasmer_types::entity::{EntityRef, PrimaryMap};
use wasmer_types::{
    DataInitializer, ExportIndex, Features, FunctionIndex, FunctionType, FunctionTypeRef,
    GlobalInit, GlobalType, ImportCounts, ImportIndex, LocalFunctionIndex, LocalGlobalIndex,
    MemoryIndex, SignatureIndex, TableIndex,
};
use wasmer_vm::{
    FuncDataRegistry, FunctionBodyPtr, SectionBodyPtr, SignatureRegistry, Tunables,
    VMCallerCheckedAnyfunc, VMFuncRef, VMFunctionBody, VMImportType, VMLocalFunction, VMOffsets,
    VMSharedSignatureIndex, VMTrampoline,
};

/// A WebAssembly `Universal` Engine.
#[derive(Clone)]
pub struct UniversalEngine {
    inner: Arc<Mutex<UniversalEngineInner>>,
    /// The target for the compiler
    target: Arc<Target>,
    engine_id: EngineId,
}

impl UniversalEngine {
    /// Create a new `UniversalEngine` with the given config
    #[cfg(feature = "compiler")]
    pub fn new(compiler: Box<dyn Compiler>, target: Target, features: Features) -> Self {
        Self {
            inner: Arc::new(Mutex::new(UniversalEngineInner {
                compiler: Some(compiler),
                code_memory: vec![],
                signatures: SignatureRegistry::new(),
                func_data: Arc::new(FuncDataRegistry::new()),
                features,
            })),
            target: Arc::new(target),
            engine_id: EngineId::default(),
        }
    }

    /// Create a headless `UniversalEngine`
    ///
    /// A headless engine is an engine without any compiler attached.
    /// This is useful for assuring a minimal runtime for running
    /// WebAssembly modules.
    ///
    /// For example, for running in IoT devices where compilers are very
    /// expensive, or also to optimize startup speed.
    ///
    /// # Important
    ///
    /// Headless engines can't compile or validate any modules,
    /// they just take already processed Modules (via `Module::serialize`).
    pub fn headless() -> Self {
        Self {
            inner: Arc::new(Mutex::new(UniversalEngineInner {
                #[cfg(feature = "compiler")]
                compiler: None,
                code_memory: vec![],
                signatures: SignatureRegistry::new(),
                func_data: Arc::new(FuncDataRegistry::new()),
                features: Features::default(),
            })),
            target: Arc::new(Target::default()),
            engine_id: EngineId::default(),
        }
    }

    pub(crate) fn inner(&self) -> std::sync::MutexGuard<'_, UniversalEngineInner> {
        self.inner.lock().unwrap()
    }

    pub(crate) fn inner_mut(&self) -> std::sync::MutexGuard<'_, UniversalEngineInner> {
        self.inner.lock().unwrap()
    }

    /// Compile a WebAssembly binary
    #[cfg(feature = "compiler")]
    pub fn compile_universal(
        &self,
        binary: &[u8],
        tunables: &dyn Tunables,
    ) -> Result<crate::UniversalExecutable, CompileError> {
        let inner_engine = self.inner_mut();
        let features = inner_engine.features();
        let compiler = inner_engine.compiler()?;
        let environ = wasmer_compiler::ModuleEnvironment::new();
        let translation = environ.translate(binary).map_err(CompileError::Wasm)?;

        let memory_styles: PrimaryMap<wasmer_types::MemoryIndex, _> = translation
            .module
            .memories
            .values()
            .map(|memory_type| tunables.memory_style(memory_type))
            .collect();
        let table_styles: PrimaryMap<wasmer_types::TableIndex, _> = translation
            .module
            .tables
            .values()
            .map(|table_type| tunables.table_style(table_type))
            .collect();

        // Compile the Module
        let compile_info = wasmer_compiler::CompileModuleInfo {
            module: Arc::new(translation.module),
            features: features.clone(),
            memory_styles,
            table_styles,
        };
        let compilation = compiler.compile_module(
            &self.target(),
            &compile_info,
            // SAFETY: Calling `unwrap` is correct since
            // `environ.translate()` above will write some data into
            // `module_translation_state`.
            translation.module_translation_state.as_ref().unwrap(),
            translation.function_body_inputs,
        )?;
        let function_call_trampolines = compilation.get_function_call_trampolines();
        let dynamic_function_trampolines = compilation.get_dynamic_function_trampolines();
        let data_initializers = translation
            .data_initializers
            .iter()
            .map(wasmer_types::OwnedDataInitializer::new)
            .collect();

        let frame_infos = compilation.get_frame_info();
        Ok(crate::UniversalExecutable {
            function_bodies: compilation.get_function_bodies(),
            function_relocations: compilation.get_relocations(),
            function_jt_offsets: compilation.get_jt_offsets(),
            function_frame_info: frame_infos,
            function_call_trampolines,
            dynamic_function_trampolines,
            custom_sections: compilation.get_custom_sections(),
            custom_section_relocations: compilation.get_custom_section_relocations(),
            debug: compilation.get_debug(),
            trampolines: compilation.get_trampolines(),
            compile_info,
            data_initializers,
            cpu_features: self.target().cpu_features().as_u64(),
        })
    }

    /// Load a [`UniversalExecutable`](crate::UniversalExecutable) with this engine.
    pub fn load_universal_executable(
        &self,
        executable: &UniversalExecutable,
    ) -> Result<UniversalArtifact, CompileError> {
        let info = &executable.compile_info;
        let module = &info.module;
        let local_memories = (module.import_counts.memories as usize..module.memories.len())
            .map(|idx| {
                let idx = MemoryIndex::new(idx);
                (module.memories[idx], info.memory_styles[idx].clone())
            })
            .collect();
        let local_tables = (module.import_counts.tables as usize..module.tables.len())
            .map(|idx| {
                let idx = TableIndex::new(idx);
                (module.tables[idx], info.table_styles[idx].clone())
            })
            .collect();
        let local_globals: Vec<(GlobalType, GlobalInit)> = module
            .globals
            .iter()
            .skip(module.import_counts.globals as usize)
            .enumerate()
            .map(|(idx, (_, t))| {
                let init = module.global_initializers[LocalGlobalIndex::new(idx)];
                (*t, init)
            })
            .collect();
        let mut inner_engine = self.inner_mut();

        let local_functions = executable.function_bodies.iter().map(|(_, b)| b.into());
        let function_call_trampolines = &executable.function_call_trampolines;
        let dynamic_function_trampolines = &executable.dynamic_function_trampolines;
        let signatures = module
            .signatures
            .iter()
            .map(|(_, sig)| inner_engine.signatures.register(sig.into()))
            .collect::<PrimaryMap<SignatureIndex, _>>()
            .into_boxed_slice();
        let (functions, trampolines, dynamic_trampolines, custom_sections) = inner_engine
            .allocate(
                local_functions,
                function_call_trampolines.iter().map(|(_, b)| b.into()),
                dynamic_function_trampolines.iter().map(|(_, b)| b.into()),
                executable.custom_sections.iter().map(|(_, s)| s.into()),
                |idx: LocalFunctionIndex| {
                    let func_idx = module.import_counts.function_index(idx);
                    let sig_idx = module.functions[func_idx];
                    (sig_idx, signatures[sig_idx])
                },
            )?;
        let imports = module
            .imports
            .iter()
            .map(|((module_name, field, idx), entity)| wasmer_vm::VMImport {
                module: String::from(module_name),
                field: String::from(field),
                import_no: *idx,
                ty: match entity {
                    ImportIndex::Function(i) => {
                        let sig_idx = module.functions[*i];
                        VMImportType::Function {
                            sig: signatures[sig_idx],
                            static_trampoline: trampolines[sig_idx],
                        }
                    }
                    ImportIndex::Table(i) => VMImportType::Table(module.tables[*i]),
                    &ImportIndex::Memory(i) => {
                        let ty = module.memories[i];
                        VMImportType::Memory(ty, info.memory_styles[i].clone())
                    }
                    ImportIndex::Global(i) => VMImportType::Global(module.globals[*i]),
                },
            })
            .collect();

        let function_relocations = executable.function_relocations.iter();
        let section_relocations = executable.custom_section_relocations.iter();
        crate::link_module(
            &functions,
            |func_idx, jt_idx| executable.function_jt_offsets[func_idx][jt_idx],
            function_relocations.map(|(i, rs)| (i, rs.iter().cloned())),
            &custom_sections,
            section_relocations.map(|(i, rs)| (i, rs.iter().cloned())),
            &executable.trampolines,
        );

        // Make all code loaded executable.
        inner_engine.publish_compiled_code();
        if let Some(ref d) = executable.debug {
            unsafe {
                // TODO: safety comment
                inner_engine.publish_eh_frame(std::slice::from_raw_parts(
                    *custom_sections[d.eh_frame],
                    executable.custom_sections[d.eh_frame].bytes.len(),
                ))?;
            }
        }
        let exports = module
            .exports
            .iter()
            .map(|(s, i)| (s.clone(), i.clone()))
            .collect::<BTreeMap<String, ExportIndex>>();

        Ok(UniversalArtifact {
            engine: self.clone(),
            import_counts: module.import_counts,
            start_function: module.start_function,
            vmoffsets: VMOffsets::for_host().with_module_info(&*module),
            imports,
            dynamic_function_trampolines: dynamic_trampolines.into_boxed_slice(),
            functions: functions.into_boxed_slice(),
            exports,
            signatures,
            local_memories,
            data_segments: executable.data_initializers.clone(),
            passive_data: module.passive_data.clone(),
            local_tables,
            element_segments: module.table_initializers.clone(),
            passive_elements: module.passive_elements.clone(),
            local_globals,
        })
    }

    /// Load a [`UniversalExecutableRef`](crate::UniversalExecutableRef) with this engine.
    pub fn load_universal_executable_ref(
        &self,
        executable: &UniversalExecutableRef,
    ) -> Result<UniversalArtifact, CompileError> {
        let info = &executable.compile_info;
        let module = &info.module;
        let import_counts: ImportCounts = unrkyv(&module.import_counts);
        let local_memories = (import_counts.memories as usize..module.memories.len())
            .map(|idx| {
                let idx = MemoryIndex::new(idx);
                let mty = &module.memories[&idx];
                (unrkyv(mty), unrkyv(&info.memory_styles[&idx]))
            })
            .collect();
        let local_tables = (import_counts.tables as usize..module.tables.len())
            .map(|idx| {
                let idx = TableIndex::new(idx);
                let tty = &module.tables[&idx];
                (unrkyv(tty), unrkyv(&info.table_styles[&idx]))
            })
            .collect();
        let local_globals: Vec<(GlobalType, GlobalInit)> = module
            .globals
            .iter()
            .skip(import_counts.globals as _)
            .enumerate()
            .map(|(idx, (_, t))| {
                let init = unrkyv(&module.global_initializers[&LocalGlobalIndex::new(idx)]);
                (*t, init)
            })
            .collect();

        let passive_data =
            rkyv::Deserialize::deserialize(&module.passive_data, &mut SharedDeserializeMap::new())
                .map_err(|_| CompileError::Validate("could not deserialize passive data".into()))?;
        let data_segments = executable.data_initializers.iter();
        let data_segments = data_segments
            .map(|s| DataInitializer::from(s).into())
            .collect();
        let element_segments = unrkyv(&module.table_initializers);
        let passive_elements: BTreeMap<wasmer_types::ElemIndex, Box<[FunctionIndex]>> =
            unrkyv(&module.passive_elements);

        let import_counts: ImportCounts = unrkyv(&module.import_counts);
        let mut inner_engine = self.inner_mut();

        let local_functions = executable.function_bodies.iter().map(|(_, b)| b.into());
        let call_trampolines = executable.function_call_trampolines.iter();
        let dynamic_trampolines = executable.dynamic_function_trampolines.iter();
        let signatures = module
            .signatures
            .values()
            .map(|sig| inner_engine.signatures.register(sig.into()))
            .collect::<PrimaryMap<SignatureIndex, _>>()
            .into_boxed_slice();
        let (functions, trampolines, dynamic_trampolines, custom_sections) = inner_engine
            .allocate(
                local_functions,
                call_trampolines.map(|(_, b)| b.into()),
                dynamic_trampolines.map(|(_, b)| b.into()),
                executable.custom_sections.iter().map(|(_, s)| s.into()),
                |idx: LocalFunctionIndex| {
                    let func_idx = import_counts.function_index(idx);
                    let sig_idx = module.functions[&func_idx];
                    (sig_idx, signatures[sig_idx])
                },
            )?;
        let imports = {
            module
                .imports
                .iter()
                .map(|((module_name, field, idx), entity)| wasmer_vm::VMImport {
                    module: String::from(module_name.as_str()),
                    field: String::from(field.as_str()),
                    import_no: *idx,
                    ty: match entity {
                        ImportIndex::Function(i) => {
                            let sig_idx = module.functions[i];
                            VMImportType::Function {
                                sig: signatures[sig_idx],
                                static_trampoline: trampolines[sig_idx],
                            }
                        }
                        ImportIndex::Table(i) => VMImportType::Table(unrkyv(&module.tables[i])),
                        ImportIndex::Memory(i) => {
                            let ty = unrkyv(&module.memories[i]);
                            VMImportType::Memory(ty, unrkyv(&info.memory_styles[i]))
                        }
                        ImportIndex::Global(i) => VMImportType::Global(unrkyv(&module.globals[i])),
                    },
                })
                .collect()
        };

        let function_relocations = executable.function_relocations.iter();
        let section_relocations = executable.custom_section_relocations.iter();
        crate::link_module(
            &functions,
            |func_idx, jt_idx| {
                let func_idx = rkyv::Archived::<LocalFunctionIndex>::new(func_idx.index());
                let jt_idx = rkyv::Archived::<JumpTable>::new(jt_idx.index());
                executable.function_jt_offsets[&func_idx][&jt_idx]
            },
            function_relocations.map(|(i, r)| (i, r.iter().map(unrkyv))),
            &custom_sections,
            section_relocations.map(|(i, r)| (i, r.iter().map(unrkyv))),
            &unrkyv(&executable.trampolines),
        );

        // Make all code compiled thus far executable.
        inner_engine.publish_compiled_code();
        if let rkyv::option::ArchivedOption::Some(ref d) = executable.debug {
            unsafe {
                // TODO: safety comment
                let s = CustomSectionRef::from(&executable.custom_sections[&d.eh_frame]);
                inner_engine.publish_eh_frame(std::slice::from_raw_parts(
                    *custom_sections[unrkyv(&d.eh_frame)],
                    s.bytes.len(),
                ))?;
            }
        }
        let exports = module
            .exports
            .iter()
            .map(|(s, i)| (unrkyv(s), unrkyv(i)))
            .collect::<BTreeMap<String, ExportIndex>>();
        Ok(UniversalArtifact {
            engine: self.clone(),
            import_counts,
            start_function: unrkyv(&module.start_function),
            vmoffsets: VMOffsets::for_host().with_archived_module_info(&*module),
            imports,
            dynamic_function_trampolines: dynamic_trampolines.into_boxed_slice(),
            functions: functions.into_boxed_slice(),
            exports,
            signatures,
            local_memories,
            data_segments,
            passive_data,
            local_tables,
            element_segments,
            passive_elements,
            local_globals,
        })
    }
}

impl Engine for UniversalEngine {
    /// The target
    fn target(&self) -> &Target {
        &self.target
    }

    /// Register a signature
    fn register_signature(&self, func_type: FunctionTypeRef<'_>) -> VMSharedSignatureIndex {
        self.inner().signatures.register(func_type)
    }

    fn register_function_metadata(&self, func_data: VMCallerCheckedAnyfunc) -> VMFuncRef {
        self.inner().func_data().register(func_data)
    }

    /// Lookup a signature
    fn lookup_signature(&self, sig: VMSharedSignatureIndex) -> Option<FunctionType> {
        self.inner().signatures.lookup(sig).cloned()
    }

    /// Validates a WebAssembly module
    fn validate(&self, binary: &[u8]) -> Result<(), CompileError> {
        self.inner().validate(binary)
    }

    #[cfg(not(feature = "compiler"))]
    fn compile(
        &self,
        binary: &[u8],
        tunables: &dyn Tunables,
    ) -> Result<Box<dyn wasmer_engine::Executable>, CompileError> {
        return Err(CompileError::Codegen(
            "The UniversalEngine is operating in headless mode, so it can not compile Modules."
                .to_string(),
        ));
    }

    /// Compile a WebAssembly binary
    #[cfg(feature = "compiler")]
    fn compile(
        &self,
        binary: &[u8],
        tunables: &dyn Tunables,
    ) -> Result<Box<dyn wasmer_engine::Executable>, CompileError> {
        self.compile_universal(binary, tunables)
            .map(|ex| Box::new(ex) as _)
    }

    fn load(
        &self,
        executable: &(dyn wasmer_engine::Executable),
    ) -> Result<Arc<dyn wasmer_vm::Artifact>, CompileError> {
        executable.load(self)
    }

    fn id(&self) -> &EngineId {
        &self.engine_id
    }

    fn cloned(&self) -> Arc<dyn Engine + Send + Sync> {
        Arc::new(self.clone())
    }
}

/// The inner contents of `UniversalEngine`
pub struct UniversalEngineInner {
    /// The compiler
    #[cfg(feature = "compiler")]
    compiler: Option<Box<dyn Compiler>>,
    /// The features to compile the Wasm module with
    features: Features,
    /// The code memory is responsible of publishing the compiled
    /// functions to memory.
    code_memory: Vec<CodeMemory>,
    /// The signature registry is used mainly to operate with trampolines
    /// performantly.
    pub(crate) signatures: SignatureRegistry,
    /// The backing storage of `VMFuncRef`s. This centralized store ensures that 2
    /// functions with the same `VMCallerCheckedAnyfunc` will have the same `VMFuncRef`.
    /// It also guarantees that the `VMFuncRef`s stay valid until the engine is dropped.
    func_data: Arc<FuncDataRegistry>,
}

impl UniversalEngineInner {
    /// Gets the compiler associated to this engine.
    #[cfg(feature = "compiler")]
    pub fn compiler(&self) -> Result<&dyn Compiler, CompileError> {
        if self.compiler.is_none() {
            return Err(CompileError::Codegen("The UniversalEngine is operating in headless mode, so it can only execute already compiled Modules.".to_string()));
        }
        Ok(&**self.compiler.as_ref().unwrap())
    }

    /// Validate the module
    #[cfg(feature = "compiler")]
    pub fn validate<'data>(&self, data: &'data [u8]) -> Result<(), CompileError> {
        self.compiler()?.validate_module(self.features(), data)
    }

    /// Validate the module
    #[cfg(not(feature = "compiler"))]
    pub fn validate<'data>(&self, _data: &'data [u8]) -> Result<(), CompileError> {
        Err(CompileError::Validate(
            "The UniversalEngine is not compiled with compiler support, which is required for validating"
                .to_string(),
        ))
    }

    /// The Wasm features
    pub fn features(&self) -> &Features {
        &self.features
    }

    /// Allocate compiled functions into memory
    #[allow(clippy::type_complexity)]
    pub(crate) fn allocate<'a>(
        &mut self,
        local_functions: impl ExactSizeIterator<Item = FunctionBodyRef<'a>>,
        call_trampolines: impl ExactSizeIterator<Item = FunctionBodyRef<'a>>,
        dynamic_trampolines: impl ExactSizeIterator<Item = FunctionBodyRef<'a>>,
        custom_sections: impl ExactSizeIterator<Item = CustomSectionRef<'a>>,
        function_signature: impl Fn(LocalFunctionIndex) -> (SignatureIndex, VMSharedSignatureIndex),
    ) -> Result<
        (
            PrimaryMap<LocalFunctionIndex, VMLocalFunction>,
            PrimaryMap<SignatureIndex, VMTrampoline>,
            PrimaryMap<FunctionIndex, FunctionBodyPtr>,
            PrimaryMap<SectionIndex, SectionBodyPtr>,
        ),
        CompileError,
    > {
        let code_memory = &mut self.code_memory;
        let function_count = local_functions.len();
        let call_trampoline_count = call_trampolines.len();
        let function_bodies = call_trampolines
            .chain(local_functions)
            .chain(dynamic_trampolines)
            .collect::<Vec<_>>();

        // TOOD: this shouldn't be necessary....
        let mut section_types = Vec::with_capacity(custom_sections.len());
        let mut executable_sections = Vec::new();
        let mut data_sections = Vec::new();
        for section in custom_sections {
            if let CustomSectionProtection::ReadExecute = section.protection {
                executable_sections.push(section);
            } else {
                data_sections.push(section);
            }
            section_types.push(section.protection);
        }
        code_memory.push(CodeMemory::new());
        let code_memory = self.code_memory.last_mut().expect("infallible");

        let (mut allocated_functions, allocated_executable_sections, allocated_data_sections) =
            code_memory
                .allocate(
                    function_bodies.as_slice(),
                    executable_sections.as_slice(),
                    data_sections.as_slice(),
                )
                .map_err(|message| {
                    CompileError::Resource(format!(
                        "failed to allocate memory for functions: {}",
                        message
                    ))
                })?;

        let mut allocated_function_call_trampolines: PrimaryMap<SignatureIndex, VMTrampoline> =
            PrimaryMap::new();
        for ptr in allocated_functions
            .drain(0..call_trampoline_count)
            .map(|slice| slice.as_ptr())
        {
            // TODO: What in damnation have you done?! ‚Äì Bannon
            let trampoline =
                unsafe { std::mem::transmute::<*const VMFunctionBody, VMTrampoline>(ptr) };
            allocated_function_call_trampolines.push(trampoline);
        }

        let allocated_functions_result = allocated_functions
            .drain(0..function_count)
            .enumerate()
            .map(|(index, slice)| -> Result<_, CompileError> {
                let index = LocalFunctionIndex::new(index);
                let (sig_idx, sig) = function_signature(index);
                Ok(VMLocalFunction {
                    body: FunctionBodyPtr(slice.as_ptr()),
                    length: u32::try_from(slice.len()).map_err(|_| {
                        CompileError::Codegen("function body length exceeds 4GiB".into())
                    })?,
                    signature: sig,
                    trampoline: allocated_function_call_trampolines[sig_idx],
                })
            })
            .collect::<Result<PrimaryMap<LocalFunctionIndex, _>, _>>()?;

        let allocated_dynamic_function_trampolines = allocated_functions
            .drain(..)
            .map(|slice| FunctionBodyPtr(slice.as_ptr()))
            .collect::<PrimaryMap<FunctionIndex, _>>();

        let mut exec_iter = allocated_executable_sections.iter();
        let mut data_iter = allocated_data_sections.iter();
        let allocated_custom_sections = section_types
            .into_iter()
            .map(|protection| {
                SectionBodyPtr(
                    if protection == CustomSectionProtection::ReadExecute {
                        exec_iter.next()
                    } else {
                        data_iter.next()
                    }
                    .unwrap()
                    .as_ptr(),
                )
            })
            .collect::<PrimaryMap<SectionIndex, _>>();

        Ok((
            allocated_functions_result,
            allocated_function_call_trampolines,
            allocated_dynamic_function_trampolines,
            allocated_custom_sections,
        ))
    }

    /// Make memory containing compiled code executable.
    pub(crate) fn publish_compiled_code(&mut self) {
        self.code_memory.last_mut().unwrap().publish();
    }

    /// Register DWARF-type exception handling information associated with the code.
    pub(crate) fn publish_eh_frame(&mut self, eh_frame: &[u8]) -> Result<(), CompileError> {
        self.code_memory
            .last_mut()
            .unwrap()
            .unwind_registry_mut()
            .publish(eh_frame)
            .map_err(|e| {
                CompileError::Resource(format!("Error while publishing the unwind code: {}", e))
            })?;
        Ok(())
    }

    /// Shared func metadata registry.
    pub(crate) fn func_data(&self) -> &Arc<FuncDataRegistry> {
        &self.func_data
    }
}

'''
'''--- lib/engine-universal/src/executable.rs ---
use std::sync::Arc;

use enumset::EnumSet;
use rkyv::de::deserializers::SharedDeserializeMap;
use rkyv::ser::serializers::{
    AllocScratchError, AllocSerializer, CompositeSerializerError, SharedSerializeMapError,
};
use wasmer_compiler::{
    CompileError, CompileModuleInfo, CompiledFunctionFrameInfo, CpuFeature, CustomSection, Dwarf,
    Features, FunctionBody, JumpTableOffsets, Relocation, SectionIndex, TrampolinesSection,
};
use wasmer_engine::{DeserializeError, Engine};
use wasmer_types::entity::PrimaryMap;
use wasmer_types::{
    ExportIndex, FunctionIndex, ImportIndex, LocalFunctionIndex, OwnedDataInitializer,
    SignatureIndex,
};
use wasmer_vm::Artifact;

const MAGIC_HEADER: [u8; 32] = {
    let value = *b"\0wasmer-universal\xFF\xFF\xFF\xFF\xFF\xFF\xFF\xFF\xFF\xFF\xFF\xFF\xFF\xFF\xFF";
    let _length_must_be_multiple_of_16: bool = [true][value.len() % 16];
    value
};

/// A 0-copy view of the encoded `UniversalExecutable` payload.
#[derive(Clone, Copy)]
pub struct UniversalExecutableRef<'a> {
    buffer: &'a [u8],
    archive: &'a ArchivedUniversalExecutable,
}

impl<'a> std::ops::Deref for UniversalExecutableRef<'a> {
    type Target = ArchivedUniversalExecutable;
    fn deref(&self) -> &Self::Target {
        self.archive
    }
}

impl<'a> UniversalExecutableRef<'a> {
    /// Verify the buffer for whether it is a valid `UniversalExecutable`.
    pub fn verify_serialized(data: &[u8]) -> Result<(), &'static str> {
        if !data.starts_with(&MAGIC_HEADER) {
            return Err("the provided bytes are not wasmer-universal");
        }
        if data.len() < MAGIC_HEADER.len() + 8 {
            return Err("the data buffer is too small to be valid");
        }
        let (remaining, position) = data.split_at(data.len() - 8);
        let mut position_value = [0u8; 8];
        position_value.copy_from_slice(position);
        if u64::from_le_bytes(position_value) > remaining.len() as u64 {
            return Err("the buffer is malformed");
        }
        // TODO(0-copy): bytecheck too.
        Ok(())
    }

    /// # Safety
    ///
    /// This method is unsafe since it deserializes data directly
    /// from memory.
    /// Right now we are not doing any extra work for validation, but
    /// `rkyv` has an option to do bytecheck on the serialized data before
    /// serializing (via `rkyv::check_archived_value`).
    pub unsafe fn deserialize(
        data: &'a [u8],
    ) -> Result<UniversalExecutableRef<'a>, DeserializeError> {
        Self::verify_serialized(data).map_err(|e| DeserializeError::Incompatible(e.to_string()))?;
        let (archive, position) = data.split_at(data.len() - 8);
        let mut position_value = [0u8; 8];
        position_value.copy_from_slice(position);
        let (_, data) = archive.split_at(MAGIC_HEADER.len());
        Ok(UniversalExecutableRef {
            buffer: data,
            archive: rkyv::archived_value::<UniversalExecutable>(
                data,
                u64::from_le_bytes(position_value) as usize,
            ),
        })
    }

    // TODO(0-copy): this should never fail.
    /// Convert this reference to an owned `UniversalExecutable` value.
    pub fn to_owned(self) -> Result<UniversalExecutable, DeserializeError> {
        let mut deserializer = SharedDeserializeMap::new();
        rkyv::Deserialize::deserialize(self.archive, &mut deserializer)
            .map_err(|e| DeserializeError::CorruptedBinary(format!("{:?}", e)))
    }
}

/// A wasm module compiled to some shape, ready to be loaded with `UniversalEngine` to produce an
/// `UniversalArtifact`.
///
/// This is the result obtained after validating and compiling a WASM module with any of the
/// supported compilers. This type falls in-between a module and [`Artifact`](crate::Artifact).
#[derive(rkyv::Archive, rkyv::Deserialize, rkyv::Serialize)]
pub struct UniversalExecutable {
    pub(crate) function_bodies: PrimaryMap<LocalFunctionIndex, FunctionBody>,
    pub(crate) function_relocations: PrimaryMap<LocalFunctionIndex, Vec<Relocation>>,
    pub(crate) function_jt_offsets: PrimaryMap<LocalFunctionIndex, JumpTableOffsets>,
    pub(crate) function_frame_info: PrimaryMap<LocalFunctionIndex, CompiledFunctionFrameInfo>,
    pub(crate) function_call_trampolines: PrimaryMap<SignatureIndex, FunctionBody>,
    pub(crate) dynamic_function_trampolines: PrimaryMap<FunctionIndex, FunctionBody>,
    pub(crate) custom_sections: PrimaryMap<SectionIndex, CustomSection>,
    pub(crate) custom_section_relocations: PrimaryMap<SectionIndex, Vec<Relocation>>,
    // The section indices corresponding to the Dwarf debug info
    pub(crate) debug: Option<Dwarf>,
    // the Trampoline for Arm arch
    pub(crate) trampolines: Option<TrampolinesSection>,
    pub(crate) compile_info: CompileModuleInfo,
    pub(crate) data_initializers: Vec<OwnedDataInitializer>,
    pub(crate) cpu_features: u64,
}

#[derive(thiserror::Error, Debug)]
pub enum ExecutableSerializeError {
    #[error("could not serialize the executable data")]
    Executable(
        #[source]
        CompositeSerializerError<
            std::convert::Infallible,
            AllocScratchError,
            SharedSerializeMapError,
        >,
    ),
}

impl wasmer_engine::Executable for UniversalExecutable {
    fn load(
        &self,
        engine: &(dyn Engine + 'static),
    ) -> Result<std::sync::Arc<dyn Artifact>, CompileError> {
        engine
            .downcast_ref::<crate::UniversalEngine>()
            .ok_or(CompileError::EngineDowncast)?
            .load_universal_executable(self)
            .map(|a| Arc::new(a) as _)
    }

    fn features(&self) -> Features {
        self.compile_info.features.clone()
    }

    fn cpu_features(&self) -> EnumSet<CpuFeature> {
        EnumSet::from_u64(self.cpu_features)
    }

    fn serialize(&self) -> Result<Vec<u8>, Box<(dyn std::error::Error + Send + Sync + 'static)>> {
        // The format is as thus:
        //
        // HEADER
        // RKYV PAYLOAD
        // RKYV POSITION
        //
        // It is expected that any framing for message length is handled by the caller.
        let mut serializer = AllocSerializer::<1024>::default();
        let pos = rkyv::ser::Serializer::serialize_value(&mut serializer, self)
            .map_err(ExecutableSerializeError::Executable)? as u64;
        let pos_bytes = pos.to_le_bytes();
        let data = serializer.into_serializer().into_inner();
        let mut out = Vec::with_capacity(MAGIC_HEADER.len() + pos_bytes.len() + data.len());
        out.extend(&MAGIC_HEADER);
        out.extend(data.as_slice());
        out.extend(&pos_bytes);
        Ok(out)
    }

    fn function_name(&self, index: FunctionIndex) -> Option<&str> {
        let module = &self.compile_info.module;
        // First, lets see if there's a name by which this function is exported.
        for (name, idx) in module.exports.iter() {
            match idx {
                &ExportIndex::Function(fi) if fi == index => return Some(&*name),
                _ => continue,
            }
        }
        if let Some(r) = module.function_names.get(&index) {
            return Some(&**r);
        }
        for ((_, field, _), idx) in module.imports.iter() {
            match idx {
                &ImportIndex::Function(fi) if fi == index => return Some(&*field),
                _ => continue,
            }
        }
        None
    }
}

impl<'a> wasmer_engine::Executable for UniversalExecutableRef<'a> {
    fn load(
        &self,
        engine: &(dyn Engine + 'static),
    ) -> Result<std::sync::Arc<dyn Artifact>, CompileError> {
        engine
            .downcast_ref::<crate::UniversalEngine>()
            .ok_or_else(|| CompileError::Codegen("can't downcast TODO FIXME".into()))?
            .load_universal_executable_ref(self)
            .map(|a| Arc::new(a) as _)
    }

    fn features(&self) -> Features {
        unrkyv(&self.archive.compile_info.features)
    }

    fn cpu_features(&self) -> EnumSet<CpuFeature> {
        EnumSet::from_u64(unrkyv(&self.archive.cpu_features))
    }

    fn serialize(&self) -> Result<Vec<u8>, Box<dyn std::error::Error + Send + Sync>> {
        Ok(self.buffer.to_vec())
    }

    fn function_name(&self, index: FunctionIndex) -> Option<&str> {
        let module = &self.compile_info.module;
        // First, lets see if there's a name by which this function is exported.
        for (name, idx) in module.exports.iter() {
            match idx {
                &ExportIndex::Function(fi) if fi == index => return Some(&*name),
                _ => continue,
            }
        }
        if let Some(r) = module.function_names.get(&index) {
            return Some(&**r);
        }
        for ((_, field, _), idx) in module.imports.iter() {
            match idx {
                &ImportIndex::Function(fi) if fi == index => return Some(&*field),
                _ => continue,
            }
        }
        None
    }
}

pub(crate) fn unrkyv<T>(archive: &T::Archived) -> T
where
    T: rkyv::Archive,
    T::Archived: rkyv::Deserialize<T, rkyv::Infallible>,
{
    Result::<_, std::convert::Infallible>::unwrap(rkyv::Deserialize::deserialize(
        archive,
        &mut rkyv::Infallible,
    ))
}

'''
'''--- lib/engine-universal/src/lib.rs ---
//! Universal backend for Wasmer compilers.
//!
//! Given a compiler (such as `CraneliftCompiler` or `LLVMCompiler`)
//! it generates the compiled machine code, and publishes it into
//! memory so it can be used externally.

#![deny(missing_docs, trivial_numeric_casts, unused_extern_crates)]
#![warn(unused_import_braces)]
#![warn(unsafe_op_in_unsafe_fn)]
#![cfg_attr(
    feature = "cargo-clippy",
    allow(clippy::new_without_default, clippy::new_without_default)
)]
#![cfg_attr(
    feature = "cargo-clippy",
    warn(
        clippy::float_arithmetic,
        clippy::mut_mut,
        clippy::nonminimal_bool,
        clippy::option_map_unwrap_or,
        clippy::option_map_unwrap_or_else,
        clippy::print_stdout,
        clippy::unicode_not_nfc,
        clippy::use_self
    )
)]

mod artifact;
mod builder;
mod code_memory;
mod engine;
mod executable;
mod link;
mod unwind;

pub use crate::artifact::UniversalArtifact;
pub use crate::builder::Universal;
pub use crate::code_memory::CodeMemory;
pub use crate::engine::UniversalEngine;
pub use crate::executable::{UniversalExecutable, UniversalExecutableRef};
pub use crate::link::link_module;

/// Version number of this crate.
pub const VERSION: &str = env!("CARGO_PKG_VERSION");

'''
'''--- lib/engine-universal/src/link.rs ---
//! Linking for Universal-compiled code.

use std::collections::HashMap;
use std::ptr::{read_unaligned, write_unaligned};
use wasmer_compiler::{
    JumpTable, Relocation, RelocationKind, RelocationTarget, SectionIndex, TrampolinesSection,
};
use wasmer_types::entity::PrimaryMap;
use wasmer_types::LocalFunctionIndex;
use wasmer_vm::{SectionBodyPtr, VMLocalFunction};

/// Add a new trampoline address, given the base adress of the Section. Return the address of the jump
/// The trampoline itself still have to be writen
fn trampolines_add(
    map: &mut HashMap<usize, usize>,
    trampoline: &TrampolinesSection,
    address: usize,
    baseaddress: usize,
) -> usize {
    if let Some(target) = map.get(&address) {
        return *target;
    }
    let ret = map.len();
    if ret == trampoline.slots {
        panic!("No more slot in Trampolines");
    }
    map.insert(address, baseaddress + ret * trampoline.size);
    baseaddress + ret * trampoline.size
}

fn use_trampoline(
    address: usize,
    allocated_sections: &PrimaryMap<SectionIndex, SectionBodyPtr>,
    trampolines: &Option<TrampolinesSection>,
    map: &mut HashMap<usize, usize>,
) -> Option<usize> {
    match trampolines {
        Some(trampolines) => Some(trampolines_add(
            map,
            trampolines,
            address,
            *allocated_sections[trampolines.section_index] as usize,
        )),
        _ => None,
    }
}

fn fill_trampoline_map(
    allocated_sections: &PrimaryMap<SectionIndex, SectionBodyPtr>,
    trampolines: &Option<TrampolinesSection>,
) -> HashMap<usize, usize> {
    let mut map: HashMap<usize, usize> = HashMap::new();
    match trampolines {
        Some(trampolines) => {
            let baseaddress = *allocated_sections[trampolines.section_index] as usize;
            for i in 0..trampolines.size {
                let jmpslot: usize = unsafe {
                    read_unaligned((baseaddress + i * trampolines.size + 8) as *mut usize)
                };
                if jmpslot != 0 {
                    map.insert(jmpslot, baseaddress + i * trampolines.size);
                }
            }
        }
        _ => {}
    };
    map
}

fn apply_relocation(
    body: usize,
    r: &Relocation,
    allocated_functions: &PrimaryMap<LocalFunctionIndex, VMLocalFunction>,
    jt_offsets: impl Fn(LocalFunctionIndex, JumpTable) -> wasmer_compiler::CodeOffset,
    allocated_sections: &PrimaryMap<SectionIndex, SectionBodyPtr>,
    trampolines: &Option<TrampolinesSection>,
    trampolines_map: &mut HashMap<usize, usize>,
) {
    let target_func_address: usize = match r.reloc_target {
        RelocationTarget::LocalFunc(index) => *allocated_functions[index].body as usize,
        RelocationTarget::LibCall(libcall) => libcall.function_pointer(),
        RelocationTarget::CustomSection(custom_section) => {
            *allocated_sections[custom_section] as usize
        }
        RelocationTarget::JumpTable(func_index, jt) => {
            let offset = jt_offsets(func_index, jt);
            *allocated_functions[func_index].body as usize + offset as usize
        }
    };

    match r.kind {
        #[cfg(target_pointer_width = "64")]
        RelocationKind::Abs8 => unsafe {
            let (reloc_address, reloc_delta) = r.for_address(body, target_func_address as u64);
            write_unaligned(reloc_address as *mut u64, reloc_delta);
        },
        #[cfg(target_pointer_width = "32")]
        RelocationKind::X86PCRel4 => unsafe {
            let (reloc_address, reloc_delta) = r.for_address(body, target_func_address as u64);
            write_unaligned(reloc_address as *mut u32, reloc_delta as _);
        },
        #[cfg(target_pointer_width = "64")]
        RelocationKind::X86PCRel8 => unsafe {
            let (reloc_address, reloc_delta) = r.for_address(body, target_func_address as u64);
            write_unaligned(reloc_address as *mut u64, reloc_delta);
        },
        RelocationKind::X86CallPCRel4 => unsafe {
            let (reloc_address, reloc_delta) = r.for_address(body, target_func_address as u64);
            write_unaligned(reloc_address as *mut u32, reloc_delta as _);
        },
        RelocationKind::X86PCRelRodata4 => {}
        RelocationKind::Arm64Call => unsafe {
            let (reloc_address, mut reloc_delta) = r.for_address(body, target_func_address as u64);
            if (reloc_delta as i64).abs() >= 0x1000_0000 {
                let new_address = match use_trampoline(
                    target_func_address,
                    allocated_sections,
                    trampolines,
                    trampolines_map,
                ) {
                    Some(new_address) => new_address,
                    _ => panic!(
                        "Relocation to big for {:?} for {:?} with {:x}, current val {:x}",
                        r.kind,
                        r.reloc_target,
                        reloc_delta,
                        read_unaligned(reloc_address as *mut u32)
                    ),
                };
                write_unaligned((new_address + 8) as *mut u64, target_func_address as u64); // write the jump address
                let (_, new_delta) = r.for_address(body, new_address as u64);
                reloc_delta = new_delta;
            }
            let reloc_delta = (((reloc_delta / 4) as u32) & 0x3ff_ffff)
                | read_unaligned(reloc_address as *mut u32);
            write_unaligned(reloc_address as *mut u32, reloc_delta);
        },
        RelocationKind::Arm64Movw0 => unsafe {
            let (reloc_address, reloc_delta) = r.for_address(body, target_func_address as u64);
            let reloc_delta =
                (((reloc_delta & 0xffff) as u32) << 5) | read_unaligned(reloc_address as *mut u32);
            write_unaligned(reloc_address as *mut u32, reloc_delta);
        },
        RelocationKind::Arm64Movw1 => unsafe {
            let (reloc_address, reloc_delta) = r.for_address(body, target_func_address as u64);
            let reloc_delta = ((((reloc_delta >> 16) & 0xffff) as u32) << 5)
                | read_unaligned(reloc_address as *mut u32);
            write_unaligned(reloc_address as *mut u32, reloc_delta);
        },
        RelocationKind::Arm64Movw2 => unsafe {
            let (reloc_address, reloc_delta) = r.for_address(body, target_func_address as u64);
            let reloc_delta = ((((reloc_delta >> 32) & 0xffff) as u32) << 5)
                | read_unaligned(reloc_address as *mut u32);
            write_unaligned(reloc_address as *mut u32, reloc_delta);
        },
        RelocationKind::Arm64Movw3 => unsafe {
            let (reloc_address, reloc_delta) = r.for_address(body, target_func_address as u64);
            let reloc_delta = ((((reloc_delta >> 48) & 0xffff) as u32) << 5)
                | read_unaligned(reloc_address as *mut u32);
            write_unaligned(reloc_address as *mut u32, reloc_delta);
        },
        kind => panic!(
            "Relocation kind unsupported in the current architecture {}",
            kind
        ),
    }
}

/// Links a module, patching the allocated functions with the
/// required relocations and jump tables.
pub fn link_module(
    allocated_functions: &PrimaryMap<LocalFunctionIndex, VMLocalFunction>,
    jt_offsets: impl Fn(LocalFunctionIndex, JumpTable) -> wasmer_compiler::CodeOffset,
    function_relocations: impl Iterator<Item = (LocalFunctionIndex, impl Iterator<Item = Relocation>)>,
    allocated_sections: &PrimaryMap<SectionIndex, SectionBodyPtr>,
    section_relocations: impl Iterator<Item = (SectionIndex, impl Iterator<Item = Relocation>)>,
    trampolines: &Option<TrampolinesSection>,
) {
    let mut trampolines_map = fill_trampoline_map(allocated_sections, trampolines);
    for (i, section_relocs) in section_relocations {
        let body = *allocated_sections[i] as usize;
        for r in section_relocs {
            apply_relocation(
                body,
                &r,
                allocated_functions,
                &jt_offsets,
                allocated_sections,
                trampolines,
                &mut trampolines_map,
            );
        }
    }
    for (i, function_relocs) in function_relocations {
        let body = *allocated_functions[i].body as usize;
        for r in function_relocs {
            apply_relocation(
                body,
                &r,
                allocated_functions,
                &jt_offsets,
                allocated_sections,
                trampolines,
                &mut trampolines_map,
            );
        }
    }
}

'''
'''--- lib/engine-universal/src/unwind/dummy.rs ---
//! Module for Dummy unwind registry.

use wasmer_compiler::CompiledFunctionUnwindInfo;

/// Represents a registry of function unwind information when the host system
/// support any one in specific.
pub struct DummyUnwindRegistry {}

impl DummyUnwindRegistry {
    /// Creates a new unwind registry with the given base address.
    pub fn new() -> Self {
        DummyUnwindRegistry {}
    }

    /// Registers a function given the start offset, length, and unwind information.
    pub fn register(
        &mut self,
        _base_address: usize,
        _func_start: u32,
        _func_len: u32,
        _info: &CompiledFunctionUnwindInfo,
    ) -> Result<(), String> {
        // Do nothing
        Ok(())
    }

    /// Publishes all registered functions.
    pub fn publish(&mut self, eh_frame: Option<&[u8]>) -> Result<(), String> {
        // Do nothing
        Ok(())
    }
}

'''
'''--- lib/engine-universal/src/unwind/mod.rs ---
cfg_if::cfg_if! {
    if #[cfg(all(windows, target_arch = "x86_64"))] {
        mod windows_x64;
        pub use self::windows_x64::*;
    } else if #[cfg(unix)] {
        mod systemv;
        pub use self::systemv::*;
    } else {
        // Otherwise, we provide a dummy fallback without unwinding
        mod dummy;
        pub use self::dummy::DummyUnwindRegistry as UnwindRegistry;
    }
}

'''
'''--- lib/engine-universal/src/unwind/systemv.rs ---
// This file contains code from external sources.
// Attributions: https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md

//! Module for System V ABI unwind registry.

use wasmer_compiler::CompiledFunctionUnwindInfoRef;

/// Represents a registry of function unwind information for System V ABI.
pub struct UnwindRegistry {
    registrations: Vec<usize>,
    published: bool,
}

extern "C" {
    // libunwind import
    fn __register_frame(fde: *const u8);
    fn __deregister_frame(fde: *const u8);
}

impl UnwindRegistry {
    /// Creates a new unwind registry with the given base address.
    pub fn new() -> Self {
        Self {
            registrations: Vec::new(),
            published: false,
        }
    }

    /// Registers a function given the start offset, length, and unwind information.
    pub fn register(
        &mut self,
        _base_address: usize,
        _func_start: u32,
        _func_len: u32,
        info: CompiledFunctionUnwindInfoRef<'_>,
    ) -> Result<(), String> {
        match info {
            CompiledFunctionUnwindInfoRef::Dwarf => {}
            _ => return Err("unsupported unwind information".to_string()),
        };
        Ok(())
    }

    /// Publishes all registered functions.
    pub fn publish(&mut self, eh_frame: &[u8]) -> Result<(), String> {
        if self.published {
            return Err("unwind registry has already been published".to_string());
        }

        unsafe {
            self.register_frames(eh_frame);
        }

        self.published = true;

        Ok(())
    }

    #[allow(clippy::cast_ptr_alignment)]
    unsafe fn register_frames(&mut self, eh_frame: &[u8]) {
        if cfg!(all(target_os = "linux", target_env = "gnu")) {
            // Registering an empty `eh_frame` (i.e. which
            // contains empty FDEs) cause problems on Linux when
            // deregistering it. We must avoid this
            // scenario. Usually, this is handled upstream by the
            // compilers.
            debug_assert_ne!(
                eh_frame,
                &[0, 0, 0, 0],
                "`eh_frame` seems to contain empty FDEs"
            );

            // On gnu (libgcc), `__register_frame` will walk the FDEs until an entry of length 0
            let ptr = eh_frame.as_ptr();
            __register_frame(ptr);
            self.registrations.push(ptr as usize);
        } else {
            // For libunwind, `__register_frame` takes a pointer to a single FDE
            let start = eh_frame.as_ptr();
            let end = start.add(eh_frame.len());
            let mut current = start;

            // Walk all of the entries in the frame table and register them
            while current < end {
                let len = std::ptr::read::<u32>(current as *const u32) as usize;

                // Skip over the CIE and zero-length FDEs.
                // LLVM's libunwind emits a warning on zero-length FDEs.
                if current != start && len != 0 {
                    __register_frame(current);
                    self.registrations.push(current as usize);
                }

                // Move to the next table entry (+4 because the length itself is not inclusive)
                current = current.add(len + 4);
            }
        }
    }
}

impl Drop for UnwindRegistry {
    fn drop(&mut self) {
        if self.published {
            unsafe {
                // libgcc stores the frame entries as a linked list in decreasing sort order
                // based on the PC value of the registered entry.
                //
                // As we store the registrations in increasing order, it would be O(N^2) to
                // deregister in that order.
                //
                // To ensure that we just pop off the first element in the list upon every
                // deregistration, walk our list of registrations backwards.
                for fde in self.registrations.iter().rev() {
                    __deregister_frame(*fde as *const _);
                }
            }
        }
    }
}

'''
'''--- lib/engine-universal/src/unwind/windows_x64.rs ---
// This file contains code from external sources.
// Attributions: https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md

//! Module for Windows x64 ABI unwind registry.
use std::collections::HashMap;
use wasmer_compiler::CompiledFunctionUnwindInfo;
use winapi::um::winnt;

/// Represents a registry of function unwind information for Windows x64 ABI.
pub struct UnwindRegistry {
    // A hashmap mapping the baseaddress with the registered runtime functions
    functions: HashMap<usize, Vec<winnt::RUNTIME_FUNCTION>>,
    published: bool,
}

impl UnwindRegistry {
    /// Creates a new unwind registry with the given base address.
    pub fn new() -> Self {
        Self {
            functions: HashMap::new(),
            published: false,
        }
    }

    /// Registers a function given the start offset, length, and unwind information.
    pub fn register(
        &mut self,
        base_address: usize,
        func_start: u32,
        func_len: u32,
        info: &CompiledFunctionUnwindInfo,
    ) -> Result<(), String> {
        if self.published {
            return Err("unwind registry has already been published".to_string());
        }

        match info {
            CompiledFunctionUnwindInfo::WindowsX64(_) => {}
            _ => return Err("unsupported unwind information".to_string()),
        };

        let mut entry = winnt::RUNTIME_FUNCTION::default();

        entry.BeginAddress = func_start;
        entry.EndAddress = func_start + func_len;

        // The unwind information should be immediately following the function
        // with padding for 4 byte alignment
        unsafe {
            *entry.u.UnwindInfoAddress_mut() = (entry.EndAddress + 3) & !3;
        }
        let entries = self
            .functions
            .entry(base_address)
            .or_insert_with(|| Vec::new());

        entries.push(entry);

        Ok(())
    }

    /// Publishes all registered functions.
    pub fn publish(&mut self, _eh_frame: Option<&[u8]>) -> Result<(), String> {
        if self.published {
            return Err("unwind registry has already been published".to_string());
        }

        self.published = true;

        if !self.functions.is_empty() {
            for (base_address, functions) in self.functions.iter_mut() {
                // Windows heap allocations are 32-bit aligned, but assert just in case
                assert_eq!(
                    (functions.as_mut_ptr() as u64) % 4,
                    0,
                    "function table allocation was not aligned"
                );
                unsafe {
                    if winnt::RtlAddFunctionTable(
                        functions.as_mut_ptr(),
                        functions.len() as u32,
                        *base_address as u64,
                    ) == 0
                    {
                        return Err("failed to register function tables".to_string());
                    }
                }
            }
        }

        Ok(())
    }
}

impl Drop for UnwindRegistry {
    fn drop(&mut self) {
        if self.published {
            unsafe {
                for functions in self.functions.values_mut() {
                    winnt::RtlDeleteFunctionTable(functions.as_mut_ptr());
                }
            }
        }
    }
}

'''
'''--- lib/engine/Cargo.toml ---
[package]
name = "wasmer-engine-near"
version = "2.4.1"
description = "Wasmer Engine abstraction"
categories = ["wasm"]
keywords = ["wasm", "webassembly", "engine"]
authors = ["Wasmer Engineering Team <engineering@wasmer.io>"]
repository = "https://github.com/wasmerio/wasmer"
license = "MIT OR Apache-2.0 WITH LLVM-exception "
readme = "README.md"
edition = "2018"

[lib]
name = "wasmer_engine"

[dependencies]
wasmer-types = { path = "../types", version = "=2.4.1", package = "wasmer-types-near" }
wasmer-compiler = { path = "../compiler", version = "=2.4.1", package = "wasmer-compiler-near" }
wasmer-vm = { path = "../vm", version = "=2.4.1", package = "wasmer-vm-near" }
target-lexicon = { version = "0.12.2", default-features = false }
# flexbuffers = { path = "../../../flatbuffers/rust/flexbuffers", version = "0.1.0" }
backtrace = "0.3"
rustc-demangle = "0.1"
memmap2 = "0.5"
more-asserts = "0.2"
thiserror = "1.0"
lazy_static = "1.4"
enumset = "1.0"

[badges]
maintenance = { status = "actively-developed" }

'''
'''--- lib/engine/README.md ---
# `wasmer-engine` [![Build Status](https://github.com/wasmerio/wasmer/workflows/build/badge.svg?style=flat-square)](https://github.com/wasmerio/wasmer/actions?query=workflow%3Abuild) [![Join Wasmer Slack](https://img.shields.io/static/v1?label=Slack&message=join%20chat&color=brighgreen&style=flat-square)](https://slack.wasmer.io) [![MIT License](https://img.shields.io/github/license/wasmerio/wasmer.svg?style=flat-square)](https://github.com/wasmerio/wasmer/blob/master/LICENSE)

This crate is the general abstraction for creating Engines in Wasmer.

Wasmer Engines are mainly responsible for two things:
* Transform the compilation code (from any Wasmer Compiler) to
  **create** an `Artifact`,
* **Load** an`Artifact` so it can be used by the user (normally,
  pushing the code into executable memory and so on).

It currently has three implementations:

1. Universal with [`wasmer-engine-universal`],
2. Native with [`wasmer-engine-dylib`],
3. Object with [`wasmer-engine-staticlib`].

## Example Implementation

Please check [`wasmer-engine-dummy`] for an example implementation for
an `Engine`.

### Acknowledgments

This project borrowed some of the code of the trap implementation from
the [`wasmtime-api`], the code since then has evolved significantly.

Please check [Wasmer `ATTRIBUTIONS`] to further see licenses and other
attributions of the project.

[`wasmer-engine-universal`]: https://github.com/wasmerio/wasmer/tree/master/lib/engine-universal
[`wasmer-engine-dylib`]: https://github.com/wasmerio/wasmer/tree/master/lib/engine-dylib
[`wasmer-engine-staticlib`]: https://github.com/wasmerio/wasmer/tree/master/lib/engine-staticlib
[`wasmer-engine-dummy`]: https://github.com/wasmerio/wasmer/tree/master/tests/lib/engine-dummy
[`wasmtime-api`]: https://crates.io/crates/wasmtime
[Wasmer `ATTRIBUTIONS`]: https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md

'''
'''--- lib/engine/src/engine.rs ---
//! Engine trait and associated types.

use std::sync::atomic::{AtomicUsize, Ordering::SeqCst};
use std::sync::Arc;
use wasmer_compiler::{CompileError, Target};
use wasmer_types::{FunctionType, FunctionTypeRef};
use wasmer_vm::{Artifact, Tunables, VMCallerCheckedAnyfunc, VMFuncRef, VMSharedSignatureIndex};

mod private {
    pub struct Internal(pub(super) ());
}

/// A unimplemented Wasmer `Engine`.
///
/// This trait is used by implementors to implement custom engines
/// such as: Universal or Native.
///
/// The product that an `Engine` produces and consumes is the [`Artifact`].
pub trait Engine {
    /// Gets the target
    fn target(&self) -> &Target;

    /// Register a signature
    fn register_signature(&self, func_type: FunctionTypeRef<'_>) -> VMSharedSignatureIndex;

    /// Register a function's data.
    fn register_function_metadata(&self, func_data: VMCallerCheckedAnyfunc) -> VMFuncRef;

    /// Lookup a signature
    fn lookup_signature(&self, sig: VMSharedSignatureIndex) -> Option<FunctionType>;

    /// Validates a WebAssembly module
    fn validate(&self, binary: &[u8]) -> Result<(), CompileError>;

    /// Compile a WebAssembly binary
    fn compile(
        &self,
        binary: &[u8],
        tunables: &dyn Tunables,
    ) -> Result<Box<dyn crate::Executable>, CompileError>;

    /// Load a compiled executable with this engine.
    fn load(&self, executable: &(dyn crate::Executable))
        -> Result<Arc<dyn Artifact>, CompileError>;

    /// A unique identifier for this object.
    ///
    /// This exists to allow us to compare two Engines for equality. Otherwise,
    /// comparing two trait objects unsafely relies on implementation details
    /// of trait representation.
    fn id(&self) -> &EngineId;

    /// Clone the engine
    fn cloned(&self) -> Arc<dyn Engine + Send + Sync>;

    /// Internal: support for downcasting `Engine`s.
    #[doc(hidden)]
    fn type_id(&self, _: private::Internal) -> std::any::TypeId
    where
        Self: 'static,
    {
        std::any::TypeId::of::<Self>()
    }
}

#[derive(Debug, PartialEq, Eq, PartialOrd, Ord)]
#[repr(transparent)]
/// A unique identifier for an Engine.
pub struct EngineId {
    id: usize,
}

impl EngineId {
    /// Format this identifier as a string.
    pub fn id(&self) -> String {
        format!("{}", &self.id)
    }
}

impl Clone for EngineId {
    fn clone(&self) -> Self {
        Self::default()
    }
}

impl Default for EngineId {
    fn default() -> Self {
        static NEXT_ID: AtomicUsize = AtomicUsize::new(0);
        Self {
            id: NEXT_ID.fetch_add(1, SeqCst),
        }
    }
}

impl dyn Engine {
    /// Downcast a dynamic Executable object to a concrete implementation of the trait.
    pub fn downcast_ref<T: Engine + 'static>(&self) -> Option<&T> {
        if std::any::TypeId::of::<T>() == self.type_id(private::Internal(())) {
            unsafe { Some(&*(self as *const dyn Engine as *const T)) }
        } else {
            None
        }
    }
}

'''
'''--- lib/engine/src/error.rs ---
//! The WebAssembly possible errors
use crate::trap::RuntimeError;
use std::io;
use thiserror::Error;
use wasmer_compiler::CompileError;
use wasmer_types::ExternType;

/// The Deserialize error can occur when loading a
/// compiled Module from a binary.
#[derive(Error, Debug)]
pub enum DeserializeError {
    /// An IO error
    #[error(transparent)]
    Io(#[from] io::Error),
    /// A generic deserialization error
    #[error("{0}")]
    Generic(String),
    /// Incompatible serialized binary
    #[error("incompatible binary: {0}")]
    Incompatible(String),
    /// The provided binary is corrupted
    #[error("corrupted binary: {0}")]
    CorruptedBinary(String),
    /// The binary was valid, but we got an error when
    /// trying to allocate the required resources.
    #[error(transparent)]
    Compiler(CompileError),
}

/// An ImportError.
///
/// Note: this error is not standard to WebAssembly, but it's
/// useful to determine the import issue on the API side.
#[derive(Error, Debug)]
pub enum ImportError {
    /// Incompatible Import Type.
    /// This error occurs when the import types mismatch.
    #[error("incompatible import type. Expected {0:?} but received {1:?}")]
    IncompatibleType(ExternType, ExternType),

    /// Unknown Import.
    /// This error occurs when an import was expected but not provided.
    #[error("unknown import. Expected {0:?}")]
    UnknownImport(ExternType),
}

/// The WebAssembly.LinkError object indicates an error during
/// module instantiation (besides traps from the start function).
///
/// This is based on the [link error][link-error] API.
///
/// [link-error]: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/WebAssembly/LinkError
#[derive(Error, Debug)]
#[error("Link error: {0}")]
pub enum LinkError {
    /// An error occurred when checking the import types.
    #[error("Error while importing {0:?}.{1:?}: {2}")]
    Import(String, String, ImportError),

    /// A trap ocurred during linking.
    #[error("RuntimeError occurred during linking: {0}")]
    Trap(#[source] RuntimeError),

    /// Insufficient resources available for linking.
    #[error("Insufficient resources: {0}")]
    Resource(String),
}

/// An error while instantiating a module.
///
/// This is not a common WebAssembly error, however
/// we need to differentiate from a `LinkError` (an error
/// that happens while linking, on instantiation) and a
/// Trap that occurs when calling the WebAssembly module
/// start function.
#[derive(Error, Debug)]
pub enum InstantiationError {
    /// A linking ocurred during instantiation.
    #[error(transparent)]
    Link(LinkError),

    /// The module was compiled with a CPU feature that is not available on
    /// the current host.
    #[error("module compiled with CPU feature that is missing from host")]
    CpuFeature(String),

    /// A runtime error occured while invoking the start function
    #[error(transparent)]
    Start(RuntimeError),
}

'''
'''--- lib/engine/src/executable.rs ---
use crate::Engine;
use enumset::EnumSet;
use wasmer_compiler::{CompileError, CpuFeature, Features};
use wasmer_types::FunctionIndex;
use wasmer_vm::Artifact;

mod private {
    pub struct Internal(pub(super) ());
}

/// A WASM module built by some [`Engine`](crate::Engine).
///
/// Types implementing this trait are ready to be saved (to e.g. disk) for later use or loaded with
/// the `Engine` to in order to produce an [`Artifact`](crate::Artifact).
pub trait Executable {
    /// Load this executable with the specified engine.
    ///
    /// TODO(0-copy): change error type here.
    fn load(
        &self,
        engine: &(dyn Engine + 'static),
    ) -> Result<std::sync::Arc<dyn Artifact>, CompileError>;

    /// The features with which this `Executable` was built.
    fn features(&self) -> Features;

    /// The CPU features this `Executable` requires.
    fn cpu_features(&self) -> EnumSet<CpuFeature>;

    /// Serializes the artifact into bytes
    fn serialize(&self) -> Result<Vec<u8>, Box<dyn std::error::Error + Send + Sync>>;

    /// Obtain a best effort description for the function at the given function index.
    ///
    /// Implementations are not required to maintain symbol names, so this may always return None.
    fn function_name(&self, index: FunctionIndex) -> Option<&str>;

    /// Internal: support for downcasting `Executable`s.
    #[doc(hidden)]
    fn type_id(&self, _: private::Internal) -> std::any::TypeId
    where
        Self: 'static,
    {
        std::any::TypeId::of::<Self>()
    }
}

impl dyn Executable {
    /// Downcast a dynamic Executable object to a concrete implementation of the trait.
    pub fn downcast_ref<T: Executable + 'static>(&self) -> Option<&T> {
        if std::any::TypeId::of::<T>() == self.type_id(private::Internal(())) {
            unsafe { Some(&*(self as *const dyn Executable as *const T)) }
        } else {
            None
        }
    }
}

'''
'''--- lib/engine/src/export.rs ---
use std::sync::Arc;
use wasmer_vm::{ImportInitializerFuncPtr, VMExtern, VMFunction, VMGlobal, VMMemory, VMTable};

'''
'''--- lib/engine/src/lib.rs ---
//! Generic Engine abstraction for Wasmer Engines.

#![deny(missing_docs, trivial_numeric_casts, unused_extern_crates)]
#![warn(unused_import_braces)]
#![cfg_attr(
    feature = "cargo-clippy",
    allow(clippy::new_without_default, clippy::new_without_default)
)]
#![cfg_attr(
    feature = "cargo-clippy",
    warn(
        clippy::float_arithmetic,
        clippy::mut_mut,
        clippy::nonminimal_bool,
        clippy::option_map_unwrap_or,
        clippy::option_map_unwrap_or_else,
        clippy::print_stdout,
        clippy::unicode_not_nfc,
        clippy::use_self
    )
)]

mod engine;
mod error;
mod executable;
mod resolver;
mod trap;

pub use crate::engine::{Engine, EngineId};
pub use crate::error::{DeserializeError, ImportError, InstantiationError, LinkError};
pub use crate::executable::Executable;
pub use crate::resolver::resolve_imports;
pub use crate::trap::*;

/// Version number of this crate.
pub const VERSION: &str = env!("CARGO_PKG_VERSION");

'''
'''--- lib/engine/src/resolver.rs ---
//! Define the `Resolver` trait, allowing custom resolution for external
//! references.

use crate::{Engine, ImportError, LinkError};
use more_asserts::assert_ge;
use wasmer_types::entity::{BoxedSlice, EntityRef, PrimaryMap};
use wasmer_types::{ExternType, FunctionIndex, ImportCounts, MemoryType, TableType};

use wasmer_vm::{
    Export, ExportFunctionMetadata, FunctionBodyPtr, ImportFunctionEnv, Imports, MemoryStyle,
    Resolver, VMFunctionBody, VMFunctionEnvironment, VMFunctionImport, VMFunctionKind,
    VMGlobalImport, VMImport, VMImportType, VMMemoryImport, VMTableImport,
};

fn is_compatible_table(ex: &TableType, im: &TableType) -> bool {
    (ex.ty == wasmer_types::Type::FuncRef || ex.ty == im.ty)
        && im.minimum <= ex.minimum
        && (im.maximum.is_none()
            || (!ex.maximum.is_none() && im.maximum.unwrap() >= ex.maximum.unwrap()))
}

fn is_compatible_memory(ex: &MemoryType, im: &MemoryType) -> bool {
    im.minimum <= ex.minimum
        && (im.maximum.is_none()
            || (!ex.maximum.is_none() && im.maximum.unwrap() >= ex.maximum.unwrap()))
        && ex.shared == im.shared
}

/// This function allows to match all imports of a `ModuleInfo` with concrete definitions provided by
/// a `Resolver`.
///
/// If all imports are satisfied returns an `Imports` instance required for a module instantiation.
pub fn resolve_imports(
    engine: &dyn Engine,
    resolver: &dyn Resolver,
    import_counts: &ImportCounts,
    imports: &[VMImport],
    finished_dynamic_function_trampolines: &BoxedSlice<FunctionIndex, FunctionBodyPtr>,
) -> Result<Imports, LinkError> {
    let mut function_imports = PrimaryMap::with_capacity(import_counts.functions as _);
    let mut host_function_env_initializers =
        PrimaryMap::with_capacity(import_counts.functions as _);
    let mut table_imports = PrimaryMap::with_capacity(import_counts.tables as _);
    let mut memory_imports = PrimaryMap::with_capacity(import_counts.memories as _);
    let mut global_imports = PrimaryMap::with_capacity(import_counts.globals as _);
    for VMImport {
        import_no,
        module,
        field,
        ty,
    } in imports
    {
        let resolved = resolver.resolve(*import_no, module, field);
        let import_extern = || match ty {
            &VMImportType::Table(t) => ExternType::Table(t),
            &VMImportType::Memory(t, _) => ExternType::Memory(t),
            &VMImportType::Global(t) => ExternType::Global(t),
            &VMImportType::Function {
                sig,
                static_trampoline: _,
            } => ExternType::Function(
                engine
                    .lookup_signature(sig)
                    .expect("VMSharedSignatureIndex is not valid?"),
            ),
        };
        let resolved = match resolved {
            Some(r) => r,
            None => {
                return Err(LinkError::Import(
                    module.to_string(),
                    field.to_string(),
                    ImportError::UnknownImport(import_extern()),
                ));
            }
        };
        let export_extern = || match resolved {
            Export::Function(ref f) => ExternType::Function(
                engine
                    .lookup_signature(f.vm_function.signature)
                    .expect("VMSharedSignatureIndex not registered with engine (wrong engine?)")
                    .clone(),
            ),
            Export::Table(ref t) => ExternType::Table(*t.ty()),
            Export::Memory(ref m) => ExternType::Memory(m.ty()),
            Export::Global(ref g) => {
                let global = g.from.ty();
                ExternType::Global(*global)
            }
        };
        match (&resolved, ty) {
            (
                Export::Function(ex),
                VMImportType::Function {
                    sig,
                    static_trampoline,
                },
            ) if ex.vm_function.signature == *sig => {
                let address = match ex.vm_function.kind {
                    VMFunctionKind::Dynamic => {
                        // If this is a dynamic imported function,
                        // the address of the function is the address of the
                        // reverse trampoline.
                        let index = FunctionIndex::new(function_imports.len());
                        finished_dynamic_function_trampolines[index].0 as *mut VMFunctionBody as _

                        // TODO: We should check that the f.vmctx actually matches
                        // the shape of `VMDynamicFunctionImportContext`
                    }
                    VMFunctionKind::Static => ex.vm_function.address,
                };

                // Clone the host env for this `Instance`.
                let env = if let Some(ExportFunctionMetadata {
                    host_env_clone_fn: clone,
                    ..
                }) = ex.metadata.as_deref()
                {
                    // TODO: maybe start adding asserts in all these
                    // unsafe blocks to prevent future changes from
                    // horribly breaking things.
                    unsafe {
                        assert!(!ex.vm_function.vmctx.host_env.is_null());
                        (clone)(ex.vm_function.vmctx.host_env)
                    }
                } else {
                    // No `clone` function means we're dealing with some
                    // other kind of `vmctx`, not a host env of any
                    // kind.
                    unsafe { ex.vm_function.vmctx.host_env }
                };

                let trampoline = if let Some(t) = ex.vm_function.call_trampoline {
                    Some(t)
                } else if let VMFunctionKind::Static = ex.vm_function.kind {
                    // Look up a trampoline by finding one by the signature and fill it in.
                    Some(*static_trampoline)
                } else {
                    // FIXME: remove this possibility entirely.
                    None
                };

                function_imports.push(VMFunctionImport {
                    body: FunctionBodyPtr(address),
                    signature: *sig,
                    environment: VMFunctionEnvironment { host_env: env },
                    trampoline,
                });

                let initializer = ex
                    .metadata
                    .as_ref()
                    .and_then(|m| m.import_init_function_ptr);
                let clone = ex.metadata.as_ref().map(|m| m.host_env_clone_fn);
                let destructor = ex.metadata.as_ref().map(|m| m.host_env_drop_fn);
                let import_function_env =
                    if let (Some(clone), Some(destructor)) = (clone, destructor) {
                        ImportFunctionEnv::Env {
                            env,
                            clone,
                            initializer,
                            destructor,
                        }
                    } else {
                        ImportFunctionEnv::NoEnv
                    };

                host_function_env_initializers.push(import_function_env);
            }
            (Export::Table(ex), VMImportType::Table(im)) if is_compatible_table(ex.ty(), im) => {
                let import_table_ty = ex.from.ty();
                if import_table_ty.ty != im.ty {
                    return Err(LinkError::Import(
                        module.to_string(),
                        field.to_string(),
                        ImportError::IncompatibleType(import_extern(), export_extern()),
                    ));
                }
                table_imports.push(VMTableImport {
                    definition: ex.from.vmtable(),
                    from: ex.from.clone(),
                });
            }
            (Export::Memory(ex), VMImportType::Memory(im, import_memory_style))
                if is_compatible_memory(&ex.ty(), im) =>
            {
                // Sanity-check: Ensure that the imported memory has at least
                // guard-page protections the importing module expects it to have.
                let export_memory_style = ex.style();
                if let (
                    MemoryStyle::Static { bound, .. },
                    MemoryStyle::Static {
                        bound: import_bound,
                        ..
                    },
                ) = (export_memory_style.clone(), &import_memory_style)
                {
                    assert_ge!(bound, *import_bound);
                }
                assert_ge!(
                    export_memory_style.offset_guard_size(),
                    import_memory_style.offset_guard_size()
                );
                memory_imports.push(VMMemoryImport {
                    definition: ex.from.vmmemory(),
                    from: ex.from.clone(),
                });
            }

            (Export::Global(ex), VMImportType::Global(im)) if ex.from.ty() == im => {
                global_imports.push(VMGlobalImport {
                    definition: ex.from.vmglobal(),
                    from: ex.from.clone(),
                });
            }
            _ => {
                return Err(LinkError::Import(
                    module.to_string(),
                    field.to_string(),
                    ImportError::IncompatibleType(import_extern(), export_extern()),
                ));
            }
        }
    }
    Ok(Imports::new(
        function_imports,
        host_function_env_initializers,
        table_imports,
        memory_imports,
        global_imports,
    ))
}

'''
'''--- lib/engine/src/trap/error.rs ---
use super::frame_info::{FrameInfo, GlobalFrameInfo, FRAME_INFO};
use backtrace::Backtrace;
use std::error::Error;
use std::fmt;
use std::sync::Arc;
use wasmer_vm::{raise_user_trap, Trap, TrapCode};

/// A struct representing an aborted instruction execution, with a message
/// indicating the cause.
#[derive(Clone)]
pub struct RuntimeError {
    inner: Arc<RuntimeErrorInner>,
}

/// The source of the `RuntimeError`.
#[derive(Debug)]
enum RuntimeErrorSource {
    Generic(String),
    OOM,
    User(Box<dyn Error + Send + Sync>),
    Trap(TrapCode),
}

impl fmt::Display for RuntimeErrorSource {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
            Self::Generic(s) => write!(f, "{}", s),
            Self::User(s) => write!(f, "{}", s),
            Self::OOM => write!(f, "Wasmer VM out of memory"),
            Self::Trap(s) => write!(f, "{}", s.message()),
        }
    }
}

struct RuntimeErrorInner {
    /// The source error (this can be a custom user `Error` or a [`TrapCode`])
    source: RuntimeErrorSource,
    /// The reconstructed Wasm trace (from the native trace and the `GlobalFrameInfo`).
    wasm_trace: Vec<FrameInfo>,
    /// The native backtrace
    native_trace: Backtrace,
}

fn _assert_trap_is_sync_and_send(t: &Trap) -> (&dyn Sync, &dyn Send) {
    (t, t)
}

impl RuntimeError {
    /// Creates a new generic `RuntimeError` with the given `message`.
    ///
    /// # Example
    /// ```
    /// let trap = wasmer_engine::RuntimeError::new("unexpected error");
    /// assert_eq!("unexpected error", trap.message());
    /// ```
    pub fn new<I: Into<String>>(message: I) -> Self {
        let info = FRAME_INFO.read().unwrap();
        let msg = message.into();
        Self::new_with_trace(
            &info,
            None,
            RuntimeErrorSource::Generic(msg),
            Backtrace::new_unresolved(),
        )
    }

    /// Create a new RuntimeError from a Trap.
    pub fn from_trap(trap: Trap) -> Self {
        let info = FRAME_INFO.read().unwrap();
        match trap {
            // A user error
            Trap::User(error) => {
                match error.downcast::<Self>() {
                    // The error is already a RuntimeError, we return it directly
                    Ok(runtime_error) => *runtime_error,
                    Err(e) => Self::new_with_trace(
                        &info,
                        None,
                        RuntimeErrorSource::User(e),
                        Backtrace::new_unresolved(),
                    ),
                }
            }
            // A trap caused by the VM being Out of Memory
            Trap::OOM { backtrace } => {
                Self::new_with_trace(&info, None, RuntimeErrorSource::OOM, backtrace)
            }
            // A trap caused by an error on the generated machine code for a Wasm function
            Trap::Wasm {
                pc,
                signal_trap,
                backtrace,
            } => {
                let code = info
                    .lookup_trap_info(pc)
                    .map_or(signal_trap.unwrap_or(TrapCode::StackOverflow), |info| {
                        info.trap_code
                    });
                Self::new_with_trace(&info, Some(pc), RuntimeErrorSource::Trap(code), backtrace)
            }
            // A trap triggered manually from the Wasmer runtime
            Trap::Lib {
                trap_code,
                backtrace,
            } => Self::new_with_trace(&info, None, RuntimeErrorSource::Trap(trap_code), backtrace),
        }
    }

    /// Raises a custom user Error
    pub fn raise(error: Box<dyn Error + Send + Sync>) -> ! {
        unsafe { raise_user_trap(error) }
    }

    fn new_with_trace(
        info: &GlobalFrameInfo,
        trap_pc: Option<usize>,
        source: RuntimeErrorSource,
        native_trace: Backtrace,
    ) -> Self {
        let frames: Vec<usize> = native_trace
            .frames()
            .iter()
            .filter_map(|frame| {
                let pc = frame.ip() as usize;
                if pc == 0 {
                    None
                } else {
                    // Note that we need to be careful about the pc we pass in here to
                    // lookup frame information. This program counter is used to
                    // translate back to an original source location in the origin wasm
                    // module. If this pc is the exact pc that the trap happened at,
                    // then we look up that pc precisely. Otherwise backtrace
                    // information typically points at the pc *after* the call
                    // instruction (because otherwise it's likely a call instruction on
                    // the stack). In that case we want to lookup information for the
                    // previous instruction (the call instruction) so we subtract one as
                    // the lookup.
                    let pc_to_lookup = if Some(pc) == trap_pc { pc } else { pc - 1 };
                    Some(pc_to_lookup)
                }
            })
            .collect();

        // Let's construct the trace
        let wasm_trace = frames
            .into_iter()
            .filter_map(|pc| info.lookup_frame_info(pc))
            .collect::<Vec<_>>();

        Self {
            inner: Arc::new(RuntimeErrorInner {
                source,
                wasm_trace,
                native_trace,
            }),
        }
    }

    /// Returns a reference the `message` stored in `Trap`.
    pub fn message(&self) -> String {
        self.inner.source.to_string()
    }

    /// Returns a list of function frames in WebAssembly code that led to this
    /// trap happening.
    pub fn trace(&self) -> &[FrameInfo] {
        &self.inner.wasm_trace
    }

    /// Attempts to downcast the `RuntimeError` to a concrete type.
    pub fn downcast<T: Error + 'static>(self) -> Result<T, Self> {
        match Arc::try_unwrap(self.inner) {
            // We only try to downcast user errors
            Ok(RuntimeErrorInner {
                source: RuntimeErrorSource::User(err),
                ..
            }) if err.is::<T>() => Ok(*err.downcast::<T>().unwrap()),
            Ok(inner) => Err(Self {
                inner: Arc::new(inner),
            }),
            Err(inner) => Err(Self { inner }),
        }
    }

    /// Returns trap code, if it's a Trap
    pub fn to_trap(self) -> Option<TrapCode> {
        if let RuntimeErrorSource::Trap(trap_code) = self.inner.source {
            Some(trap_code)
        } else {
            None
        }
    }

    /// Returns true if the `RuntimeError` is the same as T
    pub fn is<T: Error + 'static>(&self) -> bool {
        match &self.inner.source {
            RuntimeErrorSource::User(err) => err.is::<T>(),
            _ => false,
        }
    }
}

impl fmt::Debug for RuntimeError {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        f.debug_struct("RuntimeError")
            .field("source", &self.inner.source)
            .field("wasm_trace", &self.inner.wasm_trace)
            .field("native_trace", &self.inner.native_trace)
            .finish()
    }
}

impl fmt::Display for RuntimeError {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(f, "RuntimeError: {}", self.message())?;
        let trace = self.trace();
        if trace.is_empty() {
            return Ok(());
        }
        for frame in self.trace().iter() {
            let name = frame.module_name();
            let func_index = frame.func_index();
            writeln!(f)?;
            write!(f, "    at ")?;
            match frame.function_name() {
                Some(name) => match rustc_demangle::try_demangle(name) {
                    Ok(name) => write!(f, "{}", name)?,
                    Err(_) => write!(f, "{}", name)?,
                },
                None => write!(f, "<unnamed>")?,
            }
            write!(
                f,
                " ({}[{}]:0x{:x})",
                name,
                func_index,
                frame.module_offset()
            )?;
        }
        Ok(())
    }
}

impl std::error::Error for RuntimeError {
    fn source(&self) -> Option<&(dyn std::error::Error + 'static)> {
        match &self.inner.source {
            RuntimeErrorSource::User(err) => Some(&**err),
            RuntimeErrorSource::Trap(err) => Some(err),
            _ => None,
        }
    }
}

impl From<Trap> for RuntimeError {
    fn from(trap: Trap) -> Self {
        Self::from_trap(trap)
    }
}

'''
'''--- lib/engine/src/trap/frame_info.rs ---
//! This module is used for having backtraces in the Wasm runtime.
//! Once the Compiler has compiled the ModuleInfo, and we have a set of
//! compiled functions (addresses and function index) and a module,
//! then we can use this to set a backtrace for that module.
//!
//! # Example
//! ```ignore
//! use wasmer_vm::{FRAME_INFO};
//! use wasmer_types::ModuleInfo;
//!
//! let module: ModuleInfo = ...;
//! FRAME_INFO.register(module, compiled_functions);
//! ```
use std::collections::BTreeMap;
use std::sync::{Arc, RwLock};
use wasmer_compiler::{CompiledFunctionFrameInfo, SourceLoc, TrapInformation};
use wasmer_types::entity::{EntityRef, PrimaryMap};
use wasmer_types::{LocalFunctionIndex, ModuleInfo};

lazy_static::lazy_static! {
    /// This is a global cache of backtrace frame information for all active
    ///
    /// This global cache is used during `Trap` creation to symbolicate frames.
    /// This is populated on module compilation, and it is cleared out whenever
    /// all references to a module are dropped.
    pub static ref FRAME_INFO: RwLock<GlobalFrameInfo> = Default::default();
}

#[derive(Default)]
pub struct GlobalFrameInfo {
    /// An internal map that keeps track of backtrace frame information for
    /// each module.
    ///
    /// This map is morally a map of ranges to a map of information for that
    /// module. Each module is expected to reside in a disjoint section of
    /// contiguous memory. No modules can overlap.
    ///
    /// The key of this map is the highest address in the module and the value
    /// is the module's information, which also contains the start address.
    ranges: BTreeMap<usize, ModuleInfoFrameInfo>,
}

/// An RAII structure used to unregister a module's frame information when the
/// module is destroyed.
pub struct GlobalFrameInfoRegistration {
    /// The key that will be removed from the global `ranges` map when this is
    /// dropped.
    key: usize,
}

#[derive(Debug)]
struct ModuleInfoFrameInfo {
    start: usize,
    functions: BTreeMap<usize, FunctionInfo>,
    module: Arc<ModuleInfo>,
    frame_infos: PrimaryMap<LocalFunctionIndex, CompiledFunctionFrameInfo>,
}

impl ModuleInfoFrameInfo {
    fn function_debug_info(&self, local_index: LocalFunctionIndex) -> &CompiledFunctionFrameInfo {
        &self.frame_infos.get(local_index).unwrap()
    }

    /// Gets a function given a pc
    fn function_info(&self, pc: usize) -> Option<&FunctionInfo> {
        let (end, func) = self.functions.range(pc..).next()?;
        if func.start <= pc && pc <= *end {
            return Some(func);
        } else {
            None
        }
    }
}

#[derive(Debug)]
struct FunctionInfo {
    start: usize,
    local_index: LocalFunctionIndex,
}

impl GlobalFrameInfo {
    /// Fetches frame information about a program counter in a backtrace.
    ///
    /// Returns an object if this `pc` is known to some previously registered
    /// module, or returns `None` if no information can be found.
    pub fn lookup_frame_info(&self, pc: usize) -> Option<FrameInfo> {
        let module = self.module_info(pc)?;
        let func = module.function_info(pc)?;

        // Use our relative position from the start of the function to find the
        // machine instruction that corresponds to `pc`, which then allows us to
        // map that to a wasm original source location.
        let rel_pos = pc - func.start;
        let instr_map = &module.function_debug_info(func.local_index).address_map;
        let pos = match instr_map
            .instructions
            .binary_search_by_key(&rel_pos, |map| map.code_offset)
        {
            // Exact hit!
            Ok(pos) => Some(pos),

            // This *would* be at the first slot in the array, so no
            // instructions cover `pc`.
            Err(0) => None,

            // This would be at the `nth` slot, so check `n-1` to see if we're
            // part of that instruction. This happens due to the minus one when
            // this function is called form trap symbolication, where we don't
            // always get called with a `pc` that's an exact instruction
            // boundary.
            Err(n) => {
                let instr = &instr_map.instructions[n - 1];
                if instr.code_offset <= rel_pos && rel_pos < instr.code_offset + instr.code_len {
                    Some(n - 1)
                } else {
                    None
                }
            }
        };

        let instr = match pos {
            Some(pos) => instr_map.instructions[pos].srcloc,
            // Some compilers don't emit yet the full trap information for each of
            // the instructions (such as LLVM).
            // In case no specific instruction is found, we return by default the
            // start offset of the function.
            None => instr_map.start_srcloc,
        };
        let func_index = module.module.func_index(func.local_index);
        Some(FrameInfo {
            module_name: module.module.name(),
            func_index: func_index.index() as u32,
            function_name: module.module.function_names.get(&func_index).cloned(),
            instr,
            func_start: instr_map.start_srcloc,
        })
    }

    /// Fetches trap information about a program counter in a backtrace.
    pub fn lookup_trap_info(&self, pc: usize) -> Option<&TrapInformation> {
        let module = self.module_info(pc)?;
        let func = module.function_info(pc)?;
        let traps = &module.function_debug_info(func.local_index).traps;
        let idx = traps
            .binary_search_by_key(&((pc - func.start) as u32), |info| info.code_offset)
            .ok()?;
        Some(&traps[idx])
    }

    /// Gets a module given a pc
    fn module_info(&self, pc: usize) -> Option<&ModuleInfoFrameInfo> {
        let (end, module_info) = self.ranges.range(pc..).next()?;
        if module_info.start <= pc && pc <= *end {
            Some(module_info)
        } else {
            None
        }
    }
}

impl Drop for GlobalFrameInfoRegistration {
    fn drop(&mut self) {
        if let Ok(mut info) = FRAME_INFO.write() {
            info.ranges.remove(&self.key);
        }
    }
}

/// Description of a frame in a backtrace for a [`RuntimeError::trace`](crate::RuntimeError::trace).
///
/// Whenever a WebAssembly trap occurs an instance of [`RuntimeError`]
/// is created. Each [`RuntimeError`] has a backtrace of the
/// WebAssembly frames that led to the trap, and each frame is
/// described by this structure.
///
/// [`RuntimeError`]: crate::RuntimeError
#[derive(Debug, Clone)]
pub struct FrameInfo {
    module_name: String,
    func_index: u32,
    function_name: Option<String>,
    func_start: SourceLoc,
    instr: SourceLoc,
}

impl FrameInfo {
    /// Returns the WebAssembly function index for this frame.
    ///
    /// This function index is the index in the function index space of the
    /// WebAssembly module that this frame comes from.
    pub fn func_index(&self) -> u32 {
        self.func_index
    }

    /// Returns the identifer of the module that this frame is for.
    ///
    /// ModuleInfo identifiers are present in the `name` section of a WebAssembly
    /// binary, but this may not return the exact item in the `name` section.
    /// ModuleInfo names can be overwritten at construction time or perhaps inferred
    /// from file names. The primary purpose of this function is to assist in
    /// debugging and therefore may be tweaked over time.
    ///
    /// This function returns `None` when no name can be found or inferred.
    pub fn module_name(&self) -> &str {
        &self.module_name
    }

    /// Returns a descriptive name of the function for this frame, if one is
    /// available.
    ///
    /// The name of this function may come from the `name` section of the
    /// WebAssembly binary, or wasmer may try to infer a better name for it if
    /// not available, for example the name of the export if it's exported.
    ///
    /// This return value is primarily used for debugging and human-readable
    /// purposes for things like traps. Note that the exact return value may be
    /// tweaked over time here and isn't guaranteed to be something in
    /// particular about a wasm module due to its primary purpose of assisting
    /// in debugging.
    ///
    /// This function returns `None` when no name could be inferred.
    pub fn function_name(&self) -> Option<&str> {
        self.function_name.as_deref()
    }

    /// Returns the offset within the original wasm module this frame's program
    /// counter was at.
    ///
    /// The offset here is the offset from the beginning of the original wasm
    /// module to the instruction that this frame points to.
    pub fn module_offset(&self) -> usize {
        self.instr.bits() as usize
    }

    /// Returns the offset from the original wasm module's function to this
    /// frame's program counter.
    ///
    /// The offset here is the offset from the beginning of the defining
    /// function of this frame (within the wasm module) to the instruction this
    /// frame points to.
    pub fn func_offset(&self) -> usize {
        (self.instr.bits() - self.func_start.bits()) as usize
    }
}

'''
'''--- lib/engine/src/trap/mod.rs ---
mod error;
mod frame_info;
pub use error::RuntimeError;
pub use frame_info::{FrameInfo, GlobalFrameInfoRegistration};

'''
'''--- lib/types/Cargo.toml ---
[package]
name = "wasmer-types-near"
version = "2.4.1"
description = "Wasmer Common Types"
categories = ["wasm", "no-std", "data-structures"]
keywords = ["wasm", "webassembly", "types"]
authors = ["Wasmer Engineering Team <engineering@wasmer.io>"]
repository = "https://github.com/wasmerio/wasmer"
license = "MIT OR Apache-2.0 WITH LLVM-exception"
readme = "README.md"
edition = "2018"

[lib]
name = "wasmer_types"

[dependencies]
thiserror = "1.0"
indexmap = { version = "1.6" }
rkyv = { version = "0.7.20" }

[features]
default = ["std"]
std = []
core = []

# experimental / in-development features
experimental-reference-types-extern-ref = []

'''
'''--- lib/types/README.md ---
# `wasmer-types` [![Build Status](https://github.com/wasmerio/wasmer/workflows/build/badge.svg?style=flat-square)](https://github.com/wasmerio/wasmer/actions?query=workflow%3Abuild) [![Join Wasmer Slack](https://img.shields.io/static/v1?label=Slack&message=join%20chat&color=brighgreen&style=flat-square)](https://slack.wasmer.io) [![MIT License](https://img.shields.io/github/license/wasmerio/wasmer.svg?style=flat-square)](https://github.com/wasmerio/wasmer/blob/master/LICENSE)

This library provides all the types and traits necessary to use
WebAssembly easily anywhere.

Among other things, it defines the following _types_:

* `units` like `Pages` or `Bytes`
* `types` and `values` like `I32`, `I64`, `F32`, `F64`, `ExternRef`,
  `FuncRef`, `V128`, value conversions, `ExternType`, `FunctionType`
  etc.
* `native` contains a set of trait and implementations to deal with
  WebAssembly types that have a direct representation on the host,
* `memory_view`, an API to read/write memories when bytes are
  interpreted as particular types (`i8`, `i16`, `i32` etc.)
* `indexes` contains all the possible WebAssembly module indexes for
  various types
* `initializers` for tables, data etc.
* `features` to enable or disable some WebAssembly features inside the
  Wasmer runtime

### Acknowledgments

This project borrowed some of the code for the entity structure from [cranelift-entity](https://crates.io/crates/cranelift-entity).
We decided to move it here to help on serialization/deserialization and also to ease the integration with other tools like `loupe`.

Please check [Wasmer ATTRIBUTIONS](https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md) to further see licenses and other attributions of the project. 

'''
'''--- lib/types/src/archives.rs ---
#[cfg(feature = "core")]
use core::hash::Hash;
use indexmap::IndexMap;
use rkyv::Archive;
#[cfg(feature = "std")]
use std::hash::Hash;

#[derive(rkyv::Serialize, rkyv::Deserialize, rkyv::Archive)]
/// See [`IndexMap`]
pub struct ArchivableIndexMap<K: Hash + Ord + Archive, V: Archive> {
    entries: Vec<(K, V)>,
}

impl<K: Hash + Ord + Archive, V: Archive> ArchivedArchivableIndexMap<K, V> {
    pub fn iter(&self) -> core::slice::Iter<'_, (K::Archived, V::Archived)> {
        self.entries.iter()
    }
}

impl<K: Hash + Ord + Archive + Clone, V: Archive> From<IndexMap<K, V>>
    for ArchivableIndexMap<K, V>
{
    fn from(it: IndexMap<K, V>) -> ArchivableIndexMap<K, V> {
        let mut r = ArchivableIndexMap {
            entries: Vec::new(),
        };
        for (k, v) in it.into_iter() {
            r.entries.push((k, v));
        }
        r
    }
}

impl<K: Hash + Ord + Archive + Clone, V: Archive> Into<IndexMap<K, V>>
    for ArchivableIndexMap<K, V>
{
    fn into(self) -> IndexMap<K, V> {
        let mut r = IndexMap::new();
        for (k, v) in self.entries.into_iter() {
            r.insert(k, v);
        }
        r
    }
}

'''
'''--- lib/types/src/entity/boxed_slice.rs ---
// This file contains code from external sources.
// Attributions: https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md

//! Boxed slices for `PrimaryMap`.

use crate::entity::iter::{Iter, IterMut};
use crate::entity::keys::Keys;
use crate::entity::EntityRef;
use crate::lib::std::boxed::Box;
use crate::lib::std::marker::PhantomData;
use crate::lib::std::ops::{Index, IndexMut};
use crate::lib::std::slice;

/// A slice mapping `K -> V` allocating dense entity references.
///
/// The `BoxedSlice` data structure uses the dense index space to implement a map with a boxed
/// slice.
#[derive(Debug, Clone)]
pub struct BoxedSlice<K, V>
where
    K: EntityRef,
{
    elems: Box<[V]>,
    unused: PhantomData<K>,
}

impl<K, V> BoxedSlice<K, V>
where
    K: EntityRef,
{
    /// Create a new slice from a raw pointer. A safer way to create slices is
    /// to use `PrimaryMap::into_boxed_slice()`.
    ///
    /// # Safety
    ///
    /// This relies on `raw` pointing to a valid slice of `V`s.
    pub unsafe fn from_raw(raw: *mut [V]) -> Self {
        Self {
            elems: Box::from_raw(raw),
            unused: PhantomData,
        }
    }

    /// Check if `k` is a valid key in the map.
    pub fn is_valid(&self, k: K) -> bool {
        k.index() < self.elems.len()
    }

    /// Get the element at `k` if it exists.
    pub fn get(&self, k: K) -> Option<&V> {
        self.elems.get(k.index())
    }

    /// Get the element at `k` if it exists, mutable version.
    pub fn get_mut(&mut self, k: K) -> Option<&mut V> {
        self.elems.get_mut(k.index())
    }

    /// Is this map completely empty?
    pub fn is_empty(&self) -> bool {
        self.elems.is_empty()
    }

    /// Get the total number of entity references created.
    pub fn len(&self) -> usize {
        self.elems.len()
    }

    /// Iterate over all the keys in this map.
    pub fn keys(&self) -> Keys<K> {
        Keys::with_len(self.elems.len())
    }

    /// Iterate over all the values in this map.
    pub fn values(&self) -> slice::Iter<V> {
        self.elems.iter()
    }

    /// Iterate over all the values in this map, mutable edition.
    pub fn values_mut(&mut self) -> slice::IterMut<V> {
        self.elems.iter_mut()
    }

    /// Iterate over all the keys and values in this map.
    pub fn iter(&self) -> Iter<K, V> {
        Iter::new(self.elems.iter())
    }

    /// Iterate over all the keys and values in this map, mutable edition.
    pub fn iter_mut(&mut self) -> IterMut<K, V> {
        IterMut::new(self.elems.iter_mut())
    }

    /// Returns the last element that was inserted in the map.
    pub fn last(&self) -> Option<&V> {
        self.elems.last()
    }
}

/// Immutable indexing into a `BoxedSlice`.
/// The indexed value must be in the map.
impl<K, V> Index<K> for BoxedSlice<K, V>
where
    K: EntityRef,
{
    type Output = V;

    fn index(&self, k: K) -> &V {
        &self.elems[k.index()]
    }
}

/// Mutable indexing into a `BoxedSlice`.
impl<K, V> IndexMut<K> for BoxedSlice<K, V>
where
    K: EntityRef,
{
    fn index_mut(&mut self, k: K) -> &mut V {
        &mut self.elems[k.index()]
    }
}

impl<'a, K, V> IntoIterator for &'a BoxedSlice<K, V>
where
    K: EntityRef,
{
    type Item = (K, &'a V);
    type IntoIter = Iter<'a, K, V>;

    fn into_iter(self) -> Self::IntoIter {
        Iter::new(self.elems.iter())
    }
}

impl<'a, K, V> IntoIterator for &'a mut BoxedSlice<K, V>
where
    K: EntityRef,
{
    type Item = (K, &'a mut V);
    type IntoIter = IterMut<'a, K, V>;

    fn into_iter(self) -> Self::IntoIter {
        IterMut::new(self.elems.iter_mut())
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::entity::PrimaryMap;
    use crate::lib::std::vec::Vec;

    // `EntityRef` impl for testing.
    #[derive(Clone, Copy, Debug, PartialEq, Eq)]
    struct E(u32);

    impl EntityRef for E {
        fn new(i: usize) -> Self {
            E(i as u32)
        }
        fn index(self) -> usize {
            self.0 as usize
        }
    }

    #[test]
    fn basic() {
        let r0 = E(0);
        let r1 = E(1);
        let p = PrimaryMap::<E, isize>::new();
        let m = p.into_boxed_slice();

        let v: Vec<E> = m.keys().collect();
        assert_eq!(v, []);

        assert!(!m.is_valid(r0));
        assert!(!m.is_valid(r1));
    }

    #[test]
    fn iter() {
        let mut p: PrimaryMap<E, usize> = PrimaryMap::new();
        p.push(12);
        p.push(33);
        let mut m = p.into_boxed_slice();

        let mut i = 0;
        for (key, value) in &m {
            assert_eq!(key.index(), i);
            match i {
                0 => assert_eq!(*value, 12),
                1 => assert_eq!(*value, 33),
                _ => panic!(),
            }
            i += 1;
        }
        i = 0;
        for (key_mut, value_mut) in m.iter_mut() {
            assert_eq!(key_mut.index(), i);
            match i {
                0 => assert_eq!(*value_mut, 12),
                1 => assert_eq!(*value_mut, 33),
                _ => panic!(),
            }
            i += 1;
        }
    }

    #[test]
    fn iter_rev() {
        let mut p: PrimaryMap<E, usize> = PrimaryMap::new();
        p.push(12);
        p.push(33);
        let mut m = p.into_boxed_slice();

        let mut i = 2;
        for (key, value) in m.iter().rev() {
            i -= 1;
            assert_eq!(key.index(), i);
            match i {
                0 => assert_eq!(*value, 12),
                1 => assert_eq!(*value, 33),
                _ => panic!(),
            }
        }

        i = 2;
        for (key, value) in m.iter_mut().rev() {
            i -= 1;
            assert_eq!(key.index(), i);
            match i {
                0 => assert_eq!(*value, 12),
                1 => assert_eq!(*value, 33),
                _ => panic!(),
            }
        }
    }
    #[test]
    fn keys() {
        let mut p: PrimaryMap<E, usize> = PrimaryMap::new();
        p.push(12);
        p.push(33);
        let m = p.into_boxed_slice();

        let mut i = 0;
        for key in m.keys() {
            assert_eq!(key.index(), i);
            i += 1;
        }
    }

    #[test]
    fn keys_rev() {
        let mut p: PrimaryMap<E, usize> = PrimaryMap::new();
        p.push(12);
        p.push(33);
        let m = p.into_boxed_slice();

        let mut i = 2;
        for key in m.keys().rev() {
            i -= 1;
            assert_eq!(key.index(), i);
        }
    }

    #[test]
    fn values() {
        let mut p: PrimaryMap<E, usize> = PrimaryMap::new();
        p.push(12);
        p.push(33);
        let mut m = p.into_boxed_slice();

        let mut i = 0;
        for value in m.values() {
            match i {
                0 => assert_eq!(*value, 12),
                1 => assert_eq!(*value, 33),
                _ => panic!(),
            }
            i += 1;
        }
        i = 0;
        for value_mut in m.values_mut() {
            match i {
                0 => assert_eq!(*value_mut, 12),
                1 => assert_eq!(*value_mut, 33),
                _ => panic!(),
            }
            i += 1;
        }
    }

    #[test]
    fn values_rev() {
        let mut p: PrimaryMap<E, usize> = PrimaryMap::new();
        p.push(12);
        p.push(33);
        let mut m = p.into_boxed_slice();

        let mut i = 2;
        for value in m.values().rev() {
            i -= 1;
            match i {
                0 => assert_eq!(*value, 12),
                1 => assert_eq!(*value, 33),
                _ => panic!(),
            }
        }
        i = 2;
        for value_mut in m.values_mut().rev() {
            i -= 1;
            match i {
                0 => assert_eq!(*value_mut, 12),
                1 => assert_eq!(*value_mut, 33),
                _ => panic!(),
            }
        }
    }
}

'''
'''--- lib/types/src/entity/iter.rs ---
// This file contains code from external sources.
// Attributions: https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md

//! A double-ended iterator over entity references and entities.

use crate::entity::EntityRef;
use crate::lib::std::iter::Enumerate;
use crate::lib::std::marker::PhantomData;
use crate::lib::std::slice;
use crate::lib::std::vec;

/// Iterate over all keys in order.
pub struct Iter<'a, K: EntityRef, V>
where
    V: 'a,
{
    enumerate: Enumerate<slice::Iter<'a, V>>,
    unused: PhantomData<K>,
}

impl<'a, K: EntityRef, V> Iter<'a, K, V> {
    /// Create an `Iter` iterator that visits the `PrimaryMap` keys and values
    /// of `iter`.
    pub fn new(iter: slice::Iter<'a, V>) -> Self {
        Self {
            enumerate: iter.enumerate(),
            unused: PhantomData,
        }
    }
}

impl<'a, K: EntityRef, V> Iterator for Iter<'a, K, V> {
    type Item = (K, &'a V);

    fn next(&mut self) -> Option<Self::Item> {
        self.enumerate.next().map(|(i, v)| (K::new(i), v))
    }

    fn size_hint(&self) -> (usize, Option<usize>) {
        self.enumerate.size_hint()
    }
}

impl<'a, K: EntityRef, V> DoubleEndedIterator for Iter<'a, K, V> {
    fn next_back(&mut self) -> Option<Self::Item> {
        self.enumerate.next_back().map(|(i, v)| (K::new(i), v))
    }
}

impl<'a, K: EntityRef, V> ExactSizeIterator for Iter<'a, K, V> {}

/// Iterate over all keys in order.
pub struct IterMut<'a, K: EntityRef, V>
where
    V: 'a,
{
    enumerate: Enumerate<slice::IterMut<'a, V>>,
    unused: PhantomData<K>,
}

impl<'a, K: EntityRef, V> IterMut<'a, K, V> {
    /// Create an `IterMut` iterator that visits the `PrimaryMap` keys and values
    /// of `iter`.
    pub fn new(iter: slice::IterMut<'a, V>) -> Self {
        Self {
            enumerate: iter.enumerate(),
            unused: PhantomData,
        }
    }
}

impl<'a, K: EntityRef, V> Iterator for IterMut<'a, K, V> {
    type Item = (K, &'a mut V);

    fn next(&mut self) -> Option<Self::Item> {
        self.enumerate.next().map(|(i, v)| (K::new(i), v))
    }

    fn size_hint(&self) -> (usize, Option<usize>) {
        self.enumerate.size_hint()
    }
}

impl<'a, K: EntityRef, V> DoubleEndedIterator for IterMut<'a, K, V> {
    fn next_back(&mut self) -> Option<Self::Item> {
        self.enumerate.next_back().map(|(i, v)| (K::new(i), v))
    }
}

impl<'a, K: EntityRef, V> ExactSizeIterator for IterMut<'a, K, V> {}

/// Iterate over all keys in order.
pub struct IntoIter<K: EntityRef, V> {
    enumerate: Enumerate<vec::IntoIter<V>>,
    unused: PhantomData<K>,
}

impl<K: EntityRef, V> IntoIter<K, V> {
    /// Create an `IntoIter` iterator that visits the `PrimaryMap` keys and values
    /// of `iter`.
    pub fn new(iter: vec::IntoIter<V>) -> Self {
        Self {
            enumerate: iter.enumerate(),
            unused: PhantomData,
        }
    }
}

impl<K: EntityRef, V> Iterator for IntoIter<K, V> {
    type Item = (K, V);

    fn next(&mut self) -> Option<Self::Item> {
        self.enumerate.next().map(|(i, v)| (K::new(i), v))
    }

    fn size_hint(&self) -> (usize, Option<usize>) {
        self.enumerate.size_hint()
    }
}

impl<K: EntityRef, V> DoubleEndedIterator for IntoIter<K, V> {
    fn next_back(&mut self) -> Option<Self::Item> {
        self.enumerate.next_back().map(|(i, v)| (K::new(i), v))
    }
}

impl<K: EntityRef, V> ExactSizeIterator for IntoIter<K, V> {}

'''
'''--- lib/types/src/entity/keys.rs ---
// This file contains code from external sources.
// Attributions: https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md

//! A double-ended iterator over entity references.
//!
//! When `core::iter::Step` is stabilized, `Keys` could be implemented as a wrapper around
//! `core::ops::Range`, but for now, we implement it manually.

use crate::entity::EntityRef;
use crate::lib::std::marker::PhantomData;

/// Iterate over all keys in order.
pub struct Keys<K: EntityRef> {
    pos: usize,
    rev_pos: usize,
    unused: PhantomData<K>,
}

impl<K: EntityRef> Keys<K> {
    /// Create a `Keys` iterator that visits `len` entities starting from 0.
    pub fn with_len(len: usize) -> Self {
        Self {
            pos: 0,
            rev_pos: len,
            unused: PhantomData,
        }
    }
}

impl<K: EntityRef> Iterator for Keys<K> {
    type Item = K;

    fn next(&mut self) -> Option<Self::Item> {
        if self.pos < self.rev_pos {
            let k = K::new(self.pos);
            self.pos += 1;
            Some(k)
        } else {
            None
        }
    }

    fn size_hint(&self) -> (usize, Option<usize>) {
        let size = self.rev_pos - self.pos;
        (size, Some(size))
    }
}

impl<K: EntityRef> DoubleEndedIterator for Keys<K> {
    fn next_back(&mut self) -> Option<Self::Item> {
        if self.rev_pos > self.pos {
            let k = K::new(self.rev_pos - 1);
            self.rev_pos -= 1;
            Some(k)
        } else {
            None
        }
    }
}

impl<K: EntityRef> ExactSizeIterator for Keys<K> {}

'''
'''--- lib/types/src/entity/mod.rs ---
// This file contains code from external sources.
// Attributions: https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md

/// A type wrapping a small integer index should implement `EntityRef` so it can be used as the key
/// of an `SecondaryMap` or `SparseMap`.
pub trait EntityRef: Copy + Eq {
    /// Create a new entity reference from a small integer.
    /// This should crash if the requested index is not representable.
    fn new(_: usize) -> Self;

    /// Get the index that was used to create this entity reference.
    fn index(self) -> usize;
}

/// Macro which provides the common implementation of a 32-bit entity reference.
#[macro_export]
macro_rules! entity_impl {
    // Basic traits.
    ($entity:ident) => {
        impl $crate::entity::EntityRef for $entity {
            fn new(index: usize) -> Self {
                debug_assert!(index < ($crate::lib::std::u32::MAX as usize));
                $entity(index as u32)
            }

            fn index(self) -> usize {
                self.0 as usize
            }
        }

        impl $crate::entity::packed_option::ReservedValue for $entity {
            fn reserved_value() -> $entity {
                $entity($crate::lib::std::u32::MAX)
            }

            fn is_reserved_value(&self) -> bool {
                self.0 == $crate::lib::std::u32::MAX
            }
        }

        impl $entity {
            /// Create a new instance from a `u32`.
            #[allow(dead_code)]
            pub fn from_u32(x: u32) -> Self {
                debug_assert!(x < $crate::lib::std::u32::MAX);
                $entity(x)
            }

            /// Return the underlying index value as a `u32`.
            #[allow(dead_code)]
            pub fn as_u32(self) -> u32 {
                self.0
            }
        }
    };

    // Include basic `Display` impl using the given display prefix.
    // Display a `Block` reference as "block12".
    ($entity:ident, $display_prefix:expr) => {
        entity_impl!($entity);

        impl $crate::lib::std::fmt::Display for $entity {
            fn fmt(
                &self,
                f: &mut $crate::lib::std::fmt::Formatter,
            ) -> $crate::lib::std::fmt::Result {
                write!(f, concat!($display_prefix, "{}"), self.0)
            }
        }

        impl $crate::lib::std::fmt::Debug for $entity {
            fn fmt(
                &self,
                f: &mut $crate::lib::std::fmt::Formatter,
            ) -> $crate::lib::std::fmt::Result {
                (self as &dyn $crate::lib::std::fmt::Display).fmt(f)
            }
        }
    };
}

pub mod packed_option;

mod boxed_slice;
mod iter;
mod keys;
mod primary_map;
mod secondary_map;

pub use crate::entity_impl;
pub use boxed_slice::BoxedSlice;
pub use iter::{Iter, IterMut};
pub use keys::Keys;
pub use primary_map::PrimaryMap;
pub use secondary_map::SecondaryMap;

'''
'''--- lib/types/src/entity/packed_option.rs ---
// This file contains code from external sources.
// Attributions: https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md

//! Compact representation of `Option<T>` for types with a reserved value.
//!
//! Small types are often used in tables and linked lists where an
//! `Option<T>` is needed. Unfortunately, that would double the size of the tables
//! because `Option<T>` is twice as big as `T`.
//!
//! This module provides a `PackedOption<T>` for types that have a reserved value that can be used
//! to represent `None`.

use crate::lib::std::fmt;
use crate::lib::std::mem;

/// Types that have a reserved value which can't be created any other way.
pub trait ReservedValue {
    /// Create an instance of the reserved value.
    fn reserved_value() -> Self;
    /// Checks whether value is the reserved one.
    fn is_reserved_value(&self) -> bool;
}

/// Packed representation of `Option<T>`.
#[derive(Clone, Copy, PartialEq, PartialOrd, Eq, Ord, Hash)]
pub struct PackedOption<T: ReservedValue>(T);

impl<T: ReservedValue> PackedOption<T> {
    /// Returns `true` if the packed option is a `None` value.
    pub fn is_none(&self) -> bool {
        self.0.is_reserved_value()
    }

    /// Returns `true` if the packed option is a `Some` value.
    pub fn is_some(&self) -> bool {
        !self.0.is_reserved_value()
    }

    /// Expand the packed option into a normal `Option`.
    pub fn expand(self) -> Option<T> {
        if self.is_none() {
            None
        } else {
            Some(self.0)
        }
    }

    /// Maps a `PackedOption<T>` to `Option<U>` by applying a function to a contained value.
    pub fn map<U, F>(self, f: F) -> Option<U>
    where
        F: FnOnce(T) -> U,
    {
        self.expand().map(f)
    }

    /// Unwrap a packed `Some` value or panic.
    pub fn unwrap(self) -> T {
        self.expand().unwrap()
    }

    /// Unwrap a packed `Some` value or panic.
    pub fn expect(self, msg: &str) -> T {
        self.expand().expect(msg)
    }

    /// Takes the value out of the packed option, leaving a `None` in its place.
    pub fn take(&mut self) -> Option<T> {
        mem::replace(self, None.into()).expand()
    }
}

impl<T: ReservedValue> Default for PackedOption<T> {
    /// Create a default packed option representing `None`.
    fn default() -> Self {
        Self(T::reserved_value())
    }
}

impl<T: ReservedValue> From<T> for PackedOption<T> {
    /// Convert `t` into a packed `Some(x)`.
    fn from(t: T) -> Self {
        debug_assert!(
            !t.is_reserved_value(),
            "Can't make a PackedOption from the reserved value."
        );
        Self(t)
    }
}

impl<T: ReservedValue> From<Option<T>> for PackedOption<T> {
    /// Convert an option into its packed equivalent.
    fn from(opt: Option<T>) -> Self {
        match opt {
            None => Self::default(),
            Some(t) => t.into(),
        }
    }
}

impl<T: ReservedValue> Into<Option<T>> for PackedOption<T> {
    fn into(self) -> Option<T> {
        self.expand()
    }
}

impl<T> fmt::Debug for PackedOption<T>
where
    T: ReservedValue + fmt::Debug,
{
    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
        if self.is_none() {
            write!(f, "None")
        } else {
            write!(f, "Some({:?})", self.0)
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    // Dummy entity class, with no Copy or Clone.
    #[derive(Debug, PartialEq, Eq)]
    struct NoC(u32);

    impl ReservedValue for NoC {
        fn reserved_value() -> Self {
            NoC(13)
        }

        fn is_reserved_value(&self) -> bool {
            self.0 == 13
        }
    }

    #[test]
    fn moves() {
        let x = NoC(3);
        let somex: PackedOption<NoC> = x.into();
        assert!(!somex.is_none());
        assert_eq!(somex.expand(), Some(NoC(3)));

        let none: PackedOption<NoC> = None.into();
        assert!(none.is_none());
        assert_eq!(none.expand(), None);
    }

    // Dummy entity class, with Copy.
    #[derive(Clone, Copy, Debug, PartialEq, Eq)]
    struct Ent(u32);

    impl ReservedValue for Ent {
        fn reserved_value() -> Self {
            Ent(13)
        }

        fn is_reserved_value(&self) -> bool {
            self.0 == 13
        }
    }

    #[test]
    fn copies() {
        let x = Ent(2);
        let some: PackedOption<Ent> = x.into();
        let some2: Option<Ent> = x.into();
        assert_eq!(some.expand(), some2);
        assert_eq!(some, x.into());
    }
}

'''
'''--- lib/types/src/entity/primary_map.rs ---
// This file contains code from external sources.
// Attributions: https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md

//! Densely numbered entity references as mapping keys.
use rkyv::Archive;

use crate::entity::boxed_slice::BoxedSlice;
use crate::entity::iter::{IntoIter, Iter, IterMut};
use crate::entity::keys::Keys;
use crate::entity::EntityRef;
use crate::lib::std::boxed::Box;
use crate::lib::std::iter::FromIterator;
use crate::lib::std::marker::PhantomData;
use crate::lib::std::ops::{Index, IndexMut};
use crate::lib::std::slice;
use crate::lib::std::vec::Vec;

/// A primary mapping `K -> V` allocating dense entity references.
///
/// The `PrimaryMap` data structure uses the dense index space to implement a map with a vector.
///
/// A primary map contains the main definition of an entity, and it can be used to allocate new
/// entity references with the `push` method.
///
/// There should only be a single `PrimaryMap` instance for a given `EntityRef` type, otherwise
/// conflicting references will be created. Using unknown keys for indexing will cause a panic.
///
/// Note that `PrimaryMap` doesn't implement `Deref` or `DerefMut`, which would allow
/// `&PrimaryMap<K, V>` to convert to `&[V]`. One of the main advantages of `PrimaryMap` is
/// that it only allows indexing with the distinct `EntityRef` key type, so converting to a
/// plain slice would make it easier to use incorrectly. To make a slice of a `PrimaryMap`, use
/// `into_boxed_slice`.
#[derive(Debug, Clone, Hash, PartialEq, Eq, rkyv::Serialize, rkyv::Deserialize, rkyv::Archive)]
pub struct PrimaryMap<K, V>
where
    K: EntityRef,
{
    pub(crate) elems: Vec<V>,
    pub(crate) unused: PhantomData<K>,
}

impl<K, V> PrimaryMap<K, V>
where
    K: EntityRef,
{
    /// Create a new empty map.
    pub fn new() -> Self {
        Self {
            elems: Vec::new(),
            unused: PhantomData,
        }
    }

    /// Create a new empty map with the given capacity.
    pub fn with_capacity(capacity: usize) -> Self {
        Self {
            elems: Vec::with_capacity(capacity),
            unused: PhantomData,
        }
    }

    /// Check if `k` is a valid key in the map.
    pub fn is_valid(&self, k: K) -> bool {
        k.index() < self.elems.len()
    }

    /// Get the element at `k` if it exists.
    pub fn get(&self, k: K) -> Option<&V> {
        self.elems.get(k.index())
    }

    /// Get the element at `k` if it exists, mutable version.
    pub fn get_mut(&mut self, k: K) -> Option<&mut V> {
        self.elems.get_mut(k.index())
    }

    /// Is this map completely empty?
    pub fn is_empty(&self) -> bool {
        self.elems.is_empty()
    }

    /// Get the total number of entity references created.
    pub fn len(&self) -> usize {
        self.elems.len()
    }

    /// Iterate over all the keys in this map.
    pub fn keys(&self) -> Keys<K> {
        Keys::with_len(self.elems.len())
    }

    /// Iterate over all the values in this map.
    pub fn values(&self) -> slice::Iter<V> {
        self.elems.iter()
    }

    /// Iterate over all the values in this map, mutable edition.
    pub fn values_mut(&mut self) -> slice::IterMut<V> {
        self.elems.iter_mut()
    }

    /// Iterate over all the keys and values in this map.
    pub fn iter(&self) -> Iter<K, V> {
        Iter::new(self.elems.iter())
    }

    /// Iterate over all the keys and values in this map, mutable edition.
    pub fn iter_mut(&mut self) -> IterMut<K, V> {
        IterMut::new(self.elems.iter_mut())
    }

    /// Remove all entries from this map.
    pub fn clear(&mut self) {
        self.elems.clear()
    }

    /// Get the key that will be assigned to the next pushed value.
    pub fn next_key(&self) -> K {
        K::new(self.elems.len())
    }

    /// Append `v` to the mapping, assigning a new key which is returned.
    pub fn push(&mut self, v: V) -> K {
        let k = self.next_key();
        self.elems.push(v);
        k
    }

    /// Returns the last element that was inserted in the map.
    pub fn last(&self) -> Option<&V> {
        self.elems.last()
    }

    /// Reserves capacity for at least `additional` more elements to be inserted.
    pub fn reserve(&mut self, additional: usize) {
        self.elems.reserve(additional)
    }

    /// Reserves the minimum capacity for exactly `additional` more elements to be inserted.
    pub fn reserve_exact(&mut self, additional: usize) {
        self.elems.reserve_exact(additional)
    }

    /// Shrinks the capacity of the `PrimaryMap` as much as possible.
    pub fn shrink_to_fit(&mut self) {
        self.elems.shrink_to_fit()
    }

    /// Consumes this `PrimaryMap` and produces a `BoxedSlice`.
    pub fn into_boxed_slice(self) -> BoxedSlice<K, V> {
        unsafe { BoxedSlice::<K, V>::from_raw(Box::<[V]>::into_raw(self.elems.into_boxed_slice())) }
    }
}

impl<K, V> Default for PrimaryMap<K, V>
where
    K: EntityRef,
{
    fn default() -> PrimaryMap<K, V> {
        PrimaryMap::new()
    }
}

/// Immutable indexing into an `PrimaryMap`.
/// The indexed value must be in the map.
impl<K, V> Index<K> for PrimaryMap<K, V>
where
    K: EntityRef,
{
    type Output = V;

    fn index(&self, k: K) -> &V {
        &self.elems[k.index()]
    }
}

/// Mutable indexing into an `PrimaryMap`.
impl<K, V> IndexMut<K> for PrimaryMap<K, V>
where
    K: EntityRef,
{
    fn index_mut(&mut self, k: K) -> &mut V {
        &mut self.elems[k.index()]
    }
}

impl<K, V> IntoIterator for PrimaryMap<K, V>
where
    K: EntityRef,
{
    type Item = (K, V);
    type IntoIter = IntoIter<K, V>;

    fn into_iter(self) -> Self::IntoIter {
        IntoIter::new(self.elems.into_iter())
    }
}

impl<'a, K, V> IntoIterator for &'a PrimaryMap<K, V>
where
    K: EntityRef,
{
    type Item = (K, &'a V);
    type IntoIter = Iter<'a, K, V>;

    fn into_iter(self) -> Self::IntoIter {
        Iter::new(self.elems.iter())
    }
}

impl<'a, K, V> IntoIterator for &'a mut PrimaryMap<K, V>
where
    K: EntityRef,
{
    type Item = (K, &'a mut V);
    type IntoIter = IterMut<'a, K, V>;

    fn into_iter(self) -> Self::IntoIter {
        IterMut::new(self.elems.iter_mut())
    }
}

impl<K, V> FromIterator<V> for PrimaryMap<K, V>
where
    K: EntityRef,
{
    fn from_iter<T>(iter: T) -> Self
    where
        T: IntoIterator<Item = V>,
    {
        Self {
            elems: Vec::from_iter(iter),
            unused: PhantomData,
        }
    }
}

impl<K, V> ArchivedPrimaryMap<K, V>
where
    K: Archive + EntityRef,
    V: Archive,
{
    /// Get the total number of entity references created.
    pub fn len(&self) -> usize {
        self.elems.len()
    }

    /// Iterate over all the values in this map.
    pub fn values(&self) -> slice::Iter<rkyv::Archived<V>> {
        self.elems.iter()
    }

    /// Iterate over all the keys and values in this map.
    pub fn iter(&self) -> Iter<K, rkyv::Archived<V>> {
        Iter::new(self.elems.iter())
    }
}

/// Immutable indexing into an `PrimaryMap`.
/// The indexed value must be in the map.
impl<K, V> Index<&K::Archived> for ArchivedPrimaryMap<K, V>
where
    K: EntityRef + Archive,
    K::Archived: EntityRef,
    V: Archive,
{
    type Output = <V as rkyv::Archive>::Archived;

    fn index(&self, k: &K::Archived) -> &Self::Output {
        &self.elems[k.index()]
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    // `EntityRef` impl for testing.
    #[derive(Clone, Copy, Debug, PartialEq, Eq)]
    struct E(u32);

    impl EntityRef for E {
        fn new(i: usize) -> Self {
            E(i as u32)
        }
        fn index(self) -> usize {
            self.0 as usize
        }
    }

    #[test]
    fn basic() {
        let r0 = E(0);
        let r1 = E(1);
        let m = PrimaryMap::<E, isize>::new();

        let v: Vec<E> = m.keys().collect();
        assert_eq!(v, []);

        assert!(!m.is_valid(r0));
        assert!(!m.is_valid(r1));
    }

    #[test]
    fn push() {
        let mut m = PrimaryMap::new();
        let k0: E = m.push(12);
        let k1 = m.push(33);

        assert_eq!(m[k0], 12);
        assert_eq!(m[k1], 33);

        let v: Vec<E> = m.keys().collect();
        assert_eq!(v, [k0, k1]);
    }

    #[test]
    fn iter() {
        let mut m: PrimaryMap<E, usize> = PrimaryMap::new();
        m.push(12);
        m.push(33);

        let mut i = 0;
        for (key, value) in &m {
            assert_eq!(key.index(), i);
            match i {
                0 => assert_eq!(*value, 12),
                1 => assert_eq!(*value, 33),
                _ => panic!(),
            }
            i += 1;
        }
        i = 0;
        for (key_mut, value_mut) in m.iter_mut() {
            assert_eq!(key_mut.index(), i);
            match i {
                0 => assert_eq!(*value_mut, 12),
                1 => assert_eq!(*value_mut, 33),
                _ => panic!(),
            }
            i += 1;
        }
    }

    #[test]
    fn iter_rev() {
        let mut m: PrimaryMap<E, usize> = PrimaryMap::new();
        m.push(12);
        m.push(33);

        let mut i = 2;
        for (key, value) in m.iter().rev() {
            i -= 1;
            assert_eq!(key.index(), i);
            match i {
                0 => assert_eq!(*value, 12),
                1 => assert_eq!(*value, 33),
                _ => panic!(),
            }
        }

        i = 2;
        for (key, value) in m.iter_mut().rev() {
            i -= 1;
            assert_eq!(key.index(), i);
            match i {
                0 => assert_eq!(*value, 12),
                1 => assert_eq!(*value, 33),
                _ => panic!(),
            }
        }
    }
    #[test]
    fn keys() {
        let mut m: PrimaryMap<E, usize> = PrimaryMap::new();
        m.push(12);
        m.push(33);

        let mut i = 0;
        for key in m.keys() {
            assert_eq!(key.index(), i);
            i += 1;
        }
    }

    #[test]
    fn keys_rev() {
        let mut m: PrimaryMap<E, usize> = PrimaryMap::new();
        m.push(12);
        m.push(33);

        let mut i = 2;
        for key in m.keys().rev() {
            i -= 1;
            assert_eq!(key.index(), i);
        }
    }

    #[test]
    fn values() {
        let mut m: PrimaryMap<E, usize> = PrimaryMap::new();
        m.push(12);
        m.push(33);

        let mut i = 0;
        for value in m.values() {
            match i {
                0 => assert_eq!(*value, 12),
                1 => assert_eq!(*value, 33),
                _ => panic!(),
            }
            i += 1;
        }
        i = 0;
        for value_mut in m.values_mut() {
            match i {
                0 => assert_eq!(*value_mut, 12),
                1 => assert_eq!(*value_mut, 33),
                _ => panic!(),
            }
            i += 1;
        }
    }

    #[test]
    fn values_rev() {
        let mut m: PrimaryMap<E, usize> = PrimaryMap::new();
        m.push(12);
        m.push(33);

        let mut i = 2;
        for value in m.values().rev() {
            i -= 1;
            match i {
                0 => assert_eq!(*value, 12),
                1 => assert_eq!(*value, 33),
                _ => panic!(),
            }
        }
        i = 2;
        for value_mut in m.values_mut().rev() {
            i -= 1;
            match i {
                0 => assert_eq!(*value_mut, 12),
                1 => assert_eq!(*value_mut, 33),
                _ => panic!(),
            }
        }
    }

    #[test]
    fn from_iter() {
        let mut m: PrimaryMap<E, usize> = PrimaryMap::new();
        m.push(12);
        m.push(33);

        let n = m.values().collect::<PrimaryMap<E, _>>();
        assert!(m.len() == n.len());
        for (me, ne) in m.values().zip(n.values()) {
            assert!(*me == **ne);
        }
    }
}

'''
'''--- lib/types/src/entity/secondary_map.rs ---
// This file contains code from external sources.
// Attributions: https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md

//! Densely numbered entity references as mapping keys.

use crate::entity::iter::{Iter, IterMut};
use crate::entity::keys::Keys;
use crate::entity::EntityRef;
use crate::lib::std::cmp::min;
use crate::lib::std::marker::PhantomData;
use crate::lib::std::ops::{Index, IndexMut};
use crate::lib::std::slice;
use crate::lib::std::vec::Vec;
use rkyv::Archive;

/// A mapping `K -> V` for densely indexed entity references.
///
/// The `SecondaryMap` data structure uses the dense index space to implement a map with a vector.
/// Unlike `PrimaryMap`, an `SecondaryMap` can't be used to allocate entity references. It is used
/// to associate secondary information with entities.
///
/// The map does not track if an entry for a key has been inserted or not. Instead it behaves as if
/// all keys have a default entry from the beginning.
#[derive(Debug, Clone, rkyv::Serialize, rkyv::Deserialize, rkyv::Archive)]
pub struct SecondaryMap<K, V>
where
    K: EntityRef,
    V: Clone,
{
    pub(crate) elems: Vec<V>,
    pub(crate) default: V,
    pub(crate) unused: PhantomData<K>,
}

/// Shared `SecondaryMap` implementation for all value types.
impl<K, V> SecondaryMap<K, V>
where
    K: EntityRef,
    V: Clone,
{
    /// Create a new empty map.
    pub fn new() -> Self
    where
        V: Default,
    {
        Self {
            elems: Vec::new(),
            default: Default::default(),
            unused: PhantomData,
        }
    }

    /// Create a new, empty map with the specified capacity.
    ///
    /// The map will be able to hold exactly `capacity` elements without reallocating.
    pub fn with_capacity(capacity: usize) -> Self
    where
        V: Default,
    {
        Self {
            elems: Vec::with_capacity(capacity),
            default: Default::default(),
            unused: PhantomData,
        }
    }

    /// Create a new empty map with a specified default value.
    ///
    /// This constructor does not require V to implement Default.
    pub fn with_default(default: V) -> Self {
        Self {
            elems: Vec::new(),
            default,
            unused: PhantomData,
        }
    }

    /// Returns the number of elements the map can hold without reallocating.
    pub fn capacity(&self) -> usize {
        self.elems.capacity()
    }

    /// Get the element at `k` if it exists.
    #[inline(always)]
    pub fn get(&self, k: K) -> Option<&V> {
        self.elems.get(k.index())
    }

    /// Is this map completely empty?
    #[inline(always)]
    pub fn is_empty(&self) -> bool {
        self.elems.is_empty()
    }

    /// Remove all entries from this map.
    #[inline(always)]
    pub fn clear(&mut self) {
        self.elems.clear()
    }

    /// Iterate over all the keys and values in this map.
    pub fn iter(&self) -> Iter<K, V> {
        Iter::new(self.elems.iter())
    }

    /// Iterate over all the keys and values in this map, mutable edition.
    pub fn iter_mut(&mut self) -> IterMut<K, V> {
        IterMut::new(self.elems.iter_mut())
    }

    /// Iterate over all the keys in this map.
    pub fn keys(&self) -> Keys<K> {
        Keys::with_len(self.elems.len())
    }

    /// Iterate over all the values in this map.
    pub fn values(&self) -> slice::Iter<V> {
        self.elems.iter()
    }

    /// Iterate over all the values in this map, mutable edition.
    pub fn values_mut(&mut self) -> slice::IterMut<V> {
        self.elems.iter_mut()
    }

    /// Resize the map to have `n` entries by adding default entries as needed.
    pub fn resize(&mut self, n: usize) {
        self.elems.resize(n, self.default.clone());
    }
}

impl<K, V> Default for SecondaryMap<K, V>
where
    K: EntityRef,
    V: Clone + Default,
{
    fn default() -> SecondaryMap<K, V> {
        SecondaryMap::new()
    }
}

/// Immutable indexing into an `SecondaryMap`.
///
/// All keys are permitted. Untouched entries have the default value.
impl<K, V> Index<K> for SecondaryMap<K, V>
where
    K: EntityRef,
    V: Clone,
{
    type Output = V;

    #[inline(always)]
    fn index(&self, k: K) -> &V {
        self.elems.get(k.index()).unwrap_or(&self.default)
    }
}

/// Immutable indexing into an `SecondaryMap`.
///
/// All keys are permitted. Untouched entries have the default value.
impl<K, V> Index<&K::Archived> for ArchivedSecondaryMap<K, V>
where
    K: EntityRef + Archive,
    K::Archived: EntityRef,
    V: Archive + Clone,
{
    type Output = <V as rkyv::Archive>::Archived;

    fn index(&self, k: &K::Archived) -> &Self::Output {
        &self.elems.get(k.index()).unwrap_or(&self.default)
    }
}

/// Mutable indexing into an `SecondaryMap`.
///
/// The map grows as needed to accommodate new keys.
impl<K, V> IndexMut<K> for SecondaryMap<K, V>
where
    K: EntityRef,
    V: Clone,
{
    #[inline(always)]
    fn index_mut(&mut self, k: K) -> &mut V {
        let i = k.index();
        if i >= self.elems.len() {
            self.elems.resize(i + 1, self.default.clone());
        }
        &mut self.elems[i]
    }
}

impl<K, V> PartialEq for SecondaryMap<K, V>
where
    K: EntityRef,
    V: Clone + PartialEq,
{
    fn eq(&self, other: &Self) -> bool {
        let min_size = min(self.elems.len(), other.elems.len());
        self.default == other.default
            && self.elems[..min_size] == other.elems[..min_size]
            && self.elems[min_size..].iter().all(|e| *e == self.default)
            && other.elems[min_size..].iter().all(|e| *e == other.default)
    }
}

impl<K, V> Eq for SecondaryMap<K, V>
where
    K: EntityRef,
    V: Clone + PartialEq + Eq,
{
}

#[cfg(test)]
mod tests {
    use super::*;

    // `EntityRef` impl for testing.
    #[derive(Clone, Copy, Debug, PartialEq, Eq)]
    struct E(u32);

    impl EntityRef for E {
        fn new(i: usize) -> Self {
            E(i as u32)
        }
        fn index(self) -> usize {
            self.0 as usize
        }
    }

    #[test]
    fn basic() {
        let r0 = E(0);
        let r1 = E(1);
        let r2 = E(2);
        let mut m = SecondaryMap::new();

        let v: Vec<E> = m.keys().collect();
        assert_eq!(v, []);

        m[r2] = 3;
        m[r1] = 5;

        assert_eq!(m[r1], 5);
        assert_eq!(m[r2], 3);

        let v: Vec<E> = m.keys().collect();
        assert_eq!(v, [r0, r1, r2]);

        let shared = &m;
        assert_eq!(shared[r0], 0);
        assert_eq!(shared[r1], 5);
        assert_eq!(shared[r2], 3);
    }
}

'''
'''--- lib/types/src/extern_ref.rs ---
use std::any::Any;
use std::ptr;
use std::sync::atomic;

/// This type does not do reference counting automatically, reference counting can be done with
/// [`Self::ref_clone`] and [`Self::ref_drop`].
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
#[repr(transparent)]
pub struct VMExternRef(*const VMExternRefInner);

impl VMExternRef {
    /// The maximum number of references allowed to this data.
    const MAX_REFCOUNT: usize = std::usize::MAX - 1;

    /// Checks if the given ExternRef is null.
    pub fn is_null(&self) -> bool {
        self.0.is_null()
    }

    /// New null extern ref
    pub const fn null() -> Self {
        Self(ptr::null())
    }

    /// Get a bit-level representation of an externref.
    /// For internal use for packing / unpacking it for calling functions.
    pub(crate) fn to_binary(self) -> i128 {
        self.0 as i128
    }

    /// Create an externref from bit-level representation.
    /// For internal use for packing / unpacking it for calling functions.
    ///
    /// # Safety
    /// The pointer is assumed valid or null. Passing arbitrary data to this function will
    /// result in undefined behavior. It is the caller's responsibility to verify that only
    /// valid externref bit patterns are passed to this function.
    pub(crate) unsafe fn from_binary(bits: i128) -> Self {
        Self(bits as usize as *const _)
    }

    /// Make a new extern reference
    pub fn new<T>(value: T) -> Self
    where
        T: Any + Send + Sync + 'static + Sized,
    {
        Self(Box::into_raw(Box::new(VMExternRefInner::new::<T>(value))))
    }

    /// Try to downcast to the given value
    pub fn downcast<T>(&self) -> Option<&T>
    where
        T: Any + Send + Sync + 'static + Sized,
    {
        if self.is_null() {
            return None;
        }
        unsafe {
            let inner = &*self.0;

            inner.data.downcast_ref::<T>()
        }
    }

    /// Panic if the ref count gets too high.
    #[track_caller]
    fn sanity_check_ref_count(old_size: usize, growth_amount: usize) {
        // If we exceed 18_446_744_073_709_551_614 references on a 64bit system (or
        // 2_147_483_646 references on a 32bit system) then we either live in a future with
        // magic technology or we have a bug in our ref counting logic (i.e. a leak).
        // Either way, the best course of action is to terminate the program and update
        // some code on our side.
        //
        // Note to future readers: exceeding `usize` ref count is trivially provable as a
        // bug on systems that can address `usize` sized memory blocks or smaller because
        // the reference itself is at least `usize` in size and all virtual memory would be
        // taken by references to the data leaving no room for the data itself.
        if old_size
            .checked_add(growth_amount)
            .map(|v| v > Self::MAX_REFCOUNT)
            .unwrap_or(true)
        {
            panic!("Too many references to `ExternRef`");
        }
    }

    /// A low-level function to increment the strong-count a given number of times.
    ///
    /// This is used as an optimization when implementing some low-level VM primitives.
    /// If you're using this type directly for whatever reason, you probably want
    /// [`Self::ref_clone`] instead.
    pub fn ref_inc_by(&self, val: usize) {
        if self.0.is_null() {
            return;
        }

        let old_size = unsafe {
            let ref_inner = &*self.0;
            ref_inner.increment_ref_count(val)
        };

        Self::sanity_check_ref_count(old_size, val);
    }

    /// A deep copy of the reference, increments the strong count.
    pub fn ref_clone(&self) -> Self {
        if self.0.is_null() {
            return Self(self.0);
        }

        let old_size = unsafe {
            let ref_inner = &*self.0;
            ref_inner.increment_ref_count(1)
        };

        // See comments in [`Self::sanity_check_ref_count`] for more information.
        if old_size > Self::MAX_REFCOUNT {
            panic!("Too many references to `ExternRef`");
        }

        Self(self.0)
    }

    /// Does an inner drop, decrementing the strong count
    pub fn ref_drop(&mut self) {
        if !self.0.is_null() {
            unsafe {
                let should_drop = {
                    let ref_inner: &VMExternRefInner = &*self.0;
                    ref_inner.decrement_and_drop()
                };
                if should_drop {
                    let _ = Box::from_raw(self.0 as *mut VMExternRefInner);
                }
            }
        }
    }

    #[allow(dead_code)]
    /// Get the number of strong references to this data.
    fn strong_count(&self) -> usize {
        if self.0.is_null() {
            0
        } else {
            unsafe { (&*self.0).strong.load(atomic::Ordering::SeqCst) }
        }
    }
}

#[derive(Debug)]
#[repr(C)]
pub(crate) struct VMExternRefInner {
    strong: atomic::AtomicUsize,
    /// Do something obviously correct to get started. This can "easily" be improved
    /// to be an inline allocation later as the logic is fully encapsulated.
    data: Box<dyn Any + Send + Sync + 'static>,
}

impl VMExternRefInner {
    fn new<T>(value: T) -> Self
    where
        T: Any + Send + Sync + Sized + 'static,
    {
        Self {
            strong: atomic::AtomicUsize::new(1),
            data: Box::new(value),
        }
    }

    /// Increments the reference count.
    /// Returns the old value.
    fn increment_ref_count(&self, val: usize) -> usize {
        // Using a relaxed ordering is alright here, as knowledge of
        // the original reference prevents other threads from
        // erroneously deleting the object.
        //
        // As explained in the [Boost documentation][1]:
        //
        // > Increasing the reference counter can always be done with
        // > `memory_order_relaxed`: New references to an object can
        // > only be formed from an existing reference, and passing an
        // > existing reference from one thread to another must already
        // > provide any required synchronization.
        //
        // [1]: https://www.boost.org/doc/libs/1_55_0/doc/html/atomic/usage_examples.html
        self.strong.fetch_add(val, atomic::Ordering::Relaxed)
    }

    /// Decrement the count and drop the data if the count hits 0
    /// returns `true` if the containing allocation should be dropped
    fn decrement_and_drop(&self) -> bool {
        // Because `fetch_sub` is already atomic, we do not need to
        // synchronize with other thread.
        if self.strong.fetch_sub(1, atomic::Ordering::Release) != 1 {
            return false;
        }

        // This fence is needed to prevent reordering of use of the data and
        // deletion of the data. Because it is marked `Release`, the decreasing
        // of the reference count synchronizes with this `Acquire` fence. This
        // means that use of the data happens before decreasing the reference
        // count, which happens before this fence, which happens before the
        // deletion of the data.
        //
        // As explained in the [Boost documentation][1]:
        //
        // > It is important to enforce any possible access to the object in one
        // > thread (through an existing reference) to *happen before* deleting
        // > the object in a different thread. This is achieved by a "release"
        // > operation after dropping a reference (any access to the object
        // > through this reference must obviously happened before), and an
        // > "acquire" operation before deleting the object.
        //
        // [1]: https://www.boost.org/doc/libs/1_55_0/doc/html/atomic/usage_examples.html
        atomic::fence(atomic::Ordering::Acquire);

        return true;
    }
}

#[derive(Debug, PartialEq, Eq)]
#[repr(transparent)]
/// An opaque reference to some data. This reference can be passed through Wasm.
pub struct ExternRef {
    inner: VMExternRef,
}

impl Clone for ExternRef {
    fn clone(&self) -> Self {
        Self {
            inner: self.inner.ref_clone(),
        }
    }
}

impl Drop for ExternRef {
    fn drop(&mut self) {
        self.inner.ref_drop()
    }
}

impl ExternRef {
    /// Checks if the given ExternRef is null.
    pub fn is_null(&self) -> bool {
        self.inner.is_null()
    }

    /// New null extern ref
    pub fn null() -> Self {
        Self {
            inner: VMExternRef::null(),
        }
    }

    #[cfg(feature = "experimental-reference-types-extern-ref")]
    /// Make a new extern reference
    pub fn new<T>(value: T) -> Self
    where
        T: Any + Send + Sync + 'static + Sized,
    {
        Self {
            inner: VMExternRef::new(value),
        }
    }

    #[cfg(feature = "experimental-reference-types-extern-ref")]
    /// Try to downcast to the given value
    pub fn downcast<T>(&self) -> Option<&T>
    where
        T: Any + Send + Sync + 'static + Sized,
    {
        self.inner.downcast::<T>()
    }

    #[cfg(feature = "experimental-reference-types-extern-ref")]
    /// Get the number of strong references to this data.
    pub fn strong_count(&self) -> usize {
        self.inner.strong_count()
    }
}

impl From<VMExternRef> for ExternRef {
    fn from(other: VMExternRef) -> Self {
        Self { inner: other }
    }
}

impl From<ExternRef> for VMExternRef {
    fn from(other: ExternRef) -> Self {
        let out = other.inner;
        // We want to make this transformation without decrementing the count.
        std::mem::forget(other);
        out
    }
}

'''
'''--- lib/types/src/features.rs ---
/// Controls which experimental features will be enabled.
/// Features usually have a corresponding [WebAssembly proposal].
///
/// [WebAssembly proposal]: https://github.com/WebAssembly/proposals
#[derive(Clone, Debug, Eq, PartialEq, rkyv::Serialize, rkyv::Deserialize, rkyv::Archive)]
pub struct Features {
    /// Threads proposal should be enabled
    pub threads: bool,
    /// Reference Types proposal should be enabled
    pub reference_types: bool,
    /// SIMD proposal should be enabled
    pub simd: bool,
    /// Bulk Memory proposal should be enabled
    pub bulk_memory: bool,
    /// Multi Value proposal should be enabled
    pub multi_value: bool,
    /// Tail call proposal should be enabled
    pub tail_call: bool,
    /// Module Linking proposal should be enabled
    pub module_linking: bool,
    /// Multi Memory proposal should be enabled
    pub multi_memory: bool,
    /// 64-bit Memory proposal should be enabled
    pub memory64: bool,
    /// Wasm exceptions proposal should be enabled
    pub exceptions: bool,
}

impl Features {
    /// Create a new feature
    pub fn new() -> Self {
        Self {
            threads: false,
            // Reference types should be on by default
            reference_types: true,
            // SIMD should be on by default
            simd: true,
            // Bulk Memory should be on by default
            bulk_memory: true,
            // Multivalue should be on by default
            multi_value: true,
            tail_call: false,
            module_linking: false,
            multi_memory: false,
            memory64: false,
            exceptions: false,
        }
    }

    /// Configures whether the WebAssembly threads proposal will be enabled.
    ///
    /// The [WebAssembly threads proposal][threads] is not currently fully
    /// standardized and is undergoing development. Support for this feature can
    /// be enabled through this method for appropriate WebAssembly modules.
    ///
    /// This feature gates items such as shared memories and atomic
    /// instructions.
    ///
    /// This is `false` by default.
    ///
    /// [threads]: https://github.com/webassembly/threads
    pub fn threads(&mut self, enable: bool) -> &mut Self {
        self.threads = enable;
        self
    }

    /// Configures whether the WebAssembly reference types proposal will be
    /// enabled.
    ///
    /// The [WebAssembly reference types proposal][proposal] is now
    /// fully standardized and enabled by default.
    ///
    /// This feature gates items such as the `externref` type and multiple tables
    /// being in a module. Note that enabling the reference types feature will
    /// also enable the bulk memory feature.
    ///
    /// This is `true` by default.
    ///
    /// [proposal]: https://github.com/webassembly/reference-types
    pub fn reference_types(&mut self, enable: bool) -> &mut Self {
        self.reference_types = enable;
        // The reference types proposal depends on the bulk memory proposal
        if enable {
            self.bulk_memory(true);
        }
        self
    }

    /// Configures whether the WebAssembly SIMD proposal will be
    /// enabled.
    ///
    /// The [WebAssembly SIMD proposal][proposal] is not currently
    /// fully standardized and is undergoing development. Support for this
    /// feature can be enabled through this method for appropriate WebAssembly
    /// modules.
    ///
    /// This feature gates items such as the `v128` type and all of its
    /// operators being in a module.
    ///
    /// This is `false` by default.
    ///
    /// [proposal]: https://github.com/webassembly/simd
    pub fn simd(&mut self, enable: bool) -> &mut Self {
        self.simd = enable;
        self
    }

    /// Configures whether the WebAssembly bulk memory operations proposal will
    /// be enabled.
    ///
    /// The [WebAssembly bulk memory operations proposal][proposal] is now
    /// fully standardized and enabled by default.
    ///
    /// This feature gates items such as the `memory.copy` instruction, passive
    /// data/table segments, etc, being in a module.
    ///
    /// This is `true` by default.
    ///
    /// [proposal]: https://github.com/webassembly/bulk-memory-operations
    pub fn bulk_memory(&mut self, enable: bool) -> &mut Self {
        self.bulk_memory = enable;
        // In case is false, we disable both threads and reference types
        // since they both depend on bulk memory
        if !enable {
            self.reference_types(false);
        }
        self
    }

    /// Configures whether the WebAssembly multi-value proposal will
    /// be enabled.
    ///
    /// The [WebAssembly multi-value proposal][proposal] is now fully
    /// standardized and enabled by default, except with the singlepass
    /// compiler which does not support it.
    ///
    /// This feature gates functions and blocks returning multiple values in a
    /// module, for example.
    ///
    /// This is `true` by default.
    ///
    /// [proposal]: https://github.com/webassembly/multi-value
    pub fn multi_value(&mut self, enable: bool) -> &mut Self {
        self.multi_value = enable;
        self
    }

    /// Configures whether the WebAssembly tail-call proposal will
    /// be enabled.
    ///
    /// The [WebAssembly tail-call proposal][proposal] is not
    /// currently fully standardized and is undergoing development.
    /// Support for this feature can be enabled through this method for
    /// appropriate WebAssembly modules.
    ///
    /// This feature gates tail-call functions in WebAssembly.
    ///
    /// This is `false` by default.
    ///
    /// [proposal]: https://github.com/webassembly/tail-call
    pub fn tail_call(&mut self, enable: bool) -> &mut Self {
        self.tail_call = enable;
        self
    }

    /// Configures whether the WebAssembly module linking proposal will
    /// be enabled.
    ///
    /// The [WebAssembly module linking proposal][proposal] is not
    /// currently fully standardized and is undergoing development.
    /// Support for this feature can be enabled through this method for
    /// appropriate WebAssembly modules.
    ///
    /// This feature allows WebAssembly modules to define, import and
    /// export modules and instances.
    ///
    /// This is `false` by default.
    ///
    /// [proposal]: https://github.com/webassembly/module-linking
    pub fn module_linking(&mut self, enable: bool) -> &mut Self {
        self.module_linking = enable;
        self
    }

    /// Configures whether the WebAssembly multi-memory proposal will
    /// be enabled.
    ///
    /// The [WebAssembly multi-memory proposal][proposal] is not
    /// currently fully standardized and is undergoing development.
    /// Support for this feature can be enabled through this method for
    /// appropriate WebAssembly modules.
    ///
    /// This feature adds the ability to use multiple memories within a
    /// single Wasm module.
    ///
    /// This is `false` by default.
    ///
    /// [proposal]: https://github.com/WebAssembly/multi-memory
    pub fn multi_memory(&mut self, enable: bool) -> &mut Self {
        self.multi_memory = enable;
        self
    }

    /// Configures whether the WebAssembly 64-bit memory proposal will
    /// be enabled.
    ///
    /// The [WebAssembly 64-bit memory proposal][proposal] is not
    /// currently fully standardized and is undergoing development.
    /// Support for this feature can be enabled through this method for
    /// appropriate WebAssembly modules.
    ///
    /// This feature gates support for linear memory of sizes larger than
    /// 2^32 bits.
    ///
    /// This is `false` by default.
    ///
    /// [proposal]: https://github.com/WebAssembly/memory64
    pub fn memory64(&mut self, enable: bool) -> &mut Self {
        self.memory64 = enable;
        self
    }
}

impl Default for Features {
    fn default() -> Self {
        Self::new()
    }
}

#[cfg(test)]
mod test_features {
    use super::*;
    #[test]
    fn default_features() {
        let default = Features::default();
        assert_eq!(
            default,
            Features {
                threads: false,
                reference_types: true,
                simd: true,
                bulk_memory: true,
                multi_value: true,
                tail_call: false,
                module_linking: false,
                multi_memory: false,
                memory64: false,
                exceptions: false,
            }
        );
    }

    #[test]
    fn enable_threads() {
        let mut features = Features::new();
        features.bulk_memory(false).threads(true);

        assert!(features.threads);
    }

    #[test]
    fn enable_reference_types() {
        let mut features = Features::new();
        features.bulk_memory(false).reference_types(true);
        assert!(features.reference_types);
        assert!(features.bulk_memory);
    }

    #[test]
    fn enable_simd() {
        let mut features = Features::new();
        features.simd(true);
        assert!(features.simd);
    }

    #[test]
    fn enable_multi_value() {
        let mut features = Features::new();
        features.multi_value(true);
        assert!(features.multi_value);
    }

    #[test]
    fn enable_bulk_memory() {
        let mut features = Features::new();
        features.bulk_memory(true);
        assert!(features.bulk_memory);
    }

    #[test]
    fn disable_bulk_memory() {
        let mut features = Features::new();
        features
            .threads(true)
            .reference_types(true)
            .bulk_memory(false);
        assert!(!features.bulk_memory);
        assert!(!features.reference_types);
    }

    #[test]
    fn enable_tail_call() {
        let mut features = Features::new();
        features.tail_call(true);
        assert!(features.tail_call);
    }

    #[test]
    fn enable_module_linking() {
        let mut features = Features::new();
        features.module_linking(true);
        assert!(features.module_linking);
    }

    #[test]
    fn enable_multi_memory() {
        let mut features = Features::new();
        features.multi_memory(true);
        assert!(features.multi_memory);
    }

    #[test]
    fn enable_memory64() {
        let mut features = Features::new();
        features.memory64(true);
        assert!(features.memory64);
    }
}

'''
'''--- lib/types/src/indexes.rs ---
//! Helper functions and structures for the translation.
use crate::entity::entity_impl;
use core::u32;

/// Index type of a function defined locally inside the WebAssembly module.
#[derive(
    Copy,
    Clone,
    PartialEq,
    Eq,
    Hash,
    PartialOrd,
    Ord,
    Debug,
    rkyv::Serialize,
    rkyv::Deserialize,
    rkyv::Archive,
)]
#[archive(as = "Self")]
#[repr(transparent)]
pub struct LocalFunctionIndex(u32);
entity_impl!(LocalFunctionIndex);

/// Index type of a table defined locally inside the WebAssembly module.
#[derive(Copy, Clone, PartialEq, Eq, Hash, PartialOrd, Ord, Debug)]
pub struct LocalTableIndex(u32);
entity_impl!(LocalTableIndex);

/// Index type of a memory defined locally inside the WebAssembly module.
#[derive(Copy, Clone, PartialEq, Eq, Hash, PartialOrd, Ord, Debug)]
pub struct LocalMemoryIndex(u32);
entity_impl!(LocalMemoryIndex);

/// Index type of a global defined locally inside the WebAssembly module.
#[derive(
    Copy,
    Clone,
    PartialEq,
    Eq,
    Hash,
    PartialOrd,
    Ord,
    Debug,
    rkyv::Serialize,
    rkyv::Deserialize,
    rkyv::Archive,
)]
#[archive(as = "Self")]
#[repr(transparent)]
pub struct LocalGlobalIndex(u32);
entity_impl!(LocalGlobalIndex);

/// Index type of a function (imported or local) inside the WebAssembly module.
#[derive(
    Copy,
    Clone,
    PartialEq,
    Eq,
    Hash,
    PartialOrd,
    Ord,
    Debug,
    rkyv::Serialize,
    rkyv::Deserialize,
    rkyv::Archive,
)]
#[archive(as = "Self")]
#[repr(transparent)]
pub struct FunctionIndex(u32);
entity_impl!(FunctionIndex);

/// Index type of a table (imported or local) inside the WebAssembly module.
#[derive(
    Copy,
    Clone,
    PartialEq,
    Eq,
    Hash,
    PartialOrd,
    Ord,
    Debug,
    rkyv::Serialize,
    rkyv::Deserialize,
    rkyv::Archive,
)]
#[archive(as = "Self")]
#[repr(transparent)]
pub struct TableIndex(u32);
entity_impl!(TableIndex);

/// Index type of a global variable (imported or local) inside the WebAssembly module.
#[derive(
    Copy,
    Clone,
    PartialEq,
    Eq,
    Hash,
    PartialOrd,
    Ord,
    Debug,
    rkyv::Serialize,
    rkyv::Deserialize,
    rkyv::Archive,
)]
#[archive(as = "Self")]
#[repr(transparent)]
pub struct GlobalIndex(u32);
entity_impl!(GlobalIndex);

/// Index type of a linear memory (imported or local) inside the WebAssembly module.
#[derive(
    Copy,
    Clone,
    PartialEq,
    Eq,
    Hash,
    PartialOrd,
    Ord,
    Debug,
    rkyv::Serialize,
    rkyv::Deserialize,
    rkyv::Archive,
)]
#[archive(as = "Self")]
#[repr(transparent)]
pub struct MemoryIndex(u32);
entity_impl!(MemoryIndex);

/// Index type of a signature (imported or local) inside the WebAssembly module.
#[derive(
    Copy,
    Clone,
    PartialEq,
    Eq,
    Hash,
    PartialOrd,
    Ord,
    Debug,
    rkyv::Serialize,
    rkyv::Deserialize,
    rkyv::Archive,
)]
#[archive(as = "Self")]
#[repr(transparent)]
pub struct SignatureIndex(u32);
entity_impl!(SignatureIndex);

/// Index type of a passive data segment inside the WebAssembly module.
#[derive(
    Copy,
    Clone,
    PartialEq,
    Eq,
    Hash,
    PartialOrd,
    Ord,
    Debug,
    rkyv::Serialize,
    rkyv::Deserialize,
    rkyv::Archive,
)]
#[archive(as = "Self")]
#[repr(transparent)]
pub struct DataIndex(u32);
entity_impl!(DataIndex);

/// Index type of a passive element segment inside the WebAssembly module.
#[derive(
    Copy,
    Clone,
    PartialEq,
    Eq,
    Hash,
    PartialOrd,
    Ord,
    Debug,
    rkyv::Serialize,
    rkyv::Deserialize,
    rkyv::Archive,
)]
#[archive(as = "Self")]
#[repr(transparent)]
pub struct ElemIndex(u32);
entity_impl!(ElemIndex);

/// Index type of a custom section inside a WebAssembly module.
#[derive(
    Copy,
    Clone,
    PartialEq,
    Eq,
    Hash,
    PartialOrd,
    Ord,
    Debug,
    rkyv::Serialize,
    rkyv::Deserialize,
    rkyv::Archive,
)]
#[archive(as = "Self")]
#[repr(transparent)]
pub struct CustomSectionIndex(u32);
entity_impl!(CustomSectionIndex);

/// An entity to export.
#[derive(
    Clone,
    Debug,
    Hash,
    PartialEq,
    Eq,
    PartialOrd,
    Ord,
    rkyv::Serialize,
    rkyv::Deserialize,
    rkyv::Archive,
)]
#[archive(as = "Self")]
#[repr(u8)]
pub enum ExportIndex {
    /// Function export.
    Function(FunctionIndex),
    /// Table export.
    Table(TableIndex),
    /// Memory export.
    Memory(MemoryIndex),
    /// Global export.
    Global(GlobalIndex),
}

/// An entity to import.
#[derive(
    Clone,
    Debug,
    Hash,
    PartialEq,
    Eq,
    PartialOrd,
    Ord,
    rkyv::Serialize,
    rkyv::Deserialize,
    rkyv::Archive,
)]
#[archive(as = "Self")]
#[repr(u8)]
pub enum ImportIndex {
    /// Function import.
    Function(FunctionIndex),
    /// Table import.
    Table(TableIndex),
    /// Memory import.
    Memory(MemoryIndex),
    /// Global import.
    Global(GlobalIndex),
}

'''
'''--- lib/types/src/initializers.rs ---
use crate::indexes::{FunctionIndex, GlobalIndex, MemoryIndex, TableIndex};
use crate::lib::std::boxed::Box;

/// A WebAssembly table initializer.
#[derive(Clone, Debug, Hash, PartialEq, Eq, rkyv::Serialize, rkyv::Deserialize, rkyv::Archive)]
pub struct OwnedTableInitializer {
    /// The index of a table to initialize.
    pub table_index: TableIndex,
    /// Optionally, a global variable giving a base index.
    pub base: Option<GlobalIndex>,
    /// The offset to add to the base.
    pub offset: usize,
    /// The values to write into the table elements.
    pub elements: Box<[FunctionIndex]>,
}

/// A memory index and offset within that memory where a data initialization
/// should be performed.
#[derive(Clone, Debug, PartialEq, Eq, rkyv::Serialize, rkyv::Deserialize, rkyv::Archive)]
pub struct DataInitializerLocation {
    /// The index of the memory to initialize.
    pub memory_index: MemoryIndex,

    /// Optionally a Global variable base to initialize at.
    pub base: Option<GlobalIndex>,

    /// A constant offset to initialize at.
    pub offset: usize,
}

/// A data initializer for linear memory.
#[derive(Debug)]
pub struct DataInitializer<'data> {
    /// The location where the initialization is to be performed.
    pub location: DataInitializerLocation,

    /// The initialization data.
    pub data: &'data [u8],
}

/// As `DataInitializer` but owning the data rather than
/// holding a reference to it
#[derive(Debug, Clone, PartialEq, Eq, rkyv::Serialize, rkyv::Deserialize, rkyv::Archive)]
pub struct OwnedDataInitializer {
    /// The location where the initialization is to be performed.
    pub location: DataInitializerLocation,

    /// The initialization owned data.
    pub data: Vec<u8>,
}

impl OwnedDataInitializer {
    /// Creates a new `OwnedDataInitializer` from a `DataInitializer`.
    pub fn new(borrowed: &DataInitializer<'_>) -> Self {
        Self {
            location: borrowed.location.clone(),
            data: borrowed.data.to_vec(),
        }
    }
}

impl<'a> From<&'a OwnedDataInitializer> for DataInitializer<'a> {
    fn from(init: &'a OwnedDataInitializer) -> Self {
        DataInitializer {
            location: init.location.clone(),
            data: &*init.data,
        }
    }
}

impl<'a> From<&'a ArchivedOwnedDataInitializer> for DataInitializer<'a> {
    fn from(init: &'a ArchivedOwnedDataInitializer) -> Self {
        DataInitializer {
            location: rkyv::Deserialize::deserialize(&init.location, &mut rkyv::Infallible)
                .expect("deserialization cannot fail"),
            data: &*init.data,
        }
    }
}

impl<'a> From<DataInitializer<'a>> for OwnedDataInitializer {
    fn from(init: DataInitializer<'a>) -> Self {
        OwnedDataInitializer {
            location: init.location.clone(),
            data: init.data.to_vec(),
        }
    }
}

'''
'''--- lib/types/src/lib.rs ---
//! This are the common types and utility tools for using WebAssembly
//! in a Rust environment.
//!
//! This crate provides common structures such as `Type` or `Value`, type indexes
//! and native function wrappers with `Func`.

#![deny(missing_docs, unused_extern_crates)]
#![warn(unused_import_braces)]
#![cfg_attr(feature = "std", deny(unstable_features))]
#![cfg_attr(not(feature = "std"), no_std)]
#![cfg_attr(feature = "cargo-clippy", allow(clippy::new_without_default))]
#![cfg_attr(
    feature = "cargo-clippy",
    warn(
        clippy::float_arithmetic,
        clippy::mut_mut,
        clippy::nonminimal_bool,
        clippy::option_map_unwrap_or,
        clippy::option_map_unwrap_or_else,
        clippy::print_stdout,
        clippy::unicode_not_nfc,
        clippy::use_self
    )
)]

#[cfg(all(feature = "std", feature = "core"))]
compile_error!(
    "The `std` and `core` features are both enabled, which is an error. Please enable only once."
);

#[cfg(all(not(feature = "std"), not(feature = "core")))]
compile_error!("Both the `std` and `core` features are disabled. Please enable one of them.");

#[cfg(feature = "core")]
extern crate alloc;

/// The `lib` module defines a `std` module that is identical whether
/// the `core` or the `std` feature is enabled.
pub mod lib {
    /// Custom `std` module.
    #[cfg(feature = "core")]
    pub mod std {
        pub use alloc::{borrow, boxed, format, iter, rc, slice, string, vec};
        pub use core::{any, cell, cmp, convert, fmt, hash, marker, mem, ops, ptr, sync, u32};
    }

    /// Custom `std` module.
    #[cfg(feature = "std")]
    pub mod std {
        pub use std::{
            any, borrow, boxed, cell, cmp, convert, fmt, format, hash, iter, marker, mem, ops, ptr,
            rc, slice, string, sync, u32, vec,
        };
    }
}

mod archives;
mod extern_ref;
mod features;
mod indexes;
mod initializers;
mod memory_view;
mod module;
mod native;
mod types;
mod units;
mod values;

/// The entity module, with common helpers for Rust structures
pub mod entity;
pub use crate::extern_ref::{ExternRef, VMExternRef};
pub use crate::features::Features;
pub use crate::indexes::{
    CustomSectionIndex, DataIndex, ElemIndex, ExportIndex, FunctionIndex, GlobalIndex, ImportIndex,
    LocalFunctionIndex, LocalGlobalIndex, LocalMemoryIndex, LocalTableIndex, MemoryIndex,
    SignatureIndex, TableIndex,
};
pub use crate::initializers::{
    DataInitializer, DataInitializerLocation, OwnedDataInitializer, OwnedTableInitializer,
};
pub use crate::memory_view::{Atomically, MemoryView};
pub use crate::module::{ImportCounts, ModuleInfo};
pub use crate::native::{NativeWasmType, ValueType};
pub use crate::units::{
    Bytes, PageCountOutOfRange, Pages, WASM_MAX_PAGES, WASM_MIN_PAGES, WASM_PAGE_SIZE,
};
pub use crate::values::{Value, WasmValueType};
pub use types::{
    ExportType, ExternType, FastGasCounter, FunctionType, FunctionTypeRef, GlobalInit, GlobalType,
    Import, InstanceConfig, MemoryType, Mutability, TableType, Type, V128,
};

pub use archives::ArchivableIndexMap;

/// Version number of this crate.
pub const VERSION: &str = env!("CARGO_PKG_VERSION");

'''
'''--- lib/types/src/memory_view.rs ---
use crate::lib::std::cell::Cell;
use crate::lib::std::marker::PhantomData;
use crate::lib::std::ops::Deref;
// use crate::lib::std::ops::{Bound, RangeBounds};
use crate::lib::std::slice;
use crate::lib::std::sync::atomic::{
    AtomicI16, AtomicI32, AtomicI64, AtomicI8, AtomicU16, AtomicU32, AtomicU64, AtomicU8,
};
use crate::native::ValueType;

pub trait Atomic {
    type Output;
}

macro_rules! atomic {
    ( $($for:ty => $output:ty),+ ) => {
        $(
            impl Atomic for $for {
                type Output = $output;
            }
        )+
    }
}

atomic!(
    i8 => AtomicI8,
    i16 => AtomicI16,
    i32 => AtomicI32,
    i64 => AtomicI64,
    u8 => AtomicU8,
    u16 => AtomicU16,
    u32 => AtomicU32,
    u64 => AtomicU64,
    f32 => AtomicU32,
    f64 => AtomicU64
);

/// A trait that represants an atomic type.
pub trait Atomicity {}

/// Atomically.
pub struct Atomically;
impl Atomicity for Atomically {}

/// Non-atomically.
pub struct NonAtomically;
impl Atomicity for NonAtomically {}

/// A view into a memory.
pub struct MemoryView<'a, T: 'a, A = NonAtomically> {
    ptr: *mut T,
    // Note: the length is in the terms of `size::<T>()`.
    // The total length in memory is `size::<T>() * length`.
    length: usize,
    _phantom: PhantomData<(&'a [Cell<T>], A)>,
}

impl<'a, T> MemoryView<'a, T, NonAtomically>
where
    T: ValueType,
{
    /// Creates a new MemoryView given a `pointer` and `length`.
    pub unsafe fn new(ptr: *mut T, length: u32) -> Self {
        Self {
            ptr,
            length: length as usize,
            _phantom: PhantomData,
        }
    }

    /// Creates a subarray view from this `MemoryView`.
    pub fn subarray(&self, start: u32, end: u32) -> Self {
        assert!(
            (start as usize) < self.length,
            "The range start is bigger than current length"
        );
        assert!(
            (end as usize) < self.length,
            "The range end is bigger than current length"
        );

        Self {
            ptr: unsafe { self.ptr.add(start as usize) },
            length: (end - start) as usize,
            _phantom: PhantomData,
        }
    }

    /// Copy the contents of the source slice into this `MemoryView`.
    ///
    /// This function will efficiently copy the memory from within the wasm
    /// module‚Äôs own linear memory to this typed array.
    ///
    /// # Safety
    ///
    /// This method is unsafe because the caller will need to make sure
    /// there are no data races when copying memory into the view.
    pub unsafe fn copy_from(&self, src: &[T]) {
        // We cap at a max length
        let sliced_src = &src[..self.length];
        for (i, byte) in sliced_src.iter().enumerate() {
            *self.ptr.offset(i as isize) = *byte;
        }
    }
}

impl<'a, T: Atomic> MemoryView<'a, T> {
    /// Get atomic access to a memory view.
    pub fn atomically(&self) -> MemoryView<'a, T::Output, Atomically> {
        MemoryView {
            ptr: self.ptr as *mut T::Output,
            length: self.length,
            _phantom: PhantomData,
        }
    }
}

impl<'a, T> Deref for MemoryView<'a, T, NonAtomically> {
    type Target = [Cell<T>];
    fn deref(&self) -> &[Cell<T>] {
        let mut_slice: &mut [T] = unsafe { slice::from_raw_parts_mut(self.ptr, self.length) };
        let cell_slice: &Cell<[T]> = Cell::from_mut(mut_slice);
        cell_slice.as_slice_of_cells()
    }
}

impl<'a, T> Deref for MemoryView<'a, T, Atomically> {
    type Target = [T];
    fn deref(&self) -> &[T] {
        unsafe { slice::from_raw_parts(self.ptr as *const T, self.length) }
    }
}

'''
'''--- lib/types/src/module.rs ---
// This file contains code from external sources.
// Attributions: https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md

//! Data structure for representing WebAssembly modules in a
//! `wasmer::Module`.

use crate::entity::{EntityRef, PrimaryMap};
use crate::ArchivableIndexMap;
use crate::{
    CustomSectionIndex, DataIndex, ElemIndex, ExportIndex, FunctionIndex, FunctionType,
    GlobalIndex, GlobalInit, GlobalType, ImportIndex, LocalFunctionIndex, LocalGlobalIndex,
    LocalMemoryIndex, LocalTableIndex, MemoryIndex, MemoryType, OwnedTableInitializer,
    SignatureIndex, TableIndex, TableType,
};
use indexmap::IndexMap;
use rkyv::{
    de::SharedDeserializeRegistry, ser::ScratchSpace, ser::Serializer,
    ser::SharedSerializeRegistry, Archive, Archived, Fallible,
};
use std::collections::BTreeMap;
use std::collections::HashMap;
use std::fmt;
use std::sync::atomic::{AtomicUsize, Ordering::SeqCst};
use std::sync::Arc;

#[derive(Debug, Clone, rkyv::Serialize, rkyv::Deserialize, rkyv::Archive)]
pub struct ModuleId {
    id: usize,
}

impl ModuleId {
    pub fn id(&self) -> String {
        format!("{}", &self.id)
    }
}

impl Default for ModuleId {
    fn default() -> Self {
        static NEXT_ID: AtomicUsize = AtomicUsize::new(0);
        Self {
            id: NEXT_ID.fetch_add(1, SeqCst),
        }
    }
}

/// The counts of imported entities in a WebAssembly module.
#[derive(
    Debug, Copy, Clone, Default, PartialEq, Eq, rkyv::Serialize, rkyv::Deserialize, rkyv::Archive,
)]
#[archive(as = "Self")]
pub struct ImportCounts {
    /// Number of imported functions in the module.
    pub functions: u32,

    /// Number of imported tables in the module.
    pub tables: u32,

    /// Number of imported memories in the module.
    pub memories: u32,

    /// Number of imported globals in the module.
    pub globals: u32,
}

impl ImportCounts {
    fn make_local<R: EntityRef, I: EntityRef>(idx: I, imports: u32) -> Result<R, I> {
        EntityRef::index(idx)
            .checked_sub(imports as _)
            .map(R::new)
            .ok_or(idx)
    }

    /// Convert the `FunctionIndex` to a `LocalFunctionIndex`.
    pub fn local_function_index(
        &self,
        idx: FunctionIndex,
    ) -> Result<LocalFunctionIndex, FunctionIndex> {
        Self::make_local(idx, self.functions)
    }

    /// Convert the `TableIndex` to a `LocalTableIndex`.
    pub fn local_table_index(&self, idx: TableIndex) -> Result<LocalTableIndex, TableIndex> {
        Self::make_local(idx, self.tables)
    }

    /// Convert the `MemoryIndex` to a `LocalMemoryIndex`.
    pub fn local_memory_index(&self, idx: MemoryIndex) -> Result<LocalMemoryIndex, MemoryIndex> {
        Self::make_local(idx, self.memories)
    }

    /// Convert the `GlobalIndex` to a `LocalGlobalIndex`.
    pub fn local_global_index(&self, idx: GlobalIndex) -> Result<LocalGlobalIndex, GlobalIndex> {
        Self::make_local(idx, self.globals)
    }

    fn make_index<R: EntityRef, I: EntityRef>(idx: I, imports: u32) -> R {
        let imports = imports as usize;
        R::new(idx.index() + imports)
    }

    /// Convert the `LocalFunctionIndex` to a `FunctionIndex`.
    pub fn function_index(&self, idx: LocalFunctionIndex) -> FunctionIndex {
        Self::make_index(idx, self.functions)
    }

    /// Convert the `LocalTableIndex` to a `TableIndex`.
    pub fn table_index(&self, idx: LocalTableIndex) -> TableIndex {
        Self::make_index(idx, self.tables)
    }

    /// Convert the `LocalMemoryIndex` to a `MemoryIndex`.
    pub fn memory_index(&self, idx: LocalMemoryIndex) -> MemoryIndex {
        Self::make_index(idx, self.memories)
    }

    /// Convert the `LocalGlobalIndex` to a `GlobalIndex`.
    pub fn global_index(&self, idx: LocalGlobalIndex) -> GlobalIndex {
        Self::make_index(idx, self.globals)
    }
}

/// A translated WebAssembly module, excluding the function bodies and
/// memory initializers.
#[derive(Debug, Clone, Default)]
pub struct ModuleInfo {
    /// A unique identifier (within this process) for this module.
    ///
    /// We skip serialization/deserialization of this field, as it
    /// should be computed by the process.
    ///
    /// It's not skipped in rkyv, but that is okay, because even though it's skipped in
    /// bincode/serde it's still deserialized back as a garbage number, and later override from
    /// computed by the process
    pub id: ModuleId,

    /// The name of this wasm module, often found in the wasm file.
    pub name: Option<String>,

    /// Imported entities with the (module, field, index_of_the_import)
    ///
    /// Keeping the `index_of_the_import` is important, as there can be
    /// two same references to the same import, and we don't want to confuse
    /// them.
    pub imports: IndexMap<(String, String, u32), ImportIndex>,

    /// Exported entities.
    pub exports: IndexMap<String, ExportIndex>,

    /// The module "start" function, if present.
    pub start_function: Option<FunctionIndex>,

    /// WebAssembly table initializers.
    pub table_initializers: Vec<OwnedTableInitializer>,

    /// WebAssembly passive elements.
    pub passive_elements: BTreeMap<ElemIndex, Box<[FunctionIndex]>>,

    /// WebAssembly passive data segments.
    pub passive_data: BTreeMap<DataIndex, Arc<[u8]>>,

    /// WebAssembly global initializers.
    pub global_initializers: PrimaryMap<LocalGlobalIndex, GlobalInit>,

    /// WebAssembly function names.
    pub function_names: HashMap<FunctionIndex, String>,

    /// WebAssembly function signatures.
    pub signatures: PrimaryMap<SignatureIndex, FunctionType>,

    /// WebAssembly functions (imported and local).
    pub functions: PrimaryMap<FunctionIndex, SignatureIndex>,

    /// WebAssembly tables (imported and local).
    pub tables: PrimaryMap<TableIndex, TableType>,

    /// WebAssembly linear memories (imported and local).
    pub memories: PrimaryMap<MemoryIndex, MemoryType>,

    /// WebAssembly global variables (imported and local).
    pub globals: PrimaryMap<GlobalIndex, GlobalType>,

    /// Custom sections in the module.
    pub custom_sections: IndexMap<String, CustomSectionIndex>,

    /// The data for each CustomSection in the module.
    pub custom_sections_data: PrimaryMap<CustomSectionIndex, Arc<[u8]>>,

    /// The counts of imported entities.
    pub import_counts: ImportCounts,
}

/// Mirror version of ModuleInfo that can derive rkyv traits
#[derive(rkyv::Serialize, rkyv::Deserialize, rkyv::Archive)]
pub struct ArchivableModuleInfo {
    pub name: Option<String>,
    pub imports: ArchivableIndexMap<(String, String, u32), ImportIndex>,
    pub exports: ArchivableIndexMap<String, ExportIndex>,
    pub start_function: Option<FunctionIndex>,
    pub table_initializers: Vec<OwnedTableInitializer>,
    pub passive_elements: BTreeMap<ElemIndex, Box<[FunctionIndex]>>,
    pub passive_data: BTreeMap<DataIndex, Arc<[u8]>>,
    pub global_initializers: PrimaryMap<LocalGlobalIndex, GlobalInit>,
    pub function_names: BTreeMap<FunctionIndex, String>,
    pub signatures: PrimaryMap<SignatureIndex, FunctionType>,
    pub functions: PrimaryMap<FunctionIndex, SignatureIndex>,
    pub tables: PrimaryMap<TableIndex, TableType>,
    pub memories: PrimaryMap<MemoryIndex, MemoryType>,
    pub globals: PrimaryMap<GlobalIndex, GlobalType>,
    pub custom_sections: ArchivableIndexMap<String, CustomSectionIndex>,
    pub custom_sections_data: PrimaryMap<CustomSectionIndex, Arc<[u8]>>,
    pub import_counts: ImportCounts,
}

impl From<ModuleInfo> for ArchivableModuleInfo {
    fn from(it: ModuleInfo) -> ArchivableModuleInfo {
        ArchivableModuleInfo {
            name: it.name,
            imports: ArchivableIndexMap::from(it.imports),
            exports: ArchivableIndexMap::from(it.exports),
            start_function: it.start_function,
            table_initializers: it.table_initializers,
            passive_elements: it.passive_elements.into_iter().collect(),
            passive_data: it.passive_data.into_iter().collect(),
            global_initializers: it.global_initializers,
            function_names: it.function_names.into_iter().collect(),
            signatures: it.signatures,
            functions: it.functions,
            tables: it.tables,
            memories: it.memories,
            globals: it.globals,
            custom_sections: ArchivableIndexMap::from(it.custom_sections),
            custom_sections_data: it.custom_sections_data,
            import_counts: it.import_counts,
        }
    }
}

impl From<ArchivableModuleInfo> for ModuleInfo {
    fn from(it: ArchivableModuleInfo) -> ModuleInfo {
        ModuleInfo {
            id: Default::default(),
            name: it.name,
            imports: it.imports.into(),
            exports: it.exports.into(),
            start_function: it.start_function,
            table_initializers: it.table_initializers,
            passive_elements: it.passive_elements.into_iter().collect(),
            passive_data: it.passive_data.into_iter().collect(),
            global_initializers: it.global_initializers,
            function_names: it.function_names.into_iter().collect(),
            signatures: it.signatures,
            functions: it.functions,
            tables: it.tables,
            memories: it.memories,
            globals: it.globals,
            custom_sections: it.custom_sections.into(),
            custom_sections_data: it.custom_sections_data,
            import_counts: it.import_counts,
        }
    }
}

impl From<&ModuleInfo> for ArchivableModuleInfo {
    fn from(it: &ModuleInfo) -> ArchivableModuleInfo {
        ArchivableModuleInfo::from(it.clone())
    }
}

impl Archive for ModuleInfo {
    type Archived = <ArchivableModuleInfo as Archive>::Archived;
    type Resolver = <ArchivableModuleInfo as Archive>::Resolver;

    unsafe fn resolve(&self, pos: usize, resolver: Self::Resolver, out: *mut Self::Archived) {
        ArchivableModuleInfo::from(self).resolve(pos, resolver, out)
    }
}

impl<S: Serializer + SharedSerializeRegistry + ScratchSpace + ?Sized> rkyv::Serialize<S>
    for ModuleInfo
{
    fn serialize(&self, serializer: &mut S) -> Result<Self::Resolver, S::Error> {
        ArchivableModuleInfo::from(self).serialize(serializer)
    }
}

impl<D: Fallible + ?Sized + SharedDeserializeRegistry> rkyv::Deserialize<ModuleInfo, D>
    for Archived<ModuleInfo>
{
    fn deserialize(&self, deserializer: &mut D) -> Result<ModuleInfo, D::Error> {
        let r: ArchivableModuleInfo =
            rkyv::Deserialize::<ArchivableModuleInfo, D>::deserialize(self, deserializer)?;
        Ok(ModuleInfo::from(r))
    }
}

// For test serialization correctness, everything except module id should be same
impl PartialEq for ModuleInfo {
    fn eq(&self, other: &ModuleInfo) -> bool {
        self.name == other.name
            && self.imports == other.imports
            && self.exports == other.exports
            && self.start_function == other.start_function
            && self.table_initializers == other.table_initializers
            && self.passive_elements == other.passive_elements
            && self.passive_data == other.passive_data
            && self.global_initializers == other.global_initializers
            && self.function_names == other.function_names
            && self.signatures == other.signatures
            && self.functions == other.functions
            && self.tables == other.tables
            && self.memories == other.memories
            && self.globals == other.globals
            && self.custom_sections == other.custom_sections
            && self.custom_sections_data == other.custom_sections_data
            && self.import_counts == other.import_counts
    }
}

impl Eq for ModuleInfo {}

impl ModuleInfo {
    /// Allocates the module data structures.
    pub fn new() -> Self {
        Default::default()
    }

    /// Get the given passive element, if it exists.
    pub fn get_passive_element(&self, index: ElemIndex) -> Option<&[FunctionIndex]> {
        self.passive_elements.get(&index).map(|es| &**es)
    }

    /// Get the exported signatures of the module
    pub fn exported_signatures(&self) -> Vec<FunctionType> {
        self.exports
            .iter()
            .filter_map(|(_name, export_index)| match export_index {
                ExportIndex::Function(i) => {
                    let signature = self.functions.get(*i).unwrap();
                    let func_type = self.signatures.get(*signature).unwrap();
                    Some(func_type.clone())
                }
                _ => None,
            })
            .collect::<Vec<FunctionType>>()
    }

    /// Get the custom sections of the module given a `name`.
    pub fn custom_sections<'a>(&'a self, name: &'a str) -> impl Iterator<Item = Arc<[u8]>> + 'a {
        self.custom_sections
            .iter()
            .filter_map(move |(section_name, section_index)| {
                if name != section_name {
                    return None;
                }
                Some(self.custom_sections_data[*section_index].clone())
            })
    }

    /// Convert a `LocalFunctionIndex` into a `FunctionIndex`.
    pub fn func_index(&self, local_func: LocalFunctionIndex) -> FunctionIndex {
        self.import_counts.function_index(local_func)
    }

    /// Convert a `FunctionIndex` into a `LocalFunctionIndex`. Returns None if the
    /// index is an imported function.
    pub fn local_func_index(&self, func: FunctionIndex) -> Option<LocalFunctionIndex> {
        self.import_counts.local_function_index(func).ok()
    }

    /// Test whether the given function index is for an imported function.
    pub fn is_imported_function(&self, index: FunctionIndex) -> bool {
        self.local_func_index(index).is_none()
    }

    /// Convert a `LocalTableIndex` into a `TableIndex`.
    pub fn table_index(&self, local_table: LocalTableIndex) -> TableIndex {
        self.import_counts.table_index(local_table)
    }

    /// Convert a `TableIndex` into a `LocalTableIndex`. Returns None if the
    /// index is an imported table.
    pub fn local_table_index(&self, table: TableIndex) -> Option<LocalTableIndex> {
        self.import_counts.local_table_index(table).ok()
    }

    /// Test whether the given table index is for an imported table.
    pub fn is_imported_table(&self, index: TableIndex) -> bool {
        self.local_table_index(index).is_none()
    }

    /// Convert a `LocalMemoryIndex` into a `MemoryIndex`.
    pub fn memory_index(&self, local_memory: LocalMemoryIndex) -> MemoryIndex {
        self.import_counts.memory_index(local_memory)
    }

    /// Convert a `MemoryIndex` into a `LocalMemoryIndex`. Returns None if the
    /// index is an imported memory.
    pub fn local_memory_index(&self, memory: MemoryIndex) -> Option<LocalMemoryIndex> {
        self.import_counts.local_memory_index(memory).ok()
    }

    /// Test whether the given memory index is for an imported memory.
    pub fn is_imported_memory(&self, index: MemoryIndex) -> bool {
        self.local_memory_index(index).is_none()
    }

    /// Convert a `LocalGlobalIndex` into a `GlobalIndex`.
    pub fn global_index(&self, local_global: LocalGlobalIndex) -> GlobalIndex {
        self.import_counts.global_index(local_global)
    }

    /// Convert a `GlobalIndex` into a `LocalGlobalIndex`. Returns None if the
    /// index is an imported global.
    pub fn local_global_index(&self, global: GlobalIndex) -> Option<LocalGlobalIndex> {
        self.import_counts.local_global_index(global).ok()
    }

    /// Test whether the given global index is for an imported global.
    pub fn is_imported_global(&self, index: GlobalIndex) -> bool {
        self.local_global_index(index).is_none()
    }

    /// Get the Module name
    pub fn name(&self) -> String {
        match self.name {
            Some(ref name) => name.to_string(),
            None => "<module>".to_string(),
        }
    }

    /// Get the imported function types of the module.
    pub fn imported_function_types<'a>(&'a self) -> impl Iterator<Item = FunctionType> + 'a {
        self.functions
            .values()
            .take(self.import_counts.functions as usize)
            .map(move |sig_index| self.signatures[*sig_index].clone())
    }
}

impl fmt::Display for ModuleInfo {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(f, "{}", self.name())
    }
}

'''
'''--- lib/types/src/native.rs ---
//! This module permits to create native functions
//! easily in Rust, thanks to its advanced typing system.

use crate::extern_ref::VMExternRef;
use crate::lib::std::fmt;
use crate::types::Type;
use crate::values::{Value, WasmValueType};

/// `NativeWasmType` represents a Wasm type that has a direct
/// representation on the host (hence the ‚Äúnative‚Äù term).
///
/// It uses the Rust Type system to automatically detect the
/// Wasm type associated with a native Rust type.
///
/// ```
/// use wasmer_types::{NativeWasmType, Type};
///
/// let wasm_type = i32::WASM_TYPE;
/// assert_eq!(wasm_type, Type::I32);
/// ```
///
/// > Note: This strategy will be needed later to
/// > automatically detect the signature of a Rust function.
pub trait NativeWasmType: Sized {
    /// The ABI for this type (i32, i64, f32, f64)
    type Abi: Copy + fmt::Debug;

    /// Type for this `NativeWasmType`.
    const WASM_TYPE: Type;

    #[doc(hidden)]
    fn from_abi(abi: Self::Abi) -> Self;

    #[doc(hidden)]
    fn into_abi(self) -> Self::Abi;

    /// Convert self to i128 binary representation.
    fn to_binary(self) -> i128;

    /// Convert self to a `Value`.
    fn to_value<T: WasmValueType>(self) -> Value<T> {
        let binary = self.to_binary();
        // we need a store, we're just hoping we don't actually use it via funcref
        // TODO(reftypes): we need an actual solution here
        let hack = 3;

        unsafe { Value::read_value_from(&hack, &binary, Self::WASM_TYPE) }
    }

    /// Convert to self from i128 binary representation.
    fn from_binary(binary: i128) -> Self;
}

impl NativeWasmType for i32 {
    const WASM_TYPE: Type = Type::I32;
    type Abi = Self;

    #[inline]
    fn from_abi(abi: Self::Abi) -> Self {
        abi
    }

    #[inline]
    fn into_abi(self) -> Self::Abi {
        self
    }

    #[inline]
    fn to_binary(self) -> i128 {
        self as _
    }

    #[inline]
    fn from_binary(bits: i128) -> Self {
        bits as _
    }
}

impl NativeWasmType for i64 {
    const WASM_TYPE: Type = Type::I64;
    type Abi = Self;

    #[inline]
    fn from_abi(abi: Self::Abi) -> Self {
        abi
    }

    #[inline]
    fn into_abi(self) -> Self::Abi {
        self
    }

    #[inline]
    fn to_binary(self) -> i128 {
        self as _
    }

    #[inline]
    fn from_binary(bits: i128) -> Self {
        bits as _
    }
}

impl NativeWasmType for f32 {
    const WASM_TYPE: Type = Type::F32;
    type Abi = Self;

    #[inline]
    fn from_abi(abi: Self::Abi) -> Self {
        abi
    }

    #[inline]
    fn into_abi(self) -> Self::Abi {
        self
    }

    #[inline]
    fn to_binary(self) -> i128 {
        self.to_bits() as _
    }

    #[inline]
    fn from_binary(bits: i128) -> Self {
        Self::from_bits(bits as _)
    }
}

impl NativeWasmType for f64 {
    const WASM_TYPE: Type = Type::F64;
    type Abi = Self;

    #[inline]
    fn from_abi(abi: Self::Abi) -> Self {
        abi
    }

    #[inline]
    fn into_abi(self) -> Self::Abi {
        self
    }

    #[inline]
    fn to_binary(self) -> i128 {
        self.to_bits() as _
    }

    #[inline]
    fn from_binary(bits: i128) -> Self {
        Self::from_bits(bits as _)
    }
}

impl NativeWasmType for u128 {
    const WASM_TYPE: Type = Type::V128;
    type Abi = Self;

    #[inline]
    fn from_abi(abi: Self::Abi) -> Self {
        abi
    }

    #[inline]
    fn into_abi(self) -> Self::Abi {
        self
    }

    #[inline]
    fn to_binary(self) -> i128 {
        self as _
    }

    #[inline]
    fn from_binary(bits: i128) -> Self {
        bits as _
    }
}

impl NativeWasmType for VMExternRef {
    const WASM_TYPE: Type = Type::ExternRef;
    type Abi = Self;

    #[inline]
    fn from_abi(abi: Self::Abi) -> Self {
        abi
    }

    #[inline]
    fn into_abi(self) -> Self::Abi {
        self
    }

    #[inline]
    fn to_binary(self) -> i128 {
        self.to_binary()
    }

    #[inline]
    fn from_binary(bits: i128) -> Self {
        // TODO(reftypes): ensure that the safety invariants are actually upheld here
        unsafe { Self::from_binary(bits) }
    }
}

#[cfg(test)]
mod test_native_type {
    use super::*;
    use crate::types::Type;

    #[test]
    fn test_wasm_types() {
        assert_eq!(i32::WASM_TYPE, Type::I32);
        assert_eq!(i64::WASM_TYPE, Type::I64);
        assert_eq!(f32::WASM_TYPE, Type::F32);
        assert_eq!(f64::WASM_TYPE, Type::F64);
        assert_eq!(u128::WASM_TYPE, Type::V128);
    }

    #[test]
    fn test_roundtrip() {
        assert_eq!(i32::from_binary(42i32.to_binary()), 42i32);
        assert_eq!(i64::from_binary(42i64.to_binary()), 42i64);
        assert_eq!(f32::from_binary(42f32.to_binary()), 42f32);
        assert_eq!(f64::from_binary(42f64.to_binary()), 42f64);
        assert_eq!(u128::from_binary(42u128.to_binary()), 42u128);
    }
}

// pub trait IntegerAtomic
// where
//     Self: Sized
// {
//     type Primitive;

//     fn add(&self, other: Self::Primitive) -> Self::Primitive;
//     fn sub(&self, other: Self::Primitive) -> Self::Primitive;
//     fn and(&self, other: Self::Primitive) -> Self::Primitive;
//     fn or(&self, other: Self::Primitive) -> Self::Primitive;
//     fn xor(&self, other: Self::Primitive) -> Self::Primitive;
//     fn load(&self) -> Self::Primitive;
//     fn store(&self, other: Self::Primitive) -> Self::Primitive;
//     fn compare_exchange(&self, expected: Self::Primitive, new: Self::Primitive) -> Self::Primitive;
//     fn swap(&self, other: Self::Primitive) -> Self::Primitive;
// }

/// Trait for a Value type. A Value type is a type that is always valid and may
/// be safely copied.
///
/// That is, for all possible bit patterns a valid Value type can be constructed
/// from those bits.
///
/// Concretely a `u32` is a Value type because every combination of 32 bits is
/// a valid `u32`. However a `bool` is _not_ a Value type because any bit patterns
/// other than `0` and `1` are invalid in Rust and may cause undefined behavior if
/// a `bool` is constructed from those bytes.
pub unsafe trait ValueType: Copy
where
    Self: Sized,
{
}

macro_rules! impl_value_type_for {
    ( $($type:ty),* ) => {
        $(
            unsafe impl ValueType for $type {}
        )*
    };
}

impl_value_type_for!(u8, i8, u16, i16, u32, i32, u64, i64, f32, f64);

'''
'''--- lib/types/src/types.rs ---
use crate::indexes::{FunctionIndex, GlobalIndex};
use crate::lib::std::fmt;
use crate::lib::std::format;
use crate::lib::std::string::{String, ToString};
use crate::lib::std::vec::Vec;
use crate::units::Pages;
use crate::values::{Value, WasmValueType};
use std::cell::UnsafeCell;
use std::rc::Rc;
use std::sync::Arc;

// Type Representations

// Value Types

/// A list of all possible value types in WebAssembly.
#[derive(
    Copy, Debug, Clone, Eq, PartialEq, Hash, rkyv::Serialize, rkyv::Deserialize, rkyv::Archive,
)]
#[archive(as = "Self")]
pub enum Type {
    /// Signed 32 bit integer.
    I32,
    /// Signed 64 bit integer.
    I64,
    /// Floating point 32 bit integer.
    F32,
    /// Floating point 64 bit integer.
    F64,
    /// A 128 bit number.
    V128,
    /// A reference to opaque data in the Wasm instance.
    ExternRef, /* = 128 */
    /// A reference to a Wasm function.
    FuncRef,
}

impl Type {
    /// Returns true if `Type` matches any of the numeric types. (e.g. `I32`,
    /// `I64`, `F32`, `F64`, `V128`).
    pub fn is_num(self) -> bool {
        matches!(
            self,
            Self::I32 | Self::I64 | Self::F32 | Self::F64 | Self::V128
        )
    }

    /// Returns true if `Type` matches either of the reference types.
    pub fn is_ref(self) -> bool {
        matches!(self, Self::ExternRef | Self::FuncRef)
    }
}

impl fmt::Display for Type {
    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
        write!(f, "{:?}", self)
    }
}

#[derive(
    Copy, Clone, Debug, Eq, PartialEq, Hash, rkyv::Serialize, rkyv::Deserialize, rkyv::Archive,
)]
#[archive(as = "Self")]
/// The WebAssembly V128 type
pub struct V128(pub(crate) [u8; 16]);

impl V128 {
    /// Get the bytes corresponding to the V128 value
    pub fn bytes(&self) -> &[u8; 16] {
        &self.0
    }
    /// Iterate over the bytes in the constant.
    pub fn iter(&self) -> impl Iterator<Item = &u8> {
        self.0.iter()
    }

    /// Convert the immediate into a vector.
    pub fn to_vec(self) -> Vec<u8> {
        self.0.to_vec()
    }

    /// Convert the immediate into a slice.
    pub fn as_slice(&self) -> &[u8] {
        &self.0[..]
    }
}

impl From<[u8; 16]> for V128 {
    fn from(array: [u8; 16]) -> Self {
        Self(array)
    }
}

impl From<&[u8]> for V128 {
    fn from(slice: &[u8]) -> Self {
        assert_eq!(slice.len(), 16);
        let mut buffer = [0; 16];
        buffer.copy_from_slice(slice);
        Self(buffer)
    }
}

// External Types

/// A list of all possible types which can be externally referenced from a
/// WebAssembly module.
///
/// This list can be found in [`ImportType`] or [`ExportType`], so these types
/// can either be imported or exported.
#[derive(Debug, Clone, PartialEq, Eq, Hash)]
pub enum ExternType {
    /// This external type is the type of a WebAssembly function.
    Function(FunctionType),
    /// This external type is the type of a WebAssembly global.
    Global(GlobalType),
    /// This external type is the type of a WebAssembly table.
    Table(TableType),
    /// This external type is the type of a WebAssembly memory.
    Memory(MemoryType),
}

macro_rules! accessors {
    ($(($variant:ident($ty:ty) $get:ident $unwrap:ident))*) => ($(
        /// Attempt to return the underlying type of this external type,
        /// returning `None` if it is a different type.
        pub fn $get(&self) -> Option<&$ty> {
            if let Self::$variant(e) = self {
                Some(e)
            } else {
                None
            }
        }

        /// Returns the underlying descriptor of this [`ExternType`], panicking
        /// if it is a different type.
        ///
        /// # Panics
        ///
        /// Panics if `self` is not of the right type.
        pub fn $unwrap(&self) -> &$ty {
            self.$get().expect(concat!("expected ", stringify!($ty)))
        }
    )*)
}

impl ExternType {
    accessors! {
        (Function(FunctionType) func unwrap_func)
        (Global(GlobalType) global unwrap_global)
        (Table(TableType) table unwrap_table)
        (Memory(MemoryType) memory unwrap_memory)
    }
}

// TODO: `shrink_to_fit` these or change it to `Box<[Type]>` if not using
// Cow or something else
/// The signature of a function that is either implemented
/// in a Wasm module or exposed to Wasm by the host.
///
/// WebAssembly functions can have 0 or more parameters and results.
#[derive(Debug, Clone, PartialEq, Eq, Hash, rkyv::Serialize, rkyv::Deserialize, rkyv::Archive)]
pub struct FunctionType {
    /// The parameters of the function
    params: Arc<[Type]>,
    /// The return values of the function
    results: Arc<[Type]>,
}

impl FunctionType {
    /// Creates a new Function Type with the given parameter and return types.
    pub fn new<Params, Returns>(params: Params, returns: Returns) -> Self
    where
        Params: Into<Arc<[Type]>>,
        Returns: Into<Arc<[Type]>>,
    {
        Self {
            params: params.into(),
            results: returns.into(),
        }
    }

    /// Parameter types.
    pub fn params(&self) -> &[Type] {
        &self.params
    }

    /// Return types.
    pub fn results(&self) -> &[Type] {
        &self.results
    }
}

impl fmt::Display for FunctionType {
    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
        let params = self
            .params
            .iter()
            .map(|p| format!("{:?}", p))
            .collect::<Vec<_>>()
            .join(", ");
        let results = self
            .results
            .iter()
            .map(|p| format!("{:?}", p))
            .collect::<Vec<_>>()
            .join(", ");
        write!(f, "[{}] -> [{}]", params, results)
    }
}

// Macro needed until https://rust-lang.github.io/rfcs/2000-const-generics.html is stable.
// See https://users.rust-lang.org/t/how-to-implement-trait-for-fixed-size-array-of-any-size/31494
macro_rules! implement_from_pair_to_functiontype {
    ($($N:literal,$M:literal)+) => {
        $(
            impl From<([Type; $N], [Type; $M])> for FunctionType {
                fn from(pair: ([Type; $N], [Type; $M])) -> Self {
                    Self::new(&pair.0[..], &pair.1[..])
                }
            }
        )+
    }
}

implement_from_pair_to_functiontype! {
    0,0 0,1 0,2 0,3 0,4 0,5 0,6 0,7 0,8 0,9
    1,0 1,1 1,2 1,3 1,4 1,5 1,6 1,7 1,8 1,9
    2,0 2,1 2,2 2,3 2,4 2,5 2,6 2,7 2,8 2,9
    3,0 3,1 3,2 3,3 3,4 3,5 3,6 3,7 3,8 3,9
    4,0 4,1 4,2 4,3 4,4 4,5 4,6 4,7 4,8 4,9
    5,0 5,1 5,2 5,3 5,4 5,5 5,6 5,7 5,8 5,9
    6,0 6,1 6,2 6,3 6,4 6,5 6,6 6,7 6,8 6,9
    7,0 7,1 7,2 7,3 7,4 7,5 7,6 7,7 7,8 7,9
    8,0 8,1 8,2 8,3 8,4 8,5 8,6 8,7 8,8 8,9
    9,0 9,1 9,2 9,3 9,4 9,5 9,6 9,7 9,8 9,9
}

impl From<&FunctionType> for FunctionType {
    fn from(as_ref: &FunctionType) -> Self {
        as_ref.clone()
    }
}

/// Borrowed version of [`FunctionType`].
pub struct FunctionTypeRef<'a> {
    /// The parameters of the function
    params: &'a [Type],
    /// The return values of the function
    results: &'a [Type],
}

impl<'a> FunctionTypeRef<'a> {
    /// Create a new temporary function type.
    pub fn new(params: &'a [Type], results: &'a [Type]) -> Self {
        Self { params, results }
    }

    /// Parameter types.
    pub fn params(&self) -> &[Type] {
        self.params
    }

    /// Return types.
    pub fn results(&self) -> &[Type] {
        self.results
    }
}

impl<'a> From<&'a FunctionType> for FunctionTypeRef<'a> {
    fn from(FunctionType { params, results }: &'a FunctionType) -> Self {
        Self { params, results }
    }
}

impl<'a> From<&'a ArchivedFunctionType> for FunctionTypeRef<'a> {
    fn from(ArchivedFunctionType { params, results }: &'a ArchivedFunctionType) -> Self {
        Self {
            params: &**params,
            results: &**results,
        }
    }
}

/// Indicator of whether a global is mutable or not
#[derive(
    Debug, Clone, Copy, PartialEq, Eq, Hash, rkyv::Serialize, rkyv::Deserialize, rkyv::Archive,
)]
#[archive(as = "Self")]
pub enum Mutability {
    /// The global is constant and its value does not change
    Const,
    /// The value of the global can change over time
    Var,
}

impl Mutability {
    /// Returns a boolean indicating if the enum is set to mutable.
    pub fn is_mutable(self) -> bool {
        match self {
            Self::Const => false,
            Self::Var => true,
        }
    }
}

/// WebAssembly global.
#[derive(
    Debug, Clone, Copy, PartialEq, Eq, Hash, rkyv::Serialize, rkyv::Deserialize, rkyv::Archive,
)]
#[archive(as = "Self")]
pub struct GlobalType {
    /// The type of the value stored in the global.
    pub ty: Type,
    /// A flag indicating whether the value may change at runtime.
    pub mutability: Mutability,
}

// Global Types

/// A WebAssembly global descriptor.
///
/// This type describes an instance of a global in a WebAssembly
/// module. Globals are local to an `Instance` and are either
/// immutable or mutable.
impl GlobalType {
    /// Create a new Global variable
    /// # Usage:
    /// ```
    /// use wasmer_types::{GlobalType, Type, Mutability, Value};
    ///
    /// // An I32 constant global
    /// let global = GlobalType::new(Type::I32, Mutability::Const);
    /// // An I64 mutable global
    /// let global = GlobalType::new(Type::I64, Mutability::Var);
    /// ```
    pub fn new(ty: Type, mutability: Mutability) -> Self {
        Self { ty, mutability }
    }
}

impl fmt::Display for GlobalType {
    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
        let mutability = match self.mutability {
            Mutability::Const => "constant",
            Mutability::Var => "mutable",
        };
        write!(f, "{} ({})", self.ty, mutability)
    }
}

/// Globals are initialized via the `const` operators or by referring to another import.
#[derive(Debug, Clone, Copy, PartialEq, rkyv::Serialize, rkyv::Deserialize, rkyv::Archive)]
#[archive(as = "Self")]
pub enum GlobalInit {
    /// An `i32.const`.
    I32Const(i32),
    /// An `i64.const`.
    I64Const(i64),
    /// An `f32.const`.
    F32Const(f32),
    /// An `f64.const`.
    F64Const(f64),
    /// A `v128.const`.
    V128Const(V128),
    /// A `global.get` of another global.
    GetGlobal(GlobalIndex),
    // TODO(reftypes): `ref.null func` and `ref.null extern` seem to be 2 different
    // things: we need to handle both. Perhaps this handled in context by the
    // global knowing its own type?
    /// A `ref.null`.
    RefNullConst,
    /// A `ref.func <index>`.
    RefFunc(FunctionIndex),
}

impl Eq for GlobalInit {}

impl GlobalInit {
    /// Get the `GlobalInit` from a given `Value`
    pub fn from_value<T: WasmValueType>(value: Value<T>) -> Self {
        match value {
            Value::I32(i) => Self::I32Const(i),
            Value::I64(i) => Self::I64Const(i),
            Value::F32(f) => Self::F32Const(f),
            Value::F64(f) => Self::F64Const(f),
            _ => unimplemented!("GlobalInit from_value for {:?}", value),
        }
    }
    /// Get the `Value` from the Global init value
    pub fn to_value<T: WasmValueType>(&self) -> Value<T> {
        match self {
            Self::I32Const(i) => Value::I32(*i),
            Self::I64Const(i) => Value::I64(*i),
            Self::F32Const(f) => Value::F32(*f),
            Self::F64Const(f) => Value::F64(*f),
            _ => unimplemented!("GlobalInit to_value for {:?}", self),
        }
    }
}

// Table Types

/// A descriptor for a table in a WebAssembly module.
///
/// Tables are contiguous chunks of a specific element, typically a `funcref` or
/// an `externref`. The most common use for tables is a function table through
/// which `call_indirect` can invoke other functions.
#[derive(
    Debug, Clone, Copy, PartialEq, Eq, Hash, rkyv::Serialize, rkyv::Deserialize, rkyv::Archive,
)]
pub struct TableType {
    /// The type of data stored in elements of the table.
    pub ty: Type,
    /// The minimum number of elements in the table.
    pub minimum: u32,
    /// The maximum number of elements in the table.
    pub maximum: Option<u32>,
}

impl TableType {
    /// Creates a new table descriptor which will contain the specified
    /// `element` and have the `limits` applied to its length.
    pub fn new(ty: Type, minimum: u32, maximum: Option<u32>) -> Self {
        Self {
            ty,
            minimum,
            maximum,
        }
    }
}

impl fmt::Display for TableType {
    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
        if let Some(maximum) = self.maximum {
            write!(f, "{} ({}..{})", self.ty, self.minimum, maximum)
        } else {
            write!(f, "{} ({}..)", self.ty, self.minimum)
        }
    }
}

// Memory Types

/// A descriptor for a WebAssembly memory type.
///
/// Memories are described in units of pages (64KB) and represent contiguous
/// chunks of addressable memory.
#[derive(
    Debug, Clone, Copy, PartialEq, Eq, Hash, rkyv::Serialize, rkyv::Deserialize, rkyv::Archive,
)]
pub struct MemoryType {
    /// The minimum number of pages in the memory.
    pub minimum: Pages,
    /// The maximum number of pages in the memory.
    pub maximum: Option<Pages>,
    /// Whether the memory may be shared between multiple threads.
    pub shared: bool,
}

impl MemoryType {
    /// Creates a new descriptor for a WebAssembly memory given the specified
    /// limits of the memory.
    pub fn new<IntoPages>(minimum: IntoPages, maximum: Option<IntoPages>, shared: bool) -> Self
    where
        IntoPages: Into<Pages>,
    {
        Self {
            minimum: minimum.into(),
            maximum: maximum.map(Into::into),
            shared,
        }
    }
}

impl fmt::Display for MemoryType {
    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
        let shared = if self.shared { "shared" } else { "not shared" };
        if let Some(maximum) = self.maximum {
            write!(f, "{} ({:?}..{:?})", shared, self.minimum, maximum)
        } else {
            write!(f, "{} ({:?}..)", shared, self.minimum)
        }
    }
}

// Import Types

/// A descriptor for an imported value into a wasm module.
///
/// This type is primarily accessed from the `Module::imports`
/// API. Each `ImportType` describes an import into the wasm module
/// with the module/name that it's imported from as well as the type
/// of item that's being imported.
#[derive(Debug, Clone, PartialEq, Eq, Hash)]
pub struct Import<S = String, T = ExternType> {
    module: S,
    name: S,
    index: u32,
    ty: T,
}

impl<S: AsRef<str>, T> Import<S, T> {
    /// Creates a new import descriptor which comes from `module` and `name` and
    /// is of type `ty`.
    pub fn new(module: S, name: S, index: u32, ty: T) -> Self {
        Self {
            module,
            name,
            index,
            ty,
        }
    }

    /// Returns the module name that this import is expected to come from.
    pub fn module(&self) -> &str {
        self.module.as_ref()
    }

    /// Returns the field name of the module that this import is expected to
    /// come from.
    pub fn name(&self) -> &str {
        self.name.as_ref()
    }

    /// The index of the import in the module.
    pub fn index(&self) -> u32 {
        self.index
    }

    /// Returns the expected type of this import.
    pub fn ty(&self) -> &T {
        &self.ty
    }
}

// Export Types

/// A descriptor for an exported WebAssembly value.
///
/// This type is primarily accessed from the `Module::exports`
/// accessor and describes what names are exported from a wasm module
/// and the type of the item that is exported.
///
/// The `<T>` refefers to `ExternType`, however it can also refer to use
/// `MemoryType`, `TableType`, `FunctionType` and `GlobalType` for ease of
/// use.
#[derive(Debug, Clone, PartialEq, Eq, Hash)]
pub struct ExportType<T = ExternType> {
    name: String,
    ty: T,
}

impl<T> ExportType<T> {
    /// Creates a new export which is exported with the given `name` and has the
    /// given `ty`.
    pub fn new(name: &str, ty: T) -> Self {
        Self {
            name: name.to_string(),
            ty,
        }
    }

    /// Returns the name by which this export is known by.
    pub fn name(&self) -> &str {
        &self.name
    }

    /// Returns the type of this export.
    pub fn ty(&self) -> &T {
        &self.ty
    }
}

/// Fast gas counter with very simple structure, could be exposed to compiled code in the VM.
#[repr(C)]
#[derive(Debug, Clone, PartialEq, Eq)]
pub struct FastGasCounter {
    /// The following three fields must be put next to another to make sure
    /// generated gas counting code can use and adjust them.
    /// We will share counter to ensure we never miss synchronization.
    /// This could change and in such a case synchronization required between compiled WASM code
    /// and the host code.

    /// The amount of gas that was irreversibly used for contract execution.
    pub burnt_gas: u64,
    /// Hard gas limit for execution
    pub gas_limit: u64,
    /// Single WASM opcode cost
    pub opcode_cost: u64,
}

impl FastGasCounter {
    /// New fast gas counter.
    pub fn new(limit: u64, opcode: u64) -> Self {
        FastGasCounter {
            burnt_gas: 0,
            gas_limit: limit,
            opcode_cost: opcode,
        }
    }
    /// Amount of gas burnt, maybe load as atomic to avoid aliasing issues.
    pub fn burnt(&self) -> u64 {
        self.burnt_gas
    }
}

impl fmt::Display for FastGasCounter {
    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
        write!(
            f,
            "burnt: {} limit: {} op_cost: {} ",
            self.burnt(),
            self.gas_limit,
            self.opcode_cost
        )
    }
}

/// External configuration of execution environment for Instance.
#[derive(Clone)]
pub struct InstanceConfig {
    /// External gas counter pointer.
    pub gas_counter: *mut FastGasCounter,
    default_gas_counter: Option<Rc<UnsafeCell<FastGasCounter>>>,
    /// Stack limit, in 8-byte slots.
    pub stack_limit: i32,
}

// Default stack limit, in 8-byte stack slots.
const DEFAULT_STACK_LIMIT: i32 = 100 * 1024;

impl InstanceConfig {
    /// Create default instance configuration.
    pub fn default() -> Self {
        let result = Rc::new(UnsafeCell::new(FastGasCounter {
            burnt_gas: 0,
            gas_limit: u64::MAX,
            opcode_cost: 0,
        }));
        Self {
            gas_counter: result.get(),
            default_gas_counter: Some(result),
            stack_limit: DEFAULT_STACK_LIMIT,
        }
    }

    /// Create instance configuration with an external gas counter, unsafe as it creates
    /// an alias on raw memory of gas_counter. This memory could be accessed until
    /// instance configured with this `InstanceConfig` exists.
    pub unsafe fn with_counter(mut self, gas_counter: *mut FastGasCounter) -> Self {
        self.gas_counter = gas_counter;
        self.default_gas_counter = None;
        self
    }

    /// Create instance configuration with given stack limit.
    pub unsafe fn with_stack_limit(mut self, stack_limit: i32) -> Self {
        self.stack_limit = stack_limit;
        self
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    const VOID_TO_VOID: ([Type; 0], [Type; 0]) = ([], []);
    const I32_I32_TO_VOID: ([Type; 2], [Type; 0]) = ([Type::I32, Type::I32], []);
    const V128_I64_TO_I32: ([Type; 2], [Type; 1]) = ([Type::V128, Type::I64], [Type::I32]);
    const NINE_V128_TO_NINE_I32: ([Type; 9], [Type; 9]) = ([Type::V128; 9], [Type::I32; 9]);

    #[test]
    fn convert_tuple_to_functiontype() {
        let ty: FunctionType = VOID_TO_VOID.into();
        assert_eq!(ty.params().len(), 0);
        assert_eq!(ty.results().len(), 0);

        let ty: FunctionType = I32_I32_TO_VOID.into();
        assert_eq!(ty.params().len(), 2);
        assert_eq!(ty.params()[0], Type::I32);
        assert_eq!(ty.params()[1], Type::I32);
        assert_eq!(ty.results().len(), 0);

        let ty: FunctionType = V128_I64_TO_I32.into();
        assert_eq!(ty.params().len(), 2);
        assert_eq!(ty.params()[0], Type::V128);
        assert_eq!(ty.params()[1], Type::I64);
        assert_eq!(ty.results().len(), 1);
        assert_eq!(ty.results()[0], Type::I32);

        let ty: FunctionType = NINE_V128_TO_NINE_I32.into();
        assert_eq!(ty.params().len(), 9);
        assert_eq!(ty.results().len(), 9);
    }
}

'''
'''--- lib/types/src/units.rs ---
use crate::lib::std::convert::TryFrom;
use crate::lib::std::fmt;
use crate::lib::std::ops::{Add, Sub};
use std::convert::TryInto;
use thiserror::Error;

/// WebAssembly page sizes are fixed to be 64KiB.
/// Note: large page support may be added in an opt-in manner in the [future].
///
/// [future]: https://webassembly.org/docs/future-features/#large-page-support
pub const WASM_PAGE_SIZE: usize = 0x10000;

/// The number of pages we can have before we run out of byte index space.
pub const WASM_MAX_PAGES: u32 = 0x10000;

/// The minimum number of pages allowed.
pub const WASM_MIN_PAGES: u32 = 0x100;

/// Units of WebAssembly pages (as specified to be 65,536 bytes).
#[derive(
    Copy,
    Clone,
    PartialEq,
    Eq,
    PartialOrd,
    Ord,
    Hash,
    rkyv::Serialize,
    rkyv::Deserialize,
    rkyv::Archive,
)]
#[archive(as = "Self")]
#[repr(transparent)]
pub struct Pages(pub u32);

impl Pages {
    /// Returns the largest value that can be represented by the Pages type.
    ///
    /// This is defined by the WebAssembly standard as 65,536 pages.
    #[inline(always)]
    pub const fn max_value() -> Self {
        Self(WASM_MAX_PAGES)
    }

    /// Checked addition. Computes `self + rhs`,
    /// returning `None` if overflow occurred.
    pub fn checked_add(self, rhs: Self) -> Option<Self> {
        let added = (self.0 as usize) + (rhs.0 as usize);
        if added <= (WASM_MAX_PAGES as usize) {
            Some(Self(added as u32))
        } else {
            None
        }
    }

    /// Calculate number of bytes from pages.
    pub fn bytes(self) -> Bytes {
        self.into()
    }
}

impl fmt::Debug for Pages {
    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
        write!(f, "{} pages", self.0)
    }
}

impl From<u32> for Pages {
    fn from(other: u32) -> Self {
        Self(other)
    }
}

/// Units of WebAssembly memory in terms of 8-bit bytes.
#[derive(Copy, Clone, PartialEq, Eq, PartialOrd, Ord, Hash)]
pub struct Bytes(pub usize);

impl fmt::Debug for Bytes {
    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
        write!(f, "{} bytes", self.0)
    }
}

impl From<Pages> for Bytes {
    fn from(pages: Pages) -> Self {
        Self((pages.0 as usize) * WASM_PAGE_SIZE)
    }
}

impl From<usize> for Bytes {
    fn from(other: usize) -> Self {
        Self(other)
    }
}

impl From<u32> for Bytes {
    fn from(other: u32) -> Self {
        Self(other.try_into().unwrap())
    }
}

impl<T> Sub<T> for Pages
where
    T: Into<Self>,
{
    type Output = Self;
    fn sub(self, rhs: T) -> Self {
        Self(self.0 - rhs.into().0)
    }
}

impl<T> Add<T> for Pages
where
    T: Into<Self>,
{
    type Output = Self;
    fn add(self, rhs: T) -> Self {
        Self(self.0 + rhs.into().0)
    }
}

/// The only error that can happen when converting `Bytes` to `Pages`
#[derive(Debug, Clone, Copy, PartialEq, Error)]
#[error("Number of pages exceeds uint32 range")]
pub struct PageCountOutOfRange;

impl TryFrom<Bytes> for Pages {
    type Error = PageCountOutOfRange;

    fn try_from(bytes: Bytes) -> Result<Self, Self::Error> {
        let pages: u32 = (bytes.0 / WASM_PAGE_SIZE)
            .try_into()
            .or(Err(PageCountOutOfRange))?;
        Ok(Self(pages))
    }
}

impl<T> Sub<T> for Bytes
where
    T: Into<Self>,
{
    type Output = Self;
    fn sub(self, rhs: T) -> Self {
        Self(self.0 - rhs.into().0)
    }
}

impl<T> Add<T> for Bytes
where
    T: Into<Self>,
{
    type Output = Self;
    fn add(self, rhs: T) -> Self {
        Self(self.0 + rhs.into().0)
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn convert_bytes_to_pages() {
        // rounds down
        let pages = Pages::try_from(Bytes(0)).unwrap();
        assert_eq!(pages, Pages(0));
        let pages = Pages::try_from(Bytes(1)).unwrap();
        assert_eq!(pages, Pages(0));
        let pages = Pages::try_from(Bytes(WASM_PAGE_SIZE - 1)).unwrap();
        assert_eq!(pages, Pages(0));
        let pages = Pages::try_from(Bytes(WASM_PAGE_SIZE)).unwrap();
        assert_eq!(pages, Pages(1));
        let pages = Pages::try_from(Bytes(WASM_PAGE_SIZE + 1)).unwrap();
        assert_eq!(pages, Pages(1));
        let pages = Pages::try_from(Bytes(28 * WASM_PAGE_SIZE + 42)).unwrap();
        assert_eq!(pages, Pages(28));
        let pages = Pages::try_from(Bytes((u32::MAX as usize) * WASM_PAGE_SIZE)).unwrap();
        assert_eq!(pages, Pages(u32::MAX));
        let pages = Pages::try_from(Bytes((u32::MAX as usize) * WASM_PAGE_SIZE + 1)).unwrap();
        assert_eq!(pages, Pages(u32::MAX));

        // Errors when page count cannot be represented as u32
        let result = Pages::try_from(Bytes((u32::MAX as usize + 1) * WASM_PAGE_SIZE));
        assert_eq!(result.unwrap_err(), PageCountOutOfRange);
        let result = Pages::try_from(Bytes(usize::MAX));
        assert_eq!(result.unwrap_err(), PageCountOutOfRange);
    }
}

'''
'''--- lib/types/src/values.rs ---
use crate::extern_ref::ExternRef;
use crate::lib::std::convert::TryFrom;
use crate::lib::std::fmt;
use crate::lib::std::ptr;
use crate::lib::std::string::{String, ToString};
use crate::types::Type;

/// Possible runtime values that a WebAssembly module can either consume or
/// produce.
#[derive(Clone, PartialEq)]
pub enum Value<T> {
    /// A 32-bit integer.
    ///
    /// In Wasm integers are sign-agnostic, i.e. this can either be signed or unsigned.
    I32(i32),

    /// A 64-bit integer.
    ///
    /// In Wasm integers are sign-agnostic, i.e. this can either be signed or unsigned.
    I64(i64),

    /// A 32-bit float.
    F32(f32),

    /// A 64-bit float.
    F64(f64),

    /// An `externref` value which can hold opaque data to the wasm instance itself.
    ///
    /// Note that this is a nullable value as well.
    ExternRef(ExternRef),

    /// A first-class reference to a WebAssembly function.
    FuncRef(Option<T>),

    /// A 128-bit number
    V128(u128),
}

macro_rules! accessors {
    ($bind:ident $(($variant:ident($ty:ty) $get:ident $unwrap:ident $cvt:expr))*) => ($(
        /// Attempt to access the underlying value of this `Value`, returning
        /// `None` if it is not the correct type.
        pub fn $get(&self) -> Option<$ty> {
            if let Self::$variant($bind) = self {
                Some($cvt)
            } else {
                None
            }
        }

        /// Returns the underlying value of this `Value`, panicking if it's the
        /// wrong type.
        ///
        /// # Panics
        ///
        /// Panics if `self` is not of the right type.
        pub fn $unwrap(&self) -> $ty {
            self.$get().expect(concat!("expected ", stringify!($ty)))
        }
    )*)
}

/// Trait for reading and writing Wasm values into binary for use on the layer
/// between the API and the VM internals, specifically with `wasmer_types::Value`.
pub trait WasmValueType: std::fmt::Debug + 'static {
    /// Write the value
    unsafe fn write_value_to(&self, p: *mut i128);

    /// read the value
    // TODO(reftypes): passing the store as `dyn Any` is a hack to work around the
    // structure of our crates. We need to talk about the store in the rest of the
    // VM (for example where this method is used) but cannot do so. Fixing this
    // may be non-trivial.
    unsafe fn read_value_from(store: &dyn std::any::Any, p: *const i128) -> Self;
}

impl WasmValueType for () {
    unsafe fn write_value_to(&self, _p: *mut i128) {}

    unsafe fn read_value_from(_store: &dyn std::any::Any, _p: *const i128) -> Self {
        ()
    }
}

impl<T> Value<T>
where
    T: WasmValueType,
{
    /// Returns a null `externref` value.
    pub fn null() -> Self {
        Self::ExternRef(ExternRef::null())
    }

    /// Returns the corresponding [`Type`] for this `Value`.
    pub fn ty(&self) -> Type {
        match self {
            Self::I32(_) => Type::I32,
            Self::I64(_) => Type::I64,
            Self::F32(_) => Type::F32,
            Self::F64(_) => Type::F64,
            Self::ExternRef(_) => Type::ExternRef,
            Self::FuncRef(_) => Type::FuncRef,
            Self::V128(_) => Type::V128,
        }
    }

    /// Writes it's value to a given pointer
    ///
    /// # Safety
    /// `p` must be:
    /// - Sufficiently aligned for the Rust equivalent of the type in `self`
    /// - Non-null and pointing to valid, mutable memory
    pub unsafe fn write_value_to(&self, p: *mut i128) {
        match self {
            Self::I32(i) => ptr::write(p as *mut i32, *i),
            Self::I64(i) => ptr::write(p as *mut i64, *i),
            Self::F32(u) => ptr::write(p as *mut f32, *u),
            Self::F64(u) => ptr::write(p as *mut f64, *u),
            Self::V128(b) => ptr::write(p as *mut u128, *b),
            Self::FuncRef(Some(b)) => T::write_value_to(b, p),
            Self::FuncRef(None) => ptr::write(p as *mut usize, 0),
            // TODO(reftypes): review clone here
            Self::ExternRef(extern_ref) => ptr::write(p as *mut ExternRef, extern_ref.clone()),
        }
    }

    /// Gets a `Value` given a pointer and a `Type`
    ///
    /// # Safety
    /// `p` must be:
    /// - Properly aligned to the specified `ty`'s Rust equivalent
    /// - Non-null and pointing to valid memory
    pub unsafe fn read_value_from(store: &dyn std::any::Any, p: *const i128, ty: Type) -> Self {
        match ty {
            Type::I32 => Self::I32(ptr::read(p as *const i32)),
            Type::I64 => Self::I64(ptr::read(p as *const i64)),
            Type::F32 => Self::F32(ptr::read(p as *const f32)),
            Type::F64 => Self::F64(ptr::read(p as *const f64)),
            Type::V128 => Self::V128(ptr::read(p as *const u128)),
            Type::FuncRef => {
                // We do the null check ourselves
                if (*(p as *const usize)) == 0 {
                    Self::FuncRef(None)
                } else {
                    Self::FuncRef(Some(T::read_value_from(store, p)))
                }
            }
            Type::ExternRef => {
                let extern_ref = (&*(p as *const ExternRef)).clone();
                Self::ExternRef(extern_ref)
            }
        }
    }

    accessors! {
        e
        (I32(i32) i32 unwrap_i32 *e)
        (I64(i64) i64 unwrap_i64 *e)
        (F32(f32) f32 unwrap_f32 *e)
        (F64(f64) f64 unwrap_f64 *e)
        (ExternRef(ExternRef) externref unwrap_externref e.clone())
        (FuncRef(&Option<T>) funcref unwrap_funcref e)
        (V128(u128) v128 unwrap_v128 *e)
    }
}

impl<T> fmt::Debug for Value<T>
where
    T: WasmValueType,
{
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
            Self::I32(v) => write!(f, "I32({:?})", v),
            Self::I64(v) => write!(f, "I64({:?})", v),
            Self::F32(v) => write!(f, "F32({:?})", v),
            Self::F64(v) => write!(f, "F64({:?})", v),
            Self::ExternRef(v) => write!(f, "ExternRef({:?})", v),
            Self::FuncRef(None) => write!(f, "Null FuncRef"),
            Self::FuncRef(Some(v)) => write!(f, "FuncRef({:?})", v),
            Self::V128(v) => write!(f, "V128({:?})", v),
        }
    }
}

impl<T> ToString for Value<T>
where
    T: WasmValueType,
{
    fn to_string(&self) -> String {
        match self {
            Self::I32(v) => v.to_string(),
            Self::I64(v) => v.to_string(),
            Self::F32(v) => v.to_string(),
            Self::F64(v) => v.to_string(),
            Self::ExternRef(_) => "externref".to_string(),
            Self::FuncRef(_) => "funcref".to_string(),
            Self::V128(v) => v.to_string(),
        }
    }
}

impl<T> From<i32> for Value<T>
where
    T: WasmValueType,
{
    fn from(val: i32) -> Self {
        Self::I32(val)
    }
}

impl<T> From<u32> for Value<T>
where
    T: WasmValueType,
{
    fn from(val: u32) -> Self {
        // In Wasm integers are sign-agnostic, so i32 is basically a 4 byte storage we can use for signed or unsigned 32-bit integers.
        Self::I32(val as i32)
    }
}

impl<T> From<i64> for Value<T>
where
    T: WasmValueType,
{
    fn from(val: i64) -> Self {
        Self::I64(val)
    }
}

impl<T> From<u64> for Value<T>
where
    T: WasmValueType,
{
    fn from(val: u64) -> Self {
        // In Wasm integers are sign-agnostic, so i64 is basically an 8 byte storage we can use for signed or unsigned 64-bit integers.
        Self::I64(val as i64)
    }
}

impl<T> From<f32> for Value<T>
where
    T: WasmValueType,
{
    fn from(val: f32) -> Self {
        Self::F32(val)
    }
}

impl<T> From<f64> for Value<T>
where
    T: WasmValueType,
{
    fn from(val: f64) -> Self {
        Self::F64(val)
    }
}

impl<T> From<ExternRef> for Value<T>
where
    T: WasmValueType,
{
    fn from(val: ExternRef) -> Self {
        Self::ExternRef(val)
    }
}

// impl<T> From<T> for Value<T> {
//     fn from(val: T) -> Self {
//         Self::FuncRef(val)
//     }
// }

const NOT_I32: &str = "Value is not of Wasm type i32";
const NOT_I64: &str = "Value is not of Wasm type i64";
const NOT_F32: &str = "Value is not of Wasm type f32";
const NOT_F64: &str = "Value is not of Wasm type f64";

impl<T> TryFrom<Value<T>> for i32
where
    T: WasmValueType,
{
    type Error = &'static str;

    fn try_from(value: Value<T>) -> Result<Self, Self::Error> {
        value.i32().ok_or(NOT_I32)
    }
}

impl<T> TryFrom<Value<T>> for u32
where
    T: WasmValueType,
{
    type Error = &'static str;

    fn try_from(value: Value<T>) -> Result<Self, Self::Error> {
        value.i32().ok_or(NOT_I32).map(|int| int as Self)
    }
}

impl<T> TryFrom<Value<T>> for i64
where
    T: WasmValueType,
{
    type Error = &'static str;

    fn try_from(value: Value<T>) -> Result<Self, Self::Error> {
        value.i64().ok_or(NOT_I64)
    }
}

impl<T> TryFrom<Value<T>> for u64
where
    T: WasmValueType,
{
    type Error = &'static str;

    fn try_from(value: Value<T>) -> Result<Self, Self::Error> {
        value.i64().ok_or(NOT_I64).map(|int| int as Self)
    }
}

impl<T> TryFrom<Value<T>> for f32
where
    T: WasmValueType,
{
    type Error = &'static str;

    fn try_from(value: Value<T>) -> Result<Self, Self::Error> {
        value.f32().ok_or(NOT_F32)
    }
}

impl<T> TryFrom<Value<T>> for f64
where
    T: WasmValueType,
{
    type Error = &'static str;

    fn try_from(value: Value<T>) -> Result<Self, Self::Error> {
        value.f64().ok_or(NOT_F64)
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_value_i32_from_u32() {
        let bytes = [0x00, 0x00, 0x00, 0x00];
        let v = Value::<()>::from(u32::from_be_bytes(bytes.clone()));
        assert_eq!(v, Value::I32(i32::from_be_bytes(bytes.clone())));

        let bytes = [0x00, 0x00, 0x00, 0x01];
        let v = Value::<()>::from(u32::from_be_bytes(bytes.clone()));
        assert_eq!(v, Value::I32(i32::from_be_bytes(bytes.clone())));

        let bytes = [0xAA, 0xBB, 0xCC, 0xDD];
        let v = Value::<()>::from(u32::from_be_bytes(bytes.clone()));
        assert_eq!(v, Value::I32(i32::from_be_bytes(bytes.clone())));

        let bytes = [0xFF, 0xFF, 0xFF, 0xFF];
        let v = Value::<()>::from(u32::from_be_bytes(bytes.clone()));
        assert_eq!(v, Value::I32(i32::from_be_bytes(bytes.clone())));
    }

    #[test]
    fn test_value_i64_from_u64() {
        let bytes = [0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00];
        let v = Value::<()>::from(u64::from_be_bytes(bytes.clone()));
        assert_eq!(v, Value::I64(i64::from_be_bytes(bytes.clone())));

        let bytes = [0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x01];
        let v = Value::<()>::from(u64::from_be_bytes(bytes.clone()));
        assert_eq!(v, Value::I64(i64::from_be_bytes(bytes.clone())));

        let bytes = [0xAA, 0xBB, 0xCC, 0xDD, 0xEE, 0xFF, 0x00, 0x11];
        let v = Value::<()>::from(u64::from_be_bytes(bytes.clone()));
        assert_eq!(v, Value::I64(i64::from_be_bytes(bytes.clone())));

        let bytes = [0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF];
        let v = Value::<()>::from(u64::from_be_bytes(bytes.clone()));
        assert_eq!(v, Value::I64(i64::from_be_bytes(bytes.clone())));
    }

    #[test]
    fn convert_value_to_i32() {
        let value = Value::<()>::I32(5678);
        let result = i32::try_from(value);
        assert_eq!(result.unwrap(), 5678);

        let value = Value::<()>::from(u32::MAX);
        let result = i32::try_from(value);
        assert_eq!(result.unwrap(), -1);

        let value = Value::<()>::V128(42);
        let result = i32::try_from(value);
        assert_eq!(result.unwrap_err(), "Value is not of Wasm type i32");
    }

    #[test]
    fn convert_value_to_u32() {
        let value = Value::<()>::from(u32::MAX);
        let result = u32::try_from(value);
        assert_eq!(result.unwrap(), u32::MAX);

        let value = Value::<()>::I32(-1);
        let result = u32::try_from(value);
        assert_eq!(result.unwrap(), u32::MAX);

        let value = Value::<()>::V128(42);
        let result = u32::try_from(value);
        assert_eq!(result.unwrap_err(), "Value is not of Wasm type i32");
    }

    #[test]
    fn convert_value_to_i64() {
        let value = Value::<()>::I64(5678);
        let result = i64::try_from(value);
        assert_eq!(result.unwrap(), 5678);

        let value = Value::<()>::from(u64::MAX);
        let result = i64::try_from(value);
        assert_eq!(result.unwrap(), -1);

        let value = Value::<()>::V128(42);
        let result = i64::try_from(value);
        assert_eq!(result.unwrap_err(), "Value is not of Wasm type i64");
    }

    #[test]
    fn convert_value_to_u64() {
        let value = Value::<()>::from(u64::MAX);
        let result = u64::try_from(value);
        assert_eq!(result.unwrap(), u64::MAX);

        let value = Value::<()>::I64(-1);
        let result = u64::try_from(value);
        assert_eq!(result.unwrap(), u64::MAX);

        let value = Value::<()>::V128(42);
        let result = u64::try_from(value);
        assert_eq!(result.unwrap_err(), "Value is not of Wasm type i64");
    }

    #[test]
    fn convert_value_to_f32() {
        let value = Value::<()>::F32(1.234);
        let result = f32::try_from(value);
        assert_eq!(result.unwrap(), 1.234);

        let value = Value::<()>::V128(42);
        let result = f32::try_from(value);
        assert_eq!(result.unwrap_err(), "Value is not of Wasm type f32");

        let value = Value::<()>::F64(1.234);
        let result = f32::try_from(value);
        assert_eq!(result.unwrap_err(), "Value is not of Wasm type f32");
    }

    #[test]
    fn convert_value_to_f64() {
        let value = Value::<()>::F64(1.234);
        let result = f64::try_from(value);
        assert_eq!(result.unwrap(), 1.234);

        let value = Value::<()>::V128(42);
        let result = f64::try_from(value);
        assert_eq!(result.unwrap_err(), "Value is not of Wasm type f64");

        let value = Value::<()>::F32(1.234);
        let result = f64::try_from(value);
        assert_eq!(result.unwrap_err(), "Value is not of Wasm type f64");
    }
}

'''
'''--- lib/vm/Cargo.toml ---
[package]
name = "wasmer-vm-near"
version = "2.4.1"
description = "Runtime library support for Wasmer"
categories = ["wasm"]
keywords = ["wasm", "webassembly"]
authors = ["Wasmer Engineering Team <engineering@wasmer.io>"]
repository = "https://github.com/wasmerio/wasmer"
license = "MIT OR Apache-2.0 WITH LLVM-exception"
readme = "README.md"
edition = "2018"

[lib]
name = "wasmer_vm"

[dependencies]
wasmer-types = { path = "../types", package = "wasmer-types-near", version = "=2.4.1" }
region = "3.0"
libc = { version = "^0.2", default-features = false }
memoffset = "0.6"
indexmap = { version = "1.6" }
thiserror = "1.0"
more-asserts = "0.2"
cfg-if = "1.0"
backtrace = "0.3"
rkyv = { version = "0.7.20" }

[target.'cfg(target_os = "windows")'.dependencies]
winapi = { version = "0.3", features = ["winbase", "memoryapi", "errhandlingapi"] }

[build-dependencies]
cc = "1.0"

[badges]
maintenance = { status = "actively-developed" }

[features]
default = []

'''
'''--- lib/vm/README.md ---
# `wasmer-vm` [![Build Status](https://github.com/wasmerio/wasmer/workflows/build/badge.svg?style=flat-square)](https://github.com/wasmerio/wasmer/actions?query=workflow%3Abuild) [![Join Wasmer Slack](https://img.shields.io/static/v1?label=Slack&message=join%20chat&color=brighgreen&style=flat-square)](https://slack.wasmer.io) [![MIT License](https://img.shields.io/github/license/wasmerio/wasmer.svg?style=flat-square)](https://github.com/wasmerio/wasmer/blob/master/LICENSE)

This crate contains the Wasmer VM runtime library, supporting the Wasm ABI used by any [`wasmer-engine`] implementation.

The Wasmer runtime is modular by design, and provides several
libraries where each of them provides a specific set of features. This
`wasmer-vm` library contains the low-level foundation for the runtime
itself.

It provides all the APIs the
[`wasmer-engine`](https://crates.io/crates/wasmer-engine) needs to operate,
from the `instance`, to `memory`, `probestack`, signature registry, `trap`,
`table`, `VMContext`, `libcalls` etc.

It is very unlikely that a user will need to deal with `wasmer-vm`
directly. The `wasmer` crate provides types that embed types from
`wasmer-vm` with a higher-level API.

[`wasmer-engine`]: https://crates.io/crates/wasmer-engine

### Acknowledgments

This project borrowed some of the code for the VM structure and trapping from the [wasmtime-runtime](https://crates.io/crates/wasmtime-runtime).

Please check [Wasmer ATTRIBUTIONS](https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md) to further see licenses and other attributions of the project. 

'''
'''--- lib/vm/build.rs ---
//! Runtime build script compiles C code using setjmp for trap handling.

use std::env;

fn main() {
    println!("cargo:rerun-if-changed=src/trap/handlers.c");

    cc::Build::new()
        .warnings(true)
        .define(
            &format!(
                "CFG_TARGET_OS_{}",
                env::var("CARGO_CFG_TARGET_OS").unwrap().to_uppercase()
            ),
            None,
        )
        .file("src/trap/handlers.c")
        .compile("handlers");
}

'''
'''--- lib/vm/src/artifact.rs ---
use crate::{InstanceHandle, Resolver, Tunables, VMLocalFunction, VMSharedSignatureIndex};
use std::{any::Any, collections::BTreeMap, sync::Arc};
use wasmer_types::{
    entity::BoxedSlice, ElemIndex, FunctionIndex, GlobalInit, GlobalType, ImportCounts,
    InstanceConfig, LocalFunctionIndex, OwnedDataInitializer, OwnedTableInitializer,
};

mod private {
    pub struct Internal(pub(super) ());
}

/// [`Artifact`]s that can be instantiated.
pub trait Instantiatable: Artifact {
    /// The errors that can occur when instantiating.
    type Error: std::error::Error + Send + Sync;

    /// Crate an `Instance` from this `Artifact`.
    ///
    /// # Safety
    ///
    /// See [`InstanceHandle::new`].
    unsafe fn instantiate(
        self: Arc<Self>,
        tunables: &dyn Tunables,
        resolver: &dyn Resolver,
        host_state: Box<dyn Any>,
        config: InstanceConfig,
    ) -> Result<InstanceHandle, Self::Error>;
}

/// A predecesor of a full module Instance.
///
/// This type represents parts of a compiled WASM module ([`Executable`](crate::Executable)) that
/// are pre-allocated in within some Engine's store.
///
/// Some other operations such as linking, relocating and similar may also be performed during
/// constructon of the Artifact, making this type particularly well suited for caching in-memory.
pub trait Artifact: Send + Sync {
    /// Internal: support for downcasting `Executable`s.
    #[doc(hidden)]
    fn type_id(&self, _: private::Internal) -> std::any::TypeId
    where
        Self: 'static,
    {
        std::any::TypeId::of::<Self>()
    }

    /// The information about offsets into the VM context table.
    fn offsets(&self) -> &crate::VMOffsets;

    /// The count of imported entities.
    fn import_counts(&self) -> &ImportCounts;

    /// The locally defined functions.
    ///
    /// These are published and ready to call.
    fn functions(&self) -> &BoxedSlice<LocalFunctionIndex, VMLocalFunction>;

    /// Passive table elements.
    fn passive_elements(&self) -> &BTreeMap<ElemIndex, Box<[FunctionIndex]>>;

    /// Table initializers.
    fn element_segments(&self) -> &[OwnedTableInitializer];

    /// Memory initializers.
    /// TODO: consider making it an iterator of `DataInitializer`s instead?
    fn data_segments(&self) -> &[OwnedDataInitializer];

    /// Passive table elements.
    fn globals(&self) -> &[(GlobalType, GlobalInit)];

    /// The function index to the start function.
    fn start_function(&self) -> Option<FunctionIndex>;

    /// Function by export name.
    fn export_field(&self, name: &str) -> Option<wasmer_types::ExportIndex>;

    /// Mapping between module SignatureIndex and VMSharedSignatureIndex.
    fn signatures(&self) -> &[VMSharedSignatureIndex];

    /// Obtain the function signature for either the import or local definition.
    fn function_signature(&self, index: FunctionIndex) -> Option<VMSharedSignatureIndex>;
}

impl dyn Artifact {
    /// Downcast a dynamic Executable object to a concrete implementation of the trait.
    pub fn downcast_arc<T: Artifact + 'static>(self: Arc<Self>) -> Result<Arc<T>, Arc<Self>> {
        if std::any::TypeId::of::<T>() == Artifact::type_id(&*self, private::Internal(())) {
            // SAFETY: err, its probably sound, we effectively construct a transmute here.
            unsafe {
                let ptr = Arc::into_raw(self).cast::<T>();
                Ok(Arc::from_raw(ptr))
            }
        } else {
            Err(self)
        }
    }
}

'''
'''--- lib/vm/src/export.rs ---
// This file contains code from external sources.
// Attributions: https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md

use crate::global::Global;
use crate::instance::WeakOrStrongInstanceRef;
use crate::memory::{Memory, MemoryStyle};
use crate::table::{Table, TableStyle};
use crate::vmcontext::{VMFunctionBody, VMFunctionEnvironment, VMFunctionKind, VMTrampoline};
use crate::VMSharedSignatureIndex;
use std::sync::Arc;
use wasmer_types::{MemoryType, TableType};

/// The value of an export passed from one instance to another.
#[derive(Debug)]
pub enum VMExtern {
    /// A function export value.
    Function(VMFunction),

    /// A table export value.
    Table(VMTable),

    /// A memory export value.
    Memory(VMMemory),

    /// A global export value.
    Global(VMGlobal),
}

/// A function export value.
#[derive(Clone, Debug, PartialEq)]
pub struct VMFunction {
    /// The address of the native-code function.
    pub address: *const VMFunctionBody,

    /// Pointer to the containing `VMContext`.
    pub vmctx: VMFunctionEnvironment,

    /// The function type, used for compatibility checking.
    pub signature: VMSharedSignatureIndex,

    /// The function kind (specifies the calling convention for the
    /// function).
    pub kind: VMFunctionKind,

    /// Address of the function call trampoline owned by the same
    /// VMContext that owns the VMFunctionBody.
    ///
    /// May be `None` when the function is a host function (`FunctionType`
    /// == `Dynamic` or `vmctx` == `nullptr`).
    pub call_trampoline: Option<VMTrampoline>,

    /// A ‚Äúreference‚Äù to the instance through the
    /// `InstanceRef`. `None` if it is a host function.
    pub instance_ref: Option<WeakOrStrongInstanceRef>,
}

impl VMFunction {
    /// Converts the stored instance ref into a strong `InstanceRef` if it is weak.
    /// Returns None if it cannot be upgraded.
    pub fn upgrade_instance_ref(&mut self) -> Option<()> {
        if let Some(ref mut ir) = self.instance_ref {
            *ir = ir.upgrade()?;
        }
        Some(())
    }
}

/// # Safety
/// There is no non-threadsafe logic directly in this type. Calling the function
/// may not be threadsafe.
unsafe impl Send for VMFunction {}
/// # Safety
/// The members of an VMFunction are immutable after construction.
unsafe impl Sync for VMFunction {}

impl From<VMFunction> for VMExtern {
    fn from(func: VMFunction) -> Self {
        Self::Function(func)
    }
}

/// A table export value.
#[derive(Clone, Debug)]
pub struct VMTable {
    /// Pointer to the containing `Table`.
    pub from: Arc<dyn Table>,

    /// A ‚Äúreference‚Äù to the instance through the
    /// `InstanceRef`. `None` if it is a host table.
    pub instance_ref: Option<WeakOrStrongInstanceRef>,
}

/// # Safety
/// This is correct because there is no non-threadsafe logic directly in this type;
/// correct use of the raw table from multiple threads via `definition` requires `unsafe`
/// and is the responsibilty of the user of this type.
unsafe impl Send for VMTable {}

/// # Safety
/// This is correct because the values directly in `definition` should be considered immutable
/// and the type is both `Send` and `Clone` (thus marking it `Sync` adds no new behavior, it
/// only makes this type easier to use)
unsafe impl Sync for VMTable {}

impl VMTable {
    /// Get the table type for this exported table
    pub fn ty(&self) -> &TableType {
        self.from.ty()
    }

    /// Get the style for this exported table
    pub fn style(&self) -> &TableStyle {
        self.from.style()
    }

    /// Returns whether or not the two `VMTable`s refer to the same Memory.
    pub fn same(&self, other: &Self) -> bool {
        Arc::ptr_eq(&self.from, &other.from)
    }

    /// Converts the stored instance ref into a strong `InstanceRef` if it is weak.
    /// Returns None if it cannot be upgraded.
    pub fn upgrade_instance_ref(&mut self) -> Option<()> {
        if let Some(ref mut ir) = self.instance_ref {
            *ir = ir.upgrade()?;
        }
        Some(())
    }
}

impl From<VMTable> for VMExtern {
    fn from(table: VMTable) -> Self {
        Self::Table(table)
    }
}

/// A memory export value.
#[derive(Debug, Clone)]
pub struct VMMemory {
    /// Pointer to the containing `Memory`.
    pub from: Arc<dyn Memory>,

    /// A ‚Äúreference‚Äù to the instance through the
    /// `InstanceRef`. `None` if it is a host memory.
    pub instance_ref: Option<WeakOrStrongInstanceRef>,
}

/// # Safety
/// This is correct because there is no non-threadsafe logic directly in this type;
/// correct use of the raw memory from multiple threads via `definition` requires `unsafe`
/// and is the responsibilty of the user of this type.
unsafe impl Send for VMMemory {}

/// # Safety
/// This is correct because the values directly in `definition` should be considered immutable
/// and the type is both `Send` and `Clone` (thus marking it `Sync` adds no new behavior, it
/// only makes this type easier to use)
unsafe impl Sync for VMMemory {}

impl VMMemory {
    /// Get the type for this exported memory
    pub fn ty(&self) -> MemoryType {
        self.from.ty()
    }

    /// Get the style for this exported memory
    pub fn style(&self) -> &MemoryStyle {
        self.from.style()
    }

    /// Returns whether or not the two `VMMemory`s refer to the same Memory.
    pub fn same(&self, other: &Self) -> bool {
        Arc::ptr_eq(&self.from, &other.from)
    }

    /// Converts the stored instance ref into a strong `InstanceRef` if it is weak.
    /// Returns None if it cannot be upgraded.
    pub fn upgrade_instance_ref(&mut self) -> Option<()> {
        if let Some(ref mut ir) = self.instance_ref {
            *ir = ir.upgrade()?;
        }
        Some(())
    }
}

impl From<VMMemory> for VMExtern {
    fn from(memory: VMMemory) -> Self {
        Self::Memory(memory)
    }
}

/// A global export value.
#[derive(Debug, Clone)]
pub struct VMGlobal {
    /// The global declaration, used for compatibility checking.
    pub from: Arc<Global>,

    /// A ‚Äúreference‚Äù to the instance through the
    /// `InstanceRef`. `None` if it is a host global.
    pub instance_ref: Option<WeakOrStrongInstanceRef>,
}

/// # Safety
/// This is correct because there is no non-threadsafe logic directly in this type;
/// correct use of the raw global from multiple threads via `definition` requires `unsafe`
/// and is the responsibilty of the user of this type.
unsafe impl Send for VMGlobal {}

/// # Safety
/// This is correct because the values directly in `definition` should be considered immutable
/// from the perspective of users of this type and the type is both `Send` and `Clone` (thus
/// marking it `Sync` adds no new behavior, it only makes this type easier to use)
unsafe impl Sync for VMGlobal {}

impl VMGlobal {
    /// Returns whether or not the two `VMGlobal`s refer to the same Global.
    pub fn same(&self, other: &Self) -> bool {
        Arc::ptr_eq(&self.from, &other.from)
    }

    /// Converts the stored instance ref into a strong `InstanceRef` if it is weak.
    /// Returns None if it cannot be upgraded.
    pub fn upgrade_instance_ref(&mut self) -> Option<()> {
        if let Some(ref mut ir) = self.instance_ref {
            *ir = ir.upgrade()?;
        }
        Some(())
    }
}

impl From<VMGlobal> for VMExtern {
    fn from(global: VMGlobal) -> Self {
        Self::Global(global)
    }
}

'''
'''--- lib/vm/src/func_data_registry.rs ---
//! A registry for `VMFuncRef`s. This allows us to deduplicate funcrefs so that
//! identical `VMCallerCheckedAnyfunc`s will give us identical funcrefs.
//!
//! This registry also helps ensure that the `VMFuncRef`s can stay valid for as
//! long as we need them to.

use crate::vmcontext::VMCallerCheckedAnyfunc;
use std::collections::HashMap;
use std::sync::Mutex;

/// The registry that holds the values that `VMFuncRef`s point to.
#[derive(Debug)]
pub struct FuncDataRegistry {
    // This structure is stored in an `Engine` and is intended to be shared
    // across many instances. Ideally instances can themselves be sent across
    // threads, and ideally we can compile across many threads. As a result we
    // use interior mutability here with a lock to avoid having callers to
    // externally synchronize calls to compilation.
    inner: Mutex<Inner>,
}

// We use raw pointers but the data never moves, so it's not a problem
unsafe impl Send for FuncDataRegistry {}
unsafe impl Sync for FuncDataRegistry {}

/// A function reference. A single word that points to metadata about a function.
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash)]
pub struct VMFuncRef(pub(crate) *const VMCallerCheckedAnyfunc);

impl wasmer_types::NativeWasmType for VMFuncRef {
    const WASM_TYPE: wasmer_types::Type = wasmer_types::Type::FuncRef;
    type Abi = Self;

    #[inline]
    fn from_abi(abi: Self::Abi) -> Self {
        abi
    }

    #[inline]
    fn into_abi(self) -> Self::Abi {
        self
    }

    #[inline]
    fn to_binary(self) -> i128 {
        self.0 as _
    }

    #[inline]
    fn from_binary(bits: i128) -> Self {
        // TODO: ensure that the safety invariants are actually upheld here
        Self(bits as _)
    }
}

impl VMFuncRef {
    /// Check if the FuncRef is null
    // TODO: make this const when `std::ptr::is_null` is const
    pub fn is_null(&self) -> bool {
        self.0.is_null()
    }

    /// Create a new null FuncRef
    pub const fn null() -> Self {
        Self(std::ptr::null())
    }
}

impl std::ops::Deref for VMFuncRef {
    type Target = *const VMCallerCheckedAnyfunc;

    fn deref(&self) -> &Self::Target {
        &self.0
    }
}

impl std::ops::DerefMut for VMFuncRef {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.0
    }
}

// We use raw pointers but the data never moves, so it's not a problem
// TODO: update docs
unsafe impl Send for VMFuncRef {}
unsafe impl Sync for VMFuncRef {}

#[derive(Debug, Default)]
struct Inner {
    func_data: Vec<VMCallerCheckedAnyfunc>,
    anyfunc_to_index: HashMap<VMCallerCheckedAnyfunc, usize>,
}

impl FuncDataRegistry {
    /// Create a new `FuncDataRegistry`.
    pub fn new() -> Self {
        Self {
            inner: Default::default(),
        }
    }

    /// Register a signature and return its unique index.
    pub fn register(&self, anyfunc: VMCallerCheckedAnyfunc) -> VMFuncRef {
        let mut inner = self.inner.lock().unwrap();
        let data = if let Some(&idx) = inner.anyfunc_to_index.get(&anyfunc) {
            &inner.func_data[idx]
        } else {
            let idx = inner.func_data.len();
            inner.func_data.push(anyfunc);
            inner.anyfunc_to_index.insert(anyfunc, idx);
            &inner.func_data[idx]
        };
        VMFuncRef(data)
    }
}

'''
'''--- lib/vm/src/global.rs ---
use crate::vmcontext::VMGlobalDefinition;
use std::cell::UnsafeCell;
use std::ptr::NonNull;
use std::sync::Mutex;
use thiserror::Error;
use wasmer_types::{GlobalType, Mutability, Type, Value, WasmValueType};

#[derive(Debug)]
/// A Global instance
pub struct Global {
    ty: GlobalType,
    // TODO: this box is unnecessary
    vm_global_definition: Box<UnsafeCell<VMGlobalDefinition>>,
    // used to synchronize gets/sets
    lock: Mutex<()>,
}

/// # Safety
/// This is safe to send between threads because there is no-thread specific logic.
/// TODO: look into other reasons that make something not `Send`
unsafe impl Send for Global {}
/// # Safety
/// This is safe to share between threads because it uses a `Mutex` internally.
unsafe impl Sync for Global {}

/// Error type describing things that can go wrong when operating on Wasm Globals.
#[derive(Error, Debug, Clone, PartialEq, Hash)]
pub enum GlobalError {
    /// The error returned when attempting to set an immutable global.
    #[error("Attempted to set an immutable global")]
    ImmutableGlobalCannotBeSet,

    /// The error returned when attempting to operate on a global as a specific type
    /// that differs from the global's own type.
    #[error("Attempted to operate on a global of type {expected} as a global of type {found}")]
    IncorrectType {
        /// The type that the global is.
        expected: Type,
        /// The type that we were asked to use it as.
        found: Type,
    },
}

impl Global {
    /// Create a new, zero bit-pattern initialized global from a [`GlobalType`].
    pub fn new(global_type: GlobalType) -> Self {
        Self {
            ty: global_type,
            vm_global_definition: Box::new(UnsafeCell::new(VMGlobalDefinition::new())),
            lock: Mutex::new(()),
        }
    }

    /// Get the type of the global.
    pub fn ty(&self) -> &GlobalType {
        &self.ty
    }

    /// Get a pointer to the underlying definition used by the generated code.
    pub fn vmglobal(&self) -> NonNull<VMGlobalDefinition> {
        let ptr = self.vm_global_definition.as_ref() as *const UnsafeCell<VMGlobalDefinition>
            as *const VMGlobalDefinition as *mut VMGlobalDefinition;
        unsafe { NonNull::new_unchecked(ptr) }
    }

    /// Get a value from the global.
    // TODO(reftypes): the `&dyn Any` here for `Store` is a work-around for the fact
    // that `Store` is defined in `API` when we need it earlier. Ideally this should
    // be removed.
    pub fn get<T: WasmValueType>(&self, store: &dyn std::any::Any) -> Value<T> {
        let _global_guard = self.lock.lock().unwrap();
        unsafe {
            let definition = &*self.vm_global_definition.get();
            match self.ty().ty {
                Type::I32 => Value::I32(definition.to_i32()),
                Type::I64 => Value::I64(definition.to_i64()),
                Type::F32 => Value::F32(definition.to_f32()),
                Type::F64 => Value::F64(definition.to_f64()),
                Type::V128 => Value::V128(definition.to_u128()),
                Type::ExternRef => Value::ExternRef(definition.to_externref().into()),
                Type::FuncRef => {
                    let p = definition.to_u128() as i128;
                    if p as usize == 0 {
                        Value::FuncRef(None)
                    } else {
                        let v = T::read_value_from(store, &p);
                        Value::FuncRef(Some(v))
                    }
                }
            }
        }
    }

    /// Set a value for the global.
    ///
    /// # Safety
    /// The caller should check that the `val` comes from the same store as this global.
    pub unsafe fn set<T: WasmValueType>(&self, val: Value<T>) -> Result<(), GlobalError> {
        let _global_guard = self.lock.lock().unwrap();
        if self.ty().mutability != Mutability::Var {
            return Err(GlobalError::ImmutableGlobalCannotBeSet);
        }
        if val.ty() != self.ty().ty {
            return Err(GlobalError::IncorrectType {
                expected: self.ty.ty,
                found: val.ty(),
            });
        }
        self.set_unchecked(val)
    }

    /// Set a value from the global (unchecked)
    ///
    /// # Safety
    /// The caller should check that the `val` comes from the same store as this global.
    /// The caller should also ensure that this global is synchronized. Otherwise, use
    /// `set` instead.
    pub unsafe fn set_unchecked<T: WasmValueType>(&self, val: Value<T>) -> Result<(), GlobalError> {
        // ideally we'd use atomics for the global value rather than needing to lock it
        let definition = &mut *self.vm_global_definition.get();
        match val {
            Value::I32(i) => *definition.as_i32_mut() = i,
            Value::I64(i) => *definition.as_i64_mut() = i,
            Value::F32(f) => *definition.as_f32_mut() = f,
            Value::F64(f) => *definition.as_f64_mut() = f,
            Value::V128(x) => *definition.as_bytes_mut() = x.to_ne_bytes(),
            Value::ExternRef(r) => {
                let extern_ref = definition.as_externref_mut();
                extern_ref.ref_drop();
                *extern_ref = r.into()
            }
            Value::FuncRef(None) => *definition.as_u128_mut() = 0,
            Value::FuncRef(Some(r)) => {
                r.write_value_to(definition.as_u128_mut() as *mut u128 as *mut i128)
            }
        }
        Ok(())
    }
}

'''
'''--- lib/vm/src/imports.rs ---
// This file contains code from external sources.
// Attributions: https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md

use crate::instance::ImportFunctionEnv;
use crate::vmcontext::{VMFunctionImport, VMGlobalImport, VMMemoryImport, VMTableImport};
use crate::{VMSharedSignatureIndex, VMTrampoline};
use wasmer_types::entity::{BoxedSlice, PrimaryMap};
use wasmer_types::{FunctionIndex, GlobalIndex, MemoryIndex, TableIndex};

/// Type of the import.
pub enum VMImportType {
    /// A function import.
    Function {
        /// Signature for the function import.
        sig: VMSharedSignatureIndex,
        /// Trampoline to use for functions that use [`VMFunctionKind::Static`].
        static_trampoline: VMTrampoline,
    },
    /// A global.
    Global(wasmer_types::GlobalType),
    /// A table.
    Table(wasmer_types::TableType),
    /// Some memory.
    Memory(wasmer_types::MemoryType, crate::MemoryStyle),
}

/// A module import.
pub struct VMImport {
    /// This is passed to the `resolve` method.
    ///
    /// This index is shared between different import types.
    pub import_no: u32,
    /// The module name.
    pub module: String,
    /// The field name.
    pub field: String,
    /// Type of the import.
    pub ty: VMImportType,
}

/// Resolved import pointers.
#[derive(Clone)]
pub struct Imports {
    /// Resolved addresses for imported functions.
    pub functions: BoxedSlice<FunctionIndex, VMFunctionImport>,

    /// Initializers for host function environments. This is split out from `functions`
    /// because the generated code never needs to touch this and the extra wasted
    /// space may affect Wasm runtime performance due to increased cache pressure.
    ///
    /// We make it optional so that we can free the data after use.
    ///
    /// We move this data in `get_imported_function_envs` because there's
    /// no value to keeping it around; host functions must be initialized
    /// exactly once so we save some memory and improve correctness by
    /// moving this data.
    pub host_function_env_initializers: Option<BoxedSlice<FunctionIndex, ImportFunctionEnv>>,

    /// Resolved addresses for imported tables.
    pub tables: BoxedSlice<TableIndex, VMTableImport>,

    /// Resolved addresses for imported memories.
    pub memories: BoxedSlice<MemoryIndex, VMMemoryImport>,

    /// Resolved addresses for imported globals.
    pub globals: BoxedSlice<GlobalIndex, VMGlobalImport>,
}

impl Imports {
    /// Construct a new `Imports` instance.
    pub fn new(
        function_imports: PrimaryMap<FunctionIndex, VMFunctionImport>,
        host_function_env_initializers: PrimaryMap<FunctionIndex, ImportFunctionEnv>,
        table_imports: PrimaryMap<TableIndex, VMTableImport>,
        memory_imports: PrimaryMap<MemoryIndex, VMMemoryImport>,
        global_imports: PrimaryMap<GlobalIndex, VMGlobalImport>,
    ) -> Self {
        Self {
            functions: function_imports.into_boxed_slice(),
            host_function_env_initializers: Some(host_function_env_initializers.into_boxed_slice()),
            tables: table_imports.into_boxed_slice(),
            memories: memory_imports.into_boxed_slice(),
            globals: global_imports.into_boxed_slice(),
        }
    }

    /// Construct a new `Imports` instance with no imports.
    pub fn none() -> Self {
        Self {
            functions: PrimaryMap::new().into_boxed_slice(),
            host_function_env_initializers: None,
            tables: PrimaryMap::new().into_boxed_slice(),
            memories: PrimaryMap::new().into_boxed_slice(),
            globals: PrimaryMap::new().into_boxed_slice(),
        }
    }

    /// Get the `WasmerEnv::init_with_instance` function pointers and the pointers
    /// to the envs to call it on.
    ///
    /// This function can only be called once, it deletes the data it returns after
    /// returning it to ensure that it's not called more than once.
    pub fn get_imported_function_envs(&mut self) -> BoxedSlice<FunctionIndex, ImportFunctionEnv> {
        self.host_function_env_initializers
            .take()
            .unwrap_or_else(|| PrimaryMap::new().into_boxed_slice())
    }
}

'''
'''--- lib/vm/src/instance/allocator.rs ---
use super::{Instance, InstanceRef};
use crate::vmcontext::{VMMemoryDefinition, VMTableDefinition};
use crate::VMOffsets;
use std::alloc::{self, Layout};
use std::convert::TryFrom;
use std::mem;
use std::ptr::{self, NonNull};
use wasmer_types::entity::EntityRef;
use wasmer_types::{LocalMemoryIndex, LocalTableIndex};

/// This is an intermediate type that manages the raw allocation and
/// metadata when creating an [`Instance`].
///
/// This type will free the allocated memory if it's dropped before
/// being used.
///
/// It is important to remind that [`Instance`] is dynamically-sized
/// based on `VMOffsets`: The `Instance.vmctx` field represents a
/// dynamically-sized array that extends beyond the nominal end of the
/// type. So in order to create an instance of it, we must:
///
/// 1. Define the correct layout for `Instance` (size and alignment),
/// 2. Allocate it properly.
///
/// The [`InstanceAllocator::instance_layout`] computes the correct
/// layout to represent the wanted [`Instance`].
///
/// Then we use this layout to allocate an empty `Instance` properly.
pub struct InstanceAllocator {
    /// The buffer that will contain the [`Instance`] and dynamic fields.
    instance_ptr: NonNull<Instance>,

    /// The layout of the `instance_ptr` buffer.
    instance_layout: Layout,

    /// Information about the offsets into the `instance_ptr` buffer for
    /// the dynamic fields.
    offsets: VMOffsets,

    /// Whether or not this type has transferred ownership of the
    /// `instance_ptr` buffer. If it has not when being dropped,
    /// the buffer should be freed.
    consumed: bool,
}

impl Drop for InstanceAllocator {
    fn drop(&mut self) {
        if !self.consumed {
            // If `consumed` has not been set, then we still have ownership
            // over the buffer and must free it.
            let instance_ptr = self.instance_ptr.as_ptr();

            unsafe {
                std::alloc::dealloc(instance_ptr as *mut u8, self.instance_layout);
            }
        }
    }
}

impl InstanceAllocator {
    /// Allocates instance data for use with [`InstanceHandle::new`].
    ///
    /// Returns a wrapper type around the allocation and 2 vectors of
    /// pointers into the allocated buffer. These lists of pointers
    /// correspond to the location in memory for the local memories and
    /// tables respectively. These pointers should be written to before
    /// calling [`InstanceHandle::new`].
    ///
    /// [`InstanceHandle::new`]: super::InstanceHandle::new
    pub fn new(
        offsets: VMOffsets,
    ) -> (
        Self,
        Vec<NonNull<VMMemoryDefinition>>,
        Vec<NonNull<VMTableDefinition>>,
    ) {
        let instance_layout = Self::instance_layout(&offsets);

        #[allow(clippy::cast_ptr_alignment)]
        let instance_ptr = unsafe { alloc::alloc(instance_layout) as *mut Instance };

        let instance_ptr = if let Some(ptr) = NonNull::new(instance_ptr) {
            ptr
        } else {
            alloc::handle_alloc_error(instance_layout);
        };

        let allocator = Self {
            instance_ptr,
            instance_layout,
            offsets,
            consumed: false,
        };

        // # Safety
        // Both of these calls are safe because we allocate the pointer
        // above with the same `offsets` that these functions use.
        // Thus there will be enough valid memory for both of them.
        let memories = unsafe { allocator.memory_definition_locations() };
        let tables = unsafe { allocator.table_definition_locations() };

        (allocator, memories, tables)
    }

    /// Calculate the appropriate layout for the [`Instance`].
    fn instance_layout(offsets: &VMOffsets) -> Layout {
        let vmctx_size = usize::try_from(offsets.size_of_vmctx())
            .expect("Failed to convert the size of `vmctx` to a `usize`");

        let instance_vmctx_layout =
            Layout::array::<u8>(vmctx_size).expect("Failed to create a layout for `VMContext`");

        let (instance_layout, _offset) = Layout::new::<Instance>()
            .extend(instance_vmctx_layout)
            .expect("Failed to extend to `Instance` layout to include `VMContext`");

        instance_layout.pad_to_align()
    }

    /// Get the locations of where the local [`VMMemoryDefinition`]s should be stored.
    ///
    /// This function lets us create `Memory` objects on the host with backing
    /// memory in the VM.
    ///
    /// # Safety
    ///
    /// - `Self.instance_ptr` must point to enough memory that all of
    ///   the offsets in `Self.offsets` point to valid locations in
    ///   memory, i.e. `Self.instance_ptr` must have been allocated by
    ///   `Self::new`.
    unsafe fn memory_definition_locations(&self) -> Vec<NonNull<VMMemoryDefinition>> {
        let num_memories = self.offsets.num_local_memories;
        let num_memories = usize::try_from(num_memories).unwrap();
        let mut out = Vec::with_capacity(num_memories);

        // We need to do some pointer arithmetic now. The unit is `u8`.
        let ptr = self.instance_ptr.cast::<u8>().as_ptr();
        let base_ptr = ptr.add(mem::size_of::<Instance>());

        for i in 0..num_memories {
            let mem_offset = self
                .offsets
                .vmctx_vmmemory_definition(LocalMemoryIndex::new(i));
            let mem_offset = usize::try_from(mem_offset).unwrap();

            let new_ptr = NonNull::new_unchecked(base_ptr.add(mem_offset));

            out.push(new_ptr.cast());
        }

        out
    }

    /// Get the locations of where the [`VMTableDefinition`]s should be stored.
    ///
    /// This function lets us create [`Table`] objects on the host with backing
    /// memory in the VM.
    ///
    /// # Safety
    ///
    /// - `Self.instance_ptr` must point to enough memory that all of
    ///   the offsets in `Self.offsets` point to valid locations in
    ///   memory, i.e. `Self.instance_ptr` must have been allocated by
    ///   `Self::new`.
    unsafe fn table_definition_locations(&self) -> Vec<NonNull<VMTableDefinition>> {
        let num_tables = self.offsets.num_local_tables;
        let num_tables = usize::try_from(num_tables).unwrap();
        let mut out = Vec::with_capacity(num_tables);

        // We need to do some pointer arithmetic now. The unit is `u8`.
        let ptr = self.instance_ptr.cast::<u8>().as_ptr();
        let base_ptr = ptr.add(std::mem::size_of::<Instance>());

        for i in 0..num_tables {
            let table_offset = self
                .offsets
                .vmctx_vmtable_definition(LocalTableIndex::new(i));
            let table_offset = usize::try_from(table_offset).unwrap();

            let new_ptr = NonNull::new_unchecked(base_ptr.add(table_offset));

            out.push(new_ptr.cast());
        }
        out
    }

    /// Finish preparing by writing the [`Instance`] into memory, and
    /// consume this `InstanceAllocator`.
    pub(crate) fn write_instance(mut self, instance: Instance) -> InstanceRef {
        // Prevent the old state's drop logic from being called as we
        // transition into the new state.
        self.consumed = true;

        unsafe {
            // `instance` is moved at `Self.instance_ptr`. This
            // pointer has been allocated by `Self::allocate_instance`
            // (so by `InstanceRef::allocate_instance`).
            ptr::write(self.instance_ptr.as_ptr(), instance);
            // Now `instance_ptr` is correctly initialized!
        }
        let instance = self.instance_ptr;
        let instance_layout = self.instance_layout;

        // This is correct because of the invariants of `Self` and
        // because we write `Instance` to the pointer in this function.
        unsafe { InstanceRef::new(instance, instance_layout) }
    }
}

'''
'''--- lib/vm/src/instance/mod.rs ---
// This file contains code from external sources.
// Attributions: https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md

//! An `Instance` contains all the runtime state used by execution of
//! a WebAssembly module (except its callstack and register state). An
//! `InstanceRef` is a wrapper around `Instance` that manages
//! how it is allocated and deallocated. An `InstanceHandle` is a
//! wrapper around an `InstanceRef`.

mod allocator;
mod r#ref;

pub use allocator::InstanceAllocator;
pub use r#ref::{InstanceRef, WeakInstanceRef, WeakOrStrongInstanceRef};

use crate::func_data_registry::VMFuncRef;
use crate::global::Global;
use crate::imports::Imports;
use crate::memory::{Memory, MemoryError};
use crate::sig_registry::VMSharedSignatureIndex;
use crate::table::{Table, TableElement};
use crate::trap::traphandlers::get_trap_handler;
use crate::trap::{catch_traps, Trap, TrapCode};
use crate::vmcontext::{
    VMBuiltinFunctionsArray, VMCallerCheckedAnyfunc, VMContext, VMFunctionBody,
    VMFunctionEnvironment, VMFunctionImport, VMFunctionKind, VMGlobalDefinition, VMGlobalImport,
    VMLocalFunction, VMMemoryDefinition, VMMemoryImport, VMTableDefinition, VMTableImport,
};
use crate::{wasmer_call_trampoline, Artifact, VMOffsets, VMTrampoline};
use crate::{VMExtern, VMFunction, VMGlobal};
use memoffset::offset_of;
use more_asserts::assert_lt;
use std::any::Any;
use std::cell::RefCell;
use std::collections::BTreeMap;
use std::convert::TryFrom;
use std::ffi;
use std::fmt;
use std::mem;
use std::ptr::{self, NonNull};
use std::slice;
use std::sync::Arc;
use wasmer_types::entity::{packed_option::ReservedValue, BoxedSlice, EntityRef, PrimaryMap};
use wasmer_types::{
    DataIndex, DataInitializer, ElemIndex, ExportIndex, FastGasCounter, FunctionIndex, GlobalIndex,
    GlobalInit, InstanceConfig, LocalGlobalIndex, LocalMemoryIndex, LocalTableIndex, MemoryIndex,
    OwnedTableInitializer, Pages, TableIndex,
};

/// The function pointer to call with data and an [`Instance`] pointer to
/// finish initializing the host env.
pub type ImportInitializerFuncPtr<ResultErr = *mut ffi::c_void> =
    fn(*mut ffi::c_void, *const ffi::c_void) -> Result<(), ResultErr>;

/// A WebAssembly instance.
///
/// The type is dynamically-sized. Indeed, the `vmctx` field can
/// contain various data. That's why the type has a C representation
/// to ensure that the `vmctx` field is last. See the documentation of
/// the `vmctx` field to learn more.
#[repr(C)]
pub(crate) struct Instance {
    pub(crate) artifact: Arc<dyn Artifact>,

    /// External configuration for instance.
    config: InstanceConfig,

    /// WebAssembly linear memory data.
    memories: BoxedSlice<LocalMemoryIndex, Arc<dyn Memory>>,

    /// Table data...
    tables: BoxedSlice<LocalTableIndex, Arc<dyn Table>>,

    /// WebAssembly global data.
    globals: BoxedSlice<LocalGlobalIndex, Arc<Global>>,

    /// Passive elements in this instantiation. As `elem.drop`s happen, these
    /// entries get removed.
    passive_elements: RefCell<BTreeMap<ElemIndex, Box<[VMFuncRef]>>>,

    /// Passive data segments from our module. As `data.drop`s happen, entries
    /// get removed. A missing entry is considered equivalent to an empty slice.
    passive_data: RefCell<BTreeMap<DataIndex, Arc<[u8]>>>,

    /// Mapping of function indices to their func ref backing data. `VMFuncRef`s
    /// will point to elements here for functions defined or imported by this
    /// instance.
    funcrefs: BoxedSlice<FunctionIndex, VMCallerCheckedAnyfunc>,

    /// Hosts can store arbitrary per-instance information here.
    host_state: Box<dyn Any>,

    /// Functions to operate on host environments in the imports
    /// and pointers to the environments.
    ///
    /// TODO: Be sure to test with serialize/deserialize and imported
    /// functions from other Wasm modules.
    imported_function_envs: BoxedSlice<FunctionIndex, ImportFunctionEnv>,

    /// Additional context used by compiled WebAssembly code. This
    /// field is last, and represents a dynamically-sized array that
    /// extends beyond the nominal end of the struct (similar to a
    /// flexible array member).
    vmctx: VMContext,
}

/// A collection of data about host envs used by imported functions.
#[derive(Debug)]
pub enum ImportFunctionEnv {
    /// The `vmctx` pointer does not refer to a host env, there is no
    /// metadata about it.
    NoEnv,
    /// We're dealing with a user-defined host env.
    ///
    /// This host env may be either unwrapped (the user-supplied host env
    /// directly) or wrapped. i.e. in the case of Dynamic functions, we
    /// store our own extra data along with the user supplied env,
    /// thus the `env` pointer here points to the outermost type.
    Env {
        /// The function environment. This is not always the user-supplied
        /// env.
        env: *mut ffi::c_void,

        /// A clone function for duplicating the env.
        clone: fn(*mut ffi::c_void) -> *mut ffi::c_void,
        /// This field is not always present. When it is present, it
        /// should be set to `None` after use to prevent double
        /// initialization.
        initializer: Option<ImportInitializerFuncPtr>,
        /// The destructor to clean up the type in `env`.
        ///
        /// # Safety
        /// - This function must be called ina synchronized way. For
        ///   example, in the `Drop` implementation of this type.
        destructor: unsafe fn(*mut ffi::c_void),
    },
}

impl Clone for ImportFunctionEnv {
    fn clone(&self) -> Self {
        match &self {
            Self::NoEnv => Self::NoEnv,
            Self::Env {
                env,
                clone,
                destructor,
                initializer,
            } => {
                let new_env = (*clone)(*env);
                Self::Env {
                    env: new_env,
                    clone: *clone,
                    destructor: *destructor,
                    initializer: *initializer,
                }
            }
        }
    }
}

impl Drop for ImportFunctionEnv {
    fn drop(&mut self) {
        match self {
            Self::Env {
                env, destructor, ..
            } => {
                // # Safety
                // - This is correct because we know no other references
                //   to this data can exist if we're dropping it.
                unsafe {
                    (destructor)(*env);
                }
            }
            Self::NoEnv => (),
        }
    }
}

impl fmt::Debug for Instance {
    fn fmt(&self, formatter: &mut fmt::Formatter) -> fmt::Result {
        formatter.debug_struct("Instance").finish()
    }
}

#[allow(clippy::cast_ptr_alignment)]
impl Instance {
    /// Helper function to access various locations offset from our `*mut
    /// VMContext` object.
    unsafe fn vmctx_plus_offset<T>(&self, offset: u32) -> *mut T {
        (self.vmctx_ptr() as *mut u8)
            .add(usize::try_from(offset).unwrap())
            .cast()
    }

    /// Offsets in the `vmctx` region.
    fn offsets(&self) -> &VMOffsets {
        self.artifact.offsets()
    }

    /// Return a pointer to the `VMSharedSignatureIndex`s.
    fn signature_ids_ptr(&self) -> *mut VMSharedSignatureIndex {
        unsafe { self.vmctx_plus_offset(self.offsets().vmctx_signature_ids_begin()) }
    }

    /// Return the indexed `VMFunctionImport`.
    fn imported_function(&self, index: FunctionIndex) -> &VMFunctionImport {
        let index = usize::try_from(index.as_u32()).unwrap();
        unsafe { &*self.imported_functions_ptr().add(index) }
    }

    /// Return a pointer to the `VMFunctionImport`s.
    fn imported_functions_ptr(&self) -> *mut VMFunctionImport {
        unsafe { self.vmctx_plus_offset(self.offsets().vmctx_imported_functions_begin()) }
    }

    /// Return the index `VMTableImport`.
    fn imported_table(&self, index: TableIndex) -> &VMTableImport {
        let index = usize::try_from(index.as_u32()).unwrap();
        unsafe { &*self.imported_tables_ptr().add(index) }
    }

    /// Return a pointer to the `VMTableImports`s.
    fn imported_tables_ptr(&self) -> *mut VMTableImport {
        unsafe { self.vmctx_plus_offset(self.offsets().vmctx_imported_tables_begin()) }
    }

    /// Return the indexed `VMMemoryImport`.
    fn imported_memory(&self, index: MemoryIndex) -> &VMMemoryImport {
        let index = usize::try_from(index.as_u32()).unwrap();
        let addr = unsafe { self.imported_memories_ptr().add(index) };
        let align = std::mem::align_of::<VMMemoryImport>();
        debug_assert!(
            addr as usize % align == 0,
            "VMMemoryImport addr is not aligned to {}: {:p}",
            align,
            addr
        );
        unsafe { &*addr }
    }

    /// Return a pointer to the `VMMemoryImport`s.
    fn imported_memories_ptr(&self) -> *mut VMMemoryImport {
        unsafe { self.vmctx_plus_offset(self.offsets().vmctx_imported_memories_begin()) }
    }

    /// Return the indexed `VMGlobalImport`.
    fn imported_global(&self, index: GlobalIndex) -> &VMGlobalImport {
        let index = usize::try_from(index.as_u32()).unwrap();
        unsafe { &*self.imported_globals_ptr().add(index) }
    }

    /// Return a pointer to the `VMGlobalImport`s.
    fn imported_globals_ptr(&self) -> *mut VMGlobalImport {
        unsafe { self.vmctx_plus_offset(self.offsets().vmctx_imported_globals_begin()) }
    }

    /// Return the indexed `VMTableDefinition`.
    #[allow(unused)]
    fn table(&self, index: LocalTableIndex) -> VMTableDefinition {
        unsafe { *self.table_ptr(index).as_ref() }
    }

    /// Updates the value for a defined table to `VMTableDefinition`.
    #[allow(unused)]
    fn set_table(&self, index: LocalTableIndex, table: &VMTableDefinition) {
        unsafe {
            *self.table_ptr(index).as_ptr() = *table;
        }
    }

    /// Return the indexed `VMTableDefinition`.
    fn table_ptr(&self, index: LocalTableIndex) -> NonNull<VMTableDefinition> {
        let index = usize::try_from(index.as_u32()).unwrap();
        NonNull::new(unsafe { self.tables_ptr().add(index) }).unwrap()
    }

    /// Return a pointer to the `VMTableDefinition`s.
    fn tables_ptr(&self) -> *mut VMTableDefinition {
        unsafe { self.vmctx_plus_offset(self.offsets().vmctx_tables_begin()) }
    }

    /// Return the indexed `VMMemoryDefinition`.
    fn memory_definition(&self, index: MemoryIndex) -> &VMMemoryDefinition {
        match self.artifact.import_counts().local_memory_index(index) {
            Ok(local) => unsafe { self.memory_ptr(local).as_ref() },
            Err(import) => unsafe { &self.imported_memory(import).from.vmmemory().as_ref() },
        }
    }

    #[allow(dead_code)]
    /// Set the indexed memory to `VMMemoryDefinition`.
    fn set_memory(&self, index: LocalMemoryIndex, mem: &VMMemoryDefinition) {
        unsafe {
            *self.memory_ptr(index).as_ptr() = *mem;
        }
    }

    /// Return the indexed `VMMemoryDefinition`.
    fn memory_ptr(&self, index: LocalMemoryIndex) -> NonNull<VMMemoryDefinition> {
        let index = usize::try_from(index.as_u32()).unwrap();
        NonNull::new(unsafe { self.memories_ptr().add(index) }).unwrap()
    }

    /// Return a pointer to the `VMMemoryDefinition`s.
    fn memories_ptr(&self) -> *mut VMMemoryDefinition {
        unsafe { self.vmctx_plus_offset(self.offsets().vmctx_memories_begin()) }
    }

    /// Return the indexed `VMGlobalDefinition`.
    fn global(&self, index: GlobalIndex) -> &VMGlobalDefinition {
        match self.artifact.import_counts().local_global_index(index) {
            Ok(local) => unsafe { self.global_ptr(local).as_ref() },
            Err(import) => unsafe { self.imported_global(import).definition.as_ref() },
        }
    }

    /// Set the indexed global to `VMGlobalDefinition`.
    #[allow(dead_code)]
    fn set_global(&self, index: LocalGlobalIndex, global: &VMGlobalDefinition) {
        unsafe {
            *self.global_ptr(index).as_ptr() = global.clone();
        }
    }

    /// Return the indexed `VMGlobalDefinition`.
    fn global_ptr(&self, index: LocalGlobalIndex) -> NonNull<VMGlobalDefinition> {
        let index = usize::try_from(index.as_u32()).unwrap();
        // TODO:
        NonNull::new(unsafe { *self.globals_ptr().add(index) }).unwrap()
    }

    /// Return a pointer to the `VMGlobalDefinition`s.
    fn globals_ptr(&self) -> *mut *mut VMGlobalDefinition {
        unsafe { self.vmctx_plus_offset(self.offsets().vmctx_globals_begin()) }
    }

    /// Return a pointer to the `VMBuiltinFunctionsArray`.
    fn builtin_functions_ptr(&self) -> *mut VMBuiltinFunctionsArray {
        unsafe { self.vmctx_plus_offset(self.offsets().vmctx_builtin_functions_begin()) }
    }

    /// Return a reference to the vmctx used by compiled wasm code.
    fn vmctx(&self) -> &VMContext {
        &self.vmctx
    }

    /// Return a raw pointer to the vmctx used by compiled wasm code.
    fn vmctx_ptr(&self) -> *mut VMContext {
        self.vmctx() as *const VMContext as *mut VMContext
    }

    /// Return a reference to the custom state attached to this instance.
    #[inline]
    pub fn host_state(&self) -> &dyn Any {
        &*self.host_state
    }

    /// Return a pointer to the trap catcher.
    fn trap_catcher_ptr(&self) -> *mut *const u8 {
        unsafe { self.vmctx_plus_offset(self.offsets().vmctx_trap_handler()) }
    }

    /// Return a pointer to the gas limiter.
    pub fn gas_counter_ptr(&self) -> *mut *const FastGasCounter {
        unsafe { self.vmctx_plus_offset(self.offsets().vmctx_gas_limiter_pointer()) }
    }

    /// Return a pointer to initial stack limit.
    pub fn stack_limit_initial_ptr(&self) -> *mut i32 {
        unsafe { self.vmctx_plus_offset(self.offsets().vmctx_stack_limit_initial_begin()) }
    }

    /// Return a pointer to current stack limit.
    pub fn stack_limit_ptr(&self) -> *mut i32 {
        unsafe { self.vmctx_plus_offset(self.offsets().vmctx_stack_limit_begin()) }
    }

    /// Invoke the WebAssembly start function of the instance, if one is present.
    fn invoke_start_function(&self) -> Result<(), Trap> {
        let start_index = match self.artifact.start_function() {
            Some(idx) => idx,
            None => return Ok(()),
        };
        let start_funcref = self.funcrefs[start_index];
        // Make the call.
        self.reset_stack_meter();
        let result = unsafe {
            catch_traps(|| {
                mem::transmute::<*const VMFunctionBody, unsafe extern "C" fn(VMFunctionEnvironment)>(
                    start_funcref.func_ptr,
                )(start_funcref.vmctx)
            })
        };
        result
    }

    fn reset_stack_meter(&self) {
        unsafe {
            *(self.stack_limit_ptr()) = *(self.stack_limit_initial_ptr());
        }
    }

    /// Return the offset from the vmctx pointer to its containing `Instance`.
    #[inline]
    pub(crate) fn vmctx_offset() -> isize {
        offset_of!(Self, vmctx) as isize
    }

    /// Return the table index for the given `VMTableDefinition`.
    pub(crate) fn table_index(&self, table: &VMTableDefinition) -> LocalTableIndex {
        let begin: *const VMTableDefinition = self.tables_ptr() as *const _;
        let end: *const VMTableDefinition = table;
        // TODO: Use `offset_from` once it stablizes.
        let index = LocalTableIndex::new(
            (end as usize - begin as usize) / mem::size_of::<VMTableDefinition>(),
        );
        assert_lt!(index.index(), self.tables.len());
        index
    }

    /// Return the memory index for the given `VMMemoryDefinition`.
    pub(crate) fn memory_index(&self, memory: &VMMemoryDefinition) -> LocalMemoryIndex {
        let begin: *const VMMemoryDefinition = self.memories_ptr() as *const _;
        let end: *const VMMemoryDefinition = memory;
        // TODO: Use `offset_from` once it stablizes.
        let index = LocalMemoryIndex::new(
            (end as usize - begin as usize) / mem::size_of::<VMMemoryDefinition>(),
        );
        assert_lt!(index.index(), self.memories.len());
        index
    }

    /// Grow memory by the specified amount of pages.
    ///
    /// Returns `None` if memory can't be grown by the specified amount
    /// of pages.
    pub(crate) fn memory_grow<IntoPages>(
        &self,
        memory_index: LocalMemoryIndex,
        delta: IntoPages,
    ) -> Result<Pages, MemoryError>
    where
        IntoPages: Into<Pages>,
    {
        let mem = self
            .memories
            .get(memory_index)
            .unwrap_or_else(|| panic!("no memory for index {}", memory_index.index()));
        mem.grow(delta.into())
    }

    /// Grow imported memory by the specified amount of pages.
    ///
    /// Returns `None` if memory can't be grown by the specified amount
    /// of pages.
    ///
    /// # Safety
    /// This and `imported_memory_size` are currently unsafe because they
    /// dereference the memory import's pointers.
    pub(crate) unsafe fn imported_memory_grow<IntoPages>(
        &self,
        memory_index: MemoryIndex,
        delta: IntoPages,
    ) -> Result<Pages, MemoryError>
    where
        IntoPages: Into<Pages>,
    {
        let import = self.imported_memory(memory_index);
        import.from.grow(delta.into())
    }

    /// Returns the number of allocated wasm pages.
    pub(crate) fn memory_size(&self, memory_index: LocalMemoryIndex) -> Pages {
        self.memories
            .get(memory_index)
            .unwrap_or_else(|| panic!("no memory for index {}", memory_index.index()))
            .size()
    }

    /// Returns the number of allocated wasm pages in an imported memory.
    ///
    /// # Safety
    /// This and `imported_memory_grow` are currently unsafe because they
    /// dereference the memory import's pointers.
    pub(crate) unsafe fn imported_memory_size(&self, memory_index: MemoryIndex) -> Pages {
        self.imported_memory(memory_index).from.size()
    }

    /// Returns the number of elements in a given table.
    pub(crate) fn table_size(&self, table_index: LocalTableIndex) -> u32 {
        self.tables[table_index].size()
    }

    /// Returns the number of elements in a given imported table.
    ///
    /// # Safety
    /// `table_index` must be a valid, imported table index.
    pub(crate) unsafe fn imported_table_size(&self, table_index: TableIndex) -> u32 {
        self.imported_table(table_index).from.size()
    }

    /// Grow table by the specified amount of elements.
    ///
    /// Returns `None` if table can't be grown by the specified amount
    /// of elements.
    pub(crate) fn table_grow(
        &self,
        table_index: LocalTableIndex,
        delta: u32,
        init_value: TableElement,
    ) -> Option<u32> {
        let result = self
            .tables
            .get(table_index)
            .unwrap_or_else(|| panic!("no table for index {}", table_index.index()))
            .grow(delta, init_value);

        result
    }

    /// Grow table by the specified amount of elements.
    ///
    /// # Safety
    /// `table_index` must be a valid, imported table index.
    pub(crate) unsafe fn imported_table_grow(
        &self,
        table_index: TableIndex,
        delta: u32,
        init_value: TableElement,
    ) -> Option<u32> {
        let import = self.imported_table(table_index);
        import.from.grow(delta, init_value)
    }

    /// Get table element by index.
    pub(crate) fn table_get(
        &self,
        table_index: LocalTableIndex,
        index: u32,
    ) -> Option<TableElement> {
        self.tables
            .get(table_index)
            .unwrap_or_else(|| panic!("no table for index {}", table_index.index()))
            .get(index)
    }

    /// Returns the element at the given index.
    ///
    /// # Safety
    /// `table_index` must be a valid, imported table index.
    pub(crate) unsafe fn imported_table_get(
        &self,
        table_index: TableIndex,
        index: u32,
    ) -> Option<TableElement> {
        let import = self.imported_table(table_index);
        import.from.get(index)
    }

    /// Set table element by index.
    pub(crate) fn table_set(
        &self,
        table_index: LocalTableIndex,
        index: u32,
        val: TableElement,
    ) -> Result<(), Trap> {
        self.tables
            .get(table_index)
            .unwrap_or_else(|| panic!("no table for index {}", table_index.index()))
            .set(index, val)
    }

    /// Set table element by index for an imported table.
    ///
    /// # Safety
    /// `table_index` must be a valid, imported table index.
    pub(crate) unsafe fn imported_table_set(
        &self,
        table_index: TableIndex,
        index: u32,
        val: TableElement,
    ) -> Result<(), Trap> {
        let import = self.imported_table(table_index);
        import.from.set(index, val)
    }

    pub(crate) fn func_ref(&self, function_index: FunctionIndex) -> Option<VMFuncRef> {
        Some(self.get_vm_funcref(function_index))
    }

    /// Get a `VMFuncRef` for the given `FunctionIndex`.
    fn get_vm_funcref(&self, index: FunctionIndex) -> VMFuncRef {
        if index == FunctionIndex::reserved_value() {
            return VMFuncRef::null();
        }
        VMFuncRef(&self.funcrefs[index])
    }

    /// The `table.init` operation: initializes a portion of a table with a
    /// passive element.
    ///
    /// # Errors
    ///
    /// Returns a `Trap` error when the range within the table is out of bounds
    /// or the range within the passive element is out of bounds.
    pub(crate) fn table_init(
        &self,
        table_index: TableIndex,
        elem_index: ElemIndex,
        dst: u32,
        src: u32,
        len: u32,
    ) -> Result<(), Trap> {
        // https://webassembly.github.io/bulk-memory-operations/core/exec/instructions.html#exec-table-init

        let table = self.get_table(table_index);
        let passive_elements = self.passive_elements.borrow();
        let elem = passive_elements
            .get(&elem_index)
            .map_or::<&[VMFuncRef], _>(&[], |e| &**e);

        if src
            .checked_add(len)
            .map_or(true, |n| n as usize > elem.len())
            || dst.checked_add(len).map_or(true, |m| m > table.size())
        {
            return Err(Trap::lib(TrapCode::TableAccessOutOfBounds));
        }

        for (dst, src) in (dst..dst + len).zip(src..src + len) {
            table
                .set(dst, TableElement::FuncRef(elem[src as usize]))
                .expect("should never panic because we already did the bounds check above");
        }

        Ok(())
    }

    /// The `table.fill` operation: fills a portion of a table with a given value.
    ///
    /// # Errors
    ///
    /// Returns a `Trap` error when the range within the table is out of bounds
    pub(crate) fn table_fill(
        &self,
        table_index: TableIndex,
        start_index: u32,
        item: TableElement,
        len: u32,
    ) -> Result<(), Trap> {
        // https://webassembly.github.io/bulk-memory-operations/core/exec/instructions.html#exec-table-init

        let table = self.get_table(table_index);
        let table_size = table.size() as usize;

        if start_index
            .checked_add(len)
            .map_or(true, |n| n as usize > table_size)
        {
            return Err(Trap::lib(TrapCode::TableAccessOutOfBounds));
        }

        for i in start_index..(start_index + len) {
            table
                .set(i, item.clone())
                .expect("should never panic because we already did the bounds check above");
        }

        Ok(())
    }

    /// Drop an element.
    pub(crate) fn elem_drop(&self, elem_index: ElemIndex) {
        // https://webassembly.github.io/reference-types/core/exec/instructions.html#exec-elem-drop

        let mut passive_elements = self.passive_elements.borrow_mut();
        passive_elements.remove(&elem_index);
        // Note that we don't check that we actually removed an element because
        // dropping a non-passive element is a no-op (not a trap).
    }

    /// Do a `memory.copy` for a locally defined memory.
    ///
    /// # Errors
    ///
    /// Returns a `Trap` error when the source or destination ranges are out of
    /// bounds.
    pub(crate) fn local_memory_copy(
        &self,
        memory_index: LocalMemoryIndex,
        dst: u32,
        src: u32,
        len: u32,
    ) -> Result<(), Trap> {
        // https://webassembly.github.io/reference-types/core/exec/instructions.html#exec-memory-copy
        let memory = unsafe { self.memory_ptr(memory_index).as_ref() };
        // The following memory copy is not synchronized and is not atomic:
        unsafe { memory.memory_copy(dst, src, len) }
    }

    /// Perform a `memory.copy` on an imported memory.
    pub(crate) fn imported_memory_copy(
        &self,
        memory_index: MemoryIndex,
        dst: u32,
        src: u32,
        len: u32,
    ) -> Result<(), Trap> {
        let import = self.imported_memory(memory_index);
        // The following memory copy is not synchronized and is not atomic:
        unsafe { import.from.vmmemory().as_ref().memory_copy(dst, src, len) }
    }

    /// Perform the `memory.fill` operation on a locally defined memory.
    ///
    /// # Errors
    ///
    /// Returns a `Trap` error if the memory range is out of bounds.
    pub(crate) fn local_memory_fill(
        &self,
        memory_index: LocalMemoryIndex,
        dst: u32,
        val: u32,
        len: u32,
    ) -> Result<(), Trap> {
        let memory = unsafe { self.memory_ptr(memory_index).as_ref() };
        // The following memory fill is not synchronized and is not atomic:
        unsafe { memory.memory_fill(dst, val, len) }
    }

    /// Perform the `memory.fill` operation on an imported memory.
    ///
    /// # Errors
    ///
    /// Returns a `Trap` error if the memory range is out of bounds.
    pub(crate) fn imported_memory_fill(
        &self,
        memory_index: MemoryIndex,
        dst: u32,
        val: u32,
        len: u32,
    ) -> Result<(), Trap> {
        let import = self.imported_memory(memory_index);
        // The following memory fill is not synchronized and is not atomic:
        unsafe { import.from.vmmemory().as_ref().memory_fill(dst, val, len) }
    }

    /// Performs the `memory.init` operation.
    ///
    /// # Errors
    ///
    /// Returns a `Trap` error if the destination range is out of this module's
    /// memory's bounds or if the source range is outside the data segment's
    /// bounds.
    pub(crate) fn memory_init(
        &self,
        memory_index: MemoryIndex,
        data_index: DataIndex,
        dst: u32,
        src: u32,
        len: u32,
    ) -> Result<(), Trap> {
        // https://webassembly.github.io/bulk-memory-operations/core/exec/instructions.html#exec-memory-init

        let memory = self.memory_definition(memory_index);
        let passive_data = self.passive_data.borrow();
        let data = passive_data.get(&data_index).map_or(&[][..], |d| &**d);

        let oob_access = src
            .checked_add(len)
            .map_or(true, |n| n as usize > data.len())
            || dst.checked_add(len).map_or(true, |m| {
                usize::try_from(m).unwrap() > memory.current_length
            });

        if oob_access {
            return Err(Trap::lib(TrapCode::HeapAccessOutOfBounds));
        }
        let src_slice = &data[src as usize..(src + len) as usize];
        unsafe {
            let dst_start = memory.base.add(dst as usize);
            let dst_slice = slice::from_raw_parts_mut(dst_start, len as usize);
            dst_slice.copy_from_slice(src_slice);
        }
        Ok(())
    }

    /// Drop the given data segment, truncating its length to zero.
    pub(crate) fn data_drop(&self, data_index: DataIndex) {
        let mut passive_data = self.passive_data.borrow_mut();
        passive_data.remove(&data_index);
    }

    /// Get a table by index regardless of whether it is locally-defined or an
    /// imported, foreign table.
    pub(crate) fn get_table(&self, table_index: TableIndex) -> &dyn Table {
        match self.artifact.import_counts().local_table_index(table_index) {
            Ok(local) => self.get_local_table(local),
            Err(import) => self.get_foreign_table(import),
        }
    }

    /// Get a locally-defined table.
    pub(crate) fn get_local_table(&self, index: LocalTableIndex) -> &dyn Table {
        self.tables[index].as_ref()
    }

    /// Get an imported, foreign table.
    pub(crate) fn get_foreign_table(&self, index: TableIndex) -> &dyn Table {
        let import = self.imported_table(index);
        &*import.from
    }
}

/// A handle holding an `InstanceRef`, which holds an `Instance`
/// of a WebAssembly module.
///
/// This is more or less a public facade of the private `Instance`,
/// providing useful higher-level API.
#[derive(Debug, PartialEq)]
pub struct InstanceHandle {
    /// The [`InstanceRef`]. See its documentation to learn more.
    instance: InstanceRef,
}

impl InstanceHandle {
    /// Create a new `InstanceHandle` pointing at a new [`InstanceRef`].
    ///
    /// # Safety
    ///
    /// This method is not necessarily inherently unsafe to call, but in general
    /// the APIs of an `Instance` are quite unsafe and have not been really
    /// audited for safety that much. As a result the unsafety here on this
    /// method is a low-overhead way of saying ‚Äúthis is an extremely unsafe type
    /// to work with‚Äù.
    ///
    /// Extreme care must be taken when working with `InstanceHandle` and it's
    /// recommended to have relatively intimate knowledge of how it works
    /// internally if you'd like to do so. If possible it's recommended to use
    /// the `wasmer` crate API rather than this type since that is vetted for
    /// safety.
    ///
    /// However the following must be taken care of before calling this function:
    /// - The memory at `instance.tables_ptr()` must be initialized with data for
    ///   all the local tables.
    /// - The memory at `instance.memories_ptr()` must be initialized with data for
    ///   all the local memories.
    // FIXME: instances should just store a reference to an Artifact
    #[allow(clippy::too_many_arguments)]
    pub unsafe fn new(
        artifact: Arc<dyn Artifact>,
        allocator: InstanceAllocator,
        finished_memories: BoxedSlice<LocalMemoryIndex, Arc<dyn Memory>>,
        finished_tables: BoxedSlice<LocalTableIndex, Arc<dyn Table>>,
        finished_globals: BoxedSlice<LocalGlobalIndex, Arc<Global>>,
        imports: Imports,
        passive_data: BTreeMap<DataIndex, Arc<[u8]>>,
        host_state: Box<dyn Any>,
        imported_function_envs: BoxedSlice<FunctionIndex, ImportFunctionEnv>,
        instance_config: InstanceConfig,
    ) -> Self {
        let vmctx_globals = finished_globals
            .values()
            .map(|m| m.vmglobal())
            .collect::<PrimaryMap<LocalGlobalIndex, _>>()
            .into_boxed_slice();
        let passive_data = RefCell::new(passive_data);

        let handle = {
            // use dummy value to create an instance so we can get the vmctx pointer
            let funcrefs = PrimaryMap::new().into_boxed_slice();
            // Create the `Instance`. The unique, the One.
            let instance = Instance {
                artifact,
                config: instance_config.clone(),
                memories: finished_memories,
                tables: finished_tables,
                globals: finished_globals,
                passive_elements: Default::default(),
                passive_data,
                host_state,
                funcrefs,
                imported_function_envs,
                vmctx: VMContext {},
            };

            let mut instance_ref = allocator.write_instance(instance);

            // Set the funcrefs after we've built the instance
            {
                let instance = instance_ref.as_mut().unwrap();
                let vmctx_ptr = instance.vmctx_ptr();
                instance.funcrefs = build_funcrefs(
                    &imports,
                    instance.artifact.functions().iter().map(|(_, f)| f),
                    vmctx_ptr,
                );
                *(instance.trap_catcher_ptr()) = get_trap_handler();
                *(instance.gas_counter_ptr()) = instance_config.gas_counter;
                *(instance.stack_limit_ptr()) = instance_config.stack_limit;
                *(instance.stack_limit_initial_ptr()) = instance_config.stack_limit;
            }

            Self {
                instance: instance_ref,
            }
        };
        let instance = handle.instance().as_ref();

        ptr::copy(
            instance.artifact.signatures().as_ptr(),
            instance.signature_ids_ptr() as *mut VMSharedSignatureIndex,
            instance.artifact.signatures().len(),
        );

        ptr::copy(
            imports.functions.values().as_slice().as_ptr(),
            instance.imported_functions_ptr() as *mut VMFunctionImport,
            imports.functions.len(),
        );
        ptr::copy(
            imports.tables.values().as_slice().as_ptr(),
            instance.imported_tables_ptr() as *mut VMTableImport,
            imports.tables.len(),
        );
        ptr::copy(
            imports.memories.values().as_slice().as_ptr(),
            instance.imported_memories_ptr() as *mut VMMemoryImport,
            imports.memories.len(),
        );
        ptr::copy(
            imports.globals.values().as_slice().as_ptr(),
            instance.imported_globals_ptr() as *mut VMGlobalImport,
            imports.globals.len(),
        );
        // these should already be set, add asserts here? for:
        // - instance.tables_ptr() as *mut VMTableDefinition
        // - instance.memories_ptr() as *mut VMMemoryDefinition
        ptr::copy(
            vmctx_globals.values().as_slice().as_ptr(),
            instance.globals_ptr() as *mut NonNull<VMGlobalDefinition>,
            vmctx_globals.len(),
        );
        ptr::write(
            instance.builtin_functions_ptr() as *mut VMBuiltinFunctionsArray,
            VMBuiltinFunctionsArray::initialized(),
        );

        // Perform infallible initialization in this constructor, while fallible
        // initialization is deferred to the `initialize` method.
        initialize_passive_elements(instance);
        initialize_globals(instance);
        handle
    }

    /// Return a reference to the contained `Instance`.
    pub(crate) fn instance(&self) -> &InstanceRef {
        &self.instance
    }

    /// Finishes the instantiation process started by `Instance::new`.
    ///
    /// # Safety
    ///
    /// Only safe to call immediately after instantiation.
    pub unsafe fn finish_instantiation(&self) -> Result<(), Trap> {
        let instance = self.instance().as_ref();

        // Apply the initializers.
        initialize_tables(instance)?;
        initialize_memories(
            instance,
            instance.artifact.data_segments().iter().map(Into::into),
        )?;

        // The WebAssembly spec specifies that the start function is
        // invoked automatically at instantiation time.
        instance.invoke_start_function()?;
        Ok(())
    }

    /// See [`traphandlers::wasmer_call_trampoline`].
    pub unsafe fn invoke_function(
        &self,
        vmctx: VMFunctionEnvironment,
        trampoline: VMTrampoline,
        callee: *const VMFunctionBody,
        values_vec: *mut u8,
    ) -> Result<(), Trap> {
        // `vmctx` is always `*mut VMContext` here, as we call to WASM.
        {
            let instance = self.instance().as_ref();
            instance.reset_stack_meter();
        }
        wasmer_call_trampoline(vmctx, trampoline, callee, values_vec)
    }

    /// Return a reference to the vmctx used by compiled wasm code.
    pub fn vmctx(&self) -> &VMContext {
        self.instance().as_ref().vmctx()
    }

    /// Return a raw pointer to the vmctx used by compiled wasm code.
    pub fn vmctx_ptr(&self) -> *mut VMContext {
        self.instance().as_ref().vmctx_ptr()
    }

    /// Return a reference to the `VMOffsets` to get offsets in the
    /// `Self::vmctx_ptr` region. Be careful when doing pointer
    /// arithmetic!
    pub fn vmoffsets(&self) -> &VMOffsets {
        self.instance().as_ref().offsets()
    }

    /// Lookup an exported function with the specified function index.
    pub fn function_by_index(&self, idx: FunctionIndex) -> Option<VMFunction> {
        let instance = self.instance.as_ref();

        let (address, signature, vmctx, call_trampoline) =
            match instance.artifact.import_counts().local_function_index(idx) {
                Ok(local) => {
                    let func = instance.artifact.functions().get(local)?;
                    (
                        *(func.body),
                        func.signature,
                        VMFunctionEnvironment {
                            vmctx: instance.vmctx_ptr(),
                        },
                        Some(func.trampoline),
                    )
                }
                Err(import) => {
                    let import = instance.imported_function(import);
                    (
                        *(import.body),
                        import.signature,
                        import.environment,
                        import.trampoline,
                    )
                }
            };
        Some(VMFunction {
            // Any function received is already static at this point as:
            // 1. All locally defined functions in the Wasm have a static signature.
            // 2. All the imported functions are already static (because
            //    they point to the trampolines rather than the dynamic addresses).
            kind: VMFunctionKind::Static,
            address,
            signature,
            vmctx,
            call_trampoline,
            instance_ref: Some(WeakOrStrongInstanceRef::Strong(self.instance().clone())),
        })
    }

    /// Return the indexed `VMMemoryDefinition`.
    fn memory_by_index(&self, index: MemoryIndex) -> Option<crate::VMMemory> {
        let instance = self.instance.as_ref();
        let from = match instance.artifact.import_counts().local_memory_index(index) {
            Ok(local) => Arc::clone(&instance.memories[local]),
            Err(import) => Arc::clone(&instance.imported_memory(import).from),
        };
        Some(crate::VMMemory {
            from,
            instance_ref: Some(WeakOrStrongInstanceRef::Strong(self.instance().clone())),
        })
    }

    /// Return the indexed `VMMemoryDefinition`.
    fn table_by_index(&self, index: TableIndex) -> Option<crate::VMTable> {
        let instance = self.instance.as_ref();
        let from = match instance.artifact.import_counts().local_table_index(index) {
            Ok(local) => Arc::clone(&instance.tables[local]),
            Err(import) => Arc::clone(&instance.imported_table(import).from),
        };
        Some(crate::VMTable {
            from,
            instance_ref: Some(WeakOrStrongInstanceRef::Strong(self.instance().clone())),
        })
    }

    /// Obtain a reference to a global entity by its index.
    pub fn global_by_index(&self, index: GlobalIndex) -> Option<VMGlobal> {
        let instance = self.instance.as_ref();
        let from = match instance.artifact.import_counts().local_global_index(index) {
            Ok(local) => Arc::clone(&instance.globals[local]),
            Err(import) => Arc::clone(&instance.imported_global(import).from),
        };
        Some(crate::VMGlobal {
            from,
            instance_ref: Some(WeakOrStrongInstanceRef::Strong(self.instance().clone())),
        })
    }

    /// Lookup an exported function with the given name.
    pub fn lookup(&self, field: &str) -> Option<VMExtern> {
        let instance = self.instance.as_ref();
        Some(match instance.artifact.export_field(field)? {
            ExportIndex::Function(idx) => VMExtern::Function(self.function_by_index(idx)?),
            ExportIndex::Table(idx) => VMExtern::Table(self.table_by_index(idx)?),
            ExportIndex::Global(idx) => VMExtern::Global(self.global_by_index(idx)?),
            ExportIndex::Memory(idx) => VMExtern::Memory(self.memory_by_index(idx)?),
        })
    }

    /// Return a reference to the custom state attached to this instance.
    pub fn host_state(&self) -> &dyn Any {
        self.instance().as_ref().host_state()
    }

    /// Return the memory index for the given `VMMemoryDefinition` in this instance.
    pub fn memory_index(&self, memory: &VMMemoryDefinition) -> LocalMemoryIndex {
        self.instance().as_ref().memory_index(memory)
    }

    /// Grow memory in this instance by the specified amount of pages.
    ///
    /// Returns `None` if memory can't be grown by the specified amount
    /// of pages.
    pub fn memory_grow<IntoPages>(
        &self,
        memory_index: LocalMemoryIndex,
        delta: IntoPages,
    ) -> Result<Pages, MemoryError>
    where
        IntoPages: Into<Pages>,
    {
        self.instance().as_ref().memory_grow(memory_index, delta)
    }

    /// Return the table index for the given `VMTableDefinition` in this instance.
    pub fn table_index(&self, table: &VMTableDefinition) -> LocalTableIndex {
        self.instance().as_ref().table_index(table)
    }

    /// Grow table in this instance by the specified amount of pages.
    ///
    /// Returns `None` if memory can't be grown by the specified amount
    /// of pages.
    pub fn table_grow(
        &self,
        table_index: LocalTableIndex,
        delta: u32,
        init_value: TableElement,
    ) -> Option<u32> {
        self.instance()
            .as_ref()
            .table_grow(table_index, delta, init_value)
    }

    /// Get table element reference.
    ///
    /// Returns `None` if index is out of bounds.
    pub fn table_get(&self, table_index: LocalTableIndex, index: u32) -> Option<TableElement> {
        self.instance().as_ref().table_get(table_index, index)
    }

    /// Set table element reference.
    ///
    /// Returns an error if the index is out of bounds
    pub fn table_set(
        &self,
        table_index: LocalTableIndex,
        index: u32,
        val: TableElement,
    ) -> Result<(), Trap> {
        self.instance().as_ref().table_set(table_index, index, val)
    }

    /// Get a table defined locally within this module.
    pub fn get_local_table(&self, index: LocalTableIndex) -> &dyn Table {
        self.instance().as_ref().get_local_table(index)
    }
}

/// Initializes the host environments.
///
/// # Safety
/// - This function must be called with the correct `Err` type parameter: the error type is not
///   visible to code in `wasmer_vm`, so it's the caller's responsibility to ensure these
///   functions are called with the correct type.
/// - `instance_ptr` must point to a valid `wasmer::Instance`.
pub unsafe fn initialize_host_envs<Err: Sized>(
    handle: &std::sync::Mutex<InstanceHandle>,
    instance_ptr: *const ffi::c_void,
) -> Result<(), Err> {
    let initializers = {
        let mut instance_lock = handle.lock().unwrap();
        let instance_ref = instance_lock.instance.as_mut_unchecked();
        let mut initializers = vec![];
        for import_function_env in instance_ref.imported_function_envs.values_mut() {
            match import_function_env {
                ImportFunctionEnv::Env {
                    env,
                    ref mut initializer,
                    ..
                } => {
                    if let Some(init) = initializer.take() {
                        initializers.push((init, *env));
                    }
                }
                ImportFunctionEnv::NoEnv => (),
            }
        }
        initializers
    };
    for (init, env) in initializers {
        let f = mem::transmute::<&ImportInitializerFuncPtr, &ImportInitializerFuncPtr<Err>>(&init);
        f(env, instance_ptr)?;
    }
    Ok(())
}

/// Compute the offset for a memory data initializer.
fn get_memory_init_start(init: &DataInitializer<'_>, instance: &Instance) -> usize {
    let mut start = init.location.offset;
    if let Some(base) = init.location.base {
        let val = instance.global(base).to_u32();
        start += usize::try_from(val).unwrap();
    }
    start
}

#[allow(clippy::mut_from_ref)]
/// Return a byte-slice view of a memory's data.
unsafe fn get_memory_slice<'instance>(
    init: &DataInitializer<'_>,
    instance: &'instance Instance,
) -> &'instance mut [u8] {
    let memory = instance.memory_definition(init.location.memory_index);
    slice::from_raw_parts_mut(memory.base, memory.current_length)
}

/// Compute the offset for a table element initializer.
fn get_table_init_start(init: &OwnedTableInitializer, instance: &Instance) -> usize {
    let mut start = init.offset;
    if let Some(base) = init.base {
        let val = instance.global(base).to_u32();
        start += usize::try_from(val).unwrap();
    }
    start
}

/// Initialize the table memory from the provided initializers.
fn initialize_tables(instance: &Instance) -> Result<(), Trap> {
    for init in instance.artifact.element_segments() {
        let start = get_table_init_start(init, instance);
        let table = instance.get_table(init.table_index);

        if start
            .checked_add(init.elements.len())
            .map_or(true, |end| end > table.size() as usize)
        {
            return Err(Trap::lib(TrapCode::TableAccessOutOfBounds));
        }

        for (i, func_idx) in init.elements.iter().enumerate() {
            let anyfunc = instance.get_vm_funcref(*func_idx);
            table
                .set(
                    u32::try_from(start + i).unwrap(),
                    TableElement::FuncRef(anyfunc),
                )
                .unwrap();
        }
    }

    Ok(())
}

/// Initialize the `Instance::passive_elements` map by resolving the
/// `ModuleInfo::passive_elements`'s `FunctionIndex`s into `VMCallerCheckedAnyfunc`s for
/// this instance.
fn initialize_passive_elements(instance: &Instance) {
    let mut passive_elements = instance.passive_elements.borrow_mut();
    debug_assert!(
        passive_elements.is_empty(),
        "should only be called once, at initialization time"
    );

    passive_elements.extend(
        instance
            .artifact
            .passive_elements()
            .iter()
            .filter(|(_, segments)| !segments.is_empty())
            .map(|(idx, segments)| {
                (
                    *idx,
                    segments
                        .iter()
                        .map(|s| instance.get_vm_funcref(*s))
                        .collect(),
                )
            }),
    );
}

/// Initialize the table memory from the provided initializers.
fn initialize_memories<'a>(
    instance: &Instance,
    data_initializers: impl Iterator<Item = DataInitializer<'a>>,
) -> Result<(), Trap> {
    for init in data_initializers {
        let memory = instance.memory_definition(init.location.memory_index);

        let start = get_memory_init_start(&init, instance);
        if start
            .checked_add(init.data.len())
            .map_or(true, |end| end > memory.current_length)
        {
            return Err(Trap::lib(TrapCode::HeapAccessOutOfBounds));
        }

        unsafe {
            let mem_slice = get_memory_slice(&init, instance);
            let end = start + init.data.len();
            let to_init = &mut mem_slice[start..end];
            to_init.copy_from_slice(init.data);
        }
    }

    Ok(())
}

fn initialize_globals(instance: &Instance) {
    for (index, (_, initializer)) in instance.artifact.globals().iter().enumerate() {
        unsafe {
            let to = instance.global_ptr(LocalGlobalIndex::new(index)).as_ptr();
            match initializer {
                GlobalInit::I32Const(x) => *(*to).as_i32_mut() = *x,
                GlobalInit::I64Const(x) => *(*to).as_i64_mut() = *x,
                GlobalInit::F32Const(x) => *(*to).as_f32_mut() = *x,
                GlobalInit::F64Const(x) => *(*to).as_f64_mut() = *x,
                GlobalInit::V128Const(x) => *(*to).as_bytes_mut() = *x.bytes(),
                GlobalInit::GetGlobal(x) => *to = instance.global(*x).clone(),
                GlobalInit::RefNullConst => *(*to).as_funcref_mut() = VMFuncRef::null(),
                GlobalInit::RefFunc(func_idx) => {
                    let funcref = instance.func_ref(*func_idx).unwrap();
                    *(*to).as_funcref_mut() = funcref;
                }
            }
        }
    }
}

/// Eagerly builds all the `VMFuncRef`s for imported and local functions so that all
/// future funcref operations are just looking up this data.
pub fn build_funcrefs<'a>(
    imports: &Imports,
    finished_functions: impl ExactSizeIterator<Item = &'a VMLocalFunction>,
    // vmshared_signatures: &BoxedSlice<SignatureIndex, VMSharedSignatureIndex>,
    vmctx_ptr: *mut VMContext,
) -> BoxedSlice<FunctionIndex, VMCallerCheckedAnyfunc> {
    let mut func_refs =
        PrimaryMap::with_capacity(imports.functions.len() + finished_functions.len());
    for (_, import) in imports.functions.iter() {
        let anyfunc = VMCallerCheckedAnyfunc {
            func_ptr: *(import.body),
            type_index: import.signature,
            vmctx: import.environment,
        };
        func_refs.push(anyfunc);
    }
    // local functions
    for function in finished_functions {
        let anyfunc = VMCallerCheckedAnyfunc {
            func_ptr: *(function.body),
            type_index: function.signature,
            vmctx: VMFunctionEnvironment { vmctx: vmctx_ptr },
        };
        func_refs.push(anyfunc);
    }
    func_refs.into_boxed_slice()
}

'''
'''--- lib/vm/src/instance/ref.rs ---
use super::Instance;
use std::alloc::Layout;
use std::convert::TryFrom;
use std::ptr::{self, NonNull};
use std::sync::{Arc, Weak};

/// Dynamic instance allocation.
///
/// This structure has a C representation because `Instance` is
/// dynamically-sized, and the `instance` field must be last.
///
/// This `InstanceRef` must be freed with [`InstanceInner::deallocate_instance`]
/// if and only if it has been set correctly. The `Drop` implementation of
/// [`InstanceInner`] calls its `deallocate_instance` method without
/// checking if this property holds, only when `Self.strong` is equal to 1.
#[derive(Debug)]
#[repr(C)]
struct InstanceInner {
    /// The layout of `Instance` (which can vary).
    instance_layout: Layout,

    /// The `Instance` itself. It must be the last field of
    /// `InstanceRef` since `Instance` is dyamically-sized.
    ///
    /// `Instance` must not be dropped manually by Rust, because it's
    /// allocated manually with `alloc` and a specific layout (Rust
    /// would be able to drop `Instance` itself but it will imply a
    /// memory leak because of `alloc`).
    ///
    /// No one in the code has a copy of the `Instance`'s
    /// pointer. `Self` is the only one.
    instance: NonNull<Instance>,
}

impl InstanceInner {
    /// Deallocate `Instance`.
    ///
    /// # Safety
    ///
    /// `Self.instance` must be correctly set and filled before being
    /// dropped and deallocated.
    unsafe fn deallocate_instance(&mut self) {
        let instance_ptr = self.instance.as_ptr();

        ptr::drop_in_place(instance_ptr);
        std::alloc::dealloc(instance_ptr as *mut u8, self.instance_layout);
    }

    /// Get a reference to the `Instance`.
    #[inline]
    pub(crate) fn as_ref(&self) -> &Instance {
        // SAFETY: The pointer is properly aligned, it is
        // ‚Äúdereferencable‚Äù, it points to an initialized memory of
        // `Instance`, and the reference has the lifetime `'a`.
        unsafe { self.instance.as_ref() }
    }

    #[inline]
    pub(super) fn as_mut(&mut self) -> &mut Instance {
        unsafe { self.instance.as_mut() }
    }
}

impl PartialEq for InstanceInner {
    /// Two `InstanceInner` are equal if and only if
    /// `Self.instance` points to the same location.
    fn eq(&self, other: &Self) -> bool {
        self.instance == other.instance
    }
}

impl Drop for InstanceInner {
    /// Drop the `InstanceInner`.
    fn drop(&mut self) {
        unsafe { Self::deallocate_instance(self) };
    }
}

/// TODO: Review this super carefully.
unsafe impl Send for InstanceInner {}
unsafe impl Sync for InstanceInner {}

/// An `InstanceRef` is responsible to properly deallocate,
/// and to give access to an `Instance`, in such a way that `Instance`
/// is unique, can be shared, safely, across threads, without
/// duplicating the pointer in multiple locations. `InstanceRef`
/// must be the only ‚Äúowner‚Äù of an `Instance`.
///
/// Consequently, one must not share `Instance` but
/// `InstanceRef`. It acts like an Atomically Reference Counter
/// to `Instance`. In short, `InstanceRef` is roughly a
/// simplified version of `std::sync::Arc`.
///
/// Note for the curious reader: [`InstanceAllocator::new`]
/// and [`InstanceHandle::new`] will respectively allocate a proper
/// `Instance` and will fill it correctly.
///
/// A little bit of background: The initial goal was to be able to
/// share an [`Instance`] between an [`InstanceHandle`] and the module
/// exports, so that one can drop a [`InstanceHandle`] but still being
/// able to use the exports properly.
#[derive(Debug, PartialEq, Clone)]
pub struct InstanceRef(Arc<InstanceInner>);

impl InstanceRef {
    /// Create a new `InstanceRef`. It allocates nothing. It fills
    /// nothing. The `Instance` must be already valid and
    /// filled.
    ///
    /// # Safety
    ///
    /// `instance` must a non-null, non-dangling, properly aligned,
    /// and correctly initialized pointer to `Instance`. See
    /// [`InstanceAllocator`] for an example of how to correctly use
    /// this API.
    pub(super) unsafe fn new(instance: NonNull<Instance>, instance_layout: Layout) -> Self {
        Self(Arc::new(InstanceInner {
            instance_layout,
            instance,
        }))
    }

    /// Get a reference to the `Instance`.
    #[inline]
    pub(crate) fn as_ref(&self) -> &Instance {
        (&*self.0).as_ref()
    }

    /// Only succeeds if ref count is 1.
    #[inline]
    pub(super) fn as_mut(&mut self) -> Option<&mut Instance> {
        Some(Arc::get_mut(&mut self.0)?.as_mut())
    }

    /// Like [`InstanceRef::as_mut`] but always succeeds.
    /// May cause undefined behavior if used improperly.
    ///
    /// # Safety
    /// It is the caller's responsibility to ensure exclusivity and synchronization of the
    /// instance before calling this function. No other pointers to any Instance data
    /// should be dereferenced for the lifetime of the returned `&mut Instance`.
    #[inline]
    pub(super) unsafe fn as_mut_unchecked(&mut self) -> &mut Instance {
        let ptr: *mut InstanceInner = Arc::as_ptr(&self.0) as *mut _;
        (&mut *ptr).as_mut()
    }
}

/// A weak instance ref. This type does not keep the underlying `Instance` alive
/// but can be converted into a full `InstanceRef` if the underlying `Instance` hasn't
/// been deallocated.
#[derive(Debug, Clone)]
pub struct WeakInstanceRef(Weak<InstanceInner>);

impl PartialEq for WeakInstanceRef {
    fn eq(&self, other: &Self) -> bool {
        self.0.ptr_eq(&other.0)
    }
}

impl WeakInstanceRef {
    /// Try to convert into a strong, `InstanceRef`.
    pub fn upgrade(&self) -> Option<InstanceRef> {
        let inner = self.0.upgrade()?;
        Some(InstanceRef(inner))
    }
}

/// An `InstanceRef` that may or may not be keeping the `Instance` alive.
///
/// This type is useful for types that conditionally must keep / not keep the
/// underlying `Instance` alive. For example, to prevent cycles in `WasmerEnv`s.
#[derive(Debug, Clone, PartialEq)]
pub enum WeakOrStrongInstanceRef {
    /// A weak instance ref.
    Weak(WeakInstanceRef),
    /// A strong instance ref.
    Strong(InstanceRef),
}

impl WeakOrStrongInstanceRef {
    /// Tries to upgrade weak references to a strong reference, returning None
    /// if it can't be done.
    pub fn upgrade(&self) -> Option<Self> {
        match self {
            Self::Weak(weak) => weak.upgrade().map(Self::Strong),
            Self::Strong(strong) => Some(Self::Strong(strong.clone())),
        }
    }

    /// Clones self into a weak reference.
    pub fn downgrade(&self) -> Self {
        match self {
            Self::Weak(weak) => Self::Weak(weak.clone()),
            Self::Strong(strong) => Self::Weak(WeakInstanceRef(Arc::downgrade(&strong.0))),
        }
    }
}

impl TryFrom<WeakOrStrongInstanceRef> for InstanceRef {
    type Error = &'static str;
    fn try_from(value: WeakOrStrongInstanceRef) -> Result<Self, Self::Error> {
        match value {
            WeakOrStrongInstanceRef::Strong(strong) => Ok(strong),
            WeakOrStrongInstanceRef::Weak(weak) => {
                weak.upgrade().ok_or("Failed to upgrade weak reference")
            }
        }
    }
}

impl From<WeakOrStrongInstanceRef> for WeakInstanceRef {
    fn from(value: WeakOrStrongInstanceRef) -> Self {
        match value {
            WeakOrStrongInstanceRef::Strong(strong) => Self(Arc::downgrade(&strong.0)),
            WeakOrStrongInstanceRef::Weak(weak) => weak,
        }
    }
}

impl From<WeakInstanceRef> for WeakOrStrongInstanceRef {
    fn from(value: WeakInstanceRef) -> Self {
        Self::Weak(value)
    }
}

impl From<InstanceRef> for WeakOrStrongInstanceRef {
    fn from(value: InstanceRef) -> Self {
        Self::Strong(value)
    }
}

'''
'''--- lib/vm/src/lib.rs ---
//! Runtime library support for Wasmer.

#![deny(missing_docs, trivial_numeric_casts, unused_extern_crates)]
#![deny(trivial_numeric_casts, unused_extern_crates)]
#![warn(unused_import_braces)]
#![cfg_attr(
    feature = "cargo-clippy",
    allow(clippy::new_without_default, clippy::vtable_address_comparisons)
)]
#![cfg_attr(
    feature = "cargo-clippy",
    warn(
        clippy::float_arithmetic,
        clippy::mut_mut,
        clippy::nonminimal_bool,
        clippy::map_unwrap_or,
        clippy::option_map_unwrap_or_else,
        clippy::print_stdout,
        clippy::unicode_not_nfc,
        clippy::use_self
    )
)]

mod artifact;
mod export;
mod func_data_registry;
mod global;
mod imports;
mod instance;
mod memory;
mod mmap;
mod probestack;
mod resolver;
mod sig_registry;
mod table;
mod trap;
mod tunables;
mod vmcontext;
mod vmoffsets;

pub mod libcalls;

pub use crate::artifact::{Artifact, Instantiatable};
pub use crate::export::*;
pub use crate::func_data_registry::{FuncDataRegistry, VMFuncRef};
pub use crate::global::*;
pub use crate::imports::{Imports, VMImport, VMImportType};
pub use crate::instance::{
    initialize_host_envs, ImportFunctionEnv, ImportInitializerFuncPtr, InstanceAllocator,
    InstanceHandle, WeakOrStrongInstanceRef,
};
pub use crate::memory::{LinearMemory, Memory, MemoryError, MemoryStyle};
pub use crate::mmap::Mmap;
pub use crate::probestack::PROBESTACK;
pub use crate::resolver::{
    ChainableNamedResolver, Export, ExportFunction, ExportFunctionMetadata, NamedResolver,
    NamedResolverChain, NullResolver, Resolver,
};
pub use crate::sig_registry::{SignatureRegistry, VMSharedSignatureIndex};
pub use crate::table::{LinearTable, Table, TableElement, TableStyle};
pub use crate::trap::*;
pub use crate::tunables::Tunables;
pub use crate::vmcontext::{
    FunctionBodyPtr, FunctionExtent, SectionBodyPtr, VMBuiltinFunctionIndex,
    VMCallerCheckedAnyfunc, VMContext, VMDynamicFunctionContext, VMFunctionBody,
    VMFunctionEnvironment, VMFunctionImport, VMFunctionKind, VMGlobalDefinition, VMGlobalImport,
    VMLocalFunction, VMMemoryDefinition, VMMemoryImport, VMTableDefinition, VMTableImport,
    VMTrampoline,
};
pub use crate::vmoffsets::{TargetSharedSignatureIndex, VMOffsets};
#[deprecated(
    since = "2.1.0",
    note = "ModuleInfo, ExportsIterator, ImportsIterator should be imported from wasmer_types."
)]
pub use wasmer_types::ModuleInfo;
pub use wasmer_types::VMExternRef;

/// Version number of this crate.
pub const VERSION: &str = env!("CARGO_PKG_VERSION");

'''
'''--- lib/vm/src/libcalls.rs ---
// This file contains code from external sources.
// Attributions: https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md

//! Runtime library calls.
//!
//! Note that Wasm compilers may sometimes perform these inline rather than
//! calling them, particularly when CPUs have special instructions which compute
//! them directly.
//!
//! These functions are called by compiled Wasm code, and therefore must take
//! certain care about some things:
//!
//! * They must always be `pub extern "C"` and should only contain basic, raw
//!   i32/i64/f32/f64/pointer parameters that are safe to pass across the system
//!   ABI!
//!
//! * If any nested function propagates an `Err(trap)` out to the library
//!   function frame, we need to raise it. This involves some nasty and quite
//!   unsafe code under the covers! Notable, after raising the trap, drops
//!   **will not** be run for local variables! This can lead to things like
//!   leaking `InstanceHandle`s which leads to never deallocating JIT code,
//!   instances, and modules! Therefore, always use nested blocks to ensure
//!   drops run before raising a trap:
//!
//!   ```ignore
//!   pub extern "C" fn my_lib_function(...) {
//!       let result = {
//!           // Do everything in here so drops run at the end of the block.
//!           ...
//!       };
//!       if let Err(trap) = result {
//!           // Now we can safely raise the trap without leaking!
//!           raise_lib_trap(trap);
//!       }
//!   }
//!   ```

#![allow(missing_docs)] // For some reason lint fails saying that `LibCall` is not documented, when it actually is

use crate::func_data_registry::VMFuncRef;
use crate::probestack::PROBESTACK;
use crate::table::{RawTableElement, TableElement};
use crate::trap::{raise_lib_trap, Trap, TrapCode};
use crate::vmcontext::VMContext;
use crate::VMExternRef;
use std::fmt;
use wasmer_types::{
    DataIndex, ElemIndex, FunctionIndex, LocalMemoryIndex, LocalTableIndex, MemoryIndex,
    TableIndex, Type,
};

/// Implementation of f32.ceil
#[no_mangle]
pub extern "C" fn wasmer_vm_f32_ceil(x: f32) -> f32 {
    x.ceil()
}

/// Implementation of f32.floor
#[no_mangle]
pub extern "C" fn wasmer_vm_f32_floor(x: f32) -> f32 {
    x.floor()
}

/// Implementation of f32.trunc
#[no_mangle]
pub extern "C" fn wasmer_vm_f32_trunc(x: f32) -> f32 {
    x.trunc()
}

/// Implementation of f32.nearest
#[allow(clippy::float_arithmetic, clippy::float_cmp)]
#[no_mangle]
pub extern "C" fn wasmer_vm_f32_nearest(x: f32) -> f32 {
    // Rust doesn't have a nearest function, so do it manually.
    if x == 0.0 {
        // Preserve the sign of zero.
        x
    } else {
        // Nearest is either ceil or floor depending on which is nearest or even.
        let u = x.ceil();
        let d = x.floor();
        let um = (x - u).abs();
        let dm = (x - d).abs();
        if um < dm
            || (um == dm && {
                let h = u / 2.;
                h.floor() == h
            })
        {
            u
        } else {
            d
        }
    }
}

/// Implementation of f64.ceil
#[no_mangle]
pub extern "C" fn wasmer_vm_f64_ceil(x: f64) -> f64 {
    x.ceil()
}

/// Implementation of f64.floor
#[no_mangle]
pub extern "C" fn wasmer_vm_f64_floor(x: f64) -> f64 {
    x.floor()
}

/// Implementation of f64.trunc
#[no_mangle]
pub extern "C" fn wasmer_vm_f64_trunc(x: f64) -> f64 {
    x.trunc()
}

/// Implementation of f64.nearest
#[allow(clippy::float_arithmetic, clippy::float_cmp)]
#[no_mangle]
pub extern "C" fn wasmer_vm_f64_nearest(x: f64) -> f64 {
    // Rust doesn't have a nearest function, so do it manually.
    if x == 0.0 {
        // Preserve the sign of zero.
        x
    } else {
        // Nearest is either ceil or floor depending on which is nearest or even.
        let u = x.ceil();
        let d = x.floor();
        let um = (x - u).abs();
        let dm = (x - d).abs();
        if um < dm
            || (um == dm && {
                let h = u / 2.;
                h.floor() == h
            })
        {
            u
        } else {
            d
        }
    }
}

/// Implementation of memory.grow for locally-defined 32-bit memories.
///
/// # Safety
///
/// `vmctx` must be dereferenceable.
#[no_mangle]
pub unsafe extern "C" fn wasmer_vm_memory32_grow(
    vmctx: *mut VMContext,
    delta: u32,
    memory_index: u32,
) -> u32 {
    let instance = (&*vmctx).instance();
    let memory_index = LocalMemoryIndex::from_u32(memory_index);

    instance
        .memory_grow(memory_index, delta)
        .map(|pages| pages.0)
        .unwrap_or(u32::max_value())
}

/// Implementation of memory.grow for imported 32-bit memories.
///
/// # Safety
///
/// `vmctx` must be dereferenceable.
#[no_mangle]
pub unsafe extern "C" fn wasmer_vm_imported_memory32_grow(
    vmctx: *mut VMContext,
    delta: u32,
    memory_index: u32,
) -> u32 {
    let instance = (&*vmctx).instance();
    let memory_index = MemoryIndex::from_u32(memory_index);

    instance
        .imported_memory_grow(memory_index, delta)
        .map(|pages| pages.0)
        .unwrap_or(u32::max_value())
}

/// Implementation of memory.size for locally-defined 32-bit memories.
///
/// # Safety
///
/// `vmctx` must be dereferenceable.
#[no_mangle]
pub unsafe extern "C" fn wasmer_vm_memory32_size(vmctx: *mut VMContext, memory_index: u32) -> u32 {
    let instance = (&*vmctx).instance();
    let memory_index = LocalMemoryIndex::from_u32(memory_index);

    instance.memory_size(memory_index).0
}

/// Implementation of memory.size for imported 32-bit memories.
///
/// # Safety
///
/// `vmctx` must be dereferenceable.
#[no_mangle]
pub unsafe extern "C" fn wasmer_vm_imported_memory32_size(
    vmctx: *mut VMContext,
    memory_index: u32,
) -> u32 {
    let instance = (&*vmctx).instance();
    let memory_index = MemoryIndex::from_u32(memory_index);

    instance.imported_memory_size(memory_index).0
}

/// Implementation of `table.copy`.
///
/// # Safety
///
/// `vmctx` must be dereferenceable.
#[no_mangle]
pub unsafe extern "C" fn wasmer_vm_table_copy(
    vmctx: *mut VMContext,
    dst_table_index: u32,
    src_table_index: u32,
    dst: u32,
    src: u32,
    len: u32,
) {
    let result = {
        let dst_table_index = TableIndex::from_u32(dst_table_index);
        let src_table_index = TableIndex::from_u32(src_table_index);
        let instance = (&*vmctx).instance();
        let dst_table = instance.get_table(dst_table_index);
        let src_table = instance.get_table(src_table_index);
        dst_table.copy(src_table, dst, src, len)
    };
    if let Err(trap) = result {
        raise_lib_trap(trap);
    }
}

/// Implementation of `table.init`.
///
/// # Safety
///
/// `vmctx` must be dereferenceable.
#[no_mangle]
pub unsafe extern "C" fn wasmer_vm_table_init(
    vmctx: *mut VMContext,
    table_index: u32,
    elem_index: u32,
    dst: u32,
    src: u32,
    len: u32,
) {
    let result = {
        let table_index = TableIndex::from_u32(table_index);
        let elem_index = ElemIndex::from_u32(elem_index);
        let instance = (&*vmctx).instance();
        instance.table_init(table_index, elem_index, dst, src, len)
    };
    if let Err(trap) = result {
        raise_lib_trap(trap);
    }
}

/// Implementation of `table.fill`.
///
/// # Safety
///
/// `vmctx` must be dereferenceable.
#[no_mangle]
pub unsafe extern "C" fn wasmer_vm_table_fill(
    vmctx: *mut VMContext,
    table_index: u32,
    start_idx: u32,
    item: RawTableElement,
    len: u32,
) {
    let result = {
        let table_index = TableIndex::from_u32(table_index);
        let instance = (&*vmctx).instance();
        let elem = match instance.get_table(table_index).ty().ty {
            Type::ExternRef => TableElement::ExternRef(item.extern_ref.into()),
            Type::FuncRef => TableElement::FuncRef(item.func_ref),
            _ => panic!("Unrecognized table type: does not contain references"),
        };

        instance.table_fill(table_index, start_idx, elem, len)
    };
    if let Err(trap) = result {
        raise_lib_trap(trap);
    }
}

/// Implementation of `table.size`.
///
/// # Safety
///
/// `vmctx` must be dereferenceable.
#[no_mangle]
pub unsafe extern "C" fn wasmer_vm_table_size(vmctx: *mut VMContext, table_index: u32) -> u32 {
    let instance = (&*vmctx).instance();
    let table_index = LocalTableIndex::from_u32(table_index);

    instance.table_size(table_index)
}

/// Implementation of `table.size` for imported tables.
///
/// # Safety
///
/// `vmctx` must be dereferenceable.
#[no_mangle]
pub unsafe extern "C" fn wasmer_vm_imported_table_size(
    vmctx: *mut VMContext,
    table_index: u32,
) -> u32 {
    let instance = (&*vmctx).instance();
    let table_index = TableIndex::from_u32(table_index);

    instance.imported_table_size(table_index)
}

/// Implementation of `table.get`.
///
/// # Safety
///
/// `vmctx` must be dereferenceable.
#[no_mangle]
pub unsafe extern "C" fn wasmer_vm_table_get(
    vmctx: *mut VMContext,
    table_index: u32,
    elem_index: u32,
) -> RawTableElement {
    let instance = (&*vmctx).instance();
    let table_index = LocalTableIndex::from_u32(table_index);

    // TODO: type checking, maybe have specialized accessors
    match instance.table_get(table_index, elem_index) {
        Some(table_ref) => table_ref.into(),
        None => raise_lib_trap(Trap::lib(TrapCode::TableAccessOutOfBounds)),
    }
}

/// Implementation of `table.get` for imported tables.
///
/// # Safety
///
/// `vmctx` must be dereferenceable.
#[no_mangle]
pub unsafe extern "C" fn wasmer_vm_imported_table_get(
    vmctx: *mut VMContext,
    table_index: u32,
    elem_index: u32,
) -> RawTableElement {
    let instance = (&*vmctx).instance();
    let table_index = TableIndex::from_u32(table_index);

    // TODO: type checking, maybe have specialized accessors
    match instance.imported_table_get(table_index, elem_index) {
        Some(table_ref) => table_ref.into(),
        None => raise_lib_trap(Trap::lib(TrapCode::TableAccessOutOfBounds)),
    }
}

/// Implementation of `table.set`.
///
/// # Safety
///
/// `vmctx` must be dereferenceable.
///
/// It is the caller's responsibility to increment the ref count of any ref counted
/// type before passing it to this function.
#[no_mangle]
pub unsafe extern "C" fn wasmer_vm_table_set(
    vmctx: *mut VMContext,
    table_index: u32,
    elem_index: u32,
    value: RawTableElement,
) {
    let instance = (&*vmctx).instance();
    let table_index = TableIndex::from_u32(table_index);
    if let Ok(local_table) = instance
        .artifact
        .import_counts()
        .local_table_index(table_index)
    {
        let elem = match instance.get_local_table(local_table).ty().ty {
            Type::ExternRef => TableElement::ExternRef(value.extern_ref.into()),
            Type::FuncRef => TableElement::FuncRef(value.func_ref),
            _ => panic!("Unrecognized table type: does not contain references"),
        };
        // TODO: type checking, maybe have specialized accessors
        let result = instance.table_set(local_table, elem_index, elem);
        if let Err(trap) = result {
            raise_lib_trap(trap);
        }
    } else {
        panic!("wasmer_vm_imported_table_set should have been called");
    }
}

/// Implementation of `table.set` for imported tables.
///
/// # Safety
///
/// `vmctx` must be dereferenceable.
#[no_mangle]
pub unsafe extern "C" fn wasmer_vm_imported_table_set(
    vmctx: *mut VMContext,
    table_index: u32,
    elem_index: u32,
    value: RawTableElement,
) {
    let instance = (&*vmctx).instance();
    let table_index = TableIndex::from_u32(table_index);
    let elem = match instance.get_foreign_table(table_index).ty().ty {
        Type::ExternRef => TableElement::ExternRef(value.extern_ref.into()),
        Type::FuncRef => TableElement::FuncRef(value.func_ref),
        _ => panic!("Unrecognized table type: does not contain references"),
    };
    let result = instance.imported_table_set(table_index, elem_index, elem);
    if let Err(trap) = result {
        raise_lib_trap(trap);
    }
}

/// Implementation of `table.grow` for locally-defined tables.
///
/// # Safety
///
/// `vmctx` must be dereferenceable.
#[no_mangle]
pub unsafe extern "C" fn wasmer_vm_table_grow(
    vmctx: *mut VMContext,
    init_value: RawTableElement,
    delta: u32,
    table_index: u32,
) -> u32 {
    let instance = (&*vmctx).instance();
    let table_index = LocalTableIndex::from_u32(table_index);
    let init_value = match instance.get_local_table(table_index).ty().ty {
        Type::ExternRef => TableElement::ExternRef(init_value.extern_ref.into()),
        Type::FuncRef => TableElement::FuncRef(init_value.func_ref),
        _ => panic!("Unrecognized table type: does not contain references"),
    };
    instance
        .table_grow(table_index, delta, init_value)
        .unwrap_or(u32::max_value())
}

/// Implementation of `table.grow` for imported tables.
///
/// # Safety
///
/// `vmctx` must be dereferenceable.
#[no_mangle]
pub unsafe extern "C" fn wasmer_vm_imported_table_grow(
    vmctx: *mut VMContext,
    init_value: RawTableElement,
    delta: u32,
    table_index: u32,
) -> u32 {
    let instance = (&*vmctx).instance();
    let table_index = TableIndex::from_u32(table_index);
    let init_value = match instance.get_table(table_index).ty().ty {
        Type::ExternRef => TableElement::ExternRef(init_value.extern_ref.into()),
        Type::FuncRef => TableElement::FuncRef(init_value.func_ref),
        _ => panic!("Unrecognized table type: does not contain references"),
    };

    instance
        .imported_table_grow(table_index, delta, init_value)
        .unwrap_or(u32::max_value())
}

/// Implementation of `func.ref`.
///
/// # Safety
///
/// `vmctx` must be dereferenceable.
#[no_mangle]
pub unsafe extern "C" fn wasmer_vm_func_ref(
    vmctx: *mut VMContext,
    function_index: u32,
) -> VMFuncRef {
    let instance = (&*vmctx).instance();
    let function_index = FunctionIndex::from_u32(function_index);

    instance.func_ref(function_index).unwrap()
}

/// Implementation of externref increment
///
/// # Safety
///
/// `vmctx` must be dereferenceable.
///
/// This function must only be called at precise locations to prevent memory leaks.
#[no_mangle]
pub unsafe extern "C" fn wasmer_vm_externref_inc(externref: VMExternRef) {
    externref.ref_clone();
}

/// Implementation of externref decrement
///
/// # Safety
///
/// `vmctx` must be dereferenceable.
///
/// This function must only be called at precise locations, otherwise use-after-free
/// and other serious memory bugs may occur.
#[no_mangle]
pub unsafe extern "C" fn wasmer_vm_externref_dec(mut externref: VMExternRef) {
    externref.ref_drop()
}

/// Implementation of `elem.drop`.
///
/// # Safety
///
/// `vmctx` must be dereferenceable.
#[no_mangle]
pub unsafe extern "C" fn wasmer_vm_elem_drop(vmctx: *mut VMContext, elem_index: u32) {
    let elem_index = ElemIndex::from_u32(elem_index);
    let instance = (&*vmctx).instance();
    instance.elem_drop(elem_index);
}

/// Implementation of `memory.copy` for locally defined memories.
///
/// # Safety
///
/// `vmctx` must be dereferenceable.
#[no_mangle]
pub unsafe extern "C" fn wasmer_vm_memory32_copy(
    vmctx: *mut VMContext,
    memory_index: u32,
    dst: u32,
    src: u32,
    len: u32,
) {
    let result = {
        let memory_index = LocalMemoryIndex::from_u32(memory_index);
        let instance = (&*vmctx).instance();
        instance.local_memory_copy(memory_index, dst, src, len)
    };
    if let Err(trap) = result {
        raise_lib_trap(trap);
    }
}

/// Implementation of `memory.copy` for imported memories.
///
/// # Safety
///
/// `vmctx` must be dereferenceable.
#[no_mangle]
pub unsafe extern "C" fn wasmer_vm_imported_memory32_copy(
    vmctx: *mut VMContext,
    memory_index: u32,
    dst: u32,
    src: u32,
    len: u32,
) {
    let result = {
        let memory_index = MemoryIndex::from_u32(memory_index);
        let instance = (&*vmctx).instance();
        instance.imported_memory_copy(memory_index, dst, src, len)
    };
    if let Err(trap) = result {
        raise_lib_trap(trap);
    }
}

/// Implementation of `memory.fill` for locally defined memories.
///
/// # Safety
///
/// `vmctx` must be dereferenceable.
#[no_mangle]
pub unsafe extern "C" fn wasmer_vm_memory32_fill(
    vmctx: *mut VMContext,
    memory_index: u32,
    dst: u32,
    val: u32,
    len: u32,
) {
    let result = {
        let memory_index = LocalMemoryIndex::from_u32(memory_index);
        let instance = (&*vmctx).instance();
        instance.local_memory_fill(memory_index, dst, val, len)
    };
    if let Err(trap) = result {
        raise_lib_trap(trap);
    }
}

/// Implementation of `memory.fill` for imported memories.
///
/// # Safety
///
/// `vmctx` must be dereferenceable.
#[no_mangle]
pub unsafe extern "C" fn wasmer_vm_imported_memory32_fill(
    vmctx: *mut VMContext,
    memory_index: u32,
    dst: u32,
    val: u32,
    len: u32,
) {
    let result = {
        let memory_index = MemoryIndex::from_u32(memory_index);
        let instance = (&*vmctx).instance();
        instance.imported_memory_fill(memory_index, dst, val, len)
    };
    if let Err(trap) = result {
        raise_lib_trap(trap);
    }
}

/// Implementation of `memory.init`.
///
/// # Safety
///
/// `vmctx` must be dereferenceable.
#[no_mangle]
pub unsafe extern "C" fn wasmer_vm_memory32_init(
    vmctx: *mut VMContext,
    memory_index: u32,
    data_index: u32,
    dst: u32,
    src: u32,
    len: u32,
) {
    let result = {
        let memory_index = MemoryIndex::from_u32(memory_index);
        let data_index = DataIndex::from_u32(data_index);
        let instance = (&*vmctx).instance();
        instance.memory_init(memory_index, data_index, dst, src, len)
    };
    if let Err(trap) = result {
        raise_lib_trap(trap);
    }
}

/// Implementation of `data.drop`.
///
/// # Safety
///
/// `vmctx` must be dereferenceable.
#[no_mangle]
pub unsafe extern "C" fn wasmer_vm_data_drop(vmctx: *mut VMContext, data_index: u32) {
    let data_index = DataIndex::from_u32(data_index);
    let instance = (&*vmctx).instance();
    instance.data_drop(data_index)
}

/// Implementation for raising a trap
///
/// # Safety
///
/// Only safe to call when wasm code is on the stack, aka `wasmer_call` or
/// `wasmer_call_trampoline` must have been previously called.
#[no_mangle]
pub unsafe extern "C" fn wasmer_vm_raise_trap(trap_code: TrapCode) -> ! {
    let trap = Trap::lib(trap_code);
    raise_lib_trap(trap)
}

/// Probestack check
///
/// # Safety
///
/// This function does not follow the standard function ABI, and is called as
/// part of the function prologue.
#[no_mangle]
pub static wasmer_vm_probestack: unsafe extern "C" fn() = PROBESTACK;

/// The name of a runtime library routine.
///
/// This list is likely to grow over time.
#[derive(
    rkyv::Serialize, rkyv::Deserialize, rkyv::Archive, Copy, Clone, Debug, PartialEq, Eq, Hash,
)]
pub enum LibCall {
    /// ceil.f32
    CeilF32,

    /// ceil.f64
    CeilF64,

    /// floor.f32
    FloorF32,

    /// floor.f64
    FloorF64,

    /// nearest.f32
    NearestF32,

    /// nearest.f64
    NearestF64,

    /// trunc.f32
    TruncF32,

    /// trunc.f64
    TruncF64,

    /// memory.size for local functions
    Memory32Size,

    /// memory.size for imported functions
    ImportedMemory32Size,

    /// table.copy
    TableCopy,

    /// table.init
    TableInit,

    /// table.fill
    TableFill,

    /// table.size for local tables
    TableSize,

    /// table.size for imported tables
    ImportedTableSize,

    /// table.get for local tables
    TableGet,

    /// table.get for imported tables
    ImportedTableGet,

    /// table.set for local tables
    TableSet,

    /// table.set for imported tables
    ImportedTableSet,

    /// table.grow for local tables
    TableGrow,

    /// table.grow for imported tables
    ImportedTableGrow,

    /// ref.func
    FuncRef,

    /// elem.drop
    ElemDrop,

    /// memory.copy for local memories
    Memory32Copy,

    /// memory.copy for imported memories
    ImportedMemory32Copy,

    /// memory.fill for local memories
    Memory32Fill,

    /// memory.fill for imported memories
    ImportedMemory32Fill,

    /// memory.init
    Memory32Init,

    /// data.drop
    DataDrop,

    /// A custom trap
    RaiseTrap,

    /// probe for stack overflow. These are emitted for functions which need
    /// when the `enable_probestack` setting is true.
    Probestack,
}

impl LibCall {
    /// The function pointer to a libcall
    pub fn function_pointer(self) -> usize {
        match self {
            Self::CeilF32 => wasmer_vm_f32_ceil as usize,
            Self::CeilF64 => wasmer_vm_f64_ceil as usize,
            Self::FloorF32 => wasmer_vm_f32_floor as usize,
            Self::FloorF64 => wasmer_vm_f64_floor as usize,
            Self::NearestF32 => wasmer_vm_f32_nearest as usize,
            Self::NearestF64 => wasmer_vm_f64_nearest as usize,
            Self::TruncF32 => wasmer_vm_f32_trunc as usize,
            Self::TruncF64 => wasmer_vm_f64_trunc as usize,
            Self::Memory32Size => wasmer_vm_memory32_size as usize,
            Self::ImportedMemory32Size => wasmer_vm_imported_memory32_size as usize,
            Self::TableCopy => wasmer_vm_table_copy as usize,
            Self::TableInit => wasmer_vm_table_init as usize,
            Self::TableFill => wasmer_vm_table_fill as usize,
            Self::TableSize => wasmer_vm_table_size as usize,
            Self::ImportedTableSize => wasmer_vm_imported_table_size as usize,
            Self::TableGet => wasmer_vm_table_get as usize,
            Self::ImportedTableGet => wasmer_vm_imported_table_get as usize,
            Self::TableSet => wasmer_vm_table_set as usize,
            Self::ImportedTableSet => wasmer_vm_imported_table_set as usize,
            Self::TableGrow => wasmer_vm_table_grow as usize,
            Self::ImportedTableGrow => wasmer_vm_imported_table_grow as usize,
            Self::FuncRef => wasmer_vm_func_ref as usize,
            Self::ElemDrop => wasmer_vm_elem_drop as usize,
            Self::Memory32Copy => wasmer_vm_memory32_copy as usize,
            Self::ImportedMemory32Copy => wasmer_vm_imported_memory32_copy as usize,
            Self::Memory32Fill => wasmer_vm_memory32_fill as usize,
            Self::ImportedMemory32Fill => wasmer_vm_memory32_fill as usize,
            Self::Memory32Init => wasmer_vm_memory32_init as usize,
            Self::DataDrop => wasmer_vm_data_drop as usize,
            Self::Probestack => wasmer_vm_probestack as usize,
            Self::RaiseTrap => wasmer_vm_raise_trap as usize,
        }
    }

    /// Return the function name associated to the libcall.
    pub fn to_function_name(&self) -> &str {
        match self {
            Self::CeilF32 => "wasmer_vm_f32_ceil",
            Self::CeilF64 => "wasmer_vm_f64_ceil",
            Self::FloorF32 => "wasmer_vm_f32_floor",
            Self::FloorF64 => "wasmer_vm_f64_floor",
            Self::NearestF32 => "wasmer_vm_f32_nearest",
            Self::NearestF64 => "wasmer_vm_f64_nearest",
            Self::TruncF32 => "wasmer_vm_f32_trunc",
            Self::TruncF64 => "wasmer_vm_f64_trunc",
            Self::Memory32Size => "wasmer_vm_memory32_size",
            Self::ImportedMemory32Size => "wasmer_vm_imported_memory32_size",
            Self::TableCopy => "wasmer_vm_table_copy",
            Self::TableInit => "wasmer_vm_table_init",
            Self::TableFill => "wasmer_vm_table_fill",
            Self::TableSize => "wasmer_vm_table_size",
            Self::ImportedTableSize => "wasmer_vm_imported_table_size",
            Self::TableGet => "wasmer_vm_table_get",
            Self::ImportedTableGet => "wasmer_vm_imported_table_get",
            Self::TableSet => "wasmer_vm_table_set",
            Self::ImportedTableSet => "wasmer_vm_imported_table_set",
            Self::TableGrow => "wasmer_vm_table_grow",
            Self::ImportedTableGrow => "wasmer_vm_imported_table_grow",
            Self::FuncRef => "wasmer_vm_func_ref",
            Self::ElemDrop => "wasmer_vm_elem_drop",
            Self::Memory32Copy => "wasmer_vm_memory32_copy",
            Self::ImportedMemory32Copy => "wasmer_vm_imported_memory32_copy",
            Self::Memory32Fill => "wasmer_vm_memory32_fill",
            Self::ImportedMemory32Fill => "wasmer_vm_imported_memory32_fill",
            Self::Memory32Init => "wasmer_vm_memory32_init",
            Self::DataDrop => "wasmer_vm_data_drop",
            Self::RaiseTrap => "wasmer_vm_raise_trap",
            // We have to do this because macOS requires a leading `_` and it's not
            // a normal function, it's a static variable, so we have to do it manually.
            #[cfg(target_vendor = "apple")]
            Self::Probestack => "_wasmer_vm_probestack",
            #[cfg(not(target_vendor = "apple"))]
            Self::Probestack => "wasmer_vm_probestack",
        }
    }
}

impl fmt::Display for LibCall {
    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
        fmt::Debug::fmt(self, f)
    }
}

'''
'''--- lib/vm/src/memory.rs ---
// This file contains code from external sources.
// Attributions: https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md

//! Memory management for linear memories.
//!
//! `LinearMemory` is to WebAssembly linear memories what `Table` is to WebAssembly tables.

use crate::mmap::Mmap;
use crate::vmcontext::VMMemoryDefinition;
use more_asserts::assert_ge;
use std::borrow::BorrowMut;
use std::cell::UnsafeCell;
use std::convert::TryInto;
use std::fmt;
use std::ptr::NonNull;
use std::sync::Mutex;
use thiserror::Error;
use wasmer_types::{Bytes, MemoryType, Pages};

/// Error type describing things that can go wrong when operating on Wasm Memories.
#[derive(Error, Debug, Clone, PartialEq, Hash)]
pub enum MemoryError {
    /// Low level error with mmap.
    #[error("Error when allocating memory: {0}")]
    Region(String),
    /// The operation would cause the size of the memory to exceed the maximum or would cause
    /// an overflow leading to unindexable memory.
    #[error("The memory could not grow: current size {} pages, requested increase: {} pages", current.0, attempted_delta.0)]
    CouldNotGrow {
        /// The current size in pages.
        current: Pages,
        /// The attempted amount to grow by in pages.
        attempted_delta: Pages,
    },
    /// The operation would cause the size of the memory size exceed the maximum.
    #[error("The memory is invalid because {}", reason)]
    InvalidMemory {
        /// The reason why the provided memory is invalid.
        reason: String,
    },
    /// Caller asked for more minimum memory than we can give them.
    #[error("The minimum requested ({} pages) memory is greater than the maximum allowed memory ({} pages)", min_requested.0, max_allowed.0)]
    MinimumMemoryTooLarge {
        /// The number of pages requested as the minimum amount of memory.
        min_requested: Pages,
        /// The maximum amount of memory we can allocate.
        max_allowed: Pages,
    },
    /// Caller asked for a maximum memory greater than we can give them.
    #[error("The maximum requested memory ({} pages) is greater than the maximum allowed memory ({} pages)", max_requested.0, max_allowed.0)]
    MaximumMemoryTooLarge {
        /// The number of pages requested as the maximum amount of memory.
        max_requested: Pages,
        /// The number of pages requested as the maximum amount of memory.
        max_allowed: Pages,
    },
    /// A user defined error value, used for error cases not listed above.
    #[error("A user-defined error occurred: {0}")]
    Generic(String),
}

/// Implementation styles for WebAssembly linear memory.
#[derive(Debug, Clone, PartialEq, Eq, Hash, rkyv::Serialize, rkyv::Deserialize, rkyv::Archive)]
pub enum MemoryStyle {
    /// The actual memory can be resized and moved.
    Dynamic {
        /// Our chosen offset-guard size.
        ///
        /// It represents the size in bytes of extra guard pages after the end
        /// to optimize loads and stores with constant offsets.
        offset_guard_size: u64,
    },
    /// Address space is allocated up front.
    Static {
        /// The number of mapped and unmapped pages.
        bound: Pages,
        /// Our chosen offset-guard size.
        ///
        /// It represents the size in bytes of extra guard pages after the end
        /// to optimize loads and stores with constant offsets.
        offset_guard_size: u64,
    },
}

impl MemoryStyle {
    /// Returns the offset-guard size
    pub fn offset_guard_size(&self) -> u64 {
        match self {
            Self::Dynamic { offset_guard_size } => *offset_guard_size,
            Self::Static {
                offset_guard_size, ..
            } => *offset_guard_size,
        }
    }
}

/// Trait for implementing Wasm Memory used by Wasmer.
pub trait Memory: fmt::Debug + Send + Sync {
    /// Returns the memory type for this memory.
    fn ty(&self) -> MemoryType;

    /// Returns the memory style for this memory.
    fn style(&self) -> &MemoryStyle;

    /// Returns the number of allocated wasm pages.
    fn size(&self) -> Pages;

    /// Grow memory by the specified amount of wasm pages.
    fn grow(&self, delta: Pages) -> Result<Pages, MemoryError>;

    /// Return a [`VMMemoryDefinition`] for exposing the memory to compiled wasm code.
    ///
    /// The pointer returned in [`VMMemoryDefinition`] must be valid for the lifetime of this memory.
    fn vmmemory(&self) -> NonNull<VMMemoryDefinition>;
}

/// A linear memory instance.
#[derive(Debug)]
pub struct LinearMemory {
    // The underlying allocation.
    mmap: Mutex<WasmMmap>,

    // The optional maximum size in wasm pages of this linear memory.
    maximum: Option<Pages>,

    /// The WebAssembly linear memory description.
    memory: MemoryType,

    /// Our chosen implementation style.
    style: MemoryStyle,

    // Size in bytes of extra guard pages after the end to optimize loads and stores with
    // constant offsets.
    offset_guard_size: usize,

    /// The owned memory definition used by the generated code
    vm_memory_definition: VMMemoryDefinitionOwnership,
}

/// A type to help manage who is responsible for the backing memory of them
/// `VMMemoryDefinition`.
#[derive(Debug)]
enum VMMemoryDefinitionOwnership {
    /// The `VMMemoryDefinition` is owned by the `Instance` and we should use
    /// its memory. This is how a local memory that's exported should be stored.
    VMOwned(NonNull<VMMemoryDefinition>),
    /// The `VMMemoryDefinition` is owned by the host and we should manage its
    /// memory. This is how an imported memory that doesn't come from another
    /// Wasm module should be stored.
    HostOwned(Box<UnsafeCell<VMMemoryDefinition>>),
}

/// We must implement this because of `VMMemoryDefinitionOwnership::VMOwned`.
/// This is correct because synchronization of memory accesses is controlled
/// by the VM.
// REVIEW: I don't believe ^; this probably shouldn't be `Send`...
// mutations from other threads into this data could be a problem, but we probably
// don't want to use atomics for this in the generated code.
// TODO:
unsafe impl Send for LinearMemory {}

/// This is correct because all internal mutability is protected by a mutex.
unsafe impl Sync for LinearMemory {}

#[derive(Debug)]
struct WasmMmap {
    // Our OS allocation of mmap'd memory.
    alloc: Mmap,
    // The current logical size in wasm pages of this linear memory.
    size: Pages,
}

impl LinearMemory {
    /// Create a new linear memory instance with specified minimum and maximum number of wasm pages.
    ///
    /// This creates a `LinearMemory` with owned metadata: this can be used to create a memory
    /// that will be imported into Wasm modules.
    pub fn new(memory: &MemoryType, style: &MemoryStyle) -> Result<Self, MemoryError> {
        unsafe { Self::new_internal(memory, style, None) }
    }

    /// Create a new linear memory instance with specified minimum and maximum number of wasm pages.
    ///
    /// This creates a `LinearMemory` with metadata owned by a VM, pointed to by
    /// `vm_memory_location`: this can be used to create a local memory.
    ///
    /// # Safety
    /// - `vm_memory_location` must point to a valid location in VM memory.
    pub unsafe fn from_definition(
        memory: &MemoryType,
        style: &MemoryStyle,
        vm_memory_location: NonNull<VMMemoryDefinition>,
    ) -> Result<Self, MemoryError> {
        Self::new_internal(memory, style, Some(vm_memory_location))
    }

    /// Build a `LinearMemory` with either self-owned or VM owned metadata.
    unsafe fn new_internal(
        memory: &MemoryType,
        style: &MemoryStyle,
        vm_memory_location: Option<NonNull<VMMemoryDefinition>>,
    ) -> Result<Self, MemoryError> {
        if memory.minimum > Pages::max_value() {
            return Err(MemoryError::MinimumMemoryTooLarge {
                min_requested: memory.minimum,
                max_allowed: Pages::max_value(),
            });
        }
        // `maximum` cannot be set to more than `65536` pages.
        if let Some(max) = memory.maximum {
            if max > Pages::max_value() {
                return Err(MemoryError::MaximumMemoryTooLarge {
                    max_requested: max,
                    max_allowed: Pages::max_value(),
                });
            }
            if max < memory.minimum {
                return Err(MemoryError::InvalidMemory {
                    reason: format!(
                        "the maximum ({} pages) is less than the minimum ({} pages)",
                        max.0, memory.minimum.0
                    ),
                });
            }
        }

        let offset_guard_bytes = style.offset_guard_size() as usize;

        let minimum_pages = match style {
            MemoryStyle::Dynamic { .. } => memory.minimum,
            MemoryStyle::Static { bound, .. } => {
                assert_ge!(*bound, memory.minimum);
                *bound
            }
        };
        let minimum_bytes = minimum_pages.bytes().0;
        let request_bytes = minimum_bytes.checked_add(offset_guard_bytes).unwrap();
        let mapped_pages = memory.minimum;
        let mapped_bytes = mapped_pages.bytes();

        let mut mmap = WasmMmap {
            alloc: Mmap::accessible_reserved(mapped_bytes.0, request_bytes)
                .map_err(MemoryError::Region)?,
            size: memory.minimum,
        };

        let base_ptr = mmap.alloc.as_mut_ptr();
        let mem_length = memory.minimum.bytes().0;
        Ok(Self {
            mmap: Mutex::new(mmap),
            maximum: memory.maximum,
            offset_guard_size: offset_guard_bytes,
            vm_memory_definition: if let Some(mem_loc) = vm_memory_location {
                {
                    let mut ptr = mem_loc;
                    let md = ptr.as_mut();
                    md.base = base_ptr;
                    md.current_length = mem_length;
                }
                VMMemoryDefinitionOwnership::VMOwned(mem_loc)
            } else {
                VMMemoryDefinitionOwnership::HostOwned(Box::new(UnsafeCell::new(
                    VMMemoryDefinition {
                        base: base_ptr,
                        current_length: mem_length,
                    },
                )))
            },
            memory: *memory,
            style: style.clone(),
        })
    }

    /// Get the `VMMemoryDefinition`.
    ///
    /// # Safety
    /// - You must ensure that you have mutually exclusive access before calling
    ///   this function. You can get this by locking the `mmap` mutex.
    unsafe fn get_vm_memory_definition(&self) -> NonNull<VMMemoryDefinition> {
        match &self.vm_memory_definition {
            VMMemoryDefinitionOwnership::VMOwned(ptr) => *ptr,
            VMMemoryDefinitionOwnership::HostOwned(boxed_ptr) => {
                NonNull::new_unchecked(boxed_ptr.get())
            }
        }
    }
}

impl Memory for LinearMemory {
    /// Returns the type for this memory.
    fn ty(&self) -> MemoryType {
        let minimum = self.size();
        let mut out = self.memory;
        out.minimum = minimum;

        out
    }

    /// Returns the memory style for this memory.
    fn style(&self) -> &MemoryStyle {
        &self.style
    }

    /// Returns the number of allocated wasm pages.
    fn size(&self) -> Pages {
        // TODO: investigate this function for race conditions
        unsafe {
            let md_ptr = self.get_vm_memory_definition();
            let md = md_ptr.as_ref();
            Bytes::from(md.current_length).try_into().unwrap()
        }
    }

    /// Grow memory by the specified amount of wasm pages.
    ///
    /// Returns `None` if memory can't be grown by the specified amount
    /// of wasm pages.
    fn grow(&self, delta: Pages) -> Result<Pages, MemoryError> {
        let mut mmap_guard = self.mmap.lock().unwrap();
        let mmap = mmap_guard.borrow_mut();
        // Optimization of memory.grow 0 calls.
        if delta.0 == 0 {
            return Ok(mmap.size);
        }

        let new_pages = mmap
            .size
            .checked_add(delta)
            .ok_or(MemoryError::CouldNotGrow {
                current: mmap.size,
                attempted_delta: delta,
            })?;
        let prev_pages = mmap.size;

        if let Some(maximum) = self.maximum {
            if new_pages > maximum {
                return Err(MemoryError::CouldNotGrow {
                    current: mmap.size,
                    attempted_delta: delta,
                });
            }
        }

        // Wasm linear memories are never allowed to grow beyond what is
        // indexable. If the memory has no maximum, enforce the greatest
        // limit here.
        if new_pages >= Pages::max_value() {
            // Linear memory size would exceed the index range.
            return Err(MemoryError::CouldNotGrow {
                current: mmap.size,
                attempted_delta: delta,
            });
        }

        let delta_bytes = delta.bytes().0;
        let prev_bytes = prev_pages.bytes().0;
        let new_bytes = new_pages.bytes().0;

        if new_bytes > mmap.alloc.len() - self.offset_guard_size {
            // If the new size is within the declared maximum, but needs more memory than we
            // have on hand, it's a dynamic heap and it can move.
            let guard_bytes = self.offset_guard_size;
            let request_bytes =
                new_bytes
                    .checked_add(guard_bytes)
                    .ok_or_else(|| MemoryError::CouldNotGrow {
                        current: new_pages,
                        attempted_delta: Bytes(guard_bytes).try_into().unwrap(),
                    })?;

            let mut new_mmap =
                Mmap::accessible_reserved(new_bytes, request_bytes).map_err(MemoryError::Region)?;

            let copy_len = mmap.alloc.len() - self.offset_guard_size;
            new_mmap.as_mut_slice()[..copy_len].copy_from_slice(&mmap.alloc.as_slice()[..copy_len]);

            mmap.alloc = new_mmap;
        } else if delta_bytes > 0 {
            // Make the newly allocated pages accessible.
            mmap.alloc
                .make_accessible(prev_bytes, delta_bytes)
                .map_err(MemoryError::Region)?;
        }

        mmap.size = new_pages;

        // update memory definition
        unsafe {
            let mut md_ptr = self.get_vm_memory_definition();
            let md = md_ptr.as_mut();
            md.current_length = new_pages.bytes().0;
            md.base = mmap.alloc.as_mut_ptr() as _;
        }

        Ok(prev_pages)
    }

    /// Return a `VMMemoryDefinition` for exposing the memory to compiled wasm code.
    fn vmmemory(&self) -> NonNull<VMMemoryDefinition> {
        let _mmap_guard = self.mmap.lock().unwrap();
        unsafe { self.get_vm_memory_definition() }
    }
}

'''
'''--- lib/vm/src/mmap.rs ---
// This file contains code from external sources.
// Attributions: https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md

//! Low-level abstraction for allocating and managing zero-filled pages
//! of memory.

use more_asserts::assert_le;
use more_asserts::assert_lt;
use std::io;
use std::ptr;
use std::slice;

/// Round `size` up to the nearest multiple of `page_size`.
fn round_up_to_page_size(size: usize, page_size: usize) -> usize {
    (size + (page_size - 1)) & !(page_size - 1)
}

/// A simple struct consisting of a page-aligned pointer to page-aligned
/// and initially-zeroed memory and a length.
#[derive(Debug)]
pub struct Mmap {
    // Note that this is stored as a `usize` instead of a `*const` or `*mut`
    // pointer to allow this structure to be natively `Send` and `Sync` without
    // `unsafe impl`. This type is sendable across threads and shareable since
    // the coordination all happens at the OS layer.
    ptr: usize,
    len: usize,
}

impl Mmap {
    /// Construct a new empty instance of `Mmap`.
    pub fn new() -> Self {
        // Rust's slices require non-null pointers, even when empty. `Vec`
        // contains code to create a non-null dangling pointer value when
        // constructed empty, so we reuse that here.
        let empty = Vec::<u8>::new();
        Self {
            ptr: empty.as_ptr() as usize,
            len: 0,
        }
    }

    /// Create a new `Mmap` pointing to at least `size` bytes of page-aligned accessible memory.
    pub fn with_at_least(size: usize) -> Result<Self, String> {
        let page_size = region::page::size();
        let rounded_size = round_up_to_page_size(size, page_size);
        Self::accessible_reserved(rounded_size, rounded_size)
    }

    /// Create a new `Mmap` pointing to `accessible_size` bytes of page-aligned accessible memory,
    /// within a reserved mapping of `mapping_size` bytes. `accessible_size` and `mapping_size`
    /// must be native page-size multiples.
    #[cfg(not(target_os = "windows"))]
    pub fn accessible_reserved(
        accessible_size: usize,
        mapping_size: usize,
    ) -> Result<Self, String> {
        let page_size = region::page::size();
        assert_le!(accessible_size, mapping_size);
        assert_eq!(mapping_size & (page_size - 1), 0);
        assert_eq!(accessible_size & (page_size - 1), 0);

        // Mmap may return EINVAL if the size is zero, so just
        // special-case that.
        if mapping_size == 0 {
            return Ok(Self::new());
        }

        Ok(if accessible_size == mapping_size {
            // Allocate a single read-write region at once.
            let ptr = unsafe {
                libc::mmap(
                    ptr::null_mut(),
                    mapping_size,
                    libc::PROT_READ | libc::PROT_WRITE,
                    libc::MAP_PRIVATE | libc::MAP_ANON,
                    -1,
                    0,
                )
            };
            if ptr as isize == -1_isize {
                return Err(io::Error::last_os_error().to_string());
            }

            Self {
                ptr: ptr as usize,
                len: mapping_size,
            }
        } else {
            // Reserve the mapping size.
            let ptr = unsafe {
                libc::mmap(
                    ptr::null_mut(),
                    mapping_size,
                    libc::PROT_NONE,
                    libc::MAP_PRIVATE | libc::MAP_ANON,
                    -1,
                    0,
                )
            };
            if ptr as isize == -1_isize {
                return Err(io::Error::last_os_error().to_string());
            }

            let mut result = Self {
                ptr: ptr as usize,
                len: mapping_size,
            };

            if accessible_size != 0 {
                // Commit the accessible size.
                result.make_accessible(0, accessible_size)?;
            }

            result
        })
    }

    /// Create a new `Mmap` pointing to `accessible_size` bytes of page-aligned accessible memory,
    /// within a reserved mapping of `mapping_size` bytes. `accessible_size` and `mapping_size`
    /// must be native page-size multiples.
    #[cfg(target_os = "windows")]
    pub fn accessible_reserved(
        accessible_size: usize,
        mapping_size: usize,
    ) -> Result<Self, String> {
        use winapi::um::memoryapi::VirtualAlloc;
        use winapi::um::winnt::{MEM_COMMIT, MEM_RESERVE, PAGE_NOACCESS, PAGE_READWRITE};

        let page_size = region::page::size();
        assert_le!(accessible_size, mapping_size);
        assert_eq!(mapping_size & (page_size - 1), 0);
        assert_eq!(accessible_size & (page_size - 1), 0);

        // VirtualAlloc may return ERROR_INVALID_PARAMETER if the size is zero,
        // so just special-case that.
        if mapping_size == 0 {
            return Ok(Self::new());
        }

        Ok(if accessible_size == mapping_size {
            // Allocate a single read-write region at once.
            let ptr = unsafe {
                VirtualAlloc(
                    ptr::null_mut(),
                    mapping_size,
                    MEM_RESERVE | MEM_COMMIT,
                    PAGE_READWRITE,
                )
            };
            if ptr.is_null() {
                return Err(io::Error::last_os_error().to_string());
            }

            Self {
                ptr: ptr as usize,
                len: mapping_size,
            }
        } else {
            // Reserve the mapping size.
            let ptr =
                unsafe { VirtualAlloc(ptr::null_mut(), mapping_size, MEM_RESERVE, PAGE_NOACCESS) };
            if ptr.is_null() {
                return Err(io::Error::last_os_error().to_string());
            }

            let mut result = Self {
                ptr: ptr as usize,
                len: mapping_size,
            };

            if accessible_size != 0 {
                // Commit the accessible size.
                result.make_accessible(0, accessible_size)?;
            }

            result
        })
    }

    /// Make the memory starting at `start` and extending for `len` bytes accessible.
    /// `start` and `len` must be native page-size multiples and describe a range within
    /// `self`'s reserved memory.
    #[cfg(not(target_os = "windows"))]
    pub fn make_accessible(&mut self, start: usize, len: usize) -> Result<(), String> {
        let page_size = region::page::size();
        assert_eq!(start & (page_size - 1), 0);
        assert_eq!(len & (page_size - 1), 0);
        assert_lt!(len, self.len);
        assert_lt!(start, self.len - len);

        // Commit the accessible size.
        let ptr = self.ptr as *const u8;
        unsafe { region::protect(ptr.add(start), len, region::Protection::READ_WRITE) }
            .map_err(|e| e.to_string())
    }

    /// Make the memory starting at `start` and extending for `len` bytes accessible.
    /// `start` and `len` must be native page-size multiples and describe a range within
    /// `self`'s reserved memory.
    #[cfg(target_os = "windows")]
    pub fn make_accessible(&mut self, start: usize, len: usize) -> Result<(), String> {
        use winapi::ctypes::c_void;
        use winapi::um::memoryapi::VirtualAlloc;
        use winapi::um::winnt::{MEM_COMMIT, PAGE_READWRITE};
        let page_size = region::page::size();
        assert_eq!(start & (page_size - 1), 0);
        assert_eq!(len & (page_size - 1), 0);
        assert_lt!(len, self.len);
        assert_lt!(start, self.len - len);

        // Commit the accessible size.
        let ptr = self.ptr as *const u8;
        if unsafe {
            VirtualAlloc(
                ptr.add(start) as *mut c_void,
                len,
                MEM_COMMIT,
                PAGE_READWRITE,
            )
        }
        .is_null()
        {
            return Err(io::Error::last_os_error().to_string());
        }

        Ok(())
    }

    /// Return the allocated memory as a slice of u8.
    pub fn as_slice(&self) -> &[u8] {
        unsafe { slice::from_raw_parts(self.ptr as *const u8, self.len) }
    }

    /// Return the allocated memory as a mutable slice of u8.
    pub fn as_mut_slice(&mut self) -> &mut [u8] {
        unsafe { slice::from_raw_parts_mut(self.ptr as *mut u8, self.len) }
    }

    /// Return the allocated memory as a pointer to u8.
    pub fn as_ptr(&self) -> *const u8 {
        self.ptr as *const u8
    }

    /// Return the allocated memory as a mutable pointer to u8.
    pub fn as_mut_ptr(&mut self) -> *mut u8 {
        self.ptr as *mut u8
    }

    /// Return the length of the allocated memory.
    pub fn len(&self) -> usize {
        self.len
    }

    /// Return whether any memory has been allocated.
    pub fn is_empty(&self) -> bool {
        self.len() == 0
    }
}

impl Drop for Mmap {
    #[cfg(not(target_os = "windows"))]
    fn drop(&mut self) {
        if self.len != 0 {
            let r = unsafe { libc::munmap(self.ptr as *mut libc::c_void, self.len) };
            assert_eq!(r, 0, "munmap failed: {}", io::Error::last_os_error());
        }
    }

    #[cfg(target_os = "windows")]
    fn drop(&mut self) {
        if self.len != 0 {
            use winapi::ctypes::c_void;
            use winapi::um::memoryapi::VirtualFree;
            use winapi::um::winnt::MEM_RELEASE;
            let r = unsafe { VirtualFree(self.ptr as *mut c_void, 0, MEM_RELEASE) };
            assert_ne!(r, 0);
        }
    }
}

fn _assert() {
    fn _assert_send_sync<T: Send + Sync>() {}
    _assert_send_sync::<Mmap>();
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_round_up_to_page_size() {
        assert_eq!(round_up_to_page_size(0, 4096), 0);
        assert_eq!(round_up_to_page_size(1, 4096), 4096);
        assert_eq!(round_up_to_page_size(4096, 4096), 4096);
        assert_eq!(round_up_to_page_size(4097, 4096), 8192);
    }
}

'''
'''--- lib/vm/src/probestack.rs ---
// This file contains code from external sources.
// Attributions: https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md

//! This section defines the `PROBESTACK` intrinsic which is used in the
//! implementation of "stack probes" on certain platforms.
//!
//! The purpose of a stack probe is to provide a static guarantee that if a
//! thread has a guard page then a stack overflow is guaranteed to hit that
//! guard page. If a function did not have a stack probe then there's a risk of
//! having a stack frame *larger* than the guard page, so a function call could
//! skip over the guard page entirely and then later hit maybe the heap or
//! another thread, possibly leading to security vulnerabilities such as [The
//! Stack Clash], for example.
//!
//! [The Stack Clash]: https://blog.qualys.com/securitylabs/2017/06/19/the-stack-clash

// A declaration for the stack probe function in Rust's standard library, for
// catching callstack overflow.
cfg_if::cfg_if! {
    if #[cfg(all(
            target_os = "windows",
            target_env = "msvc",
            target_pointer_width = "64"
            ))] {
        extern "C" {
            pub fn __chkstk();
        }
        /// The probestack for 64bit Windows when compiled with MSVC (note the double underscore)
        pub const PROBESTACK: unsafe extern "C" fn() = __chkstk;
    } else if #[cfg(all(
            target_os = "windows",
            target_env = "msvc",
            target_pointer_width = "32"
            ))] {
        extern "C" {
            pub fn _chkstk();
        }
        /// The probestack for 32bit Windows when compiled with MSVC (note the singular underscore)
        pub const PROBESTACK: unsafe extern "C" fn() = _chkstk;
    } else if #[cfg(all(target_os = "windows", target_env = "gnu"))] {
        extern "C" {
            // ___chkstk (note the triple underscore) is implemented in compiler-builtins/src/x86_64.rs
            // by the Rust compiler for the MinGW target
            #[cfg(all(target_os = "windows", target_env = "gnu"))]
            pub fn ___chkstk();
        }
        /// The probestack for Windows when compiled with GNU
        pub const PROBESTACK: unsafe extern "C" fn() = ___chkstk;
    } else if #[cfg(not(any(target_arch = "x86_64", target_arch = "x86")))] {
        // As per
        // https://github.com/rust-lang/compiler-builtins/blob/cae3e6ea23739166504f9f9fb50ec070097979d4/src/probestack.rs#L39,
        // LLVM only has stack-probe support on x86-64 and x86. Thus, on any other CPU
        // architecture, we simply use an empty stack-probe function.
        extern "C" fn empty_probestack() {}
        /// A default probestack for other architectures
        pub const PROBESTACK: unsafe extern "C" fn() = empty_probestack;
    } else {
        extern "C" {
            pub fn __rust_probestack();
        }
        /// The probestack based on the Rust probestack
        pub static PROBESTACK: unsafe extern "C" fn() = __rust_probestack;
    }
}

'''
'''--- lib/vm/src/resolver.rs ---
use std::sync::Arc;

use crate::{ImportInitializerFuncPtr, VMExtern, VMFunction, VMGlobal, VMMemory, VMTable};

/// The value of an export passed from one instance to another.
#[derive(Debug, Clone)]
pub enum Export {
    /// A function export value.
    Function(ExportFunction),

    /// A table export value.
    Table(VMTable),

    /// A memory export value.
    Memory(VMMemory),

    /// A global export value.
    Global(VMGlobal),
}

impl From<Export> for VMExtern {
    fn from(other: Export) -> Self {
        match other {
            Export::Function(ExportFunction { vm_function, .. }) => Self::Function(vm_function),
            Export::Memory(vm_memory) => Self::Memory(vm_memory),
            Export::Table(vm_table) => Self::Table(vm_table),
            Export::Global(vm_global) => Self::Global(vm_global),
        }
    }
}

impl From<VMExtern> for Export {
    fn from(other: VMExtern) -> Self {
        match other {
            VMExtern::Function(vm_function) => Self::Function(ExportFunction {
                vm_function,
                metadata: None,
            }),
            VMExtern::Memory(vm_memory) => Self::Memory(vm_memory),
            VMExtern::Table(vm_table) => Self::Table(vm_table),
            VMExtern::Global(vm_global) => Self::Global(vm_global),
        }
    }
}

/// Extra metadata about `ExportFunction`s.
///
/// The metadata acts as a kind of manual virtual dispatch. We store the
/// user-supplied `WasmerEnv` as a void pointer and have methods on it
/// that have been adapted to accept a void pointer.
///
/// This struct owns the original `host_env`, thus when it gets dropped
/// it calls the `drop` function on it.
#[derive(Debug, PartialEq)]
pub struct ExportFunctionMetadata {
    /// This field is stored here to be accessible by `Drop`.
    ///
    /// At the time it was added, it's not accessed anywhere outside of
    /// the `Drop` implementation. This field is the "master copy" of the env,
    /// that is, the original env passed in by the user. Every time we create
    /// an `Instance` we clone this with the `host_env_clone_fn` field.
    ///
    /// Thus, we only bother to store the master copy at all here so that
    /// we can free it.
    ///
    /// See `wasmer_vm::export::VMFunction::vmctx` for the version of
    /// this pointer that is used by the VM when creating an `Instance`.
    pub host_env: *mut std::ffi::c_void,

    /// Function pointer to `WasmerEnv::init_with_instance(&mut self, instance: &Instance)`.
    ///
    /// This function is called to finish setting up the environment after
    /// we create the `api::Instance`.
    // This one is optional for now because dynamic host envs need the rest
    // of this without the init fn
    pub import_init_function_ptr: Option<ImportInitializerFuncPtr>,

    /// A function analogous to `Clone::clone` that returns a leaked `Box`.
    pub host_env_clone_fn: fn(*mut std::ffi::c_void) -> *mut std::ffi::c_void,

    /// The destructor to free the host environment.
    ///
    /// # Safety
    /// - This function should only be called in when properly synchronized.
    /// For example, in the `Drop` implementation of this type.
    pub host_env_drop_fn: unsafe fn(*mut std::ffi::c_void),
}

/// This can be `Send` because `host_env` comes from `WasmerEnv` which is
/// `Send`. Therefore all operations should work on any thread.
unsafe impl Send for ExportFunctionMetadata {}
/// This data may be shared across threads, `drop` is an unsafe function
/// pointer, so care must be taken when calling it.
unsafe impl Sync for ExportFunctionMetadata {}

impl ExportFunctionMetadata {
    /// Create an `ExportFunctionMetadata` type with information about
    /// the exported function.
    ///
    /// # Safety
    /// - the `host_env` must be `Send`.
    /// - all function pointers must work on any thread.
    pub unsafe fn new(
        host_env: *mut std::ffi::c_void,
        import_init_function_ptr: Option<ImportInitializerFuncPtr>,
        host_env_clone_fn: fn(*mut std::ffi::c_void) -> *mut std::ffi::c_void,
        host_env_drop_fn: fn(*mut std::ffi::c_void),
    ) -> Self {
        Self {
            host_env,
            import_init_function_ptr,
            host_env_clone_fn,
            host_env_drop_fn,
        }
    }
}

// We have to free `host_env` here because we always clone it before using it
// so all the `host_env`s freed at the `Instance` level won't touch the original.
impl Drop for ExportFunctionMetadata {
    fn drop(&mut self) {
        if !self.host_env.is_null() {
            // # Safety
            // - This is correct because we know no other references
            //   to this data can exist if we're dropping it.
            unsafe {
                (self.host_env_drop_fn)(self.host_env);
            }
        }
    }
}

/// A function export value with an extra function pointer to initialize
/// host environments.
#[derive(Debug, Clone, PartialEq)]
pub struct ExportFunction {
    /// The VM function, containing most of the data.
    pub vm_function: VMFunction,
    /// Contains functions necessary to create and initialize host envs
    /// with each `Instance` as well as being responsible for the
    /// underlying memory of the host env.
    pub metadata: Option<Arc<ExportFunctionMetadata>>,
}

impl From<ExportFunction> for Export {
    fn from(func: ExportFunction) -> Self {
        Self::Function(func)
    }
}

impl From<VMTable> for Export {
    fn from(table: VMTable) -> Self {
        Self::Table(table)
    }
}

impl From<VMMemory> for Export {
    fn from(memory: VMMemory) -> Self {
        Self::Memory(memory)
    }
}

impl From<VMGlobal> for Export {
    fn from(global: VMGlobal) -> Self {
        Self::Global(global)
    }
}

///
/// Import resolver connects imports with available exported values.
pub trait Resolver {
    /// Resolves an import a WebAssembly module to an export it's hooked up to.
    ///
    /// The `index` provided is the index of the import in the wasm module
    /// that's being resolved. For example 1 means that it's the second import
    /// listed in the wasm module.
    ///
    /// The `module` and `field` arguments provided are the module/field names
    /// listed on the import itself.
    ///
    /// # Notes:
    ///
    /// The index is useful because some WebAssembly modules may rely on that
    /// for resolving ambiguity in their imports. Such as:
    /// ```ignore
    /// (module
    ///   (import "" "" (func))
    ///   (import "" "" (func (param i32) (result i32)))
    /// )
    /// ```
    fn resolve(&self, _index: u32, module: &str, field: &str) -> Option<Export>;
}

/// Import resolver connects imports with available exported values.
///
/// This is a specific subtrait for [`Resolver`] for those users who don't
/// care about the `index`, but only about the `module` and `field` for
/// the resolution.
pub trait NamedResolver {
    /// Resolves an import a WebAssembly module to an export it's hooked up to.
    ///
    /// It receives the `module` and `field` names and return the [`Export`] in
    /// case it's found.
    fn resolve_by_name(&self, module: &str, field: &str) -> Option<Export>;
}

// All NamedResolvers should extend `Resolver`.
impl<T: NamedResolver> Resolver for T {
    /// By default this method will be calling [`NamedResolver::resolve_by_name`],
    /// dismissing the provided `index`.
    fn resolve(&self, _index: u32, module: &str, field: &str) -> Option<Export> {
        self.resolve_by_name(module, field)
    }
}

impl<T: NamedResolver> NamedResolver for &T {
    fn resolve_by_name(&self, module: &str, field: &str) -> Option<Export> {
        (**self).resolve_by_name(module, field)
    }
}

impl NamedResolver for Box<dyn NamedResolver + Send + Sync> {
    fn resolve_by_name(&self, module: &str, field: &str) -> Option<Export> {
        (**self).resolve_by_name(module, field)
    }
}

impl NamedResolver for () {
    /// Always returns `None`.
    fn resolve_by_name(&self, _module: &str, _field: &str) -> Option<Export> {
        None
    }
}

/// `Resolver` implementation that always resolves to `None`. Equivalent to `()`.
pub struct NullResolver {}

impl Resolver for NullResolver {
    fn resolve(&self, _idx: u32, _module: &str, _field: &str) -> Option<Export> {
        None
    }
}

/// A [`Resolver`] that links two resolvers together in a chain.
pub struct NamedResolverChain<A: NamedResolver + Send + Sync, B: NamedResolver + Send + Sync> {
    a: A,
    b: B,
}

/// A trait for chaining resolvers together.
///
/// ```
/// # use wasmer_vm::{ChainableNamedResolver, NamedResolver};
/// # fn chainable_test<A, B>(imports1: A, imports2: B)
/// # where A: NamedResolver + Sized + Send + Sync,
/// #       B: NamedResolver + Sized + Send + Sync,
/// # {
/// // override duplicates with imports from `imports2`
/// imports1.chain_front(imports2);
/// # }
/// ```
pub trait ChainableNamedResolver: NamedResolver + Sized + Send + Sync {
    /// Chain a resolver in front of the current resolver.
    ///
    /// This will cause the second resolver to override the first.
    ///
    /// ```
    /// # use wasmer_vm::{ChainableNamedResolver, NamedResolver};
    /// # fn chainable_test<A, B>(imports1: A, imports2: B)
    /// # where A: NamedResolver + Sized + Send + Sync,
    /// #       B: NamedResolver + Sized + Send + Sync,
    /// # {
    /// // override duplicates with imports from `imports2`
    /// imports1.chain_front(imports2);
    /// # }
    /// ```
    fn chain_front<U>(self, other: U) -> NamedResolverChain<U, Self>
    where
        U: NamedResolver + Send + Sync,
    {
        NamedResolverChain { a: other, b: self }
    }

    /// Chain a resolver behind the current resolver.
    ///
    /// This will cause the first resolver to override the second.
    ///
    /// ```
    /// # use wasmer_vm::{ChainableNamedResolver, NamedResolver};
    /// # fn chainable_test<A, B>(imports1: A, imports2: B)
    /// # where A: NamedResolver + Sized + Send + Sync,
    /// #       B: NamedResolver + Sized + Send + Sync,
    /// # {
    /// // override duplicates with imports from `imports1`
    /// imports1.chain_back(imports2);
    /// # }
    /// ```
    fn chain_back<U>(self, other: U) -> NamedResolverChain<Self, U>
    where
        U: NamedResolver + Send + Sync,
    {
        NamedResolverChain { a: self, b: other }
    }
}

// We give these chain methods to all types implementing NamedResolver
impl<T: NamedResolver + Send + Sync> ChainableNamedResolver for T {}

impl<A, B> NamedResolver for NamedResolverChain<A, B>
where
    A: NamedResolver + Send + Sync,
    B: NamedResolver + Send + Sync,
{
    fn resolve_by_name(&self, module: &str, field: &str) -> Option<Export> {
        self.a
            .resolve_by_name(module, field)
            .or_else(|| self.b.resolve_by_name(module, field))
    }
}

impl<A, B> Clone for NamedResolverChain<A, B>
where
    A: NamedResolver + Clone + Send + Sync,
    B: NamedResolver + Clone + Send + Sync,
{
    fn clone(&self) -> Self {
        Self {
            a: self.a.clone(),
            b: self.b.clone(),
        }
    }
}

'''
'''--- lib/vm/src/sig_registry.rs ---
// This file contains code from external sources.
// Attributions: https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md

//! Implement a registry of function signatures, for fast indirect call
//! signature checking.

use std::collections::{hash_map, HashMap};
use std::convert::TryFrom;
use wasmer_types::{FunctionType, FunctionTypeRef};

/// An index into the shared signature registry, usable for checking signatures
/// at indirect calls.
#[repr(C)]
#[derive(Debug, Eq, PartialEq, Clone, Copy, Hash)]
pub struct VMSharedSignatureIndex(u32);

impl VMSharedSignatureIndex {
    /// Create a new `VMSharedSignatureIndex`.
    pub fn new(value: u32) -> Self {
        Self(value)
    }
}

/// WebAssembly requires that the caller and callee signatures in an indirect
/// call must match. To implement this efficiently, keep a registry of all
/// signatures, shared by all instances, so that call sites can just do an
/// index comparison.
#[derive(Debug)]
pub struct SignatureRegistry {
    type_to_index: HashMap<FunctionType, VMSharedSignatureIndex>,
    index_to_data: Vec<FunctionType>,
}

impl SignatureRegistry {
    /// Create a new `SignatureRegistry`.
    pub fn new() -> Self {
        Self {
            type_to_index: HashMap::new(),
            index_to_data: Vec::new(),
        }
    }

    /// Register a signature and return its unique index.
    pub fn register(&mut self, sig: FunctionTypeRef<'_>) -> VMSharedSignatureIndex {
        let len = self.index_to_data.len();
        // TODO(0-copy): this. should. not. allocate.
        //
        // This is pretty hard to avoid, however. In order to implement bijective map, we'd want
        // a `Rc<FunctionType>`, but indexing into a map keyed by `Rc<FunctionType>` with
        // `FunctionTypeRef` is‚Ä¶ not possible given the current API either.
        //
        // Consider `transmute` or `hashbrown`'s raw_entry.
        let sig = FunctionType::new(sig.params(), sig.results());
        match self.type_to_index.entry(sig.clone()) {
            hash_map::Entry::Occupied(entry) => *entry.get(),
            hash_map::Entry::Vacant(entry) => {
                debug_assert!(
                    u32::try_from(len).is_ok(),
                    "invariant: can't have more than 2¬≥¬≤-1 signatures!"
                );
                let sig_id = VMSharedSignatureIndex::new(u32::try_from(len).unwrap());
                entry.insert(sig_id);
                self.index_to_data.push(sig);
                sig_id
            }
        }
    }

    /// Looks up a shared signature index within this registry.
    ///
    /// Note that for this operation to be semantically correct the `idx` must
    /// have previously come from a call to `register` of this same object.
    pub fn lookup(&self, idx: VMSharedSignatureIndex) -> Option<&FunctionType> {
        self.index_to_data.get(idx.0 as usize)
    }
}

'''
'''--- lib/vm/src/table.rs ---
// This file contains code from external sources.
// Attributions: https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md

//! Memory management for tables.
//!
//! `Table` is to WebAssembly tables what `LinearMemory` is to WebAssembly linear memories.

use crate::func_data_registry::VMFuncRef;
use crate::trap::{Trap, TrapCode};
use crate::vmcontext::VMTableDefinition;
use crate::VMExternRef;
use std::borrow::{Borrow, BorrowMut};
use std::cell::UnsafeCell;
use std::convert::TryFrom;
use std::fmt;
use std::ptr::NonNull;
use std::sync::Mutex;
use wasmer_types::{ExternRef, TableType, Type as ValType};

/// Implementation styles for WebAssembly tables.
#[derive(Debug, Clone, Hash, PartialEq, Eq, rkyv::Serialize, rkyv::Deserialize, rkyv::Archive)]
pub enum TableStyle {
    /// Signatures are stored in the table and checked in the caller.
    CallerChecksSignature,
}

/// Trait for implementing the interface of a Wasm table.
pub trait Table: fmt::Debug + Send + Sync {
    /// Returns the style for this Table.
    fn style(&self) -> &TableStyle;

    /// Returns the type for this Table.
    fn ty(&self) -> &TableType;

    /// Returns the number of allocated elements.
    fn size(&self) -> u32;

    /// Grow table by the specified amount of elements.
    ///
    /// Returns `None` if table can't be grown by the specified amount
    /// of elements, otherwise returns the previous size of the table.
    fn grow(&self, delta: u32, init_value: TableElement) -> Option<u32>;

    /// Get reference to the specified element.
    ///
    /// Returns `None` if the index is out of bounds.
    fn get(&self, index: u32) -> Option<TableElement>;

    /// Set reference to the specified element.
    ///
    /// # Errors
    ///
    /// Returns an error if the index is out of bounds.
    fn set(&self, index: u32, reference: TableElement) -> Result<(), Trap>;

    /// Return a `VMTableDefinition` for exposing the table to compiled wasm code.
    fn vmtable(&self) -> NonNull<VMTableDefinition>;

    /// Copy `len` elements from `src_table[src_index..]` into `dst_table[dst_index..]`.
    ///
    /// # Errors
    ///
    /// Returns an error if the range is out of bounds of either the source or
    /// destination tables.
    fn copy(
        &self,
        src_table: &dyn Table,
        dst_index: u32,
        src_index: u32,
        len: u32,
    ) -> Result<(), Trap> {
        // https://webassembly.github.io/bulk-memory-operations/core/exec/instructions.html#exec-table-copy

        if src_index
            .checked_add(len)
            .map_or(true, |n| n > src_table.size())
        {
            return Err(Trap::lib(TrapCode::TableAccessOutOfBounds));
        }

        if dst_index.checked_add(len).map_or(true, |m| m > self.size()) {
            return Err(Trap::lib(TrapCode::TableAccessOutOfBounds));
        }

        let srcs = src_index..src_index + len;
        let dsts = dst_index..dst_index + len;

        // Note on the unwraps: the bounds check above means that these will
        // never panic.
        //
        // TODO: investigate replacing this get/set loop with a `memcpy`.
        if dst_index <= src_index {
            for (s, d) in (srcs).zip(dsts) {
                self.set(d, src_table.get(s).unwrap())?;
            }
        } else {
            for (s, d) in srcs.rev().zip(dsts.rev()) {
                self.set(d, src_table.get(s).unwrap())?;
            }
        }

        Ok(())
    }
}

/// A reference stored in a table. Can be either an externref or a funcref.
#[derive(Debug, Clone)]
pub enum TableElement {
    /// Opaque pointer to arbitrary host data.
    // Note: we use `ExternRef` instead of `VMExternRef` here to ensure that we don't
    // leak by not dec-refing on failure types.
    ExternRef(ExternRef),
    /// Pointer to function: contains enough information to call it.
    FuncRef(VMFuncRef),
}

impl From<TableElement> for RawTableElement {
    fn from(other: TableElement) -> Self {
        match other {
            TableElement::ExternRef(extern_ref) => Self {
                extern_ref: extern_ref.into(),
            },
            TableElement::FuncRef(func_ref) => Self { func_ref },
        }
    }
}

#[repr(C)]
#[derive(Clone, Copy)]
pub union RawTableElement {
    pub(crate) extern_ref: VMExternRef,
    pub(crate) func_ref: VMFuncRef,
}

#[cfg(test)]
#[test]
fn table_element_size_test() {
    use std::mem::size_of;
    assert_eq!(size_of::<RawTableElement>(), size_of::<VMExternRef>());
    assert_eq!(size_of::<RawTableElement>(), size_of::<VMFuncRef>());
}

impl fmt::Debug for RawTableElement {
    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
        f.debug_struct("RawTableElement").finish()
    }
}

impl Default for RawTableElement {
    fn default() -> Self {
        Self {
            func_ref: VMFuncRef::null(),
        }
    }
}

impl Default for TableElement {
    fn default() -> Self {
        Self::FuncRef(VMFuncRef::null())
    }
}

/// A table instance.
#[derive(Debug)]
pub struct LinearTable {
    // TODO: we can remove the mutex by using atomic swaps and preallocating the max table size
    vec: Mutex<Vec<RawTableElement>>,
    maximum: Option<u32>,
    /// The WebAssembly table description.
    table: TableType,
    /// Our chosen implementation style.
    style: TableStyle,
    vm_table_definition: VMTableDefinitionOwnership,
}

/// A type to help manage who is responsible for the backing table of the
/// `VMTableDefinition`.
#[derive(Debug)]
enum VMTableDefinitionOwnership {
    /// The `VMTableDefinition` is owned by the `Instance` and we should use
    /// its table. This is how a local table that's exported should be stored.
    VMOwned(NonNull<VMTableDefinition>),
    /// The `VMTableDefinition` is owned by the host and we should manage its
    /// table. This is how an imported table that doesn't come from another
    /// Wasm module should be stored.
    HostOwned(Box<UnsafeCell<VMTableDefinition>>),
}

/// This is correct because there is no thread-specific data tied to this type.
unsafe impl Send for LinearTable {}
/// This is correct because all internal mutability is protected by a mutex.
unsafe impl Sync for LinearTable {}

impl LinearTable {
    /// Create a new linear table instance with specified minimum and maximum number of elements.
    ///
    /// This creates a `LinearTable` with metadata owned by a VM, pointed to by
    /// `vm_table_location`: this can be used to create a local table.
    pub fn new(table: &TableType, style: &TableStyle) -> Result<Self, String> {
        unsafe { Self::new_inner(table, style, None) }
    }

    /// Create a new linear table instance with specified minimum and maximum number of elements.
    ///
    /// This creates a `LinearTable` with metadata owned by a VM, pointed to by
    /// `vm_table_location`: this can be used to create a local table.
    ///
    /// # Safety
    /// - `vm_table_location` must point to a valid location in VM memory.
    pub unsafe fn from_definition(
        table: &TableType,
        style: &TableStyle,
        vm_table_location: NonNull<VMTableDefinition>,
    ) -> Result<Self, String> {
        Self::new_inner(table, style, Some(vm_table_location))
    }

    /// Create a new `LinearTable` with either self-owned or VM owned metadata.
    unsafe fn new_inner(
        table: &TableType,
        style: &TableStyle,
        vm_table_location: Option<NonNull<VMTableDefinition>>,
    ) -> Result<Self, String> {
        match table.ty {
            ValType::FuncRef | ValType::ExternRef => (),
            ty => {
                return Err(format!(
                    "tables of types other than funcref or externref ({})",
                    ty
                ))
            }
        };
        if let Some(max) = table.maximum {
            if max < table.minimum {
                return Err(format!(
                    "Table minimum ({}) is larger than maximum ({})!",
                    table.minimum, max
                ));
            }
        }
        let table_minimum = usize::try_from(table.minimum)
            .map_err(|_| "Table minimum is bigger than usize".to_string())?;
        let mut vec = vec![RawTableElement::default(); table_minimum];
        let base = vec.as_mut_ptr();
        match style {
            TableStyle::CallerChecksSignature => Ok(Self {
                vec: Mutex::new(vec),
                maximum: table.maximum,
                table: *table,
                style: style.clone(),
                vm_table_definition: if let Some(table_loc) = vm_table_location {
                    {
                        let mut ptr = table_loc;
                        let td = ptr.as_mut();
                        td.base = base as _;
                        td.current_elements = table_minimum as _;
                    }
                    VMTableDefinitionOwnership::VMOwned(table_loc)
                } else {
                    VMTableDefinitionOwnership::HostOwned(Box::new(UnsafeCell::new(
                        VMTableDefinition {
                            base: base as _,
                            current_elements: table_minimum as _,
                        },
                    )))
                },
            }),
        }
    }

    /// Get the `VMTableDefinition`.
    ///
    /// # Safety
    /// - You must ensure that you have mutually exclusive access before calling
    ///   this function. You can get this by locking the `vec` mutex.
    unsafe fn get_vm_table_definition(&self) -> NonNull<VMTableDefinition> {
        match &self.vm_table_definition {
            VMTableDefinitionOwnership::VMOwned(ptr) => *ptr,
            VMTableDefinitionOwnership::HostOwned(boxed_ptr) => {
                NonNull::new_unchecked(boxed_ptr.get())
            }
        }
    }
}

impl Table for LinearTable {
    /// Returns the type for this Table.
    fn ty(&self) -> &TableType {
        &self.table
    }

    /// Returns the style for this Table.
    fn style(&self) -> &TableStyle {
        &self.style
    }

    /// Returns the number of allocated elements.
    fn size(&self) -> u32 {
        // TODO: investigate this function for race conditions
        unsafe {
            let td_ptr = self.get_vm_table_definition();
            let td = td_ptr.as_ref();
            td.current_elements
        }
    }

    /// Grow table by the specified amount of elements.
    ///
    /// Returns `None` if table can't be grown by the specified amount
    /// of elements, otherwise returns the previous size of the table.
    fn grow(&self, delta: u32, init_value: TableElement) -> Option<u32> {
        let mut vec_guard = self.vec.lock().unwrap();
        let vec = vec_guard.borrow_mut();
        let size = self.size();
        let new_len = size.checked_add(delta)?;
        if self.maximum.map_or(false, |max| new_len > max) {
            return None;
        }
        if new_len == size {
            debug_assert_eq!(delta, 0);
            return Some(size);
        }

        // Update the ref count
        let element = match init_value {
            TableElement::ExternRef(extern_ref) => {
                let extern_ref: VMExternRef = extern_ref.into();
                // We reduce the amount we increment by because `into` prevents
                // dropping `init_value` (which is a caller-inc'd ref).
                if let Some(val) = (new_len as usize).checked_sub(size as usize + 1) {
                    extern_ref.ref_inc_by(val);
                }
                RawTableElement { extern_ref }
            }
            TableElement::FuncRef(func_ref) => RawTableElement { func_ref },
        };

        vec.resize(usize::try_from(new_len).unwrap(), element);

        // update table definition
        unsafe {
            let mut td_ptr = self.get_vm_table_definition();
            let td = td_ptr.as_mut();
            td.current_elements = new_len;
            td.base = vec.as_mut_ptr() as _;
        }
        Some(size)
    }

    /// Get reference to the specified element.
    ///
    /// Returns `None` if the index is out of bounds.
    fn get(&self, index: u32) -> Option<TableElement> {
        let vec_guard = self.vec.lock().unwrap();
        let raw_data = vec_guard.borrow().get(index as usize).cloned()?;
        Some(match self.table.ty {
            ValType::ExternRef => {
                TableElement::ExternRef(unsafe { raw_data.extern_ref.ref_clone() }.into())
            }
            ValType::FuncRef => TableElement::FuncRef(unsafe { raw_data.func_ref }),
            _ => todo!("getting invalid type from table, handle this error"),
        })
    }

    /// Set reference to the specified element.
    ///
    /// # Errors
    ///
    /// Returns an error if the index is out of bounds.
    fn set(&self, index: u32, reference: TableElement) -> Result<(), Trap> {
        let mut vec_guard = self.vec.lock().unwrap();
        let vec = vec_guard.borrow_mut();
        match vec.get_mut(index as usize) {
            Some(slot) => {
                match (self.table.ty, reference) {
                    (ValType::ExternRef, TableElement::ExternRef(extern_ref)) => {
                        let extern_ref = extern_ref.into();
                        unsafe {
                            let elem = &mut *slot;
                            elem.extern_ref.ref_drop();
                            elem.extern_ref = extern_ref
                        }
                    }
                    (ValType::FuncRef, r @ TableElement::FuncRef(_)) => {
                        let element_data = r.into();
                        *slot = element_data;
                    }
                    // This path should never be hit by the generated code due to Wasm
                    // validation.
                    (ty, v) => {
                        panic!(
                            "Attempted to set a table of type {} with the value {:?}",
                            ty, v
                        )
                    }
                };

                Ok(())
            }
            None => Err(Trap::lib(TrapCode::TableAccessOutOfBounds)),
        }
    }

    /// Return a `VMTableDefinition` for exposing the table to compiled wasm code.
    fn vmtable(&self) -> NonNull<VMTableDefinition> {
        let _vec_guard = self.vec.lock().unwrap();
        unsafe { self.get_vm_table_definition() }
    }
}

'''
'''--- lib/vm/src/trap/handlers.c ---
// This file contains partial code from other sources.
// Attributions: https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md

#include <setjmp.h>
#include <stdio.h>
#if defined(CFG_TARGET_OS_MACOS)
#include <mach/task.h>
#include <mach/mach_init.h>
#include <mach/mach_port.h>
#endif
// Note that `sigsetjmp` and `siglongjmp` are used here where possible to
// explicitly pass a 0 argument to `sigsetjmp` that we don't need to preserve
// the process signal mask. This should make this call a bit faster b/c it
// doesn't need to touch the kernel signal handling routines.
// In case of macOS, stackoverflow
#if defined(CFG_TARGET_OS_WINDOWS)
// On Windows, default setjmp/longjmp sequence will try to unwind the stack
// it's fine most of the time, but not for JIT'd code that may not respect stack ordring
// Using a special setjmp here, with NULL as second parameter to disable that behaviour
// and have a regular simple setjmp/longjmp sequence
#ifdef __MINGW32__
// MINGW64 doesn't expose the __intrinsic_setjmp function, but a similar _setjump instead
#define platform_setjmp(buf) _setjmp(buf, NULL)
#else
#define platform_setjmp(buf) __intrinsic_setjmp(buf, NULL)
#endif
#define platform_longjmp(buf, arg) longjmp(buf, arg)
#define platform_jmp_buf jmp_buf
#elif defined(CFG_TARGET_OS_MACOS)
// TODO: This is not the most performant, since it adds overhead when calling functions
// https://github.com/wasmerio/wasmer/issues/2562
#define platform_setjmp(buf) sigsetjmp(buf, 1)
#define platform_longjmp(buf, arg) siglongjmp(buf, arg)
#define platform_jmp_buf sigjmp_buf
#else
#define platform_setjmp(buf) sigsetjmp(buf, 0)
#define platform_longjmp(buf, arg) siglongjmp(buf, arg)
#define platform_jmp_buf sigjmp_buf
#endif

int wasmer_register_setjmp(
    void **buf_storage,
    void (*body)(void*),
    void *payload) {
  #if 0 && defined(CFG_TARGET_OS_MACOS)
  // Enable this block to ba able to debug Segfault with lldb
  // This will mask the EXC_BAD_ACCESS from lldb...
  static int allow_bad_access = 0;
  if(!allow_bad_access) {
    allow_bad_access = 1;
    task_set_exception_ports(mach_task_self(), EXC_MASK_BAD_ACCESS, MACH_PORT_NULL, EXCEPTION_DEFAULT, 0);
  }
  #endif
  platform_jmp_buf buf;
  if (platform_setjmp(buf) != 0) {
    return 0;
  }
  *buf_storage = &buf;
  body(payload);
  return 1;
}

void wasmer_unwind(void *JmpBuf) {
  platform_jmp_buf *buf = (platform_jmp_buf*) JmpBuf;
  platform_longjmp(*buf, 1);
}

'''
'''--- lib/vm/src/trap/mod.rs ---
// This file contains code from external sources.
// Attributions: https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md

//! This is the module that facilitates the usage of Traps
//! in Wasmer Runtime
mod trapcode;
pub mod traphandlers;

pub use trapcode::TrapCode;
pub use traphandlers::resume_panic;
pub use traphandlers::{
    catch_traps, catch_traps_with_result, raise_lib_trap, raise_user_trap, wasmer_call_trampoline,
    TlsRestore, Trap,
};

'''
'''--- lib/vm/src/trap/trapcode.rs ---
// This file contains code from external sources.
// Attributions: https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md

//! Trap codes describing the reason for a trap.

use core::fmt::{self, Display, Formatter};
use core::str::FromStr;
use thiserror::Error;

/// A trap code describing the reason for a trap.
///
/// All trap instructions have an explicit trap code.
#[derive(
    Clone,
    Copy,
    PartialEq,
    Eq,
    Debug,
    Hash,
    Error,
    rkyv::Serialize,
    rkyv::Deserialize,
    rkyv::Archive,
)]
#[repr(u32)]
pub enum TrapCode {
    /// The current stack space was exhausted.
    ///
    /// On some platforms, a stack overflow may also be indicated by a segmentation fault from the
    /// stack guard page.
    StackOverflow = 0,

    /// A `heap_addr` instruction detected an out-of-bounds error.
    ///
    /// Note that not all out-of-bounds heap accesses are reported this way;
    /// some are detected by a segmentation fault on the heap unmapped or
    /// offset-guard pages.
    HeapAccessOutOfBounds = 1,

    /// A `heap_addr` instruction was misaligned.
    HeapMisaligned = 2,

    /// A `table_addr` instruction detected an out-of-bounds error.
    TableAccessOutOfBounds = 3,

    /// Other bounds checking error.
    OutOfBounds = 4,

    /// Indirect call to a null table entry.
    IndirectCallToNull = 5,

    /// Signature mismatch on indirect call.
    BadSignature = 6,

    /// An integer arithmetic operation caused an overflow.
    IntegerOverflow = 7,

    /// An integer division by zero.
    IntegerDivisionByZero = 8,

    /// Failed float-to-int conversion.
    BadConversionToInteger = 9,

    /// Code that was supposed to have been unreachable was reached.
    UnreachableCodeReached = 10,

    /// An atomic memory access was attempted with an unaligned pointer.
    UnalignedAtomic = 11,

    /// Hit the gas limit.
    GasExceeded = 12,
}

impl TrapCode {
    /// Gets the message for this trap code
    pub fn message(&self) -> &str {
        match self {
            Self::StackOverflow => "call stack exhausted",
            Self::HeapAccessOutOfBounds => "out of bounds memory access",
            Self::HeapMisaligned => "misaligned heap",
            Self::TableAccessOutOfBounds => "undefined element: out of bounds table access",
            Self::OutOfBounds => "out of bounds",
            Self::IndirectCallToNull => "uninitialized element",
            Self::BadSignature => "indirect call type mismatch",
            Self::IntegerOverflow => "integer overflow",
            Self::IntegerDivisionByZero => "integer divide by zero",
            Self::BadConversionToInteger => "invalid conversion to integer",
            Self::UnreachableCodeReached => "unreachable",
            Self::UnalignedAtomic => "unaligned atomic access",
            Self::GasExceeded => "gas limit exceeded",
        }
    }
}

impl Display for TrapCode {
    fn fmt(&self, f: &mut Formatter) -> fmt::Result {
        let identifier = match *self {
            Self::StackOverflow => "stk_ovf",
            Self::HeapAccessOutOfBounds => "heap_get_oob",
            Self::HeapMisaligned => "heap_misaligned",
            Self::TableAccessOutOfBounds => "table_get_oob",
            Self::OutOfBounds => "oob",
            Self::IndirectCallToNull => "icall_null",
            Self::BadSignature => "bad_sig",
            Self::IntegerOverflow => "int_ovf",
            Self::IntegerDivisionByZero => "int_divz",
            Self::BadConversionToInteger => "bad_toint",
            Self::UnreachableCodeReached => "unreachable",
            Self::UnalignedAtomic => "unalign_atom",
            Self::GasExceeded => "out_of_gas",
        };
        f.write_str(identifier)
    }
}

impl FromStr for TrapCode {
    type Err = ();

    fn from_str(s: &str) -> Result<Self, Self::Err> {
        match s {
            "stk_ovf" => Ok(Self::StackOverflow),
            "heap_get_oob" => Ok(Self::HeapAccessOutOfBounds),
            "heap_misaligned" => Ok(Self::HeapMisaligned),
            "table_get_oob" => Ok(Self::TableAccessOutOfBounds),
            "oob" => Ok(Self::OutOfBounds),
            "icall_null" => Ok(Self::IndirectCallToNull),
            "bad_sig" => Ok(Self::BadSignature),
            "int_ovf" => Ok(Self::IntegerOverflow),
            "int_divz" => Ok(Self::IntegerDivisionByZero),
            "bad_toint" => Ok(Self::BadConversionToInteger),
            "unreachable" => Ok(Self::UnreachableCodeReached),
            "unalign_atom" => Ok(Self::UnalignedAtomic),
            _ => Err(()),
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    // Everything but user-defined codes.
    const CODES: [TrapCode; 12] = [
        TrapCode::StackOverflow,
        TrapCode::HeapAccessOutOfBounds,
        TrapCode::HeapMisaligned,
        TrapCode::TableAccessOutOfBounds,
        TrapCode::OutOfBounds,
        TrapCode::IndirectCallToNull,
        TrapCode::BadSignature,
        TrapCode::IntegerOverflow,
        TrapCode::IntegerDivisionByZero,
        TrapCode::BadConversionToInteger,
        TrapCode::UnreachableCodeReached,
        TrapCode::UnalignedAtomic,
    ];

    #[test]
    fn display() {
        for r in &CODES {
            let tc = *r;
            assert_eq!(tc.to_string().parse(), Ok(tc));
        }
        assert_eq!("bogus".parse::<TrapCode>(), Err(()));

        // assert_eq!(TrapCode::User(17).to_string(), "user17");
        // assert_eq!("user22".parse(), Ok(TrapCode::User(22)));
        assert_eq!("user".parse::<TrapCode>(), Err(()));
        assert_eq!("user-1".parse::<TrapCode>(), Err(()));
        assert_eq!("users".parse::<TrapCode>(), Err(()));
    }
}

'''
'''--- lib/vm/src/trap/traphandlers.rs ---
// This file contains code from external sources.
// Attributions: https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md

//! WebAssembly trap handling, which is built on top of the lower-level
//! signalhandling mechanisms.

use super::trapcode::TrapCode;
use crate::vmcontext::{VMFunctionBody, VMFunctionEnvironment, VMTrampoline};
use backtrace::Backtrace;
use std::any::Any;
use std::cell::{Cell, UnsafeCell};
use std::error::Error;
use std::mem::{self, MaybeUninit};
use std::ptr;
pub use tls::TlsRestore;

extern "C" {
    fn wasmer_register_setjmp(
        jmp_buf: *mut *const u8,
        callback: extern "C" fn(*mut u8),
        payload: *mut u8,
    ) -> i32;
    fn wasmer_unwind(jmp_buf: *const u8) -> !;
}

/// Raises a user-defined trap immediately.
///
/// This function performs as-if a wasm trap was just executed, only the trap
/// has a dynamic payload associated with it which is user-provided. This trap
/// payload is then returned from `catch_traps` below.
///
/// # Safety
///
/// Only safe to call when wasm code is on the stack, aka `catch_traps` must
/// have been previous called and not yet returned.
/// Additionally no Rust destructors may be on the stack.
/// They will be skipped and not executed.
pub unsafe fn raise_user_trap(data: Box<dyn Error + Send + Sync>) -> ! {
    tls::with(|info| info.unwrap().unwind_with(UnwindReason::UserTrap(data)))
}

/// Raises a trap from inside library code immediately.
///
/// This function performs as-if a wasm trap was just executed. This trap
/// payload is then returned from `catch_traps` below.
///
/// # Safety
///
/// Only safe to call when wasm code is on the stack, aka `catch_traps` must
/// have been previous called and not yet returned.
/// Additionally no Rust destructors may be on the stack.
/// They will be skipped and not executed.
pub unsafe fn raise_lib_trap(trap: Trap) -> ! {
    tls::with(|info| info.unwrap().unwind_with(UnwindReason::LibTrap(trap)))
}

/// Carries a Rust panic across wasm code and resumes the panic on the other
/// side.
///
/// # Safety
///
/// Only safe to call when wasm code is on the stack, aka `catch_traps` must
/// have been previously called and not returned. Additionally no Rust destructors may be on the
/// stack. They will be skipped and not executed.
pub unsafe fn resume_panic(payload: Box<dyn Any + Send>) -> ! {
    tls::with(|info| info.unwrap().unwind_with(UnwindReason::Panic(payload)))
}

/// Stores trace message with backtrace.
#[derive(Debug)]
pub enum Trap {
    /// A user-raised trap through `raise_user_trap`.
    User(Box<dyn Error + Send + Sync>),

    /// A trap raised from the Wasm generated code
    ///
    /// Note: this trap is deterministic (assuming a deterministic host implementation)
    Wasm {
        /// The program counter in generated code where this trap happened.
        pc: usize,
        /// Native stack backtrace at the time the trap occurred
        backtrace: Backtrace,
        /// Optional trapcode associated to the signal that caused the trap
        signal_trap: Option<TrapCode>,
    },

    /// A trap raised from a wasm libcall
    ///
    /// Note: this trap is deterministic (assuming a deterministic host implementation)
    Lib {
        /// Code of the trap.
        trap_code: TrapCode,
        /// Native stack backtrace at the time the trap occurred
        backtrace: Backtrace,
    },

    /// A trap indicating that the runtime was unable to allocate sufficient memory.
    ///
    /// Note: this trap is nondeterministic, since it depends on the host system.
    OOM {
        /// Native stack backtrace at the time the OOM occurred
        backtrace: Backtrace,
    },
}

impl Trap {
    /// Construct a new Wasm trap with the given source location and backtrace.
    ///
    /// Internally saves a backtrace when constructed.
    pub fn wasm(pc: usize, backtrace: Backtrace, signal_trap: Option<TrapCode>) -> Self {
        Self::Wasm {
            pc,
            backtrace,
            signal_trap,
        }
    }

    /// Construct a new Wasm trap with the given trap code.
    ///
    /// Internally saves a backtrace when constructed.
    pub fn lib(trap_code: TrapCode) -> Self {
        let backtrace = Backtrace::new_unresolved();
        Self::Lib {
            trap_code,
            backtrace,
        }
    }

    /// Construct a new OOM trap with the given source location and trap code.
    ///
    /// Internally saves a backtrace when constructed.
    pub fn oom() -> Self {
        let backtrace = Backtrace::new_unresolved();
        Self::OOM { backtrace }
    }
}

/// Call the VM function pointed to by `callee`.
///
/// * `callee_env` - the function environment
/// * `trampoline` - the jit-generated trampoline whose ABI takes 3 values, the
///    callee funcenv, the `callee` argument below, and then the `values_vec` argument.
/// * `callee` - the 2nd argument to the `trampoline` function
/// * `values_vec` - points to a buffer which holds the incoming arguments, and to
///   which the outgoing return values will be written.
///
/// Prefer invoking this via `Instance::invoke_trampoline`.
///
/// # Safety
///
/// Wildly unsafe because it calls raw function pointers and reads/writes raw
/// function pointers.
pub unsafe fn wasmer_call_trampoline(
    callee_env: VMFunctionEnvironment,
    trampoline: VMTrampoline,
    callee: *const VMFunctionBody,
    values_vec: *mut u8,
) -> Result<(), Trap> {
    catch_traps(|| {
        mem::transmute::<_, extern "C" fn(VMFunctionEnvironment, *const VMFunctionBody, *mut u8)>(
            trampoline,
        )(callee_env, callee, values_vec);
    })
}

/// Catches any wasm traps that happen within the execution of `closure`,
/// returning them as a `Result`.
///
/// # Safety
///
/// Soundness must not depend on `closure` destructors being run.
pub unsafe fn catch_traps<F>(mut closure: F) -> Result<(), Trap>
where
    F: FnMut(),
{
    return CallThreadState::new().with(|cx| {
        wasmer_register_setjmp(
            cx.jmp_buf.as_ptr(),
            call_closure::<F>,
            &mut closure as *mut F as *mut u8,
        )
    });

    extern "C" fn call_closure<F>(payload: *mut u8)
    where
        F: FnMut(),
    {
        unsafe { (*(payload as *mut F))() }
    }
}

/// Catches any wasm traps that happen within the execution of `closure`,
/// returning them as a `Result`, with the closure contents.
///
/// The main difference from this method and `catch_traps`, is that is able
/// to return the results from the closure.
///
/// # Safety
///
/// Check [`catch_traps`].
pub unsafe fn catch_traps_with_result<F, R>(mut closure: F) -> Result<R, Trap>
where
    F: FnMut() -> R,
{
    let mut global_results = MaybeUninit::<R>::uninit();
    catch_traps(|| {
        global_results.as_mut_ptr().write(closure());
    })?;
    Ok(global_results.assume_init())
}

/// Temporary state stored on the stack which is registered in the `tls` module
/// below for calls into wasm.
pub struct CallThreadState {
    unwind: UnsafeCell<MaybeUninit<UnwindReason>>,
    jmp_buf: Cell<*const u8>,
    prev: Cell<tls::Ptr>,
}

enum UnwindReason {
    /// A panic caused by the host
    Panic(Box<dyn Any + Send>),
    /// A custom error triggered by the user
    UserTrap(Box<dyn Error + Send + Sync>),
    /// A Trap triggered by a wasm libcall
    LibTrap(Trap),
    /// A trap caused by the Wasm generated code
    WasmTrap {
        backtrace: Backtrace,
        pc: usize,
        signal_trap: Option<TrapCode>,
    },
}

impl<'a> CallThreadState {
    #[inline]
    fn new() -> Self {
        Self {
            unwind: UnsafeCell::new(MaybeUninit::uninit()),
            jmp_buf: Cell::new(ptr::null()),
            prev: Cell::new(ptr::null()),
        }
    }

    fn with(self, closure: impl FnOnce(&Self) -> i32) -> Result<(), Trap> {
        let ret = tls::set(&self, || closure(&self))?;
        if ret != 0 {
            return Ok(());
        }
        // We will only reach this path if ret == 0. And that will
        // only happen if a trap did happen. As such, it's safe to
        // assume that the `unwind` field is already initialized
        // at this moment.
        match unsafe { (*self.unwind.get()).as_ptr().read() } {
            UnwindReason::UserTrap(data) => Err(Trap::User(data)),
            UnwindReason::LibTrap(trap) => Err(trap),
            UnwindReason::WasmTrap {
                backtrace,
                pc,
                signal_trap,
            } => Err(Trap::wasm(pc, backtrace, signal_trap)),
            UnwindReason::Panic(panic) => std::panic::resume_unwind(panic),
        }
    }

    fn unwind_with(&self, reason: UnwindReason) -> ! {
        unsafe {
            (*self.unwind.get()).as_mut_ptr().write(reason);
            wasmer_unwind(self.jmp_buf.get());
        }
    }
}

// A private inner module for managing the TLS state that we require across
// calls in wasm. The WebAssembly code is called from C++ and then a trap may
// happen which requires us to read some contextual state to figure out what to
// do with the trap. This `tls` module is used to persist that information from
// the caller to the trap site.
mod tls {
    use super::CallThreadState;
    use crate::Trap;
    use std::mem;
    use std::ptr;

    pub use raw::Ptr;

    // An even *more* inner module for dealing with TLS. This actually has the
    // thread local variable and has functions to access the variable.
    //
    // Note that this is specially done to fully encapsulate that the accessors
    // for tls must not be inlined. Wasmer's async support will employ stack
    // switching which can resume execution on different OS threads. This means
    // that borrows of our TLS pointer must never live across accesses because
    // otherwise the access may be split across two threads and cause unsafety.
    //
    // This also means that extra care is taken by the runtime to save/restore
    // these TLS values when the runtime may have crossed threads.
    mod raw {
        use super::CallThreadState;
        use crate::Trap;
        use std::cell::Cell;
        use std::ptr;

        pub type Ptr = *const CallThreadState;

        // The first entry here is the `Ptr` which is what's used as part of the
        // public interface of this module. The second entry is a boolean which
        // allows the runtime to perform per-thread initialization if necessary
        // for handling traps (e.g. setting up ports on macOS and sigaltstack on
        // Unix).
        thread_local!(static PTR: Cell<Ptr> = Cell::new(ptr::null()));

        #[inline(never)] // see module docs for why this is here
        pub fn replace(val: Ptr) -> Result<Ptr, Trap> {
            PTR.with(|p| {
                // When a new value is configured that means that we may be
                // entering WebAssembly so check to see if this thread has
                // performed per-thread initialization for traps.
                let prev = p.get();
                p.set(val);
                Ok(prev)
            })
        }

        #[inline(never)] // see module docs for why this is here
        pub fn get() -> Ptr {
            PTR.with(|p| p.get())
        }
    }

    /// Opaque state used to help control TLS state across stack switches for
    /// async support.
    pub struct TlsRestore(raw::Ptr);

    impl TlsRestore {
        /// Takes the TLS state that is currently configured and returns a
        /// token that is used to replace it later.
        ///
        /// # Safety
        ///
        /// This is not a safe operation since it's intended to only be used
        /// with stack switching found with fibers and async wasmer.
        pub unsafe fn take() -> Result<Self, Trap> {
            // Our tls pointer must be set at this time, and it must not be
            // null. We need to restore the previous pointer since we're
            // removing ourselves from the call-stack, and in the process we
            // null out our own previous field for safety in case it's
            // accidentally used later.
            let raw = raw::get();
            assert!(!raw.is_null());
            let prev = (*raw).prev.replace(ptr::null());
            raw::replace(prev)?;
            Ok(Self(raw))
        }

        /// Restores a previous tls state back into this thread's TLS.
        ///
        /// # Safety
        ///
        /// This is unsafe because it's intended to only be used within the
        /// context of stack switching within wasmer.
        pub unsafe fn replace(self) -> Result<(), super::Trap> {
            // We need to configure our previous TLS pointer to whatever is in
            // TLS at this time, and then we set the current state to ourselves.
            let prev = raw::get();
            assert!((*self.0).prev.get().is_null());
            (*self.0).prev.set(prev);
            raw::replace(self.0)?;
            Ok(())
        }
    }

    /// Configures thread local state such that for the duration of the
    /// execution of `closure` any call to `with` will yield `ptr`, unless this
    /// is recursively called again.
    pub fn set<R>(state: &CallThreadState, closure: impl FnOnce() -> R) -> Result<R, Trap> {
        struct Reset<'a>(&'a CallThreadState);

        impl Drop for Reset<'_> {
            #[inline]
            fn drop(&mut self) {
                raw::replace(self.0.prev.replace(ptr::null()))
                    .expect("tls should be previously initialized");
            }
        }

        // Note that this extension of the lifetime to `'static` should be
        // safe because we only ever access it below with an anonymous
        // lifetime, meaning `'static` never leaks out of this module.
        let ptr = unsafe { mem::transmute::<*const CallThreadState, _>(state) };
        let prev = raw::replace(ptr)?;
        state.prev.set(prev);
        let _reset = Reset(state);
        Ok(closure())
    }

    /// Returns the last pointer configured with `set` above. Panics if `set`
    /// has not been previously called and not returned.
    pub fn with<R>(closure: impl FnOnce(Option<&CallThreadState>) -> R) -> R {
        let p = raw::get();
        unsafe { closure(if p.is_null() { None } else { Some(&*p) }) }
    }
}

extern "C" fn signal_less_trap_handler(pc: *const u8, trap: TrapCode) {
    let jmp_buf = tls::with(|info| {
        let backtrace = Backtrace::new_unresolved();
        let info = info.unwrap();
        unsafe {
            (*info.unwind.get())
                .as_mut_ptr()
                .write(UnwindReason::WasmTrap {
                    backtrace,
                    signal_trap: Some(trap),
                    pc: pc as usize,
                });
            info.jmp_buf.get()
        }
    });
    unsafe {
        wasmer_unwind(jmp_buf);
    }
}

/// Returns pointer to the trap handler used in VMContext.
pub fn get_trap_handler() -> *const u8 {
    signal_less_trap_handler as *const u8
}

'''
'''--- lib/vm/src/tunables.rs ---
use crate::MemoryError;
use crate::{Memory, Table};
use crate::{MemoryStyle, TableStyle};
use crate::{VMMemoryDefinition, VMTableDefinition};
use std::ptr::NonNull;
use std::sync::Arc;
use wasmer_types::{MemoryType, TableType};

/// An engine delegates the creation of memories, tables, and globals
/// to a foreign implementor of this trait.
pub trait Tunables {
    /// Construct a `MemoryStyle` for the provided `MemoryType`
    fn memory_style(&self, memory: &MemoryType) -> MemoryStyle;

    /// Construct a `TableStyle` for the provided `TableType`
    fn table_style(&self, table: &TableType) -> TableStyle;

    /// Create a memory owned by the host given a [`MemoryType`] and a [`MemoryStyle`].
    fn create_host_memory(
        &self,
        ty: &MemoryType,
        style: &MemoryStyle,
    ) -> Result<Arc<dyn Memory>, MemoryError>;

    /// Create a memory owned by the VM given a [`MemoryType`] and a [`MemoryStyle`].
    ///
    /// # Safety
    /// - `vm_definition_location` must point to a valid location in VM memory.
    unsafe fn create_vm_memory(
        &self,
        ty: &MemoryType,
        style: &MemoryStyle,
        vm_definition_location: NonNull<VMMemoryDefinition>,
    ) -> Result<Arc<dyn Memory>, MemoryError>;

    /// Create a table owned by the host given a [`TableType`] and a [`TableStyle`].
    fn create_host_table(
        &self,
        ty: &TableType,
        style: &TableStyle,
    ) -> Result<Arc<dyn Table>, String>;

    /// Create a table owned by the VM given a [`TableType`] and a [`TableStyle`].
    ///
    /// # Safety
    /// - `vm_definition_location` must point to a valid location in VM memory.
    unsafe fn create_vm_table(
        &self,
        ty: &TableType,
        style: &TableStyle,
        vm_definition_location: NonNull<VMTableDefinition>,
    ) -> Result<Arc<dyn Table>, String>;
}

'''
'''--- lib/vm/src/vmcontext.rs ---
// This file contains code from external sources.
// Attributions: https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md

//! This file declares `VMContext` and several related structs which contain
//! fields that compiled wasm code accesses directly.

use crate::func_data_registry::VMFuncRef;
use crate::global::Global;
use crate::instance::Instance;
use crate::memory::Memory;
use crate::sig_registry::VMSharedSignatureIndex;
use crate::table::Table;
use crate::trap::{Trap, TrapCode};
use crate::VMExternRef;
use std::any::Any;
use std::convert::TryFrom;
use std::fmt;
use std::ptr::{self, NonNull};
use std::sync::Arc;
use std::u32;

/// Union representing the first parameter passed when calling a function.
///
/// It may either be a pointer to the [`VMContext`] if it's a Wasm function
/// or a pointer to arbitrary data controlled by the host if it's a host function.
#[derive(Copy, Clone, Eq)]
pub union VMFunctionEnvironment {
    /// Wasm functions take a pointer to [`VMContext`].
    pub vmctx: *mut VMContext,
    /// Host functions can have custom environments.
    pub host_env: *mut std::ffi::c_void,
}

impl VMFunctionEnvironment {
    /// Check whether the pointer stored is null or not.
    pub fn is_null(&self) -> bool {
        unsafe { self.host_env.is_null() }
    }
}

impl std::fmt::Debug for VMFunctionEnvironment {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        f.debug_struct("VMFunctionEnvironment")
            .field("vmctx_or_hostenv", unsafe { &self.host_env })
            .finish()
    }
}

impl std::cmp::PartialEq for VMFunctionEnvironment {
    fn eq(&self, rhs: &Self) -> bool {
        unsafe { self.host_env as usize == rhs.host_env as usize }
    }
}

impl std::hash::Hash for VMFunctionEnvironment {
    fn hash<H: std::hash::Hasher>(&self, state: &mut H) {
        unsafe {
            self.vmctx.hash(state);
        }
    }
}

/// Represents a continuous region of executable memory starting with a function
/// entry point.
#[derive(Debug)]
#[repr(C)]
pub struct FunctionExtent {
    /// Entry point for normal entry of the function. All addresses in the
    /// function lie after this address.
    pub address: FunctionBodyPtr,
    /// Length in bytes.
    pub length: usize,
}

/// An imported function.
#[derive(Debug, Copy, Clone)]
#[repr(C)]
pub struct VMFunctionImport {
    /// A pointer to the imported function body.
    pub body: FunctionBodyPtr,

    /// Function signature index within the source module.
    pub signature: VMSharedSignatureIndex,

    /// Function call trampoline
    pub trampoline: Option<VMTrampoline>,

    /// A pointer to the `VMContext` that owns the function or host env data.
    pub environment: VMFunctionEnvironment,
}

#[cfg(test)]
mod test_vmfunction_import {
    use super::VMFunctionImport;
    use crate::VMOffsets;
    use memoffset::offset_of;
    use std::mem::size_of;
    use wasmer_types::ModuleInfo;

    #[test]
    fn check_vmfunction_import_offsets() {
        let module = ModuleInfo::new();
        let offsets = VMOffsets::new(size_of::<*mut u8>() as u8).with_module_info(&module);
        assert_eq!(
            size_of::<VMFunctionImport>(),
            usize::from(offsets.size_of_vmfunction_import())
        );
        assert_eq!(
            offset_of!(VMFunctionImport, body),
            usize::from(offsets.vmfunction_import_body())
        );
        assert_eq!(
            offset_of!(VMFunctionImport, environment),
            usize::from(offsets.vmfunction_import_vmctx())
        );
    }
}

/// A locally defined function.
#[derive(Debug, Copy, Clone)]
#[repr(C)]
pub struct VMLocalFunction {
    /// A pointer to the imported function body.
    pub body: FunctionBodyPtr,

    /// Length of the function code
    pub length: u32,

    /// Function signature
    pub signature: VMSharedSignatureIndex,

    /// Trampoline for host->VM function calls.
    pub trampoline: VMTrampoline,
}

/// The `VMDynamicFunctionContext` is the context that dynamic
/// functions will receive when called (rather than `vmctx`).
/// A dynamic function is a function for which we don't know the signature
/// until runtime.
///
/// As such, we need to expose the dynamic function `context`
/// containing the relevant context for running the function indicated
/// in `address`.
#[repr(C)]
pub struct VMDynamicFunctionContext<T: Sized + Send + Sync> {
    /// The address of the inner dynamic function.
    ///
    /// Note: The function must be on the form of
    /// `(*mut T, SignatureIndex, *mut i128)`.
    pub address: *const VMFunctionBody,

    /// The context that the inner dynamic function will receive.
    pub ctx: T,
}

// The `ctx` itself must be `Send`, `address` can be passed between
// threads because all usage is `unsafe` and synchronized.
unsafe impl<T: Sized + Send + Sync> Send for VMDynamicFunctionContext<T> {}
// The `ctx` itself must be `Sync`, `address` can be shared between
// threads because all usage is `unsafe` and synchronized.
unsafe impl<T: Sized + Send + Sync> Sync for VMDynamicFunctionContext<T> {}

impl<T: Sized + Clone + Send + Sync> Clone for VMDynamicFunctionContext<T> {
    fn clone(&self) -> Self {
        Self {
            address: self.address,
            ctx: self.ctx.clone(),
        }
    }
}

#[cfg(test)]
mod test_vmdynamicfunction_import_context {
    use super::VMDynamicFunctionContext;
    use crate::VMOffsets;
    use memoffset::offset_of;
    use std::mem::size_of;
    use wasmer_types::ModuleInfo;

    #[test]
    fn check_vmdynamicfunction_import_context_offsets() {
        let module = ModuleInfo::new();
        let offsets = VMOffsets::new(size_of::<*mut u8>() as u8).with_module_info(&module);
        assert_eq!(
            size_of::<VMDynamicFunctionContext<usize>>(),
            usize::from(offsets.size_of_vmdynamicfunction_import_context())
        );
        assert_eq!(
            offset_of!(VMDynamicFunctionContext<usize>, address),
            usize::from(offsets.vmdynamicfunction_import_context_address())
        );
        assert_eq!(
            offset_of!(VMDynamicFunctionContext<usize>, ctx),
            usize::from(offsets.vmdynamicfunction_import_context_ctx())
        );
    }
}

/// A placeholder byte-sized type which is just used to provide some amount of type
/// safety when dealing with pointers to JIT-compiled function bodies. Note that it's
/// deliberately not Copy, as we shouldn't be carelessly copying function body bytes
/// around.
#[repr(C)]
pub struct VMFunctionBody(u8);

#[cfg(test)]
mod test_vmfunction_body {
    use super::VMFunctionBody;
    use std::mem::size_of;

    #[test]
    fn check_vmfunction_body_offsets() {
        assert_eq!(size_of::<VMFunctionBody>(), 1);
    }
}

/// A pointer to the beginning of the function body.
#[derive(Clone, Copy, Debug)]
#[repr(transparent)]
pub struct FunctionBodyPtr(pub *const VMFunctionBody);

impl std::ops::Deref for FunctionBodyPtr {
    type Target = *const VMFunctionBody;

    fn deref(&self) -> &Self::Target {
        &self.0
    }
}

// SAFETY: The VMFunctionBody that this points to is opaque, so there's no data to read or write
// through this pointer. This is essentially a usize.
unsafe impl Send for FunctionBodyPtr {}

/// SAFETY: The VMFunctionBody that this points to is opaque, so there's no data to read or write
/// through this pointer. This is essentially a usize.
unsafe impl Sync for FunctionBodyPtr {}

/// A function kind is a calling convention into and out of wasm code.
#[derive(Debug, Copy, Clone, PartialEq)]
#[repr(C)]
pub enum VMFunctionKind {
    /// A static function has the native signature:
    /// `extern "C" (vmctx, arg1, arg2...) -> (result1, result2, ...)`.
    ///
    /// This is the default for functions that are defined:
    /// 1. In the Host, natively
    /// 2. In the WebAssembly file
    Static,

    /// A dynamic function has the native signature:
    /// `extern "C" (ctx, &[Value]) -> Vec<Value>`.
    ///
    /// This is the default for functions that are defined:
    /// 1. In the Host, dynamically
    Dynamic,
}

/// The fields compiled code needs to access to utilize a WebAssembly table
/// imported from another instance.
#[derive(Debug, Clone)]
#[repr(C)]
pub struct VMTableImport {
    /// A pointer to the imported table description.
    pub definition: NonNull<VMTableDefinition>,

    /// A pointer to the `Table` that owns the table description.
    pub from: Arc<dyn Table>,
}

#[cfg(test)]
mod test_vmtable_import {
    use super::VMTableImport;
    use crate::VMOffsets;
    use memoffset::offset_of;
    use std::mem::size_of;
    use wasmer_types::ModuleInfo;

    #[test]
    fn check_vmtable_import_offsets() {
        let module = ModuleInfo::new();
        let offsets = VMOffsets::new(size_of::<*mut u8>() as u8).with_module_info(&module);
        assert_eq!(
            size_of::<VMTableImport>(),
            usize::from(offsets.size_of_vmtable_import())
        );
        assert_eq!(
            offset_of!(VMTableImport, definition),
            usize::from(offsets.vmtable_import_definition())
        );
        assert_eq!(
            offset_of!(VMTableImport, from),
            usize::from(offsets.vmtable_import_from())
        );
    }
}

/// The fields compiled code needs to access to utilize a WebAssembly linear
/// memory imported from another instance.
#[derive(Debug, Clone)]
#[repr(C)]
pub struct VMMemoryImport {
    /// A pointer to the imported memory description.
    pub definition: NonNull<VMMemoryDefinition>,

    /// A pointer to the `Memory` that owns the memory description.
    pub from: Arc<dyn Memory>,
}

#[cfg(test)]
mod test_vmmemory_import {
    use super::VMMemoryImport;
    use crate::VMOffsets;
    use memoffset::offset_of;
    use std::mem::size_of;
    use wasmer_types::ModuleInfo;

    #[test]
    fn check_vmmemory_import_offsets() {
        let module = ModuleInfo::new();
        let offsets = VMOffsets::new(size_of::<*mut u8>() as u8).with_module_info(&module);
        assert_eq!(
            size_of::<VMMemoryImport>(),
            usize::from(offsets.size_of_vmmemory_import())
        );
        assert_eq!(
            offset_of!(VMMemoryImport, definition),
            usize::from(offsets.vmmemory_import_definition())
        );
        assert_eq!(
            offset_of!(VMMemoryImport, from),
            usize::from(offsets.vmmemory_import_from())
        );
    }
}

/// The fields compiled code needs to access to utilize a WebAssembly global
/// variable imported from another instance.
#[derive(Debug, Clone)]
#[repr(C)]
pub struct VMGlobalImport {
    /// A pointer to the imported global variable description.
    pub definition: NonNull<VMGlobalDefinition>,

    /// A pointer to the `Global` that owns the global description.
    pub from: Arc<Global>,
}

/// # Safety
/// This data is safe to share between threads because it's plain data that
/// is the user's responsibility to synchronize. Additionally, all operations
/// on `from` are thread-safe through the use of a mutex in [`Global`].
unsafe impl Send for VMGlobalImport {}
/// # Safety
/// This data is safe to share between threads because it's plain data that
/// is the user's responsibility to synchronize. And because it's `Clone`, there's
/// really no difference between passing it by reference or by value as far as
/// correctness in a multi-threaded context is concerned.
unsafe impl Sync for VMGlobalImport {}

#[cfg(test)]
mod test_vmglobal_import {
    use super::VMGlobalImport;
    use crate::VMOffsets;
    use memoffset::offset_of;
    use std::mem::size_of;
    use wasmer_types::ModuleInfo;

    #[test]
    fn check_vmglobal_import_offsets() {
        let module = ModuleInfo::new();
        let offsets = VMOffsets::new(size_of::<*mut u8>() as u8).with_module_info(&module);
        assert_eq!(
            size_of::<VMGlobalImport>(),
            usize::from(offsets.size_of_vmglobal_import())
        );
        assert_eq!(
            offset_of!(VMGlobalImport, definition),
            usize::from(offsets.vmglobal_import_definition())
        );
        assert_eq!(
            offset_of!(VMGlobalImport, from),
            usize::from(offsets.vmglobal_import_from())
        );
    }
}

/// The fields compiled code needs to access to utilize a WebAssembly linear
/// memory defined within the instance, namely the start address and the
/// size in bytes.
#[derive(Debug, Copy, Clone)]
#[repr(C)]
pub struct VMMemoryDefinition {
    /// The start address which is always valid, even if the memory grows.
    pub base: *mut u8,

    /// The current logical size of this linear memory in bytes.
    pub current_length: usize,
}

/// # Safety
/// This data is safe to share between threads because it's plain data that
/// is the user's responsibility to synchronize.
unsafe impl Send for VMMemoryDefinition {}
/// # Safety
/// This data is safe to share between threads because it's plain data that
/// is the user's responsibility to synchronize. And it's `Copy` so there's
/// really no difference between passing it by reference or by value as far as
/// correctness in a multi-threaded context is concerned.
unsafe impl Sync for VMMemoryDefinition {}

impl VMMemoryDefinition {
    /// Do an unsynchronized, non-atomic `memory.copy` for the memory.
    ///
    /// # Errors
    ///
    /// Returns a `Trap` error when the source or destination ranges are out of
    /// bounds.
    ///
    /// # Safety
    ///
    /// The memory is not copied atomically and is not synchronized: it's the
    /// caller's responsibility to synchronize.
    pub(crate) unsafe fn memory_copy(&self, dst: u32, src: u32, len: u32) -> Result<(), Trap> {
        // https://webassembly.github.io/reference-types/core/exec/instructions.html#exec-memory-copy
        if src
            .checked_add(len)
            .map_or(true, |n| usize::try_from(n).unwrap() > self.current_length)
            || dst
                .checked_add(len)
                .map_or(true, |m| usize::try_from(m).unwrap() > self.current_length)
        {
            return Err(Trap::lib(TrapCode::HeapAccessOutOfBounds));
        }

        let dst = usize::try_from(dst).unwrap();
        let src = usize::try_from(src).unwrap();

        // Bounds and casts are checked above, by this point we know that
        // everything is safe.
        let dst = self.base.add(dst);
        let src = self.base.add(src);
        ptr::copy(src, dst, len as usize);

        Ok(())
    }

    /// Perform the `memory.fill` operation for the memory in an unsynchronized,
    /// non-atomic way.
    ///
    /// # Errors
    ///
    /// Returns a `Trap` error if the memory range is out of bounds.
    ///
    /// # Safety
    /// The memory is not filled atomically and is not synchronized: it's the
    /// caller's responsibility to synchronize.
    pub(crate) unsafe fn memory_fill(&self, dst: u32, val: u32, len: u32) -> Result<(), Trap> {
        if dst
            .checked_add(len)
            .map_or(true, |m| usize::try_from(m).unwrap() > self.current_length)
        {
            return Err(Trap::lib(TrapCode::HeapAccessOutOfBounds));
        }

        let dst = isize::try_from(dst).unwrap();
        let val = val as u8;

        // Bounds and casts are checked above, by this point we know that
        // everything is safe.
        let dst = self.base.offset(dst);
        ptr::write_bytes(dst, val, len as usize);

        Ok(())
    }
}

#[cfg(test)]
mod test_vmmemory_definition {
    use super::VMMemoryDefinition;
    use crate::VMOffsets;
    use memoffset::offset_of;
    use std::mem::size_of;
    use wasmer_types::ModuleInfo;

    #[test]
    fn check_vmmemory_definition_offsets() {
        let module = ModuleInfo::new();
        let offsets = VMOffsets::new(size_of::<*mut u8>() as u8).with_module_info(&module);
        assert_eq!(
            size_of::<VMMemoryDefinition>(),
            usize::from(offsets.size_of_vmmemory_definition())
        );
        assert_eq!(
            offset_of!(VMMemoryDefinition, base),
            usize::from(offsets.vmmemory_definition_base())
        );
        assert_eq!(
            offset_of!(VMMemoryDefinition, current_length),
            usize::from(offsets.vmmemory_definition_current_length())
        );
    }
}

/// The fields compiled code needs to access to utilize a WebAssembly table
/// defined within the instance.
#[derive(Debug, Clone, Copy)]
#[repr(C)]
pub struct VMTableDefinition {
    /// Pointer to the table data.
    pub base: *mut u8,

    /// The current number of elements in the table.
    pub current_elements: u32,
}

#[cfg(test)]
mod test_vmtable_definition {
    use super::VMTableDefinition;
    use crate::VMOffsets;
    use memoffset::offset_of;
    use std::mem::size_of;
    use wasmer_types::ModuleInfo;

    #[test]
    fn check_vmtable_definition_offsets() {
        let module = ModuleInfo::new();
        let offsets = VMOffsets::new(size_of::<*mut u8>() as u8).with_module_info(&module);
        assert_eq!(
            size_of::<VMTableDefinition>(),
            usize::from(offsets.size_of_vmtable_definition())
        );
        assert_eq!(
            offset_of!(VMTableDefinition, base),
            usize::from(offsets.vmtable_definition_base())
        );
        assert_eq!(
            offset_of!(VMTableDefinition, current_elements),
            usize::from(offsets.vmtable_definition_current_elements())
        );
    }
}

/// A typesafe wrapper around the storage for a global variables.
///
/// # Safety
///
/// Accessing the different members of this union is always safe because there
/// are no invalid values for any of the types and the whole object is
/// initialized by VMGlobalDefinition::new().
#[derive(Clone, Copy)]
#[repr(C, align(16))]
pub union VMGlobalDefinitionStorage {
    as_i32: i32,
    as_u32: u32,
    as_f32: f32,
    as_i64: i64,
    as_u64: u64,
    as_f64: f64,
    as_u128: u128,
    as_funcref: VMFuncRef,
    as_externref: VMExternRef,
    bytes: [u8; 16],
}

impl fmt::Debug for VMGlobalDefinitionStorage {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        f.debug_struct("VMGlobalDefinitionStorage")
            .field("bytes", unsafe { &self.bytes })
            .finish()
    }
}

/// The storage for a WebAssembly global defined within the instance.
///
/// TODO: Pack the globals more densely, rather than using the same size
/// for every type.
#[derive(Debug, Clone)]
#[repr(C, align(16))]
pub struct VMGlobalDefinition {
    storage: VMGlobalDefinitionStorage,
    // If more elements are added here, remember to add offset_of tests below!
}

#[cfg(test)]
mod test_vmglobal_definition {
    use super::VMGlobalDefinition;
    use crate::{VMFuncRef, VMOffsets};
    use more_asserts::assert_ge;
    use std::mem::{align_of, size_of};
    use wasmer_types::ModuleInfo;

    #[test]
    fn check_vmglobal_definition_alignment() {
        assert_ge!(align_of::<VMGlobalDefinition>(), align_of::<i32>());
        assert_ge!(align_of::<VMGlobalDefinition>(), align_of::<i64>());
        assert_ge!(align_of::<VMGlobalDefinition>(), align_of::<f32>());
        assert_ge!(align_of::<VMGlobalDefinition>(), align_of::<f64>());
        assert_ge!(align_of::<VMGlobalDefinition>(), align_of::<VMFuncRef>());
        assert_ge!(align_of::<VMGlobalDefinition>(), align_of::<[u8; 16]>());
    }

    #[test]
    fn check_vmglobal_definition_offsets() {
        let module = ModuleInfo::new();
        let offsets = VMOffsets::new(size_of::<*mut u8>() as u8).with_module_info(&module);
        assert_eq!(
            size_of::<*const VMGlobalDefinition>(),
            usize::from(offsets.size_of_vmglobal_local())
        );
    }

    #[test]
    fn check_vmglobal_begins_aligned() {
        let module = ModuleInfo::new();
        let offsets = VMOffsets::new(size_of::<*mut u8>() as u8).with_module_info(&module);
        assert_eq!(offsets.vmctx_globals_begin() % 16, 0);
    }
}

impl VMGlobalDefinition {
    /// Construct a `VMGlobalDefinition`.
    pub fn new() -> Self {
        Self {
            storage: VMGlobalDefinitionStorage { bytes: [0; 16] },
        }
    }

    /// Return the value as an i32.
    ///
    /// If this is not an I32 typed global it is unspecified what value is returned.
    pub fn to_i32(&self) -> i32 {
        unsafe { self.storage.as_i32 }
    }

    /// Return a mutable reference to the value as an i32.
    ///
    /// # Safety
    ///
    /// It is the callers responsibility to make sure the global has I32 type.
    /// Until the returned borrow is dropped, reads and writes of this global
    /// must be done exclusively through this borrow. That includes reads and
    /// writes of globals inside wasm functions.
    pub unsafe fn as_i32_mut(&mut self) -> &mut i32 {
        &mut self.storage.as_i32
    }

    /// Return a reference to the value as an u32.
    ///
    /// If this is not an I32 typed global it is unspecified what value is returned.
    pub fn to_u32(&self) -> u32 {
        unsafe { self.storage.as_u32 }
    }

    /// Return a mutable reference to the value as an u32.
    ///
    /// # Safety
    ///
    /// It is the callers responsibility to make sure the global has I32 type.
    /// Until the returned borrow is dropped, reads and writes of this global
    /// must be done exclusively through this borrow. That includes reads and
    /// writes of globals inside wasm functions.
    pub unsafe fn as_u32_mut(&mut self) -> &mut u32 {
        &mut self.storage.as_u32
    }

    /// Return a reference to the value as an i64.
    ///
    /// If this is not an I64 typed global it is unspecified what value is returned.
    pub fn to_i64(&self) -> i64 {
        unsafe { self.storage.as_i64 }
    }

    /// Return a mutable reference to the value as an i64.
    ///
    /// # Safety
    ///
    /// It is the callers responsibility to make sure the global has I32 type.
    /// Until the returned borrow is dropped, reads and writes of this global
    /// must be done exclusively through this borrow. That includes reads and
    /// writes of globals inside wasm functions.
    pub unsafe fn as_i64_mut(&mut self) -> &mut i64 {
        &mut self.storage.as_i64
    }

    /// Return a reference to the value as an u64.
    ///
    /// If this is not an I64 typed global it is unspecified what value is returned.
    pub fn to_u64(&self) -> u64 {
        unsafe { self.storage.as_u64 }
    }

    /// Return a mutable reference to the value as an u64.
    ///
    /// # Safety
    ///
    /// It is the callers responsibility to make sure the global has I64 type.
    /// Until the returned borrow is dropped, reads and writes of this global
    /// must be done exclusively through this borrow. That includes reads and
    /// writes of globals inside wasm functions.
    pub unsafe fn as_u64_mut(&mut self) -> &mut u64 {
        &mut self.storage.as_u64
    }

    /// Return a reference to the value as an f32.
    ///
    /// If this is not an F32 typed global it is unspecified what value is returned.
    pub fn to_f32(&self) -> f32 {
        unsafe { self.storage.as_f32 }
    }

    /// Return a mutable reference to the value as an f32.
    ///
    /// # Safety
    ///
    /// It is the callers responsibility to make sure the global has F32 type.
    /// Until the returned borrow is dropped, reads and writes of this global
    /// must be done exclusively through this borrow. That includes reads and
    /// writes of globals inside wasm functions.
    pub unsafe fn as_f32_mut(&mut self) -> &mut f32 {
        &mut self.storage.as_f32
    }

    /// Return a reference to the value as an f64.
    ///
    /// If this is not an F64 typed global it is unspecified what value is returned.
    pub fn to_f64(&self) -> f64 {
        unsafe { self.storage.as_f64 }
    }

    /// Return a mutable reference to the value as an f64.
    ///
    /// # Safety
    ///
    /// It is the callers responsibility to make sure the global has F64 type.
    /// Until the returned borrow is dropped, reads and writes of this global
    /// must be done exclusively through this borrow. That includes reads and
    /// writes of globals inside wasm functions.
    pub unsafe fn as_f64_mut(&mut self) -> &mut f64 {
        &mut self.storage.as_f64
    }

    /// Return a reference to the value as a `VMFuncRef`.
    ///
    /// If this is not a `VMFuncRef` typed global it is unspecified what value is returned.
    pub fn to_funcref(&self) -> VMFuncRef {
        unsafe { self.storage.as_funcref }
    }

    /// Return a mutable reference to the value as a `VMFuncRef`.
    ///
    /// # Safety
    ///
    /// It is the callers responsibility to make sure the global has `VMFuncRef` type.
    /// Until the returned borrow is dropped, reads and writes of this global
    /// must be done exclusively through this borrow. That includes reads and
    /// writes of globals inside wasm functions.
    pub unsafe fn as_funcref_mut(&mut self) -> &mut VMFuncRef {
        &mut self.storage.as_funcref
    }

    /// Return a mutable reference to the value as an `VMExternRef`.
    ///
    /// # Safety
    ///
    /// It is the callers responsibility to make sure the global has I32 type.
    /// Until the returned borrow is dropped, reads and writes of this global
    /// must be done exclusively through this borrow. That includes reads and
    /// writes of globals inside wasm functions.
    pub unsafe fn as_externref_mut(&mut self) -> &mut VMExternRef {
        &mut self.storage.as_externref
    }

    /// Return a reference to the value as an `VMExternRef`.
    ///
    /// If this is not an I64 typed global it is unspecified what value is returned.
    pub fn to_externref(&self) -> VMExternRef {
        unsafe { self.storage.as_externref }
    }

    /// Return a reference to the value as an u128.
    ///
    /// If this is not an V128 typed global it is unspecified what value is returned.
    pub fn to_u128(&self) -> u128 {
        unsafe { self.storage.as_u128 }
    }

    /// Return a mutable reference to the value as an u128.
    ///
    /// # Safety
    ///
    /// It is the callers responsibility to make sure the global has V128 type.
    /// Until the returned borrow is dropped, reads and writes of this global
    /// must be done exclusively through this borrow. That includes reads and
    /// writes of globals inside wasm functions.
    pub unsafe fn as_u128_mut(&mut self) -> &mut u128 {
        &mut self.storage.as_u128
    }

    /// Return a reference to the value as bytes.
    pub fn to_bytes(&self) -> [u8; 16] {
        unsafe { self.storage.bytes }
    }

    /// Return a mutable reference to the value as bytes.
    ///
    /// # Safety
    ///
    /// Until the returned borrow is dropped, reads and writes of this global
    /// must be done exclusively through this borrow. That includes reads and
    /// writes of globals inside wasm functions.
    pub unsafe fn as_bytes_mut(&mut self) -> &mut [u8; 16] {
        &mut self.storage.bytes
    }
}

#[cfg(test)]
mod test_vmshared_signature_index {
    use super::VMSharedSignatureIndex;
    use crate::vmoffsets::{TargetSharedSignatureIndex, VMOffsets};
    use std::mem::size_of;
    use wasmer_types::ModuleInfo;

    #[test]
    fn check_vmshared_signature_index() {
        let module = ModuleInfo::new();
        let offsets = VMOffsets::new(size_of::<*mut u8>() as u8).with_module_info(&module);
        assert_eq!(
            size_of::<VMSharedSignatureIndex>(),
            usize::from(offsets.size_of_vmshared_signature_index())
        );
    }

    #[test]
    fn check_target_shared_signature_index() {
        assert_eq!(
            size_of::<VMSharedSignatureIndex>(),
            size_of::<TargetSharedSignatureIndex>()
        );
    }
}

/// The VM caller-checked "anyfunc" record, for caller-side signature checking.
/// It consists of the actual function pointer and a signature id to be checked
/// by the caller.
#[derive(Debug, Clone, Copy, Hash, PartialEq, Eq)]
#[repr(C)]
pub struct VMCallerCheckedAnyfunc {
    /// Function body.
    pub func_ptr: *const VMFunctionBody,
    /// Function signature id.
    pub type_index: VMSharedSignatureIndex,
    /// Function `VMContext` or host env.
    pub vmctx: VMFunctionEnvironment,
    // If more elements are added here, remember to add offset_of tests below!
}

#[cfg(test)]
mod test_vmcaller_checked_anyfunc {
    use super::VMCallerCheckedAnyfunc;
    use crate::VMOffsets;
    use memoffset::offset_of;
    use std::mem::size_of;
    use wasmer_types::ModuleInfo;

    #[test]
    fn check_vmcaller_checked_anyfunc_offsets() {
        let module = ModuleInfo::new();
        let offsets = VMOffsets::new(size_of::<*mut u8>() as u8).with_module_info(&module);
        assert_eq!(
            size_of::<VMCallerCheckedAnyfunc>(),
            usize::from(offsets.size_of_vmcaller_checked_anyfunc())
        );
        assert_eq!(
            offset_of!(VMCallerCheckedAnyfunc, func_ptr),
            usize::from(offsets.vmcaller_checked_anyfunc_func_ptr())
        );
        assert_eq!(
            offset_of!(VMCallerCheckedAnyfunc, type_index),
            usize::from(offsets.vmcaller_checked_anyfunc_type_index())
        );
        assert_eq!(
            offset_of!(VMCallerCheckedAnyfunc, vmctx),
            usize::from(offsets.vmcaller_checked_anyfunc_vmctx())
        );
    }
}

/// An index type for builtin functions.
#[derive(Copy, Clone, Debug)]
pub struct VMBuiltinFunctionIndex(u32);

impl VMBuiltinFunctionIndex {
    /// Returns an index for wasm's `memory.grow` builtin function.
    pub const fn get_memory32_grow_index() -> Self {
        Self(0)
    }
    /// Returns an index for wasm's imported `memory.grow` builtin function.
    pub const fn get_imported_memory32_grow_index() -> Self {
        Self(1)
    }
    /// Returns an index for wasm's `memory.size` builtin function.
    pub const fn get_memory32_size_index() -> Self {
        Self(2)
    }
    /// Returns an index for wasm's imported `memory.size` builtin function.
    pub const fn get_imported_memory32_size_index() -> Self {
        Self(3)
    }
    /// Returns an index for wasm's `table.copy` when both tables are locally
    /// defined.
    pub const fn get_table_copy_index() -> Self {
        Self(4)
    }
    /// Returns an index for wasm's `table.init`.
    pub const fn get_table_init_index() -> Self {
        Self(5)
    }
    /// Returns an index for wasm's `elem.drop`.
    pub const fn get_elem_drop_index() -> Self {
        Self(6)
    }
    /// Returns an index for wasm's `memory.copy` for locally defined memories.
    pub const fn get_memory_copy_index() -> Self {
        Self(7)
    }
    /// Returns an index for wasm's `memory.copy` for imported memories.
    pub const fn get_imported_memory_copy_index() -> Self {
        Self(8)
    }
    /// Returns an index for wasm's `memory.fill` for locally defined memories.
    pub const fn get_memory_fill_index() -> Self {
        Self(9)
    }
    /// Returns an index for wasm's `memory.fill` for imported memories.
    pub const fn get_imported_memory_fill_index() -> Self {
        Self(10)
    }
    /// Returns an index for wasm's `memory.init` instruction.
    pub const fn get_memory_init_index() -> Self {
        Self(11)
    }
    /// Returns an index for wasm's `data.drop` instruction.
    pub const fn get_data_drop_index() -> Self {
        Self(12)
    }
    /// Returns an index for wasm's `raise_trap` instruction.
    pub const fn get_raise_trap_index() -> Self {
        Self(13)
    }
    /// Returns an index for wasm's `table.size` instruction for local tables.
    pub const fn get_table_size_index() -> Self {
        Self(14)
    }
    /// Returns an index for wasm's `table.size` instruction for imported tables.
    pub const fn get_imported_table_size_index() -> Self {
        Self(15)
    }
    /// Returns an index for wasm's `table.grow` instruction for local tables.
    pub const fn get_table_grow_index() -> Self {
        Self(16)
    }
    /// Returns an index for wasm's `table.grow` instruction for imported tables.
    pub const fn get_imported_table_grow_index() -> Self {
        Self(17)
    }
    /// Returns an index for wasm's `table.get` instruction for local tables.
    pub const fn get_table_get_index() -> Self {
        Self(18)
    }
    /// Returns an index for wasm's `table.get` instruction for imported tables.
    pub const fn get_imported_table_get_index() -> Self {
        Self(19)
    }
    /// Returns an index for wasm's `table.set` instruction for local tables.
    pub const fn get_table_set_index() -> Self {
        Self(20)
    }
    /// Returns an index for wasm's `table.set` instruction for imported tables.
    pub const fn get_imported_table_set_index() -> Self {
        Self(21)
    }
    /// Returns an index for wasm's `func.ref` instruction.
    pub const fn get_func_ref_index() -> Self {
        Self(22)
    }
    /// Returns an index for wasm's `table.fill` instruction for local tables.
    pub const fn get_table_fill_index() -> Self {
        Self(23)
    }
    /// Returns an index for a function to increment the externref count.
    pub const fn get_externref_inc_index() -> Self {
        Self(24)
    }
    /// Returns an index for a function to decrement the externref count.
    pub const fn get_externref_dec_index() -> Self {
        Self(25)
    }
    /// Returns the total number of builtin functions.
    pub const fn builtin_functions_total_number() -> u32 {
        26
    }

    /// Return the index as an u32 number.
    pub const fn index(self) -> u32 {
        self.0
    }
}

/// An array that stores addresses of builtin functions. We translate code
/// to use indirect calls. This way, we don't have to patch the code.
#[repr(C)]
pub struct VMBuiltinFunctionsArray {
    ptrs: [usize; Self::len()],
}

impl VMBuiltinFunctionsArray {
    pub const fn len() -> usize {
        VMBuiltinFunctionIndex::builtin_functions_total_number() as usize
    }

    pub fn initialized() -> Self {
        use crate::libcalls::*;

        let mut ptrs = [0; Self::len()];

        ptrs[VMBuiltinFunctionIndex::get_memory32_grow_index().index() as usize] =
            wasmer_vm_memory32_grow as usize;
        ptrs[VMBuiltinFunctionIndex::get_imported_memory32_grow_index().index() as usize] =
            wasmer_vm_imported_memory32_grow as usize;

        ptrs[VMBuiltinFunctionIndex::get_memory32_size_index().index() as usize] =
            wasmer_vm_memory32_size as usize;
        ptrs[VMBuiltinFunctionIndex::get_imported_memory32_size_index().index() as usize] =
            wasmer_vm_imported_memory32_size as usize;

        ptrs[VMBuiltinFunctionIndex::get_table_copy_index().index() as usize] =
            wasmer_vm_table_copy as usize;

        ptrs[VMBuiltinFunctionIndex::get_table_init_index().index() as usize] =
            wasmer_vm_table_init as usize;
        ptrs[VMBuiltinFunctionIndex::get_elem_drop_index().index() as usize] =
            wasmer_vm_elem_drop as usize;

        ptrs[VMBuiltinFunctionIndex::get_memory_copy_index().index() as usize] =
            wasmer_vm_memory32_copy as usize;
        ptrs[VMBuiltinFunctionIndex::get_imported_memory_copy_index().index() as usize] =
            wasmer_vm_imported_memory32_copy as usize;
        ptrs[VMBuiltinFunctionIndex::get_memory_fill_index().index() as usize] =
            wasmer_vm_memory32_fill as usize;
        ptrs[VMBuiltinFunctionIndex::get_imported_memory_fill_index().index() as usize] =
            wasmer_vm_imported_memory32_fill as usize;
        ptrs[VMBuiltinFunctionIndex::get_memory_init_index().index() as usize] =
            wasmer_vm_memory32_init as usize;
        ptrs[VMBuiltinFunctionIndex::get_data_drop_index().index() as usize] =
            wasmer_vm_data_drop as usize;
        ptrs[VMBuiltinFunctionIndex::get_raise_trap_index().index() as usize] =
            wasmer_vm_raise_trap as usize;
        ptrs[VMBuiltinFunctionIndex::get_table_size_index().index() as usize] =
            wasmer_vm_table_size as usize;
        ptrs[VMBuiltinFunctionIndex::get_imported_table_size_index().index() as usize] =
            wasmer_vm_imported_table_size as usize;
        ptrs[VMBuiltinFunctionIndex::get_table_grow_index().index() as usize] =
            wasmer_vm_table_grow as usize;
        ptrs[VMBuiltinFunctionIndex::get_imported_table_grow_index().index() as usize] =
            wasmer_vm_imported_table_grow as usize;
        ptrs[VMBuiltinFunctionIndex::get_table_get_index().index() as usize] =
            wasmer_vm_table_get as usize;
        ptrs[VMBuiltinFunctionIndex::get_imported_table_get_index().index() as usize] =
            wasmer_vm_imported_table_get as usize;
        ptrs[VMBuiltinFunctionIndex::get_table_set_index().index() as usize] =
            wasmer_vm_table_set as usize;
        ptrs[VMBuiltinFunctionIndex::get_imported_table_set_index().index() as usize] =
            wasmer_vm_imported_table_set as usize;
        ptrs[VMBuiltinFunctionIndex::get_func_ref_index().index() as usize] =
            wasmer_vm_func_ref as usize;
        ptrs[VMBuiltinFunctionIndex::get_table_fill_index().index() as usize] =
            wasmer_vm_table_fill as usize;
        ptrs[VMBuiltinFunctionIndex::get_externref_inc_index().index() as usize] =
            wasmer_vm_externref_inc as usize;
        ptrs[VMBuiltinFunctionIndex::get_externref_dec_index().index() as usize] =
            wasmer_vm_externref_dec as usize;

        debug_assert!(ptrs.iter().cloned().all(|p| p != 0));

        Self { ptrs }
    }
}

/// The VM "context", which is pointed to by the `vmctx` arg in the compiler.
/// This has information about globals, memories, tables, and other runtime
/// state associated with the current instance.
///
/// The struct here is empty, as the sizes of these fields are dynamic, and
/// we can't describe them in Rust's type system. Sufficient memory is
/// allocated at runtime.
///
/// TODO: We could move the globals into the `vmctx` allocation too.
#[derive(Debug)]
#[repr(C, align(16))] // align 16 since globals are aligned to that and contained inside
pub struct VMContext {}

impl VMContext {
    /// Return a mutable reference to the associated `Instance`.
    ///
    /// # Safety
    /// This is unsafe because it doesn't work on just any `VMContext`, it must
    /// be a `VMContext` allocated as part of an `Instance`.
    #[allow(clippy::cast_ptr_alignment)]
    #[inline]
    pub(crate) unsafe fn instance(&self) -> &Instance {
        &*((self as *const Self as *mut u8).offset(-Instance::vmctx_offset()) as *const Instance)
    }

    /// Return a reference to the host state associated with this `Instance`.
    ///
    /// # Safety
    /// This is unsafe because it doesn't work on just any `VMContext`, it must
    /// be a `VMContext` allocated as part of an `Instance`.
    #[inline]
    pub unsafe fn host_state(&self) -> &dyn Any {
        self.instance().host_state()
    }
}

///
pub type VMTrampoline = unsafe extern "C" fn(
    *mut VMContext,        // callee vmctx
    *const VMFunctionBody, // function we're actually calling
    *mut u128,             // space for arguments and return values
);

/// Pointers to section data.
#[derive(Clone, Copy, Debug)]
#[repr(transparent)]
pub struct SectionBodyPtr(pub *const u8);

impl std::ops::Deref for SectionBodyPtr {
    type Target = *const u8;

    fn deref(&self) -> &Self::Target {
        &self.0
    }
}

'''
'''--- lib/vm/src/vmoffsets.rs ---
// This file contains code from external sources.
// Attributions: https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md

//! Offsets and sizes of various structs in wasmer-vm's vmcontext
//! module.

#![deny(rustdoc::broken_intra_doc_links)]

use crate::VMBuiltinFunctionIndex;
use more_asserts::assert_lt;
use std::convert::TryFrom;
use std::mem::align_of;
use wasmer_types::{
    FunctionIndex, GlobalIndex, LocalGlobalIndex, LocalMemoryIndex, LocalTableIndex, MemoryIndex,
    ModuleInfo, SignatureIndex, TableIndex,
};

#[cfg(target_pointer_width = "32")]
fn cast_to_u32(sz: usize) -> u32 {
    u32::try_from(sz).unwrap()
}
#[cfg(target_pointer_width = "64")]
fn cast_to_u32(sz: usize) -> u32 {
    u32::try_from(sz).expect("overflow in cast from usize to u32")
}

/// Align an offset used in this module to a specific byte-width by rounding up
const fn align(offset: u32, width: u32) -> u32 {
    (offset + (width - 1)) / width * width
}

/// This class computes offsets to fields within [`VMContext`] and other
/// related structs that JIT code accesses directly.
///
/// [`VMContext`]: crate::vmcontext::VMContext
#[derive(Clone, Debug)]
pub struct VMOffsets {
    /// The size in bytes of a pointer on the target.
    pub pointer_size: u8,
    /// The number of signature declarations in the module.
    pub num_signature_ids: u32,
    /// The number of imported functions in the module.
    pub num_imported_functions: u32,
    /// The number of imported tables in the module.
    pub num_imported_tables: u32,
    /// The number of imported memories in the module.
    pub num_imported_memories: u32,
    /// The number of imported globals in the module.
    pub num_imported_globals: u32,
    /// The number of defined tables in the module.
    pub num_local_tables: u32,
    /// The number of defined memories in the module.
    pub num_local_memories: u32,
    /// The number of defined globals in the module.
    pub num_local_globals: u32,
    /// If the module has trap handler.
    pub has_trap_handlers: bool,
}

impl VMOffsets {
    /// Return a new `VMOffsets` instance, for a given pointer size.
    ///
    /// The returned `VMOffsets` has no entities. Add entities with other builder methods for this
    /// type.
    pub fn new(pointer_size: u8) -> Self {
        Self {
            pointer_size,
            num_signature_ids: 0,
            num_imported_functions: 0,
            num_imported_tables: 0,
            num_imported_memories: 0,
            num_imported_globals: 0,
            num_local_tables: 0,
            num_local_memories: 0,
            num_local_globals: 0,
            has_trap_handlers: false,
        }
    }

    /// Return a new `VMOffsets` instance, for a host's pointer size.
    ///
    /// The returned `VMOffsets` has no entities. Add entities with other builder methods for this
    /// type.
    pub fn for_host() -> Self {
        Self::new(std::mem::size_of::<*const u8>() as u8)
    }

    /// Add imports and locals from the provided ModuleInfo.
    pub fn with_module_info(mut self, module: &ModuleInfo) -> Self {
        self.num_imported_functions = module.import_counts.functions;
        self.num_imported_tables = module.import_counts.tables;
        self.num_imported_memories = module.import_counts.memories;
        self.num_imported_globals = module.import_counts.globals;
        self.num_signature_ids = cast_to_u32(module.signatures.len());
        // FIXME = these should most likely be subtracting the corresponding imports!!?
        self.num_local_tables = cast_to_u32(module.tables.len());
        self.num_local_memories = cast_to_u32(module.memories.len());
        self.num_local_globals = cast_to_u32(module.globals.len());
        self.has_trap_handlers = true;
        self
    }

    /// Add imports and locals from the provided ModuleInfo.
    pub fn with_archived_module_info(mut self, module: &rkyv::Archived<ModuleInfo>) -> Self {
        self.num_imported_functions = module.import_counts.functions;
        self.num_imported_tables = module.import_counts.tables;
        self.num_imported_memories = module.import_counts.memories;
        self.num_imported_globals = module.import_counts.globals;
        self.num_signature_ids = cast_to_u32(module.signatures.len());
        // FIXME = these should most likely be subtracting the corresponding imports!!?
        self.num_local_tables = cast_to_u32(module.tables.len());
        self.num_local_memories = cast_to_u32(module.memories.len());
        self.num_local_globals = cast_to_u32(module.globals.len());
        self.has_trap_handlers = true;
        self
    }
}

/// Offsets for [`VMFunctionImport`].
///
/// [`VMFunctionImport`]: crate::vmcontext::VMFunctionImport
impl VMOffsets {
    /// The offset of the `body` field.
    #[allow(clippy::erasing_op)]
    pub const fn vmfunction_import_body(&self) -> u8 {
        0 * self.pointer_size
    }

    /// The offset of the `vmctx` field.
    #[allow(clippy::identity_op)]
    pub const fn vmfunction_import_vmctx(&self) -> u8 {
        3 * self.pointer_size
    }

    /// Return the size of [`VMFunctionImport`].
    ///
    /// [`VMFunctionImport`]: crate::vmcontext::VMFunctionImport
    pub const fn size_of_vmfunction_import(&self) -> u8 {
        4 * self.pointer_size
    }
}

/// Offsets for [`VMDynamicFunctionContext`].
///
/// [`VMDynamicFunctionContext`]: crate::vmcontext::VMDynamicFunctionContext
impl VMOffsets {
    /// The offset of the `address` field.
    #[allow(clippy::erasing_op)]
    pub const fn vmdynamicfunction_import_context_address(&self) -> u8 {
        0 * self.pointer_size
    }

    /// The offset of the `ctx` field.
    #[allow(clippy::identity_op)]
    pub const fn vmdynamicfunction_import_context_ctx(&self) -> u8 {
        1 * self.pointer_size
    }

    /// Return the size of [`VMDynamicFunctionContext`].
    ///
    /// [`VMDynamicFunctionContext`]: crate::vmcontext::VMDynamicFunctionContext
    pub const fn size_of_vmdynamicfunction_import_context(&self) -> u8 {
        2 * self.pointer_size
    }
}

/// Offsets for `*const VMFunctionBody`.
impl VMOffsets {
    /// The size of the `current_elements` field.
    #[allow(clippy::identity_op)]
    pub const fn size_of_vmfunction_body_ptr(&self) -> u8 {
        1 * self.pointer_size
    }
}

/// Offsets for [`VMTableImport`].
///
/// [`VMTableImport`]: crate::vmcontext::VMTableImport
impl VMOffsets {
    /// The offset of the `definition` field.
    #[allow(clippy::erasing_op)]
    pub const fn vmtable_import_definition(&self) -> u8 {
        0 * self.pointer_size
    }

    /// The offset of the `from` field.
    #[allow(clippy::identity_op)]
    pub const fn vmtable_import_from(&self) -> u8 {
        1 * self.pointer_size
    }

    /// Return the size of [`VMTableImport`].
    ///
    /// [`VMTableImport`]: crate::vmcontext::VMTableImport
    pub const fn size_of_vmtable_import(&self) -> u8 {
        3 * self.pointer_size
    }
}

/// Offsets for [`VMTableDefinition`].
///
/// [`VMTableDefinition`]: crate::vmcontext::VMTableDefinition
impl VMOffsets {
    /// The offset of the `base` field.
    #[allow(clippy::erasing_op)]
    pub const fn vmtable_definition_base(&self) -> u8 {
        0 * self.pointer_size
    }

    /// The offset of the `current_elements` field.
    #[allow(clippy::identity_op)]
    pub const fn vmtable_definition_current_elements(&self) -> u8 {
        1 * self.pointer_size
    }

    /// The size of the `current_elements` field.
    pub const fn size_of_vmtable_definition_current_elements(&self) -> u8 {
        4
    }

    /// Return the size of [`VMTableDefinition`].
    ///
    /// [`VMTableDefinition`]: crate::vmcontext::VMTableDefinition
    pub const fn size_of_vmtable_definition(&self) -> u8 {
        2 * self.pointer_size
    }
}

/// Offsets for [`VMMemoryImport`].
///
/// [`VMMemoryImport`]: crate::vmcontext::VMMemoryImport
impl VMOffsets {
    /// The offset of the `from` field.
    #[allow(clippy::erasing_op)]
    pub const fn vmmemory_import_definition(&self) -> u8 {
        0 * self.pointer_size
    }

    /// The offset of the `from` field.
    #[allow(clippy::identity_op)]
    pub const fn vmmemory_import_from(&self) -> u8 {
        1 * self.pointer_size
    }

    /// Return the size of [`VMMemoryImport`].
    ///
    /// [`VMMemoryImport`]: crate::vmcontext::VMMemoryImport
    pub const fn size_of_vmmemory_import(&self) -> u8 {
        3 * self.pointer_size
    }
}

/// Offsets for [`VMMemoryDefinition`].
///
/// [`VMMemoryDefinition`]: crate::vmcontext::VMMemoryDefinition
impl VMOffsets {
    /// The offset of the `base` field.
    #[allow(clippy::erasing_op)]
    pub const fn vmmemory_definition_base(&self) -> u8 {
        0 * self.pointer_size
    }

    /// The offset of the `current_length` field.
    #[allow(clippy::identity_op)]
    pub const fn vmmemory_definition_current_length(&self) -> u8 {
        1 * self.pointer_size
    }

    /// The size of the `current_length` field.
    pub const fn size_of_vmmemory_definition_current_length(&self) -> u8 {
        4
    }

    /// Return the size of [`VMMemoryDefinition`].
    ///
    /// [`VMMemoryDefinition`]: crate::vmcontext::VMMemoryDefinition
    pub const fn size_of_vmmemory_definition(&self) -> u8 {
        2 * self.pointer_size
    }
}

/// Offsets for [`VMGlobalImport`].
///
/// [`VMGlobalImport`]: crate::vmcontext::VMGlobalImport
impl VMOffsets {
    /// The offset of the `definition` field.
    #[allow(clippy::erasing_op)]
    pub const fn vmglobal_import_definition(&self) -> u8 {
        0 * self.pointer_size
    }

    /// The offset of the `from` field.
    #[allow(clippy::identity_op)]
    pub const fn vmglobal_import_from(&self) -> u8 {
        1 * self.pointer_size
    }

    /// Return the size of [`VMGlobalImport`].
    ///
    /// [`VMGlobalImport`]: crate::vmcontext::VMGlobalImport
    #[allow(clippy::identity_op)]
    pub const fn size_of_vmglobal_import(&self) -> u8 {
        2 * self.pointer_size
    }
}

/// Offsets for a non-null pointer to a [`VMGlobalDefinition`] used as a local global.
///
/// [`VMGlobalDefinition`]: crate::vmcontext::VMGlobalDefinition
impl VMOffsets {
    /// Return the size of a pointer to a [`VMGlobalDefinition`];
    ///
    /// The underlying global itself is the size of the largest value type (i.e. a V128),
    /// however the size of this type is just the size of a pointer.
    ///
    /// [`VMGlobalDefinition`]: crate::vmcontext::VMGlobalDefinition
    pub const fn size_of_vmglobal_local(&self) -> u8 {
        self.pointer_size
    }
}

/// Offsets for [`VMSharedSignatureIndex`].
///
/// [`VMSharedSignatureIndex`]: crate::vmcontext::VMSharedSignatureIndex
impl VMOffsets {
    /// Return the size of [`VMSharedSignatureIndex`].
    ///
    /// [`VMSharedSignatureIndex`]: crate::vmcontext::VMSharedSignatureIndex
    pub const fn size_of_vmshared_signature_index(&self) -> u8 {
        4
    }
}

/// Offsets for [`VMCallerCheckedAnyfunc`].
///
/// [`VMCallerCheckedAnyfunc`]: crate::vmcontext::VMCallerCheckedAnyfunc
impl VMOffsets {
    /// The offset of the `func_ptr` field.
    #[allow(clippy::erasing_op)]
    pub const fn vmcaller_checked_anyfunc_func_ptr(&self) -> u8 {
        0 * self.pointer_size
    }

    /// The offset of the `type_index` field.
    #[allow(clippy::identity_op)]
    pub const fn vmcaller_checked_anyfunc_type_index(&self) -> u8 {
        1 * self.pointer_size
    }

    /// The offset of the `vmctx` field.
    pub const fn vmcaller_checked_anyfunc_vmctx(&self) -> u8 {
        2 * self.pointer_size
    }

    /// Return the size of [`VMCallerCheckedAnyfunc`].
    ///
    /// [`VMCallerCheckedAnyfunc`]: crate::vmcontext::VMCallerCheckedAnyfunc
    pub const fn size_of_vmcaller_checked_anyfunc(&self) -> u8 {
        3 * self.pointer_size
    }
}

/// Offsets for [`VMFuncRef`].
///
/// [`VMFuncRef`]: crate::func_data_registry::VMFuncRef
impl VMOffsets {
    /// The offset to the pointer to the anyfunc inside the ref.
    #[allow(clippy::erasing_op)]
    pub const fn vm_funcref_anyfunc_ptr(&self) -> u8 {
        0 * self.pointer_size
    }

    /// Return the size of [`VMFuncRef`].
    ///
    /// [`VMFuncRef`]: crate::func_data_registry::VMFuncRef
    pub const fn size_of_vm_funcref(&self) -> u8 {
        self.pointer_size
    }
}

/// Offset base by num_items items of size item_size, panicking on overflow
fn offset_by(base: u32, num_items: u32, prev_item_size: u32, next_item_align: usize) -> u32 {
    align(
        base.checked_add(num_items.checked_mul(prev_item_size).unwrap())
            .unwrap(),
        next_item_align as u32,
    )
}

/// Offsets for [`VMContext`].
///
/// [`VMContext`]: crate::vmcontext::VMContext
impl VMOffsets {
    /// The offset of the `signature_ids` array.
    pub fn vmctx_signature_ids_begin(&self) -> u32 {
        0
    }

    /// The offset of the `tables` array.
    #[allow(clippy::erasing_op)]
    pub fn vmctx_imported_functions_begin(&self) -> u32 {
        offset_by(
            self.vmctx_signature_ids_begin(),
            self.num_signature_ids,
            u32::from(self.size_of_vmshared_signature_index()),
            align_of::<crate::VMFunctionImport>(),
        )
    }

    /// The offset of the `tables` array.
    #[allow(clippy::identity_op)]
    pub fn vmctx_imported_tables_begin(&self) -> u32 {
        offset_by(
            self.vmctx_imported_functions_begin(),
            self.num_imported_functions,
            u32::from(self.size_of_vmfunction_import()),
            align_of::<crate::VMTableImport>(),
        )
    }

    /// The offset of the `memories` array.
    pub fn vmctx_imported_memories_begin(&self) -> u32 {
        offset_by(
            self.vmctx_imported_tables_begin(),
            self.num_imported_tables,
            u32::from(self.size_of_vmtable_import()),
            align_of::<crate::VMMemoryImport>(),
        )
    }

    /// The offset of the `globals` array.
    pub fn vmctx_imported_globals_begin(&self) -> u32 {
        offset_by(
            self.vmctx_imported_memories_begin(),
            self.num_imported_memories,
            u32::from(self.size_of_vmmemory_import()),
            align_of::<crate::VMGlobalImport>(),
        )
    }

    /// The offset of the `tables` array.
    pub fn vmctx_tables_begin(&self) -> u32 {
        offset_by(
            self.vmctx_imported_globals_begin(),
            self.num_imported_globals,
            u32::from(self.size_of_vmglobal_import()),
            align_of::<crate::VMTableImport>(),
        )
    }

    /// The offset of the `memories` array.
    pub fn vmctx_memories_begin(&self) -> u32 {
        offset_by(
            self.vmctx_tables_begin(),
            self.num_local_tables,
            u32::from(self.size_of_vmtable_definition()),
            align_of::<crate::VMMemoryDefinition>(),
        )
    }

    /// The offset of the `globals` array.
    pub fn vmctx_globals_begin(&self) -> u32 {
        offset_by(
            self.vmctx_memories_begin(),
            self.num_local_memories,
            u32::from(self.size_of_vmmemory_definition()),
            align_of::<crate::VMGlobalDefinition>(),
        )
    }

    /// The offset of the builtin functions array.
    pub fn vmctx_builtin_functions_begin(&self) -> u32 {
        offset_by(
            self.vmctx_globals_begin(),
            self.num_local_globals,
            u32::from(self.size_of_vmglobal_local()),
            align_of::<crate::vmcontext::VMBuiltinFunctionsArray>(),
        )
    }

    /// The offset of the trap handler.
    pub fn vmctx_trap_handler_begin(&self) -> u32 {
        offset_by(
            self.vmctx_builtin_functions_begin(),
            VMBuiltinFunctionIndex::builtin_functions_total_number(),
            u32::from(self.pointer_size),
            align_of::<fn()>(),
        )
    }

    /// The offset of the gas limiter pointer.
    pub fn vmctx_gas_limiter_pointer(&self) -> u32 {
        offset_by(
            self.vmctx_trap_handler_begin(),
            if self.has_trap_handlers { 1 } else { 0 },
            u32::from(self.pointer_size),
            align_of::<*mut wasmer_types::FastGasCounter>(),
        )
    }

    /// The offset of the current stack limit.
    pub fn vmctx_stack_limit_begin(&self) -> u32 {
        offset_by(
            self.vmctx_gas_limiter_pointer(),
            1,
            u32::from(self.pointer_size),
            align_of::<u32>(),
        )
    }

    /// The offset of the initial stack limit.
    pub fn vmctx_stack_limit_initial_begin(&self) -> u32 {
        self.vmctx_stack_limit_begin().checked_add(4).unwrap()
    }

    /// Return the size of the [`VMContext`] allocation.
    ///
    /// [`VMContext`]: crate::vmcontext::VMContext
    pub fn size_of_vmctx(&self) -> u32 {
        self.vmctx_stack_limit_initial_begin()
            .checked_add(4)
            .unwrap()
    }

    /// Return the offset to [`VMSharedSignatureIndex`] index `index`.
    ///
    /// [`VMSharedSignatureIndex`]: crate::vmcontext::VMSharedSignatureIndex
    pub fn vmctx_vmshared_signature_id(&self, index: SignatureIndex) -> u32 {
        assert_lt!(index.as_u32(), self.num_signature_ids);
        self.vmctx_signature_ids_begin()
            .checked_add(
                index
                    .as_u32()
                    .checked_mul(u32::from(self.size_of_vmshared_signature_index()))
                    .unwrap(),
            )
            .unwrap()
    }

    /// Return the offset to [`VMFunctionImport`] index `index`.
    ///
    /// [`VMFunctionImport`]: crate::vmcontext::VMFunctionImport
    pub fn vmctx_vmfunction_import(&self, index: FunctionIndex) -> u32 {
        assert_lt!(index.as_u32(), self.num_imported_functions);
        self.vmctx_imported_functions_begin()
            .checked_add(
                index
                    .as_u32()
                    .checked_mul(u32::from(self.size_of_vmfunction_import()))
                    .unwrap(),
            )
            .unwrap()
    }

    /// Return the offset to [`VMTableImport`] index `index`.
    ///
    /// [`VMTableImport`]: crate::vmcontext::VMTableImport
    pub fn vmctx_vmtable_import(&self, index: TableIndex) -> u32 {
        assert_lt!(index.as_u32(), self.num_imported_tables);
        self.vmctx_imported_tables_begin()
            .checked_add(
                index
                    .as_u32()
                    .checked_mul(u32::from(self.size_of_vmtable_import()))
                    .unwrap(),
            )
            .unwrap()
    }

    /// Return the offset to [`VMMemoryImport`] index `index`.
    ///
    /// [`VMMemoryImport`]: crate::vmcontext::VMMemoryImport
    pub fn vmctx_vmmemory_import(&self, index: MemoryIndex) -> u32 {
        assert_lt!(index.as_u32(), self.num_imported_memories);
        self.vmctx_imported_memories_begin()
            .checked_add(
                index
                    .as_u32()
                    .checked_mul(u32::from(self.size_of_vmmemory_import()))
                    .unwrap(),
            )
            .unwrap()
    }

    /// Return the offset to [`VMGlobalImport`] index `index`.
    ///
    /// [`VMGlobalImport`]: crate::vmcontext::VMGlobalImport
    pub fn vmctx_vmglobal_import(&self, index: GlobalIndex) -> u32 {
        assert_lt!(index.as_u32(), self.num_imported_globals);
        self.vmctx_imported_globals_begin()
            .checked_add(
                index
                    .as_u32()
                    .checked_mul(u32::from(self.size_of_vmglobal_import()))
                    .unwrap(),
            )
            .unwrap()
    }

    /// Return the offset to [`VMTableDefinition`] index `index`.
    ///
    /// [`VMTableDefinition`]: crate::vmcontext::VMTableDefinition
    pub fn vmctx_vmtable_definition(&self, index: LocalTableIndex) -> u32 {
        assert_lt!(index.as_u32(), self.num_local_tables);
        self.vmctx_tables_begin()
            .checked_add(
                index
                    .as_u32()
                    .checked_mul(u32::from(self.size_of_vmtable_definition()))
                    .unwrap(),
            )
            .unwrap()
    }

    /// Return the offset to [`VMMemoryDefinition`] index `index`.
    ///
    /// [`VMMemoryDefinition`]: crate::vmcontext::VMMemoryDefinition
    pub fn vmctx_vmmemory_definition(&self, index: LocalMemoryIndex) -> u32 {
        assert_lt!(index.as_u32(), self.num_local_memories);
        self.vmctx_memories_begin()
            .checked_add(
                index
                    .as_u32()
                    .checked_mul(u32::from(self.size_of_vmmemory_definition()))
                    .unwrap(),
            )
            .unwrap()
    }

    /// Return the offset to the [`VMGlobalDefinition`] index `index`.
    ///
    /// [`VMGlobalDefinition`]: crate::vmcontext::VMGlobalDefinition
    pub fn vmctx_vmglobal_definition(&self, index: LocalGlobalIndex) -> u32 {
        assert_lt!(index.as_u32(), self.num_local_globals);
        self.vmctx_globals_begin()
            .checked_add(
                index
                    .as_u32()
                    .checked_mul(u32::from(self.size_of_vmglobal_local()))
                    .unwrap(),
            )
            .unwrap()
    }

    /// Return the offset to the `body` field in `*const VMFunctionBody` index `index`.
    pub fn vmctx_vmfunction_import_body(&self, index: FunctionIndex) -> u32 {
        self.vmctx_vmfunction_import(index)
            .checked_add(u32::from(self.vmfunction_import_body()))
            .unwrap()
    }

    /// Return the offset to the `vmctx` field in `*const VMFunctionBody` index `index`.
    pub fn vmctx_vmfunction_import_vmctx(&self, index: FunctionIndex) -> u32 {
        self.vmctx_vmfunction_import(index)
            .checked_add(u32::from(self.vmfunction_import_vmctx()))
            .unwrap()
    }

    /// Return the offset to the `definition` field in [`VMTableImport`] index `index`.
    ///
    /// [`VMTableImport`]: crate::vmcontext::VMTableImport
    pub fn vmctx_vmtable_import_definition(&self, index: TableIndex) -> u32 {
        self.vmctx_vmtable_import(index)
            .checked_add(u32::from(self.vmtable_import_definition()))
            .unwrap()
    }

    /// Return the offset to the `base` field in [`VMTableDefinition`] index `index`.
    ///
    /// [`VMTableDefinition`]: crate::vmcontext::VMTableDefinition
    pub fn vmctx_vmtable_definition_base(&self, index: LocalTableIndex) -> u32 {
        self.vmctx_vmtable_definition(index)
            .checked_add(u32::from(self.vmtable_definition_base()))
            .unwrap()
    }

    /// Return the offset to the `current_elements` field in [`VMTableDefinition`] index `index`.
    ///
    /// [`VMTableDefinition`]: crate::vmcontext::VMTableDefinition
    pub fn vmctx_vmtable_definition_current_elements(&self, index: LocalTableIndex) -> u32 {
        self.vmctx_vmtable_definition(index)
            .checked_add(u32::from(self.vmtable_definition_current_elements()))
            .unwrap()
    }

    /// Return the offset to the `from` field in [`VMMemoryImport`] index `index`.
    ///
    /// [`VMMemoryImport`]: crate::vmcontext::VMMemoryImport
    pub fn vmctx_vmmemory_import_definition(&self, index: MemoryIndex) -> u32 {
        self.vmctx_vmmemory_import(index)
            .checked_add(u32::from(self.vmmemory_import_definition()))
            .unwrap()
    }

    /// Return the offset to the `vmctx` field in [`VMMemoryImport`] index `index`.
    ///
    /// [`VMMemoryImport`]: crate::vmcontext::VMMemoryImport
    pub fn vmctx_vmmemory_import_from(&self, index: MemoryIndex) -> u32 {
        self.vmctx_vmmemory_import(index)
            .checked_add(u32::from(self.vmmemory_import_from()))
            .unwrap()
    }

    /// Return the offset to the `base` field in [`VMMemoryDefinition`] index `index`.
    ///
    /// [`VMMemoryDefinition`]: crate::vmcontext::VMMemoryDefinition
    pub fn vmctx_vmmemory_definition_base(&self, index: LocalMemoryIndex) -> u32 {
        self.vmctx_vmmemory_definition(index)
            .checked_add(u32::from(self.vmmemory_definition_base()))
            .unwrap()
    }

    /// Return the offset to the `current_length` field in [`VMMemoryDefinition`] index `index`.
    ///
    /// [`VMMemoryDefinition`]: crate::vmcontext::VMMemoryDefinition
    pub fn vmctx_vmmemory_definition_current_length(&self, index: LocalMemoryIndex) -> u32 {
        self.vmctx_vmmemory_definition(index)
            .checked_add(u32::from(self.vmmemory_definition_current_length()))
            .unwrap()
    }

    /// Return the offset to the `from` field in [`VMGlobalImport`] index `index`.
    ///
    /// [`VMGlobalImport`]: crate::vmcontext::VMGlobalImport
    pub fn vmctx_vmglobal_import_definition(&self, index: GlobalIndex) -> u32 {
        self.vmctx_vmglobal_import(index)
            .checked_add(u32::from(self.vmglobal_import_definition()))
            .unwrap()
    }

    /// Return the offset to builtin function in `VMBuiltinFunctionsArray` index `index`.
    pub fn vmctx_builtin_function(&self, index: VMBuiltinFunctionIndex) -> u32 {
        self.vmctx_builtin_functions_begin()
            .checked_add(
                index
                    .index()
                    .checked_mul(u32::from(self.pointer_size))
                    .unwrap(),
            )
            .unwrap()
    }

    /// Return the offset to the trap handler.
    pub fn vmctx_trap_handler(&self) -> u32 {
        // Ensure that we never ask for trap handler offset if it's not enabled.
        assert!(self.has_trap_handlers);
        self.vmctx_trap_handler_begin()
    }
}

/// Target specific type for shared signature index.
#[derive(Debug, Copy, Clone)]
pub struct TargetSharedSignatureIndex(u32);

impl TargetSharedSignatureIndex {
    /// Constructs `TargetSharedSignatureIndex`.
    pub const fn new(value: u32) -> Self {
        Self(value)
    }

    /// Returns index value.
    pub const fn index(self) -> u32 {
        self.0
    }
}

#[cfg(test)]
mod tests {
    use crate::vmoffsets::align;

    #[test]
    fn alignment() {
        fn is_aligned(x: u32) -> bool {
            x % 16 == 0
        }
        assert!(is_aligned(align(0, 16)));
        assert!(is_aligned(align(32, 16)));
        assert!(is_aligned(align(33, 16)));
        assert!(is_aligned(align(31, 16)));
    }
}

'''
'''--- scripts/publish.py ---
#! /usr/bin/env python3

# This is a script for publishing the wasmer crates to crates.io.
# It should be run in the root of wasmer like `python3 scripts/publish.py --no-dry-run`.
# By default the script executes a test run and does not publish the crates to crates.io.

# install dependencies:
# pip3 install toposort

import argparse
import os
import re
import subprocess
import time

from typing import Optional
try:
    from toposort import toposort_flatten
except ImportError:
    print("Please install toposort, `pip3 install toposort`")

# TODO: find this automatically
target_version = "2.1.0"

# TODO: generate this by parsing toml files
dep_graph = {
    "wasmer-types": set([]),
    "wasmer-derive": set([]),
    "wasmer-vm": set(["wasmer-types"]),
    "wasmer-compiler": set(["wasmer-vm", "wasmer-types"]),
    "wasmer-object": set(["wasmer-types", "wasmer-compiler"]),
    "wasmer-engine": set(["wasmer-types", "wasmer-vm", "wasmer-compiler"]),
    "wasmer-compiler-singlepass": set(["wasmer-types", "wasmer-vm", "wasmer-compiler"]),
    "wasmer-compiler-cranelift": set(["wasmer-types", "wasmer-vm", "wasmer-compiler"]),
    "wasmer-compiler-llvm": set(["wasmer-types", "wasmer-vm", "wasmer-compiler"]),
    "wasmer-engine-universal": set(["wasmer-types", "wasmer-vm", "wasmer-compiler", "wasmer-engine"]),
    "wasmer-engine-dylib": set(["wasmer-types", "wasmer-vm", "wasmer-compiler", "wasmer-engine",
                                 "wasmer-object"]),
    "wasmer-engine-staticlib": set(["wasmer-types", "wasmer-vm", "wasmer-compiler", "wasmer-engine",
                                      "wasmer-object"]),
    "wasmer": set(["wasmer-vm", "wasmer-compiler-singlepass", "wasmer-compiler-cranelift",
                   "wasmer-compiler-llvm", "wasmer-compiler", "wasmer-engine", "wasmer-engine-universal",
                   "wasmer-engine-dylib", "wasmer-engine-staticlib", "wasmer-types", "wasmer-derive"]),
    "wasmer-cache": set(["wasmer"]),
}

# where each crate is located in the `lib` directory
# TODO: this could also be generated from the toml files
location = {
    "wasmer-types": "types",
    "wasmer-derive": "derive",
    "wasmer-vm": "vm",
    "wasmer-compiler": "compiler",
    "wasmer-object": "object",
    "wasmer-engine": "engine",
    "wasmer-compiler-singlepass": "compiler-singlepass",
    "wasmer-compiler-cranelift": "compiler-cranelift",
    "wasmer-compiler-llvm": "compiler-llvm",
    "wasmer-engine": "engine",
    "wasmer-engine-universal": "engine-universal",
    "wasmer-engine-dylib": "engine-dylib",
    "wasmer-engine-staticlib": "engine-staticlib",
    "wasmer-cache": "cache",
    "wasmer": "api",
}

no_dry_run = False

def get_latest_version_for_crate(crate_name: str) -> Optional[str]:
    output = subprocess.run(["cargo", "search", crate_name], capture_output=True)
    rexp_src = '^{} = "([^"]+)"'.format(crate_name)
    prog = re.compile(rexp_src)
    haystack = output.stdout.decode("utf-8")
    for line in haystack.splitlines():
        result = prog.match(line)
        if result:
            return result.group(1)

def is_crate_already_published(crate_name: str) -> bool:
    found_string = get_latest_version_for_crate(crate_name)
    if found_string is None:
        return False

    return target_version == found_string

def publish_crate(crate: str):
    starting_dir = os.getcwd()
    os.chdir("lib/{}".format(location[crate]))

    global no_dry_run
    if no_dry_run:
        output = subprocess.run(["cargo", "publish"])
    else:
        print("In dry-run: not publishing crate `{}`".format(crate))

    os.chdir(starting_dir)

def main():
    os.environ['WASMER_PUBLISH_SCRIPT_IS_RUNNING'] = '1'
    parser = argparse.ArgumentParser(description='Publish the Wasmer crates to crates.io')
    parser.add_argument('--no-dry-run', default=False, action='store_true',
                        help='Run the script without actually publishing anything to crates.io')
    args = vars(parser.parse_args())

    global no_dry_run
    no_dry_run = args['no_dry_run']

    # get the order to publish the crates in
    order = list(toposort_flatten(dep_graph, sort=True))

    for crate in order:
        print("Publishing `{}`...".format(crate))
        if not is_crate_already_published(crate):
            publish_crate(crate)
        else:
            print("`{}` was already published!".format(crate))
            continue
        # sleep for 16 seconds between crates to ensure the crates.io index has time to update
        # this can be optimized with knowledge of our dep graph via toposort; we can even publish
        # crates in parallel; however this is out of scope for the first version of this script
        if no_dry_run:
            print("Sleeping for 16 seconds to allow the `crates.io` index to update...")
            time.sleep(16)
        else:
            print("In dry-run: not sleeping for crates.io to update.")

if __name__ == "__main__":
    main()

'''
'''--- scripts/update-version.sh ---
#! /bin/sh

# How to install `fd`: https://github.com/sharkdp/fd#installation
: "${FD:=fd}"

# A script to update the version of all the crates at the same time
PREVIOUS_VERSION='2.0.0'
NEXT_VERSION='2.1.0'

# quick hack
${FD} Cargo.toml --exec sed -i '{}' -e "s/version = \"$PREVIOUS_VERSION\"/version = \"$NEXT_VERSION\"/"
echo "manually check changes to Cargo.toml"

${FD} wasmer.iss --exec sed -i '{}' -e "s/AppVersion=$PREVIOUS_VERSION/AppVersion=$NEXT_VERSION/"
echo "manually check changes to wasmer.iss"

# Order to upload packages in
## wasmer-types
## win-exception-handler
## compiler
## compiler-cranelift
## compiler-llvm
## compiler-singlepass
## emscripten
## wasi
## wasmer (api)

'''
'''--- scripts/windows-installer/wax.cmd ---
@echo off
wapm.exe execute %*

'''
'''--- tests/compilers/compilation.rs ---
use wasmer::*;
use wasmer_engine::{Engine, Executable};
use wasmer_engine_universal::Universal;
use wasmer_vm::Artifact;

fn slow_to_compile_contract(n_fns: usize, n_locals: usize) -> Vec<u8> {
    let fns = format!("(func (local {}))\n", "i32 ".repeat(n_locals)).repeat(n_fns);
    let wat = format!(r#"(module {} (func (export "main")))"#, fns);
    wat2wasm(wat.as_bytes()).unwrap().to_vec()
}

fn compile_uncached<'a>(
    store: &'a Store,
    engine: &'a dyn Engine,
    code: &'a [u8],
    time: bool,
) -> Result<Box<dyn wasmer_engine::Executable>, CompileError> {
    use std::time::Instant;
    let now = Instant::now();
    engine.validate(code)?;
    let validate = now.elapsed().as_millis();
    let now = Instant::now();
    let res = engine.compile(code, store.tunables());
    let compile = now.elapsed().as_millis();
    if time {
        println!("validate {}ms compile {}ms", validate, compile);
    }
    res
}

#[test]
#[ignore]
fn compilation_test() {
    let compiler = Singlepass::default();
    let engine = Universal::new(compiler).engine();
    let store = Store::new(&engine);
    for factor in 1..1000 {
        let code = slow_to_compile_contract(3, 25 * factor);
        match compile_uncached(&store, &engine, &code, false) {
            Ok(art) => {
                let serialized = art.serialize().unwrap();
                println!(
                    "{}: artifact is compiled, size is {}",
                    factor,
                    serialized.len()
                );
            }
            Err(err) => {
                println!("err is {:?}", err);
            }
        }
    }
}

/*
Code to create perf map.

fn write_perf_profiler_map(functions: &Vec<NamedFunction>) -> Result<(), Box<dyn std::error::Error>>{
    let pid = process::id();
    let filename = format!("/tmp/perf-{}.map", pid);
    let mut file = File::create(filename).expect("Unable to create file");
    for f in functions {
        file.write_fmt(format_args!("{:x} {:x} {}\n", f.address, f.size, f.name))?;
    }
    Ok(())
}
*/

#[test]
fn profiling() {
    let wat = r#"
       (import "env" "impf" (func))
       (func $f0)
       (func (export "f1"))
       (func (export "f2"))
       (func (export "f3"))
    "#;
    let wasm = wat2wasm(wat.as_bytes()).unwrap();
    let compiler = Singlepass::default();
    let engine = Universal::new(compiler).engine();
    let store = Store::new(&engine);
    match compile_uncached(&store, &engine, &wasm, false) {
        Ok(art) => unsafe {
            let serialized = art.serialize().unwrap();
            let executable =
                wasmer_engine_universal::UniversalExecutableRef::deserialize(&serialized).unwrap();
            let artifact = engine.load_universal_executable_ref(&executable).unwrap();
            let info = artifact
                .functions()
                .iter()
                .filter_map(|(idx, _)| {
                    let extent = artifact.function_extent(idx)?;
                    let idx = artifact.import_counts().function_index(idx);
                    let name = executable.function_name(idx)?;
                    Some((name, extent))
                })
                .collect::<Vec<_>>();
            assert_eq!(4, info.len());
            assert_eq!("f0", info[0].0);
            assert_eq!("f1", info[1].0);
            assert_eq!("f2", info[2].0);
            assert_eq!("f3", info[3].0);
        },
        Err(_) => {
            assert!(false)
        }
    }
}

'''
'''--- tests/compilers/config.rs ---
use wasmer::{CompilerConfig, Engine as WasmerEngine, Features, Store};

#[derive(Clone, Debug, PartialEq)]
pub enum Compiler {
    LLVM,
    Cranelift,
    Singlepass,
}

#[derive(Clone, Debug, PartialEq)]
pub enum Engine {
    Dylib,
    Universal,
}

#[derive(Clone)]
pub struct Config {
    pub compiler: Compiler,
    pub engine: Engine,
    pub features: Option<Features>,
    pub canonicalize_nans: bool,
}

impl Config {
    pub fn new(engine: Engine, compiler: Compiler) -> Self {
        Self {
            compiler,
            engine,
            features: None,
            canonicalize_nans: false,
        }
    }

    pub fn set_features(&mut self, features: Features) {
        self.features = Some(features);
    }

    pub fn set_nan_canonicalization(&mut self, canonicalize_nans: bool) {
        self.canonicalize_nans = canonicalize_nans;
    }

    pub fn store(&self) -> Store {
        let compiler_config = self.compiler_config(self.canonicalize_nans);
        let engine = self.engine(compiler_config);
        Store::new(&*engine)
    }

    pub fn headless_store(&self) -> Store {
        let engine = self.engine_headless();
        Store::new(&*engine)
    }

    pub fn engine(&self, compiler_config: Box<dyn CompilerConfig>) -> Box<dyn WasmerEngine> {
        #[cfg(not(feature = "engine"))]
        compile_error!("Plese enable at least one engine via the features");
        match &self.engine {
            #[cfg(feature = "dylib")]
            Engine::Dylib => {
                let mut engine = wasmer_engine_dylib::Dylib::new(compiler_config);
                if let Some(ref features) = self.features {
                    engine = engine.features(features.clone())
                }
                Box::new(engine.engine())
            }
            #[cfg(feature = "universal")]
            Engine::Universal => {
                let mut engine = wasmer_engine_universal::Universal::new(compiler_config);
                if let Some(ref features) = self.features {
                    engine = engine.features(features.clone())
                }
                Box::new(engine.engine())
            }
            #[allow(unreachable_patterns)]
            engine => panic!(
                "The {:?} Engine is not enabled. Please enable it using the features",
                engine
            ),
        }
    }

    pub fn engine_headless(&self) -> Box<dyn WasmerEngine> {
        match &self.engine {
            #[cfg(feature = "dylib")]
            Engine::Dylib => Box::new(wasmer_engine_dylib::Dylib::headless().engine()),
            #[cfg(feature = "universal")]
            Engine::Universal => Box::new(wasmer_engine_universal::Universal::headless().engine()),
            #[allow(unreachable_patterns)]
            engine => panic!(
                "The {:?} Engine is not enabled. Please enable it using the features",
                engine
            ),
        }
    }

    pub fn compiler_config(&self, canonicalize_nans: bool) -> Box<dyn CompilerConfig> {
        match &self.compiler {
            #[cfg(feature = "cranelift")]
            Compiler::Cranelift => {
                let mut compiler = wasmer_compiler_cranelift::Cranelift::new();
                compiler.canonicalize_nans(canonicalize_nans);
                compiler.enable_verifier();
                Box::new(compiler)
            }
            #[cfg(feature = "llvm")]
            Compiler::LLVM => {
                let mut compiler = wasmer_compiler_llvm::LLVM::new();
                compiler.canonicalize_nans(canonicalize_nans);
                compiler.enable_verifier();
                Box::new(compiler)
            }
            #[cfg(feature = "singlepass")]
            Compiler::Singlepass => {
                let mut compiler = wasmer_compiler_singlepass::Singlepass::new();
                compiler.canonicalize_nans(canonicalize_nans);
                compiler.enable_verifier();
                Box::new(compiler)
            }
            #[allow(unreachable_patterns)]
            compiler => {
                panic!(
                    "The {:?} Compiler is not enabled. Enable it via the features",
                    compiler
                )
            }
        }
    }
}

'''
'''--- tests/compilers/deterministic.rs ---
use anyhow::Result;
use wasmer::{wat2wasm, BaseTunables, Engine};
use wasmer_compiler_singlepass::Singlepass;
use wasmer_engine_universal::Universal;

fn compile_and_compare(wasm: &[u8]) -> Result<()> {
    let compiler = Singlepass::default();
    let engine = Universal::new(compiler).engine();
    let tunables = BaseTunables::for_target(engine.target());

    // compile for first time
    let executable = engine.compile(wasm, &tunables).unwrap();
    let serialized1 = executable.serialize().unwrap();

    // compile for second time
    let executable = engine.compile(wasm, &tunables).unwrap();
    let serialized2 = executable.serialize().unwrap();

    assert_eq!(serialized1, serialized2);

    Ok(())
}

#[test]
fn deterministic_empty() -> Result<()> {
    let wasm_bytes = wat2wasm(
        br#"
    (module)
    "#,
    )?;

    compile_and_compare(&wasm_bytes)
}

#[test]
fn deterministic_table() -> Result<()> {
    let wasm_bytes = wat2wasm(
        br#"
(module
  (table 2 funcref)
  (func $f1)
  (func $f2)
  (elem (i32.const 0) $f1 $f2))
"#,
    )?;

    compile_and_compare(&wasm_bytes)
}

'''
'''--- tests/compilers/fast_gas_metering.rs ---
use std::ptr;
use std::sync::atomic::AtomicUsize;
use std::sync::atomic::Ordering::SeqCst;
use wasmer::*;
use wasmer_compiler_singlepass::Singlepass;
use wasmer_engine_universal::Universal;
use wasmer_types::{FastGasCounter, InstanceConfig};

fn get_module_with_start(store: &Store) -> Module {
    let wat = r#"
        (import "host" "func" (func))
        (import "host" "gas" (func (param i32)))
        (memory $mem 1)
        (export "memory" (memory $mem))
        (export "bar" (func $bar))
        (func $foo
            call 0
            i32.const 42
            call 1
            call 0
            i32.const 100
            call 1
            call 0
        )
        (func $bar
            call 0
            i32.const 100
            call 1
        )
        (start $foo)
    "#;

    Module::new(&store, &wat).unwrap()
}

fn get_module(store: &Store) -> Module {
    let wat = r#"
        (import "host" "func" (func))
        (import "host" "has" (func (param i32)))
        (import "host" "gas" (func (param i32)))
        (memory $mem 1)
        (export "memory" (memory $mem))
        (func (export "foo")
            call 0
            i32.const 442
            call 1
            i32.const 42
            call 2
            call 0
            i32.const 100
            call 2
            call 0
        )
        (func (export "bar")
            call 0
            i32.const 100
            call 2
        )
        (func (export "zoo")
            loop
                i32.const 100
                call 2
                br 0
            end
        )
    "#;

    Module::new(&store, &wat).unwrap()
}

fn get_module_tricky_arg(store: &Store) -> Module {
    let wat = r#"
        (import "host" "func" (func))
        (import "host" "gas" (func (param i32)))
        (memory $mem 1)
        (export "memory" (memory $mem))
        (func $get_gas (param i32) (result i32)
         i32.const 1
         get_local 0
         i32.add)
        (func (export "foo")
            i32.const 1000000000
            call $get_gas
            call 1
        )
        (func (export "zoo")
            i32.const -2
            call 1
        )
    "#;

    Module::new(&store, &wat).unwrap()
}

fn get_store() -> Store {
    let compiler = Singlepass::default();
    let store = Store::new(&Universal::new(compiler).engine());
    store
}

#[test]
fn test_gas_intrinsic_in_start() {
    let store = get_store();
    let mut gas_counter = FastGasCounter::new(300, 3);
    let module = get_module_with_start(&store);
    static HITS: AtomicUsize = AtomicUsize::new(0);
    let result = Instance::new_with_config(
        &module,
        unsafe { InstanceConfig::default().with_counter(ptr::addr_of_mut!(gas_counter)) },
        &imports! {
            "host" => {
                "func" => Function::new(&store, FunctionType::new(vec![], vec![]), |_values| {
                    HITS.fetch_add(1, SeqCst);
                    Ok(vec![])
                }),
                "gas" => Function::new(&store, FunctionType::new(vec![ValType::I32], vec![]), |_| {
                    // It shall be never called, as call is intrinsified.
                    assert!(false);
                    Ok(vec![])
                }),
            },
        },
    );
    assert!(result.is_err());
    match result {
        Err(InstantiationError::Start(runtime_error)) => {
            assert_eq!(runtime_error.message(), "gas limit exceeded")
        }
        _ => assert!(false),
    }
    // Ensure "func" was called twice.
    assert_eq!(HITS.swap(0, SeqCst), 2);
    // Ensure gas was partially spent.
    assert_eq!(gas_counter.burnt(), 426);
    assert_eq!(gas_counter.gas_limit, 300);
    assert_eq!(gas_counter.opcode_cost, 3);
}

#[test]
fn test_gas_intrinsic_regular() {
    let store = get_store();
    let mut gas_counter = FastGasCounter::new(500, 3);
    let module = get_module(&store);
    static HITS: AtomicUsize = AtomicUsize::new(0);
    let instance = Instance::new_with_config(
        &module,
        unsafe { InstanceConfig::default().with_counter(ptr::addr_of_mut!(gas_counter)) },
        &imports! {
            "host" => {
                "func" => Function::new(&store, FunctionType::new(vec![], vec![]), |_values| {
                    HITS.fetch_add(1, SeqCst);
                    Ok(vec![])
                }),
                "has" => Function::new(&store, FunctionType::new(vec![ValType::I32], vec![]), |_| {
                    HITS.fetch_add(1, SeqCst);
                    Ok(vec![])
                }),
                "gas" => Function::new(&store, FunctionType::new(vec![ValType::I32], vec![]), |_| {
                    // It shall be never called, as call is intrinsified.
                    assert!(false);
                    Ok(vec![])
                }),
            },
        },
    );
    assert!(instance.is_ok());
    let instance = instance.unwrap();
    let foo_func = instance
        .lookup_function("foo")
        .expect("expected function foo");
    let bar_func = instance
        .lookup_function("bar")
        .expect("expected function bar");
    let zoo_func = instance
        .lookup_function("zoo")
        .expect("expected function zoo");
    // Ensure "func" was not called.
    assert_eq!(HITS.load(SeqCst), 0);
    let e = bar_func.call(&[]);
    assert!(e.is_ok());
    // Ensure "func" was called.
    assert_eq!(HITS.load(SeqCst), 1);
    assert_eq!(gas_counter.burnt(), 300);
    let _e = foo_func.call(&[]).err().expect("error calling function");
    // Ensure "func" and "has" was called again.
    assert_eq!(HITS.load(SeqCst), 4);
    assert_eq!(gas_counter.burnt(), 726);
    // Finally try to exhaust rather large limit.
    gas_counter.gas_limit = 10_000_000_000_000_000;
    gas_counter.opcode_cost = 100_000_000;
    let _e = zoo_func.call(&[]).err().expect("error calling function");
    assert_eq!(gas_counter.burnt(), 10_000_000_000_000_726);
}

#[test]
fn test_gas_intrinsic_default() {
    let store = get_store();
    let module = get_module(&store);
    static HITS: AtomicUsize = AtomicUsize::new(0);
    let instance = Instance::new(
        &module,
        &imports! {
            "host" => {
                "func" => Function::new(&store, FunctionType::new(vec![], vec![]), |_values| {
                    HITS.fetch_add(1, SeqCst);
                    Ok(vec![])
                }),
                "has" => Function::new(&store, FunctionType::new(vec![ValType::I32], vec![]), |_| {
                    HITS.fetch_add(1, SeqCst);
                    Ok(vec![])
                }),
                "gas" => Function::new(&store, FunctionType::new(vec![ValType::I32], vec![]), |_| {
                    // It shall be never called, as call is intrinsified.
                    assert!(false);
                    Ok(vec![])
                }),
            },
        },
    );
    assert!(instance.is_ok());
    let instance = instance.unwrap();
    let foo_func = instance
        .lookup_function("foo")
        .expect("expected function foo");
    let bar_func = instance
        .lookup_function("bar")
        .expect("expected function bar");
    // Ensure "func" was called.
    assert_eq!(HITS.load(SeqCst), 0);
    let e = bar_func.call(&[]);
    assert!(e.is_ok());
    // Ensure "func" was called.
    assert_eq!(HITS.load(SeqCst), 1);
    let _e = foo_func.call(&[]);
    // Ensure "func" and "has" was called.
    assert_eq!(HITS.load(SeqCst), 5);
}

#[test]
fn test_gas_intrinsic_tricky() {
    let store = get_store();
    let module = get_module_tricky_arg(&store);
    static BURNT_GAS: AtomicUsize = AtomicUsize::new(0);
    static HITS: AtomicUsize = AtomicUsize::new(0);
    let instance = Instance::new(
        &module,
        &imports! {
            "host" => {
                "func" => Function::new(&store, FunctionType::new(vec![], vec![]), |_values| {
                    HITS.fetch_add(1, SeqCst);
                    Ok(vec![])
                }),
                "gas" => Function::new(&store, FunctionType::new(vec![ValType::I32], vec![]), |arg| {
                    // It shall be called, as tricky call is not intrinsified.
                    HITS.fetch_add(1, SeqCst);
                    match arg[0] {
                        Value::I32(arg) => {
                            BURNT_GAS.fetch_add(arg as usize, SeqCst);
                        },
                        _ => {
                            assert!(false)
                        }
                    }
                    Ok(vec![])
                }),
            },
        },
    );
    assert!(instance.is_ok());
    let instance = instance.unwrap();
    let foo_func = instance
        .lookup_function("foo")
        .expect("expected function foo");

    let _e = foo_func.call(&[]);

    assert_eq!(BURNT_GAS.load(SeqCst), 1000000001);
    // Ensure "gas" was called.
    assert_eq!(HITS.load(SeqCst), 1);

    let zoo_func = instance
        .lookup_function("zoo")
        .expect("expected function zoo");

    let _e = zoo_func.call(&[]);
    // We decremented gas by two.
    assert_eq!(BURNT_GAS.load(SeqCst), 999999999);
    // Ensure "gas" was called.
    assert_eq!(HITS.load(SeqCst), 2);
}

'''
'''--- tests/compilers/imports.rs ---
//! Testing the imports with different provided functions.
//! This tests checks that the provided functions (both native and
//! dynamic ones) work properly.

use anyhow::Result;
use std::convert::Infallible;
use std::sync::atomic::AtomicBool;
use std::sync::{
    atomic::{AtomicUsize, Ordering::SeqCst},
    Arc,
};
use wasmer::*;

fn get_module(store: &Store) -> Result<Module> {
    let wat = r#"
        (import "host" "0" (func))
        (import "host" "1" (func (param i32) (result i32)))
        (import "host" "2" (func (param i32) (param i64)))
        (import "host" "3" (func (param i32 i64 i32 f32 f64)))
        (memory $mem 1)
        (export "memory" (memory $mem))

        (func $foo
            call 0
            i32.const 0
            call 1
            i32.const 1
            i32.add
            i64.const 3
            call 2

            i32.const 100
            i64.const 200
            i32.const 300
            f32.const 400
            f64.const 500
            call 3
        )
        (start $foo)
    "#;

    let module = Module::new(&store, &wat)?;
    Ok(module)
}

#[compiler_test(imports)]
#[serial_test::serial(dynamic_function)]
fn dynamic_function(config: crate::Config) -> Result<()> {
    let store = config.store();
    let module = get_module(&store)?;
    static HITS: AtomicUsize = AtomicUsize::new(0);
    Instance::new(
        &module,
        &imports! {
            "host" => {
                "0" => Function::new(&store, FunctionType::new(vec![], vec![]), |_values| {
                    assert_eq!(HITS.fetch_add(1, SeqCst), 0);
                    Ok(vec![])
                }),
                "1" => Function::new(&store, FunctionType::new(vec![ValType::I32], vec![ValType::I32]), |values| {
                    assert_eq!(values[0], Value::I32(0));
                    assert_eq!(HITS.fetch_add(1, SeqCst), 1);
                    Ok(vec![Value::I32(1)])
                }),
                "2" => Function::new(&store, FunctionType::new(vec![ValType::I32, ValType::I64], vec![]), |values| {
                    assert_eq!(values[0], Value::I32(2));
                    assert_eq!(values[1], Value::I64(3));
                    assert_eq!(HITS.fetch_add(1, SeqCst), 2);
                    Ok(vec![])
                }),
                "3" => Function::new(&store, FunctionType::new(vec![ValType::I32, ValType::I64, ValType::I32, ValType::F32, ValType::F64], vec![]), |values| {
                    assert_eq!(values[0], Value::I32(100));
                    assert_eq!(values[1], Value::I64(200));
                    assert_eq!(values[2], Value::I32(300));
                    assert_eq!(values[3], Value::F32(400.0));
                    assert_eq!(values[4], Value::F64(500.0));
                    assert_eq!(HITS.fetch_add(1, SeqCst), 3);
                    Ok(vec![])
                }),
            },
        },
    )?;
    assert_eq!(HITS.swap(0, SeqCst), 4);
    Ok(())
}

#[compiler_test(imports)]
fn dynamic_function_with_env(config: crate::Config) -> Result<()> {
    let store = config.store();
    let module = get_module(&store)?;

    #[derive(WasmerEnv, Clone)]
    struct Env {
        counter: Arc<AtomicUsize>,
    }

    impl std::ops::Deref for Env {
        type Target = Arc<AtomicUsize>;
        fn deref(&self) -> &Self::Target {
            &self.counter
        }
    }

    let env: Env = Env {
        counter: Arc::new(AtomicUsize::new(0)),
    };
    Instance::new(
        &module,
        &imports! {
            "host" => {
                "0" => Function::new_with_env(&store, FunctionType::new(vec![], vec![]), env.clone(), |env, _values| {
                    assert_eq!(env.fetch_add(1, SeqCst), 0);
                    Ok(vec![])
                }),
                "1" => Function::new_with_env(&store, FunctionType::new(vec![ValType::I32], vec![ValType::I32]), env.clone(), |env, values| {
                    assert_eq!(values[0], Value::I32(0));
                    assert_eq!(env.fetch_add(1, SeqCst), 1);
                    Ok(vec![Value::I32(1)])
                }),
                "2" => Function::new_with_env(&store, FunctionType::new(vec![ValType::I32, ValType::I64], vec![]), env.clone(), |env, values| {
                    assert_eq!(values[0], Value::I32(2));
                    assert_eq!(values[1], Value::I64(3));
                    assert_eq!(env.fetch_add(1, SeqCst), 2);
                    Ok(vec![])
                }),
                "3" => Function::new_with_env(&store, FunctionType::new(vec![ValType::I32, ValType::I64, ValType::I32, ValType::F32, ValType::F64], vec![]), env.clone(), |env, values| {
                    assert_eq!(values[0], Value::I32(100));
                    assert_eq!(values[1], Value::I64(200));
                    assert_eq!(values[2], Value::I32(300));
                    assert_eq!(values[3], Value::F32(400.0));
                    assert_eq!(values[4], Value::F64(500.0));
                    assert_eq!(env.fetch_add(1, SeqCst), 3);
                    Ok(vec![])
                }),
            },
        },
    )?;
    assert_eq!(env.load(SeqCst), 4);
    Ok(())
}

#[compiler_test(imports)]
#[serial_test::serial(static_function)]
fn static_function(config: crate::Config) -> Result<()> {
    let store = config.store();
    let module = get_module(&store)?;

    static HITS: AtomicUsize = AtomicUsize::new(0);
    Instance::new(
        &module,
        &imports! {
            "host" => {
                "0" => Function::new_native(&store, || {
                    assert_eq!(HITS.fetch_add(1, SeqCst), 0);
                }),
                "1" => Function::new_native(&store, |x: i32| -> i32 {
                    assert_eq!(x, 0);
                    assert_eq!(HITS.fetch_add(1, SeqCst), 1);
                    1
                }),
                "2" => Function::new_native(&store, |x: i32, y: i64| {
                    assert_eq!(x, 2);
                    assert_eq!(y, 3);
                    assert_eq!(HITS.fetch_add(1, SeqCst), 2);
                }),
                "3" => Function::new_native(&store, |a: i32, b: i64, c: i32, d: f32, e: f64| {
                    assert_eq!(a, 100);
                    assert_eq!(b, 200);
                    assert_eq!(c, 300);
                    assert_eq!(d, 400.0);
                    assert_eq!(e, 500.0);
                    assert_eq!(HITS.fetch_add(1, SeqCst), 3);
                }),
            },
        },
    )?;
    assert_eq!(HITS.swap(0, SeqCst), 4);
    Ok(())
}

#[compiler_test(imports)]
#[serial_test::serial(static_function_with_results)]
fn static_function_with_results(config: crate::Config) -> Result<()> {
    let store = config.store();
    let module = get_module(&store)?;

    static HITS: AtomicUsize = AtomicUsize::new(0);
    Instance::new(
        &module,
        &imports! {
            "host" => {
                "0" => Function::new_native(&store, || {
                    assert_eq!(HITS.fetch_add(1, SeqCst), 0);
                }),
                "1" => Function::new_native(&store, |x: i32| -> Result<i32, Infallible> {
                    assert_eq!(x, 0);
                    assert_eq!(HITS.fetch_add(1, SeqCst), 1);
                    Ok(1)
                }),
                "2" => Function::new_native(&store, |x: i32, y: i64| {
                    assert_eq!(x, 2);
                    assert_eq!(y, 3);
                    assert_eq!(HITS.fetch_add(1, SeqCst), 2);
                }),
                "3" => Function::new_native(&store, |a: i32, b: i64, c: i32, d: f32, e: f64| {
                    assert_eq!(a, 100);
                    assert_eq!(b, 200);
                    assert_eq!(c, 300);
                    assert_eq!(d, 400.0);
                    assert_eq!(e, 500.0);
                    assert_eq!(HITS.fetch_add(1, SeqCst), 3);
                }),
            },
        },
    )?;
    assert_eq!(HITS.swap(0, SeqCst), 4);
    Ok(())
}

#[compiler_test(imports)]
fn static_function_with_env(config: crate::Config) -> Result<()> {
    let store = config.store();
    let module = get_module(&store)?;

    #[derive(WasmerEnv, Clone)]
    struct Env(Arc<AtomicUsize>);

    impl std::ops::Deref for Env {
        type Target = Arc<AtomicUsize>;
        fn deref(&self) -> &Self::Target {
            &self.0
        }
    }

    let env: Env = Env(Arc::new(AtomicUsize::new(0)));
    Instance::new(
        &module,
        &imports! {
            "host" => {
                "0" => Function::new_native_with_env(&store, env.clone(), |env: &Env| {
                    assert_eq!(env.fetch_add(1, SeqCst), 0);
                }),
                "1" => Function::new_native_with_env(&store, env.clone(), |env: &Env, x: i32| -> i32 {
                    assert_eq!(x, 0);
                    assert_eq!(env.fetch_add(1, SeqCst), 1);
                    1
                }),
                "2" => Function::new_native_with_env(&store, env.clone(), |env: &Env, x: i32, y: i64| {
                    assert_eq!(x, 2);
                    assert_eq!(y, 3);
                    assert_eq!(env.fetch_add(1, SeqCst), 2);
                }),
                "3" => Function::new_native_with_env(&store, env.clone(), |env: &Env, a: i32, b: i64, c: i32, d: f32, e: f64| {
                    assert_eq!(a, 100);
                    assert_eq!(b, 200);
                    assert_eq!(c, 300);
                    assert_eq!(d, 400.0);
                    assert_eq!(e, 500.0);
                    assert_eq!(env.fetch_add(1, SeqCst), 3);
                }),
            },
        },
    )?;
    assert_eq!(env.load(SeqCst), 4);
    Ok(())
}

#[compiler_test(imports)]
fn static_function_that_fails(config: crate::Config) -> Result<()> {
    let store = config.store();
    let wat = r#"
        (import "host" "0" (func))

        (func $foo
            call 0
        )
        (start $foo)
    "#;

    let module = Module::new(&store, &wat)?;

    let result = Instance::new(
        &module,
        &imports! {
            "host" => {
                "0" => Function::new_native(&store, || -> Result<Infallible, RuntimeError> {
                    Err(RuntimeError::new("oops"))
                }),
            },
        },
    );

    assert!(result.is_err());

    match result {
        Err(InstantiationError::Start(runtime_error)) => {
            assert_eq!(runtime_error.message(), "oops")
        }
        _ => assert!(false),
    }

    Ok(())
}

fn get_module2(store: &Store) -> Result<Module> {
    let wat = r#"
        (import "host" "fn" (func))
        (memory $mem 1)
        (export "memory" (memory $mem))
        (export "main" (func $main))
        (func $main (param) (result)
          (call 0))
    "#;

    let module = Module::new(&store, &wat)?;
    Ok(module)
}

#[compiler_test(imports)]
fn dynamic_function_with_env_wasmer_env_init_works(config: crate::Config) -> Result<()> {
    let store = config.store();
    let module = get_module2(&store)?;

    #[allow(dead_code)]
    #[derive(WasmerEnv, Clone)]
    struct Env {
        memory: Memory,
    }
    let env: Env = Env {
        memory: Memory::new(
            &store,
            MemoryType {
                minimum: 0.into(),
                maximum: None,
                shared: false,
            },
        )?,
    };
    let function_fn = Function::new_with_env(
        &store,
        FunctionType::new(vec![], vec![]),
        env.clone(),
        |env, _values| Ok(vec![]),
    );
    let instance = Instance::new(
        &module,
        &imports! {
            "host" => {
                "fn" => function_fn,
            },
        },
    )?;
    let f: NativeFunc<(), ()> = instance.get_native_function("main")?;
    f.call()?;
    Ok(())
}

static REGRESSION_IMPORT_TRAMPOLINES: &str = r#"(module
  (type (;0;) (func))
  (type (;1;) (func (param i32)))
  (import "env" "panic" (func (;0;) (type 0)))
  (import "env" "gas" (func (;1;) (type 1)))
  (export "panic" (func 0))
  (export "gas" (func 1))
)"#;

#[compiler_test(imports)]
fn regression_import_trampolines(config: crate::Config) -> Result<()> {
    let store = config.store();
    let module = Module::new(&store, &REGRESSION_IMPORT_TRAMPOLINES)?;
    let panic = Function::new_native(&store, || ());
    static GAS_CALLED: AtomicBool = AtomicBool::new(false);
    let gas = Function::new_native(&store, |p: i32| {
        GAS_CALLED.store(true, SeqCst);
        assert_eq!(p, 42)
    });
    let imports = imports! {
        "env" => {
            "panic" => panic,
            "gas" => gas,
        }
    };
    let instance = Instance::new(&module, &imports)?;
    let panic = instance.lookup_function("panic").unwrap();
    panic.call(&[])?;
    let gas = instance.lookup_function("gas").unwrap();
    gas.call(&[Value::I32(42)])?;
    assert_eq!(GAS_CALLED.load(SeqCst), true);
    Ok(())
}

// TODO(0-copy): no longer possible to get references to exported entities other than functions
//               (we don't need that functionality)
// #[compiler_test(imports)]
// fn multi_use_host_fn_manages_memory_correctly(config: crate::Config) -> Result<()> {
//     let store = config.store();
//     let module = get_module2(&store)?;
//
//     #[allow(dead_code)]
//     #[derive(Clone)]
//     struct Env {
//         memory: LazyInit<Memory>,
//     }
//
//     impl WasmerEnv for Env {
//         fn init_with_instance(&mut self, instance: &Instance) -> Result<(), HostEnvInitError> {
//             let memory = instance.exports.get_memory("memory")?.clone();
//             self.memory.initialize(memory);
//             Ok(())
//         }
//     }
//
//     let env: Env = Env {
//         memory: LazyInit::default(),
//     };
//     fn host_fn(env: &Env) {
//         assert!(env.memory.get_ref().is_some());
//         println!("Hello, world!");
//     }
//
//     let imports = imports! {
//         "host" => {
//             "fn" => Function::new_native_with_env(&store, env.clone(), host_fn),
//         },
//     };
//     let instance1 = Instance::new(&module, &imports)?;
//     let instance2 = Instance::new(&module, &imports)?;
//     {
//         let f1: NativeFunc<(), ()> = instance1.get_native_function("main")?;
//         f1.call()?;
//     }
//     drop(instance1);
//     {
//         let f2: NativeFunc<(), ()> = instance2.get_native_function("main")?;
//         f2.call()?;
//     }
//     drop(instance2);
//     Ok(())
// }
//
// #[compiler_test(imports)]
// fn instance_local_memory_lifetime(config: crate::Config) -> Result<()> {
//     let store = config.store();
//
//     let memory: Memory = {
//         let wat = r#"(module
//     (memory $mem 1)
//     (export "memory" (memory $mem))
// )"#;
//         let module = Module::new(&store, wat)?;
//         let instance = Instance::new(&module, &imports! {})?;
//         instance.exports.get_memory("memory")?.clone()
//     };
//
//     let wat = r#"(module
//     (import "env" "memory" (memory $mem 1) )
//     (func $get_at (type $get_at_t) (param $idx i32) (result i32)
//       (i32.load (local.get $idx)))
//     (type $get_at_t (func (param i32) (result i32)))
//     (type $set_at_t (func (param i32) (param i32)))
//     (func $set_at (type $set_at_t) (param $idx i32) (param $val i32)
//       (i32.store (local.get $idx) (local.get $val)))
//     (export "get_at" (func $get_at))
//     (export "set_at" (func $set_at))
// )"#;
//     let module = Module::new(&store, wat)?;
//     let imports = imports! {
//         "env" => {
//             "memory" => memory,
//         },
//     };
//     let instance = Instance::new(&module, &imports)?;
//     let set_at: NativeFunc<(i32, i32), ()> = instance.get_native_function("set_at")?;
//     let get_at: NativeFunc<i32, i32> = instance.get_native_function("get_at")?;
//     set_at.call(200, 123)?;
//     assert_eq!(get_at.call(200)?, 123);
//
//     Ok(())
// }

'''
'''--- tests/compilers/issues.rs ---
//! This file is mainly to assure specific issues are working well
use anyhow::Result;
use wasmer::*;

/// Corruption of WasmerEnv when using call indirect.
///
/// Note: this one is specific to Singlepass, but we want to test in all
/// available compilers.
///
/// https://github.com/wasmerio/wasmer/issues/2329
#[compiler_test(issues)]
fn issue_2329(mut config: crate::Config) -> Result<()> {
    let store = config.store();

    #[derive(Clone, Default, WasmerEnv)]
    pub struct Env {
        memory: LazyInit<Memory>,
    }

    impl Env {
        pub fn new() -> Self {
            Self {
                memory: LazyInit::new(),
            }
        }
    }

    pub fn read_memory(env: &Env, guest_ptr: u32) -> u32 {
        dbg!(env.memory.get_ref());
        dbg!(guest_ptr);
        0
    }

    let wat = r#"
    (module
        (type (;0;) (func (param i32) (result i32)))
        (type (;1;) (func))
        (type (;2;) (func (param i32 i32) (result i32)))
        (import "env" "__read_memory" (func $__read_memory (type 0)))
        (func $read_memory (type 1)
          (drop
            (call $_ZN5other8dispatch17h053cb34ef5d0d7b0E
              (i32.const 1)
              (i32.const 2)))
          (drop
            (call $__read_memory
              (i32.const 1))))
        (func $_ZN5other8dispatch17h053cb34ef5d0d7b0E (type 2) (param i32 i32) (result i32)
          (call_indirect (type 0)
            (local.get 1)
            (local.get 0)))
        (table (;0;) 2 2 funcref)
        (memory (;0;) 16)
        (global (;0;) (mut i32) (i32.const 1048576))
        (global (;1;) i32 (i32.const 1048576))
        (global (;2;) i32 (i32.const 1048576))
        (export "memory" (memory 0))
        (export "read_memory" (func $read_memory))
        (export "__data_end" (global 1))
        (export "__heap_base" (global 2))
        (elem (;0;) (i32.const 1) func $__read_memory))
    "#;
    let module = Module::new(&store, wat)?;
    let env = Env::new();
    let imports: ImportObject = imports! {
        "env" => {
            "__read_memory" => Function::new_native_with_env(
                &store,
                env.clone(),
                read_memory
            ),
        }
    };
    let instance = Instance::new(&module, &imports)?;
    instance.lookup_function("read_memory").unwrap().call(&[])?;
    Ok(())
}

/// Exhaustion of GPRs when calling a function with many floating point arguments
///
/// Note: this one is specific to Singlepass, but we want to test in all
/// available compilers.
#[compiler_test(issues)]
fn regression_gpr_exhaustion_for_calls(mut config: crate::Config) -> Result<()> {
    let store = config.store();
    let wat = r#"
        (module
          (type (;0;) (func (param f64) (result i32)))
          (type (;1;) (func (param f64 f64 f64 f64 f64 f64)))
          (func (;0;) (type 0) (param f64) (result i32)
            local.get 0
            local.get 0
            local.get 0
            local.get 0
            f64.const 0
            f64.const 0
            f64.const 0
            f64.const 0
            f64.const 0
            f64.const 0
            f64.const 0
            i32.const 0
            call_indirect (type 0)
            call_indirect (type 1)
            drop
            drop
            drop
            drop
            i32.const 0)
          (table (;0;) 1 1 funcref))
    "#;
    let module = Module::new(&store, wat)?;
    let imports: ImportObject = imports! {};
    let instance = Instance::new(&module, &imports)?;
    Ok(())
}

'''
'''--- tests/compilers/main.rs ---
//! This test suite does all the tests that involve any compiler
//! implementation, such as: singlepass, cranelift or llvm depending
//! on what's available on the target.

#[macro_use]
extern crate compiler_test_derive;

mod config;
mod deterministic;
mod fast_gas_metering;
mod imports;
mod issues;
// mod multi_value_imports;
mod compilation;
mod native_functions;
mod serialize;
mod stack_limiter;
mod traps;
mod wast;

pub use crate::config::{Compiler, Config, Engine};
pub use crate::wast::run_wast;

'''
'''--- tests/compilers/multi_value_imports.rs ---
//! Testing the imports with different provided functions.
//! This tests checks that the provided functions (both native and
//! dynamic ones) work properly.

use wasmer::*;

macro_rules! mvr_test {
    ($test_name:ident, $( $result_type:ty ),* ) => {
        mod $test_name {
            use wasmer::*;

            fn get_module(store: &Store) -> anyhow::Result<wasmer::Module> {
                let wat: String = r#"
  (type $type (func (param i32) (result
"#.to_string() +
                    &stringify!( $( $result_type ),* ).replace(",", "").replace("(", "").replace(")", "") + &r#")))
  (import "host" "callback_fn" (func $callback_fn (type $type)))
  (func (export "test_call") (type $type)
    get_local 0
    call $callback_fn)
  (func (export "test_call_indirect") (type $type)
    (i32.const 1)
    (call_indirect (type $type) (i32.const 0))
  )
  (table funcref
    (elem
      $callback_fn
    )
  )
"#.to_string();
                Ok(wasmer::Module::new(&store, &wat)?)
            }

            fn callback_fn(n: i32) -> ( $( $result_type ),* ) {
                ( $( <$result_type>::expected_value(n) ),* )
            }

            #[compiler_test(multi_value_imports)]
            fn native(config: crate::Config) -> anyhow::Result<()> {
                let store = config.store();
                let module = get_module(&store)?;
                let instance = wasmer::Instance::new(
                    &module,
                    &wasmer::imports! {
                        "host" => {
                            "callback_fn" => wasmer::Function::new_native(&store, callback_fn)
                        }
                    }
                )?;
                let expected_value = vec![ $( <$result_type>::expected_val(1) ),* ].into_boxed_slice();
                assert_eq!(instance.exports.get_function("test_call")?.call(&[wasmer::Val::I32(1)])?,
                           expected_value);
                assert_eq!(instance.exports.get_function("test_call_indirect")?.call(&[wasmer::Val::I32(1)])?,
                           expected_value);
                Ok(())
            }

            fn dynamic_callback_fn(values: &[wasmer::Value]) -> anyhow::Result<Vec<wasmer::Val>, wasmer::RuntimeError> {
                assert_eq!(values[0], wasmer::Value::I32(1));
                Ok(vec![ $( <$result_type>::expected_val(1) ),* ])
            }

            #[compiler_test(multi_value_imports)]
            fn dynamic(config: crate::Config) -> anyhow::Result<()> {
                let store = config.store();
                let module = get_module(&store)?;
                let callback_fn = wasmer::Function::new(&store, &wasmer::FunctionType::new(vec![wasmer::ValType::I32], vec![ $( <$result_type>::expected_valtype() ),* ]), dynamic_callback_fn);
                let instance = wasmer::Instance::new(
                    &module,
                    &wasmer::imports! {
                        "host" => {
                            "callback_fn" => callback_fn
                        }
                    }
                )?;
                let expected_value = vec![ $( <$result_type>::expected_val(1) ),* ].into_boxed_slice();
                assert_eq!(instance.exports.get_function("test_call")?.call(&[wasmer::Val::I32(1)])?,
                           expected_value);
                assert_eq!(instance.exports.get_function("test_call_indirect")?.call(&[wasmer::Val::I32(1)])?,
                           expected_value);
                Ok(())
            }
        }
    }
}

trait ExpectedExpr {
    fn expected_value(n: i32) -> Self;
    fn expected_val(n: i32) -> wasmer::Val;
    fn expected_valtype() -> wasmer::ValType;
}
impl ExpectedExpr for i32 {
    fn expected_value(n: i32) -> i32 {
        n + 1
    }
    fn expected_val(n: i32) -> wasmer::Val {
        wasmer::Val::I32(Self::expected_value(n))
    }
    fn expected_valtype() -> wasmer::ValType {
        wasmer::ValType::I32
    }
}
impl ExpectedExpr for i64 {
    fn expected_value(n: i32) -> i64 {
        n as i64 + 2i64
    }
    fn expected_val(n: i32) -> wasmer::Val {
        wasmer::Val::I64(Self::expected_value(n))
    }
    fn expected_valtype() -> wasmer::ValType {
        wasmer::ValType::I64
    }
}
impl ExpectedExpr for f32 {
    fn expected_value(n: i32) -> f32 {
        n as f32 * 0.1
    }
    fn expected_val(n: i32) -> wasmer::Val {
        wasmer::Val::F32(Self::expected_value(n))
    }
    fn expected_valtype() -> wasmer::ValType {
        wasmer::ValType::F32
    }
}
impl ExpectedExpr for f64 {
    fn expected_value(n: i32) -> f64 {
        n as f64 * 0.12
    }
    fn expected_val(n: i32) -> wasmer::Val {
        wasmer::Val::F64(Self::expected_value(n))
    }
    fn expected_valtype() -> wasmer::ValType {
        wasmer::ValType::F64
    }
}

mvr_test!(test_mvr_i32_i32, i32, i32);
mvr_test!(test_mvr_i32_f32, i32, f32);
mvr_test!(test_mvr_f32_i32, f32, i32);
mvr_test!(test_mvr_f32_f32, f32, f32);

mvr_test!(test_mvr_i64_i32, i64, i32);
mvr_test!(test_mvr_i64_f32, i64, f32);
mvr_test!(test_mvr_f64_i32, f64, i32);
mvr_test!(test_mvr_f64_f32, f64, f32);

mvr_test!(test_mvr_i32_i64, i32, i64);
mvr_test!(test_mvr_f32_i64, f32, i64);
mvr_test!(test_mvr_i32_f64, i32, f64);
mvr_test!(test_mvr_f32_f64, f32, f64);

mvr_test!(test_mvr_i32_i32_i32, i32, i32, i32);
mvr_test!(test_mvr_i32_i32_f32, i32, i32, f32);
mvr_test!(test_mvr_i32_f32_i32, i32, f32, i32);
mvr_test!(test_mvr_i32_f32_f32, i32, f32, f32);
mvr_test!(test_mvr_f32_i32_i32, f32, i32, i32);
mvr_test!(test_mvr_f32_i32_f32, f32, i32, f32);
mvr_test!(test_mvr_f32_f32_i32, f32, f32, i32);
mvr_test!(test_mvr_f32_f32_f32, f32, f32, f32);

mvr_test!(test_mvr_i32_i32_i64, i32, i32, i64);
mvr_test!(test_mvr_i32_f32_i64, i32, f32, i64);
mvr_test!(test_mvr_f32_i32_i64, f32, i32, i64);
mvr_test!(test_mvr_f32_f32_i64, f32, f32, i64);
mvr_test!(test_mvr_i32_i32_f64, i32, i32, f64);
mvr_test!(test_mvr_i32_f32_f64, i32, f32, f64);
mvr_test!(test_mvr_f32_i32_f64, f32, i32, f64);
mvr_test!(test_mvr_f32_f32_f64, f32, f32, f64);

mvr_test!(test_mvr_i32_i64_i32, i32, i64, i32);
mvr_test!(test_mvr_i32_i64_f32, i32, i64, f32);
mvr_test!(test_mvr_f32_i64_i32, f32, i64, i32);
mvr_test!(test_mvr_f32_i64_f32, f32, i64, f32);
mvr_test!(test_mvr_i32_f64_i32, i32, f64, i32);
mvr_test!(test_mvr_i32_f64_f32, i32, f64, f32);
mvr_test!(test_mvr_f32_f64_i32, f32, f64, i32);
mvr_test!(test_mvr_f32_f64_f32, f32, f64, f32);

mvr_test!(test_mvr_i64_i32_i32, i64, i32, i32);
mvr_test!(test_mvr_i64_i32_f32, i64, i32, f32);
mvr_test!(test_mvr_i64_f32_i32, i64, f32, i32);
mvr_test!(test_mvr_i64_f32_f32, i64, f32, f32);
mvr_test!(test_mvr_f64_i32_i32, f64, i32, i32);
mvr_test!(test_mvr_f64_i32_f32, f64, i32, f32);
mvr_test!(test_mvr_f64_f32_i32, f64, f32, i32);
mvr_test!(test_mvr_f64_f32_f32, f64, f32, f32);

mvr_test!(test_mvr_i32_i32_i32_i32, i32, i32, i32, i32);
mvr_test!(test_mvr_i32_i32_i32_f32, i32, i32, i32, f32);
mvr_test!(test_mvr_i32_i32_f32_i32, i32, i32, f32, i32);
mvr_test!(test_mvr_i32_i32_f32_f32, i32, i32, f32, f32);
mvr_test!(test_mvr_i32_f32_i32_i32, i32, f32, i32, i32);
mvr_test!(test_mvr_i32_f32_i32_f32, i32, f32, i32, f32);
mvr_test!(test_mvr_i32_f32_f32_i32, i32, f32, f32, i32);
mvr_test!(test_mvr_i32_f32_f32_f32, i32, f32, f32, f32);
mvr_test!(test_mvr_f32_i32_i32_i32, f32, i32, i32, i32);
mvr_test!(test_mvr_f32_i32_i32_f32, f32, i32, i32, f32);
mvr_test!(test_mvr_f32_i32_f32_i32, f32, i32, f32, i32);
mvr_test!(test_mvr_f32_i32_f32_f32, f32, i32, f32, f32);
mvr_test!(test_mvr_f32_f32_i32_i32, f32, f32, i32, i32);
mvr_test!(test_mvr_f32_f32_i32_f32, f32, f32, i32, f32);
mvr_test!(test_mvr_f32_f32_f32_i32, f32, f32, f32, i32);
mvr_test!(test_mvr_f32_f32_f32_f32, f32, f32, f32, f32);

mvr_test!(test_mvr_i32_i32_i32_i32_i32, i32, i32, i32, i32, i32);

'''
'''--- tests/compilers/native_functions.rs ---
use anyhow::Result;
use std::convert::Infallible;
use std::sync::{Arc, Mutex};

use wasmer::*;

fn long_f(a: u32, b: u32, c: u32, d: u32, e: u32, f: u16, g: u64, h: u64, i: u16, j: u32) -> u64 {
    j as u64
        + i as u64 * 10
        + h * 100
        + g * 1000
        + f as u64 * 10000
        + e as u64 * 100000
        + d as u64 * 1000000
        + c as u64 * 10000000
        + b as u64 * 100000000
        + a as u64 * 1000000000
}

fn long_f_dynamic(values: &[Value]) -> Result<Vec<Value>, RuntimeError> {
    Ok(vec![Value::I64(
        values[9].unwrap_i32() as i64
            + values[8].unwrap_i32() as i64 * 10
            + values[7].unwrap_i64() * 100
            + values[6].unwrap_i64() * 1000
            + values[5].unwrap_i32() as i64 * 10000
            + values[4].unwrap_i32() as i64 * 100000
            + values[3].unwrap_i32() as i64 * 1000000
            + values[2].unwrap_i32() as i64 * 10000000
            + values[1].unwrap_i32() as i64 * 100000000
            + values[0].unwrap_i32() as i64 * 1000000000,
    )])
}

#[compiler_test(native_functions)]
fn native_function_works_for_wasm(config: crate::Config) -> anyhow::Result<()> {
    let store = config.store();
    let wat = r#"(module
        (func $multiply (import "env" "multiply") (param i32 i32) (result i32))
        (func (export "add") (param i32 i32) (result i32)
           (i32.add (local.get 0)
                    (local.get 1)))
        (func (export "double_then_add") (param i32 i32) (result i32)
           (i32.add (call $multiply (local.get 0) (i32.const 2))
                    (call $multiply (local.get 1) (i32.const 2))))
)"#;
    let module = Module::new(&store, wat).unwrap();

    let import_object = imports! {
        "env" => {
            "multiply" => Function::new_native(&store, |a: i32, b: i32| a * b),
        },
    };

    let instance = Instance::new(&module, &import_object)?;

    {
        let f: NativeFunc<(i32, i32), i32> = instance.get_native_function("add")?;
        let result = f.call(4, 6)?;
        assert_eq!(result, 10);
    }

    {
        let f: Function = instance
            .lookup_function("double_then_add")
            .expect("lookup function");
        let result = f.call(&[Val::I32(4), Val::I32(6)])?;
        assert_eq!(result[0], Val::I32(20));
    }

    {
        let dyn_f: Function = instance
            .lookup_function("double_then_add")
            .expect("lookup function");
        let f: NativeFunc<(i32, i32), i32> = dyn_f.native().unwrap();
        let result = f.call(4, 6)?;
        assert_eq!(result, 20);
    }

    Ok(())
}

#[should_panic(
    expected = "Closures (functions with captured environments) are currently unsupported with native functions. See: https://github.com/wasmerio/wasmer/issues/1840"
)]
#[compiler_test(native_functions)]
fn native_host_function_closure_panics(config: crate::Config) {
    let store = config.store();
    let state = 3;
    Function::new_native(&store, move |_: i32| {
        println!("{}", state);
    });
}

#[should_panic(
    expected = "Closures (functions with captured environments) are currently unsupported with native functions. See: https://github.com/wasmerio/wasmer/issues/1840"
)]
#[compiler_test(native_functions)]
fn native_with_env_host_function_closure_panics(config: crate::Config) {
    let store = config.store();
    let state = 3;
    let env = 4;
    Function::new_native_with_env(&store, env, move |_env: &i32, _: i32| {
        println!("{}", state);
    });
}

#[compiler_test(native_functions)]
fn non_native_functions_and_closures_with_no_env_work(config: crate::Config) -> anyhow::Result<()> {
    let store = config.store();
    let wat = r#"(module
        (func $multiply1 (import "env" "multiply1") (param i32 i32) (result i32))
        (func $multiply2 (import "env" "multiply2") (param i32 i32) (result i32))
        (func $multiply3 (import "env" "multiply3") (param i32 i32) (result i32))
        (func $multiply4 (import "env" "multiply4") (param i32 i32) (result i32))

        (func (export "test") (param i32 i32 i32 i32 i32) (result i32)
           (call $multiply4
             (call $multiply3
               (call $multiply2
                  (call $multiply1
                    (local.get 0)
                    (local.get 1))
                  (local.get 2))
               (local.get 3))
              (local.get 4)))
)"#;
    let module = Module::new(&store, wat).unwrap();

    let ty = FunctionType::new(vec![Type::I32, Type::I32], vec![Type::I32]);
    let env = 10;
    let captured_by_closure = 20;
    let import_object = imports! {
        "env" => {
            "multiply1" => Function::new(&store, &ty, move |args| {
                if let (Value::I32(v1), Value::I32(v2)) = (&args[0], &args[1]) {
                    Ok(vec![Value::I32(v1 * v2 * captured_by_closure)])
                } else {
                    panic!("Invalid arguments");
                }
            }),
            "multiply2" => Function::new_with_env(&store, &ty, env, move |&env, args| {
                if let (Value::I32(v1), Value::I32(v2)) = (&args[0], &args[1]) {
                    Ok(vec![Value::I32(v1 * v2 * captured_by_closure * env)])
                } else {
                    panic!("Invalid arguments");
                }
            }),
            "multiply3" => Function::new_native(&store, |arg1: i32, arg2: i32| -> i32
                                                {arg1 * arg2 }),
            "multiply4" => Function::new_native_with_env(&store, env, |&env: &i32, arg1: i32, arg2: i32| -> i32
                                                         {arg1 * arg2 * env }),
        },
    };

    let instance = Instance::new(&module, &import_object)?;

    let test: NativeFunc<(i32, i32, i32, i32, i32), i32> = instance.get_native_function("test")?;

    let result = test.call(2, 3, 4, 5, 6)?;
    let manually_computed_result = 6 * (5 * (4 * (3 * 2 * 20) * 10 * 20)) * 10;
    assert_eq!(result, manually_computed_result);
    Ok(())
}

#[compiler_test(native_functions)]
fn native_function_works_for_wasm_function_manyparams(config: crate::Config) -> anyhow::Result<()> {
    let store = config.store();
    let wat = r#"(module
        (func $longf (import "env" "longf") (param i32 i32 i32 i32 i32 i32 i64 i64 i32 i32) (result i64))
        (func (export "longf_pure") (param i32 i32 i32 i32 i32 i32 i64 i64 i32 i32) (result i64)
           (call $longf (local.get 0) (local.get 1) (local.get 2) (local.get 3) (local.get 4) (local.get 5) (local.get 6) (local.get 7) (local.get 8) (local.get 9)))
        (func (export "longf") (result i64)
           (call $longf (i32.const 1) (i32.const 2) (i32.const 3) (i32.const 4) (i32.const 5) (i32.const 6) (i64.const 7) (i64.const 8) (i32.const 9) (i32.const 0)))
)"#;
    let module = Module::new(&store, wat).unwrap();

    let import_object = imports! {
        "env" => {
            "longf" => Function::new_native(&store, long_f),
        },
    };

    let instance = Instance::new(&module, &import_object)?;

    {
        let dyn_f: Function = instance.lookup_function("longf").unwrap();
        let f: NativeFunc<(), i64> = dyn_f.native().unwrap();
        let result = f.call()?;
        assert_eq!(result, 1234567890);
    }

    {
        let dyn_f: Function = instance.lookup_function("longf_pure").unwrap();
        let f: NativeFunc<(u32, u32, u32, u32, u32, u16, u64, u64, u16, u32), i64> =
            dyn_f.native().unwrap();
        let result = f.call(1, 2, 3, 4, 5, 6, 7, 8, 9, 0)?;
        assert_eq!(result, 1234567890);
    }

    Ok(())
}

#[compiler_test(native_functions)]
fn native_function_works_for_wasm_function_manyparams_dynamic(
    config: crate::Config,
) -> anyhow::Result<()> {
    let store = config.store();
    let wat = r#"(module
        (func $longf (import "env" "longf") (param i32 i32 i32 i32 i32 i32 i64 i64 i32 i32) (result i64))
        (func (export "longf_pure") (param i32 i32 i32 i32 i32 i32 i64 i64 i32 i32) (result i64)
           (call $longf (local.get 0) (local.get 1) (local.get 2) (local.get 3) (local.get 4) (local.get 5) (local.get 6) (local.get 7) (local.get 8) (local.get 9)))
        (func (export "longf") (result i64)
           (call $longf (i32.const 1) (i32.const 2) (i32.const 3) (i32.const 4) (i32.const 5) (i32.const 6) (i64.const 7) (i64.const 8) (i32.const 9) (i32.const 0)))
)"#;
    let module = Module::new(&store, wat).unwrap();

    let import_object = imports! {
        "env" => {
            "longf" => Function::new(&store, FunctionType::new(vec![ValType::I32, ValType::I32, ValType::I32, ValType::I32, ValType::I32, ValType::I32, ValType::I64 , ValType::I64 ,ValType::I32, ValType::I32], vec![ValType::I64]), long_f_dynamic),
        },
    };

    let instance = Instance::new(&module, &import_object)?;

    {
        let dyn_f: Function = instance.lookup_function("longf").unwrap();
        let f: NativeFunc<(), i64> = dyn_f.native().unwrap();
        let result = f.call()?;
        assert_eq!(result, 1234567890);
    }

    {
        let dyn_f: Function = instance.lookup_function("longf_pure").unwrap();
        let f: NativeFunc<(u32, u32, u32, u32, u32, u16, u64, u64, u16, u32), i64> =
            dyn_f.native().unwrap();
        let result = f.call(1, 2, 3, 4, 5, 6, 7, 8, 9, 0)?;
        assert_eq!(result, 1234567890);
    }

    Ok(())
}

#[compiler_test(native_functions)]
fn static_host_function_without_env(config: crate::Config) -> anyhow::Result<()> {
    let store = config.store();

    fn f(a: i32, b: i64, c: f32, d: f64) -> (f64, f32, i64, i32) {
        (d * 4.0, c * 3.0, b * 2, a * 1)
    }

    fn f_ok(a: i32, b: i64, c: f32, d: f64) -> Result<(f64, f32, i64, i32), Infallible> {
        Ok((d * 4.0, c * 3.0, b * 2, a * 1))
    }

    fn long_f(
        a: u32,
        b: u32,
        c: u32,
        d: u32,
        e: u32,
        f: u16,
        g: u64,
        h: u64,
        i: u16,
        j: u32,
    ) -> (u32, u64, u32) {
        (
            a + b * 10 + c * 100 + d * 1000 + e * 10000 + f as u32 * 100000,
            g + h * 10,
            i as u32 + j * 10,
        )
    }

    // Native static host function that returns a tuple.
    {
        let f = Function::new_native(&store, f);
        let f_native: NativeFunc<(i32, i64, f32, f64), (f64, f32, i64, i32)> = f.native().unwrap();
        let result = f_native.call(1, 3, 5.0, 7.0)?;
        assert_eq!(result, (28.0, 15.0, 6, 1));
    }

    // Native static host function that returns a tuple.
    {
        let long_f = Function::new_native(&store, long_f);
        let long_f_native: NativeFunc<
            (u32, u32, u32, u32, u32, u16, u64, u64, u16, u32),
            (u32, u64, u32),
        > = long_f.native().unwrap();
        let result = long_f_native.call(1, 2, 3, 4, 5, 6, 7, 8, 9, 0)?;
        assert_eq!(result, (654321, 87, 09));
    }

    // Native static host function that returns a result of a tuple.
    {
        let f = Function::new_native(&store, f_ok);
        let f_native: NativeFunc<(i32, i64, f32, f64), (f64, f32, i64, i32)> = f.native().unwrap();
        let result = f_native.call(1, 3, 5.0, 7.0)?;
        assert_eq!(result, (28.0, 15.0, 6, 1));
    }

    Ok(())
}

#[compiler_test(native_functions)]
fn static_host_function_with_env(config: crate::Config) -> anyhow::Result<()> {
    let store = config.store();

    fn f(env: &Env, a: i32, b: i64, c: f32, d: f64) -> (f64, f32, i64, i32) {
        let mut guard = env.0.lock().unwrap();
        assert_eq!(*guard, 100);
        *guard = 101;

        (d * 4.0, c * 3.0, b * 2, a * 1)
    }

    fn f_ok(env: &Env, a: i32, b: i64, c: f32, d: f64) -> Result<(f64, f32, i64, i32), Infallible> {
        let mut guard = env.0.lock().unwrap();
        assert_eq!(*guard, 100);
        *guard = 101;

        Ok((d * 4.0, c * 3.0, b * 2, a * 1))
    }

    #[derive(WasmerEnv, Clone)]
    struct Env(Arc<Mutex<i32>>);

    impl std::ops::Deref for Env {
        type Target = Arc<Mutex<i32>>;
        fn deref(&self) -> &Self::Target {
            &self.0
        }
    }

    // Native static host function that returns a tuple.
    {
        let env = Env(Arc::new(Mutex::new(100)));

        let f = Function::new_native_with_env(&store, env.clone(), f);
        let f_native: NativeFunc<(i32, i64, f32, f64), (f64, f32, i64, i32)> = f.native().unwrap();

        assert_eq!(*env.0.lock().unwrap(), 100);

        let result = f_native.call(1, 3, 5.0, 7.0)?;

        assert_eq!(result, (28.0, 15.0, 6, 1));
        assert_eq!(*env.0.lock().unwrap(), 101);
    }

    // Native static host function that returns a result of a tuple.
    {
        let env = Env(Arc::new(Mutex::new(100)));

        let f = Function::new_native_with_env(&store, env.clone(), f_ok);
        let f_native: NativeFunc<(i32, i64, f32, f64), (f64, f32, i64, i32)> = f.native().unwrap();

        assert_eq!(*env.0.lock().unwrap(), 100);

        let result = f_native.call(1, 3, 5.0, 7.0)?;

        assert_eq!(result, (28.0, 15.0, 6, 1));
        assert_eq!(*env.0.lock().unwrap(), 101);
    }

    Ok(())
}

#[compiler_test(native_functions)]
fn dynamic_host_function_without_env(config: crate::Config) -> anyhow::Result<()> {
    let store = config.store();

    let f = Function::new(
        &store,
        FunctionType::new(
            vec![ValType::I32, ValType::I64, ValType::F32, ValType::F64],
            vec![ValType::F64, ValType::F32, ValType::I64, ValType::I32],
        ),
        |values| {
            Ok(vec![
                Value::F64(values[3].unwrap_f64() * 4.0),
                Value::F32(values[2].unwrap_f32() * 3.0),
                Value::I64(values[1].unwrap_i64() * 2),
                Value::I32(values[0].unwrap_i32() * 1),
            ])
        },
    );
    let f_native: NativeFunc<(i32, i64, f32, f64), (f64, f32, i64, i32)> = f.native().unwrap();
    let result = f_native.call(1, 3, 5.0, 7.0)?;

    assert_eq!(result, (28.0, 15.0, 6, 1));

    Ok(())
}

#[compiler_test(native_functions)]
fn dynamic_host_function_with_env(config: crate::Config) -> anyhow::Result<()> {
    let store = config.store();

    #[derive(WasmerEnv, Clone)]
    struct Env(Arc<Mutex<i32>>);

    impl std::ops::Deref for Env {
        type Target = Arc<Mutex<i32>>;
        fn deref(&self) -> &Self::Target {
            &self.0
        }
    }

    let env = Env(Arc::new(Mutex::new(100)));
    let f = Function::new_with_env(
        &store,
        FunctionType::new(
            vec![ValType::I32, ValType::I64, ValType::F32, ValType::F64],
            vec![ValType::F64, ValType::F32, ValType::I64, ValType::I32],
        ),
        env.clone(),
        |env, values| {
            let mut guard = env.0.lock().unwrap();
            assert_eq!(*guard, 100);

            *guard = 101;

            Ok(vec![
                Value::F64(values[3].unwrap_f64() * 4.0),
                Value::F32(values[2].unwrap_f32() * 3.0),
                Value::I64(values[1].unwrap_i64() * 2),
                Value::I32(values[0].unwrap_i32() * 1),
            ])
        },
    );

    let f_native: NativeFunc<(i32, i64, f32, f64), (f64, f32, i64, i32)> = f.native().unwrap();

    assert_eq!(*env.0.lock().unwrap(), 100);

    let result = f_native.call(1, 3, 5.0, 7.0)?;

    assert_eq!(result, (28.0, 15.0, 6, 1));
    assert_eq!(*env.0.lock().unwrap(), 101);

    Ok(())
}

'''
'''--- tests/compilers/serialize.rs ---
use anyhow::Result;
use wasmer::*;

#[compiler_test(serialize)]
fn test_serialize(config: crate::Config) -> Result<()> {
    let store = config.store();
    let wasm = wat2wasm(
        r#"
        (module
        (func $hello (import "" "hello"))
        (func (export "run") (call $hello))
        )
    "#
        .as_bytes(),
    )
    .unwrap();
    let engine = store.engine();
    let tunables = BaseTunables::for_target(engine.target());
    let executable = engine.compile(&wasm, &tunables).unwrap();
    let serialized = executable.serialize().unwrap();
    assert!(!serialized.is_empty());
    Ok(())
}

// #[compiler_test(serialize)]
// fn test_deserialize(config: crate::Config) -> Result<()> {
//     let store = config.store();
//     let wasm = wat2wasm(r#"
//         (module $name
//             (import "host" "sum_part" (func (param i32 i64 i32 f32 f64) (result i64)))
//             (func (export "test_call") (result i64)
//                 i32.const 100
//                 i64.const 200
//                 i32.const 300
//                 f32.const 400
//                 f64.const 500
//                 call 0
//             )
//         )
//     "#.as_bytes()).unwrap();
//
//     let engine = store.engine();
//     let tunables = BaseTunables::for_target(engine.target());
//     let executable = engine.compile(&wasm, &tunables).unwrap();
//     let writer = std::io::Cursor::new(vec![]);
//     executable.serialize(&mut writer).unwrap();
//     let serialized_bytes = writer.into_inner();
//
//     let headless_store = config.headless_store();
//
//
//     let deserialized_module = unsafe { Module::deserialize(&headless_store, &serialized_bytes)? };
//     assert_eq!(deserialized_module.name(), Some("name"));
//     assert_eq!(
//         deserialized_module.artifact().module_ref().exports().collect::<Vec<_>>(),
//         module.exports().collect::<Vec<_>>()
//     );
//     assert_eq!(
//         deserialized_module.artifact().module_ref().imports().collect::<Vec<_>>(),
//         module.imports().collect::<Vec<_>>()
//     );
//
//     let func_type = FunctionType::new(
//         vec![Type::I32, Type::I64, Type::I32, Type::F32, Type::F64],
//         vec![Type::I64],
//     );
//     let instance = Instance::new(
//         &module,
//         &imports! {
//             "host" => {
//                 "sum_part" => Function::new(&store, &func_type, |params| {
//                     let param_0: i64 = params[0].unwrap_i32() as i64;
//                     let param_1: i64 = params[1].unwrap_i64() as i64;
//                     let param_2: i64 = params[2].unwrap_i32() as i64;
//                     let param_3: i64 = params[3].unwrap_f32() as i64;
//                     let param_4: i64 = params[4].unwrap_f64() as i64;
//                     Ok(vec![Value::I64(param_0 + param_1 + param_2 + param_3 + param_4)])
//                 })
//             }
//         },
//     )?;
//
//     let test_call = instance.exports.get_function("test_call")?;
//     let result = test_call.call(&[])?;
//     assert_eq!(result.to_vec(), vec![Value::I64(1500)]);
//     Ok(())
// }

'''
'''--- tests/compilers/stack_limiter.rs ---
use wasmer::*;
use wasmer_compiler_singlepass::Singlepass;
use wasmer_engine_universal::Universal;
use wasmer_types::InstanceConfig;
use wasmer_vm::TrapCode;

fn get_store() -> Store {
    let compiler = Singlepass::default();
    let store = Store::new(&Universal::new(compiler).engine());
    store
}

#[test]
fn stack_limit_hit() {
    /* This contracts is
    (module
    (type (;0;) (func))
    (func (;0;) (type 0)
      (local f64 <32750 times>)
       local.get 1
       local.get 0
       f64.copysign
       call 0
       unreachable)
    (memory (;0;) 16 144)
    (export "main" (func 0)))
     */
    let wasm: [u8; 53] = [
        0x00, 0x61, 0x73, 0x6d, 0x01, 0x00, 0x00, 0x00, 0x01, 0x04, 0x01, 0x60, 0x00, 0x00, 0x03,
        0x02, 0x01, 0x00, 0x05, 0x05, 0x01, 0x01, 0x10, 0x90, 0x01, 0x07, 0x08, 0x01, 0x04, 0x6d,
        0x61, 0x69, 0x6e, 0x00, 0x00, 0x0a, 0x10, 0x01, 0x0e, 0x01, 0xee, 0xff, 0x01, 0x7c, 0x20,
        0x01, 0x20, 0x00, 0xa6, 0x10, 0x00, 0x00, 0x0b,
    ];
    let store = get_store();
    let module = Module::new(&store, &wasm).unwrap();
    let instance = Instance::new_with_config(
        &module,
        unsafe { InstanceConfig::default().with_stack_limit(100000) },
        &imports! {},
    );
    assert!(instance.is_ok());
    let instance = instance.unwrap();
    let main_func = instance
        .lookup_function("main")
        .expect("expected function main");
    match main_func.call(&[]) {
        Err(err) => {
            let trap = err.to_trap().unwrap();
            assert_eq!(trap, TrapCode::StackOverflow);
        }
        _ => assert!(false),
    }
}

#[test]
fn stack_limit_operand_stack() {
    let wat = format!(
        r#"
        (func $foo (param $depth i32)
            block
                (br_if 1 (i32.eq (local.get $depth) (i32.const 0)))
                local.get $depth
                i32.const 1
                i32.sub
                local.set $depth
                {extra_operand_stack}
                local.get $depth
                call $foo
                {depopulate_operand_stack}
            end
        )
        (func (export "main")
            (call $foo (i32.const 1000))
        )
    "#,
        extra_operand_stack = "local.get $depth\n".repeat(10000),
        depopulate_operand_stack = "drop\n".repeat(10000)
    );

    let store = get_store();
    let module = Module::new(&store, &wat).unwrap();
    let instance = Instance::new_with_config(
        &module,
        unsafe { InstanceConfig::default().with_stack_limit(1000) },
        &imports! {},
    );
    assert!(instance.is_ok());
    let instance = instance.unwrap();
    let main_func = instance
        .lookup_function("main")
        .expect("expected function main");
    match main_func.call(&[]) {
        Err(err) => {
            let trap = err.to_trap().unwrap();
            assert_eq!(trap, TrapCode::StackOverflow);
        }
        _ => assert!(false),
    }
}

const OK_WAT: &str = r#"
    (memory (;0;) 1000 10000)
    (func $foo
        (local f64)
        i32.const 0
        i32.const 1
        i32.add
        drop
    )
    (func (export "main")
        (local $v0 i32)
        i32.const 1000000
        local.set $v0
        loop $L0
            local.get $v0
            i32.const 1
            i32.sub
            local.set $v0
            call $foo
            local.get $v0
            i32.const 0
            i32.gt_s
            br_if $L0
        end
    )
"#;

#[test]
fn stack_limit_ok() {
    let wat = OK_WAT;
    let store = get_store();
    let module = Module::new(&store, &wat).unwrap();
    let instance = Instance::new_with_config(
        &module,
        unsafe { InstanceConfig::default().with_stack_limit(1000) },
        &imports! {},
    );
    assert!(instance.is_ok());
    let instance = instance.unwrap();
    let main_func = instance
        .lookup_function("main")
        .expect("expected function main");
    let e = main_func.call(&[]);
    assert!(e.is_ok());
}

#[test]
fn stack_limit_huge_limit() {
    let wat = OK_WAT;
    let store = get_store();
    let module = Module::new(&store, &wat).unwrap();
    let instance = Instance::new_with_config(
        &module,
        unsafe { InstanceConfig::default().with_stack_limit(0x7FFF_FFFF) },
        &imports! {},
    );
    assert!(instance.is_ok());
    let instance = instance.unwrap();
    let main_func = instance
        .lookup_function("main")
        .expect("expected function main");
    main_func.call(&[]).unwrap();
}

#[test]
fn stack_limit_no_args() {
    let wat = r#"
        (func $foo
            call $foo
        )
        (func (export "main")
            call $foo
        )
    "#;

    let store = get_store();
    let module = Module::new(&store, &wat).unwrap();
    let instance = Instance::new_with_config(
        &module,
        unsafe { InstanceConfig::default().with_stack_limit(1000) },
        &imports! {},
    );
    assert!(instance.is_ok());
    let instance = instance.unwrap();
    let main_func = instance
        .lookup_function("main")
        .expect("expected function main");
    match main_func.call(&[]) {
        Err(err) => {
            let trap = err.to_trap().unwrap();
            assert_eq!(trap, TrapCode::StackOverflow);
        }
        _ => assert!(false),
    }
}

#[test]
fn deep_but_sane() {
    let wat = r#"
        (func $foo (param $p0 i32) (result i32)
            local.get $p0
            i32.const 1
            i32.sub
            local.set $p0
            block $B0
                local.get $p0
                i32.const 0
                i32.le_s
                br_if $B0
                local.get $p0
                call $foo
                drop
            end
            local.get $p0
        )
        (func (export "main")
            i32.const 1000
            call $foo
            drop
        )
    "#;

    let store = get_store();
    let module = Module::new(&store, &wat).unwrap();
    let instance = Instance::new_with_config(&module, InstanceConfig::default(), &imports! {});
    assert!(instance.is_ok());
    let instance = instance.unwrap();
    let main_func = instance
        .lookup_function("main")
        .expect("expected function main");

    let e = main_func.call(&[]);
    assert!(e.is_ok());
}

'''
'''--- tests/compilers/traps.rs ---
use anyhow::Result;
use std::panic::{self, AssertUnwindSafe};
use wasmer::*;

#[compiler_test(traps)]
fn test_trap_return(config: crate::Config) -> Result<()> {
    let store = config.store();
    let wat = r#"
        (module
        (func $hello (import "" "hello"))
        (func (export "run") (call $hello))
        )
    "#;

    let module = Module::new(&store, wat)?;
    let hello_type = FunctionType::new(vec![], vec![]);
    let hello_func = Function::new(&store, &hello_type, |_| Err(RuntimeError::new("test 123")));

    let instance = Instance::new(
        &module,
        &imports! {
            "" => {
                "hello" => hello_func
            }
        },
    )?;
    let run_func = instance
        .lookup_function("run")
        .expect("expected function export");

    let e = run_func.call(&[]).err().expect("error calling function");

    assert_eq!(e.message(), "test 123");

    Ok(())
}

#[cfg_attr(target_env = "musl", ignore)]
#[compiler_test(traps)]
fn test_trap_trace(config: crate::Config) -> Result<()> {
    let store = config.store();
    let wat = r#"
        (module $hello_mod
            (func (export "run") (call $hello))
            (func $hello (unreachable))
        )
    "#;

    let module = Module::new(&store, wat)?;
    let instance = Instance::new(&module, &imports! {})?;
    let run_func = instance
        .lookup_function("run")
        .expect("expected function export");

    let e = run_func.call(&[]).err().expect("error calling function");

    let trace = e.trace();
    assert_eq!(trace.len(), 2);
    assert_eq!(trace[0].module_name(), "hello_mod");
    assert_eq!(trace[0].func_index(), 1);
    assert_eq!(trace[0].function_name(), Some("hello"));
    assert_eq!(trace[1].module_name(), "hello_mod");
    assert_eq!(trace[1].func_index(), 0);
    assert_eq!(trace[1].function_name(), None);
    assert!(
        e.message().contains("unreachable"),
        "wrong message: {}",
        e.message()
    );

    Ok(())
}

#[compiler_test(traps)]
fn test_trap_trace_cb(config: crate::Config) -> Result<()> {
    let store = config.store();
    let wat = r#"
        (module $hello_mod
            (import "" "throw" (func $throw))
            (func (export "run") (call $hello))
            (func $hello (call $throw))
        )
    "#;

    let fn_type = FunctionType::new(vec![], vec![]);
    let fn_func = Function::new(&store, &fn_type, |_| Err(RuntimeError::new("cb throw")));

    let module = Module::new(&store, wat)?;
    let instance = Instance::new(
        &module,
        &imports! {
            "" => {
                "throw" => fn_func
            }
        },
    )?;
    let run_func = instance
        .lookup_function("run")
        .expect("expected function export");

    let e = run_func.call(&[]).err().expect("error calling function");

    let trace = e.trace();
    println!("Trace {:?}", trace);
    // TODO: Reenable this (disabled as it was not working with llvm/singlepass)
    // assert_eq!(trace.len(), 2);
    // assert_eq!(trace[0].module_name(), "hello_mod");
    // assert_eq!(trace[0].func_index(), 2);
    // assert_eq!(trace[1].module_name(), "hello_mod");
    // assert_eq!(trace[1].func_index(), 1);
    assert_eq!(e.message(), "cb throw");

    Ok(())
}

#[cfg_attr(target_env = "musl", ignore)]
#[compiler_test(traps)]
fn test_trap_stack_overflow(config: crate::Config) -> Result<()> {
    let store = config.store();
    let wat = r#"
        (module $rec_mod
            (func $run (export "run") (call $run))
        )
    "#;

    let module = Module::new(&store, wat)?;
    let instance = Instance::new(&module, &imports! {})?;
    let run_func = instance
        .lookup_function("run")
        .expect("expected function export");

    let e = run_func.call(&[]).err().expect("error calling function");

    let trace = e.trace();
    assert!(trace.len() >= 32);
    for i in 0..trace.len() {
        assert_eq!(trace[i].module_name(), "rec_mod");
        assert_eq!(trace[i].func_index(), 0);
        assert_eq!(trace[i].function_name(), Some("run"));
    }
    assert!(e.message().contains("call stack exhausted"));

    Ok(())
}

#[cfg_attr(target_env = "musl", ignore)]
#[compiler_test(traps)]
fn trap_display_pretty(config: crate::Config) -> Result<()> {
    let store = config.store();
    let wat = r#"
        (module $m
            (func $die unreachable)
            (func call $die)
            (func $foo call 1)
            (func (export "bar") call $foo)
        )
    "#;

    let module = Module::new(&store, wat)?;
    let instance = Instance::new(&module, &imports! {})?;
    let run_func = instance
        .lookup_function("bar")
        .expect("expected function export");

    let e = run_func.call(&[]).err().expect("error calling function");
    assert_eq!(
        e.to_string(),
        "\
RuntimeError: unreachable
    at die (m[0]:0x23)
    at <unnamed> (m[1]:0x27)
    at foo (m[2]:0x2c)
    at <unnamed> (m[3]:0x31)"
    );
    Ok(())
}

#[cfg_attr(target_env = "musl", ignore)]
#[compiler_test(traps)]
fn trap_display_multi_module(config: crate::Config) -> Result<()> {
    let store = config.store();
    let wat = r#"
        (module $a
            (func $die unreachable)
            (func call $die)
            (func $foo call 1)
            (func (export "bar") call $foo)
        )
    "#;

    let module = Module::new(&store, wat)?;
    let instance = Instance::new(&module, &imports! {})?;
    let bar = instance.lookup_function("bar").unwrap();

    let wat = r#"
        (module $b
            (import "" "" (func $bar))
            (func $middle call $bar)
            (func (export "bar2") call $middle)
        )
    "#;
    let module = Module::new(&store, wat)?;
    let instance = Instance::new(
        &module,
        &imports! {
            "" => {
                "" => bar
            }
        },
    )?;
    let bar2 = instance
        .lookup_function("bar2")
        .expect("expected function export");

    let e = bar2.call(&[]).err().expect("error calling function");
    assert_eq!(
        e.to_string(),
        "\
RuntimeError: unreachable
    at die (a[0]:0x23)
    at <unnamed> (a[1]:0x27)
    at foo (a[2]:0x2c)
    at <unnamed> (a[3]:0x31)
    at middle (b[1]:0x29)
    at <unnamed> (b[2]:0x2e)"
    );
    Ok(())
}

#[compiler_test(traps)]
fn trap_start_function_import(config: crate::Config) -> Result<()> {
    let store = config.store();
    let binary = r#"
        (module $a
            (import "" "" (func $foo))
            (start $foo)
        )
    "#;

    let module = Module::new(&store, &binary)?;
    let sig = FunctionType::new(vec![], vec![]);
    let func = Function::new(&store, &sig, |_| Err(RuntimeError::new("user trap")));
    let err = Instance::new(
        &module,
        &imports! {
            "" => {
                "" => func
            }
        },
    )
    .err()
    .unwrap();
    match err {
        InstantiationError::Start(err) => {
            assert_eq!(err.message(), "user trap");
        }
        _ => {
            panic!("It should be a start error")
        }
    }

    Ok(())
}

#[compiler_test(traps)]
fn rust_panic_import(config: crate::Config) -> Result<()> {
    let store = config.store();
    let binary = r#"
        (module $a
            (import "" "foo" (func $foo))
            (import "" "bar" (func $bar))
            (func (export "foo") call $foo)
            (func (export "bar") call $bar)
        )
    "#;

    let module = Module::new(&store, &binary)?;
    let sig = FunctionType::new(vec![], vec![]);
    let func = Function::new(&store, &sig, |_| panic!("this is a panic"));
    let instance = Instance::new(
        &module,
        &imports! {
            "" => {
                "foo" => func,
                "bar" => Function::new_native(&store, || panic!("this is another panic"))
            }
        },
    )?;
    let func = instance.lookup_function("foo").unwrap();
    let err = panic::catch_unwind(AssertUnwindSafe(|| {
        drop(func.call(&[]));
    }))
    .unwrap_err();
    assert_eq!(err.downcast_ref::<&'static str>(), Some(&"this is a panic"));

    // TODO: Reenable this (disabled as it was not working with llvm/singlepass)
    // It doesn't work either with cranelift and `--test-threads=1`.
    // let func = instance.lookup_function("bar")?.clone();
    // let err = panic::catch_unwind(AssertUnwindSafe(|| {
    //     drop(func.call(&[]));
    // }))
    // .unwrap_err();
    // assert_eq!(
    //     err.downcast_ref::<&'static str>(),
    //     Some(&"this is another panic")
    // );
    Ok(())
}

#[compiler_test(traps)]
fn rust_panic_start_function(config: crate::Config) -> Result<()> {
    let store = config.store();
    let binary = r#"
        (module $a
            (import "" "" (func $foo))
            (start $foo)
        )
    "#;

    let module = Module::new(&store, &binary)?;
    let sig = FunctionType::new(vec![], vec![]);
    let func = Function::new(&store, &sig, |_| panic!("this is a panic"));
    let err = panic::catch_unwind(AssertUnwindSafe(|| {
        drop(Instance::new(
            &module,
            &imports! {
                "" => {
                    "" => func
                }
            },
        ));
    }))
    .unwrap_err();
    assert_eq!(err.downcast_ref::<&'static str>(), Some(&"this is a panic"));

    let func = Function::new_native(&store, || panic!("this is another panic"));
    let err = panic::catch_unwind(AssertUnwindSafe(|| {
        drop(Instance::new(
            &module,
            &imports! {
                "" => {
                    "" => func
                }
            },
        ));
    }))
    .unwrap_err();
    assert_eq!(
        err.downcast_ref::<&'static str>(),
        Some(&"this is another panic")
    );
    Ok(())
}

#[compiler_test(traps)]
fn mismatched_arguments(config: crate::Config) -> Result<()> {
    let store = config.store();
    let binary = r#"
        (module $a
            (func (export "foo") (param i32))
        )
    "#;

    let module = Module::new(&store, &binary)?;
    let instance = Instance::new(&module, &imports! {})?;
    let func: Function = instance.lookup_function("foo").unwrap();
    assert_eq!(
        func.call(&[]).unwrap_err().message(),
        "Parameters of type [] did not match signature [I32] -> []"
    );
    assert_eq!(
        func.call(&[Val::F32(0.0)]).unwrap_err().message(),
        "Parameters of type [F32] did not match signature [I32] -> []",
    );
    assert_eq!(
        func.call(&[Val::I32(0), Val::I32(1)])
            .unwrap_err()
            .message(),
        "Parameters of type [I32, I32] did not match signature [I32] -> []"
    );
    Ok(())
}

#[cfg_attr(target_env = "musl", ignore)]
#[compiler_test(traps)]
fn call_signature_mismatch(config: crate::Config) -> Result<()> {
    let store = config.store();
    let binary = r#"
        (module $a
            (func $foo
                i32.const 0
                call_indirect)
            (func $bar (param i32))
            (start $foo)

            (table 1 anyfunc)
            (elem (i32.const 0) 1)
        )
    "#;

    let module = Module::new(&store, &binary)?;
    let err = Instance::new(&module, &imports! {})
        .err()
        .expect("expected error");
    assert_eq!(
        format!("{}", err),
        "\
RuntimeError: indirect call type mismatch
    at foo (a[0]:0x30)\
"
    );
    Ok(())
}

#[compiler_test(traps)]
#[cfg_attr(target_env = "musl", ignore)]
fn start_trap_pretty(config: crate::Config) -> Result<()> {
    let store = config.store();
    let wat = r#"
        (module $m
            (func $die unreachable)
            (func call $die)
            (func $foo call 1)
            (func $start call $foo)
            (start $start)
        )
    "#;

    let module = Module::new(&store, wat)?;
    let err = Instance::new(&module, &imports! {})
        .err()
        .expect("expected error");

    assert_eq!(
        format!("{}", err),
        "\
RuntimeError: unreachable
    at die (m[0]:0x1d)
    at <unnamed> (m[1]:0x21)
    at foo (m[2]:0x26)
    at start (m[3]:0x2b)\
"
    );
    Ok(())
}

#[compiler_test(traps)]
fn present_after_module_drop(config: crate::Config) -> Result<()> {
    let store = config.store();
    let module = Module::new(&store, r#"(func (export "foo") unreachable)"#)?;
    let instance = Instance::new(&module, &imports! {})?;
    let func: Function = instance.lookup_function("foo").unwrap();

    println!("asserting before we drop modules");
    assert_trap(func.call(&[]).unwrap_err());
    drop((instance, module));

    println!("asserting after drop");
    assert_trap(func.call(&[]).unwrap_err());
    return Ok(());

    fn assert_trap(t: RuntimeError) {
        println!("{}", t);
        // assert_eq!(t.trace().len(), 1);
        // assert_eq!(t.trace()[0].func_index(), 0);
    }
}

'''
'''--- tests/compilers/wast.rs ---
use ::wasmer::Features;
use std::path::Path;
use wasmer_wast::Wast;

// The generated tests (from build.rs) look like:
// #[cfg(test)]
// mod [compiler] {
//     mod [spec] {
//         mod [vfs] {
//             #[test]
//             fn [test_name]() -> anyhow::Result<()> {
//                 crate::run_wasi("tests/spectests/[test_name].wast", "[compiler]", WasiFileSystemKind::[vfs])
//             }
//         }
//     }
// }
include!(concat!(env!("OUT_DIR"), "/generated_spectests.rs"));

pub fn run_wast(mut config: crate::Config, wast_path: &str) -> anyhow::Result<()> {
    println!("Running wast `{}`", wast_path);
    let try_nan_canonicalization = wast_path.contains("nan-canonicalization");
    let mut features = Features::default();
    let is_bulkmemory = wast_path.contains("bulk-memory");
    let is_simd = wast_path.contains("simd");
    if is_bulkmemory {
        features.bulk_memory(true);
    }
    if is_simd {
        features.simd(true);
    }
    if config.compiler == crate::Compiler::Singlepass {
        features.multi_value(false);
    }
    config.set_features(features);
    config.set_nan_canonicalization(try_nan_canonicalization);

    let store = config.store();
    let mut wast = Wast::new_with_spectest(store);
    // `bulk-memory-operations/bulk.wast` checks for a message that
    // specifies which element is uninitialized, but our traps don't
    // shepherd that information out.
    wast.allow_trap_message("uninitialized element 2", "uninitialized element");
    // `liking.wast` has different wording but the same meaning
    wast.allow_trap_message("out of bounds memory access", "memory out of bounds");
    if config.compiler == crate::Compiler::Cranelift && config.engine == crate::Engine::Dylib {
        wast.allow_trap_message("call stack exhausted", "out of bounds memory access");
        wast.allow_trap_message("indirect call type mismatch", "call stack exhausted");
        wast.allow_trap_message("integer divide by zero", "call stack exhausted");
        wast.allow_trap_message("integer overflow", "call stack exhausted");
        wast.allow_trap_message("invalid conversion to integer", "call stack exhausted");
        wast.allow_trap_message("undefined element", "call stack exhausted");
        wast.allow_trap_message("uninitialized element", "call stack exhausted");
        wast.allow_trap_message("unreachable", "call stack exhausted");
    }
    if cfg!(feature = "coverage") {
        wast.disable_assert_and_exhaustion();
    }
    if is_simd {
        // We allow this, so tests can be run properly for `simd_const` test.
        wast.allow_instantiation_failures(&[
            "Validation error: multiple tables",
            "Validation error: unknown memory 0",
            "Validation error: Invalid var_u32",
        ]);
    }
    if config.compiler == crate::Compiler::Singlepass {
        // We don't support multivalue yet in singlepass
        wast.allow_instantiation_failures(&[
            "Validation error: invalid result arity: func type returns multiple values",
            "Validation error: blocks, loops, and ifs accept no parameters when multi-value is not enabled",
        ]);
    }
    wast.fail_fast = false;
    let path = Path::new(wast_path);
    wast.run_file(path)
}

'''
'''--- tests/deprecated.rs ---

'''
'''--- tests/deprecated/README.md ---
# Tests: Deprecated

In this folder we will test deprecated APIs to verify that they
still work the way it should, so users of Wasmer are happy while
transitioning to newer APIs.

As time passes and adoption of new APIs rises, tests on this folder
should start tending towards 0.

Therefore, deprecated tests are intended to be deleted on the long
term.

'''
'''--- tests/ignores.txt ---
# Compilers
singlepass spec::multi_value # Singlepass has not implemented multivalue (functions that returns "structs"/"tuples")
singlepass spec::simd # Singlepass doesn't support yet SIMD (no one asked for this feature)

singlepass+dylib * # It needs to add support for PIC in Singlepass. Not implemented at the moment
windows+dylib * # This might be trivial to fix?
musl+dylib * # Dynamic loading not supported in Musl

# Traps
## Traps. Tracing doesn't work properly in Singlepass
## Unwinding is not properly implemented in Singlepass
# Needs investigation
singlepass traps::test_trap_trace
dylib     traps::test_trap_trace
aarch64    traps::test_trap_trace
singlepass traps::test_trap_stack_overflow # Need to investigate
dylib     traps::test_trap_stack_overflow # Need to investigate
aarch64    traps::test_trap_stack_overflow # Need to investigate
singlepass traps::trap_display_pretty
llvm       traps::trap_display_pretty
dylib     traps::trap_display_pretty
aarch64    traps::trap_display_pretty
singlepass traps::trap_display_multi_module
llvm       traps::trap_display_multi_module
dylib     traps::trap_display_multi_module
aarch64    traps::trap_display_multi_module
singlepass traps::call_signature_mismatch
llvm       traps::call_signature_mismatch
dylib     traps::call_signature_mismatch
macos+aarch64    traps::call_signature_mismatch
singlepass traps::start_trap_pretty
llvm       traps::start_trap_pretty
dylib     traps::start_trap_pretty
aarch64    traps::start_trap_pretty

cranelift  multi_value_imports::dylib # Needs investigation
singlepass multi_value_imports::dylib # Singlepass doesn't support multivalue
singlepass multi_value_imports::dynamic # Singlepass doesn't support multivalue

# LLVM doesn't fully work in macOS M1
llvm+universal+macos+aarch64 * # We are using the object crate, it was not fully supporting aarch64 relocations emitted by LLVM. Needs reassesment
llvm+dylib+macos+aarch64 * # Tests seem to be randomly failing

# TODO: We need to fix this in ARM. The issue is caused by libunwind overflowing
# the stack while creating the stacktrace.
# https://github.com/rust-lang/backtrace-rs/issues/356
cranelift+aarch64 spec::skip_stack_guard_page # This is skipped for ARM, not fully fixed yet
llvm+aarch64      spec::skip_stack_guard_page # This is skipped for ARM, not fully fixed yet
singlepass+windows spec::skip_stack_guard_page # Needs investigation.
cranelift+windows spec::skip_stack_guard_page # Needs investigation. Issue: `STATUS_ACCESS_VIOLATION` trap happened
cranelift+macos   spec::skip_stack_guard_page # Needs investigation. process didn't exit successfully: (signal: 6, SIGABRT: process abort signal)
llvm+macos        spec::skip_stack_guard_page # Needs investigation. process didn't exit successfully: (signal: 6, SIGABRT: process abort signal)

# TODO(https://github.com/wasmerio/wasmer/issues/1727): Traps in dylib engine
cranelift+dylib spec::linking # Needs investigation
cranelift+dylib spec::bulk # Needs investigation

# Some SIMD opperations are not yet supported by Cranelift
# Cranelift just added support for most of those recently, it might be easy to update
cranelift spec::simd::simd_conversions
cranelift spec::simd::simd_i16x8_extadd_pairwise_i8x16
cranelift spec::simd::simd_i16x8_extmul_i8x16
cranelift spec::simd::simd_i16x8_q15mulr_sat_s
cranelift spec::simd::simd_i32x4_extadd_pairwise_i16x8
cranelift spec::simd::simd_i32x4_extmul_i16x8
cranelift spec::simd::simd_i32x4_trunc_sat_f64x2
cranelift spec::simd::simd_i64x2_extmul_i32x4
cranelift spec::simd::simd_i8x16_arith2
cranelift spec::simd::simd_int_to_int_extend

# Windows doesn't overcommit and fails to allocate 4GB of memory
windows wasmer::max_size_of_memory

# Frontends

## WASI

### These tests don't pass due to race conditions in the new way we run tests.
### It's not built to be run in parallel with itself, so we disable it for now.

wasitests::snapshot1::host_fs::writing
wasitests::unstable::host_fs::writing
wasitests::snapshot1::mem_fs::writing
wasitests::unstable::mem_fs::writing

### due to hard-coded direct calls into WASI for wasi unstable

wasitests::snapshot1::host_fs::fd_read
wasitests::snapshot1::host_fs::poll_oneoff
wasitests::snapshot1::host_fs::fd_pread
wasitests::snapshot1::host_fs::fd_close
wasitests::snapshot1::host_fs::fd_allocate
wasitests::snapshot1::host_fs::close_preopen_fd
wasitests::snapshot1::host_fs::envvar
wasitests::snapshot1::mem_fs::fd_read
wasitests::snapshot1::mem_fs::poll_oneoff
wasitests::snapshot1::mem_fs::fd_pread
wasitests::snapshot1::mem_fs::fd_close
wasitests::snapshot1::mem_fs::fd_allocate
wasitests::snapshot1::mem_fs::close_preopen_fd
wasitests::snapshot1::mem_fs::envvar

### TODO: resolve the disabled tests below. These are newly disabled tests from the migration:

### due to git clone not preserving symlinks:
wasitests::snapshot1::host_fs::readlink
wasitests::unstable::host_fs::readlink
wasitests::snapshot1::mem_fs::readlink
wasitests::unstable::mem_fs::readlink

### failing due to `remove_dir_all`. this test is also bad for parallelism
wasitests::snapshot1::host_fs::create_dir
wasitests::unstable::host_fs::create_dir
wasitests::snapshot1::mem_fs::create_dir
wasitests::unstable::mem_fs::create_dir

### failing because it closes `stdout` which breaks our testing system
wasitests::unstable::host_fs::fd_close
wasitests::unstable::mem_fs::fd_close

### failing because we're operating on stdout which is now overridden.
### TODO: check WasiFile implementation
### Alterative: split test into 2 parts, one printing to stderr, the other printing to stdout to test the real versions
wasitests::unstable::host_fs::poll_oneoff
wasitests::unstable::mem_fs::poll_oneoff

### randomly failed, mainly on windows but also on macos, due to a race condition when concurently testing multiple compiler / engines
wasitests::snapshot1::host_fs::fd_rename_path

# This tests are disabled for now
wasitests::unstable::host_fs::unix_open_special_files
wasitests::snapshot1::host_fs::unix_open_special_files
wasitests::unstable::mem_fs::unix_open_special_files
wasitests::snapshot1::mem_fs::unix_open_special_files

'''
'''--- tests/integration/README.md ---
# Wasmer Integration tests

All Wasmer end to end integration tests live here.
We have different kind of integration tests:

## CLI Integration tests

This tests check that the `wasmer` CLI works as it should when running it
as a Command in a shell, for each of the supported compilers.

## C Integration tests

This tests verify that Wasmer wasm-c-api tests are passing for each of the
supported compilers.

## Rust Integration tests

This tests verify that the `wasmer` API fulfill the required API that
external users use.

'''
'''--- tests/integration/cli/Cargo.toml ---
[package]
name = "wasmer-integration-tests-cli"
version = "2.1.0"
authors = ["Wasmer Engineering Team <engineering@wasmer.io>"]
description = "CLI integration tests"
repository = "https://github.com/wasmerio/wasmer"
edition = "2018"
publish = false

[dependencies]
anyhow = "1"
tempfile = "3"

'''
'''--- tests/integration/cli/src/assets.rs ---
use std::env;
use std::path::PathBuf;

pub const C_ASSET_PATH: &str = concat!(
    env!("CARGO_MANIFEST_DIR"),
    "/../../../lib/c-api/examples/assets"
);
pub const ASSET_PATH: &str = concat!(env!("CARGO_MANIFEST_DIR"), "/../../../tests/examples");

pub const WASMER_INCLUDE_PATH: &str = concat!(env!("CARGO_MANIFEST_DIR"), "/../../../lib/c-api");

#[cfg(feature = "debug")]
pub const WASMER_PATH: &str = concat!(env!("CARGO_MANIFEST_DIR"), "/../../../target/debug/wasmer");

#[cfg(not(feature = "debug"))]
pub const WASMER_PATH: &str = concat!(
    env!("CARGO_MANIFEST_DIR"),
    "/../../../target/release/wasmer"
);

#[cfg(not(windows))]
pub const LIBWASMER_PATH: &str = concat!(
    env!("CARGO_MANIFEST_DIR"),
    "/../../../target/release/libwasmer.a"
);
#[cfg(windows)]
pub const LIBWASMER_PATH: &str = concat!(
    env!("CARGO_MANIFEST_DIR"),
    "/../../../target/release/wasmer.lib"
);

/// Get the path to the `libwasmer.a` static library.
pub fn get_libwasmer_path() -> PathBuf {
    PathBuf::from(
        env::var("WASMER_TEST_LIBWASMER_PATH").unwrap_or_else(|_| LIBWASMER_PATH.to_string()),
    )
}

/// Get the path to the `wasmer` executable to be used in this test.
pub fn get_wasmer_path() -> PathBuf {
    PathBuf::from(env::var("WASMER_TEST_WASMER_PATH").unwrap_or_else(|_| WASMER_PATH.to_string()))
}

'''
'''--- tests/integration/cli/src/lib.rs ---
#![forbid(unsafe_code)]

//! CLI integration tests

pub mod assets;
pub mod link_code;
pub mod util;

pub use assets::*;
pub use util::*;

'''
'''--- tests/integration/cli/src/link_code.rs ---
use crate::assets::*;
use anyhow::bail;
use std::path::PathBuf;
use std::process::Command;

/// Data used to run a linking command for generated artifacts.
#[derive(Debug)]
pub struct LinkCode {
    /// The directory to operate in.
    pub current_dir: PathBuf,
    /// Path to the linker used to run the linking command.
    pub linker_path: PathBuf,
    /// String used as an optimization flag.
    pub optimization_flag: String,
    /// Paths of objects to link.
    pub object_paths: Vec<PathBuf>,
    /// Path to the output target.
    pub output_path: PathBuf,
    /// Path to the static libwasmer library.
    pub libwasmer_path: PathBuf,
}

impl Default for LinkCode {
    fn default() -> Self {
        #[cfg(not(windows))]
        let linker = "cc";
        #[cfg(windows)]
        let linker = "clang";
        Self {
            current_dir: std::env::current_dir().unwrap(),
            linker_path: PathBuf::from(linker),
            optimization_flag: String::from("-O2"),
            object_paths: vec![],
            output_path: PathBuf::from("a.out"),
            libwasmer_path: get_libwasmer_path(),
        }
    }
}

impl LinkCode {
    pub fn run(&self) -> anyhow::Result<()> {
        let mut command = Command::new(&self.linker_path);
        let command = command
            .current_dir(&self.current_dir)
            .arg(&self.optimization_flag)
            .args(
                self.object_paths
                    .iter()
                    .map(|path| path.canonicalize().unwrap()),
            )
            .arg(&self.libwasmer_path.canonicalize()?);
        #[cfg(windows)]
        let command = command
            .arg("-luserenv")
            .arg("-lWs2_32")
            .arg("-ladvapi32")
            .arg("-lbcrypt");
        #[cfg(not(windows))]
        let command = command.arg("-ldl").arg("-lm").arg("-pthread");
        let output = command.arg("-o").arg(&self.output_path).output()?;

        if !output.status.success() {
            bail!(
                "linking failed with: stdout: {}\n\nstderr: {}",
                std::str::from_utf8(&output.stdout)
                    .expect("stdout is not utf8! need to handle arbitrary bytes"),
                std::str::from_utf8(&output.stderr)
                    .expect("stderr is not utf8! need to handle arbitrary bytes")
            );
        }
        Ok(())
    }
}

'''
'''--- tests/integration/cli/src/util.rs ---
use anyhow::bail;
use std::path::Path;
use std::process::Command;

#[derive(Debug, Copy, Clone)]
pub enum Compiler {
    Cranelift,
    LLVM,
    Singlepass,
}

impl Compiler {
    pub const fn to_flag(self) -> &'static str {
        match self {
            Compiler::Cranelift => "--cranelift",
            Compiler::LLVM => "--llvm",
            Compiler::Singlepass => "--singlepass",
        }
    }
}

#[derive(Debug, Copy, Clone)]
pub enum Engine {
    Universal,
    Dylib,
    Staticlib,
}

impl Engine {
    pub const fn to_flag(self) -> &'static str {
        match self {
            Engine::Universal => "--universal",
            Engine::Dylib => "--dylib",
            Engine::Staticlib => "--staticlib",
        }
    }
}

pub fn run_code(
    operating_dir: &Path,
    executable_path: &Path,
    args: &[String],
) -> anyhow::Result<String> {
    let output = Command::new(executable_path.canonicalize()?)
        .current_dir(operating_dir)
        .args(args)
        .output()?;

    if !output.status.success() {
        bail!(
            "running executable failed: stdout: {}\n\nstderr: {}",
            std::str::from_utf8(&output.stdout)
                .expect("stdout is not utf8! need to handle arbitrary bytes"),
            std::str::from_utf8(&output.stderr)
                .expect("stderr is not utf8! need to handle arbitrary bytes")
        );
    }
    let output =
        std::str::from_utf8(&output.stdout).expect("output from running executable is not utf-8");

    Ok(output.to_owned())
}

'''
'''--- tests/integration/cli/tests/compile.rs ---
//! CLI tests for the compile subcommand.

use anyhow::{bail, Context};
use std::fs;
use std::io::Write;
use std::path::{Path, PathBuf};
use std::process::Command;
use wasmer_integration_tests_cli::link_code::*;
use wasmer_integration_tests_cli::*;

const STATICLIB_ENGINE_TEST_C_SOURCE: &[u8] = include_bytes!("staticlib_engine_test_c_source.c");

fn staticlib_engine_test_wasm_path() -> String {
    format!("{}/{}", C_ASSET_PATH, "qjs.wasm")
}

/// Data used to run the `wasmer compile` command.
#[derive(Debug)]
struct WasmerCompile {
    /// The directory to operate in.
    current_dir: PathBuf,
    /// Path to wasmer executable used to run the command.
    wasmer_path: PathBuf,
    /// Path to the Wasm file to compile.
    wasm_path: PathBuf,
    /// Path to the static object file produced by compiling the Wasm.
    wasm_object_path: PathBuf,
    /// Path to output the generated header to.
    header_output_path: PathBuf,
    /// Compiler with which to compile the Wasm.
    compiler: Compiler,
    /// Engine with which to use to generate the artifacts.
    engine: Engine,
}

impl Default for WasmerCompile {
    fn default() -> Self {
        #[cfg(not(windows))]
        let wasm_obj_path = "wasm.o";
        #[cfg(windows)]
        let wasm_obj_path = "wasm.obj";
        Self {
            current_dir: std::env::current_dir().unwrap(),
            wasmer_path: get_wasmer_path(),
            wasm_path: PathBuf::from(staticlib_engine_test_wasm_path()),
            wasm_object_path: PathBuf::from(wasm_obj_path),
            header_output_path: PathBuf::from("my_wasm.h"),
            compiler: Compiler::Cranelift,
            engine: Engine::Staticlib,
        }
    }
}

impl WasmerCompile {
    fn run(&self) -> anyhow::Result<()> {
        let output = Command::new(&self.wasmer_path)
            .current_dir(&self.current_dir)
            .arg("compile")
            .arg(&self.wasm_path.canonicalize()?)
            .arg(&self.compiler.to_flag())
            .arg(&self.engine.to_flag())
            .arg("-o")
            .arg(&self.wasm_object_path)
            .arg("--header")
            .arg(&self.header_output_path)
            .output()?;

        if !output.status.success() {
            bail!(
                "wasmer compile failed with: stdout: {}\n\nstderr: {}",
                std::str::from_utf8(&output.stdout)
                    .expect("stdout is not utf8! need to handle arbitrary bytes"),
                std::str::from_utf8(&output.stderr)
                    .expect("stderr is not utf8! need to handle arbitrary bytes")
            );
        }
        Ok(())
    }
}

/// Compile the C code.
fn run_c_compile(
    current_dir: &Path,
    path_to_c_src: &Path,
    output_name: &Path,
) -> anyhow::Result<()> {
    #[cfg(not(windows))]
    let c_compiler = "cc";
    #[cfg(windows)]
    let c_compiler = "clang++";

    let output = Command::new(c_compiler)
        .current_dir(current_dir)
        .arg("-O2")
        .arg("-c")
        .arg(path_to_c_src)
        .arg("-I")
        .arg(WASMER_INCLUDE_PATH)
        .arg("-o")
        .arg(output_name)
        .output()?;

    if !output.status.success() {
        bail!(
            "C code compile failed with: stdout: {}\n\nstderr: {}",
            std::str::from_utf8(&output.stdout)
                .expect("stdout is not utf8! need to handle arbitrary bytes"),
            std::str::from_utf8(&output.stderr)
                .expect("stderr is not utf8! need to handle arbitrary bytes")
        );
    }
    Ok(())
}

#[test]
fn staticlib_engine_works() -> anyhow::Result<()> {
    let temp_dir = tempfile::tempdir().context("Making a temp dir")?;
    let operating_dir: PathBuf = temp_dir.path().to_owned();

    let wasm_path = operating_dir.join(staticlib_engine_test_wasm_path());
    #[cfg(not(windows))]
    let wasm_object_path = operating_dir.join("wasm.o");
    #[cfg(windows)]
    let wasm_object_path = operating_dir.join("wasm.obj");
    let header_output_path = operating_dir.join("my_wasm.h");

    WasmerCompile {
        current_dir: operating_dir.clone(),
        wasm_path: wasm_path.clone(),
        wasm_object_path: wasm_object_path.clone(),
        header_output_path,
        compiler: Compiler::Cranelift,
        engine: Engine::Staticlib,
        ..Default::default()
    }
    .run()
    .context("Failed to compile wasm with Wasmer")?;

    let c_src_file_name = operating_dir.join("c_src.c");
    #[cfg(not(windows))]
    let c_object_path = operating_dir.join("c_src.o");
    #[cfg(windows)]
    let c_object_path = operating_dir.join("c_src.obj");
    let executable_path = operating_dir.join("a.out");

    // TODO: adjust C source code based on locations of things
    {
        let mut c_src_file = fs::OpenOptions::new()
            .create_new(true)
            .write(true)
            .open(&c_src_file_name)
            .context("Failed to open C source code file")?;
        c_src_file.write_all(STATICLIB_ENGINE_TEST_C_SOURCE)?;
    }
    run_c_compile(&operating_dir, &c_src_file_name, &c_object_path)
        .context("Failed to compile C source code")?;
    LinkCode {
        current_dir: operating_dir.clone(),
        object_paths: vec![c_object_path, wasm_object_path],
        output_path: executable_path.clone(),
        ..Default::default()
    }
    .run()
    .context("Failed to link objects together")?;

    let result = run_code(&operating_dir, &executable_path, &[])
        .context("Failed to run generated executable")?;
    let result_lines = result.lines().collect::<Vec<&str>>();
    assert_eq!(result_lines, vec!["Initializing...", "\"Hello, World\""],);

    Ok(())
}

'''
'''--- tests/integration/cli/tests/create_exe.rs ---
//! Tests of the `wasmer create-exe` command.

use anyhow::{bail, Context};
use std::fs;
use std::io::prelude::*;
use std::path::PathBuf;
use std::process::Command;
use wasmer_integration_tests_cli::*;

fn create_exe_test_wasm_path() -> String {
    format!("{}/{}", C_ASSET_PATH, "qjs.wasm")
}
const JS_TEST_SRC_CODE: &[u8] =
    b"function greet(name) { return JSON.stringify('Hello, ' + name); }; print(greet('World'));\n";

/// Data used to run the `wasmer compile` command.
#[derive(Debug)]
struct WasmerCreateExe {
    /// The directory to operate in.
    current_dir: PathBuf,
    /// Path to wasmer executable used to run the command.
    wasmer_path: PathBuf,
    /// Path to the Wasm file to compile.
    wasm_path: PathBuf,
    /// Path to the native executable produced by compiling the Wasm.
    native_executable_path: PathBuf,
    /// Compiler with which to compile the Wasm.
    compiler: Compiler,
}

impl Default for WasmerCreateExe {
    fn default() -> Self {
        #[cfg(not(windows))]
        let native_executable_path = PathBuf::from("wasm.out");
        #[cfg(windows)]
        let native_executable_path = PathBuf::from("wasm.exe");
        Self {
            current_dir: std::env::current_dir().unwrap(),
            wasmer_path: get_wasmer_path(),
            wasm_path: PathBuf::from(create_exe_test_wasm_path()),
            native_executable_path,
            compiler: Compiler::Cranelift,
        }
    }
}

impl WasmerCreateExe {
    fn run(&self) -> anyhow::Result<()> {
        let output = Command::new(&self.wasmer_path)
            .current_dir(&self.current_dir)
            .arg("create-exe")
            .arg(&self.wasm_path.canonicalize()?)
            .arg(&self.compiler.to_flag())
            .arg("-o")
            .arg(&self.native_executable_path)
            .output()?;

        if !output.status.success() {
            bail!(
                "wasmer create-exe failed with: stdout: {}\n\nstderr: {}",
                std::str::from_utf8(&output.stdout)
                    .expect("stdout is not utf8! need to handle arbitrary bytes"),
                std::str::from_utf8(&output.stderr)
                    .expect("stderr is not utf8! need to handle arbitrary bytes")
            );
        }
        Ok(())
    }
}

#[test]
fn create_exe_works() -> anyhow::Result<()> {
    let temp_dir = tempfile::tempdir()?;
    let operating_dir: PathBuf = temp_dir.path().to_owned();

    let wasm_path = operating_dir.join(create_exe_test_wasm_path());
    #[cfg(not(windows))]
    let executable_path = operating_dir.join("wasm.out");
    #[cfg(windows)]
    let executable_path = operating_dir.join("wasm.exe");

    WasmerCreateExe {
        current_dir: operating_dir.clone(),
        wasm_path: wasm_path.clone(),
        native_executable_path: executable_path.clone(),
        compiler: Compiler::Cranelift,
        ..Default::default()
    }
    .run()
    .context("Failed to create-exe wasm with Wasmer")?;

    let result = run_code(
        &operating_dir,
        &executable_path,
        &["--eval".to_string(), "function greet(name) { return JSON.stringify('Hello, ' + name); }; print(greet('World'));".to_string()],
    )
    .context("Failed to run generated executable")?;
    let result_lines = result.lines().collect::<Vec<&str>>();
    assert_eq!(result_lines, vec!["\"Hello, World\""],);

    Ok(())
}

#[test]
fn create_exe_works_with_file() -> anyhow::Result<()> {
    let temp_dir = tempfile::tempdir()?;
    let operating_dir: PathBuf = temp_dir.path().to_owned();

    let wasm_path = operating_dir.join(create_exe_test_wasm_path());
    #[cfg(not(windows))]
    let executable_path = operating_dir.join("wasm.out");
    #[cfg(windows)]
    let executable_path = operating_dir.join("wasm.exe");

    WasmerCreateExe {
        current_dir: operating_dir.clone(),
        wasm_path: wasm_path.clone(),
        native_executable_path: executable_path.clone(),
        compiler: Compiler::Cranelift,
        ..Default::default()
    }
    .run()
    .context("Failed to create-exe wasm with Wasmer")?;

    {
        let mut f = fs::OpenOptions::new()
            .write(true)
            .create_new(true)
            .open(operating_dir.join("test.js"))?;
        f.write_all(JS_TEST_SRC_CODE)?;
    }

    // test with `--dir`
    let result = run_code(
        &operating_dir,
        &executable_path,
        &[
            "--dir=.".to_string(),
            "--script".to_string(),
            "test.js".to_string(),
        ],
    )
    .context("Failed to run generated executable")?;
    let result_lines = result.lines().collect::<Vec<&str>>();
    assert_eq!(result_lines, vec!["\"Hello, World\""],);

    // test with `--mapdir`
    let result = run_code(
        &operating_dir,
        &executable_path,
        &[
            "--mapdir=abc:.".to_string(),
            "--script".to_string(),
            "abc/test.js".to_string(),
        ],
    )
    .context("Failed to run generated executable")?;
    let result_lines = result.lines().collect::<Vec<&str>>();
    assert_eq!(result_lines, vec!["\"Hello, World\""],);

    Ok(())
}

'''
'''--- tests/integration/cli/tests/run.rs ---
//! Basic tests for the `run` subcommand

use anyhow::bail;
use std::process::Command;
use wasmer_integration_tests_cli::{ASSET_PATH, C_ASSET_PATH, WASMER_PATH};

fn wasi_test_wasm_path() -> String {
    format!("{}/{}", C_ASSET_PATH, "qjs.wasm")
}

fn test_no_imports_wat_path() -> String {
    format!("{}/{}", ASSET_PATH, "fib.wat")
}

fn test_no_start_wat_path() -> String {
    format!("{}/{}", ASSET_PATH, "no_start.wat")
}

#[test]
fn run_wasi_works() -> anyhow::Result<()> {
    let output = Command::new(WASMER_PATH)
        .arg("run")
        .arg(wasi_test_wasm_path())
        .arg("--")
        .arg("-e")
        .arg("print(3 * (4 + 5))")
        .output()?;

    if !output.status.success() {
        bail!(
            "linking failed with: stdout: {}\n\nstderr: {}",
            std::str::from_utf8(&output.stdout)
                .expect("stdout is not utf8! need to handle arbitrary bytes"),
            std::str::from_utf8(&output.stderr)
                .expect("stderr is not utf8! need to handle arbitrary bytes")
        );
    }

    let stdout_output = std::str::from_utf8(&output.stdout).unwrap();
    assert_eq!(stdout_output, "27\n");

    Ok(())
}

#[test]

fn run_no_imports_wasm_works() -> anyhow::Result<()> {
    let output = Command::new(WASMER_PATH)
        .arg("run")
        .arg(test_no_imports_wat_path())
        .output()?;

    if !output.status.success() {
        bail!(
            "linking failed with: stdout: {}\n\nstderr: {}",
            std::str::from_utf8(&output.stdout)
                .expect("stdout is not utf8! need to handle arbitrary bytes"),
            std::str::from_utf8(&output.stderr)
                .expect("stderr is not utf8! need to handle arbitrary bytes")
        );
    }

    Ok(())
}

#[test]
fn run_no_start_wasm_report_error() -> anyhow::Result<()> {
    let output = Command::new(WASMER_PATH)
        .arg("run")
        .arg(test_no_start_wat_path())
        .output()?;

    assert_eq!(output.status.success(), false);
    let result = std::str::from_utf8(&output.stderr).unwrap().to_string();
    assert_eq!(result.contains("Can not find any export functions."), true);
    Ok(())
}

'''
'''--- tests/integration/cli/tests/staticlib_engine_test_c_source.c ---
#include "wasmer.h"
#include "my_wasm.h"

#include <stdio.h>
#include <stdlib.h>

#define own

static void print_wasmer_error() {
  int error_len = wasmer_last_error_length();
  printf("Error len: `%d`\n", error_len);
  char *error_str = (char *)malloc(error_len);
  wasmer_last_error_message(error_str, error_len);
  printf("Error str: `%s`\n", error_str);
  free(error_str);
}

int main() {
  printf("Initializing...\n");
  wasm_config_t *config = wasm_config_new();
  wasm_config_set_engine(config, STATICLIB);
  wasm_engine_t *engine = wasm_engine_new_with_config(config);
  wasm_store_t *store = wasm_store_new(engine);

  wasm_module_t *module = wasmer_staticlib_engine_new(store, "qjs.wasm");

  if (!module) {
    printf("Failed to create module\n");
    print_wasmer_error();
    return -1;
  }

  // We have now finished the memory buffer book keeping and we have a valid
  // Module.

  // In this example we're passing some JavaScript source code as a command line
  // argument to a WASI module that can evaluate JavaScript.
  wasi_config_t *wasi_config = wasi_config_new("constant_value_here");
  const char *js_string =
      "function greet(name) { return JSON.stringify('Hello, ' + name); }; "
      "print(greet('World'));";
  wasi_config_arg(wasi_config, "--eval");
  wasi_config_arg(wasi_config, js_string);
  wasi_env_t *wasi_env = wasi_env_new(wasi_config);

  if (!wasi_env) {
    printf("> Error building WASI env!\n");
    print_wasmer_error();
    return 1;
  }

  wasm_importtype_vec_t import_types;
  wasm_module_imports(module, &import_types);

  wasm_extern_vec_t imports;
  wasm_extern_vec_new_uninitialized(&imports, import_types.size);
  wasm_importtype_vec_delete(&import_types);

  bool get_imports_result = wasi_get_imports(store, module, wasi_env, &imports);
  wasi_env_delete(wasi_env);

  if (!get_imports_result) {
    printf("> Error getting WASI imports!\n");
    print_wasmer_error();
    return 1;
  }

  wasm_instance_t *instance = wasm_instance_new(store, module, &imports, NULL);

  if (!instance) {
    printf("Failed to create instance\n");
    print_wasmer_error();
    return -1;
  }

  // WASI is now set up.
  own wasm_func_t *start_function = wasi_get_start_function(instance);
  if (!start_function) {
    fprintf(stderr, "`_start` function not found\n");
    print_wasmer_error();
    return -1;
  }

  fflush(stdout);

  wasm_val_vec_t args = WASM_EMPTY_VEC;
  wasm_val_vec_t results = WASM_EMPTY_VEC;
  own wasm_trap_t *trap = wasm_func_call(start_function, &args, &results);
  if (trap) {
    fprintf(stderr, "Trap is not NULL: TODO:\n");
    return -1;
  }

  wasm_instance_delete(instance);
  wasm_module_delete(module);
  wasm_store_delete(store);
  wasm_engine_delete(engine);

  return 0;
}

'''
'''--- tests/integration/cli/tests/version.rs ---
use anyhow::bail;
use std::process::Command;
use wasmer_integration_tests_cli::WASMER_PATH;

const WASMER_VERSION: &str = env!("CARGO_PKG_VERSION");

#[test]
fn version_string_is_correct() -> anyhow::Result<()> {
    let expected_version_output = format!("wasmer {}\n", WASMER_VERSION);

    let outputs = [
        Command::new(WASMER_PATH).arg("--version").output()?,
        Command::new(WASMER_PATH).arg("-V").output()?,
    ];

    for output in &outputs {
        if !output.status.success() {
            bail!(
                "version failed with: stdout: {}\n\nstderr: {}",
                std::str::from_utf8(&output.stdout)
                    .expect("stdout is not utf8! need to handle arbitrary bytes"),
                std::str::from_utf8(&output.stderr)
                    .expect("stderr is not utf8! need to handle arbitrary bytes")
            );
        }

        let stdout_output = std::str::from_utf8(&output.stdout).unwrap();
        assert_eq!(stdout_output, &expected_version_output);
    }

    Ok(())
}

#[test]
fn help_text_contains_version() -> anyhow::Result<()> {
    let expected_version_output = format!("wasmer {}", WASMER_VERSION);

    let outputs = [
        Command::new(WASMER_PATH).arg("--help").output()?,
        Command::new(WASMER_PATH).arg("-h").output()?,
    ];

    for output in &outputs {
        if !output.status.success() {
            bail!(
                "version failed with: stdout: {}\n\nstderr: {}",
                std::str::from_utf8(&output.stdout)
                    .expect("stdout is not utf8! need to handle arbitrary bytes"),
                std::str::from_utf8(&output.stderr)
                    .expect("stderr is not utf8! need to handle arbitrary bytes")
            );
        }

        let stdout_output = std::str::from_utf8(&output.stdout).unwrap();
        assert_eq!(
            stdout_output.lines().next().unwrap(),
            &expected_version_output
        );
    }

    Ok(())
}

'''
'''--- tests/integration/ios/Cargo.toml ---
[package]
name = "wasmer-integration-tests-ios"
version = "2.1.0"
authors = ["Wasmer Engineering Team <engineering@wasmer.io>"]
description = "iOS integration tests"
repository = "https://github.com/wasmerio/wasmer"
edition = "2018"
publish = false

'''
'''--- tests/integration/ios/DylibExample/DylibExample/AppDelegate.swift ---
//
//  AppDelegate.swift
//  DylibExample
//
//  Created by Nathan Horrigan on 15/08/2021.
//

import UIKit

@main
class AppDelegate: UIResponder, UIApplicationDelegate {

    func application(_ application: UIApplication, didFinishLaunchingWithOptions launchOptions: [UIApplication.LaunchOptionsKey: Any]?) -> Bool {
        // Override point for customization after application launch.
        return true
    }

    // MARK: UISceneSession Lifecycle

    func application(_ application: UIApplication, configurationForConnecting connectingSceneSession: UISceneSession, options: UIScene.ConnectionOptions) -> UISceneConfiguration {
        // Called when a new scene session is being created.
        // Use this method to select a configuration to create the new scene with.
        return UISceneConfiguration(name: "Default Configuration", sessionRole: connectingSceneSession.role)
    }

    func application(_ application: UIApplication, didDiscardSceneSessions sceneSessions: Set<UISceneSession>) {
        // Called when the user discards a scene session.
        // If any sessions were discarded while the application was not running, this will be called shortly after application:didFinishLaunchingWithOptions.
        // Use this method to release any resources that were specific to the discarded scenes, as they will not return.
    }

}

'''
'''--- tests/integration/ios/DylibExample/DylibExample/Assets.xcassets/AccentColor.colorset/Contents.json ---
{
  "colors" : [
    {
      "idiom" : "universal"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}

'''
'''--- tests/integration/ios/DylibExample/DylibExample/Assets.xcassets/AppIcon.appiconset/Contents.json ---
{
  "images" : [
    {
      "idiom" : "iphone",
      "scale" : "2x",
      "size" : "20x20"
    },
    {
      "idiom" : "iphone",
      "scale" : "3x",
      "size" : "20x20"
    },
    {
      "idiom" : "iphone",
      "scale" : "2x",
      "size" : "29x29"
    },
    {
      "idiom" : "iphone",
      "scale" : "3x",
      "size" : "29x29"
    },
    {
      "idiom" : "iphone",
      "scale" : "2x",
      "size" : "40x40"
    },
    {
      "idiom" : "iphone",
      "scale" : "3x",
      "size" : "40x40"
    },
    {
      "idiom" : "iphone",
      "scale" : "2x",
      "size" : "60x60"
    },
    {
      "idiom" : "iphone",
      "scale" : "3x",
      "size" : "60x60"
    },
    {
      "idiom" : "ipad",
      "scale" : "1x",
      "size" : "20x20"
    },
    {
      "idiom" : "ipad",
      "scale" : "2x",
      "size" : "20x20"
    },
    {
      "idiom" : "ipad",
      "scale" : "1x",
      "size" : "29x29"
    },
    {
      "idiom" : "ipad",
      "scale" : "2x",
      "size" : "29x29"
    },
    {
      "idiom" : "ipad",
      "scale" : "1x",
      "size" : "40x40"
    },
    {
      "idiom" : "ipad",
      "scale" : "2x",
      "size" : "40x40"
    },
    {
      "idiom" : "ipad",
      "scale" : "1x",
      "size" : "76x76"
    },
    {
      "idiom" : "ipad",
      "scale" : "2x",
      "size" : "76x76"
    },
    {
      "idiom" : "ipad",
      "scale" : "2x",
      "size" : "83.5x83.5"
    },
    {
      "idiom" : "ios-marketing",
      "scale" : "1x",
      "size" : "1024x1024"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}

'''
'''--- tests/integration/ios/DylibExample/DylibExample/Assets.xcassets/Contents.json ---
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}

'''
'''--- tests/integration/ios/DylibExample/DylibExample/DylibExample-Bridging-Header.h ---
#include "wasm.h"
#include "wasmer.h"
#include "wasmer_wasm.h"
#include "calc.h"

'''
'''--- tests/integration/ios/DylibExample/DylibExample/SceneDelegate.swift ---
//
//  SceneDelegate.swift
//  DylibExample
//
//  Created by Nathan Horrigan on 15/08/2021.
//

import UIKit

class SceneDelegate: UIResponder, UIWindowSceneDelegate {

    var window: UIWindow?

    func scene(_ scene: UIScene, willConnectTo session: UISceneSession, options connectionOptions: UIScene.ConnectionOptions) {
        // Use this method to optionally configure and attach the UIWindow `window` to the provided UIWindowScene `scene`.
        // If using a storyboard, the `window` property will automatically be initialized and attached to the scene.
        // This delegate does not imply the connecting scene or session are new (see `application:configurationForConnectingSceneSession` instead).
        guard let _ = (scene as? UIWindowScene) else { return }
    }

    func sceneDidDisconnect(_ scene: UIScene) {
        // Called as the scene is being released by the system.
        // This occurs shortly after the scene enters the background, or when its session is discarded.
        // Release any resources associated with this scene that can be re-created the next time the scene connects.
        // The scene may re-connect later, as its session was not necessarily discarded (see `application:didDiscardSceneSessions` instead).
    }

    func sceneDidBecomeActive(_ scene: UIScene) {
        // Called when the scene has moved from an inactive state to an active state.
        // Use this method to restart any tasks that were paused (or not yet started) when the scene was inactive.
    }

    func sceneWillResignActive(_ scene: UIScene) {
        // Called when the scene will move from an active state to an inactive state.
        // This may occur due to temporary interruptions (ex. an incoming phone call).
    }

    func sceneWillEnterForeground(_ scene: UIScene) {
        // Called as the scene transitions from the background to the foreground.
        // Use this method to undo the changes made on entering the background.
    }

    func sceneDidEnterBackground(_ scene: UIScene) {
        // Called as the scene transitions from the foreground to the background.
        // Use this method to save data, release shared resources, and store enough scene-specific state information
        // to restore the scene back to its current state.
    }

}

'''
'''--- tests/integration/ios/DylibExample/DylibExample/ViewController.swift ---
//
//  ViewController.swift
//  DylibExample
//
//  Created by Nathan Horrigan on 15/08/2021.
//

import UIKit

class ViewController: UIViewController {
    @IBOutlet weak var label: UILabel!
    
    override func viewDidLoad() {
        super.viewDidLoad()
        let sum = calculate_sum(1, 3)
        label.text = "The sum of 1 + 3 = \(sum)"
    }
}

'''
'''--- tests/integration/ios/DylibExample/DylibExample/calc.cpp ---
//
//  WASM.cpp
//  DylibExample
//
//  Created by Nathan Horrigan on 17/08/2021.
//

#include "calc.h"

#include <any>
#include <cstdio>
#include <cstring>
#include <fstream>
#include <iostream>
#include <sstream>
#include <vector>
#include <filesystem>
#include <CoreFoundation/CFBundle.h>

std::string get_resources_dir()
{

  CFURLRef resourceURL = CFBundleCopyResourcesDirectoryURL(CFBundleGetMainBundle());
  char resourcePath[PATH_MAX];
  if (CFURLGetFileSystemRepresentation(resourceURL, true,
                                       (UInt8 *)resourcePath,
                                       PATH_MAX))
  {
    if (resourceURL != NULL)
    {
      CFRelease(resourceURL);
    }

    return resourcePath;
  }

  return "";
}

inline std::vector<uint8_t> read_vector_from_disk(std::string file_path)
{
  std::ifstream instream(file_path, std::ios::in | std::ios::binary);
  std::vector<uint8_t> data((std::istreambuf_iterator<char>(instream)), std::istreambuf_iterator<char>());
  return data;
}

int calculate_sum(int a, int b)
{
  printf("Creating the store...\n");
  wasm_engine_t *engine = wasm_engine_new();
  wasm_store_t *store = wasm_store_new(engine);

  printf("Loading .dylib file...\n");
  std::string wasm_path = get_resources_dir() + "/sum.dylib";
  std::vector<uint8_t> dylib = read_vector_from_disk(wasm_path.c_str());
  uint8_t *wasm_bytes = dylib.data();

  wasm_byte_vec_t imported_bytes;
  imported_bytes.size = dylib.size();
  imported_bytes.data = (wasm_byte_t *)wasm_bytes;

  printf("Compiling module...\n");
  wasm_module_t *module;
  module = wasm_module_deserialize(store, &imported_bytes);

  if (!module)
  {
    printf("> Error compiling module!\n");

    return 1;
  }

  printf("Creating imports...\n");
  wasm_extern_vec_t import_object = WASM_EMPTY_VEC;

  printf("Instantiating module...\n");
  wasm_instance_t *instance = wasm_instance_new(store, module, &import_object, NULL);

  if (!instance)
  {
    printf("> Error instantiating module!\n");

    return 1;
  }

  printf("Retrieving exports...\n");
  wasm_extern_vec_t exports;
  wasm_instance_exports(instance, &exports);

  if (exports.size == 0)
  {
    printf("> Error accessing exports!\n");

    return 1;
  }

  printf("Retrieving the `sum` function...\n");
  wasm_func_t *sum_func = wasm_extern_as_func(exports.data[0]);

  if (sum_func == NULL)
  {
    printf("> Failed to get the `sum` function!\n");

    return 1;
  }

  printf("Calling `sum` function...\n");
  wasm_val_t args_val[2] = {WASM_I32_VAL(a), WASM_I32_VAL(b)};
  wasm_val_t results_val[1] = {WASM_INIT_VAL};
  wasm_val_vec_t args = WASM_ARRAY_VEC(args_val);
  wasm_val_vec_t results = WASM_ARRAY_VEC(results_val);

  if (wasm_func_call(sum_func, &args, &results))
  {
    printf("> Error calling the `sum` function!\n");

    return 1;
  }

  printf("Results of `sum`: %d\n", results_val[0].of.i32);

  return results_val[0].of.i32;
}

'''
'''--- tests/integration/ios/DylibExample/DylibExample/calc.h ---
//
//  WASM.hpp
//  DylibExample
//
//  Created by Nathan Horrigan on 17/08/2021.
//

#ifndef calc_h
#define calc_h
#include "wasm.h"
#include <stdio.h>

#ifdef __cplusplus
extern "C"
{
#endif

    int calculate_sum(int a, int b);

#ifdef __cplusplus
}
#endif

#endif /* calc_h */

'''
'''--- tests/integration/ios/DylibExample/DylibExampleTests/DylibExampleTests.swift ---
//
//  DylibExampleTests.swift
//  DylibExampleTests
//
//  Created by Nathan Horrigan on 15/08/2021.
//

import XCTest
@testable import DylibExample

class DylibExampleTests: XCTestCase {

    override func setUpWithError() throws {
        // Put setup code here. This method is called before the invocation of each test method in the class.
    }

    override func tearDownWithError() throws {
        // Put teardown code here. This method is called after the invocation of each test method in the class.
    }

    func testExample() throws {
        let sum = calculate_sum(5, 2)
        assert(sum == 7, "WASM loaded successfully")
    }

    func testPerformanceExample() throws {
        // This is an example of a performance test case.
        self.measure {
            // Put the code you want to measure the time of here.
        }
    }

}

'''
'''--- tests/integration/ios/tests/dylib.rs ---
#[cfg(test)]
#[cfg(target_os = "macos")]
mod tests {
    use std::process::{Command, Output, Stdio};

    #[test]
    fn test_runtime() {
        // Remove anuthing left over from tests
        remove_existing_artificats();

        // Tets the 'DylibExample' scheme
        let success = run_ios_test("DylibExample/DylibExample.xcodeproj", "DylibExample");
        if !success {
            panic!("Dylib iOS Tests failed with the above output!");
        }
    }

    fn run_ios_test(dir: &str, scheme: &str) -> bool {
        let command = Command::new("xcodebuild")
            .arg("test")
            .arg("-project")
            .arg(dir)
            .arg("-scheme")
            .arg(scheme)
            .arg("-destination")
            .arg("platform=iOS Simulator,name=iPhone 12 Pro")
            .arg("CODE_SIGNING_ALLOWED=NO")
            .stdout(Stdio::inherit())
            .stderr(Stdio::inherit())
            .output()
            .expect("Could not run iOS Test");

        // Get output from xcodebuild CLI:
        let stderr = String::from_utf8(command.stderr).unwrap();

        /*
            An iOS Test Result is quite odd, we check stderr for the phrase 'TEST FAILED'
            and then return stdout which contains the failure reason;
            We also check that the command executed correctly!
        */
        let command_success = command.status.success();
        let test_success = stderr.contains("** TEST FAILED **") == false;
        let success = command_success && test_success;

        return success;
    }

    fn remove_existing_artificats() -> Output {
        Command::new("rm")
            .arg("-f")
            .arg("DylibExample/DylibExample/sum.dylib")
            .output()
            .expect("Could not clear artificats")
    }
}

'''
'''--- tests/lib/README.md ---
# Wasmer Test libraries

Here is the place where the test libraries will live.

'''
'''--- tests/lib/compiler-test-derive/Cargo.toml ---
[lib]
proc-macro = true

[package]
name = "compiler-test-derive"
version = "0.0.1"
authors = ["Wasmer Engineering <engineering@wasmer.io>"]
edition = "2018"
license = "MIT"

description = "A macro to generate easily tests across compilers and engines"
keywords = ["unsafe", "body", "fn", "safety", "hygiene"]
categories = ["rust-patterns", ]

[dependencies]
proc-macro2 = "1.*"
quote = "1.*"
syn =  { version = "1.*", features = ["full"] }

[features]

[dev-dependencies]
pretty_assertions = "1.0"
trybuild = "1.0.11"

'''
'''--- tests/lib/compiler-test-derive/build.rs ---
fn main() {
    println!("cargo:rerun-if-changed=../../ignores.txt");
    if let Ok(os) = std::env::var("CARGO_CFG_TARGET_OS") {
        println!("cargo:rustc-env=CFG_TARGET_OS={}", os);
    }
    if let Ok(os) = std::env::var("CARGO_CFG_TARGET_ARCH") {
        println!("cargo:rustc-env=CFG_TARGET_ARCH={}", os);
    }
    if let Ok(os) = std::env::var("CARGO_CFG_TARGET_ENV") {
        println!("cargo:rustc-env=CFG_TARGET_ENV={}", os);
    }
}

'''
'''--- tests/lib/compiler-test-derive/src/ignores.rs ---
use std::fs::File;
use std::path::PathBuf;

use std::io::{BufRead, BufReader};

pub const CFG_TARGET_OS: &'static str = env!("CFG_TARGET_OS");
pub const CFG_TARGET_ARCH: &'static str = env!("CFG_TARGET_ARCH");
pub const CFG_TARGET_ENV: &'static str = env!("CFG_TARGET_ENV");

#[derive(Debug, Clone)]
struct IgnorePattern {
    os: Option<String>,
    arch: Option<String>,
    target_env: Option<String>,
    engine: Option<String>,
    compiler: Option<String>,
    pattern_to_ignore: String,
}

impl IgnorePattern {
    fn should_ignore(
        &self,
        os: &str,
        arch: &str,
        target_env: &str,
        engine: &str,
        compiler: &str,
        canonical_path: &str,
    ) -> bool {
        self.os.as_ref().map_or(true, |val| val == os)
            && self.arch.as_ref().map_or(true, |val| val == arch)
            && self
                .target_env
                .as_ref()
                .map_or(true, |val| val == target_env)
            && self.engine.as_ref().map_or(true, |val| val == engine)
            && self.compiler.as_ref().map_or(true, |val| val == compiler)
            && (self.pattern_to_ignore == "*" || canonical_path.contains(&*self.pattern_to_ignore))
    }
}

#[derive(Debug, Clone)]
pub struct Ignores {
    /// The canonical path, and the set of features
    patterns: Vec<IgnorePattern>,
}

impl Ignores {
    /// If the path matches any of the paths on the list
    pub fn should_ignore(
        &self,
        os: &str,
        arch: &str,
        target_env: &str,
        engine: &str,
        compiler: &str,
        canonical_path: &str,
    ) -> bool {
        self.patterns.iter().any(|p| {
            // println!(" -> {:?}", p);
            p.should_ignore(os, arch, target_env, engine, compiler, canonical_path)
        })
    }

    pub fn should_ignore_host(&self, engine: &str, compiler: &str, canonical_path: &str) -> bool {
        self.should_ignore(
            CFG_TARGET_OS,
            CFG_TARGET_ARCH,
            CFG_TARGET_ENV,
            engine,
            compiler,
            canonical_path,
        )
    }

    /// Build a Ignore structure from a file path
    pub fn build_from_path(path: PathBuf) -> Ignores {
        let file = File::open(path).unwrap();
        let reader = BufReader::new(file);
        let mut patterns = Vec::new();

        for (i, line) in reader.lines().enumerate() {
            let line = line.unwrap();
            // If the line has a `#` we discard all the content that comes after
            let line = if line.contains('#') {
                let l: Vec<&str> = line.splitn(2, '#').collect();
                l[0].to_string()
            } else {
                line
            };

            let line = line.trim().to_string();

            // If the lines contains ` ` it means the test should be ignored
            // on the features exposed
            if line.contains(" ") {
                let l: Vec<&str> = line.splitn(2, " ").collect();
                let mut os: Option<String> = None;
                let mut arch: Option<String> = None;
                let mut target_env: Option<String> = None;
                let mut engine: Option<String> = None;
                let mut compiler: Option<String> = None;
                for alias in l[0].trim().split("+") {
                    match alias {
                        // Operating Systems
                        "windows" | "macos" | "linux" => {
                            os = Some(alias.to_string());
                        }
                        // Environments
                        "musl" => {
                            target_env = Some(alias.to_string());
                        }
                        // Chipset architectures
                        "aarch64" | "x86" | "x64" => {
                            arch = Some(alias.to_string());
                        }
                        // Engines
                        "universal" | "dylib" => {
                            engine = Some(alias.to_string());
                        }
                        // Compilers
                        "cranelift" | "llvm" | "singlepass" => {
                            compiler = Some(alias.to_string());
                        }
                        other => {
                            panic!("Alias {:?} not currently supported (defined in ignores.txt in line {})", other, i+1);
                        }
                    }
                }
                let pattern_to_ignore = l[1].trim().to_string();
                patterns.push(IgnorePattern {
                    os,
                    arch,
                    target_env,
                    engine,
                    compiler,
                    pattern_to_ignore,
                });
            } else {
                if line.is_empty() {
                    continue;
                }
                patterns.push(IgnorePattern {
                    os: None,
                    arch: None,
                    target_env: None,
                    engine: None,
                    compiler: None,
                    pattern_to_ignore: line,
                });
            };
        }
        Ignores { patterns }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn features_match() -> Result<(), ()> {
        assert!(IgnorePattern {
            os: None,
            arch: None,
            target_env: None,
            engine: None,
            compiler: None,
            pattern_to_ignore: "*".to_string()
        }
        .should_ignore(
            "unknown",
            "unknown",
            "",
            "engine",
            "compiler",
            "some::random::text"
        ));
        assert!(IgnorePattern {
            os: None,
            arch: None,
            target_env: None,
            engine: None,
            compiler: None,
            pattern_to_ignore: "some::random".to_string()
        }
        .should_ignore(
            "unknown",
            "unknown",
            "",
            "engine",
            "compiler",
            "some::random::text"
        ));
        assert!(!IgnorePattern {
            os: Some("macos".to_string()),
            arch: None,
            target_env: None,
            engine: None,
            compiler: None,
            pattern_to_ignore: "other".to_string()
        }
        .should_ignore(
            "unknown",
            "unknown",
            "",
            "engine",
            "compiler",
            "some::random::text"
        ));
        assert!(!IgnorePattern {
            os: Some("macos".to_string()),
            arch: None,
            target_env: None,
            engine: Some("universal".to_string()),
            compiler: None,
            pattern_to_ignore: "other".to_string()
        }
        .should_ignore(
            "macos",
            "unknown",
            "",
            "universal",
            "compiler",
            "some::random::text"
        ));
        Ok(())
    }
}

'''
'''--- tests/lib/compiler-test-derive/src/lib.rs ---
#[cfg(not(test))]
extern crate proc_macro;
#[cfg(not(test))]
use proc_macro::TokenStream;
#[cfg(test)]
use proc_macro2::TokenStream;
use quote::quote;
use std::path::PathBuf;
#[cfg(not(test))]
use syn::parse;
#[cfg(test)]
use syn::parse2 as parse;
use syn::*;

mod ignores;

// Reimplement parse_macro_input to use the imported `parse`
// function. This way parse_macro_input will parse a TokenStream2 when
// unit-testing.
macro_rules! parse_macro_input {
    (
        $token_stream:ident as $T:ty
    ) => {
        match parse::<$T>($token_stream) {
            Ok(data) => data,
            Err(err) => {
                return TokenStream::from(err.to_compile_error());
            }
        }
    };

    (
        $token_stream:ident
    ) => {
        parse_macro_input!($token_stream as _)
    };
}

#[proc_macro_attribute]
pub fn compiler_test(attrs: TokenStream, input: TokenStream) -> TokenStream {
    let path: Option<ExprPath> = parse::<ExprPath>(attrs).ok();
    let mut my_fn: ItemFn = parse_macro_input!(input as ItemFn);
    let fn_name = my_fn.sig.ident.clone();

    // Let's build the ignores to append an `#[ignore]` macro to the
    // autogenerated tests in case the test appears in the `ignores.txt` path;

    let mut ignores_txt_path = PathBuf::new();
    ignores_txt_path.push(env!("CARGO_MANIFEST_DIR"));
    ignores_txt_path.push("../../ignores.txt");

    let ignores = crate::ignores::Ignores::build_from_path(ignores_txt_path);

    let should_ignore = |test_name: &str, compiler_name: &str, engine_name: &str| {
        let compiler_name = compiler_name.to_lowercase();
        let engine_name = engine_name.to_lowercase();
        // We construct the path manually because we can't get the
        // source_file location from the `Span` (it's only available in nightly)
        let full_path = format!(
            "{}::{}::{}::{}",
            quote! { #path },
            test_name,
            compiler_name,
            engine_name
        )
        .replace(" ", "");
        let should_ignore = ignores.should_ignore_host(&engine_name, &compiler_name, &full_path);
        // println!("{} -> Should ignore: {}", full_path, should_ignore);
        return should_ignore;
    };
    let construct_engine_test = |func: &::syn::ItemFn,
                                 compiler_name: &str,
                                 engine_name: &str,
                                 engine_feature_name: &str|
     -> ::proc_macro2::TokenStream {
        let config_compiler = ::quote::format_ident!("{}", compiler_name);
        let config_engine = ::quote::format_ident!("{}", engine_name);
        let test_name = ::quote::format_ident!("{}", engine_name.to_lowercase());
        let mut new_sig = func.sig.clone();
        let attrs = func
            .attrs
            .clone()
            .iter()
            .fold(quote! {}, |acc, new| quote! {#acc #new});
        new_sig.ident = test_name;
        new_sig.inputs = ::syn::punctuated::Punctuated::new();
        let f = quote! {
            #[test_log::test]
            #attrs
            #[cfg(feature = #engine_feature_name)]
            #new_sig {
                #fn_name(crate::Config::new(crate::Engine::#config_engine, crate::Compiler::#config_compiler))
            }
        };
        if should_ignore(
            &func.sig.ident.to_string().replace("r#", ""),
            compiler_name,
            engine_name,
        ) && !cfg!(test)
        {
            quote! {
                #[ignore]
                #f
            }
        } else {
            f
        }
    };

    let construct_compiler_test =
        |func: &::syn::ItemFn, compiler_name: &str| -> ::proc_macro2::TokenStream {
            let mod_name = ::quote::format_ident!("{}", compiler_name.to_lowercase());
            let universal_engine_test =
                construct_engine_test(func, compiler_name, "Universal", "universal");
            let dylib_engine_test = construct_engine_test(func, compiler_name, "Dylib", "dylib");
            let compiler_name_lowercase = compiler_name.to_lowercase();

            quote! {
                #[cfg(feature = #compiler_name_lowercase)]
                mod #mod_name {
                    use super::*;

                    #universal_engine_test
                    #dylib_engine_test
                }
            }
        };

    let singlepass_compiler_test = construct_compiler_test(&my_fn, "Singlepass");
    let cranelift_compiler_test = construct_compiler_test(&my_fn, "Cranelift");
    let llvm_compiler_test = construct_compiler_test(&my_fn, "LLVM");

    // We remove the method decorators
    my_fn.attrs = vec![];

    let x = quote! {
        #[cfg(test)]
        mod #fn_name {
            use super::*;

            #[allow(unused)]
            #my_fn

            #singlepass_compiler_test
            #cranelift_compiler_test
            #llvm_compiler_test
        }
    };
    x.into()
}

#[cfg(test)]
mod tests;

'''
'''--- tests/lib/compiler-test-derive/src/tests.rs ---
use super::*;
use pretty_assertions::assert_eq;

macro_rules! gen_tests {(
    $(
        $test_name:ident:
        stringify! {
            #[$function:ident $(($($attrs:tt)*))?]
            $($input:tt)*
        } == $output:expr;
    )*
) => (
    $(
        #[test]
        fn $test_name()
        {
            let input: TokenStream =
                stringify!($($input)*)
                    .parse()
                    .expect("Syntax error in test");
            let output: TokenStream =
                $output
                    .parse()
                    .expect("Syntax error in test");
            let attrs: TokenStream =
                stringify!($($($attrs)*)?)
                    .parse()
                    .expect("Syntax error in test");
            let ret = $function(attrs, input).to_string();
            eprintln!("{}", ret);
            assert_eq!(ret, output.to_string());
        }
    )*
)}

gen_tests! {
    identity_for_no_unsafe:
    stringify! {
        #[compiler_test(derive_test)]
        #[cold]
        fn foo(config: crate::Config) {
            // Do tests
        }
    } == stringify! {
        #[cfg(test)]
        mod foo {
            use super::*;

            #[allow(unused)]
            fn foo(config: crate::Config) {
                // Do tests
            }

            #[cfg(feature = "singlepass")]
            mod singlepass {
                use super::*;
                #[test_log::test]
                #[cold]
                #[cfg(feature = "universal")]
                fn universal() {
                    foo(crate::Config::new(
                        crate::Engine::Universal,
                        crate::Compiler::Singlepass
                    ))
                }
                #[test_log::test]
                #[cold]
                #[cfg(feature = "dylib")]
                fn dylib() {
                    foo(crate::Config::new(
                        crate::Engine::Dylib,
                        crate::Compiler::Singlepass
                    ))
                }
            }

            #[cfg(feature = "cranelift")]
            mod cranelift {
                use super::*;
                #[test_log::test]
                #[cold]
                #[cfg(feature = "universal")]
                fn universal() {
                    foo(crate::Config::new(
                        crate::Engine::Universal,
                        crate::Compiler::Cranelift
                    ))
                }
                #[test_log::test]
                #[cold]
                #[cfg(feature = "dylib")]
                fn dylib() {
                    foo(crate::Config::new(
                        crate::Engine::Dylib,
                        crate::Compiler::Cranelift
                    ))
                }
            }

            #[cfg(feature = "llvm")]
            mod llvm {
                use super::*;
                #[test_log::test]
                #[cold]
                #[cfg(feature = "universal")]
                fn universal() {
                    foo(crate::Config::new(
                        crate::Engine::Universal,
                        crate::Compiler::LLVM
                    ))
                }
                #[test_log::test]
                #[cold]
                #[cfg(feature = "dylib")]
                fn dylib() {
                    foo(crate::Config::new(
                        crate::Engine::Dylib,
                        crate::Compiler::LLVM
                    ))
                }
            }
        }
    };
}

'''
'''--- tests/lib/test-generator/Cargo.toml ---
[package]
name = "test-generator"
version = "0.1.0"
edition = "2018"
publish = false
license = "Apache-2.0 WITH LLVM-exception"

[dependencies]
anyhow = "1.0"
target-lexicon = "0.12"

[features]
test-dylib = []
test-universal = []

'''
'''--- tests/lib/test-generator/src/lib.rs ---
//! Build library to generate a program which runs all the testsuites.
//!
//! By generating a separate `#[test]` test for each file, we allow cargo test
//! to automatically run the files in parallel.
//!
//! > This program is inspired/forked from:
//! > https://github.com/bytecodealliance/wasmtime/blob/master/build.rs
mod processors;

pub use crate::processors::{emscripten_processor, wasi_processor, wast_processor};
use anyhow::Context;
use std::fmt::Write;
use std::path::{Path, PathBuf};

pub struct Testsuite {
    pub buffer: String,
    pub path: Vec<String>,
}

#[derive(PartialEq, Eq, PartialOrd, Ord)]
pub struct Test {
    pub name: String,
    pub body: String,
}

pub fn test_directory_module(
    out: &mut Testsuite,
    path: impl AsRef<Path>,
    processor: impl Fn(&mut Testsuite, PathBuf) -> Option<Test>,
) -> anyhow::Result<usize> {
    let path = path.as_ref();
    let testsuite = &extract_name(path);
    with_test_module(out, testsuite, |out| test_directory(out, path, processor))
}

fn write_test(out: &mut Testsuite, testname: &str, body: &str) -> anyhow::Result<()> {
    writeln!(
        out.buffer,
        "#[compiler_test({})]",
        out.path[..out.path.len() - 1].join("::")
    )?;
    writeln!(
        out.buffer,
        "fn r#{}(config: crate::Config) -> anyhow::Result<()> {{",
        &testname
    )?;
    writeln!(out.buffer, "{}", body)?;
    writeln!(out.buffer, "}}")?;
    writeln!(out.buffer)?;
    Ok(())
}

pub fn test_directory(
    out: &mut Testsuite,
    path: impl AsRef<Path>,
    processor: impl Fn(&mut Testsuite, PathBuf) -> Option<Test>,
) -> anyhow::Result<usize> {
    let path = path.as_ref();
    let mut dir_entries: Vec<_> = path
        .read_dir()
        .context(format!("failed to read {:?}", path))?
        .map(|r| r.expect("reading testsuite directory entry"))
        .filter_map(|dir_entry| processor(out, dir_entry.path()))
        .collect();

    dir_entries.sort();

    for Test {
        name: testname,
        body,
    } in dir_entries.iter()
    {
        out.path.push(testname.to_string());
        write_test(out, &testname, &body).unwrap();
        out.path.pop().unwrap();
    }

    Ok(dir_entries.len())
}

/// Extract a valid Rust identifier from the stem of a path.
pub fn extract_name(path: impl AsRef<Path>) -> String {
    path.as_ref()
        .file_stem()
        .expect("filename should have a stem")
        .to_str()
        .expect("filename should be representable as a string")
        .replace("-", "_")
        .replace("/", "_")
}

pub fn with_test_module<T>(
    out: &mut Testsuite,
    testsuite: &str,
    f: impl FnOnce(&mut Testsuite) -> anyhow::Result<T>,
) -> anyhow::Result<T> {
    out.path.push(testsuite.to_string());
    out.buffer.push_str("mod ");
    out.buffer.push_str(testsuite);
    out.buffer.push_str(" {\n");

    let result = f(out)?;

    out.buffer.push_str("}\n");
    out.path.pop().unwrap();
    Ok(result)
}

'''
'''--- tests/lib/test-generator/src/processors.rs ---
//! Here we define the processors usable for each test genrator
use crate::{extract_name, Test, Testsuite};
use std::path::PathBuf;

/// Given a Testsuite and a path, process the path in case is a wast
/// file.
pub fn wast_processor(_out: &mut Testsuite, p: PathBuf) -> Option<Test> {
    let ext = p.extension()?;
    // Only look at wast files.
    if ext != "wast" {
        return None;
    }

    // Ignore files starting with `.`, which could be editor temporary files
    if p.file_stem()?.to_str()?.starts_with('.') {
        return None;
    }

    let testname = extract_name(&p);

    // The implementation of `run_wast` lives in /tests/spectest.rs
    let body = format!("crate::run_wast(config, r#\"{}\"#)", p.display());

    Some(Test {
        name: testname,
        body,
    })
}

/// Given a Testsuite and a path, process the path in case is a Emscripten
/// wasm file.
pub fn emscripten_processor(_out: &mut Testsuite, p: PathBuf) -> Option<Test> {
    let ext = p.extension()?;
    // Only look at wast files.
    if ext != "wasm" {
        return None;
    }

    let outfile = {
        let mut out_ext = p.clone();
        out_ext.set_extension("out");
        if out_ext.exists() {
            out_ext
        } else {
            return None;
        }
    };

    let testname = extract_name(&p);

    // The implementation of `run_emscripten` lives in /tests/emtest.rs
    let body = format!(
        "crate::emscripten::run_emscripten(config, r#\"{}\"#, r#\"{}\"#)",
        p.display(),
        outfile.display()
    );

    Some(Test {
        name: testname,
        body,
    })
}

/// Given a Testsuite and a path, process the path in case is a WASI
/// wasm file.
pub fn wasi_processor(
    _out: &mut Testsuite,
    p: PathBuf,
    wasi_filesystem_kind: &str,
) -> Option<Test> {
    let ext = p.extension()?;
    // Only look at wast files.
    if ext != "wast" {
        return None;
    }

    let wasm_dir = {
        let mut inner = p.clone();
        inner.pop();
        inner
    };
    let testname = extract_name(&p);

    let body = format!(
        "crate::run_wasi(config, r#\"{}\"#, \"{}\", crate::{})",
        p.display(),
        wasm_dir.display(),
        wasi_filesystem_kind,
    );

    Some(Test {
        name: testname,
        body,
    })
}

'''
'''--- tests/lib/wast/Cargo.toml ---
[package]
name = "wasmer-wast"
version = "2.1.0"
authors = ["Wasmer Engineering Team <engineering@wasmer.io>"]
description = "wast testing support for wasmer"
license = "MIT OR Apache-2.0 WITH LLVM-exception"
categories = ["wasm"]
keywords = ["wasm", "webassembly"]
repository = "https://github.com/wasmerio/wasmer"
readme = "README.md"
edition = "2018"

[dependencies]
anyhow = "1.0"
wasmer = { path = "../../../lib/api", version = "=2.4.1", package = "wasmer-near", default-features = false, features = ["experimental-reference-types-extern-ref"] }
wast = "38.0"
tempfile = "3"
thiserror = "1.0"

[features]
default = ["wat"]
wat = ["wasmer/wat"]

[badges]
maintenance = { status = "actively-developed" }

'''
'''--- tests/lib/wast/README.md ---
This is the `wasmer-wast` crate, which contains an implementation of WebAssembly's
"wast" test scripting language, which is used in the
[WebAssembly spec testsuite], using wasmer for execution.

[WebAssembly spec testsuite]: https://github.com/WebAssembly/testsuite

> Note: this project started as a fork of [this crate](https://crates.io/crates/wasmtime-wast).

'''
'''--- tests/lib/wast/src/error.rs ---
use std::fmt;
use thiserror::Error;

/// A Directive Error
#[derive(Debug)]
pub struct DirectiveError {
    /// The line where the directive is defined
    pub line: usize,
    /// The column where the directive is defined
    pub col: usize,
    /// The failing message received when running the directive
    pub message: String,
}

/// A structure holding the list of all executed directives
#[derive(Error, Debug)]
pub struct DirectiveErrors {
    /// The filename where the error occured
    pub filename: String,
    /// The list of errors
    pub errors: Vec<DirectiveError>,
}

impl fmt::Display for DirectiveErrors {
    // This trait requires `fmt` with this exact signature.
    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
        // Write strictly the first element into the supplied output
        // stream: `f`. Returns `fmt::Result` which indicates whether the
        // operation succeeded or failed. Note that `write!` uses syntax which
        // is very similar to `println!`.
        writeln!(f, "Failed directives on {}:", self.filename)?;
        for error in self.errors.iter() {
            writeln!(f, "  ‚Ä¢ {} ({}:{})", error.message, error.line, error.col)?;
        }
        Ok(())
    }
}

'''
'''--- tests/lib/wast/src/lib.rs ---
//! Implementation of the WAST text format for wasmer.

#![deny(missing_docs, trivial_numeric_casts, unused_extern_crates)]
#![warn(unused_import_braces)]
#![deny(unstable_features)]
#![cfg_attr(feature = "cargo-clippy", allow(clippy::new_without_default))]
#![cfg_attr(
    feature = "cargo-clippy",
    warn(
        clippy::float_arithmetic,
        clippy::mut_mut,
        clippy::nonminimal_bool,
        clippy::map_unwrap_or,
        clippy::map_unwrap_or_else,
        clippy::print_stdout,
        clippy::unicode_not_nfc,
        clippy::use_self
    )
)]

mod error;
mod spectest;
mod wast;

pub use crate::error::{DirectiveError, DirectiveErrors};
pub use crate::spectest::spectest_importobject;
pub use crate::wast::Wast;

/// Version number of this crate.
pub const VERSION: &str = env!("CARGO_PKG_VERSION");

'''
'''--- tests/lib/wast/src/spectest.rs ---
use wasmer::*;

/// Return an instance implementing the "spectest" interface used in the
/// spec testsuite.
pub fn spectest_importobject(store: &Store) -> ImportObject {
    let print = Function::new_native(store, || {});
    let print_i32 = Function::new_native(store, |val: i32| println!("{}: i32", val));
    let print_i64 = Function::new_native(store, |val: i64| println!("{}: i64", val));
    let print_f32 = Function::new_native(store, |val: f32| println!("{}: f32", val));
    let print_f64 = Function::new_native(store, |val: f64| println!("{}: f64", val));
    let print_i32_f32 = Function::new_native(store, |i: i32, f: f32| {
        println!("{}: i32", i);
        println!("{}: f32", f);
    });
    let print_f64_f64 = Function::new_native(store, |f1: f64, f2: f64| {
        println!("{}: f64", f1);
        println!("{}: f64", f2);
    });

    let global_i32 = Global::new(store, Val::I32(666));
    let global_i64 = Global::new(store, Val::I64(666));
    let global_f32 = Global::new(store, Val::F32(f32::from_bits(0x4426_8000)));
    let global_f64 = Global::new(store, Val::F64(f64::from_bits(0x4084_d000_0000_0000)));

    let ty = TableType::new(ValType::FuncRef, 10, Some(20));
    let table = Table::new(store, ty, Val::FuncRef(None)).unwrap();

    let ty = MemoryType::new(1, Some(2), false);
    let memory = Memory::new(store, ty).unwrap();

    imports! {
        "spectest" => {
            "print" => print,
            "print_i32" => print_i32,
            "print_i64" => print_i64,
            "print_f32" => print_f32,
            "print_f64" => print_f64,
            "print_i32_f32" => print_i32_f32,
            "print_f64_f64" => print_f64_f64,
            "global_i32" => global_i32,
            "global_i64" => global_i64,
            "global_f32" => global_f32,
            "global_f64" => global_f64,
            "table" => table,
            "memory" => memory,
        },
    }
}

'''
'''--- tests/lib/wast/src/wast.rs ---
use crate::error::{DirectiveError, DirectiveErrors};
use crate::spectest::spectest_importobject;
use anyhow::{anyhow, bail, Result};
use std::collections::{BTreeMap, HashMap, HashSet};
use std::path::Path;
use std::str;
use wasmer::*;

/// The wast test script language allows modules to be defined and actions
/// to be performed on them.
pub struct Wast {
    /// Wast files have a concept of a "current" module, which is the most
    /// recently defined.
    current: Option<Instance>,
    /// The Import Object that all wast tests will have
    import_object: ImportObject,
    /// The instances in the test
    instances: HashMap<String, Instance>,
    /// Allowed failures (ideally this should be empty)
    allowed_instantiation_failures: HashSet<String>,
    /// If the (expected from .wast, actual) message pair is in this list,
    /// treat the strings as matching.
    match_trap_messages: HashMap<String, String>,
    /// If the current module was an allowed failure, we allow test to fail
    current_is_allowed_failure: bool,
    /// Extern-ref manager: used for testing extern refs: they're referred to by
    /// number in WAST, so we map here.
    extern_refs: BTreeMap<u32, ExternRef>,
    /// The wasm Store
    store: Store,
    /// A flag indicating if Wast tests should stop as soon as one test fails.
    pub fail_fast: bool,
    /// A flag indicating that assert_trap and assert_exhaustion should be skipped.
    /// See https://github.com/wasmerio/wasmer/issues/1550 for more info
    disable_assert_trap_exhaustion: bool,
}

impl Wast {
    /// Construct a new instance of `Wast` with a given imports.
    pub fn new(store: Store, import_object: ImportObject) -> Self {
        Self {
            current: None,
            store,
            import_object,
            allowed_instantiation_failures: HashSet::new(),
            match_trap_messages: HashMap::new(),
            current_is_allowed_failure: false,
            instances: HashMap::new(),
            extern_refs: BTreeMap::new(),
            fail_fast: true,
            disable_assert_trap_exhaustion: false,
        }
    }

    /// A list of instantiation failures to allow.
    pub fn allow_instantiation_failures(&mut self, failures: &[&str]) {
        for &failure_str in failures.iter() {
            self.allowed_instantiation_failures
                .insert(failure_str.to_string());
        }
    }

    /// A list of alternative messages to permit for a trap failure.
    pub fn allow_trap_message(&mut self, expected: &str, allowed: &str) {
        self.match_trap_messages
            .insert(expected.into(), allowed.into());
    }

    /// Do not run any code in assert_trap or assert_exhaustion.
    pub fn disable_assert_and_exhaustion(&mut self) {
        self.disable_assert_trap_exhaustion = true;
    }

    /// Construct a new instance of `Wast` with the spectests imports.
    pub fn new_with_spectest(store: Store) -> Self {
        let import_object = spectest_importobject(&store);
        Self::new(store, import_object)
    }

    fn get_instance(&self, instance_name: Option<&str>) -> Result<Instance> {
        match instance_name {
            Some(name) => self
                .instances
                .get(name)
                .cloned()
                .ok_or_else(|| anyhow!("failed to find instance named `{}`", name)),
            None => self
                .current
                .clone()
                .ok_or_else(|| anyhow!("no previous instance found")),
        }
    }

    /// Perform the action portion of a command.
    fn perform_execute(&mut self, exec: wast::WastExecute<'_>) -> Result<Vec<Val>> {
        match exec {
            wast::WastExecute::Invoke(invoke) => self.perform_invoke(invoke),
            wast::WastExecute::Module(mut module) => {
                let binary = module.encode()?;
                let result = self.instantiate(&binary);
                result.map(|_| Vec::new())
            }
            wast::WastExecute::Get { module, global } => {
                let instance_name = module.map(|i| i.name());
                let instance = self.get_instance(instance_name.as_deref())?;
                let global = if let Some(Export::Global(global)) = instance.lookup(global) {
                    global
                } else {
                    bail!("could not find the global: {}", global)
                };
                Ok(vec![global.from.get(&self.store)])
            }
        }
    }

    fn perform_invoke(&mut self, exec: wast::WastInvoke<'_>) -> Result<Vec<Val>> {
        let values = exec
            .args
            .iter()
            .map(|a| self.runtime_value(a))
            .collect::<Result<Vec<_>>>()?;
        self.invoke(exec.module.map(|i| i.name()), exec.name, &values)
    }

    fn assert_return(
        &self,
        result: Result<Vec<Val>>,
        results: &[wast::AssertExpression],
    ) -> Result<()> {
        let values = result?;
        for (v, e) in values.iter().zip(results) {
            if self.val_matches(v, e)? {
                continue;
            }
            if let Val::V128(bits) = v {
                if let wast::AssertExpression::V128(pattern) = e {
                    bail!(
                        "expected {:?}, got {:?} (v128 bits: {})",
                        e,
                        v128_format(*bits, pattern),
                        bits
                    );
                }
            }
            bail!("expected {:?}, got {:?}", e, v)
        }
        Ok(())
    }

    fn assert_trap(&self, result: Result<Vec<Val>>, expected: &str) -> Result<()> {
        let actual = match result {
            Ok(values) => bail!("expected trap, got {:?}", values),
            Err(t) => format!("{}", t),
        };
        if self.matches_message_assert_trap(expected, &actual) {
            return Ok(());
        }
        bail!("expected '{}', got '{}'", expected, actual)
    }

    fn run_directive(&mut self, test: &Path, directive: wast::WastDirective) -> Result<()> {
        use wast::WastDirective::*;

        match directive {
            Module(mut module) => {
                let binary = module.encode()?;
                self.module(module.id.map(|s| s.name()), &binary)?;
            }
            Register {
                span: _,
                name,
                module,
            } => {
                self.register(module.map(|s| s.name()), name)?;
            }
            Invoke(i) => {
                self.perform_invoke(i)?;
            }
            AssertReturn {
                span: _,
                exec,
                results,
            } => {
                let result = self.perform_execute(exec);
                self.assert_return(result, &results)?;
            }
            AssertTrap {
                span: _,
                exec,
                message,
            } => {
                if !self.disable_assert_trap_exhaustion {
                    let result = self.perform_execute(exec);
                    self.assert_trap(result, message)?;
                }
            }
            AssertExhaustion {
                span: _,
                call,
                message,
            } => {
                if !self.disable_assert_trap_exhaustion {
                    let result = self.perform_invoke(call);
                    self.assert_trap(result, message)?;
                }
            }
            AssertInvalid {
                span: _,
                module,
                message,
            } => {
                let wasm = match module {
                    wast::QuoteModule::Module(mut m) => m.encode()?,
                    wast::QuoteModule::Quote(list) => self.parse_quote_module(test, &list)?,
                };
                let err = match self.module(None, &wasm) {
                    Ok(()) => bail!("expected module to fail to build"),
                    Err(e) => e,
                };
                let error_message = format!("{:?}", err);
                if !Self::matches_message_assert_invalid(message, &error_message) {
                    bail!(
                        "assert_invalid: expected \"{}\", got \"{}\"",
                        message,
                        error_message
                    )
                }
            }
            QuoteModule { .. } => {
                // Do nothing
            }
            AssertException { .. } => {
                // Do nothing for now
            }
            AssertMalformed {
                module,
                span: _,
                message: _,
            } => {
                let mut module = match module {
                    wast::QuoteModule::Module(m) => m,
                    // This is a `*.wat` parser test which we're not
                    // interested in.
                    wast::QuoteModule::Quote(_) => return Ok(()),
                };
                let bytes = module.encode()?;
                if self.module(None, &bytes).is_ok() {
                    bail!("expected malformed module to fail to instantiate");
                }
            }
            AssertUnlinkable {
                span: _,
                mut module,
                message,
            } => {
                let bytes = module.encode()?;
                let err = match self.module(None, &bytes) {
                    Ok(()) => bail!("expected module to fail to link"),
                    Err(e) => e,
                };
                let error_message = format!("{:?}", err);
                if !Self::matches_message_assert_unlinkable(message, &error_message) {
                    bail!(
                        "assert_unlinkable: expected {}, got {}",
                        message,
                        error_message
                    )
                }
            }
        }

        Ok(())
    }

    /// Run a wast script from a byte buffer.
    pub fn run_buffer(&mut self, test: &Path, wast: &[u8]) -> Result<()> {
        let wast = str::from_utf8(wast)?;
        let filename = test.to_str().unwrap();
        let adjust_wast = |mut err: wast::Error| {
            err.set_path(filename.as_ref());
            err.set_text(wast);
            err
        };

        let buf = wast::parser::ParseBuffer::new(wast).map_err(adjust_wast)?;
        let ast = wast::parser::parse::<wast::Wast>(&buf).map_err(adjust_wast)?;
        let mut errors = Vec::with_capacity(ast.directives.len());
        for directive in ast.directives {
            let sp = directive.span();
            if let Err(e) = self.run_directive(test, directive) {
                let message = format!("{}", e);
                // If depends on an instance that doesn't exist
                if message.contains("no previous instance found") {
                    continue;
                }
                // We don't compute it, comes from instantiating an instance
                // that we expected to fail.
                if self.current.is_none() && self.current_is_allowed_failure {
                    continue;
                }
                let (line, col) = sp.linecol_in(wast);
                errors.push(DirectiveError {
                    line: line + 1,
                    col,
                    message,
                });
                if self.fail_fast {
                    break;
                }
            }
        }
        if !errors.is_empty() {
            return Err(DirectiveErrors {
                filename: filename.to_string(),
                errors,
            }
            .into());
        }
        Ok(())
    }

    fn parse_quote_module(&self, test: &Path, source: &[&[u8]]) -> Result<Vec<u8>> {
        let mut ret = String::new();
        for src in source {
            match str::from_utf8(src) {
                Ok(s) => ret.push_str(s),
                Err(_) => bail!("malformed UTF-8 encoding"),
            }
            ret.push(' ');
        }
        let buf = wast::parser::ParseBuffer::new(&ret)?;
        let mut wat = wast::parser::parse::<wast::Wat>(&buf)?;

        // TODO: when memory64 merges into the proper spec then this should be
        // removed since it will presumably no longer be a text-format error but
        // rather a validation error. Currently all non-memory64 proposals
        // assert that this offset is a text-parser error, whereas with memory64
        // support that error is deferred until later.
        if ret.contains("offset=4294967296") && !test.iter().any(|t| t == "memory64") {
            bail!("i32 constant out of bounds");
        }
        Ok(wat.module.encode()?)
    }

    /// Run a wast script from a file.
    pub fn run_file(&mut self, path: &Path) -> Result<()> {
        let bytes = std::fs::read(path)?;
        self.run_buffer(path, &bytes)
    }
}

// This is the implementation specific to the Runtime
impl Wast {
    /// Define a module and register it.
    fn module(&mut self, instance_name: Option<&str>, module: &[u8]) -> Result<()> {
        let instance = match self.instantiate(module) {
            Ok(i) => i,
            Err(e) => {
                // We set the current to None to allow running other
                // spectests when `fail_fast` is `false`.
                self.current = None;
                let error_message = format!("{}", e);
                self.current_is_allowed_failure = false;
                for allowed_failure in self.allowed_instantiation_failures.iter() {
                    if error_message.contains(allowed_failure) {
                        self.current_is_allowed_failure = true;
                        break;
                    }
                }
                bail!("instantiation failed with: {}", e)
            }
        };
        if let Some(name) = instance_name {
            self.instances.insert(name.to_string(), instance.clone());
        }
        self.current = Some(instance);
        self.current_is_allowed_failure = false;
        Ok(())
    }

    fn instantiate(&self, module: &[u8]) -> Result<Instance> {
        let module = Module::new(&self.store, module)?;
        let instance = Instance::new(&module, &self)?;
        Ok(instance)
    }

    /// Register an instance to make it available for performing actions.
    fn register(&mut self, name: Option<&str>, as_name: &str) -> Result<()> {
        let instance = self.get_instance(name)?;
        self.instances.insert(as_name.to_string(), instance);
        Ok(())
    }

    /// Invoke an exported function from an instance.
    fn invoke(
        &mut self,
        instance_name: Option<&str>,
        field: &str,
        args: &[Val],
    ) -> Result<Vec<Val>> {
        let instance = self.get_instance(instance_name.as_deref())?;
        let func: Function = instance
            .lookup_function(field)
            .expect("should find the function");
        match func.call(args) {
            Ok(result) => Ok(result.into()),
            Err(e) => Err(e.into()),
        }
    }

    /// Translate from a `script::Value` to a `Val`.
    fn runtime_value(&mut self, v: &wast::Expression<'_>) -> Result<Val> {
        use wast::Instruction::*;

        if v.instrs.len() != 1 {
            bail!("too many instructions in {:?}", v);
        }
        Ok(match &v.instrs[0] {
            I32Const(x) => Val::I32(*x),
            I64Const(x) => Val::I64(*x),
            F32Const(x) => Val::F32(f32::from_bits(x.bits)),
            F64Const(x) => Val::F64(f64::from_bits(x.bits)),
            V128Const(x) => Val::V128(u128::from_le_bytes(x.to_le_bytes())),
            RefNull(wast::HeapType::Func) => Val::FuncRef(None),
            RefNull(wast::HeapType::Extern) => Val::null(),
            RefExtern(number) => {
                let extern_ref = self
                    .extern_refs
                    .entry(*number)
                    .or_insert_with(|| ExternRef::new(*number));
                Val::ExternRef(extern_ref.clone())
            }
            other => bail!("couldn't convert {:?} to a runtime value", other),
        })
    }

    // Checks if the `assert_unlinkable` message matches the expected one
    fn matches_message_assert_unlinkable(expected: &str, actual: &str) -> bool {
        actual.contains(&expected)
    }

    // Checks if the `assert_invalid` message matches the expected one
    fn matches_message_assert_invalid(expected: &str, actual: &str) -> bool {
        actual.contains(expected)
            // Waiting on https://github.com/WebAssembly/bulk-memory-operations/pull/137
            // to propagate to WebAssembly/testsuite.
            || (expected.contains("unknown table") && actual.contains("unknown elem"))
            // wasmparser return the wrong message
            || (expected.contains("unknown memory") && actual.contains("no linear memories are present"))
            // `elem.wast` and `proposals/bulk-memory-operations/elem.wast` disagree
            // on the expected error message for the same error.
            || (expected.contains("out of bounds") && actual.contains("does not fit"))
            // handle `unknown global $NUM` error messages that wasmparser doesn't return yet
            || (expected.contains("unknown global") && actual.contains("unknown global"))
            // handle `unknown memory $NUM` error messages that wasmparser doesn't return yet
            || (expected.contains("unknown memory") && actual.contains("unknown memory"))
            || (expected.contains("unknown memory") && actual.contains("Data segment extends past end of the data section"))
    }

    // Checks if the `assert_trap` message matches the expected one
    fn matches_message_assert_trap(&self, expected: &str, actual: &str) -> bool {
        actual.contains(expected)
            || self
                .match_trap_messages
                .get(expected)
                .map_or(false, |alternative| actual.contains(alternative))
    }

    fn val_matches(&self, actual: &Val, expected: &wast::AssertExpression) -> Result<bool> {
        Ok(match (actual, expected) {
            (Val::I32(a), wast::AssertExpression::I32(b)) => a == b,
            (Val::I64(a), wast::AssertExpression::I64(b)) => a == b,
            // Note that these float comparisons are comparing bits, not float
            // values, so we're testing for bit-for-bit equivalence
            (Val::F32(a), wast::AssertExpression::F32(b)) => f32_matches(*a, b),
            (Val::F64(a), wast::AssertExpression::F64(b)) => f64_matches(*a, b),
            (Val::V128(a), wast::AssertExpression::V128(b)) => v128_matches(*a, b),
            (Val::FuncRef(None), wast::AssertExpression::RefNull(Some(wast::HeapType::Func))) => {
                true
            }
            (Val::FuncRef(Some(_)), wast::AssertExpression::RefNull(_)) => false,
            (Val::FuncRef(None), wast::AssertExpression::RefFunc(None)) => true,
            (Val::FuncRef(None), wast::AssertExpression::RefFunc(Some(_))) => false,
            (
                Val::ExternRef(extern_ref),
                wast::AssertExpression::RefNull(Some(wast::HeapType::Extern)),
            ) if extern_ref.is_null() => true,
            (Val::ExternRef(extern_ref), wast::AssertExpression::RefExtern(_))
                if extern_ref.is_null() =>
            {
                false
            }

            (Val::ExternRef(_), wast::AssertExpression::RefNull(_)) => false,
            (Val::ExternRef(extern_ref), wast::AssertExpression::RefExtern(num)) => {
                if let Some(stored_extern_ref) = self.extern_refs.get(num) {
                    extern_ref == stored_extern_ref
                } else {
                    false
                }
            }
            _ => bail!(
                "don't know how to compare {:?} and {:?} yet",
                actual,
                expected
            ),
        })
    }
}

impl NamedResolver for Wast {
    fn resolve_by_name(&self, module: &str, field: &str) -> Option<Export> {
        let imports = self.import_object.clone();

        if imports.contains_namespace(module) {
            imports.resolve_by_name(module, field)
        } else {
            let instance = self.instances.get(module)?;
            instance.lookup(field)
        }
    }
}

fn extract_lane_as_i8(bytes: u128, lane: usize) -> i8 {
    (bytes >> (lane * 8)) as i8
}

fn extract_lane_as_i16(bytes: u128, lane: usize) -> i16 {
    (bytes >> (lane * 16)) as i16
}

fn extract_lane_as_i32(bytes: u128, lane: usize) -> i32 {
    (bytes >> (lane * 32)) as i32
}

fn extract_lane_as_i64(bytes: u128, lane: usize) -> i64 {
    (bytes >> (lane * 64)) as i64
}

fn f32_matches(actual: f32, expected: &wast::NanPattern<wast::Float32>) -> bool {
    match expected {
        wast::NanPattern::CanonicalNan => actual.is_canonical_nan(),
        wast::NanPattern::ArithmeticNan => actual.is_arithmetic_nan(),
        wast::NanPattern::Value(expected_value) => actual.to_bits() == expected_value.bits,
    }
}

fn f64_matches(actual: f64, expected: &wast::NanPattern<wast::Float64>) -> bool {
    match expected {
        wast::NanPattern::CanonicalNan => actual.is_canonical_nan(),
        wast::NanPattern::ArithmeticNan => actual.is_arithmetic_nan(),
        wast::NanPattern::Value(expected_value) => actual.to_bits() == expected_value.bits,
    }
}

fn v128_matches(actual: u128, expected: &wast::V128Pattern) -> bool {
    match expected {
        wast::V128Pattern::I8x16(b) => b
            .iter()
            .enumerate()
            .all(|(i, b)| *b == extract_lane_as_i8(actual, i)),
        wast::V128Pattern::I16x8(b) => b
            .iter()
            .enumerate()
            .all(|(i, b)| *b == extract_lane_as_i16(actual, i)),
        wast::V128Pattern::I32x4(b) => b
            .iter()
            .enumerate()
            .all(|(i, b)| *b == extract_lane_as_i32(actual, i)),
        wast::V128Pattern::I64x2(b) => b
            .iter()
            .enumerate()
            .all(|(i, b)| *b == extract_lane_as_i64(actual, i)),
        wast::V128Pattern::F32x4(b) => b.iter().enumerate().all(|(i, b)| {
            let a = extract_lane_as_i32(actual, i) as u32;
            f32_matches(f32::from_bits(a), b)
        }),
        wast::V128Pattern::F64x2(b) => b.iter().enumerate().all(|(i, b)| {
            let a = extract_lane_as_i64(actual, i) as u64;
            f64_matches(f64::from_bits(a), b)
        }),
    }
}

fn v128_format(actual: u128, expected: &wast::V128Pattern) -> wast::V128Pattern {
    match expected {
        wast::V128Pattern::I8x16(_) => wast::V128Pattern::I8x16([
            extract_lane_as_i8(actual, 0),
            extract_lane_as_i8(actual, 1),
            extract_lane_as_i8(actual, 2),
            extract_lane_as_i8(actual, 3),
            extract_lane_as_i8(actual, 4),
            extract_lane_as_i8(actual, 5),
            extract_lane_as_i8(actual, 6),
            extract_lane_as_i8(actual, 7),
            extract_lane_as_i8(actual, 8),
            extract_lane_as_i8(actual, 9),
            extract_lane_as_i8(actual, 10),
            extract_lane_as_i8(actual, 11),
            extract_lane_as_i8(actual, 12),
            extract_lane_as_i8(actual, 13),
            extract_lane_as_i8(actual, 14),
            extract_lane_as_i8(actual, 15),
        ]),
        wast::V128Pattern::I16x8(_) => wast::V128Pattern::I16x8([
            extract_lane_as_i16(actual, 0),
            extract_lane_as_i16(actual, 1),
            extract_lane_as_i16(actual, 2),
            extract_lane_as_i16(actual, 3),
            extract_lane_as_i16(actual, 4),
            extract_lane_as_i16(actual, 5),
            extract_lane_as_i16(actual, 6),
            extract_lane_as_i16(actual, 7),
        ]),
        wast::V128Pattern::I32x4(_) => wast::V128Pattern::I32x4([
            extract_lane_as_i32(actual, 0),
            extract_lane_as_i32(actual, 1),
            extract_lane_as_i32(actual, 2),
            extract_lane_as_i32(actual, 3),
        ]),
        wast::V128Pattern::I64x2(_) => wast::V128Pattern::I64x2([
            extract_lane_as_i64(actual, 0),
            extract_lane_as_i64(actual, 1),
        ]),
        wast::V128Pattern::F32x4(_) => wast::V128Pattern::F32x4([
            wast::NanPattern::Value(wast::Float32 {
                bits: extract_lane_as_i32(actual, 0) as _,
            }),
            wast::NanPattern::Value(wast::Float32 {
                bits: extract_lane_as_i32(actual, 1) as _,
            }),
            wast::NanPattern::Value(wast::Float32 {
                bits: extract_lane_as_i32(actual, 2) as _,
            }),
            wast::NanPattern::Value(wast::Float32 {
                bits: extract_lane_as_i32(actual, 3) as _,
            }),
        ]),
        wast::V128Pattern::F64x2(_) => wast::V128Pattern::F64x2([
            wast::NanPattern::Value(wast::Float64 {
                bits: extract_lane_as_i64(actual, 0) as _,
            }),
            wast::NanPattern::Value(wast::Float64 {
                bits: extract_lane_as_i64(actual, 1) as _,
            }),
        ]),
    }
}

pub trait NaNCheck {
    fn is_arithmetic_nan(&self) -> bool;
    fn is_canonical_nan(&self) -> bool;
}

impl NaNCheck for f32 {
    fn is_arithmetic_nan(&self) -> bool {
        const AF32_NAN: u32 = 0x0040_0000;
        (self.to_bits() & AF32_NAN) == AF32_NAN
    }

    fn is_canonical_nan(&self) -> bool {
        (self.to_bits() & 0x7fff_ffff) == 0x7fc0_0000
    }
}

impl NaNCheck for f64 {
    fn is_arithmetic_nan(&self) -> bool {
        const AF64_NAN: u64 = 0x0008_0000_0000_0000;
        (self.to_bits() & AF64_NAN) == AF64_NAN
    }

    fn is_canonical_nan(&self) -> bool {
        (self.to_bits() & 0x7fff_ffff_ffff_ffff) == 0x7ff8_0000_0000_0000
    }
}

'''
'''--- tests/wast/README.md ---
# WebAssembly testsuite

Here is where all the `.wast` tests live.

'''
'''--- tests/wast/spec/Contributing.md ---
# Contributing to WebAssembly

Interested in participating? Please follow
[the same contributing guidelines as the design repository][].

  [the same contributing guidelines as the design repository]: https://github.com/WebAssembly/design/blob/master/Contributing.md

Also, please be sure to read [the README.md](README.md) for this repository.

'''
'''--- tests/wast/spec/README.md ---
Amalgamated WebAssembly Test Suite
==================================

This repository holds a mirror of the WebAssembly core testsuite which is
maintained [here](https://github.com/WebAssembly/spec/tree/master/test/core),
as well as the tests from the various [proposals
repositories](https://github.com/WebAssembly/proposals/blob/master/README.md).

In addition it also contains tests from various proposals which are currently
forks of the primary spec repo.

To add new tests or report problems in existing tests, please file issues and
PRs within the spec or individual proposal repositories rather than within this
mirror repository.

To update the tests in this repo from their upstream sources:

1. Run `update-tests.sh`

'''
'''--- tests/wast/spec/extract-parts.sh ---
#!/bin/bash
set -e
set -u
set -o pipefail

# This script extracts the modules from the testsuite test files into
# individual files in the following directories:
#  - valid - valid wasm modules
#  - invalid - wasm modules that fail to validate
#  - malformed - wasm text tests that fail to parse

wabt="../wabt"
wabtbin="$wabt/bin"

mkdir -p valid invalid malformed
rm -f valid/*.wasm
rm -f invalid/*.wasm
rm -f malformed/*.wat

for wast in *.wast; do
    base="${wast##*/}"
    json="invalid/${base%.wast}.json"
    "$wabtbin/wast2json" "$wast" -o "$json"
    rm "$json"
done

mv invalid/*.wat malformed

for wasm in invalid/*.wasm; do
    if "$wabtbin/wasm2wat" "$wasm" -o invalid/t.wat 2>/dev/null && \
       "$wabtbin/wat2wasm" invalid/t.wat -o /dev/null 2>/dev/null ; then
        mv "$wasm" valid
    fi
done
rm invalid/t.wat

'''
'''--- tests/wast/spec/repos/README.md ---
The spec and proposal repositories will be cloned in this directory by the
`update-testsuite.sh` script. Don't apply local changes to these repositories,
as the script may destroy them.

'''
'''--- tests/wast/spec/update-testsuite.sh ---
#!/bin/bash
# Update tests based on upstream repositories.
set -e
set -u
set -o pipefail

repos='
  spec
  threads
  simd
  exception-handling
  gc
  bulk-memory-operations
  tail-call
  nontrapping-float-to-int-conversions
  multi-value
  host-bindings
  sign-extension-ops
  reference-types
  annotations
'

log_and_run() {
    echo ">>" $*
    if ! $*; then
        echo "sub-command failed: $*"
        exit
    fi
}

try_log_and_run() {
    echo ">>" $*
    $*
}

pushdir() {
    pushd $1 >/dev/null || exit
}

popdir() {
    popd >/dev/null || exit
}

update_repo() {
    local repo=$1
    pushdir repos
        if [ -d ${repo} ]; then
            log_and_run git -C ${repo} fetch origin
            log_and_run git -C ${repo} reset origin/master --hard
        else
            log_and_run git clone https://github.com/WebAssembly/${repo}
        fi

        # Add upstream spec as "spec" remote.
        if [ "${repo}" != "spec" ]; then
            pushdir ${repo}
                if ! git remote | grep spec >/dev/null; then
                    log_and_run git remote add spec https://github.com/WebAssembly/spec
                fi

                log_and_run git fetch spec
            popdir
        fi
    popdir
}

merge_with_spec() {
    local repo=$1

    [ "${repo}" == "spec" ] && return

    pushdir repos/${repo}
        # Create and checkout "try-merge" branch.
        if ! git branch | grep try-merge >/dev/null; then
            log_and_run git branch try-merge origin/master
        fi
        log_and_run git checkout try-merge

        # Attempt to merge with spec/master.
        log_and_run git reset origin/master --hard
        try_log_and_run git merge -q spec/master -m "merged"
        if [ $? -ne 0 ]; then
            # Ignore merge conflicts in non-test directories.
            # We don't care about those changes.
            try_log_and_run git checkout --ours document interpreter
            try_log_and_run git add document interpreter
            try_log_and_run git -c core.editor=true merge --continue
            if [ $? -ne 0 ]; then
                git merge --abort
                popdir
                return 1
            fi
        fi
    popdir
    return 0
}

echo -e "Update repos\n" > commit_message

failed_repos=

for repo in ${repos}; do
    echo "++ updating ${repo}"
    update_repo ${repo}

    if ! merge_with_spec ${repo}; then
        echo -e "!! error merging ${repo}, skipping\n"
        failed_repos="${failed_repos} ${repo}"
        continue
    fi

    if [ "${repo}" = "spec" ]; then
        wast_dir=.
        log_and_run cp $(find repos/${repo}/test/core -name \*.wast) ${wast_dir}
    else
        wast_dir=proposals/${repo}
        mkdir -p ${wast_dir}

        # Don't add tests from propsoal that are the same as spec.
        pushdir repos/${repo}
            for new in $(find test/core -name \*.wast); do
                old=../../repos/spec/${new}
                if [[ ! -f ${old} ]] || ! diff ${old} ${new} >/dev/null; then
                    log_and_run cp ${new} ../../${wast_dir}
                fi
            done
        popdir
    fi

    # Check whether any files were updated.
    if [ $(git status -s ${wast_dir}/*.wast | wc -l) -ne 0 ]; then
        log_and_run git add ${wast_dir}/*.wast

        repo_sha=$(git -C repos/${repo} log --max-count=1 --oneline origin/master| sed -e 's/ .*//')
        echo "  ${repo}:" >> commit_message
        echo "    https://github.com/WebAssembly/${repo}/commit/${repo_sha}" >> commit_message
    fi

    echo -e "-- ${repo}\n"
done

echo "" >> commit_message
echo "This change was automatically generated by \`update-testsuite.sh\`" >> commit_message
git commit -a -F commit_message
# git push

echo "done"

if [ -n "${failed_repos}" ]; then
  echo "!! failed to update repos: ${failed_repos}"
fi

'''
'''--- tests/wast/wasmer/README.md ---
# Custom wast tests

In this directory we have created wast tests for different cases
where we want to test other scenarios than the ones offered
by the standard WebAssembly spectests.

## NaN canonicalization: `nan-canonicalization.wast`

This is an extra set of tests that assure that operations with NaNs
are deterministic regarless of the environment/chipset where it executes in.

## Call Indirect Spilled Stack: `call-indirect-spilledd-stack.wast`

We had an issue occuring that was making singlepass not working properly
on the WebAssembly benchmark: https://00f.net/2019/10/22/updated-webassembly-benchmark/.

This is a test case to ensure it doesn't reproduce again in the future.

## Multiple Traps: `multiple-traps.wast`

This is a test assuring functions that trap can be called multiple times.

## Fac: `fac.wast`

This is a simple factorial program.

## Check that struct-return on the stack doesn't overflow: `stack-overflow-sret.wast`

Stack space for a structure returning function call should be allocated once up
front, not once in each call.

'''