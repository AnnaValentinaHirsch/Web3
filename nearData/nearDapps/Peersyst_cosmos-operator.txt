*GitHub Repository "Peersyst/cosmos-operator"*

'''--- .github/dependabot.yml ---
# To get started with Dependabot version updates, you'll need to specify which
# package ecosystems to update and where the package manifests are located.
# Please see the documentation for all configuration options:
# https://docs.github.com/github/administering-a-repository/configuration-options-for-dependency-updates

version: 2
updates:
  - package-ecosystem: "gomod" # See documentation for possible values
    directory: "/" # Location of package manifests
    schedule:
      interval: "monthly"

'''
'''--- CONTRIBUTING.md ---
# Tests

We strive to 80% or higher unit test coverage. If your code is not well tested, your PR will not be merged.

Run tests via:
```sh
make test
```

# Architecture

For a high-level overview of the architecture, see [docs/architecture.md](./docs/architecture.md).

# Release Process

Prereq: Write access to the repo.

Releases should follow https://0ver.org.

1. Create and push a git tag on branch `main`. `git tag v0.X.X && git push --tags`
2. Triggers CICD action to build and push docker image to ghcr.
3. When complete, view the docker image in packages.

# Local Development 

The [Makefile](../Makefile) is your friend. Run `make help` to see all available targets.

Run these commands to setup your environment:

```shell
make tools
```

Youâ€™ll need a Kubernetes cluster to run against. You can use [KIND](https://sigs.k8s.io/kind) to get a local cluster for testing, or run against a remote cluster.
**Note:** Your controller will automatically use the current context in your kubeconfig file (i.e. whatever cluster `kubectl cluster-info` shows).

## Running a Prerelease on the Cluster

Prereq: Write access to the repo.

1. Authenticate with docker to push images to repository.

Create a [PAT on Github](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token) with package read and write permissions.

```sh
printenv GH_PAT | docker login ghcr.io -u <your GH username> --password-stdin 
```

2. Deploy a prerelease.

*Warning: Make sure you're kube context is set appropriately, so you don't install in the wrong cluster!*

```sh
make deploy-prerelease
```

## Uninstall CRDs
To delete the CRDs from the cluster:

```sh
make uninstall
```

## Undeploy controller
UnDeploy the controller to the cluster:

```sh
make undeploy
```

## How it works
This project aims to follow the Kubernetes [Operator pattern](https://kubernetes.io/docs/concepts/extend-kubernetes/operator/)

It uses [Controllers](https://kubernetes.io/docs/concepts/architecture/controller/)
which provides a reconcile function responsible for synchronizing resources untile the desired state is reached on the cluster

## Test It Out
1. Install the CRDs into the cluster:

```sh
make install
```

2. Run your controller (this will run in the foreground, so switch to a new terminal if you want to leave it running):

```sh
make run
```

**NOTE:** You can also run this in one step by running: `make install run`

## Modifying the API definitions
If you are editing the API definitions, generate the manifests such as CRs or CRDs using:

```sh
make manifests
```

More information can be found via the [Kubebuilder Documentation](https://book.kubebuilder.io/introduction.html)

'''
'''--- README.md ---
# Cosmos Operator

[![Project Status: Initial Release](https://img.shields.io/badge/repo%20status-active-green.svg?style=flat-square)](https://www.repostatus.org/#active)
[![GoDoc](https://img.shields.io/badge/godoc-reference-blue?style=flat-square&logo=go)](https://pkg.go.dev/github.com/strangelove-ventures/cosmos-operator)
[![Go Report Card](https://goreportcard.com/badge/github.com/strangelove-ventures/cosmos-operator)](https://goreportcard.com/report/github.com/strangelove-ventures/cosmos-operator)
[![License: Apache-2.0](https://img.shields.io/github/license/strangelove-ventures/cosmos-operator.svg?style=flat-square)](https://github.com/strangelove-ventures/cosmos-operator/blob/main/LICENSE)
[![Version](https://img.shields.io/github/tag/strangelove-ventures/cosmos-operator.svg?style=flat-square)](https://github.com/cosmos/strangelove-ventures/cosmos-operator)

Cosmos Operator is a [Kubernetes Operator](https://kubernetes.io/docs/concepts/extend-kubernetes/operator/) for blockchains built with the [Cosmos SDK](https://github.com/cosmos/cosmos-sdk). 

The long-term vision of this operator is to allow you to "configure it and forget it". 

## Motivation

Kubernetes provides a foundation for creating highly-available, scalable, fault-tolerant applications. 
Additionally, Kubernetes provides well-known DevOps patterns and abstractions vs. 
traditional DevOps which often requires "re-inventing the wheel".

Furthermore, the Operator Pattern allows us to mix infrastructure with business logic, 
thus minimizing human intervention and human error.

# Disclaimers

* Tested on Google's GKE and Bare-metal with Kubeadm. Although kubernetes is portable, we cannot guarantee or provide support for AWS, Azure, or other kubernetes providers.
* Requires a recent version of kubernetes: v1.23+.
* CosmosFullNode: The chain must be built from the [Cosmos SDK](https://github.com/cosmos/cosmos-sdk).
* CosmosFullNode: Validator sentries require a remote signer such as [horcrux](https://github.com/strangelove-ventures/horcrux).
* CosmosFullNode: The controller requires [heighliner](https://github.com/strangelove-ventures/heighliner) images. If you build your own image, you will need a shell `sh` and set the uid:gid to 1025:1025. If running as a validator sentry, you need `sleep` as well.
* CosmosFullNode: May not work for all Cosmos chains. (Some chains diverge from common conventions.) Strangelove has yet to encounter a Cosmos chain that does not work with this operator.

# CosmosFullNode CRD

Status: v1, stable

CosmosFullNode is the flagship CRD. Its purpose is to deploy highly-available, fault-tolerant blockchain nodes. 

The CosmosFullNode controller is like a StatefulSet for running Cosmos SDK blockchains.

A CosmosFullNode can be configured to run as an RPC node, a validator sentry, or a seed node. All configurations can
be used as persistent peers.

As of this writing, Strangelove has been running CosmosFullNode in production for many months.

[Minimal example yaml](./config/samples/cosmos_v1_cosmosfullnode.yaml)

[Full example yaml](./config/samples/cosmos_v1_cosmosfullnode_full.yaml)

### Why not a StatefulSet?
Each pod requires different config, such as peer settings in config.toml and mounted node keys. Therefore, a blanket
template as found in StatefulSet did not suffice.

Additionally, CosmosFullNode gives you more control over individual pod and pvc pairs vs. a StatefulSet to help
the human operator debug and recover from situations such as a corrupted PVCs.

# Support CRDs

These CRDs are part of the operator and serve to support CosmosFullNodes.

* [ScheduledVolumeSnapshot](./docs/scheduled_volume_snapshot.md)
* [StatefulJob](./docs/stateful_job.md)

# Quick Start

See the [quick start guide](./docs/quick_start.md).

# Contributing

See the [contributing guide](./CONTRIBUTING.md).

# Best Practices

See the [best practices guide for CosmosFullNode](./docs/fullnode_best_practices.md).

# Roadmap

Disclaimer: Strangelove has not committed to these enhancements and cannot estimate when they will be completed.

- [ ] Scheduled upgrades. Set a halt height and image version. The controller performs a rolling update with the new image version after the committed halt height.
- [x] Support configuration suitable for validator sentries.
- [x] Reliable, persistent peer support.
- [x] Quicker p2p discovery using private peers.
- [ ] Advanced readiness probe behavior. (The CometBFT rpc status endpoint is not always reliable.)
- [x] Automatic rollout for PVC resizing. (Currently human intervention required to restart pods after PVC resized.) Requires ExpandInUsePersistentVolumes feature gate.
- [x] Automatic PVC resizing. The controller increases PVC size once storage reaches a configured threshold; e.g. 80% full.
- [ ] Bootstrap config using the chain registry. Query the chain registry and set config based on the registry.
- [ ] Validate p2p such as peers, seeds, etc. and filter out non-responsive peers.
- [ ] HPA support.
- [ ] Automatic upgrades. Controller monitors governance and performs upgrade without any human intervention.
- [ ] Corrupt data recovery. Detect when a PVC may have corrupted data. Restore data from a recent VolumeSnapshot.
- [x] Safe, automatic backups. Create periodic VolumeSnapshots of PVCs while minimizing chance of data corruption during snapshot creation.

# License

Copyright 2023 Strangelove Ventures LLC.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

'''
'''--- api/v1/cosmosfullnode_types.go ---
/*
Copyright 2022 Strangelove Ventures LLC.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package v1

import (
	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/util/intstr"
)

func init() {
	SchemeBuilder.Register(&CosmosFullNode{}, &CosmosFullNodeList{})
}

// CosmosFullNodeController is the canonical controller name.
const CosmosFullNodeController = "CosmosFullNode"

// EDIT THIS FILE!  THIS IS SCAFFOLDING FOR YOU TO OWN!
// NOTE: json tags are required.  Any new fields you add must have json tags for the fields to be serialized.

// FullNodeSpec defines the desired state of CosmosFullNode
type FullNodeSpec struct {
	// INSERT ADDITIONAL SPEC FIELDS - desired state of cluster
	// Important: Run "make" to regenerate code after modifying this file

	// Number of replicas to create.
	// Individual replicas have a consistent identity.
	// +kubebuilder:validation:Minimum:=0
	Replicas int32 `json:"replicas"`

	// Different flavors of the fullnode's configuration.
	// 'Sentry' configures the fullnode as a validator sentry, requiring a remote signer such as Horcrux or TMKMS.
	// The remote signer is out of scope for the operator and must be deployed separately. Each pod exposes a privval port
	// for use with the remote signer.
	// If not set, configures node for RPC.
	// +kubebuilder:validation:Enum:=FullNode;Sentry
	// +optional
	Type FullNodeType `json:"type"`

	// Blockchain-specific configuration.
	ChainSpec ChainSpec `json:"chain"`

	// Template applied to all pods.
	// Creates 1 pod per replica.
	PodTemplate PodSpec `json:"podTemplate"`

	// How to scale pods when performing an update.
	// +optional
	RolloutStrategy RolloutStrategy `json:"strategy"`

	// Will be used to create a stand-alone PVC to provision the volume.
	// One PVC per replica mapped and mounted to a corresponding pod.
	VolumeClaimTemplate PersistentVolumeClaimSpec `json:"volumeClaimTemplate"`

	// Determines how to handle PVCs when pods are scaled down.
	// One of 'Retain' or 'Delete'.
	// If 'Delete', PVCs are deleted if pods are scaled down.
	// If 'Retain', PVCs are not deleted. The admin must delete manually or are deleted if the CRD is deleted.
	// If not set, defaults to 'Delete'.
	// +kubebuilder:validation:Enum:=Retain;Delete
	// +optional
	RetentionPolicy *RetentionPolicy `json:"volumeRetentionPolicy"`

	// Configure Operator created services. A singe rpc service is created for load balancing api, grpc, rpc, etc. requests.
	// This allows a k8s admin to use the service in an Ingress, for example.
	// Additionally, multiple p2p services are created for CometBFT peer exchange.
	// +optional
	Service ServiceSpec `json:"service"`

	// Allows overriding an instance on a case-by-case basis. An instance is a pod/pvc combo with an ordinal.
	// Key must be the name of the pod including the ordinal suffix.
	// Example: cosmos-1
	// Used for debugging.
	// +optional
	InstanceOverrides map[string]InstanceOverridesSpec `json:"instanceOverrides"`

	// Strategies for automatic recovery of faults and errors.
	// Managed by a separate controller, SelfHealingController, in an effort to reduce
	// complexity of the CosmosFullNodeController.
	// +optional
	SelfHeal *SelfHealSpec `json:"selfHeal"`
}

type FullNodeType string

const (
	FullNode FullNodeType = "FullNode"
	Sentry   FullNodeType = "Sentry"
)

// FullNodeStatus defines the observed state of CosmosFullNode
type FullNodeStatus struct {
	// INSERT ADDITIONAL STATUS FIELD - define observed state of cluster
	// Important: Run "make" to regenerate code after modifying this file

	// The most recent generation observed by the controller.
	ObservedGeneration int64 `json:"observedGeneration"`

	// The current phase of the fullnode deployment.
	// "Progressing" means the deployment is under way.
	// "Complete" means the deployment is complete and reconciliation is finished.
	// "WaitingForP2PServices" means the deployment is complete but the p2p services are not yet ready.
	// "Error" means an unrecoverable error occurred, which needs human intervention.
	Phase FullNodePhase `json:"phase"`

	// A generic message for the user. May contain errors.
	// +optional
	StatusMessage *string `json:"status"`

	// Set by the ScheduledVolumeSnapshotController. Used to signal the CosmosFullNode to modify its
	// resources during VolumeSnapshot creation.
	// Map key is the source ScheduledVolumeSnapshot CRD that created the status.
	// +optional
	// +mapType:=granular
	ScheduledSnapshotStatus map[string]FullNodeSnapshotStatus `json:"scheduledSnapshotStatus"`

	// Status set by the SelfHealing controller.
	// +optional
	SelfHealing SelfHealingStatus `json:"selfHealing,omitempty"`

	// Persistent peer addresses.
	// +optional
	Peers []string `json:"peers"`

	// Current sync information. Collected every 60s.
	// +optional
	SyncInfo map[string]*SyncInfoPodStatus `json:"sync,omitempty"`

	// Latest Height information. collected when node starts up and when RPC is successfully queried.
	// +optional
	Height map[string]uint64 `json:"height,omitempty"`
}

type SyncInfoPodStatus struct {
	// When consensus information was fetched.
	Timestamp metav1.Time `json:"timestamp"`
	// Latest height if no error encountered.
	// +optional
	Height *uint64 `json:"height,omitempty"`
	// If the pod reports itself as in sync with chain tip.
	// +optional
	InSync *bool `json:"inSync,omitempty"`
	// Error message if unable to fetch consensus state.
	// +optional
	Error *string `json:"error,omitempty"`
}

type FullNodeSnapshotStatus struct {
	// Which pod name to temporarily delete. Indicates a ScheduledVolumeSnapshot is taking place. For optimal data
	// integrity, pod is temporarily removed so PVC does not have any processes writing to it.
	PodCandidate string `json:"podCandidate"`
}

type FullNodePhase string

const (
	FullNodePhaseCompete        FullNodePhase = "Complete"
	FullNodePhaseError          FullNodePhase = "Error"
	FullNodePhaseP2PServices    FullNodePhase = "WaitingForP2PServices"
	FullNodePhaseProgressing    FullNodePhase = "Progressing"
	FullNodePhaseTransientError FullNodePhase = "TransientError"
)

// Metadata is a subset of k8s object metadata.
type Metadata struct {
	// Labels are added to a resource. If there is a collision between labels the Operator creates, the Operator
	// labels take precedence.
	// +optional
	Labels map[string]string `json:"labels"`
	// Annotations are added to a resource. If there is a collision between annotations the Operator creates, the Operator
	// annotations take precedence.
	// +optional
	Annotations map[string]string `json:"annotations"`
}

type PodSpec struct {
	// Metadata is a subset of metav1.ObjectMeta applied to all pods.
	// +optional
	Metadata Metadata `json:"metadata"`

	// Image is the docker reference in "repository:tag" format. E.g. busybox:latest.
	// This is for the main container running the chain process.
	// Note: for granular control over which images are applied at certain block heights,
	// use spec.chain.versions instead.
	// +kubebuilder:validation:MinLength:=1
	// +optional
	Image string `json:"image"`

	// Image pull policy.
	// One of Always, Never, IfNotPresent.
	// Defaults to Always if :latest tag is specified, or IfNotPresent otherwise.
	// Cannot be updated.
	// More info: https://kubernetes.io/docs/concepts/containers/images#updating-images
	// This is for the main container running the chain process.
	// +optional
	ImagePullPolicy corev1.PullPolicy `json:"imagePullPolicy"`

	// ImagePullSecrets is a list of references to secrets in the same namespace to use for pulling any images
	// in pods that reference this ServiceAccount. ImagePullSecrets are distinct from Secrets because Secrets
	// can be mounted in the pod, but ImagePullSecrets are only accessed by the kubelet.
	// More info: https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod
	// This is for the main container running the chain process.
	// +optional
	ImagePullSecrets []corev1.LocalObjectReference `json:"imagePullSecrets"`

	// NodeSelector is a selector which must be true for the pod to fit on a node.
	// Selector which must match a node's labels for the pod to be scheduled on that node.
	// More info: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
	// This is an advanced configuration option.
	// +optional
	NodeSelector map[string]string `json:"nodeSelector"`

	// If specified, the pod's scheduling constraints
	// This is an advanced configuration option.
	// +optional
	Affinity *corev1.Affinity `json:"affinity"`

	// If specified, the pod's tolerations.
	// This is an advanced configuration option.
	// +optional
	Tolerations []corev1.Toleration `json:"tolerations"`

	// If specified, indicates the pod's priority. "system-node-critical" and
	// "system-cluster-critical" are two special keywords which indicate the
	// highest priorities with the former being the highest priority. Any other
	// name must be defined by creating a PriorityClass object with that name.
	// If not specified, the pod priority will be default or zero if there is no
	// default.
	// This is an advanced configuration option.
	// +optional
	PriorityClassName string `json:"priorityClassName"`

	// The priority value. Various system components use this field to find the
	// priority of the pod. When Priority Admission Controller is enabled, it
	// prevents users from setting this field. The admission controller populates
	// this field from PriorityClassName.
	// The higher the value, the higher the priority.
	// This is an advanced configuration option.
	// +optional
	Priority *int32 `json:"priority"`

	// Resources describes the compute resource requirements.
	// +optional
	Resources corev1.ResourceRequirements `json:"resources"`

	// Optional duration in seconds the pod needs to terminate gracefully. May be decreased in delete request.
	// Value must be non-negative integer. The value zero indicates stop immediately via
	// the kill signal (no opportunity to shut down).
	// If this value is nil, the default grace period will be used instead.
	// The grace period is the duration in seconds after the processes running in the pod are sent
	// a termination signal and the time when the processes are forcibly halted with a kill signal.
	// Set this value longer than the expected cleanup time for your process.
	// This is an advanced configuration option.
	// Defaults to 30 seconds.
	// +optional
	TerminationGracePeriodSeconds *int64 `json:"terminationGracePeriodSeconds"`

	// Configure probes for the pods managed by the controller.
	// +optional
	Probes FullNodeProbesSpec `json:"probes"`

	// List of volumes that can be mounted by containers belonging to the pod.
	// More info: https://kubernetes.io/docs/concepts/storage/volumes
	// A strategic merge patch is applied to the default volumes created by the controller.
	// Take extreme caution when using this feature. Use only for critical bugs.
	// Some chains do not follow conventions or best practices, so this serves as an "escape hatch" for the user
	// at the cost of maintainability.
	// +optional
	Volumes []corev1.Volume `json:"volumes"`

	// List of initialization containers belonging to the pod.
	// More info: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
	// A strategic merge patch is applied to the default init containers created by the controller.
	// Take extreme caution when using this feature. Use only for critical bugs.
	// Some chains do not follow conventions or best practices, so this serves as an "escape hatch" for the user
	// at the cost of maintainability.
	// +optional
	InitContainers []corev1.Container `json:"initContainers"`

	// List of containers belonging to the pod.
	// A strategic merge patch is applied to the default containers created by the controller.
	// Take extreme caution when using this feature. Use only for critical bugs.
	// Some chains do not follow conventions or best practices, so this serves as an "escape hatch" for the user
	// at the cost of maintainability.
	// +optional
	Containers []corev1.Container `json:"containers"`
}

type FullNodeProbeStrategy string

const (
	FullNodeProbeStrategyNone FullNodeProbeStrategy = "None"
)

// FullNodeProbesSpec configures probes for created pods
type FullNodeProbesSpec struct {
	// Strategy controls the default probes added by the controller.
	// None = Do not add any probes. May be necessary for Sentries using a remote signer.
	// +kubebuilder:validation:Enum:=None
	// +optional
	Strategy FullNodeProbeStrategy `json:"strategy"`
}

// PersistentVolumeClaimSpec describes the common attributes of storage devices
// and allows a Source for provider-specific attributes
type PersistentVolumeClaimSpec struct {
	// Applied to all PVCs.
	// +optional
	Metadata Metadata `json:"metadata"`

	// storageClassName is the name of the StorageClass required by the claim.
	// For proper pod scheduling, it's highly recommended to set "volumeBindingMode: WaitForFirstConsumer" in the StorageClass.
	// More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#class-1
	// For GKE, recommended storage class is "premium-rwo".
	// This field is immutable. Updating this field requires manually deleting the PVC.
	// This field is required.
	StorageClassName string `json:"storageClassName"`

	// resources represents the minimum resources the volume should have.
	// If RecoverVolumeExpansionFailure feature is enabled users are allowed to specify resource requirements
	// that are lower than previous value but must still be higher than capacity recorded in the
	// status field of the claim.
	// More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#resources
	// Updating the storage size is allowed but the StorageClass must support file system resizing.
	// Only increasing storage is permitted.
	// This field is required.
	Resources corev1.ResourceRequirements `json:"resources"`

	// accessModes contain the desired access modes the volume should have.
	// More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#access-modes-1
	// If not specified, defaults to ReadWriteOnce.
	// This field is immutable. Updating this field requires manually deleting the PVC.
	// +optional
	AccessModes []corev1.PersistentVolumeAccessMode `json:"accessModes"`

	// volumeMode defines what type of volume is required by the claim.
	// Value of Filesystem is implied when not included in claim spec.
	// This field is immutable. Updating this field requires manually deleting the PVC.
	// +optional
	VolumeMode *corev1.PersistentVolumeMode `json:"volumeMode"`

	// Can be used to specify either:
	// * An existing VolumeSnapshot object (snapshot.storage.k8s.io/VolumeSnapshot)
	// * An existing PVC (PersistentVolumeClaim)
	// If the provisioner or an external controller can support the specified data source,
	// it will create a new volume based on the contents of the specified data source.
	// If the AnyVolumeDataSource feature gate is enabled, this field will always have
	// the same contents as the DataSourceRef field.
	// If you choose an existing PVC, the PVC must be in the same availability zone.
	// +optional
	DataSource *corev1.TypedLocalObjectReference `json:"dataSource"`

	// If set, discovers and dynamically sets dataSource for the PVC on creation.
	// No effect if dataSource field set; that field takes precedence.
	// Configuring autoDataSource may help boostrap new replicas more quickly.
	// +optional
	AutoDataSource *AutoDataSource `json:"autoDataSource"`
}

type RetentionPolicy string

const (
	RetentionPolicyRetain RetentionPolicy = "Retain"
	RetentionPolicyDelete RetentionPolicy = "Delete"
)

type AutoDataSource struct {
	// If set, chooses the most recent VolumeSnapshot matching the selector to use as the PVC dataSource.
	// See ScheduledVolumeSnapshot for a means of creating periodic VolumeSnapshots.
	// The VolumeSnapshots must be in the same namespace as the CosmosFullNode.
	// If no VolumeSnapshots found, controller logs error and still creates PVC.
	// +optional
	VolumeSnapshotSelector map[string]string `json:"volumeSnapshotSelector"`

	// If true, the volume snapshot selector will make sure the PVC
	// is restored from a VolumeSnapshot on the same node.
	// This is useful if the VolumeSnapshots are local to the node, e.g. for topolvm.
	MatchInstance bool `json:"matchInstance"`
}

// RolloutStrategy is an update strategy that can be shared between several Cosmos CRDs.
type RolloutStrategy struct {
	// The maximum number of pods that can be unavailable during an update.
	// Value can be an absolute number (ex: 5) or a percentage of desired pods (ex: 10%).
	// Absolute number is calculated from percentage by rounding down. The minimum max unavailable is 1.
	// Defaults to 25%.
	// Example: when this is set to 30%, pods are scaled down to 70% of desired pods
	// immediately when the rolling update starts. Once new pods are ready, pods
	// can be scaled down further, ensuring that the total number of pods available
	// at all times during the update is at least 70% of desired pods.
	// +kubebuilder:validation:XIntOrString
	// +optional
	MaxUnavailable *intstr.IntOrString `json:"maxUnavailable"`
}

type ChainSpec struct {
	// Genesis file chain-id.
	// +kubebuilder:validation:MinLength:=1
	ChainID string `json:"chainID"`

	// The network environment. Typically, mainnet, testnet, devnet, etc.
	// +kubebuilder:validation:MinLength:=1
	Network string `json:"network"`

	// Binary name which runs commands. E.g. gaiad, junod, osmosisd
	// +kubebuilder:validation:MinLength:=1
	Binary string `json:"binary"`

	// The chain's home directory is where the chain's data and config is stored.
	// This should be a single folder. E.g. .gaia, .dydxprotocol, .osmosisd, etc.
	// Set via --home flag when running the binary.
	// If empty, defaults to "cosmos" which translates to `chain start --home /home/operator/cosmos`.
	// Historically, several chains do not respect the --home and save data outside --home which crashes the pods.
	// Therefore, this option was introduced to mitigate those edge cases, so that you can specify the home directory
	// to match the chain's default home dir.
	// +optional
	HomeDir string `json:"homeDir"`

	// CometBFT (formerly Tendermint) configuration applied to config.toml.
	// Although optional, it's highly recommended you configure this field.
	// +optional
	Comet CometConfig `json:"config"`

	// App configuration applied to app.toml.
	App SDKAppConfig `json:"app"`

	// One of trace|debug|info|warn|error|fatal|panic.
	// If not set, defaults to info.
	// +kubebuilder:validation:Enum:=trace;debug;info;warn;error;fatal;panic
	// +optional
	LogLevel *string `json:"logLevel"`

	// One of plain or json.
	// If not set, defaults to plain.
	// +kubebuilder:validation:Enum:=plain;json
	// +optional
	LogFormat *string `json:"logFormat"`

	// URL to address book file to download from the internet.
	// The operator detects and properly handles the following file extensions:
	// .json, .json.gz, .tar, .tar.gz, .tar.gzip, .zip
	// Use AddrbookScript if the chain has an unconventional file format or address book location.
	// +optional
	AddrbookURL *string `json:"addrbookURL"`

	// Specify shell (sh) script commands to properly download and save the address book file.
	// Prefer AddrbookURL if the file is in a conventional format.
	// The available shell commands are from docker image ghcr.io/strangelove-ventures/infra-toolkit, including wget and curl.
	// Save the file to env var $ADDRBOOK_FILE.
	// E.g. curl https://url-to-addrbook.com > $ADDRBOOK_FILE
	// Takes precedence over AddrbookURL.
	// Hint: Use "set -eux" in your script.
	// Available env vars:
	// $HOME: The home directory.
	// $ADDRBOOK_FILE: The location of the final address book file.
	// $CONFIG_DIR: The location of the config dir that houses the address book file. Used for extracting from archives. The archive must have a single file called "addrbook.json".
	// +optional
	AddrbookScript *string `json:"addrbookScript"`

	// URL to genesis file to download from the internet.
	// Although this field is optional, you will almost always want to set it.
	// If not set, uses the genesis file created from the init subcommand. (This behavior may be desirable for new chains or testing.)
	// The operator detects and properly handles the following file extensions:
	// .json, .json.gz, .tar, .tar.gz, .tar.gzip, .zip
	// Use GenesisScript if the chain has an unconventional file format or genesis location.
	// +optional
	GenesisURL *string `json:"genesisURL"`

	// Specify shell (sh) script commands to properly download and save the genesis file.
	// Prefer GenesisURL if the file is in a conventional format.
	// The available shell commands are from docker image ghcr.io/strangelove-ventures/infra-toolkit, including wget and curl.
	// Save the file to env var $GENESIS_FILE.
	// E.g. curl https://url-to-genesis.com | jq '.genesis' > $GENESIS_FILE
	// Takes precedence over GenesisURL.
	// Hint: Use "set -eux" in your script.
	// Available env vars:
	// $HOME: The home directory.
	// $GENESIS_FILE: The location of the final genesis file.
	// $CONFIG_DIR: The location of the config dir that houses the genesis file. Used for extracting from archives. The archive must have a single file called "genesis.json".
	// +optional
	GenesisScript *string `json:"genesisScript"`

	// Skip x/crisis invariants check on startup.
	// +optional
	SkipInvariants bool `json:"skipInvariants"`

	// URL for a snapshot archive to download from the internet.
	// Unarchiving the snapshot populates the data directory.
	// Although this field is optional, you will almost always want to set it.
	// The operator detects and properly handles the following file extensions:
	// .tar, .tar.gz, .tar.gzip, .tar.lz4
	// Use SnapshotScript if the snapshot archive is unconventional or requires special handling.
	// +optional
	SnapshotURL *string `json:"snapshotURL"`

	// Specify shell (sh) script commands to properly download and process a snapshot archive.
	// Prefer SnapshotURL if possible.
	// The available shell commands are from docker image ghcr.io/strangelove-ventures/infra-toolkit, including wget and curl.
	// Save the file to env var $GENESIS_FILE.
	// Takes precedence over SnapshotURL.
	// Hint: Use "set -eux" in your script.
	// Available env vars:
	// $HOME: The user's home directory.
	// $CHAIN_HOME: The home directory for the chain, aka: --home flag
	// $DATA_DIR: The directory for the database files.
	// +optional
	SnapshotScript *string `json:"snapshotScript"`

	// If configured as a Sentry, invokes sleep command with this value before running chain start command.
	// Currently, requires the privval laddr to be available immediately without any retry.
	// This workaround gives time for the connection to be made to a remote signer.
	// If a Sentry and not set, defaults to 10.
	// If set to 0, omits injecting sleep command.
	// Assumes chain image has `sleep` in $PATH.
	// +kubebuilder:validation:Minimum:=0
	// +optional
	PrivvalSleepSeconds *int32 `json:"privvalSleepSeconds"`

	// DatabaseBackend must match in order to detect the block height
	// of the chain prior to starting in order to pick the correct image version.
	// options: goleveldb, rocksdb, pebbledb
	// Defaults to goleveldb.
	// +optional
	DatabaseBackend *string `json:"databaseBackend"`

	// Versions of the chain and which height they should be applied.
	// When provided, the operator will automatically upgrade the chain as it reaches the specified heights.
	// If not provided, the operator will not upgrade the chain, and will use the image specified in the pod spec.
	// +optional
	Versions []ChainVersion `json:"versions"`

	// Additional arguments to pass to the chain init command.
	// +optional
	AdditionalInitArgs []string `json:"additionalInitArgs"`

	// Additional arguments to pass to the chain start command.
	// +optional
	AdditionalStartArgs []string `json:"additionalStartArgs"`
}

type ChainVersion struct {
	// The block height when this version should be applied.
	UpgradeHeight uint64 `json:"height"`

	// The docker image for this version in "repository:tag" format. E.g. busybox:latest.
	Image string `json:"image"`

	// Determines if the node should forcefully halt at the upgrade height.
	// +optional
	SetHaltHeight bool `json:"setHaltHeight,omitempty"`
}

// CometConfig configures the config.toml.
type CometConfig struct {
	// Comma delimited list of p2p nodes in <ID>@<IP>:<PORT> format to keep persistent p2p connections.
	// +kubebuilder:validation:MinLength:=1
	// +optional
	PersistentPeers string `json:"peers"`

	// Comma delimited list of p2p seed nodes in <ID>@<IP>:<PORT> format.
	// +kubebuilder:validation:MinLength:=1
	// +optional
	Seeds string `json:"seeds"`

	// Comma delimited list of node/peer IDs to keep private (will not be gossiped to other peers)
	// +optional
	PrivatePeerIDs string `json:"privatePeerIDs"`

	// Comma delimited list of node/peer IDs, to which a connection will be (re)established ignoring any existing limits.
	// +optional
	UnconditionalPeerIDs string `json:"unconditionalPeerIDs"`

	// p2p maximum number of inbound peers.
	// If unset, defaults to 20.
	// +kubebuilder:validation:Minimum:=0
	// +optional
	MaxInboundPeers *int32 `json:"maxInboundPeers"`

	// p2p maximum number of outbound peers.
	// If unset, defaults to 20.
	// +kubebuilder:validation:Minimum:=0
	// +optional
	MaxOutboundPeers *int32 `json:"maxOutboundPeers"`

	// rpc list of origins a cross-domain request can be executed from.
	// Default value '[]' disables cors support.
	// Use '["*"]' to allow any origin.
	// +optional
	CorsAllowedOrigins []string `json:"corsAllowedOrigins"`

	// Customize config.toml.
	// Values entered here take precedence over all other configuration.
	// Must be valid toml.
	// Important: all keys must be "snake_case" which differs from app.toml.
	// WARNING: Overriding may clobber some values that the operator sets such as persistent_peers, private_peer_ids,
	// and unconditional_peer_ids. Use the dedicated fields for these values which will merge values.
	// +optional
	TomlOverrides *string `json:"overrides"`
}

// SDKAppConfig configures the cosmos sdk application app.toml.
type SDKAppConfig struct {
	// The minimum gas prices a validator is willing to accept for processing a
	// transaction. A transaction's fees must meet the minimum of any denomination
	// specified in this config (e.g. 0.25token1;0.0001token2).
	// +kubebuilder:validation:MinLength:=1
	MinGasPrice string `json:"minGasPrice"`

	// Defines if CORS should be enabled for the API (unsafe - use it at your own risk).
	// +optional
	APIEnableUnsafeCORS bool `json:"apiEnableUnsafeCORS"`

	// Defines if CORS should be enabled for grpc-web (unsafe - use it at your own risk).
	// +optional
	GRPCWebEnableUnsafeCORS bool `json:"grpcWebEnableUnsafeCORS"`

	// Controls pruning settings. i.e. How much data to keep on disk.
	// If not set, defaults to "default" pruning strategy.
	// +optional
	Pruning *Pruning `json:"pruning"`

	// If set, block height at which to gracefully halt the chain and shutdown the node.
	// Useful for testing or upgrades.
	// +kubebuilder:validation:Minimum:=0
	// +optional
	HaltHeight *uint64 `json:"haltHeight"`

	// Custom app config toml.
	// Values entered here take precedence over all other configuration.
	// Must be valid toml.
	// Important: all keys must be "kebab-case" which differs from config.toml.
	// +optional
	TomlOverrides *string `json:"overrides"`
}

// Pruning controls the pruning settings.
type Pruning struct {
	// One of default|nothing|everything|custom.
	// default: the last 100 states are kept in addition to every 500th state; pruning at 10 block intervals.
	// nothing: all historic states will be saved, nothing will be deleted (i.e. archiving node).
	// everything: all saved states will be deleted, storing only the current state; pruning at 10 block intervals.
	// custom: allow pruning options to be manually specified through Interval, KeepEvery, KeepRecent.
	// +kubebuilder:validation:Enum:=default;nothing;everything;custom
	Strategy PruningStrategy `json:"strategy"`

	// Bock height interval at which pruned heights are removed from disk (ignored if pruning is not 'custom').
	// If not set, defaults to 0.
	// +optional
	Interval *uint32 `json:"interval"`

	// Offset heights to keep on disk after 'keep-every' (ignored if pruning is not 'custom')
	// Often, setting this to 0 is appropriate.
	// If not set, defaults to 0.
	// +optional
	KeepEvery *uint32 `json:"keepEvery"`

	// Number of recent block heights to keep on disk (ignored if pruning is not 'custom')
	// If not set, defaults to 0.
	// +optional
	KeepRecent *uint32 `json:"keepRecent"`

	// Defines the minimum block height offset from the current
	// block being committed, such that all blocks past this offset are pruned
	// from CometBFT. It is used as part of the process of determining the
	// ResponseCommit.RetainHeight value during ABCI Commit. A value of 0 indicates
	// that no blocks should be pruned.
	//
	// This configuration value is only responsible for pruning Comet blocks.
	// It has no bearing on application state pruning which is determined by the
	// "pruning-*" configurations.
	//
	// Note: CometBFT block pruning is dependent on this parameter in conjunction
	// with the unbonding (safety threshold) period, state pruning and state sync
	// snapshot parameters to determine the correct minimum value of
	// ResponseCommit.RetainHeight.
	//
	// If not set, defaults to 0.
	// +optional
	MinRetainBlocks *uint32 `json:"minRetainBlocks"`
}

// PruningStrategy control pruning.
type PruningStrategy string

const (
	PruningDefault    PruningStrategy = "default"
	PruningNothing    PruningStrategy = "nothing"
	PruningEverything PruningStrategy = "everything"
	PruningCustom     PruningStrategy = "custom"
)

type ServiceSpec struct {
	// Max number of external p2p services to create for CometBFT peer exchange.
	// The public endpoint is set as the "p2p.external_address" in the config.toml.
	// Controller creates p2p services for each pod so that every pod can peer with each other internally in the cluster.
	// This setting allows you to control the number of p2p services exposed for peers outside of the cluster to use.
	// If not set, defaults to 1.
	// +kubebuilder:validation:Minimum:=0
	// +optional
	MaxP2PExternalAddresses *int32 `json:"maxP2PExternalAddresses"`

	// Overrides for all P2P services that need external addresses.
	// +optional
	P2PTemplate ServiceOverridesSpec `json:"p2pTemplate"`

	// Overrides for the single RPC service.
	// +optional
	RPCTemplate ServiceOverridesSpec `json:"rpcTemplate"`
}

// ServiceOverridesSpec allows some overrides for the created, single RPC service.
type ServiceOverridesSpec struct {
	// +optional
	Metadata Metadata `json:"metadata"`

	// Describes ingress methods for a service.
	// If not set, defaults to "ClusterIP".
	// +kubebuilder:validation:Enum:=ClusterIP;NodePort;LoadBalancer;ExternalName
	// +optional
	Type *corev1.ServiceType `json:"type"`

	// Sets endpoint and routing behavior.
	// See: https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#caveats-and-limitations-when-preserving-source-ips
	// If not set, defaults to "Cluster".
	// +kubebuilder:validation:Enum:=Cluster;Local
	// +optional
	ExternalTrafficPolicy *corev1.ServiceExternalTrafficPolicyType `json:"externalTrafficPolicy"`
}

// InstanceOverridesSpec allows overriding an instance which is pod/pvc combo with an ordinal
type InstanceOverridesSpec struct {
	// Disables whole or part of the instance.
	// Used for scenarios like debugging or deleting the PVC and restoring from a dataSource.
	// Set to "Pod" to prevent controller from creating a pod for this instance, leaving the PVC.
	// Set to "All" to prevent the controller from managing a pod and pvc. Note, the PVC may not be deleted if
	// the RetainStrategy is set to "Retain". If you need to remove the PVC, delete manually.
	// +kubebuilder:validation:Enum:=Pod;All
	// +optional
	DisableStrategy *DisableStrategy `json:"disable"`

	// Overrides an individual instance's PVC.
	// +optional
	VolumeClaimTemplate *PersistentVolumeClaimSpec `json:"volumeClaimTemplate"`

	// Overrides an individual instance's Image.
	// +optional
	Image string `json:"image"`

	// Sets an individual instance's external address.
	// +optional
	ExternalAddress *string `json:"externalAddress"`
}

type DisableStrategy string

const (
	DisableAll DisableStrategy = "All"
	DisablePod DisableStrategy = "Pod"
)

//+kubebuilder:object:root=true
//+kubebuilder:subresource:status
//+kubebuilder:printcolumn:name="Phase",type=string,JSONPath=`.status.phase`
//+kubebuilder:printcolumn:name="Age",type="date",JSONPath=".metadata.creationTimestamp"

// CosmosFullNode is the Schema for the cosmosfullnodes API
type CosmosFullNode struct {
	metav1.TypeMeta   `json:",inline"`
	metav1.ObjectMeta `json:"metadata,omitempty"`

	Spec   FullNodeSpec   `json:"spec,omitempty"`
	Status FullNodeStatus `json:"status,omitempty"`
}

//+kubebuilder:object:root=true

// CosmosFullNodeList contains a list of CosmosFullNode
type CosmosFullNodeList struct {
	metav1.TypeMeta `json:",inline"`
	metav1.ListMeta `json:"metadata,omitempty"`
	Items           []CosmosFullNode `json:"items"`
}

'''
'''--- api/v1/groupversion_info.go ---
/*
Copyright 2022 Strangelove Ventures LLC.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

// Package v1 contains API Schema definitions for the cosmos v1 API group
// +kubebuilder:object:generate=true
// +groupName=cosmos.strange.love
package v1

import (
	"k8s.io/apimachinery/pkg/runtime/schema"
	"sigs.k8s.io/controller-runtime/pkg/scheme"
)

var (
	// GroupVersion is group version used to register these objects
	GroupVersion = schema.GroupVersion{Group: "cosmos.strange.love", Version: "v1"}

	// SchemeBuilder is used to add go types to the GroupVersionKind scheme
	SchemeBuilder = &scheme.Builder{GroupVersion: GroupVersion}

	// AddToScheme adds the types in this group-version to the given scheme.
	AddToScheme = SchemeBuilder.AddToScheme
)

'''
'''--- api/v1/self_healing_types.go ---
package v1

import (
	"k8s.io/apimachinery/pkg/api/resource"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

// SelfHealingController is the canonical controller name.
const SelfHealingController = "SelfHealing"

// SelfHealSpec is part of a CosmosFullNode but is managed by a separate controller, SelfHealingReconciler.
// This is an effort to reduce complexity in the CosmosFullNodeReconciler.
// The controller only modifies the CosmosFullNode's status subresource relying on the CosmosFullNodeReconciler
// to reconcile appropriately.
type SelfHealSpec struct {
	// Automatically increases PVC storage as they approach capacity.
	//
	// Your cluster must support and use the ExpandInUsePersistentVolumes feature gate. This allows volumes to
	// expand while a pod is attached to it, thus eliminating the need to restart pods.
	// If you cluster does not support ExpandInUsePersistentVolumes, you will need to manually restart pods after
	// resizing is complete.
	// +optional
	PVCAutoScale *PVCAutoScaleSpec `json:"pvcAutoScale"`

	// Take action when a pod's height falls behind the max height of all pods AND still reports itself as in-sync.
	//
	// +optional
	HeightDriftMitigation *HeightDriftMitigationSpec `json:"heightDriftMitigation"`
}

type PVCAutoScaleSpec struct {
	// The percentage of used disk space required to trigger scaling.
	// Example, if set to 80, autoscaling will not trigger until used space reaches >=80% of capacity.
	// +kubebuilder:validation:Minimum=1
	// +kubebuilder:validation:MaxSize=100
	UsedSpacePercentage int32 `json:"usedSpacePercentage"`

	// How much to increase the PVC's capacity.
	// Either a percentage (e.g. 20%) or a resource storage quantity (e.g. 100Gi).
	//
	// If a percentage, the existing capacity increases by the percentage.
	// E.g. PVC of 100Gi capacity + IncreaseQuantity of 20% increases disk to 120Gi.
	//
	// If a storage quantity (e.g. 100Gi), increases by that amount.
	IncreaseQuantity string `json:"increaseQuantity"`

	// A resource storage quantity (e.g. 2000Gi).
	// When increasing PVC capacity reaches >= MaxSize, autoscaling ceases.
	// Safeguards against storage quotas and costs.
	// +optional
	MaxSize resource.Quantity `json:"maxSize"`
}

type HeightDriftMitigationSpec struct {
	// If pod's height falls behind the max height of all pods by this value or more AND the pod's RPC /status endpoint
	// reports itself as in-sync, the pod is deleted. The CosmosFullNodeController creates a new pod to replace it.
	// Pod deletion respects the CosmosFullNode.Spec.RolloutStrategy and will not delete more pods than set
	// by the strategy to prevent downtime.
	// This workaround is necessary to mitigate a bug in the Cosmos SDK and/or CometBFT where pods report themselves as
	// in-sync even though they can lag thousands of blocks behind the chain tip and cannot catch up.
	// A "rebooted" pod /status reports itself correctly and allows it to catch up to chain tip.
	// +kubebuilder:validation:Minimum:=1
	Threshold uint32 `json:"threshold"`
}

type SelfHealingStatus struct {
	// PVC auto-scaling status.
	// +optional
	PVCAutoScale map[string]*PVCAutoScaleStatus `json:"pvcAutoScaler"`
}

type PVCAutoScaleStatus struct {
	// The PVC size requested by the SelfHealing controller.
	RequestedSize resource.Quantity `json:"requestedSize"`
	// The timestamp the SelfHealing controller requested a PVC increase.
	RequestedAt metav1.Time `json:"requestedAt"`
}

'''
'''--- api/v1/zz_generated.deepcopy.go ---
//go:build !ignore_autogenerated
// +build !ignore_autogenerated

/*
Copyright 2022 Strangelove Ventures LLC.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

// Code generated by controller-gen. DO NOT EDIT.

package v1

import (
	corev1 "k8s.io/api/core/v1"
	runtime "k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/util/intstr"
)

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *AutoDataSource) DeepCopyInto(out *AutoDataSource) {
	*out = *in
	if in.VolumeSnapshotSelector != nil {
		in, out := &in.VolumeSnapshotSelector, &out.VolumeSnapshotSelector
		*out = make(map[string]string, len(*in))
		for key, val := range *in {
			(*out)[key] = val
		}
	}
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new AutoDataSource.
func (in *AutoDataSource) DeepCopy() *AutoDataSource {
	if in == nil {
		return nil
	}
	out := new(AutoDataSource)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *ChainSpec) DeepCopyInto(out *ChainSpec) {
	*out = *in
	in.Comet.DeepCopyInto(&out.Comet)
	in.App.DeepCopyInto(&out.App)
	if in.LogLevel != nil {
		in, out := &in.LogLevel, &out.LogLevel
		*out = new(string)
		**out = **in
	}
	if in.LogFormat != nil {
		in, out := &in.LogFormat, &out.LogFormat
		*out = new(string)
		**out = **in
	}
	if in.AddrbookURL != nil {
		in, out := &in.AddrbookURL, &out.AddrbookURL
		*out = new(string)
		**out = **in
	}
	if in.AddrbookScript != nil {
		in, out := &in.AddrbookScript, &out.AddrbookScript
		*out = new(string)
		**out = **in
	}
	if in.GenesisURL != nil {
		in, out := &in.GenesisURL, &out.GenesisURL
		*out = new(string)
		**out = **in
	}
	if in.GenesisScript != nil {
		in, out := &in.GenesisScript, &out.GenesisScript
		*out = new(string)
		**out = **in
	}
	if in.SnapshotURL != nil {
		in, out := &in.SnapshotURL, &out.SnapshotURL
		*out = new(string)
		**out = **in
	}
	if in.SnapshotScript != nil {
		in, out := &in.SnapshotScript, &out.SnapshotScript
		*out = new(string)
		**out = **in
	}
	if in.PrivvalSleepSeconds != nil {
		in, out := &in.PrivvalSleepSeconds, &out.PrivvalSleepSeconds
		*out = new(int32)
		**out = **in
	}
	if in.DatabaseBackend != nil {
		in, out := &in.DatabaseBackend, &out.DatabaseBackend
		*out = new(string)
		**out = **in
	}
	if in.Versions != nil {
		in, out := &in.Versions, &out.Versions
		*out = make([]ChainVersion, len(*in))
		copy(*out, *in)
	}
	if in.AdditionalInitArgs != nil {
		in, out := &in.AdditionalInitArgs, &out.AdditionalInitArgs
		*out = make([]string, len(*in))
		copy(*out, *in)
	}
	if in.AdditionalStartArgs != nil {
		in, out := &in.AdditionalStartArgs, &out.AdditionalStartArgs
		*out = make([]string, len(*in))
		copy(*out, *in)
	}
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new ChainSpec.
func (in *ChainSpec) DeepCopy() *ChainSpec {
	if in == nil {
		return nil
	}
	out := new(ChainSpec)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *ChainVersion) DeepCopyInto(out *ChainVersion) {
	*out = *in
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new ChainVersion.
func (in *ChainVersion) DeepCopy() *ChainVersion {
	if in == nil {
		return nil
	}
	out := new(ChainVersion)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *CometConfig) DeepCopyInto(out *CometConfig) {
	*out = *in
	if in.MaxInboundPeers != nil {
		in, out := &in.MaxInboundPeers, &out.MaxInboundPeers
		*out = new(int32)
		**out = **in
	}
	if in.MaxOutboundPeers != nil {
		in, out := &in.MaxOutboundPeers, &out.MaxOutboundPeers
		*out = new(int32)
		**out = **in
	}
	if in.CorsAllowedOrigins != nil {
		in, out := &in.CorsAllowedOrigins, &out.CorsAllowedOrigins
		*out = make([]string, len(*in))
		copy(*out, *in)
	}
	if in.TomlOverrides != nil {
		in, out := &in.TomlOverrides, &out.TomlOverrides
		*out = new(string)
		**out = **in
	}
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new CometConfig.
func (in *CometConfig) DeepCopy() *CometConfig {
	if in == nil {
		return nil
	}
	out := new(CometConfig)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *CosmosFullNode) DeepCopyInto(out *CosmosFullNode) {
	*out = *in
	out.TypeMeta = in.TypeMeta
	in.ObjectMeta.DeepCopyInto(&out.ObjectMeta)
	in.Spec.DeepCopyInto(&out.Spec)
	in.Status.DeepCopyInto(&out.Status)
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new CosmosFullNode.
func (in *CosmosFullNode) DeepCopy() *CosmosFullNode {
	if in == nil {
		return nil
	}
	out := new(CosmosFullNode)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyObject is an autogenerated deepcopy function, copying the receiver, creating a new runtime.Object.
func (in *CosmosFullNode) DeepCopyObject() runtime.Object {
	if c := in.DeepCopy(); c != nil {
		return c
	}
	return nil
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *CosmosFullNodeList) DeepCopyInto(out *CosmosFullNodeList) {
	*out = *in
	out.TypeMeta = in.TypeMeta
	in.ListMeta.DeepCopyInto(&out.ListMeta)
	if in.Items != nil {
		in, out := &in.Items, &out.Items
		*out = make([]CosmosFullNode, len(*in))
		for i := range *in {
			(*in)[i].DeepCopyInto(&(*out)[i])
		}
	}
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new CosmosFullNodeList.
func (in *CosmosFullNodeList) DeepCopy() *CosmosFullNodeList {
	if in == nil {
		return nil
	}
	out := new(CosmosFullNodeList)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyObject is an autogenerated deepcopy function, copying the receiver, creating a new runtime.Object.
func (in *CosmosFullNodeList) DeepCopyObject() runtime.Object {
	if c := in.DeepCopy(); c != nil {
		return c
	}
	return nil
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *FullNodeProbesSpec) DeepCopyInto(out *FullNodeProbesSpec) {
	*out = *in
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new FullNodeProbesSpec.
func (in *FullNodeProbesSpec) DeepCopy() *FullNodeProbesSpec {
	if in == nil {
		return nil
	}
	out := new(FullNodeProbesSpec)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *FullNodeSnapshotStatus) DeepCopyInto(out *FullNodeSnapshotStatus) {
	*out = *in
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new FullNodeSnapshotStatus.
func (in *FullNodeSnapshotStatus) DeepCopy() *FullNodeSnapshotStatus {
	if in == nil {
		return nil
	}
	out := new(FullNodeSnapshotStatus)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *FullNodeSpec) DeepCopyInto(out *FullNodeSpec) {
	*out = *in
	in.ChainSpec.DeepCopyInto(&out.ChainSpec)
	in.PodTemplate.DeepCopyInto(&out.PodTemplate)
	in.RolloutStrategy.DeepCopyInto(&out.RolloutStrategy)
	in.VolumeClaimTemplate.DeepCopyInto(&out.VolumeClaimTemplate)
	if in.RetentionPolicy != nil {
		in, out := &in.RetentionPolicy, &out.RetentionPolicy
		*out = new(RetentionPolicy)
		**out = **in
	}
	in.Service.DeepCopyInto(&out.Service)
	if in.InstanceOverrides != nil {
		in, out := &in.InstanceOverrides, &out.InstanceOverrides
		*out = make(map[string]InstanceOverridesSpec, len(*in))
		for key, val := range *in {
			(*out)[key] = *val.DeepCopy()
		}
	}
	if in.SelfHeal != nil {
		in, out := &in.SelfHeal, &out.SelfHeal
		*out = new(SelfHealSpec)
		(*in).DeepCopyInto(*out)
	}
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new FullNodeSpec.
func (in *FullNodeSpec) DeepCopy() *FullNodeSpec {
	if in == nil {
		return nil
	}
	out := new(FullNodeSpec)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *FullNodeStatus) DeepCopyInto(out *FullNodeStatus) {
	*out = *in
	if in.StatusMessage != nil {
		in, out := &in.StatusMessage, &out.StatusMessage
		*out = new(string)
		**out = **in
	}
	if in.ScheduledSnapshotStatus != nil {
		in, out := &in.ScheduledSnapshotStatus, &out.ScheduledSnapshotStatus
		*out = make(map[string]FullNodeSnapshotStatus, len(*in))
		for key, val := range *in {
			(*out)[key] = val
		}
	}
	in.SelfHealing.DeepCopyInto(&out.SelfHealing)
	if in.Peers != nil {
		in, out := &in.Peers, &out.Peers
		*out = make([]string, len(*in))
		copy(*out, *in)
	}
	if in.SyncInfo != nil {
		in, out := &in.SyncInfo, &out.SyncInfo
		*out = make(map[string]*SyncInfoPodStatus, len(*in))
		for key, val := range *in {
			var outVal *SyncInfoPodStatus
			if val == nil {
				(*out)[key] = nil
			} else {
				in, out := &val, &outVal
				*out = new(SyncInfoPodStatus)
				(*in).DeepCopyInto(*out)
			}
			(*out)[key] = outVal
		}
	}
	if in.Height != nil {
		in, out := &in.Height, &out.Height
		*out = make(map[string]uint64, len(*in))
		for key, val := range *in {
			(*out)[key] = val
		}
	}
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new FullNodeStatus.
func (in *FullNodeStatus) DeepCopy() *FullNodeStatus {
	if in == nil {
		return nil
	}
	out := new(FullNodeStatus)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *HeightDriftMitigationSpec) DeepCopyInto(out *HeightDriftMitigationSpec) {
	*out = *in
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new HeightDriftMitigationSpec.
func (in *HeightDriftMitigationSpec) DeepCopy() *HeightDriftMitigationSpec {
	if in == nil {
		return nil
	}
	out := new(HeightDriftMitigationSpec)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *InstanceOverridesSpec) DeepCopyInto(out *InstanceOverridesSpec) {
	*out = *in
	if in.DisableStrategy != nil {
		in, out := &in.DisableStrategy, &out.DisableStrategy
		*out = new(DisableStrategy)
		**out = **in
	}
	if in.VolumeClaimTemplate != nil {
		in, out := &in.VolumeClaimTemplate, &out.VolumeClaimTemplate
		*out = new(PersistentVolumeClaimSpec)
		(*in).DeepCopyInto(*out)
	}
	if in.ExternalAddress != nil {
		in, out := &in.ExternalAddress, &out.ExternalAddress
		*out = new(string)
		**out = **in
	}
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new InstanceOverridesSpec.
func (in *InstanceOverridesSpec) DeepCopy() *InstanceOverridesSpec {
	if in == nil {
		return nil
	}
	out := new(InstanceOverridesSpec)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *Metadata) DeepCopyInto(out *Metadata) {
	*out = *in
	if in.Labels != nil {
		in, out := &in.Labels, &out.Labels
		*out = make(map[string]string, len(*in))
		for key, val := range *in {
			(*out)[key] = val
		}
	}
	if in.Annotations != nil {
		in, out := &in.Annotations, &out.Annotations
		*out = make(map[string]string, len(*in))
		for key, val := range *in {
			(*out)[key] = val
		}
	}
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new Metadata.
func (in *Metadata) DeepCopy() *Metadata {
	if in == nil {
		return nil
	}
	out := new(Metadata)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *PVCAutoScaleSpec) DeepCopyInto(out *PVCAutoScaleSpec) {
	*out = *in
	out.MaxSize = in.MaxSize.DeepCopy()
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new PVCAutoScaleSpec.
func (in *PVCAutoScaleSpec) DeepCopy() *PVCAutoScaleSpec {
	if in == nil {
		return nil
	}
	out := new(PVCAutoScaleSpec)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *PVCAutoScaleStatus) DeepCopyInto(out *PVCAutoScaleStatus) {
	*out = *in
	out.RequestedSize = in.RequestedSize.DeepCopy()
	in.RequestedAt.DeepCopyInto(&out.RequestedAt)
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new PVCAutoScaleStatus.
func (in *PVCAutoScaleStatus) DeepCopy() *PVCAutoScaleStatus {
	if in == nil {
		return nil
	}
	out := new(PVCAutoScaleStatus)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *PersistentVolumeClaimSpec) DeepCopyInto(out *PersistentVolumeClaimSpec) {
	*out = *in
	in.Metadata.DeepCopyInto(&out.Metadata)
	in.Resources.DeepCopyInto(&out.Resources)
	if in.AccessModes != nil {
		in, out := &in.AccessModes, &out.AccessModes
		*out = make([]corev1.PersistentVolumeAccessMode, len(*in))
		copy(*out, *in)
	}
	if in.VolumeMode != nil {
		in, out := &in.VolumeMode, &out.VolumeMode
		*out = new(corev1.PersistentVolumeMode)
		**out = **in
	}
	if in.DataSource != nil {
		in, out := &in.DataSource, &out.DataSource
		*out = new(corev1.TypedLocalObjectReference)
		(*in).DeepCopyInto(*out)
	}
	if in.AutoDataSource != nil {
		in, out := &in.AutoDataSource, &out.AutoDataSource
		*out = new(AutoDataSource)
		(*in).DeepCopyInto(*out)
	}
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new PersistentVolumeClaimSpec.
func (in *PersistentVolumeClaimSpec) DeepCopy() *PersistentVolumeClaimSpec {
	if in == nil {
		return nil
	}
	out := new(PersistentVolumeClaimSpec)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *PodSpec) DeepCopyInto(out *PodSpec) {
	*out = *in
	in.Metadata.DeepCopyInto(&out.Metadata)
	if in.ImagePullSecrets != nil {
		in, out := &in.ImagePullSecrets, &out.ImagePullSecrets
		*out = make([]corev1.LocalObjectReference, len(*in))
		copy(*out, *in)
	}
	if in.NodeSelector != nil {
		in, out := &in.NodeSelector, &out.NodeSelector
		*out = make(map[string]string, len(*in))
		for key, val := range *in {
			(*out)[key] = val
		}
	}
	if in.Affinity != nil {
		in, out := &in.Affinity, &out.Affinity
		*out = new(corev1.Affinity)
		(*in).DeepCopyInto(*out)
	}
	if in.Tolerations != nil {
		in, out := &in.Tolerations, &out.Tolerations
		*out = make([]corev1.Toleration, len(*in))
		for i := range *in {
			(*in)[i].DeepCopyInto(&(*out)[i])
		}
	}
	if in.Priority != nil {
		in, out := &in.Priority, &out.Priority
		*out = new(int32)
		**out = **in
	}
	in.Resources.DeepCopyInto(&out.Resources)
	if in.TerminationGracePeriodSeconds != nil {
		in, out := &in.TerminationGracePeriodSeconds, &out.TerminationGracePeriodSeconds
		*out = new(int64)
		**out = **in
	}
	out.Probes = in.Probes
	if in.Volumes != nil {
		in, out := &in.Volumes, &out.Volumes
		*out = make([]corev1.Volume, len(*in))
		for i := range *in {
			(*in)[i].DeepCopyInto(&(*out)[i])
		}
	}
	if in.InitContainers != nil {
		in, out := &in.InitContainers, &out.InitContainers
		*out = make([]corev1.Container, len(*in))
		for i := range *in {
			(*in)[i].DeepCopyInto(&(*out)[i])
		}
	}
	if in.Containers != nil {
		in, out := &in.Containers, &out.Containers
		*out = make([]corev1.Container, len(*in))
		for i := range *in {
			(*in)[i].DeepCopyInto(&(*out)[i])
		}
	}
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new PodSpec.
func (in *PodSpec) DeepCopy() *PodSpec {
	if in == nil {
		return nil
	}
	out := new(PodSpec)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *Pruning) DeepCopyInto(out *Pruning) {
	*out = *in
	if in.Interval != nil {
		in, out := &in.Interval, &out.Interval
		*out = new(uint32)
		**out = **in
	}
	if in.KeepEvery != nil {
		in, out := &in.KeepEvery, &out.KeepEvery
		*out = new(uint32)
		**out = **in
	}
	if in.KeepRecent != nil {
		in, out := &in.KeepRecent, &out.KeepRecent
		*out = new(uint32)
		**out = **in
	}
	if in.MinRetainBlocks != nil {
		in, out := &in.MinRetainBlocks, &out.MinRetainBlocks
		*out = new(uint32)
		**out = **in
	}
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new Pruning.
func (in *Pruning) DeepCopy() *Pruning {
	if in == nil {
		return nil
	}
	out := new(Pruning)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *RolloutStrategy) DeepCopyInto(out *RolloutStrategy) {
	*out = *in
	if in.MaxUnavailable != nil {
		in, out := &in.MaxUnavailable, &out.MaxUnavailable
		*out = new(intstr.IntOrString)
		**out = **in
	}
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new RolloutStrategy.
func (in *RolloutStrategy) DeepCopy() *RolloutStrategy {
	if in == nil {
		return nil
	}
	out := new(RolloutStrategy)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *SDKAppConfig) DeepCopyInto(out *SDKAppConfig) {
	*out = *in
	if in.Pruning != nil {
		in, out := &in.Pruning, &out.Pruning
		*out = new(Pruning)
		(*in).DeepCopyInto(*out)
	}
	if in.HaltHeight != nil {
		in, out := &in.HaltHeight, &out.HaltHeight
		*out = new(uint64)
		**out = **in
	}
	if in.TomlOverrides != nil {
		in, out := &in.TomlOverrides, &out.TomlOverrides
		*out = new(string)
		**out = **in
	}
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new SDKAppConfig.
func (in *SDKAppConfig) DeepCopy() *SDKAppConfig {
	if in == nil {
		return nil
	}
	out := new(SDKAppConfig)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *SelfHealSpec) DeepCopyInto(out *SelfHealSpec) {
	*out = *in
	if in.PVCAutoScale != nil {
		in, out := &in.PVCAutoScale, &out.PVCAutoScale
		*out = new(PVCAutoScaleSpec)
		(*in).DeepCopyInto(*out)
	}
	if in.HeightDriftMitigation != nil {
		in, out := &in.HeightDriftMitigation, &out.HeightDriftMitigation
		*out = new(HeightDriftMitigationSpec)
		**out = **in
	}
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new SelfHealSpec.
func (in *SelfHealSpec) DeepCopy() *SelfHealSpec {
	if in == nil {
		return nil
	}
	out := new(SelfHealSpec)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *SelfHealingStatus) DeepCopyInto(out *SelfHealingStatus) {
	*out = *in
	if in.PVCAutoScale != nil {
		in, out := &in.PVCAutoScale, &out.PVCAutoScale
		*out = make(map[string]*PVCAutoScaleStatus, len(*in))
		for key, val := range *in {
			var outVal *PVCAutoScaleStatus
			if val == nil {
				(*out)[key] = nil
			} else {
				in, out := &val, &outVal
				*out = new(PVCAutoScaleStatus)
				(*in).DeepCopyInto(*out)
			}
			(*out)[key] = outVal
		}
	}
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new SelfHealingStatus.
func (in *SelfHealingStatus) DeepCopy() *SelfHealingStatus {
	if in == nil {
		return nil
	}
	out := new(SelfHealingStatus)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *ServiceOverridesSpec) DeepCopyInto(out *ServiceOverridesSpec) {
	*out = *in
	in.Metadata.DeepCopyInto(&out.Metadata)
	if in.Type != nil {
		in, out := &in.Type, &out.Type
		*out = new(corev1.ServiceType)
		**out = **in
	}
	if in.ExternalTrafficPolicy != nil {
		in, out := &in.ExternalTrafficPolicy, &out.ExternalTrafficPolicy
		*out = new(corev1.ServiceExternalTrafficPolicyType)
		**out = **in
	}
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new ServiceOverridesSpec.
func (in *ServiceOverridesSpec) DeepCopy() *ServiceOverridesSpec {
	if in == nil {
		return nil
	}
	out := new(ServiceOverridesSpec)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *ServiceSpec) DeepCopyInto(out *ServiceSpec) {
	*out = *in
	if in.MaxP2PExternalAddresses != nil {
		in, out := &in.MaxP2PExternalAddresses, &out.MaxP2PExternalAddresses
		*out = new(int32)
		**out = **in
	}
	in.P2PTemplate.DeepCopyInto(&out.P2PTemplate)
	in.RPCTemplate.DeepCopyInto(&out.RPCTemplate)
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new ServiceSpec.
func (in *ServiceSpec) DeepCopy() *ServiceSpec {
	if in == nil {
		return nil
	}
	out := new(ServiceSpec)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *SyncInfoPodStatus) DeepCopyInto(out *SyncInfoPodStatus) {
	*out = *in
	in.Timestamp.DeepCopyInto(&out.Timestamp)
	if in.Height != nil {
		in, out := &in.Height, &out.Height
		*out = new(uint64)
		**out = **in
	}
	if in.InSync != nil {
		in, out := &in.InSync, &out.InSync
		*out = new(bool)
		**out = **in
	}
	if in.Error != nil {
		in, out := &in.Error, &out.Error
		*out = new(string)
		**out = **in
	}
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new SyncInfoPodStatus.
func (in *SyncInfoPodStatus) DeepCopy() *SyncInfoPodStatus {
	if in == nil {
		return nil
	}
	out := new(SyncInfoPodStatus)
	in.DeepCopyInto(out)
	return out
}

'''
'''--- api/v1alpha1/groupversion_info.go ---
/*
Copyright 2022 Strangelove Ventures LLC.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

// Package v1alpha1 contains API Schema definitions for the cosmos v1alpha1 API group
// +kubebuilder:object:generate=true
// +groupName=cosmos.strange.love
package v1alpha1

import (
	"k8s.io/apimachinery/pkg/runtime/schema"
	"sigs.k8s.io/controller-runtime/pkg/scheme"
)

var (
	// GroupVersion is group version used to register these objects
	GroupVersion = schema.GroupVersion{Group: "cosmos.strange.love", Version: "v1alpha1"}

	// SchemeBuilder is used to add go types to the GroupVersionKind scheme
	SchemeBuilder = &scheme.Builder{GroupVersion: GroupVersion}

	// AddToScheme adds the types in this group-version to the given scheme.
	AddToScheme = SchemeBuilder.AddToScheme
)

'''
'''--- api/v1alpha1/scheduledvolumesnapshot_types.go ---
/*
Copyright 2022 Strangelove Ventures LLC.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package v1alpha1

import (
	snapshotv1 "github.com/kubernetes-csi/external-snapshotter/client/v6/apis/volumesnapshot/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

func init() {
	SchemeBuilder.Register(&ScheduledVolumeSnapshot{}, &ScheduledVolumeSnapshotList{})
}

// ScheduledVolumeSnapshotController is the canonical controller name.
const ScheduledVolumeSnapshotController = "ScheduledVolumeSnapshot"

// EDIT THIS FILE!  THIS IS SCAFFOLDING FOR YOU TO OWN!
// NOTE: json tags are required.  Any new fields you add must have json tags for the fields to be serialized.

// ScheduledVolumeSnapshotSpec defines the desired state of ScheduledVolumeSnapshot
// Creates recurring VolumeSnapshots of a PVC managed by a CosmosFullNode.
// A VolumeSnapshot is a CRD (installed in GKE by default).
// See: https://kubernetes.io/docs/concepts/storage/volume-snapshots/
// This enables recurring, consistent backups.
// To prevent data corruption, a pod is temporarily deleted while the snapshot takes place which could take
// several minutes.
// Therefore, if you create a ScheduledVolumeSnapshot, you must use replica count >= 2 to prevent downtime.
// If <= 1 pod in a ready state, the controller will not temporarily delete the pod. The controller makes every
// effort to prevent downtime.
// Only 1 VolumeSnapshot is created at a time, so at most only 1 pod is temporarily deleted.
// Multiple, parallel VolumeSnapshots are not supported.
type ScheduledVolumeSnapshotSpec struct {
	// INSERT ADDITIONAL SPEC FIELDS - desired state of cluster
	// Important: Run "make" to regenerate code after modifying this file

	// Reference to the source CosmosFullNode.
	// This field is immutable. If you change the fullnode, you may encounter undefined behavior.
	// The CosmosFullNode must be in the same namespace as the ScheduledVolumeSnapshot.
	// Instead delete the ScheduledVolumeSnapshot and create a new one with the correct fullNodeRef.
	FullNodeRef LocalFullNodeRef `json:"fullNodeRef"`

	// A crontab schedule using the standard as described in https://en.wikipedia.org/wiki/Cron.
	// See https://crontab.guru for format.
	// Kubernetes providers rate limit VolumeSnapshot creation. Therefore, setting a crontab that's
	// too frequent may result in rate limiting errors.
	Schedule string `json:"schedule"`

	// The name of the VolumeSnapshotClass to use when creating snapshots.
	VolumeSnapshotClassName string `json:"volumeSnapshotClassName"`

	// If true, the controller will temporarily delete the candidate pod before taking a snapshot of the pod's associated PVC.
	// This option prevents writes to the PVC, ensuring the highest possible data integrity.
	// Once the snapshot is created, the pod will be restored.
	// +optional
	DeletePod bool `json:"deletePod"`

	// Minimum number of CosmosFullNode pods that must be ready before creating a VolumeSnapshot.
	// In the future, this field will have no effect unless spec.deletePod=true.
	// This controller gracefully deletes a pod while taking a snapshot. Then recreates the pod once the
	// snapshot is complete.
	// This way, the snapshot has the highest possible data integrity.
	// Defaults to 2.
	// Warning: If set to 1, you will experience downtime.
	// +optional
	// +kubebuilder:validation:Minimum:=1
	MinAvailable int32 `json:"minAvailable"`

	// The number of recent VolumeSnapshots to keep.
	// Defaults to 3.
	// +optional
	// +kubebuilder:validation:Minimum:=1
	Limit int32 `json:"limit"`

	// If true, the controller will not create any VolumeSnapshots.
	// This allows you to disable creation of VolumeSnapshots without deleting the ScheduledVolumeSnapshot resource.
	// This pattern works better when using tools such as Kustomzie.
	// If a pod is temporarily deleted, it will be restored.
	// +optional
	Suspend bool `json:"suspend"`
}

type LocalFullNodeRef struct {
	// Name of the object, metadata.name
	Name string `json:"name"`
	// DEPRECATED: CosmosFullNode must be in the same namespace as the ScheduledVolumeSnapshot. This field is ignored.
	// +optional
	Namespace string `json:"namespace"`

	// Index of the pod to snapshot. If not provided, will do any pod in the CosmosFullNode.
	// Useful when snapshots are local to the same node as the pod, requiring snapshots across multiple pods/nodes.
	// +optional
	Ordinal *int32 `json:"ordinal"`
}

// ScheduledVolumeSnapshotStatus defines the observed state of ScheduledVolumeSnapshot
type ScheduledVolumeSnapshotStatus struct {
	// INSERT ADDITIONAL STATUS FIELD - define observed state of cluster
	// Important: Run "make" to regenerate code after modifying this file

	// The most recent generation observed by the controller.
	ObservedGeneration int64 `json:"observedGeneration"`

	// A generic message for the user. May contain errors.
	// +optional
	StatusMessage *string `json:"status"`

	// The phase of the controller.
	Phase SnapshotPhase `json:"phase"`

	// The date when the CRD was created.
	// Used as a reference when calculating the next time to create a snapshot.
	CreatedAt metav1.Time `json:"createdAt"`

	// The pod/pvc pair of the CosmosFullNode from which to make a VolumeSnapshot.
	// +optional
	Candidate *SnapshotCandidate `json:"candidate"`

	// The most recent volume snapshot created by the controller.
	// +optional
	LastSnapshot *VolumeSnapshotStatus `json:"lastSnapshot"`
}

type SnapshotCandidate struct {
	PodName string `json:"podName"`
	PVCName string `json:"pvcName"`

	// +optional
	PodLabels map[string]string `json:"podLabels"`
}

type SnapshotPhase string

// These values are persisted. Do not change arbitrarily.
const (
	// SnapshotPhaseWaitingForNext means waiting for the next scheduled time to start the snapshot creation process.
	SnapshotPhaseWaitingForNext SnapshotPhase = "WaitingForNext"

	// SnapshotPhaseFindingCandidate is finding a pod/pvc candidate from which to create a VolumeSnapshot.
	SnapshotPhaseFindingCandidate SnapshotPhase = "FindingCandidate"

	// SnapshotPhaseDeletingPod signals the fullNodeRef to delete the candidate pod. This allows taking a VolumeSnapshot
	// on a "quiet" PVC, with no processes writing to it.
	SnapshotPhaseDeletingPod SnapshotPhase = "DeletingPod"

	// SnapshotPhaseWaitingForPodDeletion indicates controller is waiting for the fullNodeRef to delete the candidate pod.
	SnapshotPhaseWaitingForPodDeletion SnapshotPhase = "WaitingForPodDeletion"

	// SnapshotPhaseCreating indicates controller found a candidate and will now create a VolumeSnapshot from the PVC.
	SnapshotPhaseCreating SnapshotPhase = "CreatingSnapshot"

	// SnapshotPhaseWaitingForCreation means the VolumeSnapshot has been created and the controller is waiting for
	// the VolumeSnapshot to become ready for use.
	SnapshotPhaseWaitingForCreation SnapshotPhase = "WaitingForSnapshotCreation"

	// SnapshotPhaseRestorePod signals the fullNodeRef it can recreate the temporarily deleted pod.
	SnapshotPhaseRestorePod SnapshotPhase = "RestoringPod"

	// SnapshotPhaseSuspended means the controller is not creating snapshots. Suspended by the user.
	SnapshotPhaseSuspended SnapshotPhase = "Suspended"

	// SnapshotPhaseMissingCRDs means the controller is not creating snapshots. The required VolumeSnapshot CRDs are missing.
	SnapshotPhaseMissingCRDs SnapshotPhase = "MissingCRDs"
)

type VolumeSnapshotStatus struct {
	// The name of the created VolumeSnapshot.
	Name string `json:"name"`

	// The time the controller created the VolumeSnapshot.
	StartedAt metav1.Time `json:"startedAt"`

	// The last VolumeSnapshot's status
	// +optional
	Status *snapshotv1.VolumeSnapshotStatus `json:"status"`
}

//+kubebuilder:object:root=true
//+kubebuilder:subresource:status
//+kubebuilder:printcolumn:name="Phase",type=string,JSONPath=`.status.phase`
//+kubebuilder:printcolumn:name="Age",type="date",JSONPath=".metadata.creationTimestamp"

// ScheduledVolumeSnapshot is the Schema for the scheduledvolumesnapshots API
type ScheduledVolumeSnapshot struct {
	metav1.TypeMeta   `json:",inline"`
	metav1.ObjectMeta `json:"metadata,omitempty"`

	Spec   ScheduledVolumeSnapshotSpec   `json:"spec,omitempty"`
	Status ScheduledVolumeSnapshotStatus `json:"status,omitempty"`
}

//+kubebuilder:object:root=true

// ScheduledVolumeSnapshotList contains a list of ScheduledVolumeSnapshot
type ScheduledVolumeSnapshotList struct {
	metav1.TypeMeta `json:",inline"`
	metav1.ListMeta `json:"metadata,omitempty"`
	Items           []ScheduledVolumeSnapshot `json:"items"`
}

'''
'''--- api/v1alpha1/statefuljob_types.go ---
/*
Copyright 2022 Strangelove Ventures LLC.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package v1alpha1

import (
	batchv1 "k8s.io/api/batch/v1"
	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

func init() {
	SchemeBuilder.Register(&StatefulJob{}, &StatefulJobList{})
}

// StatefulJobController is the canonical controller name.
const StatefulJobController = "StatefulJob"

// EDIT THIS FILE!  THIS IS SCAFFOLDING FOR YOU TO OWN!
// NOTE: json tags are required.  Any new fields you add must have json tags for the fields to be serialized.

// StatefulJobSpec defines the desired state of StatefulJob
type StatefulJobSpec struct {
	// INSERT ADDITIONAL SPEC FIELDS - desired state of cluster
	// Important: Run "make" to regenerate code after modifying this file

	// The selector to target VolumeSnapshots.
	Selector map[string]string `json:"selector"`

	// Interval at which the controller runs snapshot job with pvc.
	// Expressed as a duration string, e.g. 1.5h, 24h, 12h.
	// Defaults to 24h.
	// +optional
	Interval metav1.Duration `json:"interval"`

	// Specification of the desired behavior of the job.
	// +optional
	JobTemplate JobTemplateSpec `json:"jobTemplate"`

	// Specification of the desired behavior of the job's pod.
	// You should include container commands and args to perform the upload of data to a remote location like an
	// object store such as S3 or GCS.
	// Volumes will be injected and mounted into every container in the spec.
	// Working directory will be /home/operator.
	// The chain directory will be /home/operator/cosmos and set as env var $CHAIN_HOME.
	// If not set, pod's restart policy defaults to Never.
	PodTemplate corev1.PodTemplateSpec `json:"podTemplate"`

	// Specification for the PVC associated with the job.
	VolumeClaimTemplate StatefulJobVolumeClaimTemplate `json:"volumeClaimTemplate"`
}

// JobTemplateSpec is a subset of batchv1.JobSpec.
type JobTemplateSpec struct {
	// Specifies the duration in seconds relative to the startTime that the job
	// may be continuously active before the system tries to terminate it; value
	// must be positive integer.
	// Do not set too short or you will run into PVC/VolumeSnapshot provisioning rate limits.
	// Defaults to 24 hours.
	// +kubebuilder:validation:Minimum:=1
	// +optional
	ActiveDeadlineSeconds *int64 `json:"activeDeadlineSeconds"`

	// Specifies the number of retries before marking this job failed.
	// Defaults to 5.
	// +kubebuilder:validation:Minimum:=0
	// +optional
	BackoffLimit *int32 `json:"backoffLimit"`

	// Limits the lifetime of a Job that has finished
	// execution (either Complete or Failed). If this field is set,
	// ttlSecondsAfterFinished after the Job finishes, it is eligible to be
	// automatically deleted. When the Job is being deleted, its lifecycle
	// guarantees (e.g. finalizers) will be honored. If this field is set to zero,
	// the Job becomes eligible to be deleted immediately after it finishes.
	// Defaults to 15 minutes to allow some time to inspect logs.
	// +kubebuilder:validation:Minimum:=0
	// +optional
	TTLSecondsAfterFinished *int32 `json:"ttlSecondsAfterFinished"`
}

// StatefulJobVolumeClaimTemplate is a subset of a PersistentVolumeClaimTemplate
type StatefulJobVolumeClaimTemplate struct {
	// The StorageClass to use when creating a temporary PVC for processing the data.
	// On GKE, the StorageClass must be the same as the PVC's StorageClass from which the
	// VolumeSnapshot was created.
	StorageClassName string `json:"storageClassName"`

	// The desired access modes the volume should have.
	// More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#access-modes-1
	// Defaults to ReadWriteOnce.
	// +optional
	AccessModes []corev1.PersistentVolumeAccessMode `json:"accessModes"`
}

// StatefulJobStatus defines the observed state of StatefulJob
type StatefulJobStatus struct {
	// INSERT ADDITIONAL STATUS FIELD - define observed state of cluster
	// Important: Run "make" to regenerate code after modifying this file

	// The most recent generation observed by the controller.
	ObservedGeneration int64 `json:"observedGeneration"`

	// A generic message for the user. May contain errors.
	// +optional
	StatusMessage *string `json:"status"`

	// Last 5 job statuses created by the controller ordered by more recent jobs.
	// +optional
	JobHistory []batchv1.JobStatus `json:"jobHistory"`
}

//+kubebuilder:object:root=true
//+kubebuilder:subresource:status
//+kubebuilder:printcolumn:name="Age",type="date",JSONPath=".metadata.creationTimestamp"

// StatefulJob is the Schema for the statefuljobs API
type StatefulJob struct {
	metav1.TypeMeta   `json:",inline"`
	metav1.ObjectMeta `json:"metadata,omitempty"`

	Spec   StatefulJobSpec   `json:"spec,omitempty"`
	Status StatefulJobStatus `json:"status,omitempty"`
}

//+kubebuilder:object:root=true

// StatefulJobList contains a list of StatefulJob
type StatefulJobList struct {
	metav1.TypeMeta `json:",inline"`
	metav1.ListMeta `json:"metadata,omitempty"`
	Items           []StatefulJob `json:"items"`
}

'''
'''--- api/v1alpha1/zz_generated.deepcopy.go ---
//go:build !ignore_autogenerated
// +build !ignore_autogenerated

/*
Copyright 2022 Strangelove Ventures LLC.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

// Code generated by controller-gen. DO NOT EDIT.

package v1alpha1

import (
	"github.com/kubernetes-csi/external-snapshotter/client/v6/apis/volumesnapshot/v1"
	batchv1 "k8s.io/api/batch/v1"
	corev1 "k8s.io/api/core/v1"
	runtime "k8s.io/apimachinery/pkg/runtime"
)

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *JobTemplateSpec) DeepCopyInto(out *JobTemplateSpec) {
	*out = *in
	if in.ActiveDeadlineSeconds != nil {
		in, out := &in.ActiveDeadlineSeconds, &out.ActiveDeadlineSeconds
		*out = new(int64)
		**out = **in
	}
	if in.BackoffLimit != nil {
		in, out := &in.BackoffLimit, &out.BackoffLimit
		*out = new(int32)
		**out = **in
	}
	if in.TTLSecondsAfterFinished != nil {
		in, out := &in.TTLSecondsAfterFinished, &out.TTLSecondsAfterFinished
		*out = new(int32)
		**out = **in
	}
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new JobTemplateSpec.
func (in *JobTemplateSpec) DeepCopy() *JobTemplateSpec {
	if in == nil {
		return nil
	}
	out := new(JobTemplateSpec)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *LocalFullNodeRef) DeepCopyInto(out *LocalFullNodeRef) {
	*out = *in
	if in.Ordinal != nil {
		in, out := &in.Ordinal, &out.Ordinal
		*out = new(int32)
		**out = **in
	}
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new LocalFullNodeRef.
func (in *LocalFullNodeRef) DeepCopy() *LocalFullNodeRef {
	if in == nil {
		return nil
	}
	out := new(LocalFullNodeRef)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *ScheduledVolumeSnapshot) DeepCopyInto(out *ScheduledVolumeSnapshot) {
	*out = *in
	out.TypeMeta = in.TypeMeta
	in.ObjectMeta.DeepCopyInto(&out.ObjectMeta)
	in.Spec.DeepCopyInto(&out.Spec)
	in.Status.DeepCopyInto(&out.Status)
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new ScheduledVolumeSnapshot.
func (in *ScheduledVolumeSnapshot) DeepCopy() *ScheduledVolumeSnapshot {
	if in == nil {
		return nil
	}
	out := new(ScheduledVolumeSnapshot)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyObject is an autogenerated deepcopy function, copying the receiver, creating a new runtime.Object.
func (in *ScheduledVolumeSnapshot) DeepCopyObject() runtime.Object {
	if c := in.DeepCopy(); c != nil {
		return c
	}
	return nil
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *ScheduledVolumeSnapshotList) DeepCopyInto(out *ScheduledVolumeSnapshotList) {
	*out = *in
	out.TypeMeta = in.TypeMeta
	in.ListMeta.DeepCopyInto(&out.ListMeta)
	if in.Items != nil {
		in, out := &in.Items, &out.Items
		*out = make([]ScheduledVolumeSnapshot, len(*in))
		for i := range *in {
			(*in)[i].DeepCopyInto(&(*out)[i])
		}
	}
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new ScheduledVolumeSnapshotList.
func (in *ScheduledVolumeSnapshotList) DeepCopy() *ScheduledVolumeSnapshotList {
	if in == nil {
		return nil
	}
	out := new(ScheduledVolumeSnapshotList)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyObject is an autogenerated deepcopy function, copying the receiver, creating a new runtime.Object.
func (in *ScheduledVolumeSnapshotList) DeepCopyObject() runtime.Object {
	if c := in.DeepCopy(); c != nil {
		return c
	}
	return nil
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *ScheduledVolumeSnapshotSpec) DeepCopyInto(out *ScheduledVolumeSnapshotSpec) {
	*out = *in
	in.FullNodeRef.DeepCopyInto(&out.FullNodeRef)
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new ScheduledVolumeSnapshotSpec.
func (in *ScheduledVolumeSnapshotSpec) DeepCopy() *ScheduledVolumeSnapshotSpec {
	if in == nil {
		return nil
	}
	out := new(ScheduledVolumeSnapshotSpec)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *ScheduledVolumeSnapshotStatus) DeepCopyInto(out *ScheduledVolumeSnapshotStatus) {
	*out = *in
	if in.StatusMessage != nil {
		in, out := &in.StatusMessage, &out.StatusMessage
		*out = new(string)
		**out = **in
	}
	in.CreatedAt.DeepCopyInto(&out.CreatedAt)
	if in.Candidate != nil {
		in, out := &in.Candidate, &out.Candidate
		*out = new(SnapshotCandidate)
		(*in).DeepCopyInto(*out)
	}
	if in.LastSnapshot != nil {
		in, out := &in.LastSnapshot, &out.LastSnapshot
		*out = new(VolumeSnapshotStatus)
		(*in).DeepCopyInto(*out)
	}
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new ScheduledVolumeSnapshotStatus.
func (in *ScheduledVolumeSnapshotStatus) DeepCopy() *ScheduledVolumeSnapshotStatus {
	if in == nil {
		return nil
	}
	out := new(ScheduledVolumeSnapshotStatus)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *SnapshotCandidate) DeepCopyInto(out *SnapshotCandidate) {
	*out = *in
	if in.PodLabels != nil {
		in, out := &in.PodLabels, &out.PodLabels
		*out = make(map[string]string, len(*in))
		for key, val := range *in {
			(*out)[key] = val
		}
	}
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new SnapshotCandidate.
func (in *SnapshotCandidate) DeepCopy() *SnapshotCandidate {
	if in == nil {
		return nil
	}
	out := new(SnapshotCandidate)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *StatefulJob) DeepCopyInto(out *StatefulJob) {
	*out = *in
	out.TypeMeta = in.TypeMeta
	in.ObjectMeta.DeepCopyInto(&out.ObjectMeta)
	in.Spec.DeepCopyInto(&out.Spec)
	in.Status.DeepCopyInto(&out.Status)
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new StatefulJob.
func (in *StatefulJob) DeepCopy() *StatefulJob {
	if in == nil {
		return nil
	}
	out := new(StatefulJob)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyObject is an autogenerated deepcopy function, copying the receiver, creating a new runtime.Object.
func (in *StatefulJob) DeepCopyObject() runtime.Object {
	if c := in.DeepCopy(); c != nil {
		return c
	}
	return nil
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *StatefulJobList) DeepCopyInto(out *StatefulJobList) {
	*out = *in
	out.TypeMeta = in.TypeMeta
	in.ListMeta.DeepCopyInto(&out.ListMeta)
	if in.Items != nil {
		in, out := &in.Items, &out.Items
		*out = make([]StatefulJob, len(*in))
		for i := range *in {
			(*in)[i].DeepCopyInto(&(*out)[i])
		}
	}
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new StatefulJobList.
func (in *StatefulJobList) DeepCopy() *StatefulJobList {
	if in == nil {
		return nil
	}
	out := new(StatefulJobList)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyObject is an autogenerated deepcopy function, copying the receiver, creating a new runtime.Object.
func (in *StatefulJobList) DeepCopyObject() runtime.Object {
	if c := in.DeepCopy(); c != nil {
		return c
	}
	return nil
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *StatefulJobSpec) DeepCopyInto(out *StatefulJobSpec) {
	*out = *in
	if in.Selector != nil {
		in, out := &in.Selector, &out.Selector
		*out = make(map[string]string, len(*in))
		for key, val := range *in {
			(*out)[key] = val
		}
	}
	out.Interval = in.Interval
	in.JobTemplate.DeepCopyInto(&out.JobTemplate)
	in.PodTemplate.DeepCopyInto(&out.PodTemplate)
	in.VolumeClaimTemplate.DeepCopyInto(&out.VolumeClaimTemplate)
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new StatefulJobSpec.
func (in *StatefulJobSpec) DeepCopy() *StatefulJobSpec {
	if in == nil {
		return nil
	}
	out := new(StatefulJobSpec)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *StatefulJobStatus) DeepCopyInto(out *StatefulJobStatus) {
	*out = *in
	if in.StatusMessage != nil {
		in, out := &in.StatusMessage, &out.StatusMessage
		*out = new(string)
		**out = **in
	}
	if in.JobHistory != nil {
		in, out := &in.JobHistory, &out.JobHistory
		*out = make([]batchv1.JobStatus, len(*in))
		for i := range *in {
			(*in)[i].DeepCopyInto(&(*out)[i])
		}
	}
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new StatefulJobStatus.
func (in *StatefulJobStatus) DeepCopy() *StatefulJobStatus {
	if in == nil {
		return nil
	}
	out := new(StatefulJobStatus)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *StatefulJobVolumeClaimTemplate) DeepCopyInto(out *StatefulJobVolumeClaimTemplate) {
	*out = *in
	if in.AccessModes != nil {
		in, out := &in.AccessModes, &out.AccessModes
		*out = make([]corev1.PersistentVolumeAccessMode, len(*in))
		copy(*out, *in)
	}
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new StatefulJobVolumeClaimTemplate.
func (in *StatefulJobVolumeClaimTemplate) DeepCopy() *StatefulJobVolumeClaimTemplate {
	if in == nil {
		return nil
	}
	out := new(StatefulJobVolumeClaimTemplate)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *VolumeSnapshotStatus) DeepCopyInto(out *VolumeSnapshotStatus) {
	*out = *in
	in.StartedAt.DeepCopyInto(&out.StartedAt)
	if in.Status != nil {
		in, out := &in.Status, &out.Status
		*out = new(v1.VolumeSnapshotStatus)
		(*in).DeepCopyInto(*out)
	}
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new VolumeSnapshotStatus.
func (in *VolumeSnapshotStatus) DeepCopy() *VolumeSnapshotStatus {
	if in == nil {
		return nil
	}
	out := new(VolumeSnapshotStatus)
	in.DeepCopyInto(out)
	return out
}

'''
'''--- cmd/healtcheck.go ---
package cmd

import (
	"context"
	"fmt"
	"net/http"
	"time"

	"github.com/go-logr/zapr"
	"github.com/spf13/cobra"
	"github.com/spf13/viper"
	"github.com/strangelove-ventures/cosmos-operator/internal/cosmos"
	"github.com/strangelove-ventures/cosmos-operator/internal/healthcheck"
	"golang.org/x/sync/errgroup"
)

func HealthCheckCmd() *cobra.Command {
	hc := &cobra.Command{
		Short:        "Start health check probe",
		Use:          "healthcheck",
		RunE:         startHealthCheckServer,
		SilenceUsage: true,
	}

	hc.Flags().String("rpc-host", "http://localhost:26657", "CometBFT rpc endpoint")
	hc.Flags().String("log-format", "console", "'console' or 'json'")
	hc.Flags().Duration("timeout", 5*time.Second, "how long to wait before timing out requests to rpc-host")
	hc.Flags().String("addr", fmt.Sprintf(":%d", healthcheck.Port), "listen address for server to bind")

	if err := viper.BindPFlags(hc.Flags()); err != nil {
		panic(err)
	}

	return hc
}

func startHealthCheckServer(cmd *cobra.Command, args []string) error {
	var (
		listenAddr = viper.GetString("addr")
		rpcHost    = viper.GetString("rpc-host")
		timeout    = viper.GetDuration("timeout")

		httpClient  = &http.Client{Timeout: 30 * time.Second}
		cometClient = cosmos.NewCometClient(httpClient)

		zlog   = ZapLogger("info", viper.GetString("log-format"))
		logger = zapr.NewLogger(zlog)
	)
	defer func() { _ = zlog.Sync() }()

	mux := http.NewServeMux()
	mux.Handle("/", healthcheck.NewComet(logger, cometClient, rpcHost, timeout))
	mux.HandleFunc("/disk", healthcheck.DiskUsage)

	srv := &http.Server{
		Addr:         listenAddr,
		Handler:      mux,
		ReadTimeout:  30 * time.Second,
		WriteTimeout: 30 * time.Second,
	}

	var eg errgroup.Group
	eg.Go(func() error {
		logger.Info("Healthcheck server listening", "addr", listenAddr, "rpcHost", rpcHost)
		return srv.ListenAndServe()
	})
	eg.Go(func() error {
		<-cmd.Context().Done()
		logger.Info("Healthcheck server shutting down")
		ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
		defer cancel()
		return srv.Shutdown(ctx)
	})

	return eg.Wait()
}

'''
'''--- cmd/logger.go ---
package cmd

import (
	"os"
	"time"

	"go.uber.org/zap"
	"go.uber.org/zap/zapcore"
)

func ZapLogger(level, format string) *zap.Logger {
	config := zap.NewProductionEncoderConfig()
	config.EncodeTime = func(ts time.Time, encoder zapcore.PrimitiveArrayEncoder) {
		encoder.AppendString(ts.UTC().Format(time.RFC3339))
	}
	enc := zapcore.NewConsoleEncoder(config)
	if format == "json" {
		enc = zapcore.NewJSONEncoder(config)
	}

	lvl := zap.NewAtomicLevel()
	if err := lvl.UnmarshalText([]byte(level)); err != nil {
		lvl = zap.NewAtomicLevelAt(zap.InfoLevel)
	}
	return zap.New(zapcore.NewCore(enc, os.Stdout, lvl))
}

'''
'''--- cmd/versioncheck.go ---
/*
Copyright Â© 2023 Strangelove Crypto, Inc.
*/
package cmd

import (
	"context"
	"fmt"
	"io"
	"os"
	"time"

	"cosmossdk.io/log"
	"cosmossdk.io/store/rootmulti"
	dbm "github.com/cosmos/cosmos-db"
	"github.com/spf13/cobra"
	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/types"
	"k8s.io/client-go/kubernetes"
	"k8s.io/client-go/rest"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

const (
	namespaceFile = "/var/run/secrets/kubernetes.io/serviceaccount/namespace"

	flagBackend = "backend"
	flagDaemon  = "daemon"

	tickTime = 30 * time.Second
)

// VersionCheckCmd gets the height of this node and updates the status of the crd.
// It panics if the wrong image is specified for the pod for the height,
// restarting the pod so that the correct image is used from the patched height.
// this command is intended to be run as an init container.
func VersionCheckCmd(scheme *runtime.Scheme) *cobra.Command {
	cmd := &cobra.Command{
		Use:   "versioncheck",
		Short: "Confirm correct image used for current node height",
		Long:  `Open the Cosmos SDK chain database, get the height, update the crd status with the height, then check the image for the height and panic if it is incorrect.`,
		Run: func(cmd *cobra.Command, args []string) {
			dataDir := os.Getenv("DATA_DIR")
			backend, _ := cmd.Flags().GetString(flagBackend)
			daemon, _ := cmd.Flags().GetBool(flagDaemon)

			nsbz, err := os.ReadFile(namespaceFile)
			if err != nil {
				panic(fmt.Errorf("failed to read namespace from service account: %w", err))
			}
			ns := string(nsbz)

			config, err := rest.InClusterConfig()
			if err != nil {
				panic(fmt.Errorf("failed to get in cluster config: %w", err))
			}

			clientset, err := kubernetes.NewForConfig(config)
			if err != nil {
				panic(fmt.Errorf("failed to create kube clientset: %w", err))
			}

			ctx := cmd.Context()

			thisPod, err := clientset.CoreV1().Pods(ns).Get(ctx, os.Getenv("HOSTNAME"), metav1.GetOptions{})
			if err != nil {
				panic(fmt.Errorf("failed to get this pod: %w", err))
			}

			cosmosFullNodeName := thisPod.Labels["app.kubernetes.io/name"]

			kClient, err := client.New(config, client.Options{
				Scheme: scheme,
			})
			if err != nil {
				panic(fmt.Errorf("failed to create kube client: %w", err))
			}

			namespacedName := types.NamespacedName{
				Namespace: ns,
				Name:      cosmosFullNodeName,
			}

			crd := new(cosmosv1.CosmosFullNode)
			if err = kClient.Get(ctx, namespacedName, crd); err != nil {
				panic(fmt.Errorf("failed to get crd: %w", err))
			}

			if len(crd.Spec.ChainSpec.Versions) == 0 {
				fmt.Fprintln(cmd.OutOrStdout(), "No versions specified, skipping version check")
				return
			}

			s, err := os.Stat(dataDir)
			if err != nil {
				panic(fmt.Errorf("failed to stat %s: %w", dataDir, err))
			}

			if !s.IsDir() {
				panic(fmt.Errorf("%s is not a directory", dataDir))
			}

			if daemon {
				ticker := time.NewTicker(tickTime)
				defer ticker.Stop()
				for {
					select {
					case <-cmd.Context().Done():
						return
					case <-ticker.C:
						if err := checkVersion(cmd.Context(), nil, kClient, namespacedName, thisPod, dataDir, backend, cmd.OutOrStdout()); err != nil {
							panic(err)
						}
						ticker.Reset(tickTime)
					}
				}
			}
			if err := checkVersion(cmd.Context(), crd, kClient, namespacedName, thisPod, dataDir, backend, cmd.OutOrStdout()); err != nil {
				panic(err)
			}
		},
	}

	cmd.Flags().StringP(flagBackend, "b", "goleveldb", "Database backend")
	cmd.Flags().BoolP(flagDaemon, "d", false, "Run as daemon")

	return cmd
}

func checkVersion(
	ctx context.Context,
	crd *cosmosv1.CosmosFullNode,
	kClient client.Client,
	namespacedName types.NamespacedName,
	thisPod *corev1.Pod,
	dataDir string,
	backend string,
	writer io.Writer,
) error {
	db, err := dbm.NewDB("application", getBackend(backend), dataDir)
	if err != nil {
		if crd == nil {
			fmt.Fprintf(writer, "Failed to open db: %s. The node is likely running.\n", err)
			// This is okay, we will read it later if the node shuts down.
			return nil
		} else {
			return fmt.Errorf("failed to open db: %w", err)
		}
	}
	store := rootmulti.NewStore(db, log.NewNopLogger(), nil)

	height := store.LatestVersion() + 1
	db.Close()

	if crd == nil {
		crd = new(cosmosv1.CosmosFullNode)
		if err := kClient.Get(ctx, namespacedName, crd); err != nil {
			return fmt.Errorf("failed to get crd: %w", err)
		}
	}

	if err := patchStatusHeightIfNecessary(ctx, kClient, crd, thisPod.Name, uint64(height)); err != nil {
		return err
	}

	var image string
	for _, v := range crd.Spec.ChainSpec.Versions {
		if uint64(height) < v.UpgradeHeight {
			break
		}
		image = v.Image
	}

	thisPodImage := thisPod.Spec.Containers[0].Image
	if thisPodImage != image {
		return fmt.Errorf("image mismatch for height %d: %s != %s", height, thisPodImage, image)
	}

	fmt.Fprintf(writer, "Verified correct image for height %d: %s\n", height, image)

	return nil
}

func patchStatusHeightIfNecessary(
	ctx context.Context,
	kClient client.Client,
	crd *cosmosv1.CosmosFullNode,
	instanceName string,
	height uint64,
) error {
	if crd.Status.Height != nil {
		if h, ok := crd.Status.Height[instanceName]; ok && h == height {
			// Status is up to date already.
			return nil
		}
	}

	patch := crd.DeepCopy()
	if patch.Status.Height == nil {
		patch.Status.Height = make(map[string]uint64)
	}
	patch.Status.Height[instanceName] = height

	if err := kClient.Status().Patch(
		ctx, patch, client.MergeFrom(crd.DeepCopy()),
	); err != nil {
		return fmt.Errorf("failed to patch status: %w", err)
	}

	return nil
}

func getBackend(backend string) dbm.BackendType {
	switch backend {
	case "goleveldb":
		return dbm.GoLevelDBBackend
	case "memdb":
		return dbm.MemDBBackend
	case "rocksdb":
		return dbm.RocksDBBackend
	case "pebbledb":
		return dbm.PebbleDBBackend
	default:
		panic(fmt.Errorf("unknown backend %s", backend))
	}
}

'''
'''--- controllers/cosmosfullnode_controller.go ---
/*
Copyright 2022 Strangelove Ventures LLC.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package controllers

import (
	"context"
	"fmt"
	"time"

	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	"github.com/strangelove-ventures/cosmos-operator/internal/cosmos"
	"github.com/strangelove-ventures/cosmos-operator/internal/fullnode"
	"github.com/strangelove-ventures/cosmos-operator/internal/kube"
	corev1 "k8s.io/api/core/v1"
	"k8s.io/client-go/tools/record"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/builder"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/event"
	"sigs.k8s.io/controller-runtime/pkg/handler"
	"sigs.k8s.io/controller-runtime/pkg/log"
	"sigs.k8s.io/controller-runtime/pkg/predicate"
	"sigs.k8s.io/controller-runtime/pkg/source"
)

const controllerOwnerField = ".metadata.controller"

// CosmosFullNodeReconciler reconciles a CosmosFullNode object
type CosmosFullNodeReconciler struct {
	client.Client

	cacheController           *cosmos.CacheController
	configMapControl          fullnode.ConfigMapControl
	nodeKeyControl            fullnode.NodeKeyControl
	peerCollector             *fullnode.PeerCollector
	podControl                fullnode.PodControl
	pvcControl                fullnode.PVCControl
	recorder                  record.EventRecorder
	serviceControl            fullnode.ServiceControl
	statusClient              *fullnode.StatusClient
	serviceAccountControl     fullnode.ServiceAccountControl
	clusterRoleControl        fullnode.RoleControl
	clusterRoleBindingControl fullnode.RoleBindingControl
}

// NewFullNode returns a valid CosmosFullNode controller.
func NewFullNode(
	client client.Client,
	recorder record.EventRecorder,
	statusClient *fullnode.StatusClient,
	cacheController *cosmos.CacheController,
) *CosmosFullNodeReconciler {
	return &CosmosFullNodeReconciler{
		Client: client,

		cacheController:           cacheController,
		configMapControl:          fullnode.NewConfigMapControl(client),
		nodeKeyControl:            fullnode.NewNodeKeyControl(client),
		peerCollector:             fullnode.NewPeerCollector(client),
		podControl:                fullnode.NewPodControl(client, cacheController),
		pvcControl:                fullnode.NewPVCControl(client),
		recorder:                  recorder,
		serviceControl:            fullnode.NewServiceControl(client),
		statusClient:              statusClient,
		serviceAccountControl:     fullnode.NewServiceAccountControl(client),
		clusterRoleControl:        fullnode.NewRoleControl(client),
		clusterRoleBindingControl: fullnode.NewRoleBindingControl(client),
	}
}

var (
	stopResult    ctrl.Result
	requeueResult = ctrl.Result{RequeueAfter: 3 * time.Second}
)

//+kubebuilder:rbac:groups=cosmos.strange.love,resources=cosmosfullnodes,verbs=get;list;watch;create;update;patch;delete
//+kubebuilder:rbac:groups=cosmos.strange.love,resources=cosmosfullnodes/status,verbs=get;update;patch
//+kubebuilder:rbac:groups=cosmos.strange.love,resources=cosmosfullnodes/finalizers,verbs=update
// Generate RBAC roles to watch and update resources. IMPORTANT!!!! All resource names must be lowercase or cluster role will not work.
//+kubebuilder:rbac:groups="",resources=pods;persistentvolumeclaims;services;serviceaccounts;configmaps,verbs=get;list;watch;create;update;patch;delete
//+kubebuilder:rbac:groups="rbac.authorization.k8s.io",resources=roles;rolebindings,verbs=get;list;watch;create;update;patch;delete;bind;escalate
//+kubebuilder:rbac:groups="",resources=secrets,verbs=get;list;watch;create;update;patch
//+kubebuilder:rbac:groups="",resources=events,verbs=create;update;patch

// Reconcile is part of the main kubernetes reconciliation loop which aims to
// move the current state of the cluster closer to the desired state.
//
// For more details, check Reconcile and its Result here:
// - https://pkg.go.dev/sigs.k8s.io/controller-runtime@v0.12.1/pkg/reconcile
func (r *CosmosFullNodeReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
	logger := log.FromContext(ctx)
	logger.V(1).Info("Entering reconcile loop", "request", req.NamespacedName)

	// Get the CRD
	crd := new(cosmosv1.CosmosFullNode)
	if err := r.Get(ctx, req.NamespacedName, crd); err != nil {
		// Ignore not found errors because can't be fixed by an immediate requeue. We'll have to wait for next notification.
		// Also, will get "not found" error if crd is deleted.
		// No need to explicitly delete resources. Kube GC does so automatically because we set the controller reference
		// for each resource.
		return stopResult, client.IgnoreNotFound(err)
	}

	reporter := kube.NewEventReporter(logger, r.recorder, crd)

	fullnode.ResetStatus(crd)

	syncInfo := fullnode.SyncInfoStatus(ctx, crd, r.cacheController)

	pvcStatusChanges := fullnode.PVCStatusChanges{}

	defer r.updateStatus(ctx, crd, syncInfo, &pvcStatusChanges)

	errs := &kube.ReconcileErrors{}

	// Order of operations is important. E.g. PVCs won't delete unless pods are deleted first.
	// K8S can create pods first even if the PVC isn't ready. Pods won't be in a ready state until PVC is bound.

	// Create or update Services.
	err := r.serviceControl.Reconcile(ctx, reporter, crd)
	if err != nil {
		errs.Append(err)
	}

	// Reconcile Secrets.
	err = r.nodeKeyControl.Reconcile(ctx, reporter, crd)
	if err != nil {
		errs.Append(err)
	}

	// Find peer information that's used downstream.
	peers, perr := r.peerCollector.Collect(ctx, crd)
	if perr != nil {
		peers = peers.Default()
		errs.Append(perr)
	}

	// Reconcile ConfigMaps.
	configCksums, err := r.configMapControl.Reconcile(ctx, reporter, crd, peers)
	if err != nil {
		errs.Append(err)
	}

	// Reconcile service accounts.
	err = r.serviceAccountControl.Reconcile(ctx, reporter, crd)
	if err != nil {
		errs.Append(err)
	}

	// Reconcile cluster roles.
	err = r.clusterRoleControl.Reconcile(ctx, reporter, crd)
	if err != nil {
		errs.Append(err)
	}

	// Reconcile cluster role bindings.
	err = r.clusterRoleBindingControl.Reconcile(ctx, reporter, crd)
	if err != nil {
		errs.Append(err)
	}

	// Reconcile pods.
	podRequeue, err := r.podControl.Reconcile(ctx, reporter, crd, configCksums, syncInfo)
	if err != nil {
		errs.Append(err)
	}

	// Reconcile pvcs.
	pvcRequeue, err := r.pvcControl.Reconcile(ctx, reporter, crd, &pvcStatusChanges)
	if err != nil {
		errs.Append(err)
	}

	if errs.Any() {
		return r.resultWithErr(crd, errs)
	}

	if podRequeue || pvcRequeue {
		return requeueResult, nil
	}

	// Check final state and requeue if necessary.
	if peers.HasIncompleteExternalAddress() {
		reporter.Info("Requeueing due to incomplete p2p external addresses")
		reporter.RecordInfo("P2PIncomplete", "Waiting for p2p service IPs or Hostnames to be ready.")
		crd.Status.Phase = cosmosv1.FullNodePhaseP2PServices
		// Allow more time to requeue while p2p services create their load balancers.
		return ctrl.Result{RequeueAfter: 30 * time.Second}, nil
	}

	crd.Status.Peers = peers.AllExternal()

	crd.Status.Phase = cosmosv1.FullNodePhaseCompete
	// Requeue to constantly poll consensus state.
	return ctrl.Result{RequeueAfter: 60 * time.Second}, nil
}

func (r *CosmosFullNodeReconciler) resultWithErr(crd *cosmosv1.CosmosFullNode, err kube.ReconcileError) (ctrl.Result, kube.ReconcileError) {
	if err.IsTransient() {
		r.recorder.Event(crd, kube.EventWarning, "ErrorTransient", fmt.Sprintf("%v; retrying.", err))
		crd.Status.StatusMessage = ptr(fmt.Sprintf("Transient error: system is retrying: %v", err))
		crd.Status.Phase = cosmosv1.FullNodePhaseTransientError
		return requeueResult, err
	}

	crd.Status.Phase = cosmosv1.FullNodePhaseError
	crd.Status.StatusMessage = ptr(fmt.Sprintf("Unrecoverable error: human intervention required: %v", err))
	r.recorder.Event(crd, kube.EventWarning, "Error", err.Error())
	return stopResult, err
}

func (r *CosmosFullNodeReconciler) updateStatus(
	ctx context.Context,
	crd *cosmosv1.CosmosFullNode,
	syncInfo map[string]*cosmosv1.SyncInfoPodStatus,
	pvcStatusChanges *fullnode.PVCStatusChanges,
) {
	if err := r.statusClient.SyncUpdate(ctx, client.ObjectKeyFromObject(crd), func(status *cosmosv1.FullNodeStatus) {
		status.ObservedGeneration = crd.Status.ObservedGeneration
		status.Phase = crd.Status.Phase
		status.StatusMessage = crd.Status.StatusMessage
		status.Peers = crd.Status.Peers
		status.SyncInfo = syncInfo
		for k, v := range syncInfo {
			if v.Height != nil && *v.Height > 0 {
				if status.Height == nil {
					status.Height = make(map[string]uint64)
				}
				status.Height[k] = *v.Height + 1 // we want the block that is going through consensus, not the committed one.
			}
		}
		if status.SelfHealing.PVCAutoScale != nil {
			for _, k := range pvcStatusChanges.Deleted {
				delete(status.SelfHealing.PVCAutoScale, k)
			}
		}
	}); err != nil {
		log.FromContext(ctx).Error(err, "Failed to patch status")
	}
}

// SetupWithManager sets up the controller with the Manager.
func (r *CosmosFullNodeReconciler) SetupWithManager(ctx context.Context, mgr ctrl.Manager) error {
	// Index pods.
	err := mgr.GetFieldIndexer().IndexField(
		ctx,
		&corev1.Pod{},
		controllerOwnerField,
		kube.IndexOwner[*corev1.Pod](cosmosv1.CosmosFullNodeController),
	)
	if err != nil {
		return fmt.Errorf("pod index field %s: %w", controllerOwnerField, err)
	}

	// Index PVCs.
	err = mgr.GetFieldIndexer().IndexField(
		ctx,
		&corev1.PersistentVolumeClaim{},
		controllerOwnerField,
		kube.IndexOwner[*corev1.PersistentVolumeClaim](cosmosv1.CosmosFullNodeController),
	)
	if err != nil {
		return fmt.Errorf("pvc index field %s: %w", controllerOwnerField, err)
	}

	// Index ConfigMaps.
	err = mgr.GetFieldIndexer().IndexField(
		ctx,
		&corev1.ConfigMap{},
		controllerOwnerField,
		kube.IndexOwner[*corev1.ConfigMap](cosmosv1.CosmosFullNodeController),
	)
	if err != nil {
		return fmt.Errorf("configmap index field %s: %w", controllerOwnerField, err)
	}

	// Index Secrets.
	err = mgr.GetFieldIndexer().IndexField(
		ctx,
		&corev1.Secret{},
		controllerOwnerField,
		kube.IndexOwner[*corev1.Secret](cosmosv1.CosmosFullNodeController),
	)
	if err != nil {
		return fmt.Errorf("secret index field %s: %w", controllerOwnerField, err)
	}

	// Index Services.
	err = mgr.GetFieldIndexer().IndexField(
		ctx,
		&corev1.Service{},
		controllerOwnerField,
		kube.IndexOwner[*corev1.Service](cosmosv1.CosmosFullNodeController),
	)
	if err != nil {
		return fmt.Errorf("service index field %s: %w", controllerOwnerField, err)
	}

	cbuilder := ctrl.NewControllerManagedBy(mgr).For(&cosmosv1.CosmosFullNode{})

	// Watch for delete events for certain resources.
	for _, kind := range []*source.Kind{
		{Type: &corev1.Pod{}},
		{Type: &corev1.PersistentVolumeClaim{}},
		{Type: &corev1.ConfigMap{}},
		{Type: &corev1.Service{}},
		{Type: &corev1.Secret{}},
	} {
		cbuilder.Watches(
			kind,
			&handler.EnqueueRequestForOwner{OwnerType: &cosmosv1.CosmosFullNode{}, IsController: true},
			builder.WithPredicates(&predicate.Funcs{
				DeleteFunc: func(_ event.DeleteEvent) bool { return true },
			}),
		)
	}

	return cbuilder.Complete(r)
}

'''
'''--- controllers/ptr.go ---
package controllers

func ptr[T any](v T) *T {
	return &v
}

'''
'''--- controllers/scheduledvolumesnapshot_controller.go ---
/*
Copyright 2022 Strangelove Ventures LLC.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package controllers

import (
	"context"
	"fmt"
	"time"

	"github.com/go-logr/logr"
	cosmosv1alpha1 "github.com/strangelove-ventures/cosmos-operator/api/v1alpha1"
	"github.com/strangelove-ventures/cosmos-operator/internal/cosmos"
	"github.com/strangelove-ventures/cosmos-operator/internal/fullnode"
	"github.com/strangelove-ventures/cosmos-operator/internal/kube"
	"github.com/strangelove-ventures/cosmos-operator/internal/volsnapshot"
	"k8s.io/client-go/tools/record"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/log"
)

// ScheduledVolumeSnapshotReconciler reconciles a ScheduledVolumeSnapshot object
type ScheduledVolumeSnapshotReconciler struct {
	client.Client
	fullNodeControl       *volsnapshot.FullNodeControl
	missingVolSnapshotCRD bool
	recorder              record.EventRecorder
	scheduler             *volsnapshot.Scheduler
	volSnapshotControl    *volsnapshot.VolumeSnapshotControl
}

func NewScheduledVolumeSnapshotReconciler(
	client client.Client,
	recorder record.EventRecorder,
	statusClient *fullnode.StatusClient,
	cache *cosmos.CacheController,
	missingVolSnapCRD bool,
) *ScheduledVolumeSnapshotReconciler {
	return &ScheduledVolumeSnapshotReconciler{
		Client:                client,
		fullNodeControl:       volsnapshot.NewFullNodeControl(statusClient, client),
		missingVolSnapshotCRD: missingVolSnapCRD,
		recorder:              recorder,
		scheduler:             volsnapshot.NewScheduler(client),
		volSnapshotControl:    volsnapshot.NewVolumeSnapshotControl(client, cache),
	}
}

//+kubebuilder:rbac:groups=cosmos.strange.love,resources=scheduledvolumesnapshots,verbs=get;list;watch;create;update;patch;delete
//+kubebuilder:rbac:groups=cosmos.strange.love,resources=scheduledvolumesnapshots/status,verbs=get;update;patch
//+kubebuilder:rbac:groups=cosmos.strange.love,resources=scheduledvolumesnapshots/finalizers,verbs=update
//+kubebuilder:rbac:groups=cosmos.strange.love,resources=cosmosfullnodes/status,verbs=get;update;patch
//+kubebuilder:rbac:groups=snapshot.storage.k8s.io,resources=volumesnapshots,verbs=get;create;delete
//+kubebuilder:rbac:groups="",resources=pods,verbs=get;list;watch

// Reconcile is part of the main kubernetes reconciliation loop which aims to
// move the current state of the cluster closer to the desired state.
func (r *ScheduledVolumeSnapshotReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
	logger := log.FromContext(ctx)
	logger.V(1).Info("Entering reconcile loop", "request", req.NamespacedName)

	// Get the CRD
	crd := new(cosmosv1alpha1.ScheduledVolumeSnapshot)
	if err := r.Get(ctx, req.NamespacedName, crd); err != nil {
		// Ignore not found errors because can't be fixed by an immediate requeue. We'll have to wait for next notification.
		// Also, will get "not found" error if crd is deleted.
		// No need to explicitly delete resources. Kube GC does so automatically because we set the controller reference
		// for each resource.
		return stopResult, client.IgnoreNotFound(err)
	}

	volsnapshot.ResetStatus(crd)
	defer r.updateStatus(ctx, crd)

	if r.missingVolSnapshotCRD {
		logger.Error(errMissingVolSnapCRD, "Controller is disabled")
		r.reportError(crd, "MissingCRDs", errMissingVolSnapCRD)
		crd.Status.Phase = cosmosv1alpha1.SnapshotPhaseMissingCRDs
		return ctrl.Result{}, nil
	}

	retryResult := ctrl.Result{RequeueAfter: 10 * time.Second}

	phase := crd.Status.Phase
	switch phase {
	case cosmosv1alpha1.SnapshotPhaseWaitingForNext:
		logger.Info(string(phase))

		if err := r.volSnapshotControl.DeleteOldSnapshots(ctx, logger, crd); err != nil {
			logger.Error(err, "Failed to delete old volume snapshots")
			r.reportError(crd, "DeleteOldSnapshotsError", err)
			// Don't requeue, continue with next steps. Leaving old snapshots around is benign.
		}

		dur, err := r.scheduler.CalcNext(crd)
		if err != nil {
			logger.Error(err, "Failed to find duration until next snapshot")
			r.reportError(crd, "FindNextSnapshotTimeError", err)
			return stopResult, nil // Fatal error. Do not requeue.
		}

		if dur > 0 {
			logger.Info("Requeuing for next snapshot", "duration", dur.String())
			return ctrl.Result{RequeueAfter: dur}, nil
		}

		crd.Status.Phase = cosmosv1alpha1.SnapshotPhaseFindingCandidate

	case cosmosv1alpha1.SnapshotPhaseFindingCandidate:
		logger.Info(string(phase))
		candidate, err := r.volSnapshotControl.FindCandidate(ctx, crd)
		if err != nil {
			logger.Error(err, "Failed to find candidate for volume snapshot")
			r.reportError(crd, "FindCandidateError", err)
			return retryResult, nil
		}
		crd.Status.Candidate = &candidate
		crd.Status.Phase = cosmosv1alpha1.SnapshotPhaseCreating
		if crd.Spec.DeletePod {
			crd.Status.Phase = cosmosv1alpha1.SnapshotPhaseDeletingPod
		}

	case cosmosv1alpha1.SnapshotPhaseDeletingPod:
		logger.Info(string(phase))
		if err := r.fullNodeControl.SignalPodDeletion(ctx, crd); err != nil {
			logger.Error(err, "Failed to patch fullnode status for pod deletion")
			r.reportError(crd, "DeletePodError", err)
			return retryResult, nil
		}
		crd.Status.Phase = cosmosv1alpha1.SnapshotPhaseWaitingForPodDeletion

	case cosmosv1alpha1.SnapshotPhaseWaitingForPodDeletion:
		logger.Info(string(phase))
		if err := r.fullNodeControl.ConfirmPodDeletion(ctx, crd); err != nil {
			logger.Error(err, "Failed to confirm pod deletion", "candidatePod", crd.Status.Candidate.PodName)
			r.reportError(crd, "WaitingForPodDeletionError", err)
			return retryResult, nil
		}
		crd.Status.Phase = cosmosv1alpha1.SnapshotPhaseCreating

	case cosmosv1alpha1.SnapshotPhaseCreating:
		candidate := crd.Status.Candidate
		logger.Info(string(phase), "candidatePod", candidate.PodName, "candidatePVC", candidate.PVCName)
		if err := r.volSnapshotControl.CreateSnapshot(ctx, crd, *candidate); err != nil {
			logger.Error(err, "Failed to create volume snapshot")
			r.reportError(crd, "CreateVolumeSnapshotError", err)
			return retryResult, nil
		}
		crd.Status.Phase = cosmosv1alpha1.SnapshotPhaseWaitingForCreation

	case cosmosv1alpha1.SnapshotPhaseWaitingForCreation:
		logger.Info(string(phase))
		ready, err := r.scheduler.IsSnapshotReady(ctx, crd)
		if err != nil {
			logger.Error(err, "Failed to find VolumeSnapshot ready status")
			r.reportError(crd, "VolumeSnapshotReadyError", err)
			return retryResult, nil
		}
		if !ready {
			logger.Info("VolumeSnapshot not ready for use; requeueing")
			return retryResult, nil
		}
		crd.Status.Phase = cosmosv1alpha1.SnapshotPhaseRestorePod

	case cosmosv1alpha1.SnapshotPhaseRestorePod:
		logger.Info(string(phase))
		if err := r.restorePod(ctx, logger, crd); err != nil {
			return retryResult, nil
		}
		if crd.Spec.Suspend {
			crd.Status.Phase = cosmosv1alpha1.SnapshotPhaseSuspended
		} else {
			// Reset to beginning.
			crd.Status.Phase = cosmosv1alpha1.SnapshotPhaseWaitingForNext
		}
	}

	// Updating status in the defer above triggers a new reconcile loop.
	return stopResult, nil
}

func (r *ScheduledVolumeSnapshotReconciler) restorePod(ctx context.Context, logger logr.Logger, crd *cosmosv1alpha1.ScheduledVolumeSnapshot) error {
	if err := r.fullNodeControl.ConfirmPodRestoration(ctx, crd); err != nil {
		logger.Info("Pod not restored; signaling fullnode to restore pod", "error", err)
		if err = r.fullNodeControl.SignalPodRestoration(ctx, crd); err != nil {
			logger.Error(err, "Failed to update fullnode status for restoring pod")
			r.reportError(crd, "RestorePodError", err)
			return err
		}
	}
	return nil
}

func (r *ScheduledVolumeSnapshotReconciler) reportError(crd *cosmosv1alpha1.ScheduledVolumeSnapshot, reason string, err error) {
	r.recorder.Event(crd, kube.EventWarning, reason, err.Error())
	crd.Status.StatusMessage = ptr(fmt.Sprint("Error: ", err))
}

func (r *ScheduledVolumeSnapshotReconciler) updateStatus(ctx context.Context, crd *cosmosv1alpha1.ScheduledVolumeSnapshot) {
	if err := r.Status().Update(ctx, crd); err != nil {
		log.FromContext(ctx).Error(err, "Failed to update status")
	}
}

// SetupWithManager sets up the controller with the Manager.
func (r *ScheduledVolumeSnapshotReconciler) SetupWithManager(_ context.Context, mgr ctrl.Manager) error {
	// We do not have to index Pods by CosmosFullNode because the CosmosFullNodeReconciler already does so.
	// If we repeat it here, the manager returns an error.
	return ctrl.NewControllerManagedBy(mgr).
		For(&cosmosv1alpha1.ScheduledVolumeSnapshot{}).
		Complete(r)
}

'''
'''--- controllers/selfhealing_controller.go ---
/*
Copyright 2022 Strangelove Ventures LLC.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package controllers

import (
	"context"
	"errors"
	"fmt"
	"net/http"
	"time"

	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	"github.com/strangelove-ventures/cosmos-operator/internal/cosmos"
	"github.com/strangelove-ventures/cosmos-operator/internal/fullnode"
	"github.com/strangelove-ventures/cosmos-operator/internal/healthcheck"
	"github.com/strangelove-ventures/cosmos-operator/internal/kube"
	"k8s.io/client-go/tools/record"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/log"
)

// SelfHealingReconciler reconciles the self healing portion of a CosmosFullNode object
type SelfHealingReconciler struct {
	client.Client
	cacheController *cosmos.CacheController
	diskClient      *fullnode.DiskUsageCollector
	driftDetector   fullnode.DriftDetection
	pvcAutoScaler   *fullnode.PVCAutoScaler
	recorder        record.EventRecorder
}

func NewSelfHealing(
	client client.Client,
	recorder record.EventRecorder,
	statusClient *fullnode.StatusClient,
	httpClient *http.Client,
	cacheController *cosmos.CacheController,
) *SelfHealingReconciler {
	return &SelfHealingReconciler{
		Client:          client,
		cacheController: cacheController,
		diskClient:      fullnode.NewDiskUsageCollector(healthcheck.NewClient(httpClient), client),
		driftDetector:   fullnode.NewDriftDetection(cacheController),
		pvcAutoScaler:   fullnode.NewPVCAutoScaler(statusClient),
		recorder:        recorder,
	}
}

// Reconcile reconciles only the self-healing spec in CosmosFullNode. If changes needed, this controller
// updates a CosmosFullNode status subresource thus triggering another reconcile loop. The CosmosFullNode
// uses the status object to reconcile its state.
func (r *SelfHealingReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
	logger := log.FromContext(ctx).WithName(cosmosv1.SelfHealingController)
	logger.V(1).Info("Entering reconcile loop", "request", req.NamespacedName)

	crd := new(cosmosv1.CosmosFullNode)
	if err := r.Get(ctx, req.NamespacedName, crd); err != nil {
		// Ignore not found errors because can't be fixed by an immediate requeue. We'll have to wait for next notification.
		// Also, will get "not found" error if crd is deleted.
		// No need to explicitly delete resources. Kube GC does so automatically because we set the controller reference
		// for each resource.
		return stopResult, client.IgnoreNotFound(err)
	}

	if crd.Spec.SelfHeal == nil {
		return stopResult, nil
	}

	reporter := kube.NewEventReporter(logger, r.recorder, crd)

	r.pvcAutoScale(ctx, reporter, crd)
	r.mitigateHeightDrift(ctx, reporter, crd)

	return ctrl.Result{RequeueAfter: 60 * time.Second}, nil
}

func (r *SelfHealingReconciler) pvcAutoScale(ctx context.Context, reporter kube.Reporter, crd *cosmosv1.CosmosFullNode) {
	if crd.Spec.SelfHeal.PVCAutoScale == nil {
		return
	}
	usage, err := r.diskClient.CollectDiskUsage(ctx, crd)
	if err != nil {
		reporter.Error(err, "Failed to collect pvc disk usage")
		// This error can be noisy so we record a generic error. Check logs for error details.
		reporter.RecordError("PVCAutoScaleCollectUsage", errors.New("failed to collect pvc disk usage"))
		return
	}
	didSignal, err := r.pvcAutoScaler.SignalPVCResize(ctx, crd, usage)
	if err != nil {
		reporter.Error(err, "Failed to signal pvc resize")
		reporter.RecordError("PVCAutoScaleSignalResize", err)
		return
	}
	if !didSignal {
		return
	}
	const msg = "PVC auto scaling requested disk expansion"
	reporter.Info(msg)
	reporter.RecordInfo("PVCAutoScale", msg)
}

func (r *SelfHealingReconciler) mitigateHeightDrift(ctx context.Context, reporter kube.Reporter, crd *cosmosv1.CosmosFullNode) {
	if crd.Spec.SelfHeal.HeightDriftMitigation == nil {
		return
	}

	pods := r.driftDetector.LaggingPods(ctx, crd)
	var deleted int
	for _, pod := range pods {
		// CosmosFullNodeController will detect missing pod and re-create it.
		if err := r.Delete(ctx, pod); kube.IgnoreNotFound(err) != nil {
			reporter.Error(err, "Failed to delete pod", "pod", pod.Name)
			reporter.RecordError("HeightDriftMitigationDeletePod", err)
			continue
		}
		reporter.Info("Deleted pod for meeting height drift threshold", "pod", pod.Name)
		deleted++
	}
	if deleted > 0 {
		msg := fmt.Sprintf("Height lagged behind by %d or more blocks; deleted pod(s)", crd.Spec.SelfHeal.HeightDriftMitigation.Threshold)
		reporter.RecordInfo("HeightDriftMitigation", msg)
	}
}

// SetupWithManager sets up the controller with the Manager.
func (r *SelfHealingReconciler) SetupWithManager(_ context.Context, mgr ctrl.Manager) error {
	// We do not have to index Pods because the CosmosFullNodeReconciler already does so.
	// If we repeat it here, the manager returns an error.
	return ctrl.NewControllerManagedBy(mgr).
		For(&cosmosv1.CosmosFullNode{}).
		Complete(r)
}

'''
'''--- controllers/statefuljob_controller.go ---
/*
Copyright 2022 Strangelove Ventures LLC.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package controllers

import (
	"context"
	"errors"
	"fmt"
	"time"

	"github.com/go-logr/logr"
	snapshotv1 "github.com/kubernetes-csi/external-snapshotter/client/v6/apis/volumesnapshot/v1"
	cosmosalpha "github.com/strangelove-ventures/cosmos-operator/api/v1alpha1"
	"github.com/strangelove-ventures/cosmos-operator/internal/kube"
	"github.com/strangelove-ventures/cosmos-operator/internal/statefuljob"
	batchv1 "k8s.io/api/batch/v1"
	corev1 "k8s.io/api/core/v1"
	"k8s.io/client-go/tools/record"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/builder"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/handler"
	"sigs.k8s.io/controller-runtime/pkg/log"
	"sigs.k8s.io/controller-runtime/pkg/source"
)

var errMissingVolSnapCRD = errors.New("cluster does not have VolumeSnapshot CRDs installed")

// IndexVolumeSnapshots indexes all VolumeSnapshots by name. Exposed as a separate method so caller can
// test for presence of VolumeSnapshot CRDs in the cluster.
func IndexVolumeSnapshots(ctx context.Context, mgr ctrl.Manager) error {
	// Index all VolumeSnapshots. Controller does not own any because it does not create them.
	if err := mgr.GetFieldIndexer().IndexField(
		ctx,
		&snapshotv1.VolumeSnapshot{},
		".metadata.name",
		func(object client.Object) []string {
			return []string{object.GetName()}
		},
	); err != nil {
		return fmt.Errorf("volume snapshot index: %w", err)
	}
	return nil
}

// StatefulJobReconciler reconciles a StatefulJob object.
type StatefulJobReconciler struct {
	client.Client
	recorder              record.EventRecorder
	missingVolSnapshotCRD bool
}

// NewStatefulJob returns a valid controller. If missingVolSnapCRD is true, the controller errors on every reconcile loop
// and will not function.
func NewStatefulJob(client client.Client, recorder record.EventRecorder, missingVolSnapCRD bool) *StatefulJobReconciler {
	return &StatefulJobReconciler{
		Client:                client,
		recorder:              recorder,
		missingVolSnapshotCRD: missingVolSnapCRD,
	}
}

var requeueStatefulJob = ctrl.Result{RequeueAfter: 60 * time.Second}

//+kubebuilder:rbac:groups=cosmos.strange.love,resources=statefuljobs,verbs=get;list;watch;create;update;patch;delete
//+kubebuilder:rbac:groups=cosmos.strange.love,resources=statefuljobs/status,verbs=get;update;patch
//+kubebuilder:rbac:groups=cosmos.strange.love,resources=statefuljobs/finalizers,verbs=update
//+kubebuilder:rbac:groups=snapshot.storage.k8s.io,resources=volumesnapshots,verbs=get;list;watch
//+kubebuilder:rbac:groups="",resources=persistentvolumeclaims,verbs=get;list;watch;create;delete
//+kubebuilder:rbac:groups="batch",resources=jobs,verbs=get;list;watch;create

// Reconcile is part of the main kubernetes reconciliation loop which aims to
// move the current state of the cluster closer to the desired state.
//
// For more details, check Reconcile and its Result here:
// - https://pkg.go.dev/sigs.k8s.io/controller-runtime@v0.13.0/pkg/reconcile
func (r *StatefulJobReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
	logger := log.FromContext(ctx)
	logger.V(1).Info("Entering reconcile loop")

	crd := new(cosmosalpha.StatefulJob)
	if err := r.Get(ctx, req.NamespacedName, crd); err != nil {
		// Ignore not found errors because can't be fixed by an immediate requeue. We'll have to wait for next notification.
		// Also, will get "not found" error if crd is deleted.
		// No need to explicitly delete resources. Kube GC does so automatically because we set the controller reference
		// for each resource.
		return requeueStatefulJob, kube.IgnoreNotFound(err)
	}

	if r.missingVolSnapshotCRD {
		r.reportErr(logger, crd, errMissingVolSnapCRD)
		return ctrl.Result{}, nil
	}

	crd.Status.ObservedGeneration = crd.Generation
	crd.Status.StatusMessage = nil
	defer r.updateStatus(ctx, crd)

	// Find active job, if any.
	found, active, err := statefuljob.FindActiveJob(ctx, r, crd)
	if err != nil {
		r.reportErr(logger, crd, err)
		return requeueStatefulJob, nil
	}

	// Update status if job still present and requeue.
	if found {
		crd.Status.JobHistory = statefuljob.UpdateJobStatus(crd.Status.JobHistory, active.Status)
		// Requeue quickly to minimize races where job is deleted before we can grab final status.
		return ctrl.Result{RequeueAfter: 5 * time.Second}, nil
	}

	// Delete any existing PVCs so we can create new ones.
	r.deletePVCs(ctx, crd)

	// Check if we need to fire new job/pvc combos.
	if !statefuljob.ReadyForSnapshot(crd, time.Now()) {
		return requeueResult, nil
	}

	// Create new jobs and pvcs.
	err = r.createResources(ctx, crd)
	if err != nil {
		r.reportErr(logger, crd, err)
		return requeueStatefulJob, nil
	}

	crd.Status.JobHistory = statefuljob.AddJobStatus(crd.Status.JobHistory, batchv1.JobStatus{})
	// Requeue quickly so we get updated job status on the next reconcile.
	return ctrl.Result{RequeueAfter: time.Second}, nil
}

func (r *StatefulJobReconciler) createResources(ctx context.Context, crd *cosmosalpha.StatefulJob) error {
	logger := log.FromContext(ctx)

	// Find most recent VolumeSnapshot.
	recent, err := kube.RecentVolumeSnapshot(ctx, r, crd.Namespace, crd.Spec.Selector)
	if err != nil {
		return err
	}
	logger.V(1).Info("Found VolumeSnapshot", "name", recent.Name)

	// Create PVCs.
	if err = statefuljob.NewCreator(r, func() ([]*corev1.PersistentVolumeClaim, error) {
		return statefuljob.BuildPVCs(crd, recent)
	}).Create(ctx, crd); err != nil {
		return err
	}

	// Create jobs.
	return statefuljob.NewCreator(r, func() ([]*batchv1.Job, error) {
		return statefuljob.BuildJobs(crd), nil
	}).Create(ctx, crd)
}

func (r *StatefulJobReconciler) deletePVCs(ctx context.Context, crd *cosmosalpha.StatefulJob) {
	logger := log.FromContext(ctx)

	var pvc corev1.PersistentVolumeClaim
	pvc.Namespace = crd.Namespace
	pvc.Name = statefuljob.ResourceName(crd)

	err := r.Delete(ctx, &pvc)
	switch {
	case kube.IsNotFound(err):
		return
	case err != nil:
		r.reportErr(logger, crd, err)
	default:
		logger.Info("Deleted PVC", "resource", pvc.Name)
	}
}

func (r *StatefulJobReconciler) reportErr(logger logr.Logger, crd *cosmosalpha.StatefulJob, err error) {
	logger.Error(err, "An error occurred")
	msg := err.Error()
	r.recorder.Event(crd, kube.EventWarning, "Error", msg)
	crd.Status.StatusMessage = &msg
}

func (r *StatefulJobReconciler) updateStatus(ctx context.Context, crd *cosmosalpha.StatefulJob) {
	if err := r.Status().Update(ctx, crd); err != nil {
		log.FromContext(ctx).Error(err, "Failed to update status")
	}
}

// SetupWithManager sets up the controller with the Manager. IndexVolumeSnapshots should be called first.
func (r *StatefulJobReconciler) SetupWithManager(ctx context.Context, mgr ctrl.Manager) error {
	cbuilder := ctrl.NewControllerManagedBy(mgr).For(&cosmosalpha.StatefulJob{})

	// Watch for delete events for jobs.
	cbuilder.Watches(
		&source.Kind{Type: &batchv1.Job{}},
		&handler.EnqueueRequestForObject{},
		builder.WithPredicates(
			statefuljob.LabelSelectorPredicate(),
			statefuljob.DeletePredicate(),
		),
	)

	return cbuilder.Complete(r)
}

'''
'''--- docs/architecture.md ---
# Cosmos Operator Architecture

This is a high-level overview of the architecture of the Cosmos Operator. It is intended to be a reference for
developers.

## Overview

The operator was written with the [kubebuilder](https://github.com/kubernetes-sigs/kubebuilder) framework.

Kubebuilder simplifies and provides abstractions for creating a Kubernetes controller.

In a nutshell, an operator observes
a [CRD](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/). Its job is to match
cluster state with the desired state in the CRD. It
continually watches for changes and updates the cluster accordingly - a "control loop" pattern.

Each controller implements a Reconcile method:

```go
Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error)
```

Unlike "built-in" controllers like Deployments or StatefulSets, operator controllers are visible in the cluster - one pod
backed by a Deployment under the cosmos-operator-system namespace.

A controller can watch resources outside of the CRD it manages. For example, CosmosFullNode watches for pod deletions,
so it can spin up new pods if a user deletes one manually.

The watching of resources is in this method for each controller:

```go
SetupWithManager(ctx context.Context, mgr ctrl.Manager) error
```

Refer to kubebuilder docs for more info.

### Makefile

Kubebuilder generated much of the Makefile. It contains common tasks for developers.

### `api` directory

This directory contains the different CRDs.

You should run `make generate manifests` each time you change CRDs.

A CI job should fail if you forget to run this command after modifying the api structs.

### `config` directory

The config directory contains kustomize files generated by Kubebuilder. 
Strangelove uses these files to deploy the operator (instead of a helm chart). 
A helm chart is on the road map but presents challenges in keeping the kustomize and helm code in sync.

### `controllers` directory

The controllers directory contains every controller.

This directory is not unit tested. The code in controllers should act like `main()` functions where it's mostly wiring
up of dependencies from `internal`.

### `internal` directory

Almost all the business logic lives in `internal` and houses the unit and integration tests.

# CosmosFullNode

This is the flagship CRD of the Cosmos Operator and contains the most complexity.

### Builder, Diff, and Control Pattern

Each resource has its own builder and controller (referred as "control" in this context). For example,
see `pvc_builder.go` and `pvc_control.go` which only manages PVCs. All builders should have file suffix `_builder.go`
and all control objects `_control.go`.

The most complex builder is `pod_builder.go`. There may be opportunities to refactor it.

The "control" pattern was loosely inspired by Kubernetes source code.

Within the controller's `Reconcile(...)` method, the controller determines the order of operations of the separate 
Control objects.

On process start, each Control is initialized with a Diff and a Builder.

On each reconcile loop:

1. The Builder builds the desired resources from the CRD.
2. Control fetches a list of existing resources.
3. Control uses Diff to compute a diff of the existing to the desired.
4. Control makes changes based on what Diff reports.

The Control tests are *integration tests* where we mock out the Kubernetes API, but not the Builder or Diff. The
tests run quickly (like unit tests) because we do not make any network calls.

The Diff object (`type Diff[T client.Object] struct`) took several iterations to get right. There is probably little
need to tweak it further.

The hardest problem with diffing is determining updates. Essentially, Diff looks for a `Revision() string` method on the
resource and sets a revision annotation. The revision is a simple fnv hash. It compares `Revision` to the existing annotation. 
If different, we know it's an update. We cannot compare equality of existing resources directly because Kubernetes adds additional 
annotations and fields.

Builders return a `diff.Resource[T]` which Diff can use. Therefore, Control does not need to adapt resources.

The fnv hash is computed from a resource's JSON representation, which has proven to be stable.

### Special Note on Updating Status

There are several controllers that update a
CosmosFullNode's [status subresource](https://book-v1.book.kubebuilder.io/basics/status_subresource):

* CosmosFullNode
* ScheduledVolumeSnapshot
* SelfHealing

Each update to the status subresource triggers another reconcile loop. We found multiple controllers updating status
caused race conditions. Updates were not applied or applied incorrectly. 
Some controllers read the status to take action, so it's important to preserve the integrity of the status.

Therefore, you must use the special `SyncUpdate(...)` method from `fullnode.StatusClient`. It ensures updates are
performed serially per CosmosFullNode.

### Sentries

Sentries are special because you should not include a readiness probe due to the way Tendermint/Comet remote
signing works.

The remote signer reaches out to the sentry on the privval port. This is the inverse of what you'd expect, the sentry
reaching out to the remote signer.

If the sentry does not detect a remote signer connection, it crashes. And the stable way to connect to a pod is through
a Kube Service. So we have a chicken or egg problem. The sentry must be "ready" to be added to the Service, but the 
remote signer must connect to the sentry through the Service so it doesn't crash.

Therefore, the CosmosFullNode controller inspects Tendermint/Comet as part of its rolling update strategy - not just 
pod readiness state. 

### CacheController

The CacheController is special in that it does not manage a CRD.

It periodically polls every pod for its Tendermint/Comet status such as block height. The polling is done in 
the background. It's a controller because it needs the reconcile loop to update which pods it needs to poll.

The CacheController prevents slow reconcile loops. Previously, we queried this status on every reconcile loop.

When other controllers want Comet status, they always hit the cache controller.

# Scheduled Volume Snapshot

Scheduled Volume Snapshot takes periodic backups.

To preserve data integrity, it will temporarily delete a pod, so it can capture a PVC snapshot without any process
writing to it.

It uses a finite state machine pattern in the main reconcile loop.

# StatefulJob

StatefulJob periodically runs a job on an interval (crontab not supported yet). The purpose is to run a job that
attaches to a PVC created from a VolumeSnapshot.

It's the least developed of the CRDs.

'''
'''--- docs/fullnode_best_practices.md ---
# CosmosFullNode Best Practices

## Resource Names

If you plan to have multiple network environments in the same cluster or namespace, append the network name and any other identifying information.

Example:
```yaml
apiVersion: cosmos.strange.love/v1
kind: CosmosFullNode
metadata:
  name: cosmoshub-mainnet-fullnode
spec:
  chain:
    network: mainnet # Should align with metadata.name above.
```

Like a StatefulSet, the Operator uses the .metadata.name of the CosmosFullNode to name resources it creates and manages.

## Volumes, PVCs and StorageClass

Generally, Volumes are bound to a single Availability Zone (AZ). Therefore, use or define a [StorageClass](https://kubernetes.io/docs/concepts/storage/storage-classes/)
which has `volumeBindingMode: WaitForFirstConsumer`. This way, kubernetes will not provision the volume until there is a pod ready to bind to it.

If you do not configure `volumeBindingMode` to wait, you risk the scheduler ignoring pod topology rules such as `Affinity`.
For example, in GKE, volumes will be provisioned [in random zones](https://cloud.google.com/kubernetes-engine/docs/concepts/persistent-volumes).

The Operator cannot define a StorageClass for you. Instead, you must configure the CRD with a pre-existing StorageClass.

Cloud providers generally provide default StorageClasses for you. Some of them set `volumeBindingMode: WaitForFirstConsumer` such as GKE's `premium-rwo`.
```shell
kubectl get storageclass
```

Additionally, Cosmos nodes require heavy disk IO. Therefore, choose a faster StorageClass such as GKE's `premium-rwo`.

## Resizing Volumes

The StorageClass must support resizing. Most cloud providers (like GKE) support it.

To resize, update resources in the CRD like so:
```yaml
resources:
  requests:
    storage: 100Gi # increase size here
```

You can only increase the storage (never decrease).

You must manually watch the PVC for a status of `FileSystemResizePending`. Then manually restart the pod associated with the PVC to complete resizing.

The above is a workaround; there is [future work](https://github.com/strangelove-ventures/cosmos-operator/issues/37) planned to allow the Operator to handle this scenario for you.

## Updating Volumes

Most PVC fields are immutable (such as StorageClass), so once the Operator creates PVCs, immutable fields are not updated even if you change values in the CRD.

As mentioned in the above section, you can only update the storage size.

If you need to update an immutable field like the StorageClass, the workaround is to `kubectl apply` the CRD. Then manually delete PVCs and pods. The Operator will recreate them with the new configuration.

There is [future work](https://github.com/strangelove-ventures/cosmos-operator/issues/38) planned for the Operator to handle this scenario for you.

## Pod Affinity

The Operator cannot assume your preferred topology. Therefore, set affinity appropriately to fit your use case.

E.g. To encourage the scheduler to spread pods across nodes:

```yaml
template:
  affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                      - <name of crd>
              topologyKey: kubernetes.io/hostname
```

'''
'''--- docs/quick_start.md ---
# Quick Start

This quick start guide creates a CosmosFullNode that runs as an RPC node for the Cosmos Hub.

### Prerequisites

You will need kuberentes nodes that can provide up to 32GB and 4CPU per replica. The following example
deploys 2 replicas.

### Install the CRDs and deploy operator in your cluster

View [docker images here](https://github.com/strangelove-ventures/cosmos-operator/pkgs/container/cosmos-operator).

```sh
# Deploy the latest release. Warning: May be a release candidate.
make deploy IMG="ghcr.io/strangelove-ventures/cosmos-operator:$(git describe --tags --abbrev=0)"

# Deploy a specific version
make deploy IMG="ghcr.io/strangelove-ventures/cosmos-operator:<version you choose>"
```

#### TODO

Helm chart coming soon.

### Choose a StorageClass

View storage classes in your cluster:
```sh
kubectl get storageclass
```

Choose one that provides SSD. On GKE, we recommend `premium-rwo`.

### Find latest mainnet version of Gaia

See "Recommended Version" on [Minstcan](https://www.mintscan.io/cosmos/info).

### Find seeds

Copy "Peers" -> "Seeds" on [Minstcan](https://www.mintscan.io/cosmos/info).

### Find a recent snapshot

We recommend [Polkachu](https://www.polkachu.com/tendermint_snapshots/cosmos). Copy the URL of the Download link for the `.tar.lz4` file.

### Create a CosmosFullNode

Using the information from the previous steps, create a yaml file using the below template.

Then `kubectl apply -f` the yaml file.

```yaml
apiVersion: cosmos.strange.love/v1
kind: CosmosFullNode
metadata:
  name: cosmoshub
  namespace: default
spec:
  chain:
    app:
      minGasPrice: 0.001uatom
    binary: gaiad
    chainID: cosmoshub-4
    config:
      seeds: <your seeds> # TODO
    genesisURL: https://snapshots.polkachu.com/genesis/cosmos/genesis.json
    network: mainnet
    skipInvariants: true
    snapshotURL: <your snapshot, probably from Polkachu> # TODO
  podTemplate:
    image: ghcr.io/strangelove-ventures/heighliner/gaia:<latest version of gaia> # TODO
    resources:
      requests:
        memory: 16Gi
  replicas: 2 # TODO change to 1 to use less resources
  volumeClaimTemplate:
    resources:
      requests:
        storage: 200Gi
    storageClassName: <your chosen storage class> # TODO
```

### Monitor pods

Once created, monitor the pods in the `default` namespace for any errors.
'''
'''--- docs/scheduled_volume_snapshot.md ---
## ScheduledVolumeSnapshot

Status: v1alpha1

**Warning: May have backwards breaking changes!**

ScheduledVolumeSnapshot allows you to periodically backup your data.

It creates [VolumeSnapshots]([VolumeSnapshot](https://kubernetes.io/docs/concepts/storage/volume-snapshots/))
from a healthy CosmosFullNode PVC on a recurring schedule described by a [crontab](https://en.wikipedia.org/wiki/Cron). The controller
chooses a candidate pod/pvc combo from a source CosmosFullNode. This allows you to create reliable, scheduled backups
of blockchain state.

**Warning: Backups may include private keys and secrets.** For validators, we strongly recommend using [Horcrux](https://github.com/strangelove-ventures/horcrux),
[TMKMS](https://github.com/iqlusioninc/tmkms), or another CometBFT remote signer.

To minimize data corruption, the operator temporarily deletes the CosmosFullNode pod writing to the PVC while taking the snapshot. Deleting the pod allows the process to
exit gracefully and prevents writes to the disk. Once the snapshot is complete, the operator re-creates the pod. Therefore, use of this CRD may affect
availability of the source CosmosFullNode. At least 2 CosmosFullNode replicas is necessary to prevent downtime; 3
replicas recommended. In the future, this behavior may be configurable.

Limitations:
- The CosmosFullNode and ScheduledVolumeSnapshot must be in the same namespace.

[Example yaml](../config/samples/cosmos_v1alpha1_scheduledvolumesnapshot.yaml)

'''
'''--- docs/stateful_job.md ---
## StatefulJob

Status: v1alpha1

**Warning: May have backwards breaking changes!**

A StatefulJob is a means to process persistent data from a recent [VolumeSnapshot](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
It periodically creates a job and PVC using the most recent VolumeSnapshot as its data source. It mounts the PVC as volume "snapshot" into the job's pod.
The user must configure container volume mounts.
It's similar to a CronJob but does not offer advanced scheduling via a crontab.

Strangelove uses it to compress and upload snapshots of chain data.

[Example yaml](../config/samples/cosmos_v1alpha1_statefuljob.yaml)

'''
'''--- hack/boilerplate.go.txt ---
/*
Copyright 2022 Strangelove Ventures LLC.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/
'''
'''--- internal/cosmos/cache_controller.go ---
package cosmos

import (
	"context"
	"fmt"
	"sync"
	"time"

	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	"github.com/strangelove-ventures/cosmos-operator/internal/kube"
	"golang.org/x/sync/errgroup"
	corev1 "k8s.io/api/core/v1"
	"k8s.io/client-go/tools/record"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/log"
	"sigs.k8s.io/controller-runtime/pkg/reconcile"
)

type cache struct {
	sync.RWMutex
	m map[client.ObjectKey]*cacheItem
}

type cacheItem struct {
	coll   StatusCollection
	cancel context.CancelFunc
}

func newCache() *cache {
	return &cache{
		m: make(map[client.ObjectKey]*cacheItem),
	}
}

func (c *cache) Get(key client.ObjectKey) (StatusCollection, bool) {
	c.RLock()
	defer c.RUnlock()
	v, ok := c.m[key]
	if !ok {
		return nil, false
	}
	return v.coll, ok
}

func (c *cache) Init(key client.ObjectKey, cancel context.CancelFunc) {
	c.Lock()
	defer c.Unlock()
	c.m[key] = &cacheItem{
		coll:   make(StatusCollection, 0),
		cancel: cancel,
	}
}

func (c *cache) Update(key client.ObjectKey, value StatusCollection) {
	c.Lock()
	defer c.Unlock()
	v, ok := c.m[key]
	if !ok {
		return
	}
	v.coll = value
}

func (c *cache) Del(key client.ObjectKey) {
	c.Lock()
	defer c.Unlock()
	if v, ok := c.m[key]; ok {
		v.cancel()
	}
	delete(c.m, key)
}

func (c *cache) DelAll() {
	c.Lock()
	defer c.Unlock()
	for _, v := range c.m {
		v.cancel()
	}
	c.m = make(map[client.ObjectKey]*cacheItem)
}

type Collector interface {
	Collect(ctx context.Context, pods []corev1.Pod) StatusCollection
}

const CacheControllerName = "CosmosCache"

// CacheController periodically polls pods for their CometBFT status and caches the result.
// The cache is a controller so it can watch CosmosFullNode objects to warm or invalidate the cache.
type CacheController struct {
	cache     *cache
	client    client.Reader
	collector Collector
	eg        errgroup.Group
	interval  time.Duration
	recorder  record.EventRecorder
}

func NewCacheController(collector Collector, reader client.Reader, recorder record.EventRecorder) *CacheController {
	return &CacheController{
		cache:     newCache(),
		client:    reader,
		collector: collector,
		interval:  5 * time.Second,
		recorder:  recorder,
	}
}

// SetupWithManager watches CosmosFullNode objects and starts cache collecting.
func (c *CacheController) SetupWithManager(_ context.Context, mgr ctrl.Manager) error {
	// We do not index pods because we presume another controller is already doing so.
	// If we repeat it here, the manager returns an error.
	return ctrl.NewControllerManagedBy(mgr).
		For(&cosmosv1.CosmosFullNode{}).
		Complete(c)
}

// Close stops all cache collecting and waits for all goroutines to exit.
func (c *CacheController) Close() error {
	c.cache.DelAll()
	return c.eg.Wait()
}

var finishResult reconcile.Result

func (c *CacheController) Reconcile(ctx context.Context, req reconcile.Request) (reconcile.Result, error) {
	crd := new(cosmosv1.CosmosFullNode)
	if err := c.client.Get(ctx, req.NamespacedName, crd); err != nil {
		if kube.IsNotFound(err) {
			c.cache.Del(req.NamespacedName)
		}
		return finishResult, kube.IgnoreNotFound(err)
	}

	reporter := kube.NewEventReporter(log.FromContext(ctx), c.recorder, crd)

	// If not already cached, start collecting status from pods.
	if _, ok := c.cache.Get(req.NamespacedName); !ok {
		cctx, cancel := context.WithCancel(ctx)
		c.cache.Init(req.NamespacedName, cancel)
		c.eg.Go(func() error {
			defer cancel()
			c.collectFromPods(cctx, reporter, req.NamespacedName)
			return nil
		})
	}

	return finishResult, nil
}

// Invalidate removes the given pods status from the cache.
func (c *CacheController) Invalidate(controller client.ObjectKey, pods []string) {
	v, _ := c.cache.Get(controller)
	now := time.Now()
	for _, s := range v {
		for _, pod := range pods {
			if s.Pod.Name == pod {
				s.Status = CometStatus{}
				s.Err = fmt.Errorf("invalidated")
				s.TS = now
			}
		}
	}
	c.cache.Update(controller, v)
}

// Collect returns a StatusCollection for the given controller. Only returns cached CometStatus.
func (c *CacheController) Collect(ctx context.Context, controller client.ObjectKey) StatusCollection {
	pods, err := c.listPods(ctx, controller)
	if err != nil {
		return nil
	}
	v, _ := c.cache.Get(controller)
	IntersectPods(&v, pods)
	for i := range pods {
		UpsertPod(&v, &pods[i])
	}
	return v
}

// SyncedPods returns only the pods that are ready and in sync (i.e. caught up with chain tip).
func (c *CacheController) SyncedPods(ctx context.Context, controller client.ObjectKey) []*corev1.Pod {
	return kube.AvailablePods(c.Collect(ctx, controller).SyncedPods(), 5*time.Second, time.Now())
}

func (c *CacheController) listPods(ctx context.Context, controller client.ObjectKey) ([]corev1.Pod, error) {
	var pods corev1.PodList
	if err := c.client.List(ctx, &pods,
		client.InNamespace(controller.Namespace),
		client.MatchingFields{kube.ControllerOwnerField: controller.Name},
	); err != nil {
		return nil, err
	}
	return pods.Items, nil
}

func (c *CacheController) collectFromPods(ctx context.Context, reporter kube.Reporter, controller client.ObjectKey) {
	defer c.cache.Del(controller)

	collect := func() {
		pods, err := c.listPods(ctx, controller)
		if err != nil {
			err = fmt.Errorf("%s: %w", controller, err)
			reporter.Error(err, "Failed to list pods")
			reporter.RecordError("ListPods", err)
			return
		}
		c.cache.Update(controller, c.collector.Collect(ctx, pods))
	}

	collect() // Collect once immediately.
	tick := time.NewTicker(c.interval)
	defer tick.Stop()
	for {
		select {
		case <-ctx.Done():
			return
		case <-tick.C:
			collect()
		}
	}
}

'''
'''--- internal/cosmos/cache_controller_test.go ---
package cosmos

import (
	"context"
	"sync"
	"sync/atomic"
	"testing"
	"time"

	"github.com/samber/lo"
	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	"github.com/stretchr/testify/require"
	"go.uber.org/goleak"
	corev1 "k8s.io/api/core/v1"
	apierrors "k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime/schema"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/reconcile"
)

type mockCollector struct {
	Called         int64
	GotPods        []corev1.Pod
	StubCollection StatusCollection
}

func (m *mockCollector) Collect(ctx context.Context, pods []corev1.Pod) StatusCollection {
	atomic.AddInt64(&m.Called, 1)
	if ctx == nil {
		panic("nil context")
	}
	m.GotPods = pods
	return m.StubCollection
}

type mockReader struct {
	sync.Mutex
	GetErr error

	ListPods []corev1.Pod
	ListOpts []client.ListOption
}

func (m *mockReader) Get(ctx context.Context, key client.ObjectKey, obj client.Object, opts ...client.GetOption) error {
	m.Lock()
	defer m.Unlock()
	if ctx == nil {
		panic("nil context")
	}
	var crd cosmosv1.CosmosFullNode
	crd.Name = key.Name
	crd.Namespace = key.Namespace
	*obj.(*cosmosv1.CosmosFullNode) = crd
	return m.GetErr
}

func (m *mockReader) List(ctx context.Context, list client.ObjectList, opts ...client.ListOption) error {
	m.Lock()
	defer m.Unlock()
	if ctx == nil {
		panic("nil context")
	}
	m.ListOpts = opts
	list.(*corev1.PodList).Items = m.ListPods
	return nil
}

func TestCacheController_Reconcile(t *testing.T) {
	t.Parallel()

	ctx := context.Background()
	const (
		namespace = "strangelove"
		name      = "nolus"
	)

	validStatusColl := StatusCollection{
		{Pod: new(corev1.Pod)},
		{Pod: new(corev1.Pod)},
		{Pod: new(corev1.Pod)},
	}

	t.Run("crd created or updated", func(t *testing.T) {
		defer goleak.VerifyNone(t, goleak.IgnoreCurrent())

		pods := make([]corev1.Pod, 2)
		var reader mockReader
		reader.ListPods = pods

		var collector mockCollector
		collector.StubCollection = validStatusColl

		controller := NewCacheController(&collector, &reader, nil)

		var req reconcile.Request
		req.Name = name
		req.Namespace = namespace

		// Ensures we don't cache more than once per request
		for i := 0; i < 3; i++ {
			res, err := controller.Reconcile(ctx, req)
			require.Equal(t, reconcile.Result{}, res)
			require.NoError(t, err)
		}

		key := client.ObjectKey{Name: name, Namespace: namespace}
		require.Eventually(t, func() bool {
			got := controller.Collect(ctx, key)
			return len(got) == 3
		}, time.Second, time.Millisecond)

		require.Equal(t, collector.StubCollection, controller.Collect(ctx, key))
		require.Equal(t, pods, collector.GotPods)

		opts := reader.ListOpts
		require.Len(t, opts, 2)
		var listOpt client.ListOptions
		for _, opt := range opts {
			opt.ApplyToList(&listOpt)
		}
		require.Equal(t, namespace, listOpt.Namespace)
		require.Zero(t, listOpt.Limit)
		require.Equal(t, ".metadata.controller=nolus", listOpt.FieldSelector.String())

		require.NoError(t, controller.Close())

		require.Equal(t, int64(1), collector.Called)
	})

	t.Run("crd deleted", func(t *testing.T) {
		defer goleak.VerifyNone(t, goleak.IgnoreCurrent())

		pods := make([]corev1.Pod, 1)
		reader := &mockReader{}
		reader.ListPods = pods

		var collector mockCollector
		collector.StubCollection = validStatusColl[:1]

		controller := NewCacheController(&collector, reader, nil)

		var req reconcile.Request
		req.Name = name
		req.Namespace = namespace
		_, err := controller.Reconcile(ctx, req)
		require.NoError(t, err)

		key := client.ObjectKey{Name: name, Namespace: namespace}
		require.Eventually(t, func() bool {
			return len(controller.Collect(ctx, key)) > 0
		}, time.Second, time.Millisecond)

		reader.GetErr = apierrors.NewNotFound(schema.GroupResource{}, name)

		_, err = controller.Reconcile(ctx, req)
		require.NoError(t, err)

		require.NoError(t, controller.Close())
	})

	t.Run("zero state", func(t *testing.T) {
		defer goleak.VerifyNone(t, goleak.IgnoreCurrent())

		var reader mockReader
		var collector mockCollector
		collector.StubCollection = make(StatusCollection, 1)

		controller := NewCacheController(&collector, &reader, nil)
		key := client.ObjectKey{Name: name, Namespace: namespace}
		require.Empty(t, controller.Collect(ctx, key))
		require.Empty(t, controller.SyncedPods(ctx, key))
	})
}

func TestCacheController_SyncedPods(t *testing.T) {
	t.Parallel()

	defer goleak.VerifyNone(t, goleak.IgnoreCurrent())

	ctx := context.Background()
	const (
		namespace = "default"
		name      = "axelar"
	)

	reader := new(mockReader)
	reader.ListPods = make([]corev1.Pod, 2)

	var catchingUp CometStatus
	catchingUp.Result.SyncInfo.CatchingUp = true

	var collector mockCollector
	collector.StubCollection = StatusCollection{
		{Pod: &corev1.Pod{ObjectMeta: metav1.ObjectMeta{UID: "1"}}},
		{Pod: &corev1.Pod{ObjectMeta: metav1.ObjectMeta{UID: "2"}}, Status: catchingUp},
		{Pod: &corev1.Pod{ObjectMeta: metav1.ObjectMeta{UID: "3"}}},
		{Pod: &corev1.Pod{ObjectMeta: metav1.ObjectMeta{UID: "should not see me"}}, Status: catchingUp},
	}

	pods := []corev1.Pod{
		{ObjectMeta: metav1.ObjectMeta{UID: "1"}},
	}
	reader.ListPods = pods

	controller := NewCacheController(&collector, reader, nil)

	var req reconcile.Request
	req.Name = name
	req.Namespace = namespace
	_, err := controller.Reconcile(ctx, req)
	require.NoError(t, err)

	key := client.ObjectKey{Name: name, Namespace: namespace}
	// Wait until we've fetched comet status in the background and cached it.
	require.Eventually(t, func() bool {
		p := controller.Collect(ctx, key)
		l := len(p)
		_, e := p[0].GetStatus()
		return l == 1 && e == nil
	}, time.Second, time.Millisecond)

	readyStatus := corev1.PodStatus{Conditions: []corev1.PodCondition{
		{Type: corev1.PodReady, Status: corev1.ConditionTrue, LastTransitionTime: metav1.NewTime(time.Now().Add(-5 * time.Second))}},
	}

	pods = []corev1.Pod{
		{ObjectMeta: metav1.ObjectMeta{UID: "1"}, Status: readyStatus},
		{ObjectMeta: metav1.ObjectMeta{UID: "2"}, Status: readyStatus},
		{ObjectMeta: metav1.ObjectMeta{UID: "3"}}, // not ready
		{ObjectMeta: metav1.ObjectMeta{UID: "new"}},
	}

	reader.Lock()
	reader.ListPods = pods
	reader.Unlock()

	gotColl := controller.Collect(ctx, key)
	uids := lo.Map(gotColl, func(item StatusItem, _ int) string { return string(item.Pod.UID) })
	require.Equal(t, []string{"1", "2", "3", "new"}, uids)

	_, err = gotColl[3].GetStatus()
	require.Error(t, err)
	require.EqualError(t, err, "missing status")

	gotPods := controller.SyncedPods(ctx, key)
	require.Len(t, gotPods, 1)
	require.Equal(t, pods[0], *gotPods[0])

	require.NoError(t, controller.Close())

	opts := reader.ListOpts
	require.Len(t, opts, 2)
	var listOpt client.ListOptions
	for _, opt := range opts {
		opt.ApplyToList(&listOpt)
	}
	require.Equal(t, namespace, listOpt.Namespace)
	require.Zero(t, listOpt.Limit)
	require.Equal(t, ".metadata.controller=axelar", listOpt.FieldSelector.String())
}

'''
'''--- internal/cosmos/comet_client.go ---
package cosmos

import (
	"context"
	"encoding/json"
	"errors"
	"fmt"
	"net/http"
	"net/url"
	"strconv"
	"time"
)

type ValidatorInfo struct {
	Address string `json:"address"`
	PubKey  struct {
		Type  string `json:"type"`
		Value string `json:"value"`
	} `json:"pub_key"`
	VotingPower string `json:"voting_power"`
}

type NodeInfo struct {
	ProtocolVersion struct {
		P2P   string `json:"p2p"`
		Block string `json:"block"`
		App   string `json:"app"`
	} `json:"protocol_version"`
	ID         string `json:"id"`
	ListenAddr string `json:"listen_addr"`
	Network    string `json:"network"`
	Version    string `json:"version"`
	Channels   string `json:"channels"`
	Moniker    string `json:"moniker"`
	Other      struct {
		TxIndex    string `json:"tx_index"`
		RPCAddress string `json:"rpc_address"`
	} `json:"other"`
}

type SyncInfo struct {
	LatestBlockHash     string    `json:"latest_block_hash"`
	LatestAppHash       string    `json:"latest_app_hash"`
	LatestBlockHeight   string    `json:"latest_block_height"`
	LatestBlockTime     time.Time `json:"latest_block_time"`
	EarliestBlockHash   string    `json:"earliest_block_hash"`
	EarliestAppHash     string    `json:"earliest_app_hash"`
	EarliestBlockHeight string    `json:"earliest_block_height"`
	EarliestBlockTime   time.Time `json:"earliest_block_time"`
	CatchingUp          bool      `json:"catching_up"`
}

// rpcCometStatusResponse is the union of possible server response
type rpcCometStatusResponse struct {
	JSONRPC string `json:"jsonrpc"`
	ID      int    `json:"id"`
	Result  struct {
		NodeInfo      *NodeInfo      `json:"node_info"`
		SyncInfo      *SyncInfo      `json:"sync_info"`
		ValidatorInfo *ValidatorInfo `json:"validator_info"`
	} `json:"result"`
	NodeInfo      *NodeInfo      `json:"node_info"`
	SyncInfo      *SyncInfo      `json:"sync_info"`
	ValidatorInfo *ValidatorInfo `json:"validator_info"`
}

// CometStatus is the common response from the /status RPC endpoint.
type CometStatus struct {
	JSONRPC string
	ID      int
	Result  struct {
		NodeInfo      NodeInfo
		SyncInfo      SyncInfo
		ValidatorInfo ValidatorInfo
	}
}

// LatestBlockHeight parses the latest block height string. If the string is malformed, returns 0.
func (status CometStatus) LatestBlockHeight() uint64 {
	h, _ := strconv.ParseUint(status.Result.SyncInfo.LatestBlockHeight, 10, 64)
	return h
}

// CometClient knows how to make requests to the CometBFT (formerly Comet) RPC endpoints.
// This package uses a custom client because 1) parsing JSON is simple and 2) we prevent any dependency on
// CometBFT packages.
type CometClient struct {
	httpDo func(req *http.Request) (*http.Response, error)
}

func NewCometClient(client *http.Client) *CometClient {
	return &CometClient{httpDo: client.Do}
}

// Status finds the latest status.
func (client *CometClient) Status(ctx context.Context, rpcHost string) (CometStatus, error) {
	var status CometStatus
	u, err := url.ParseRequestURI(rpcHost)
	if err != nil {
		return status, fmt.Errorf("malformed host: %w", err)
	}
	u.Path = "status"
	req, err := http.NewRequest("GET", u.String(), nil)
	if err != nil {
		return status, fmt.Errorf("malformed request: %w", err)
	}
	req = req.WithContext(ctx)
	resp, err := client.httpDo(req)
	if err != nil {
		return status, err
	}
	defer resp.Body.Close()
	if resp.StatusCode != http.StatusOK {
		return status, errors.New(resp.Status)
	}
	var rpcStatusResponse rpcCometStatusResponse
	err = json.NewDecoder(resp.Body).Decode(&rpcStatusResponse)
	if err != nil {
		return status, fmt.Errorf("malformed json: %w", err)
	}
	if rpcStatusResponse.ValidatorInfo != nil {
		status.Result.ValidatorInfo = *rpcStatusResponse.ValidatorInfo
		status.Result.SyncInfo = *rpcStatusResponse.SyncInfo
		status.Result.NodeInfo = *rpcStatusResponse.NodeInfo
	} else {
		status.Result.ValidatorInfo = *rpcStatusResponse.Result.ValidatorInfo
		status.Result.SyncInfo = *rpcStatusResponse.Result.SyncInfo
		status.Result.NodeInfo = *rpcStatusResponse.Result.NodeInfo
	}
	return status, err
}

'''
'''--- internal/cosmos/comet_client_test.go ---
package cosmos

import (
	"context"
	"errors"
	"io"
	"net/http"
	"strings"
	"testing"

	"github.com/stretchr/testify/require"
)

func TestCometStatus_LatestBlockHeight(t *testing.T) {
	t.Parallel()

	for _, tt := range []struct {
		Height string
		Want   uint64
	}{
		{"", 0},
		{"huh", 0},
		{"-1", 0},
		{"1", 1},
		{"1234567", 1234567},
	} {
		var status CometStatus
		status.Result.SyncInfo.LatestBlockHeight = tt.Height

		require.Equal(t, tt.Want, status.LatestBlockHeight(), tt)
	}
}

func TestCometClient_Status(t *testing.T) {
	t.Parallel()

	t.Run("when there are no errors", func(t *testing.T) {
		t.Run("given common response it returns the status", func(t *testing.T) {
			// This context ensures we're not comparing instances of context.Background().
			cctx, cancel := context.WithCancel(context.Background())
			defer cancel()

			client := NewCometClient(http.DefaultClient)
			require.NotNil(t, client.httpDo)

			client.httpDo = func(req *http.Request) (*http.Response, error) {
				require.Same(t, cctx, req.Context())
				require.Equal(t, "GET", req.Method)
				require.Equal(t, "http://10.2.3.4:26657/status", req.URL.String())

				return &http.Response{
					StatusCode: 200,
					Body:       io.NopCloser(strings.NewReader(statusResponseFixture)),
				}, nil
			}

			got, err := client.Status(cctx, "http://10.2.3.4:26657")
			require.NoError(t, err)
			require.Equal(t, "cosmoshub-testnet-fullnode-0", got.Result.NodeInfo.Moniker)
			require.Equal(t, false, got.Result.SyncInfo.CatchingUp)
			require.Equal(t, "13348657", got.Result.SyncInfo.LatestBlockHeight)
			require.Equal(t, "9034670", got.Result.SyncInfo.EarliestBlockHeight)
		})

		t.Run("given SEI response it returns the status", func(t *testing.T) {
			// This context ensures we're not comparing instances of context.Background().
			cctx, cancel := context.WithCancel(context.Background())
			defer cancel()

			client := NewCometClient(http.DefaultClient)
			require.NotNil(t, client.httpDo)

			client.httpDo = func(req *http.Request) (*http.Response, error) {
				require.Same(t, cctx, req.Context())
				require.Equal(t, "GET", req.Method)
				require.Equal(t, "http://10.2.3.4:26657/status", req.URL.String())

				return &http.Response{
					StatusCode: 200,
					Body:       io.NopCloser(strings.NewReader(seiStatusResponseFixture)),
				}, nil
			}

			got, err := client.Status(cctx, "http://10.2.3.4:26657")
			require.NoError(t, err)
			require.Equal(t, "hello-sei-relayer", got.Result.NodeInfo.Moniker)
			require.Equal(t, false, got.Result.SyncInfo.CatchingUp)
			require.Equal(t, "37909189", got.Result.SyncInfo.LatestBlockHeight)
			require.Equal(t, "33517999", got.Result.SyncInfo.EarliestBlockHeight)
		})
	})

	t.Run("when there is an error", func(t *testing.T) {
		t.Run("non 200 response", func(t *testing.T) {
			client := NewCometClient(http.DefaultClient)
			client.httpDo = func(req *http.Request) (*http.Response, error) {
				return &http.Response{
					StatusCode: 500,
					Status:     "internal server error",
					Body:       io.NopCloser(strings.NewReader("")),
				}, nil
			}

			_, err := client.Status(context.Background(), "http://10.2.3.4:26657")
			require.Error(t, err)
			require.EqualError(t, err, "internal server error")
		})

		t.Run("http error", func(t *testing.T) {
			client := NewCometClient(http.DefaultClient)
			client.httpDo = func(req *http.Request) (*http.Response, error) {
				return nil, errors.New("boom")
			}

			_, err := client.Status(context.Background(), "http://10.2.3.4:26657")
			require.Error(t, err)
			require.EqualError(t, err, "boom")
		})
	})
}

const statusResponseFixture = `
{
  "jsonrpc": "2.0",
  "id": -1,
  "result": {
    "node_info": {
      "protocol_version": {
        "p2p": "8",
        "block": "11",
        "app": "0"
      },
      "id": "f5fe383c6338c14f94319a96813ea77df1ab9060",
      "listen_addr": "tcp://0.0.0.0:26656",
      "network": "theta-testnet-001",
      "version": "v0.34.21",
      "channels": "40202122233038606100",
      "moniker": "cosmoshub-testnet-fullnode-0",
      "other": {
        "tx_index": "on",
        "rpc_address": "tcp://0.0.0.0:26657"
      }
    },
    "sync_info": {
      "latest_block_hash": "B6672ADAAC1B94DA33C98A78451E78A6A1E53616DFFFEAAA89C138A59FDA0C5B",
      "latest_app_hash": "7839C1CC04613DFE51EB2218B50FA824B4A32EC5F04B12C8DE8B1A6A815BD406",
      "latest_block_height": "13348657",
      "latest_block_time": "2022-11-28T21:58:24.609825528Z",
      "earliest_block_hash": "8C0507EBF07B2EDAE96FB598856D732D2DF30506E719E0BB6C3F7230DD14F7EF",
      "earliest_app_hash": "E3B0C44298FC1C149AFBF4C8996FB92427AE41E4649B934CA495991B7852B855",
      "earliest_block_height": "9034670",
      "earliest_block_time": "2019-12-11T16:11:34Z",
      "catching_up": false
    },
    "validator_info": {
      "address": "6F741B2D3789866798F58E25B412972E641EC035",
      "pub_key": {
        "type": "tendermint/PubKeyEd25519",
        "value": "8YZP1BmtRJDd0uVKxQbxxDPZ3KjWGxX7EcMWYGbrUKk="
      },
      "voting_power": "0"
    }
  }
}
`
const seiStatusResponseFixture = `
{
  "node_info": {
    "protocol_version": {
      "p2p": "8",
      "block": "11",
      "app": "0"
    },
    "id": "a08b049a9d1909252f96973c3038db88b4b12883",
    "listen_addr": "65.109.78.7:11956",
    "network": "pacific-1",
    "version": "0.35.0-unreleased",
    "channels": "40202122233038606162630070717273",
    "moniker": "hello-sei-relayer",
    "other": {
      "tx_index": "on",
      "rpc_address": "tcp://0.0.0.0:11957"
    }
  },
  "application_info": {
    "version": "9"
  },
  "sync_info": {
    "latest_block_hash": "24F4FEF7C5B704C99B3C36964ECCA855B9480E07F711B0A5FB2C348A4CAC5D48",
    "latest_app_hash": "65271A3CA49CDC29FDC3A33974C508626F47CA178BCEA9421275E752824BC107",
    "latest_block_height": "37909189",
    "latest_block_time": "2023-11-09T17:36:20.235115543Z",
    "earliest_block_hash": "0127F5D1CF7B53007180EB11052BB2B54D06C75EDE94E0F6686EA1988464B5B9",
    "earliest_app_hash": "643CB8B56EA1A5F58D12396D26D8D89C7F9601FC199C6ACF76569BFCEE8C548C",
    "earliest_block_height": "33517999",
    "earliest_block_time": "2023-10-20T18:56:19.244399909Z",
    "max_peer_block_height": "37909188",
    "catching_up": false,
    "total_synced_time": "0",
    "remaining_time": "0",
    "total_snapshots": "0",
    "chunk_process_avg_time": "0",
    "snapshot_height": "0",
    "snapshot_chunks_count": "0",
    "snapshot_chunks_total": "0",
    "backfilled_blocks": "0",
    "backfill_blocks_total": "0"
  },
  "validator_info": {
    "address": "FD7DCAA2E2C25770720E844E0FEA6D0004940B02",
    "pub_key": {
      "type": "tendermint/PubKeyEd25519",
      "value": "7znc+z+uh4QFjLHTQPgSrXZvUpFRNMM9hjQCXNZYtyA="
    },
    "voting_power": "0"
  }
}
`

'''
'''--- internal/cosmos/status_collection.go ---
package cosmos

import (
	"errors"
	"strconv"
	"time"

	"github.com/samber/lo"
	"github.com/strangelove-ventures/cosmos-operator/internal/kube"
	corev1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/types"
)

// StatusItem is a pod paired with its CometBFT status.
type StatusItem struct {
	Pod    *corev1.Pod
	Status CometStatus
	TS     time.Time
	Err    error
}

// GetPod returns the pod.
func (status StatusItem) GetPod() *corev1.Pod {
	return status.Pod
}

// GetStatus returns the CometBFT status or an error if the status could not be fetched.
func (status StatusItem) GetStatus() (CometStatus, error) {
	return status.Status, status.Err
}

// Timestamp returns the time when the CometBFT status was fetched.
func (status StatusItem) Timestamp() time.Time { return status.TS }

// StatusCollection is a list of pods and CometBFT status associated with the pod.
type StatusCollection []StatusItem

// Len returns the number of items in the collection. Part of the sort.Interface implementation.
func (coll StatusCollection) Len() int { return len(coll) }

// Less implements sort.Interface.
func (coll StatusCollection) Less(i, j int) bool {
	lhs, _ := strconv.Atoi(coll[i].GetPod().Annotations[kube.OrdinalAnnotation])
	rhs, _ := strconv.Atoi(coll[j].GetPod().Annotations[kube.OrdinalAnnotation])
	return lhs < rhs
}

// Swap implements sort.Interface.
func (coll StatusCollection) Swap(i, j int) {
	coll[i], coll[j] = coll[j], coll[i]
}

// UpsertPod updates the pod in the collection or adds an item to the collection if it does not exist.
// All operations are performed in-place.
func UpsertPod(coll *StatusCollection, pod *corev1.Pod) {
	if *coll == nil {
		*coll = make(StatusCollection, 0)
	}
	for i, p := range *coll {
		if p.GetPod().UID == pod.UID {
			item := (*coll)[i]
			item.Pod = pod
			(*coll)[i] = item
			return
		}
	}
	*coll = append(*coll, StatusItem{Pod: pod, TS: time.Now(), Err: errors.New("missing status")})
}

// IntersectPods removes all pods from the collection that are not in the given list.
func IntersectPods(coll *StatusCollection, pods []corev1.Pod) {
	if *coll == nil {
		*coll = make(StatusCollection, 0)
		return
	}

	set := make(map[types.UID]bool)
	for _, pod := range pods {
		set[pod.UID] = true
	}

	var j int
	for _, item := range *coll {
		if set[item.GetPod().UID] {
			(*coll)[j] = item
			j++
		}
	}
	*coll = (*coll)[:j]
}

// Pods returns all pods.
func (coll StatusCollection) Pods() []*corev1.Pod {
	return lo.Map(coll, func(status StatusItem, _ int) *corev1.Pod { return status.GetPod() })
}

// Synced returns all items that are caught up with the chain tip.
func (coll StatusCollection) Synced() StatusCollection {
	var items []StatusItem
	for _, status := range coll {
		if status.Err != nil {
			continue
		}
		if status.Status.Result.SyncInfo.CatchingUp {
			continue
		}
		items = append(items, status)
	}
	return items
}

// SyncedPods returns the pods that are caught up with the chain tip.
func (coll StatusCollection) SyncedPods() []*corev1.Pod {
	return lo.Map(coll.Synced(), func(status StatusItem, _ int) *corev1.Pod { return status.GetPod() })
}

'''
'''--- internal/cosmos/status_collection_test.go ---
package cosmos

import (
	"errors"
	"testing"
	"time"

	"github.com/stretchr/testify/require"
	corev1 "k8s.io/api/core/v1"
)

func TestStatusCollection_Synced(t *testing.T) {
	t.Parallel()

	var coll StatusCollection
	require.Empty(t, coll.Synced())
	require.Empty(t, coll.SyncedPods())

	var catchingUp CometStatus
	catchingUp.Result.SyncInfo.CatchingUp = true

	coll = StatusCollection{
		{Pod: &corev1.Pod{}, Status: catchingUp},
		{Pod: &corev1.Pod{}, Err: errors.New("some error")},
	}

	require.Empty(t, coll.Synced())
	require.Empty(t, coll.SyncedPods())

	var pod corev1.Pod
	pod.Name = "in-sync"
	coll = append(coll, StatusItem{Pod: &pod})

	require.Len(t, coll.Synced(), 1)
	require.Len(t, coll.SyncedPods(), 1)
	require.Equal(t, "in-sync", coll.SyncedPods()[0].Name)
}

func TestUpsertPod(t *testing.T) {
	t.Parallel()

	var coll StatusCollection
	var pod corev1.Pod
	pod.UID = "1"
	UpsertPod(&coll, &pod)

	require.Len(t, coll, 1)

	pod.Name = "new"
	UpsertPod(&coll, &pod)

	require.Len(t, coll, 1)
	require.Equal(t, "new", coll[0].GetPod().Name)
	require.WithinDuration(t, time.Now(), coll[0].Timestamp(), 10*time.Second)

	UpsertPod(&coll, &corev1.Pod{})
	require.Len(t, coll, 2)

	ts := time.Now()
	status := CometStatus{ID: 1}
	err := errors.New("some error")
	coll = StatusCollection{
		{Pod: &pod, TS: ts, Status: status, Err: err},
	}
	var pod2 corev1.Pod
	pod2.UID = "1"
	pod2.Name = "new2"
	UpsertPod(&coll, &pod2)

	require.Len(t, coll, 1)
	want := StatusItem{Pod: &pod2, TS: ts, Status: status, Err: err}
	require.Equal(t, want, coll[0])
}

func TestIntersectPods(t *testing.T) {
	t.Parallel()

	var coll StatusCollection
	var pod corev1.Pod
	pod.UID = "1"

	IntersectPods(&coll, []corev1.Pod{pod})
	require.NotNil(t, coll)
	require.Len(t, coll, 0)

	var pod2 corev1.Pod
	pod2.UID = "2"

	coll = append(coll, StatusItem{Pod: &pod})
	coll = append(coll, StatusItem{Pod: &pod2})

	IntersectPods(&coll, []corev1.Pod{pod})
	require.Len(t, coll, 1)
	require.Equal(t, "1", string(coll[0].GetPod().UID))
}

'''
'''--- internal/cosmos/status_collector.go ---
package cosmos

import (
	"context"
	"errors"
	"fmt"
	"sort"
	"time"

	"golang.org/x/sync/errgroup"
	corev1 "k8s.io/api/core/v1"
)

// Statuser calls the RPC status endpoint.
type Statuser interface {
	Status(ctx context.Context, rpcHost string) (CometStatus, error)
}

// StatusCollector collects the CometBFT status of all pods owned by a controller.
type StatusCollector struct {
	comet   Statuser
	timeout time.Duration
}

// NewStatusCollector returns a valid StatusCollector.
// Timeout is exposed here because it is important for good performance in reconcile loops,
// and reminds callers to set it.
func NewStatusCollector(comet Statuser, timeout time.Duration) *StatusCollector {
	return &StatusCollector{comet: comet, timeout: timeout}
}

// Collect returns a StatusCollection for the given pods.
// Any non-nil error can be treated as transient and retried.
func (coll StatusCollector) Collect(ctx context.Context, pods []corev1.Pod) StatusCollection {
	var eg errgroup.Group
	now := time.Now()
	statuses := make(StatusCollection, len(pods))

	for i := range pods {
		i := i
		eg.Go(func() error {
			pod := pods[i]
			statuses[i].TS = now
			statuses[i].Pod = &pod
			ip := pod.Status.PodIP
			if ip == "" {
				// Check for IP, so we don't pay overhead of making a request.
				statuses[i].Err = errors.New("pod has no IP")
				return nil
			}
			host := fmt.Sprintf("http://%s:26657", ip)
			cctx, cancel := context.WithTimeout(ctx, coll.timeout)
			defer cancel()
			resp, err := coll.comet.Status(cctx, host)
			if err != nil {
				statuses[i].Err = err
				return nil
			}
			statuses[i].Status = resp
			return nil
		})
	}

	_ = eg.Wait()
	sort.Sort(statuses)
	return statuses
}

'''
'''--- internal/cosmos/status_collector_test.go ---
package cosmos

import (
	"context"
	"errors"
	"fmt"
	"strconv"
	"testing"
	"time"

	"github.com/samber/lo"
	"github.com/strangelove-ventures/cosmos-operator/internal/kube"
	"github.com/stretchr/testify/require"
	corev1 "k8s.io/api/core/v1"
)

type mockStatuser func(ctx context.Context, rpcHost string) (CometStatus, error)

func (fn mockStatuser) Status(ctx context.Context, rpcHost string) (CometStatus, error) {
	return fn(ctx, rpcHost)
}

var panicStatuser = mockStatuser(func(ctx context.Context, rpcHost string) (CometStatus, error) {
	panic("should not be called")
})

func TestStatusCollector_Collect(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	const (
		namespace = "default"
		timeout   = time.Second
	)

	t.Run("happy path", func(t *testing.T) {
		pods := lo.Map(lo.Range(3), func(i int, _ int) corev1.Pod {
			pod := corev1.Pod{
				Status: corev1.PodStatus{
					PodIP: strconv.Itoa(i),
				},
			}
			pod.Name = fmt.Sprintf("pod-%d", i)
			pod.Namespace = namespace
			pod.Annotations = map[string]string{kube.OrdinalAnnotation: strconv.Itoa(i)}
			return pod
		})
		lo.Reverse(pods) // Tests ordering

		cometClient := mockStatuser(func(ctx context.Context, rpcHost string) (CometStatus, error) {
			_, ok := ctx.Deadline()
			if !ok {
				require.Fail(t, "expected deadline in context")
			}
			var status CometStatus
			status.Result.NodeInfo.ListenAddr = rpcHost
			return status, nil
		})

		coll := NewStatusCollector(cometClient, timeout)
		got := coll.Collect(ctx, pods)

		require.Len(t, got, 3)

		for i, item := range got {
			require.Equal(t, namespace, item.GetPod().Namespace)
			require.Equal(t, fmt.Sprintf("pod-%d", i), item.GetPod().Name)

			tmStatus, err := item.GetStatus()
			require.NoError(t, err)

			require.Equal(t, fmt.Sprintf("http://%d:26657", i), tmStatus.Result.NodeInfo.ListenAddr)
			require.WithinDuration(t, time.Now(), item.Timestamp(), 10*time.Second)
		}

		timestamps := lo.Map(got, func(item StatusItem, _ int) time.Time { return item.Timestamp() })
		require.Len(t, lo.Uniq(timestamps), 1)
	})

	t.Run("no pod IP", func(t *testing.T) {
		coll := NewStatusCollector(panicStatuser, timeout)
		got := coll.Collect(ctx, make([]corev1.Pod, 1))

		require.Len(t, got, 1)

		_, err := got[0].GetStatus()
		require.Error(t, err)
		require.EqualError(t, err, "pod has no IP")
		require.NotZero(t, got[0].Timestamp())
	})

	t.Run("status error", func(t *testing.T) {
		cometClient := mockStatuser(func(ctx context.Context, rpcHost string) (CometStatus, error) {
			return CometStatus{}, errors.New("status error")
		})
		coll := NewStatusCollector(cometClient, timeout)
		var pod corev1.Pod
		pod.Status.PodIP = "1.1.1.1"
		got := coll.Collect(ctx, []corev1.Pod{pod})

		require.Len(t, got, 1)

		_, err := got[0].GetStatus()
		require.Error(t, err)
		require.EqualError(t, err, "status error")
		require.NotZero(t, got[0].Timestamp())
	})

	t.Run("no pods", func(t *testing.T) {
		coll := NewStatusCollector(panicStatuser, timeout)
		got := coll.Collect(ctx, nil)

		require.Empty(t, got)
	})
}

'''
'''--- internal/diff/adapter.go ---
package diff

import (
	"encoding/hex"
	"encoding/json"
	"hash/fnv"

	"golang.org/x/exp/constraints"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

// Adapt adapts a kubernetes client.Object into a diffable Resource which can be used by Diff.
// The revision is an FNV-1 hash of the object's JSON representation.
func Adapt[T client.Object, I constraints.Integer](obj T, ordinal I) Resource[T] {
	b, err := json.Marshal(obj)
	if err != nil {
		// If we can't marshal a kube object, something is very wrong.
		panic(err)
	}
	h := fnv.New32()
	_, err = h.Write(b)
	if err != nil {
		// Similarly, if this write fails, something is very wrong.
		panic(err)
	}
	rev := hex.EncodeToString(h.Sum(nil))
	return adapter[T, I]{obj: obj, ordinal: ordinal, revision: rev}
}

type adapter[T client.Object, I constraints.Integer] struct {
	obj      T
	ordinal  I
	revision string
}

func (a adapter[T, I]) Object() T        { return a.obj }
func (a adapter[T, I]) Revision() string { return a.revision }
func (a adapter[T, I]) Ordinal() int64   { return int64(a.ordinal) }

'''
'''--- internal/diff/adapter_test.go ---
package diff

import (
	"testing"

	"github.com/stretchr/testify/require"
	corev1 "k8s.io/api/core/v1"
)

func TestAdapt(t *testing.T) {
	t.Parallel()

	var pod corev1.Pod
	pod.Name = "test"
	pod.Namespace = "default"
	pod.Spec.NodeName = "node-1"

	got := Adapt(&pod, 1)

	require.Same(t, &pod, got.Object())
	require.Equal(t, int64(1), got.Ordinal())
	require.NotEmpty(t, got.Revision())

	got2 := Adapt(&pod, 1)
	require.Equal(t, got.Revision(), got2.Revision())

	pod.Labels = map[string]string{"foo": "bar"}
	got3 := Adapt(&pod, 1)
	require.NotEqual(t, got.Revision(), got3.Revision())
}

'''
'''--- internal/diff/diff.go ---
package diff

import (
	"errors"
	"sort"
	"strconv"

	"github.com/strangelove-ventures/cosmos-operator/internal/kube"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

const revisionLabel = "app.kubernetes.io/revision"

// Resource is a diffable kubernetes object.
type Resource[T client.Object] interface {
	Object() T
	// Revision returns a unique identifier for the resource. A simple hash of the resource is sufficient.
	Revision() string
	// Ordinal returns the ordinal position of the resource. If order doesn't matter, return 0.
	Ordinal() int64
}

type ordinalSet[T client.Object] map[client.ObjectKey]Resource[T]

// Diff computes steps needed to bring a current state equal to a new state.
// Diff will add annotations to created and updated resources required for future diffing.
//
// There are several O(N) or O(2N) operations; However, we expect N to be small.
type Diff[T client.Object] struct {
	creates, deletes, updates []T
}

// New creates a valid Diff.
func New[T client.Object](current []T, want []Resource[T]) *Diff[T] {
	d := &Diff[T]{}

	currentSet := d.currentToSet(current)
	if len(currentSet) != len(current) {
		panic(errors.New("each resource in current must have unique .metadata.name"))
	}

	wantSet := d.toSet(want)
	if len(wantSet) != len(want) {
		panic(errors.New("each resource in want must have unique .metadata.name"))
	}

	d.creates = d.computeCreates(currentSet, wantSet)
	d.deletes = d.computeDeletes(currentSet, wantSet)
	d.updates = d.computeUpdates(currentSet, wantSet)
	return d
}

// Creates returns a list of resources that should be created from scratch.
// Calls are memoized, so you can call this method multiple times without incurring additional cost.
// Adds labels and annotations on the resource to aid in future diffing.
func (diff *Diff[T]) Creates() []T {
	return diff.creates
}

func (diff *Diff[T]) computeCreates(current, want ordinalSet[T]) []T {
	var creates []Resource[T]
	for objKey, resource := range want {
		_, ok := current[objKey]
		if !ok {
			creates = append(creates, resource)
		}
	}
	return diff.toObjects(diff.sortByOrdinal(creates))
}

// Deletes returns a list of resources that should be deleted.
// Calls are memoized, so you can call this method multiple times without incurring additional cost.
func (diff *Diff[T]) Deletes() []T {
	return diff.deletes
}

func (diff *Diff[T]) computeDeletes(current, want ordinalSet[T]) []T {
	var deletes []Resource[T]
	for objKey, resource := range current {
		_, ok := want[objKey]
		if !ok {
			deletes = append(deletes, resource)
		}
	}
	return diff.toObjects(diff.sortByOrdinal(deletes))
}

// Updates returns a list of resources that should be updated.
// Calls are memoized, so you can call this method multiple times without incurring additional cost.
func (diff *Diff[T]) Updates() []T {
	return diff.updates
}

func (diff *Diff[T]) computeUpdates(current, want ordinalSet[T]) []T {
	var updates []Resource[T]
	for _, existing := range current {
		target, ok := want[client.ObjectKeyFromObject(existing.Object())]
		if !ok {
			continue
		}
		// These values are necessary to be accepted by the API server.
		target.Object().SetResourceVersion(existing.Object().GetResourceVersion())
		target.Object().SetUID(existing.Object().GetUID())
		target.Object().SetGeneration(existing.Object().GetGeneration())
		target.Object().SetOwnerReferences(existing.Object().GetOwnerReferences())
		var (
			oldRev = existing.Revision()
			newRev = target.Revision()
		)
		if oldRev != newRev {
			updates = append(updates, target)
		}
	}

	return diff.toObjects(diff.sortByOrdinal(updates))
}

type currentAdapter[T client.Object] struct {
	obj T
}

func (a currentAdapter[T]) Object() T        { return a.obj }
func (a currentAdapter[T]) Revision() string { return a.obj.GetLabels()[revisionLabel] }

func (a currentAdapter[T]) Ordinal() int64 {
	val, _ := strconv.ParseInt(a.obj.GetAnnotations()[kube.OrdinalAnnotation], 10, 64)
	return val
}

func (diff *Diff[T]) currentToSet(current []T) ordinalSet[T] {
	m := make(ordinalSet[T])
	for i := range current {
		r := current[i]
		m[client.ObjectKeyFromObject(r)] = currentAdapter[T]{r}
	}
	return m
}

func (diff *Diff[T]) toSet(list []Resource[T]) ordinalSet[T] {
	m := make(ordinalSet[T])
	for i := range list {
		r := list[i]
		m[client.ObjectKeyFromObject(r.Object())] = r
	}
	return m
}

func (diff *Diff[T]) sortByOrdinal(list []Resource[T]) []Resource[T] {
	sort.Slice(list, func(i, j int) bool {
		return list[i].Ordinal() < list[j].Ordinal()
	})
	return list
}

func (diff *Diff[T]) toObjects(list []Resource[T]) []T {
	objs := make([]T, len(list))
	for i := range list {
		obj := list[i].Object()

		labels := obj.GetLabels()
		if labels == nil {
			labels = make(map[string]string)
		}
		labels[revisionLabel] = list[i].Revision()
		obj.SetLabels(labels)

		annotations := obj.GetAnnotations()
		if annotations == nil {
			annotations = make(map[string]string)
		}
		annotations[kube.OrdinalAnnotation] = strconv.FormatInt(list[i].Ordinal(), 10)
		obj.SetAnnotations(annotations)

		objs[i] = obj
	}
	return objs
}

'''
'''--- internal/diff/diff_test.go ---
package diff

import (
	"fmt"
	"testing"

	"github.com/samber/lo"
	"github.com/stretchr/testify/require"
	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

type diffAdapter struct {
	*corev1.Pod
	revision string
	ordinal  int64
}

func (d diffAdapter) Revision() string    { return d.revision }
func (d diffAdapter) Ordinal() int64      { return d.ordinal }
func (d diffAdapter) Object() *corev1.Pod { return d.Pod }

func diffablePod(ordinal int, revision string) diffAdapter {
	p := new(corev1.Pod)
	p.Name = fmt.Sprintf("pod-%d", ordinal)
	p.Namespace = "default"
	return diffAdapter{Pod: p, ordinal: int64(ordinal), revision: revision}
}

func TestOrdinalDiff_CreatesDeletesUpdates(t *testing.T) {
	t.Parallel()

	t.Run("create", func(t *testing.T) {
		current := []*corev1.Pod{
			{ObjectMeta: metav1.ObjectMeta{Name: "pod-0", Namespace: "default", Labels: map[string]string{revisionLabel: "rev"}}},
		}

		// Purposefully unordered
		want := []Resource[*corev1.Pod]{
			diffablePod(110, "rev-110"),
			diffablePod(1, "rev-1"),
			diffablePod(0, "rev"),
		}

		diff := New(current, want)

		require.Len(t, diff.Creates(), 2)

		require.Equal(t, "pod-1", diff.Creates()[0].Name)
		require.Equal(t, "rev-1", diff.Creates()[0].Labels["app.kubernetes.io/revision"])
		require.Equal(t, "1", diff.Creates()[0].Annotations["app.kubernetes.io/ordinal"])

		require.Equal(t, "pod-110", diff.Creates()[1].Name)
		require.Equal(t, "rev-110", diff.Creates()[1].Labels["app.kubernetes.io/revision"])
		require.Equal(t, "110", diff.Creates()[1].Annotations["app.kubernetes.io/ordinal"])

		require.Empty(t, diff.Deletes())
		require.Empty(t, diff.Updates())
	})

	t.Run("no current resources", func(t *testing.T) {
		var current []*corev1.Pod
		want := []Resource[*corev1.Pod]{
			diffablePod(0, "rev"),
		}
		diff := New(current, want)

		require.Len(t, diff.Creates(), 1)

		require.Empty(t, diff.Deletes())
		require.Empty(t, diff.Updates())
	})

	t.Run("simple delete", func(t *testing.T) {
		// Purposefully unordered to test lexical sorting.
		existing := []Resource[*corev1.Pod]{
			diffablePod(0, "doesn't matter"),
			diffablePod(22, "doesn't matter"),
			diffablePod(2, "doesn't matter"),
		}
		diff := New(nil, existing)
		current := diff.Creates()

		want := []Resource[*corev1.Pod]{
			diffablePod(0, "doesn't matter"),
		}
		diff = New(current, want)

		require.Len(t, diff.Deletes(), 2)
		require.Equal(t, diff.Deletes()[0].Name, "pod-2")
		require.Equal(t, diff.Deletes()[1].Name, "pod-22")

		require.Empty(t, diff.Creates())
		require.Empty(t, diff.Updates())
	})

	t.Run("updates", func(t *testing.T) {
		pod1 := diffablePod(1, "rev-1")
		pod1.SetGeneration(1)
		pod1.SetUID("uuid1")
		pod1.SetResourceVersion("rv1")
		pod1.SetOwnerReferences([]metav1.OwnerReference{{Name: "owner1"}})

		pod2 := diffablePod(11, "rev-11")
		pod2.SetGeneration(11)
		pod2.SetUID("uuid11")
		pod2.SetResourceVersion("rv11")
		pod2.SetOwnerReferences([]metav1.OwnerReference{{Name: "owner11"}})

		existing := []Resource[*corev1.Pod]{pod1, pod2}
		diff := New(nil, existing)
		current := diff.Creates()

		// Purposefully unordered to test lexical sorting.
		want := []Resource[*corev1.Pod]{
			diffablePod(11, "changed-11"),
			diffablePod(1, "changed-1"),
		}
		diff = New(current, want)

		require.Len(t, diff.Updates(), 2)
		require.Equal(t, "pod-1", diff.Updates()[0].Name)
		require.Equal(t, "changed-1", diff.Updates()[0].Labels["app.kubernetes.io/revision"])
		require.Equal(t, "1", diff.Updates()[0].Annotations["app.kubernetes.io/ordinal"])
		require.Equal(t, int64(1), diff.Updates()[0].Generation)
		require.Equal(t, "uuid1", string(diff.Updates()[0].UID))
		require.Equal(t, "rv1", diff.Updates()[0].ResourceVersion)
		require.Equal(t, "owner1", diff.Updates()[0].OwnerReferences[0].Name)

		require.Equal(t, "pod-11", diff.Updates()[1].Name)
		require.Equal(t, "changed-11", diff.Updates()[1].Labels["app.kubernetes.io/revision"])
		require.Equal(t, "11", diff.Updates()[1].Annotations["app.kubernetes.io/ordinal"])
		require.Equal(t, int64(11), diff.Updates()[1].Generation)
		require.Equal(t, "uuid11", string(diff.Updates()[1].UID))
		require.Equal(t, "rv11", diff.Updates()[1].ResourceVersion)
		require.Equal(t, "owner11", diff.Updates()[1].OwnerReferences[0].Name)

		require.Empty(t, diff.Creates())
		require.Empty(t, diff.Deletes())
	})

	t.Run("combination", func(t *testing.T) {
		existing := []Resource[*corev1.Pod]{
			diffablePod(0, "1"),
			diffablePod(1, "1"),
			diffablePod(2, "1"),
		}
		diff := New(nil, existing)
		current := diff.Creates()

		// Purposefully unordered to test lexical sorting.
		want := []Resource[*corev1.Pod]{
			diffablePod(0, "changed-1"),
			diffablePod(1, "1"), // no change
			// pod-2 is deleted
			diffablePod(3, "doesn't mater"),
		}

		diff = New(current, want)

		created := lo.Map(diff.Creates(), func(pod *corev1.Pod, _ int) string { return pod.Name })
		require.Equal(t, []string{"pod-3"}, created)

		updated := lo.Map(diff.Updates(), func(pod *corev1.Pod, _ int) string { return pod.Name })
		require.Equal(t, []string{"pod-0"}, updated)

		deleted := lo.Map(diff.Deletes(), func(pod *corev1.Pod, _ int) string { return pod.Name })
		require.Equal(t, []string{"pod-2"}, deleted)
	})
}

'''
'''--- internal/fullnode/addrbook.go ---
package fullnode

import (
	_ "embed"
	"fmt"

	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
)

var (
	//go:embed script/download-addrbook.sh
	scriptDownloadAddrbook string
)

const addrbookScriptWrapper = `ls $CONFIG_DIR/addrbook.json 1> /dev/null 2>&1
ADDRBOOK_EXISTS=$?
if [ $ADDRBOOK_EXISTS -eq 0 ]; then
	echo "Address book already exists"
	exit 0
fi
ls -l $CONFIG_DIR/addrbook.json 
%s
ls -l $CONFIG_DIR/addrbook.json 

echo "Address book $ADDRBOOK_FILE downloaded"
`

// DownloadGenesisCommand returns a proper address book command for use in an init container.
func DownloadAddrbookCommand(cfg cosmosv1.ChainSpec) (string, []string) {
	args := []string{"-c"}
	switch {
	case cfg.AddrbookScript != nil:
		args = append(args, fmt.Sprintf(addrbookScriptWrapper, *cfg.AddrbookScript))
	case cfg.AddrbookURL != nil:
		args = append(args, fmt.Sprintf(addrbookScriptWrapper, scriptDownloadAddrbook), "-s", *cfg.AddrbookURL)
	default:
		args = append(args, "echo Using default address book")
	}
	return "sh", args
}

'''
'''--- internal/fullnode/addrbook_test.go ---
package fullnode

import (
	"testing"

	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	"github.com/stretchr/testify/require"
)

func TestDownloadAddrbookCommand(t *testing.T) {
	t.Parallel()

	requireValidScript := func(t *testing.T, script string) {
		t.Helper()
		require.NotEmpty(t, script)
		require.Contains(t, script, `if [ $ADDRBOOK_EXISTS -eq 0 ]`)
	}

	t.Run("default", func(t *testing.T) {
		var cfg cosmosv1.ChainSpec

		cmd, args := DownloadAddrbookCommand(cfg)
		require.Equal(t, "sh", cmd)

		require.Len(t, args, 2)

		require.Equal(t, "-c", args[0])

		got := args[1]
		require.NotContains(t, got, "ADDRBOOK_EXISTS")
		require.Contains(t, got, "Using default address book")
	})

	t.Run("download", func(t *testing.T) {
		cfg := cosmosv1.ChainSpec{
			AddrbookURL: ptr("https://example.com/addrbook.json"),
		}
		cmd, args := DownloadAddrbookCommand(cfg)
		require.Equal(t, "sh", cmd)

		require.Len(t, args, 4)

		require.Equal(t, "-c", args[0])
		got := args[1]
		requireValidScript(t, got)
		require.Contains(t, got, `ADDRBOOK_URL`)
		require.Contains(t, got, "download_json")

		require.Equal(t, "-s", args[2])
		require.Equal(t, "https://example.com/addrbook.json", args[3])
	})

	t.Run("custom", func(t *testing.T) {
		cfg := cosmosv1.ChainSpec{
			// Keeping this to assert that custom script takes precedence.
			AddrbookURL:    ptr("https://example.com/addrbook.json"),
			AddrbookScript: ptr("echo hi"),
		}
		cmd, args := DownloadAddrbookCommand(cfg)
		require.Equal(t, "sh", cmd)

		require.Len(t, args, 2)

		require.Equal(t, "-c", args[0])

		got := args[1]
		requireValidScript(t, got)

		require.NotContains(t, got, "ADDRBOOK_URL")
		require.Contains(t, got, "echo hi")
	})
}

'''
'''--- internal/fullnode/build_pods.go ---
package fullnode

import (
	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	"github.com/strangelove-ventures/cosmos-operator/internal/diff"
	corev1 "k8s.io/api/core/v1"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

const (
	configChecksumAnnotation = "cosmos.strange.love/config-checksum"
)

// BuildPods creates the final state of pods given the crd.
func BuildPods(crd *cosmosv1.CosmosFullNode, cksums ConfigChecksums) ([]diff.Resource[*corev1.Pod], error) {
	var (
		builder = NewPodBuilder(crd)
		pods    []diff.Resource[*corev1.Pod]
	)
	candidates := podCandidates(crd)
	for i := int32(0); i < crd.Spec.Replicas; i++ {
		pod, err := builder.WithOrdinal(i).Build()
		if err != nil {
			return nil, err
		}

		if pod == nil {
			continue
		}

		if _, shouldSnapshot := candidates[pod.Name]; shouldSnapshot {
			continue
		}

		pod.Annotations[configChecksumAnnotation] = cksums[client.ObjectKeyFromObject(pod)]
		pods = append(pods, diff.Adapt(pod, i))
	}
	return pods, nil
}

func setChainContainerImage(pod *corev1.Pod, image string) {
	for i := range pod.Spec.Containers {
		if pod.Spec.Containers[i].Name == mainContainer {
			pod.Spec.Containers[i].Image = image
			break
		}
	}
	for i := range pod.Spec.InitContainers {
		if pod.Spec.InitContainers[i].Name == chainInitContainer {
			pod.Spec.InitContainers[i].Image = image
			break
		}
	}
}

func podCandidates(crd *cosmosv1.CosmosFullNode) map[string]struct{} {
	candidates := make(map[string]struct{})
	for _, v := range crd.Status.ScheduledSnapshotStatus {
		candidates[v.PodCandidate] = struct{}{}
	}
	return candidates
}

'''
'''--- internal/fullnode/build_pods_test.go ---
package fullnode

import (
	"fmt"
	"strconv"
	"testing"

	"github.com/samber/lo"
	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	"github.com/strangelove-ventures/cosmos-operator/internal/diff"
	"github.com/stretchr/testify/require"
	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

func TestBuildPods(t *testing.T) {
	t.Parallel()

	t.Run("happy path", func(t *testing.T) {
		crd := &cosmosv1.CosmosFullNode{
			ObjectMeta: metav1.ObjectMeta{
				Name:      "agoric",
				Namespace: "test",
			},
			Spec: cosmosv1.FullNodeSpec{
				Replicas:  5,
				ChainSpec: cosmosv1.ChainSpec{Network: "devnet"},
				PodTemplate: cosmosv1.PodSpec{
					Image: "busybox:latest",
				},
				InstanceOverrides: nil,
			},
		}

		cksums := make(ConfigChecksums)
		for i := 0; i < int(crd.Spec.Replicas); i++ {
			cksums[client.ObjectKey{Namespace: crd.Namespace, Name: fmt.Sprintf("agoric-%d", i)}] = strconv.Itoa(i)
		}

		pods, err := BuildPods(crd, cksums)
		require.NoError(t, err)
		require.Equal(t, 5, len(pods))

		for i, r := range pods {
			require.Equal(t, int64(i), r.Ordinal(), i)
			require.NotEmpty(t, r.Revision(), i)
			require.Equal(t, strconv.Itoa(i), r.Object().Annotations["cosmos.strange.love/config-checksum"])
		}

		want := lo.Map([]int{0, 1, 2, 3, 4}, func(_ int, i int) string {
			return fmt.Sprintf("agoric-%d", i)
		})
		got := lo.Map(pods, func(pod diff.Resource[*corev1.Pod], _ int) string { return pod.Object().Name })
		require.Equal(t, want, got)

		pod, err := NewPodBuilder(crd).WithOrdinal(0).Build()
		require.NoError(t, err)
		require.Equal(t, pod.Spec, pods[0].Object().Spec)
	})

	t.Run("instance overrides", func(t *testing.T) {
		const (
			image         = "agoric:latest"
			overrideImage = "some_image:custom"
			overridePod   = "agoric-5"
		)
		crd := &cosmosv1.CosmosFullNode{
			ObjectMeta: metav1.ObjectMeta{
				Name: "agoric",
			},
			Spec: cosmosv1.FullNodeSpec{
				Replicas: 6,
				PodTemplate: cosmosv1.PodSpec{
					Image: image,
				},
				InstanceOverrides: map[string]cosmosv1.InstanceOverridesSpec{
					"agoric-2":  {DisableStrategy: ptr(cosmosv1.DisablePod)},
					"agoric-4":  {DisableStrategy: ptr(cosmosv1.DisableAll)},
					overridePod: {Image: overrideImage},
				},
			},
		}

		pods, err := BuildPods(crd, nil)
		require.NoError(t, err)
		require.Equal(t, 4, len(pods))

		want := lo.Map([]int{0, 1, 3, 5}, func(i int, _ int) string {
			return fmt.Sprintf("agoric-%d", i)
		})
		got := lo.Map(pods, func(pod diff.Resource[*corev1.Pod], _ int) string { return pod.Object().Name })
		require.Equal(t, want, got)
		for _, pod := range pods {
			image := pod.Object().Spec.Containers[0].Image
			if pod.Object().Name == overridePod {
				require.Equal(t, overrideImage, image)
			} else {
				require.Equal(t, image, image)
			}
		}
	})

	t.Run("scheduled volume snapshot pod candidate", func(t *testing.T) {
		crd := &cosmosv1.CosmosFullNode{
			ObjectMeta: metav1.ObjectMeta{
				Name: "agoric",
			},
			Spec: cosmosv1.FullNodeSpec{
				Replicas: 6,
			},
			Status: cosmosv1.FullNodeStatus{
				ScheduledSnapshotStatus: map[string]cosmosv1.FullNodeSnapshotStatus{
					"some.scheduled.snapshot.1":       {PodCandidate: "agoric-1"},
					"some.scheduled.snapshot.2":       {PodCandidate: "agoric-2"},
					"some.scheduled.snapshot.ignored": {PodCandidate: "agoric-99"},
				},
			},
		}

		pods, err := BuildPods(crd, nil)
		require.NoError(t, err)
		require.Equal(t, 4, len(pods))

		want := lo.Map([]int{0, 3, 4, 5}, func(i int, _ int) string {
			return fmt.Sprintf("agoric-%d", i)
		})
		got := lo.Map(pods, func(pod diff.Resource[*corev1.Pod], _ int) string { return pod.Object().Name })
		require.Equal(t, want, got)
	})
}

'''
'''--- internal/fullnode/client.go ---
package fullnode

import (
	"context"

	"sigs.k8s.io/controller-runtime/pkg/client"
)

// Reader lists and gets objects.
type Reader = client.Reader

// Lister can list resources, subset of client.Client.
type Lister interface {
	List(ctx context.Context, list client.ObjectList, opts ...client.ListOption) error
}

type Getter interface {
	Get(ctx context.Context, key client.ObjectKey, obj client.Object, opts ...client.GetOption) error
}

// StatusPatcher patches the status subresource of a resource.
type StatusPatcher interface {
	Patch(ctx context.Context, obj client.Object, patch client.Patch, opts ...client.PatchOption) error
}

'''
'''--- internal/fullnode/configmap_builder.go ---
package fullnode

import (
	"bytes"
	_ "embed"
	"fmt"
	"strconv"
	"strings"

	"github.com/BurntSushi/toml"
	"github.com/peterbourgon/mergemap"
	"github.com/samber/lo"
	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	"github.com/strangelove-ventures/cosmos-operator/internal/diff"
	"github.com/strangelove-ventures/cosmos-operator/internal/kube"
	corev1 "k8s.io/api/core/v1"
)

const (
	configOverlayFile = "config-overlay.toml"
	appOverlayFile    = "app-overlay.toml"
)

// BuildConfigMaps creates a ConfigMap with configuration to be mounted as files into containers.
// Currently, the config.toml (for Comet) and app.toml (for the Cosmos SDK).
func BuildConfigMaps(crd *cosmosv1.CosmosFullNode, peers Peers) ([]diff.Resource[*corev1.ConfigMap], error) {
	var (
		buf = bufPool.Get().(*bytes.Buffer)
		cms = make([]diff.Resource[*corev1.ConfigMap], crd.Spec.Replicas)
	)
	defer bufPool.Put(buf)
	defer buf.Reset()

	for i := int32(0); i < crd.Spec.Replicas; i++ {
		data := make(map[string]string)
		instance := instanceName(crd, i)
		if err := addConfigToml(buf, data, crd, instance, peers); err != nil {
			return nil, err
		}
		buf.Reset()
		appCfg := crd.Spec.ChainSpec.App
		if len(crd.Spec.ChainSpec.Versions) > 0 {
			instanceHeight := uint64(0)
			if height, ok := crd.Status.Height[instance]; ok {
				instanceHeight = height
			}
			haltHeight := uint64(0)
			for i, v := range crd.Spec.ChainSpec.Versions {
				if v.SetHaltHeight {
					haltHeight = v.UpgradeHeight
				} else {
					haltHeight = 0
				}
				if instanceHeight < v.UpgradeHeight {
					break
				}
				if i == len(crd.Spec.ChainSpec.Versions)-1 {
					haltHeight = 0
				}
			}
			appCfg.HaltHeight = ptr(haltHeight)
		}
		if err := addAppToml(buf, data, appCfg); err != nil {
			return nil, err
		}
		buf.Reset()

		var cm corev1.ConfigMap
		cm.Name = instanceName(crd, i)
		cm.Namespace = crd.Namespace
		cm.Kind = "ConfigMap"
		cm.APIVersion = "v1"
		cm.Labels = defaultLabels(crd,
			kube.InstanceLabel, instanceName(crd, i),
		)
		cm.Data = data
		kube.NormalizeMetadata(&cm.ObjectMeta)
		cms[i] = diff.Adapt(&cm, i)
	}

	return cms, nil
}

type decodedToml = map[string]any

//go:embed toml/comet_default_config.toml
var defaultCometToml []byte

func defaultComet() decodedToml {
	var data decodedToml
	if err := toml.Unmarshal(defaultCometToml, &data); err != nil {
		panic(err)
	}
	return data
}

//go:embed toml/app_default_config.toml
var defaultAppToml []byte

func defaultApp() decodedToml {
	var data decodedToml
	if err := toml.Unmarshal(defaultAppToml, &data); err != nil {
		panic(err)
	}
	return data
}

func addConfigToml(buf *bytes.Buffer, cmData map[string]string, crd *cosmosv1.CosmosFullNode, instance string, peers Peers) error {
	var (
		spec = crd.Spec.ChainSpec
		base = make(decodedToml)
	)

	if crd.Spec.Type == cosmosv1.Sentry {
		privVal := fmt.Sprintf("tcp://0.0.0.0:%d", privvalPort)
		base["priv_validator_laddr"] = privVal
		base["priv-validator-laddr"] = privVal
		// Disable indexing for sentries; they should not serve queries.

		txIndex := map[string]string{"indexer": "null"}
		base["tx_index"] = txIndex
		base["tx-index"] = txIndex
	}
	if v := spec.LogLevel; v != nil {
		base["log_level"] = v
		base["log-level"] = v
	}
	if v := spec.LogFormat; v != nil {
		base["log_format"] = v
		base["log-format"] = v
	}

	privatePeers := peers.Except(instance, crd.Namespace)
	privatePeerStr := commaDelimited(privatePeers.AllPrivate()...)
	comet := spec.Comet
	persistentPeers := commaDelimited(privatePeerStr, comet.PersistentPeers)
	p2p := decodedToml{
		"persistent_peers": persistentPeers,
		"persistent-peers": persistentPeers,
		"seeds":            comet.Seeds,
	}

	privateIDStr := commaDelimited(privatePeers.NodeIDs()...)
	privateIDs := commaDelimited(privateIDStr, comet.PrivatePeerIDs)
	if v := privateIDs; v != "" {
		p2p["private_peer_ids"] = v
		p2p["private-peer-ids"] = v
	}

	unconditionalIDs := commaDelimited(privateIDStr, comet.UnconditionalPeerIDs)
	if v := unconditionalIDs; v != "" {
		p2p["unconditional_peer_ids"] = v
		p2p["unconditional-peer-ids"] = v
	}

	if v := comet.MaxInboundPeers; v != nil {
		p2p["max_num_inbound_peers"] = comet.MaxInboundPeers
		p2p["max-num-inbound-peers"] = comet.MaxInboundPeers
	}
	if v := comet.MaxOutboundPeers; v != nil {
		p2p["max_num_outbound_peers"] = comet.MaxOutboundPeers
		p2p["max-num-outbound-peers"] = comet.MaxOutboundPeers
	}

	var externalOverride bool
	if crd.Spec.InstanceOverrides != nil {
		if override, ok := crd.Spec.InstanceOverrides[instance]; ok && override.ExternalAddress != nil {
			addr := *override.ExternalAddress
			p2p["external_address"] = addr
			p2p["external-address"] = addr
			externalOverride = true
		}
	}

	if !externalOverride {
		if v := peers.Get(instance, crd.Namespace).ExternalAddress; v != "" {
			p2p["external_address"] = v
			p2p["external-address"] = v
		}
	}

	base["p2p"] = p2p

	if v := comet.CorsAllowedOrigins; v != nil {
		base["rpc"] = decodedToml{
			"cors_allowed_origins": v,
			"cors-allowed-origins": v,
		}
	}

	dst := defaultComet()

	mergemap.Merge(dst, base)

	if overrides := comet.TomlOverrides; overrides != nil {
		var decoded decodedToml
		_, err := toml.Decode(*overrides, &decoded)
		if err != nil {
			return fmt.Errorf("invalid toml in comet overrides: %w", err)
		}
		mergemap.Merge(dst, decoded)
	}

	if err := toml.NewEncoder(buf).Encode(dst); err != nil {
		return err
	}
	cmData[configOverlayFile] = buf.String()
	return nil
}

func commaDelimited(s ...string) string {
	return strings.Join(lo.Compact(s), ",")
}

func addAppToml(buf *bytes.Buffer, cmData map[string]string, app cosmosv1.SDKAppConfig) error {
	base := make(decodedToml)
	base["minimum-gas-prices"] = app.MinGasPrice
	// Note: The name discrepancy "enable" vs. "enabled" is intentional; a known inconsistency within the app.toml.
	base["api"] = decodedToml{"enabled-unsafe-cors": app.APIEnableUnsafeCORS}
	base["grpc-web"] = decodedToml{"enable-unsafe-cors": app.GRPCWebEnableUnsafeCORS}

	if v := app.HaltHeight; v != nil {
		base["halt-height"] = v
	}

	if pruning := app.Pruning; pruning != nil {
		intStr := func(n *uint32) string {
			v := valOrDefault(n, ptr(uint32(0)))
			return strconv.FormatUint(uint64(*v), 10)
		}
		base["pruning"] = pruning.Strategy
		base["pruning-interval"] = intStr(pruning.Interval)
		base["pruning-keep-every"] = intStr(pruning.KeepEvery)
		base["pruning-keep-recent"] = intStr(pruning.KeepRecent)
		base["min-retain-blocks"] = valOrDefault(pruning.MinRetainBlocks, ptr(uint32(0)))
	}

	dst := defaultApp()
	mergemap.Merge(dst, base)

	if overrides := app.TomlOverrides; overrides != nil {
		var decoded decodedToml
		_, err := toml.Decode(*overrides, &decoded)
		if err != nil {
			return fmt.Errorf("invalid toml in app overrides: %w", err)
		}
		mergemap.Merge(dst, decoded)
	}

	if err := toml.NewEncoder(buf).Encode(dst); err != nil {
		return err
	}
	cmData[appOverlayFile] = buf.String()
	return nil
}

'''
'''--- internal/fullnode/configmap_builder_test.go ---
package fullnode

import (
	"bytes"
	_ "embed"
	"fmt"
	"strings"
	"testing"

	"github.com/BurntSushi/toml"
	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	"github.com/strangelove-ventures/cosmos-operator/internal/test"
	"github.com/stretchr/testify/require"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

var (
	//go:embed testdata/comet.toml
	wantComet string
	//go:embed testdata/comet_defaults.toml
	wantCometDefaults string
	//go:embed testdata/comet_overrides.toml
	wantCometOverrides string

	//go:embed testdata/app.toml
	wantApp string
	//go:embed testdata/app_defaults.toml
	wantAppDefaults string
	//go:embed testdata/app_overrides.toml
	wantAppOverrides string
)

func TestBuildConfigMaps(t *testing.T) {
	t.Parallel()

	const namespace = "default"

	t.Run("happy path", func(t *testing.T) {
		crd := defaultCRD()
		crd.Spec.Replicas = 3
		crd.Name = "agoric"
		crd.Namespace = "test"
		crd.Spec.PodTemplate.Image = "agoric:v6.0.0"
		crd.Spec.ChainSpec.Network = "testnet"

		cms, err := BuildConfigMaps(&crd, nil)
		require.NoError(t, err)
		require.Equal(t, 3, len(cms))

		require.Equal(t, int64(0), cms[0].Ordinal())
		require.NotEmpty(t, cms[0].Revision())

		cm := cms[0].Object()
		require.Equal(t, "agoric-0", cm.Name)
		require.Equal(t, "test", cm.Namespace)
		require.Nil(t, cm.Immutable)

		wantLabels := map[string]string{
			"app.kubernetes.io/created-by": "cosmos-operator",
			"app.kubernetes.io/component":  "CosmosFullNode",
			"app.kubernetes.io/name":       "agoric",
			"app.kubernetes.io/instance":   "agoric-0",
			"app.kubernetes.io/version":    "v6.0.0",
			"cosmos.strange.love/network":  "testnet",
			"cosmos.strange.love/type":     "FullNode",
		}
		require.Empty(t, cm.Annotations)

		require.Equal(t, wantLabels, cm.Labels)

		cm = cms[1].Object()
		require.Equal(t, "agoric-1", cm.Name)

		require.NotEmpty(t, cms[0].Object().Data)
		require.Equal(t, cms[0].Object().Data, cms[1].Object().Data)

		crd.Spec.Type = cosmosv1.FullNode
		cms2, err := BuildConfigMaps(&crd, nil)

		require.NoError(t, err)
		require.Equal(t, cms, cms2)
	})

	t.Run("long name", func(t *testing.T) {
		crd := defaultCRD()
		crd.Spec.Replicas = 3
		crd.Name = strings.Repeat("chain", 300)
		crd.Spec.ChainSpec.Network = strings.Repeat("network", 300)

		cms, err := BuildConfigMaps(&crd, nil)
		require.NoError(t, err)
		require.NotEmpty(t, cms)

		for _, cm := range cms {
			test.RequireValidMetadata(t, cm.Object())
		}
	})

	t.Run("config-overlay.toml", func(t *testing.T) {
		crd := defaultCRD()
		crd.Namespace = namespace
		crd.Name = "osmosis"
		crd.Spec.ChainSpec.Network = "mainnet"
		crd.Spec.Replicas = 1
		crd.Spec.ChainSpec.Comet = cosmosv1.CometConfig{
			PersistentPeers: "peer1@1.2.2.2:789,peer2@2.2.2.2:789,peer3@3.2.2.2:789",
			Seeds:           "seed1@1.1.1.1:456,seed2@1.1.1.1:456",
		}

		t.Run("happy path", func(t *testing.T) {
			custom := crd.DeepCopy()
			custom.Spec.Replicas = 1
			custom.Spec.ChainSpec.LogLevel = ptr("debug")
			custom.Spec.ChainSpec.LogFormat = ptr("json")
			custom.Spec.ChainSpec.Comet.CorsAllowedOrigins = []string{"*"}
			custom.Spec.ChainSpec.Comet.MaxInboundPeers = ptr(int32(5))
			custom.Spec.ChainSpec.Comet.MaxOutboundPeers = ptr(int32(15))

			peers := Peers{
				client.ObjectKey{Namespace: namespace, Name: "osmosis-0"}: {NodeID: "should not see me", PrivateAddress: "should not see me"},
			}
			cms, err := BuildConfigMaps(custom, peers)
			require.NoError(t, err)

			cm := cms[0].Object()

			require.NotEmpty(t, cm.Data)
			require.Empty(t, cm.BinaryData)

			var (
				got  map[string]any
				want map[string]any
			)
			_, err = toml.Decode(wantComet, &want)
			require.NoError(t, err)

			_, err = toml.Decode(cm.Data["config-overlay.toml"], &got)
			require.NoError(t, err)

			require.Equal(t, want, got)
		})

		t.Run("defaults", func(t *testing.T) {
			cms, err := BuildConfigMaps(&crd, nil)
			require.NoError(t, err)

			cm := cms[0].Object()

			var (
				got  map[string]any
				want map[string]any
			)
			_, err = toml.Decode(wantCometDefaults, &want)
			require.NoError(t, err)

			_, err = toml.Decode(cm.Data["config-overlay.toml"], &got)
			require.NoError(t, err)

			require.Equal(t, want, got)
		})

		t.Run("with peers", func(t *testing.T) {
			peerCRD := crd.DeepCopy()
			peerCRD.Spec.Replicas = 3
			peerCRD.Spec.ChainSpec.Comet.UnconditionalPeerIDs = "unconditional1,unconditional2"
			peerCRD.Spec.ChainSpec.Comet.PrivatePeerIDs = "private1,private2"
			peers := Peers{
				client.ObjectKey{Namespace: namespace, Name: "osmosis-0"}: {NodeID: "0", PrivateAddress: "0.local:26656"},
				client.ObjectKey{Namespace: namespace, Name: "osmosis-1"}: {NodeID: "1", PrivateAddress: "1.local:26656"},
				client.ObjectKey{Namespace: namespace, Name: "osmosis-2"}: {NodeID: "2", PrivateAddress: "2.local:26656"},
			}
			cms, err := BuildConfigMaps(peerCRD, peers)
			require.NoError(t, err)
			require.Len(t, cms, 3)

			for i, tt := range []struct {
				WantPersistent string
				WantIDs        string
			}{
				{"1@1.local:26656,2@2.local:26656", "1,2"},
				{"0@0.local:26656,2@2.local:26656", "0,2"},
				{"0@0.local:26656,1@1.local:26656", "0,1"},
			} {
				cm := cms[i].Object()
				var got map[string]any
				_, err = toml.Decode(cm.Data["config-overlay.toml"], &got)
				require.NoError(t, err, i)

				p2p := got["p2p"].(map[string]any)

				require.Equal(t, tt.WantPersistent+",peer1@1.2.2.2:789,peer2@2.2.2.2:789,peer3@3.2.2.2:789", p2p["persistent_peers"], i)
				require.Equal(t, tt.WantIDs+",private1,private2", p2p["private_peer_ids"], i)
				require.Equal(t, tt.WantIDs+",unconditional1,unconditional2", p2p["unconditional_peer_ids"], i)
			}
		})

		t.Run("validator sentry", func(t *testing.T) {
			sentry := crd.DeepCopy()
			sentry.Spec.Type = cosmosv1.Sentry
			cms, err := BuildConfigMaps(sentry, nil)
			require.NoError(t, err)

			cm := cms[0].Object()

			var got map[string]any
			_, err = toml.Decode(cm.Data["config-overlay.toml"], &got)
			require.NoError(t, err)
			require.NotEmpty(t, got)

			require.Equal(t, "tcp://0.0.0.0:1234", got["priv_validator_laddr"])
			require.Equal(t, "null", got["tx_index"].(map[string]any)["indexer"])
		})

		t.Run("overrides", func(t *testing.T) {
			overrides := crd.DeepCopy()
			overrides.Namespace = namespace
			overrides.Spec.ChainSpec.Comet.CorsAllowedOrigins = []string{"should not see me"}
			overrides.Spec.ChainSpec.Comet.TomlOverrides = ptr(`
	log_format = "json"
	new_base = "new base value"
	
	[p2p]
	external_address = "override.example.com"
	external-address = "override.example.com"
	seeds = "override@seed"
	new_field = "p2p"
	
	[rpc]
	cors_allowed_origins = ["override"]
	cors-allowed-origins = ["override"]
	
	[new_section]
	test = "value"
	
	[tx_index]
	indexer = "null"
	`)

			peers := Peers{
				client.ObjectKey{Name: "osmosis-0", Namespace: namespace}: {ExternalAddress: "should not see me"},
			}
			cms, err := BuildConfigMaps(overrides, peers)
			require.NoError(t, err)

			cm := cms[0].Object()

			var (
				got  map[string]any
				want map[string]any
			)
			_, err = toml.Decode(wantCometOverrides, &want)
			require.NoError(t, err)

			_, err = toml.Decode(cm.Data["config-overlay.toml"], &got)
			require.NoError(t, err)

			var gotBuffer bytes.Buffer
			var wantBuffer bytes.Buffer

			require.NoError(t, toml.NewEncoder(&gotBuffer).Encode(got))
			require.NoError(t, toml.NewEncoder(&wantBuffer).Encode(want))

			fmt.Printf("got:\n%s\n", gotBuffer.String())
			fmt.Printf("want:\n%s\n", wantBuffer.String())

			require.Equal(t, want, got)
		})

		t.Run("p2p external addresses", func(t *testing.T) {
			peers := Peers{
				client.ObjectKey{Name: "osmosis-0", Namespace: namespace}: {ExternalAddress: "1.1.1.1:26657"},
				client.ObjectKey{Name: "osmosis-1", Namespace: namespace}: {ExternalAddress: "2.2.2.2:26657"},
			}
			p2pCrd := crd.DeepCopy()
			p2pCrd.Namespace = namespace
			p2pCrd.Spec.Replicas = 3
			cms, err := BuildConfigMaps(p2pCrd, peers)
			require.NoError(t, err)

			require.Equal(t, 3, len(cms))

			var decoded decodedToml
			_, err = toml.Decode(cms[0].Object().Data["config-overlay.toml"], &decoded)
			require.NoError(t, err)
			require.Equal(t, "1.1.1.1:26657", decoded["p2p"].(decodedToml)["external_address"])

			_, err = toml.Decode(cms[1].Object().Data["config-overlay.toml"], &decoded)
			require.NoError(t, err)
			require.Equal(t, "2.2.2.2:26657", decoded["p2p"].(decodedToml)["external_address"])

			_, err = toml.Decode(cms[2].Object().Data["config-overlay.toml"], &decoded)
			require.NoError(t, err)
			require.Empty(t, decoded["p2p"].(decodedToml)["external_address"])
		})

		t.Run("invalid toml", func(t *testing.T) {
			malformed := crd.DeepCopy()
			malformed.Spec.ChainSpec.Comet.TomlOverrides = ptr(`invalid_toml = should be invalid`)
			_, err := BuildConfigMaps(malformed, nil)

			require.Error(t, err)
			require.Contains(t, err.Error(), "invalid toml in comet overrides")
		})
	})

	t.Run("app-overlay.toml", func(t *testing.T) {
		crd := defaultCRD()
		crd.Spec.Replicas = 3
		crd.Spec.ChainSpec.App = cosmosv1.SDKAppConfig{
			MinGasPrice: "0.123token",
		}

		t.Run("happy path", func(t *testing.T) {
			custom := crd.DeepCopy()
			custom.Spec.ChainSpec.App.APIEnableUnsafeCORS = true
			custom.Spec.ChainSpec.App.GRPCWebEnableUnsafeCORS = true
			custom.Spec.ChainSpec.App.HaltHeight = ptr(uint64(34567))
			custom.Spec.ChainSpec.App.Pruning = &cosmosv1.Pruning{
				Strategy:        "custom",
				Interval:        ptr(uint32(222)),
				KeepEvery:       ptr(uint32(333)),
				KeepRecent:      ptr(uint32(444)),
				MinRetainBlocks: ptr(uint32(271500)),
			}

			cms, err := BuildConfigMaps(custom, nil)
			require.NoError(t, err)

			cm := cms[0].Object()

			require.NotEmpty(t, cm.Data)
			require.Empty(t, cm.BinaryData)

			var (
				got  map[string]any
				want map[string]any
			)
			_, err = toml.Decode(wantApp, &want)
			require.NoError(t, err)

			_, err = toml.Decode(cm.Data["app-overlay.toml"], &got)
			require.NoError(t, err)

			require.Equal(t, want, got)
		})

		t.Run("defaults", func(t *testing.T) {
			cms, err := BuildConfigMaps(&crd, nil)
			require.NoError(t, err)

			cm := cms[0].Object()

			var (
				got  map[string]any
				want map[string]any
			)
			_, err = toml.Decode(wantAppDefaults, &want)
			require.NoError(t, err)

			_, err = toml.Decode(cm.Data["app-overlay.toml"], &got)
			require.NoError(t, err)

			require.Equal(t, want, got)
		})

		t.Run("overrides", func(t *testing.T) {
			overrides := crd.DeepCopy()
			overrides.Spec.ChainSpec.App.MinGasPrice = "should not see me"
			overrides.Spec.ChainSpec.App.TomlOverrides = ptr(`
	minimum-gas-prices = "0.1override"
	new-base = "new base value"
	
	[api]
	enable = false
	new-field = "test"
	`)
			cms, err := BuildConfigMaps(overrides, nil)
			require.NoError(t, err)

			cm := cms[0].Object()

			var (
				got  map[string]any
				want map[string]any
			)
			_, err = toml.Decode(wantAppOverrides, &want)
			require.NoError(t, err)

			_, err = toml.Decode(cm.Data["app-overlay.toml"], &got)
			require.NoError(t, err)

			require.Equal(t, want, got)
		})

		t.Run("external address overrides", func(t *testing.T) {
			overrides := crd.DeepCopy()

			overrides.Spec.InstanceOverrides = make(map[string]cosmosv1.InstanceOverridesSpec)
			overrideAddr0 := "override0.example.com:26656"
			overrideAddr1 := "override1.example.com:26656"
			overrides.Spec.InstanceOverrides["osmosis-0"] = cosmosv1.InstanceOverridesSpec{
				ExternalAddress: &overrideAddr0,
			}
			overrides.Spec.InstanceOverrides["osmosis-1"] = cosmosv1.InstanceOverridesSpec{
				ExternalAddress: &overrideAddr1,
			}
			cms, err := BuildConfigMaps(overrides, nil)
			require.NoError(t, err)

			var config map[string]any

			_, err = toml.Decode(cms[0].Object().Data["config-overlay.toml"], &config)
			require.NoError(t, err)
			require.Equal(t, overrideAddr0, config["p2p"].(map[string]any)["external_address"])
			require.Equal(t, overrideAddr0, config["p2p"].(map[string]any)["external-address"])

			_, err = toml.Decode(cms[1].Object().Data["config-overlay.toml"], &config)
			require.NoError(t, err)
			require.Equal(t, overrideAddr1, config["p2p"].(map[string]any)["external_address"])
			require.Equal(t, overrideAddr1, config["p2p"].(map[string]any)["external-address"])
		})

		t.Run("invalid toml", func(t *testing.T) {
			malformed := crd.DeepCopy()
			malformed.Spec.ChainSpec.App.TomlOverrides = ptr(`invalid_toml = should be invalid`)
			_, err := BuildConfigMaps(malformed, nil)

			require.Error(t, err)
			require.Contains(t, err.Error(), "invalid toml in app overrides")
		})
	})

	test.HasTypeLabel(t, func(crd cosmosv1.CosmosFullNode) []map[string]string {
		cms, _ := BuildConfigMaps(&crd, nil)
		labels := make([]map[string]string, 0)
		for _, cm := range cms {
			labels = append(labels, cm.Object().Labels)
		}
		return labels
	})
}

'''
'''--- internal/fullnode/configmap_control.go ---
package fullnode

import (
	"context"
	"fmt"

	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	"github.com/strangelove-ventures/cosmos-operator/internal/diff"
	"github.com/strangelove-ventures/cosmos-operator/internal/kube"
	corev1 "k8s.io/api/core/v1"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

// ConfigMapControl creates or updates configmaps.
type ConfigMapControl struct {
	build  func(*cosmosv1.CosmosFullNode, Peers) ([]diff.Resource[*corev1.ConfigMap], error)
	client Client
}

// NewConfigMapControl returns a valid ConfigMapControl.
func NewConfigMapControl(client Client) ConfigMapControl {
	return ConfigMapControl{
		build:  BuildConfigMaps,
		client: client,
	}
}

type ConfigChecksums map[client.ObjectKey]string

// Reconcile creates or updates configmaps containing items that are mounted into pods as files.
// The ConfigMap is never deleted unless the CRD itself is deleted.
func (cmc ConfigMapControl) Reconcile(ctx context.Context, log kube.Logger, crd *cosmosv1.CosmosFullNode, peers Peers) (ConfigChecksums, kube.ReconcileError) {
	var cms corev1.ConfigMapList
	if err := cmc.client.List(ctx, &cms,
		client.InNamespace(crd.Namespace),
		client.MatchingFields{kube.ControllerOwnerField: crd.Name},
	); err != nil {
		return nil, kube.TransientError(fmt.Errorf("list existing configmaps: %w", err))
	}

	current := ptrSlice(cms.Items)

	want, err := cmc.build(crd, peers)
	if err != nil {
		return nil, kube.UnrecoverableError(err)
	}

	diffed := diff.New(current, want)

	for _, cm := range diffed.Creates() {
		log.Info("Creating configmap", "configmapName", cm.Name)
		if err := ctrl.SetControllerReference(crd, cm, cmc.client.Scheme()); err != nil {
			return nil, kube.TransientError(fmt.Errorf("set controller reference on configmap %s: %w", cm.Name, err))
		}
		// CreateOrUpdate (vs. only create) fixes a bug with current deployments where updating would remove the owner reference.
		// This ensures we update the service with the owner reference.
		if err := kube.CreateOrUpdate(ctx, cmc.client, cm); err != nil {
			return nil, kube.TransientError(fmt.Errorf("create configmap configmap %s: %w", cm.Name, err))
		}
	}

	for _, cm := range diffed.Deletes() {
		log.Info("Deleting configmap", "configmapName", cm.Name)
		if err := cmc.client.Delete(ctx, cm); err != nil {
			return nil, kube.TransientError(fmt.Errorf("delete configmap %s: %w", cm.Name, err))
		}
	}

	for _, cm := range diffed.Updates() {
		log.Info("Updating configmap", "configmapName", cm.Name)
		if err := cmc.client.Update(ctx, cm); err != nil {
			return nil, kube.TransientError(fmt.Errorf("update configmap %s: %w", cm.Name, err))
		}
	}

	cksums := make(ConfigChecksums)
	for _, cm := range want {
		cksums[client.ObjectKeyFromObject(cm.Object())] = cm.Revision()
	}
	return cksums, nil
}

'''
'''--- internal/fullnode/configmap_control_test.go ---
package fullnode

import (
	"context"
	"errors"
	"testing"

	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	"github.com/strangelove-ventures/cosmos-operator/internal/diff"
	"github.com/stretchr/testify/require"
	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

func TestConfigMapControl_Reconcile(t *testing.T) {
	t.Parallel()

	type mockConfigClient = mockClient[*corev1.ConfigMap]
	ctx := context.Background()
	const namespace = "test"

	t.Run("create", func(t *testing.T) {
		var mClient mockConfigClient
		mClient.ObjectList = corev1.ConfigMapList{Items: []corev1.ConfigMap{
			{ObjectMeta: metav1.ObjectMeta{Name: "stargaze-0", Namespace: namespace}},  // update
			{ObjectMeta: metav1.ObjectMeta{Name: "stargaze-1", Namespace: namespace}},  // update
			{ObjectMeta: metav1.ObjectMeta{Name: "stargaze-99", Namespace: namespace}}, // delete
		}}

		control := NewConfigMapControl(&mClient)
		crd := defaultCRD()
		crd.Spec.Replicas = 3
		crd.Name = "stargaze"
		crd.Namespace = namespace
		crd.Spec.ChainSpec.Network = "testnet"

		cksums, err := control.Reconcile(ctx, nopReporter, &crd, nil)
		require.NoError(t, err)

		require.Len(t, mClient.GotListOpts, 2)
		var listOpt client.ListOptions
		for _, opt := range mClient.GotListOpts {
			opt.ApplyToList(&listOpt)
		}
		require.Equal(t, "test", listOpt.Namespace)
		require.Zero(t, listOpt.Limit)
		require.Equal(t, ".metadata.controller=stargaze", listOpt.FieldSelector.String())

		require.Equal(t, 1, mClient.CreateCount)

		require.NotEmpty(t, mClient.LastCreateObject.OwnerReferences)
		require.Equal(t, crd.Name, mClient.LastCreateObject.OwnerReferences[0].Name)
		require.Equal(t, "CosmosFullNode", mClient.LastCreateObject.OwnerReferences[0].Kind)
		require.True(t, *mClient.LastCreateObject.OwnerReferences[0].Controller)

		require.Equal(t, 2, mClient.UpdateCount)
		require.Equal(t, 1, mClient.DeleteCount)

		require.Len(t, cksums, 3)
		require.NotEmpty(t, cksums[client.ObjectKey{Name: "stargaze-0", Namespace: namespace}])
		require.NotEmpty(t, cksums[client.ObjectKey{Name: "stargaze-1", Namespace: namespace}])
		require.NotEmpty(t, cksums[client.ObjectKey{Name: "stargaze-2", Namespace: namespace}])
	})

	t.Run("build error", func(t *testing.T) {
		var mClient mockConfigClient
		control := NewConfigMapControl(&mClient)
		control.build = func(crd *cosmosv1.CosmosFullNode, _ Peers) ([]diff.Resource[*corev1.ConfigMap], error) {
			return nil, errors.New("boom")
		}

		crd := defaultCRD()
		_, err := control.Reconcile(ctx, nopReporter, &crd, nil)

		require.Error(t, err)
		require.EqualError(t, err, "boom")
		require.False(t, err.IsTransient())
	})
}

'''
'''--- internal/fullnode/drift_detection.go ---
package fullnode

import (
	"context"
	"time"

	"github.com/samber/lo"
	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	"github.com/strangelove-ventures/cosmos-operator/internal/cosmos"
	"github.com/strangelove-ventures/cosmos-operator/internal/kube"
	corev1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/util/intstr"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

// DriftDetection detects pods that are lagging behind the latest block height.
type DriftDetection struct {
	available      func(pods []*corev1.Pod, minReady time.Duration, now time.Time) []*corev1.Pod
	collector      StatusCollector
	computeRollout func(maxUnavail *intstr.IntOrString, desired, ready int) int
}

func NewDriftDetection(collector StatusCollector) DriftDetection {
	return DriftDetection{
		available:      kube.AvailablePods,
		collector:      collector,
		computeRollout: kube.ComputeRollout,
	}
}

// LaggingPods returns pods that are lagging behind the latest block height.
func (d DriftDetection) LaggingPods(ctx context.Context, crd *cosmosv1.CosmosFullNode) []*corev1.Pod {
	synced := d.collector.Collect(ctx, client.ObjectKeyFromObject(crd)).Synced()

	maxHeight := lo.MaxBy(synced, func(a cosmos.StatusItem, b cosmos.StatusItem) bool {
		return a.Status.LatestBlockHeight() > b.Status.LatestBlockHeight()
	}).Status.LatestBlockHeight()

	thresh := uint64(crd.Spec.SelfHeal.HeightDriftMitigation.Threshold)
	lagging := lo.FilterMap(synced, func(item cosmos.StatusItem, _ int) (*corev1.Pod, bool) {
		isLagging := maxHeight-item.Status.LatestBlockHeight() >= thresh
		return item.GetPod(), isLagging
	})

	avail := d.available(synced.Pods(), 5*time.Second, time.Now())
	rollout := d.computeRollout(crd.Spec.RolloutStrategy.MaxUnavailable, int(crd.Spec.Replicas), len(avail))
	return lo.Slice(lagging, 0, rollout)
}

'''
'''--- internal/fullnode/drift_detection_test.go ---
package fullnode

import (
	"context"
	"fmt"
	"testing"
	"time"

	"github.com/samber/lo"
	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	"github.com/strangelove-ventures/cosmos-operator/internal/cosmos"
	"github.com/stretchr/testify/require"
	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/util/intstr"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

func TestDriftDetection_LaggingPods(t *testing.T) {
	t.Run("happy path", func(t *testing.T) {
		var crd cosmosv1.CosmosFullNode
		crd.Name = "noble"
		crd.Namespace = "default"
		maxUnavail := &intstr.IntOrString{Type: intstr.Int, IntVal: 1}
		crd.Spec.RolloutStrategy.MaxUnavailable = maxUnavail
		crd.Spec.Replicas = 3

		var coll cosmos.StatusCollection = lo.Map(lo.Range(5), func(_, i int) cosmos.StatusItem {
			return cosmos.StatusItem{Pod: &corev1.Pod{ObjectMeta: metav1.ObjectMeta{Name: fmt.Sprintf("pod-%d", i)}}}
		})

		coll[0].Status.Result.SyncInfo.LatestBlockHeight = "100"
		coll[1].Status.Result.SyncInfo.LatestBlockHeight = "100"
		coll[2].Status.Result.SyncInfo.LatestBlockHeight = "95"
		coll[3].Status.Result.SyncInfo.LatestBlockHeight = "90"
		coll[4].Status.Result.SyncInfo.CatchingUp = true

		collector := mockStatusCollector{CollectFn: func(ctx context.Context, controller client.ObjectKey) cosmos.StatusCollection {
			require.NotNil(t, ctx)
			require.Equal(t, client.ObjectKey{Namespace: "default", Name: "noble"}, controller)
			return coll
		}}

		detector := NewDriftDetection(collector)

		for _, tt := range []struct {
			Threshold uint32
			Available int
			WantPods  []string
		}{
			{10, 1, []string{"pod-3"}},
			{5, 10, []string{"pod-2", "pod-3"}},
			{3, 1, []string{"pod-2"}},
			{1, 0, []string{}},
		} {
			crd.Spec.SelfHeal = &cosmosv1.SelfHealSpec{}
			crd.Spec.SelfHeal.HeightDriftMitigation = &cosmosv1.HeightDriftMitigationSpec{
				Threshold: tt.Threshold,
			}

			detector.available = func(pods []*corev1.Pod, minReady time.Duration, now time.Time) []*corev1.Pod {
				require.GreaterOrEqual(t, len(pods), len(tt.WantPods))
				require.WithinDuration(t, time.Now(), now, 5*time.Second)
				require.Equal(t, 5*time.Second, minReady)
				return coll.SyncedPods()
			}

			detector.computeRollout = func(unavail *intstr.IntOrString, desired, ready int) int {
				require.Equal(t, maxUnavail, unavail, tt)
				require.EqualValues(t, crd.Spec.Replicas, desired, tt)
				require.NotZero(t, ready, tt)
				require.Equal(t, len(coll.SyncedPods()), ready, tt)
				return tt.Available
			}

			got := detector.LaggingPods(context.Background(), &crd)
			gotPods := lo.Map(got, func(pod *corev1.Pod, _ int) string { return pod.Name })

			require.Equal(t, tt.WantPods, gotPods, tt)
		}
	})

	t.Run("no pods or replicas", func(t *testing.T) {
		collector := mockStatusCollector{CollectFn: func(ctx context.Context, controller client.ObjectKey) cosmos.StatusCollection {
			return nil
		}}
		detector := NewDriftDetection(collector)

		var crd cosmosv1.CosmosFullNode
		crd.Spec.SelfHeal = &cosmosv1.SelfHealSpec{}
		crd.Spec.SelfHeal.HeightDriftMitigation = &cosmosv1.HeightDriftMitigationSpec{
			Threshold: 25,
		}

		got := detector.LaggingPods(context.Background(), &crd)
		require.Empty(t, got)

		crd.Spec.Replicas = 3
		got = detector.LaggingPods(context.Background(), &crd)
		require.Empty(t, got)
	})
}

'''
'''--- internal/fullnode/genesis.go ---
package fullnode

import (
	_ "embed"
	"fmt"

	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
)

var (
	//go:embed script/download-genesis.sh
	scriptDownloadGenesis string
	//go:embed script/use-init-genesis.sh
	scriptUseInitGenesis string
)

// If $DATA_DIR is populated, then we assume we have the genesis file.
const genesisScriptWrapper = `ls $DATA_DIR/*.db 1> /dev/null 2>&1
DB_INIT=$?
if [ $DB_INIT -eq 0 ]; then
	echo "Database already initialized, skipping genesis initialization"
	exit 0
fi

%s

echo "Genesis $GENESIS_FILE initialized."
`

// DownloadGenesisCommand returns a proper genesis command for use in an init container.
//
// The general strategy is if the user does not configure an external genesis file, use the genesis from the <chain-binary> init command.
// If the user supplies a custom script, we use that. Otherwise, we use attempt to download and extract the file.
func DownloadGenesisCommand(cfg cosmosv1.ChainSpec) (string, []string) {
	args := []string{"-c"}
	switch {
	case cfg.GenesisScript != nil:
		args = append(args, fmt.Sprintf(genesisScriptWrapper, *cfg.GenesisScript))
	case cfg.GenesisURL != nil:
		args = append(args, fmt.Sprintf(genesisScriptWrapper, scriptDownloadGenesis), "-s", *cfg.GenesisURL)
	default:
		args = append(args, fmt.Sprintf(genesisScriptWrapper, scriptUseInitGenesis))
	}
	return "sh", args
}

'''
'''--- internal/fullnode/genesis_test.go ---
package fullnode

import (
	"testing"

	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	"github.com/stretchr/testify/require"
)

func TestDownloadGenesisCommand(t *testing.T) {
	t.Parallel()

	requireValidScript := func(t *testing.T, script string) {
		t.Helper()
		require.NotEmpty(t, script)
		require.Contains(t, script, `if [ $DB_INIT -eq 0 ]`)
	}

	t.Run("default", func(t *testing.T) {
		var cfg cosmosv1.ChainSpec

		cmd, args := DownloadGenesisCommand(cfg)
		require.Equal(t, "sh", cmd)

		require.Len(t, args, 2)

		require.Equal(t, "-c", args[0])

		got := args[1]
		requireValidScript(t, got)
		require.NotContains(t, got, "GENESIS_URL")
		require.Contains(t, got, `mv "$INIT_GENESIS_FILE" "$GENESIS_FILE"`)
	})

	t.Run("download", func(t *testing.T) {
		cfg := cosmosv1.ChainSpec{
			GenesisURL: ptr("https://example.com/genesis.json"),
		}
		cmd, args := DownloadGenesisCommand(cfg)
		require.Equal(t, "sh", cmd)

		require.Len(t, args, 4)

		require.Equal(t, "-c", args[0])
		got := args[1]
		requireValidScript(t, got)
		require.Contains(t, got, `GENESIS_URL`)
		require.Contains(t, got, "download_json")

		require.Equal(t, "-s", args[2])
		require.Equal(t, "https://example.com/genesis.json", args[3])
	})

	t.Run("custom", func(t *testing.T) {
		cfg := cosmosv1.ChainSpec{
			// Keeping this to assert that custom script takes precedence.
			GenesisURL:    ptr("https://example.com/genesis.json"),
			GenesisScript: ptr("echo hi"),
		}
		cmd, args := DownloadGenesisCommand(cfg)
		require.Equal(t, "sh", cmd)

		require.Len(t, args, 2)

		require.Equal(t, "-c", args[0])

		got := args[1]
		requireValidScript(t, got)

		require.NotContains(t, got, "GENESIS_URL")
		require.Contains(t, got, "echo hi")
	})
}

'''
'''--- internal/fullnode/labels.go ---
package fullnode

import (
	"errors"
	"fmt"

	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	"github.com/strangelove-ventures/cosmos-operator/internal/kube"
)

const (
	networkLabel = "cosmos.strange.love/network"
	typeLabel    = "cosmos.strange.love/type"
)

// kv is a list of extra kv pairs to add to the labels. Must be even.
func defaultLabels(crd *cosmosv1.CosmosFullNode, kvPairs ...string) map[string]string {
	if len(kvPairs)%2 != 0 {
		panic(errors.New("key/value pairs must be even"))
	}
	nodeType := cosmosv1.FullNode
	if crd.Spec.Type != "" {
		nodeType = crd.Spec.Type
	}
	labels := map[string]string{
		kube.ControllerLabel: "cosmos-operator",
		kube.ComponentLabel:  cosmosv1.CosmosFullNodeController,
		kube.NameLabel:       appName(crd),
		kube.VersionLabel:    kube.ParseImageVersion(crd.Spec.PodTemplate.Image),
		networkLabel:         crd.Spec.ChainSpec.Network,
		typeLabel:            string(nodeType),
	}
	for i := 0; i < len(kvPairs); i += 2 {
		labels[kvPairs[i]] = kvPairs[i+1]
	}
	return labels
}

func appName(crd *cosmosv1.CosmosFullNode) string {
	return kube.ToName(crd.Name)
}

func instanceName(crd *cosmosv1.CosmosFullNode, ordinal int32) string {
	return kube.ToName(fmt.Sprintf("%s-%d", appName(crd), ordinal))
}

// Conditionally add custom labels or annotations, preserving key/values already set on 'into'.
// 'into' must not be nil.
func preserveMergeInto(into map[string]string, other map[string]string) {
	for k, v := range other {
		_, ok := into[k]
		if !ok {
			into[k] = v
		}
	}
}

'''
'''--- internal/fullnode/mock_test.go ---
package fullnode

import (
	"context"
	"fmt"
	"sync"

	snapshotv1 "github.com/kubernetes-csi/external-snapshotter/client/v6/apis/volumesnapshot/v1"
	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	corev1 "k8s.io/api/core/v1"
	rbacv1 "k8s.io/api/rbac/v1"
	"k8s.io/apimachinery/pkg/api/meta"
	"k8s.io/apimachinery/pkg/runtime"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

type mockClient[T client.Object] struct {
	mu sync.Mutex

	Object       any
	GetObjectKey client.ObjectKey
	GetObjectErr error

	ObjectList  any
	GotListOpts []client.ListOption
	ListErr     error

	CreateCount      int
	LastCreateObject T
	CreatedObjects   []T

	DeleteCount int

	PatchCount      int
	LastPatchObject client.Object
	LastPatch       client.Patch

	LastUpdateObject T
	UpdateCount      int
	UpdateErr        error
}

func (m *mockClient[T]) Get(ctx context.Context, key client.ObjectKey, obj client.Object, _ ...client.GetOption) error {
	m.mu.Lock()
	defer m.mu.Unlock()

	if ctx == nil {
		panic("nil context")
	}
	m.GetObjectKey = key
	if m.Object == nil {
		return m.GetObjectErr
	}

	switch ref := obj.(type) {
	case *corev1.ConfigMap:
		*ref = m.Object.(corev1.ConfigMap)
	case *corev1.PersistentVolumeClaim:
		*ref = m.Object.(corev1.PersistentVolumeClaim)
	case *cosmosv1.CosmosFullNode:
		*ref = m.Object.(cosmosv1.CosmosFullNode)
	case *snapshotv1.VolumeSnapshot:
		*ref = m.Object.(snapshotv1.VolumeSnapshot)
	default:
		panic(fmt.Errorf("unknown Object type: %T", m.ObjectList))
	}
	return m.GetObjectErr
}

func (m *mockClient[T]) List(ctx context.Context, list client.ObjectList, opts ...client.ListOption) error {
	m.mu.Lock()
	defer m.mu.Unlock()

	if ctx == nil {
		panic("nil context")
	}
	m.GotListOpts = opts

	if m.ObjectList == nil {
		return nil
	}

	switch ref := list.(type) {
	case *corev1.PodList:
		*ref = m.ObjectList.(corev1.PodList)
	case *corev1.PersistentVolumeClaimList:
		*ref = m.ObjectList.(corev1.PersistentVolumeClaimList)
	case *corev1.ServiceList:
		*ref = m.ObjectList.(corev1.ServiceList)
	case *corev1.ConfigMapList:
		*ref = m.ObjectList.(corev1.ConfigMapList)
	case *corev1.SecretList:
		*ref = m.ObjectList.(corev1.SecretList)
	case *corev1.ServiceAccountList:
		*ref = m.ObjectList.(corev1.ServiceAccountList)
	case *rbacv1.RoleList:
		*ref = m.ObjectList.(rbacv1.RoleList)
	case *rbacv1.RoleBindingList:
		*ref = m.ObjectList.(rbacv1.RoleBindingList)
	default:
		panic(fmt.Errorf("unknown ObjectList type: %T", m.ObjectList))
	}

	return m.ListErr
}

func (m *mockClient[T]) Create(ctx context.Context, obj client.Object, opts ...client.CreateOption) error {
	m.mu.Lock()
	defer m.mu.Unlock()

	if ctx == nil {
		panic("nil context")
	}
	m.LastCreateObject = obj.(T)
	m.CreatedObjects = append(m.CreatedObjects, obj.(T))
	m.CreateCount++
	return nil
}

func (m *mockClient[T]) Delete(ctx context.Context, obj client.Object, opts ...client.DeleteOption) error {
	m.mu.Lock()
	defer m.mu.Unlock()

	if ctx == nil {
		panic("nil context")
	}
	m.DeleteCount++
	return nil
}

func (m *mockClient[T]) Update(ctx context.Context, obj client.Object, opts ...client.UpdateOption) error {
	m.mu.Lock()
	defer m.mu.Unlock()

	if ctx == nil {
		panic("nil context")
	}
	m.UpdateCount++
	m.LastUpdateObject = obj.(T)
	return m.UpdateErr
}

func (m *mockClient[T]) Patch(ctx context.Context, obj client.Object, patch client.Patch, opts ...client.PatchOption) error {
	m.mu.Lock()
	defer m.mu.Unlock()

	if ctx == nil {
		panic("nil context")
	}
	m.PatchCount++
	m.LastPatchObject = obj
	m.LastPatch = patch
	return nil
}

func (m *mockClient[T]) DeleteAllOf(ctx context.Context, obj client.Object, opts ...client.DeleteAllOfOption) error {
	panic("implement me")
}

func (m *mockClient[T]) Scheme() *runtime.Scheme {
	m.mu.Lock()
	defer m.mu.Unlock()

	scheme := runtime.NewScheme()
	if err := cosmosv1.AddToScheme(scheme); err != nil {
		panic(err)
	}
	return scheme
}

func (m *mockClient[T]) Status() client.StatusWriter {
	return m
}

func (m *mockClient[T]) RESTMapper() meta.RESTMapper {
	panic("implement me")
}

'''
'''--- internal/fullnode/node_key_builder.go ---
package fullnode

import (
	"crypto/ed25519"
	"crypto/rand"
	"crypto/sha256"
	"encoding/hex"
	"encoding/json"
	"fmt"

	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	"github.com/strangelove-ventures/cosmos-operator/internal/diff"
	"github.com/strangelove-ventures/cosmos-operator/internal/kube"
	corev1 "k8s.io/api/core/v1"
)

const nodeKeyFile = "node_key.json"

// BuildNodeKeySecrets builds the node key secrets for the given CRD.
// If the secret already has a node key, it is reused.
// Returns an error if a new node key cannot be serialized. (Should never happen.)
func BuildNodeKeySecrets(existing []*corev1.Secret, crd *cosmosv1.CosmosFullNode) ([]diff.Resource[*corev1.Secret], error) {
	secrets := make([]diff.Resource[*corev1.Secret], crd.Spec.Replicas)
	for i := int32(0); i < crd.Spec.Replicas; i++ {
		var s corev1.Secret
		s.Name = nodeKeySecretName(crd, i)
		s.Namespace = crd.Namespace
		s = *kube.FindOrDefaultCopy(existing, &s)

		var secret corev1.Secret
		secret.Name = s.Name
		secret.Namespace = s.Namespace
		secret.Kind = "Secret"
		secret.APIVersion = "v1"
		secret.Data = s.Data

		secret.Labels = defaultLabels(crd,
			kube.InstanceLabel, instanceName(crd, i),
		)

		secret.Immutable = ptr(true)
		secret.Type = corev1.SecretTypeOpaque

		// Create node key if it doesn't exist
		if secret.Data[nodeKeyFile] == nil {
			nk, err := randNodeKey()
			if err != nil {
				return nil, err
			}
			secret.Data = map[string][]byte{
				nodeKeyFile: nk,
			}
		}

		secrets[i] = diff.Adapt(&secret, i)
	}
	return secrets, nil
}

type NodeKey struct {
	PrivKey NodeKeyPrivKey `json:"priv_key"`
}

type NodeKeyPrivKey struct {
	Type  string             `json:"type"`
	Value ed25519.PrivateKey `json:"value"`
}

func (nk NodeKey) ID() string {
	pub := nk.PrivKey.Value.Public()
	hash := sha256.Sum256(pub.(ed25519.PublicKey))
	return hex.EncodeToString(hash[:20])
}

func randNodeKey() ([]byte, error) {
	_, pk, err := ed25519.GenerateKey(rand.Reader)
	if err != nil {
		return nil, fmt.Errorf("failed to generate ed25519 node key: %w", err)
	}
	return json.Marshal(NodeKey{
		PrivKey: NodeKeyPrivKey{
			Type:  "tendermint/PrivKeyEd25519",
			Value: pk,
		},
	})
}

func nodeKeySecretName(crd *cosmosv1.CosmosFullNode, ordinal int32) string {
	return kube.ToName(fmt.Sprintf("%s-node-key-%d", appName(crd), ordinal))
}

'''
'''--- internal/fullnode/node_key_builder_test.go ---
package fullnode

import (
	"encoding/json"
	"fmt"
	"testing"

	"github.com/strangelove-ventures/cosmos-operator/internal/test"

	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	"github.com/stretchr/testify/require"
	corev1 "k8s.io/api/core/v1"
)

func TestBuildNodeKeySecrets(t *testing.T) {
	t.Parallel()

	t.Run("happy path", func(t *testing.T) {
		var crd cosmosv1.CosmosFullNode
		crd.Namespace = "test-namespace"
		crd.Name = "juno"
		crd.Spec.Replicas = 3
		crd.Spec.ChainSpec.Network = "mainnet"
		crd.Spec.PodTemplate.Image = "ghcr.io/juno:v1.2.3"

		secrets, err := BuildNodeKeySecrets(nil, &crd)
		require.NoError(t, err)
		require.Len(t, secrets, 3)

		for i, s := range secrets {
			require.Equal(t, int64(i), s.Ordinal())
			require.NotEmpty(t, s.Revision())
			got := s.Object()
			require.Equal(t, crd.Namespace, got.Namespace)
			require.Equal(t, fmt.Sprintf("juno-node-key-%d", i), got.Name)
			require.Equal(t, "Secret", got.Kind)
			require.Equal(t, "v1", got.APIVersion)

			wantLabels := map[string]string{
				"app.kubernetes.io/created-by": "cosmos-operator",
				"app.kubernetes.io/component":  "CosmosFullNode",
				"app.kubernetes.io/name":       "juno",
				"app.kubernetes.io/instance":   fmt.Sprintf("juno-%d", i),
				"app.kubernetes.io/version":    "v1.2.3",
				"cosmos.strange.love/network":  "mainnet",
				"cosmos.strange.love/type":     "FullNode",
			}
			require.Equal(t, wantLabels, got.Labels)

			require.Empty(t, got.Annotations)

			require.True(t, *got.Immutable)
			require.Equal(t, corev1.SecretTypeOpaque, got.Type)

			nodeKey := got.Data["node_key.json"]
			require.NotEmpty(t, nodeKey)

			var gotJSON map[string]map[string]string
			err = json.Unmarshal(nodeKey, &gotJSON)
			require.NoError(t, err)
			require.Equal(t, gotJSON["priv_key"]["type"], "tendermint/PrivKeyEd25519")
			require.NotEmpty(t, gotJSON["priv_key"]["value"])
		}
	})

	t.Run("with existing", func(t *testing.T) {
		const namespace = "test-namespace"
		var crd cosmosv1.CosmosFullNode
		crd.Namespace = namespace
		crd.Name = "juno"
		crd.Spec.Replicas = 3

		var existing corev1.Secret
		existing.Name = "juno-node-key-0"
		existing.Namespace = namespace
		existing.Annotations = map[string]string{"foo": "bar"}
		existing.Data = map[string][]byte{"node_key.json": []byte("existing")}

		got, err := BuildNodeKeySecrets([]*corev1.Secret{&existing}, &crd)
		require.NoError(t, err)
		require.Equal(t, 3, len(got))

		nodeKey := got[0].Object().Data["node_key.json"]
		require.Equal(t, "existing", string(nodeKey))

		require.Empty(t, got[0].Object().Annotations)
	})

	t.Run("zero replicas", func(t *testing.T) {
		var crd cosmosv1.CosmosFullNode
		secrets, err := BuildNodeKeySecrets(nil, &crd)
		require.NoError(t, err)
		require.Empty(t, secrets)
	})

	test.HasTypeLabel(t, func(crd cosmosv1.CosmosFullNode) []map[string]string {
		secrets, _ := BuildNodeKeySecrets(nil, &crd)
		labels := make([]map[string]string, 0)
		for _, secret := range secrets {
			labels = append(labels, secret.Object().Labels)
		}
		return labels
	})
}

'''
'''--- internal/fullnode/node_key_control.go ---
package fullnode

import (
	"context"
	"fmt"

	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	"github.com/strangelove-ventures/cosmos-operator/internal/diff"
	"github.com/strangelove-ventures/cosmos-operator/internal/kube"
	corev1 "k8s.io/api/core/v1"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

// NodeKeyControl reconciles node keys for a CosmosFullNode. Node keys are saved as secrets and later mounted
// into pods.
type NodeKeyControl struct {
	client Client
}

func NewNodeKeyControl(client Client) NodeKeyControl {
	return NodeKeyControl{
		client: client,
	}
}

// Reconcile is the control loop for node keys. The secrets are never deleted.
func (control NodeKeyControl) Reconcile(ctx context.Context, reporter kube.Reporter, crd *cosmosv1.CosmosFullNode) kube.ReconcileError {
	var secrets corev1.SecretList
	if err := control.client.List(ctx, &secrets,
		client.InNamespace(crd.Namespace),
		client.MatchingFields{kube.ControllerOwnerField: crd.Name},
	); err != nil {
		return kube.TransientError(fmt.Errorf("list existing node key secrets: %w", err))
	}

	existing := ptrSlice(secrets.Items)
	want, serr := BuildNodeKeySecrets(existing, crd)
	if serr != nil {
		return kube.UnrecoverableError(fmt.Errorf("build node key secrets: %w", serr))
	}

	diffed := diff.New(existing, want)

	for _, secret := range diffed.Creates() {
		reporter.Info("Creating node key secret", "secret", secret.Name)
		if err := ctrl.SetControllerReference(crd, secret, control.client.Scheme()); err != nil {
			return kube.TransientError(fmt.Errorf("set controller reference on node key secret %q: %w", secret.Name, err))
		}
		if err := control.client.Create(ctx, secret); kube.IgnoreAlreadyExists(err) != nil {
			return kube.TransientError(fmt.Errorf("create node key secret %q: %w", secret.Name, err))
		}
	}

	for _, secret := range diffed.Updates() {
		reporter.Info("Updating node key secret", "secret", secret.Name)
		if err := control.client.Update(ctx, secret); err != nil {
			return kube.TransientError(fmt.Errorf("update node key secret %q: %w", secret.Name, err))
		}
	}

	return nil
}

'''
'''--- internal/fullnode/node_key_control_test.go ---
package fullnode

import (
	"context"
	"testing"

	"github.com/stretchr/testify/require"
	corev1 "k8s.io/api/core/v1"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

func TestNodeKeyControl_Reconcile(t *testing.T) {
	t.Parallel()

	type mockNodeKeyClient = mockClient[*corev1.Secret]
	const namespace = "default"
	ctx := context.Background()

	var mClient mockNodeKeyClient
	var existing corev1.Secret
	existing.Name = "juno-node-key-0"
	existing.Namespace = namespace
	mClient.ObjectList = corev1.SecretList{Items: []corev1.Secret{existing}}

	crd := defaultCRD()
	crd.Namespace = namespace
	crd.Spec.Replicas = 3
	crd.Name = "juno"
	crd.Spec.ChainSpec.Network = "testnet"

	control := NewNodeKeyControl(&mClient)
	err := control.Reconcile(ctx, nopReporter, &crd)
	require.NoError(t, err)

	require.Len(t, mClient.GotListOpts, 2)
	var listOpt client.ListOptions
	for _, opt := range mClient.GotListOpts {
		opt.ApplyToList(&listOpt)
	}
	require.Equal(t, namespace, listOpt.Namespace)
	require.Zero(t, listOpt.Limit)
	require.Equal(t, ".metadata.controller=juno", listOpt.FieldSelector.String())

	require.Equal(t, 1, mClient.UpdateCount)
	require.Equal(t, 2, mClient.CreateCount)

	require.NotEmpty(t, mClient.LastCreateObject.OwnerReferences)
	require.Equal(t, crd.Name, mClient.LastCreateObject.OwnerReferences[0].Name)
	require.Equal(t, "CosmosFullNode", mClient.LastCreateObject.OwnerReferences[0].Kind)
	require.True(t, *mClient.LastCreateObject.OwnerReferences[0].Controller)
}

'''
'''--- internal/fullnode/peer_collector.go ---
package fullnode

import (
	"context"
	"encoding/json"
	"fmt"
	"net"
	"sort"
	"strconv"

	"github.com/samber/lo"
	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	"github.com/strangelove-ventures/cosmos-operator/internal/kube"
	corev1 "k8s.io/api/core/v1"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

// Peer contains information about a peer.
type Peer struct {
	NodeID          string
	PrivateAddress  string // Only the private address my-service.namespace.svc.cluster.local:<port>
	ExternalAddress string // Only the address <external-ip-or-hostname>:<port>. Not all peers will be external.

	hasExternalAddress bool
}

// PrivatePeer returns the full private identifier of the peer in the format <node_id>@<private_address>:<port>.
func (peer Peer) PrivatePeer() string {
	return peer.NodeID + "@" + peer.PrivateAddress
}

// ExternalPeer returns the full external address of the peer in the format <node_id>@<external_address>:<port>.
func (peer Peer) ExternalPeer() string {
	if peer.ExternalAddress == "" {
		return peer.NodeID + "@" + net.JoinHostPort("0.0.0.0", strconv.Itoa(p2pPort))
	}
	return peer.NodeID + "@" + peer.ExternalAddress
}

// Peers maps an ObjectKey using the instance name to Peer.
type Peers map[client.ObjectKey]Peer

func (peers Peers) Default() Peers { return make(Peers) }

// Get is a convenience getter.
func (peers Peers) Get(name, namespace string) Peer {
	if peers == nil {
		return Peer{}
	}
	return peers[client.ObjectKey{Name: name, Namespace: namespace}]
}

// Except returns a copy of the peers without the Peer for the given name and namespace.
func (peers Peers) Except(name, namespace string) Peers {
	peerCopy := make(Peers)
	objKey := client.ObjectKey{Name: name, Namespace: namespace}
	for key, peer := range peers {
		if key != objKey {
			peerCopy[key] = peer
		}
	}
	return peerCopy
}

// HasIncompleteExternalAddress returns true if any peer has an external address but it is not assigned yet.
func (peers Peers) HasIncompleteExternalAddress() bool {
	for _, peer := range peers {
		if peer.hasExternalAddress && peer.ExternalAddress == "" {
			return true
		}
	}
	return false
}

// NodeIDs returns a sorted list of all node IDs.
func (peers Peers) NodeIDs() []string {
	ids := lo.Map(lo.Values(peers), func(p Peer, _ int) string { return p.NodeID })
	sort.Strings(ids)
	return ids
}

// AllExternal returns a sorted list of all external peers in the format <node_id>@<external_address>:<port>.
func (peers Peers) AllExternal() []string {
	addrs := lo.Map(lo.Values(peers), func(info Peer, _ int) string { return info.ExternalPeer() })
	sort.Strings(addrs)
	return addrs
}

// AllPrivate returns a sorted list of all private peers in the format <node_id>@<private_address>:<port>.
func (peers Peers) AllPrivate() []string {
	addrs := lo.Map(lo.Values(peers), func(info Peer, _ int) string { return info.PrivatePeer() })
	sort.Strings(addrs)
	return addrs
}

// PeerCollector finds and collects peer information.
type PeerCollector struct {
	client Getter
}

func NewPeerCollector(client Getter) *PeerCollector {
	return &PeerCollector{client: client}
}

// Collect peer information given the crd.
func (c PeerCollector) Collect(ctx context.Context, crd *cosmosv1.CosmosFullNode) (Peers, kube.ReconcileError) {
	peers := make(Peers)
	for i := int32(0); i < crd.Spec.Replicas; i++ {
		secretName := nodeKeySecretName(crd, i)
		var secret corev1.Secret
		// Hoping the caching layer kubebuilder prevents API errors or rate limits. Simplifies logic to use a Get here
		// vs. manually filtering through a List.
		if err := c.client.Get(ctx, client.ObjectKey{Name: secretName, Namespace: crd.Namespace}, &secret); err != nil {
			return nil, kube.TransientError(fmt.Errorf("get secret %s: %w", secretName, err))
		}

		var nodeKey NodeKey
		if err := json.Unmarshal(secret.Data[nodeKeyFile], &nodeKey); err != nil {
			return nil, kube.UnrecoverableError(err)
		}
		svcName := p2pServiceName(crd, i)
		peers[c.objectKey(crd, i)] = Peer{
			NodeID:         nodeKey.ID(),
			PrivateAddress: fmt.Sprintf("%s.%s.svc.cluster.local:%d", svcName, secret.Namespace, p2pPort),
		}
		if err := c.addExternalAddress(ctx, peers, crd, i); err != nil {
			return nil, kube.TransientError(err)
		}
	}

	return peers, nil
}

func (c PeerCollector) objectKey(crd *cosmosv1.CosmosFullNode, ordinal int32) client.ObjectKey {
	return client.ObjectKey{Name: instanceName(crd, ordinal), Namespace: crd.Namespace}
}

func (c PeerCollector) addExternalAddress(ctx context.Context, peers Peers, crd *cosmosv1.CosmosFullNode, ordinal int32) error {
	svcName := p2pServiceName(crd, ordinal)
	var svc corev1.Service
	// Hoping the caching layer kubebuilder prevents API errors or rate limits. Simplifies logic to use a Get here
	// vs. manually filtering through a List.
	if err := c.client.Get(ctx, client.ObjectKey{Name: svcName, Namespace: crd.Namespace}, &svc); err != nil {
		return kube.TransientError(fmt.Errorf("get server %s: %w", svcName, err))
	}
	if svc.Spec.Type != corev1.ServiceTypeLoadBalancer {
		return nil
	}
	objKey := c.objectKey(crd, ordinal)
	info := peers[objKey]
	info.hasExternalAddress = true
	defer func() { peers[objKey] = info }()

	ingress := svc.Status.LoadBalancer.Ingress
	if len(ingress) == 0 {
		return nil
	}

	lb := ingress[0]
	host := lo.Ternary(lb.IP != "", lb.IP, lb.Hostname)
	if host != "" {
		info.ExternalAddress = net.JoinHostPort(host, strconv.Itoa(p2pPort))
	}
	return nil
}

'''
'''--- internal/fullnode/peer_collector_test.go ---
package fullnode

import (
	"context"
	"errors"
	"testing"

	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	"github.com/stretchr/testify/require"
	corev1 "k8s.io/api/core/v1"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

type mockGetter func(ctx context.Context, key client.ObjectKey, obj client.Object, opts ...client.GetOption) error

func (fn mockGetter) Get(ctx context.Context, key client.ObjectKey, obj client.Object, opts ...client.GetOption) error {
	if ctx == nil {
		panic("nil context")
	}
	if len(opts) > 0 {
		panic("unexpected opts")
	}
	return fn(ctx, key, obj, opts...)
}

var panicGetter = mockGetter(func(ctx context.Context, key client.ObjectKey, obj client.Object, opts ...client.GetOption) error {
	panic("should not be called")
})

func TestPeerCollector_Collect(t *testing.T) {
	t.Parallel()

	ctx := context.Background()
	const (
		namespace = "strangelove"
		nodeKey   = `{"priv_key":{"type":"tendermint/PrivKeyEd25519","value":"HBX8VFQ4OdWfOwIOR7jj0af8mVHik5iGW9o1xnn4vRltk1HmwQS2LLGrMPVS2LIUO9BUqmZ1Pjt+qM8x0ibHxQ=="}}`
	)

	t.Run("happy path - private addresses", func(t *testing.T) {
		var crd cosmosv1.CosmosFullNode
		crd.Name = "dydx"
		crd.Namespace = namespace
		crd.Spec.Replicas = 2
		res, err := BuildNodeKeySecrets(nil, &crd)
		require.NoError(t, err)
		secret := res[0].Object()
		secret.Data[nodeKeyFile] = []byte(nodeKey)

		var (
			getCount int
			objKeys  []client.ObjectKey
		)
		getter := mockGetter(func(ctx context.Context, key client.ObjectKey, obj client.Object, opts ...client.GetOption) error {
			objKeys = append(objKeys, key)
			getCount++
			switch ref := obj.(type) {
			case *corev1.Secret:
				*ref = *secret
			case *corev1.Service:
				*ref = corev1.Service{}
			}
			return nil
		})

		collector := NewPeerCollector(getter)
		peers, err := collector.Collect(ctx, &crd)
		require.NoError(t, err)
		require.Len(t, peers, 2)

		require.Equal(t, 4, getCount) // 2 secrets + 2 services

		wantKeys := []client.ObjectKey{
			{Name: "dydx-node-key-0", Namespace: namespace},
			{Name: "dydx-p2p-0", Namespace: namespace},
			{Name: "dydx-node-key-1", Namespace: namespace},
			{Name: "dydx-p2p-1", Namespace: namespace},
		}
		require.Equal(t, wantKeys, objKeys)

		got := peers[client.ObjectKey{Name: "dydx-0", Namespace: namespace}]
		require.Equal(t, "1e23ce0b20ae2377925537cc71d1529d723bb892", got.NodeID)
		require.Equal(t, "dydx-p2p-0.strangelove.svc.cluster.local:26656", got.PrivateAddress)
		require.Equal(t, "1e23ce0b20ae2377925537cc71d1529d723bb892@dydx-p2p-0.strangelove.svc.cluster.local:26656", got.PrivatePeer())
		require.Empty(t, got.ExternalAddress)
		require.Equal(t, "1e23ce0b20ae2377925537cc71d1529d723bb892@0.0.0.0:26656", got.ExternalPeer())

		got = peers[client.ObjectKey{Name: "dydx-1", Namespace: namespace}]
		require.NotEmpty(t, got.NodeID)
		require.Equal(t, "dydx-p2p-1.strangelove.svc.cluster.local:26656", got.PrivateAddress)
		require.Empty(t, got.ExternalAddress)

		require.False(t, peers.HasIncompleteExternalAddress())
	})

	t.Run("happy path - external addresses", func(t *testing.T) {
		var crd cosmosv1.CosmosFullNode
		crd.Name = "dydx"
		crd.Namespace = namespace
		crd.Spec.Replicas = 3
		res, err := BuildNodeKeySecrets(nil, &crd)
		require.NoError(t, err)
		secret := res[0].Object()
		secret.Data[nodeKeyFile] = []byte(nodeKey)

		getter := mockGetter(func(ctx context.Context, key client.ObjectKey, obj client.Object, opts ...client.GetOption) error {
			switch ref := obj.(type) {
			case *corev1.Secret:
				*ref = *secret
			case *corev1.Service:
				var svc corev1.Service
				switch key.Name {
				case "dydx-p2p-0":
					svc.Spec.Type = corev1.ServiceTypeLoadBalancer
				case "dydx-p2p-1":
					svc.Spec.Type = corev1.ServiceTypeLoadBalancer
					svc.Status.LoadBalancer.Ingress = []corev1.LoadBalancerIngress{{IP: "1.2.3.4"}}
				case "dydx-p2p-2":
					svc.Spec.Type = corev1.ServiceTypeLoadBalancer
					svc.Status.LoadBalancer.Ingress = []corev1.LoadBalancerIngress{{Hostname: "host.example.com"}}
				}
				*ref = svc
			}
			return nil
		})

		collector := NewPeerCollector(getter)
		peers, err := collector.Collect(ctx, &crd)
		require.NoError(t, err)
		require.Len(t, peers, 3)

		got := peers[client.ObjectKey{Name: "dydx-0", Namespace: namespace}]
		require.Equal(t, "1e23ce0b20ae2377925537cc71d1529d723bb892", got.NodeID)
		require.Empty(t, got.ExternalAddress)
		require.Equal(t, "1e23ce0b20ae2377925537cc71d1529d723bb892@0.0.0.0:26656", got.ExternalPeer())

		got = peers[client.ObjectKey{Name: "dydx-1", Namespace: namespace}]
		require.Equal(t, "1.2.3.4:26656", got.ExternalAddress)
		require.Equal(t, "1e23ce0b20ae2377925537cc71d1529d723bb892@1.2.3.4:26656", got.ExternalPeer())

		got = peers[client.ObjectKey{Name: "dydx-2", Namespace: namespace}]
		require.Equal(t, "host.example.com:26656", got.ExternalAddress)
		require.Equal(t, "1e23ce0b20ae2377925537cc71d1529d723bb892@host.example.com:26656", got.ExternalPeer())

		require.True(t, peers.HasIncompleteExternalAddress())
		want := []string{"1e23ce0b20ae2377925537cc71d1529d723bb892@0.0.0.0:26656",
			"1e23ce0b20ae2377925537cc71d1529d723bb892@1.2.3.4:26656",
			"1e23ce0b20ae2377925537cc71d1529d723bb892@host.example.com:26656"}
		require.ElementsMatch(t, want, peers.AllExternal())
	})

	t.Run("zero replicas", func(t *testing.T) {
		var crd cosmosv1.CosmosFullNode
		crd.Spec.Replicas = 0

		collector := NewPeerCollector(panicGetter)
		peers, err := collector.Collect(ctx, &crd)
		require.NoError(t, err)
		require.Len(t, peers, 0)
	})

	t.Run("get error", func(t *testing.T) {
		getter := mockGetter(func(ctx context.Context, key client.ObjectKey, obj client.Object, opts ...client.GetOption) error {
			return errors.New("boom")
		})

		collector := NewPeerCollector(getter)
		var crd cosmosv1.CosmosFullNode
		crd.Name = "dydx"
		crd.Spec.Replicas = 1
		_, err := collector.Collect(ctx, &crd)

		require.Error(t, err)
		require.EqualError(t, err, "get secret dydx-node-key-0: boom")
		require.True(t, err.IsTransient())
	})

	t.Run("invalid node key", func(t *testing.T) {
		getter := mockGetter(func(ctx context.Context, key client.ObjectKey, obj client.Object, opts ...client.GetOption) error {
			switch ref := obj.(type) {
			case *corev1.Secret:
				var secret corev1.Secret
				secret.Data = map[string][]byte{nodeKeyFile: []byte("invalid")}
				*ref = secret
			case *corev1.Service:
				panic("should not be called")
			}
			return nil
		})

		var crd cosmosv1.CosmosFullNode
		crd.Name = "dydx"
		crd.Spec.Replicas = 1
		collector := NewPeerCollector(getter)
		_, err := collector.Collect(ctx, &crd)

		require.Error(t, err)
		require.Contains(t, err.Error(), "invalid character")
		require.False(t, err.IsTransient())
	})
}

'''
'''--- internal/fullnode/pod_builder.go ---
package fullnode

import (
	"bytes"
	"errors"
	"fmt"
	"path"
	"strings"
	"sync"

	"github.com/samber/lo"
	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	"github.com/strangelove-ventures/cosmos-operator/internal/healthcheck"
	"github.com/strangelove-ventures/cosmos-operator/internal/kube"
	"github.com/strangelove-ventures/cosmos-operator/internal/version"
	corev1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/api/resource"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/util/intstr"
)

var bufPool = sync.Pool{New: func() any { return new(bytes.Buffer) }}

const (
	healthCheckPort    = healthcheck.Port
	mainContainer      = "node"
	chainInitContainer = "chain-init"
)

// PodBuilder builds corev1.Pods
type PodBuilder struct {
	crd *cosmosv1.CosmosFullNode
	pod *corev1.Pod
}

// NewPodBuilder returns a valid PodBuilder.
//
// Panics if any argument is nil.
func NewPodBuilder(crd *cosmosv1.CosmosFullNode) PodBuilder {
	if crd == nil {
		panic(errors.New("nil CosmosFullNode"))
	}

	var (
		tpl                 = crd.Spec.PodTemplate
		startCmd, startArgs = startCmdAndArgs(crd)
		probes              = podReadinessProbes(crd)
	)

	versionCheckCmd := []string{"/manager", "versioncheck", "-d"}
	if crd.Spec.ChainSpec.DatabaseBackend != nil {
		versionCheckCmd = append(versionCheckCmd, "-b", *crd.Spec.ChainSpec.DatabaseBackend)
	}

	pod := corev1.Pod{
		TypeMeta: metav1.TypeMeta{
			Kind:       "Pod",
			APIVersion: "v1",
		},
		ObjectMeta: metav1.ObjectMeta{
			Namespace:   crd.Namespace,
			Labels:      defaultLabels(crd),
			Annotations: make(map[string]string),
		},
		Spec: corev1.PodSpec{
			ServiceAccountName: serviceAccountName(crd),
			SecurityContext: &corev1.PodSecurityContext{
				RunAsUser:           ptr(int64(1025)),
				RunAsGroup:          ptr(int64(1025)),
				RunAsNonRoot:        ptr(true),
				FSGroup:             ptr(int64(1025)),
				FSGroupChangePolicy: ptr(corev1.FSGroupChangeOnRootMismatch),
				SeccompProfile:      &corev1.SeccompProfile{Type: corev1.SeccompProfileTypeRuntimeDefault},
			},
			Subdomain: crd.Name,
			Containers: []corev1.Container{
				// Main start container.
				{
					Name:  mainContainer,
					Image: tpl.Image,
					// The following is a useful hack if you need to inspect the PV.
					//Command: []string{"/bin/sh"},
					//Args:    []string{"-c", `trap : TERM INT; sleep infinity & wait`},
					Command:         []string{startCmd},
					Args:            startArgs,
					Env:             envVars(crd),
					Ports:           buildPorts(crd.Spec.Type),
					Resources:       tpl.Resources,
					ReadinessProbe:  probes[0],
					ImagePullPolicy: tpl.ImagePullPolicy,
					WorkingDir:      workDir,
				},
				// healthcheck sidecar
				{
					Name: "healthcheck",
					// Available images: https://github.com/orgs/strangelove-ventures/packages?repo_name=cosmos-operator
					// IMPORTANT: Must use v0.6.2 or later.
					Image:   "ghcr.io/strangelove-ventures/cosmos-operator:" + version.DockerTag(),
					Command: []string{"/manager", "healthcheck"},
					Ports:   []corev1.ContainerPort{{ContainerPort: healthCheckPort, Protocol: corev1.ProtocolTCP}},
					Resources: corev1.ResourceRequirements{
						Requests: corev1.ResourceList{
							corev1.ResourceCPU:    resource.MustParse("5m"),
							corev1.ResourceMemory: resource.MustParse("16Mi"),
						},
					},
					ReadinessProbe:  probes[1],
					ImagePullPolicy: tpl.ImagePullPolicy,
				},
			},
		},
	}

	if len(crd.Spec.ChainSpec.Versions) > 0 {
		// version check sidecar, runs on inverval in case the instance is halting for upgrade.
		pod.Spec.Containers = append(pod.Spec.Containers, corev1.Container{
			Name:    "version-check-interval",
			Image:   "ghcr.io/strangelove-ventures/cosmos-operator:" + version.DockerTag(),
			Command: versionCheckCmd,
			Resources: corev1.ResourceRequirements{
				Requests: corev1.ResourceList{
					corev1.ResourceCPU:    resource.MustParse("5m"),
					corev1.ResourceMemory: resource.MustParse("16Mi"),
				},
			},
			Env:             envVars(crd),
			ImagePullPolicy: tpl.ImagePullPolicy,
			WorkingDir:      workDir,
			SecurityContext: &corev1.SecurityContext{},
		})
	}

	preserveMergeInto(pod.Labels, tpl.Metadata.Labels)
	preserveMergeInto(pod.Annotations, tpl.Metadata.Annotations)

	return PodBuilder{
		crd: crd,
		pod: &pod,
	}
}

func podReadinessProbes(crd *cosmosv1.CosmosFullNode) []*corev1.Probe {
	if crd.Spec.PodTemplate.Probes.Strategy == cosmosv1.FullNodeProbeStrategyNone {
		return []*corev1.Probe{nil, nil}
	}

	mainProbe := &corev1.Probe{
		ProbeHandler: corev1.ProbeHandler{
			HTTPGet: &corev1.HTTPGetAction{
				Path:   "/health",
				Port:   intstr.FromInt(rpcPort),
				Scheme: corev1.URISchemeHTTP,
			},
		},
		InitialDelaySeconds: 1,
		TimeoutSeconds:      10,
		PeriodSeconds:       10,
		SuccessThreshold:    1,
		FailureThreshold:    5,
	}

	sidecarProbe := &corev1.Probe{
		ProbeHandler: corev1.ProbeHandler{
			HTTPGet: &corev1.HTTPGetAction{
				Path:   "/",
				Port:   intstr.FromInt(healthCheckPort),
				Scheme: corev1.URISchemeHTTP,
			},
		},
		InitialDelaySeconds: 1,
		TimeoutSeconds:      10,
		PeriodSeconds:       10,
		SuccessThreshold:    1,
		FailureThreshold:    3,
	}

	return []*corev1.Probe{mainProbe, sidecarProbe}
}

// Build assigns the CosmosFullNode crd as the owner and returns a fully constructed pod.
func (b PodBuilder) Build() (*corev1.Pod, error) {
	pod := b.pod.DeepCopy()

	if len(b.crd.Spec.ChainSpec.Versions) > 0 {
		instanceHeight := uint64(0)
		if height, ok := b.crd.Status.Height[pod.Name]; ok {
			instanceHeight = height
		}
		var image string
		for _, version := range b.crd.Spec.ChainSpec.Versions {
			if instanceHeight < version.UpgradeHeight {
				break
			}
			image = version.Image
		}
		if image != "" {
			setChainContainerImage(pod, image)
		}
	}
	if o, ok := b.crd.Spec.InstanceOverrides[pod.Name]; ok {
		if o.DisableStrategy != nil {
			return nil, nil
		}
		if o.Image != "" {
			setChainContainerImage(pod, o.Image)
		}
	}

	if err := kube.ApplyStrategicMergePatch(pod, podPatch(b.crd)); err != nil {
		return nil, err
	}

	kube.NormalizeMetadata(&pod.ObjectMeta)
	return pod, nil
}

const (
	volChainHome = "vol-chain-home" // Stores live chain data and config files.
	volTmp       = "vol-tmp"        // Stores temporary config files for manipulation later.
	volConfig    = "vol-config"     // Items from ConfigMap.
	volSystemTmp = "vol-system-tmp" // Necessary for statesync or else you may see the error: ERR State sync failed err="failed to create chunk queue: unable to create temp dir for state sync chunks: stat /tmp: no such file or directory" module=statesync
	volNodeKey   = "vol-node-key"   // Secret containing the node key.
)

// WithOrdinal updates adds name and other metadata to the pod using "ordinal" which is the pod's
// ordered sequence. Pods have deterministic, consistent names similar to a StatefulSet instead of generated names.
func (b PodBuilder) WithOrdinal(ordinal int32) PodBuilder {
	pod := b.pod.DeepCopy()
	name := instanceName(b.crd, ordinal)

	pod.Labels[kube.InstanceLabel] = name

	pod.Name = name
	pod.Spec.InitContainers = initContainers(b.crd, name)

	pod.Spec.Hostname = pod.Name
	pod.Spec.Subdomain = b.crd.Name

	pod.Spec.Volumes = []corev1.Volume{
		{
			Name: volChainHome,
			VolumeSource: corev1.VolumeSource{
				PersistentVolumeClaim: &corev1.PersistentVolumeClaimVolumeSource{ClaimName: pvcName(b.crd, ordinal)},
			},
		},
		{
			Name: volTmp,
			VolumeSource: corev1.VolumeSource{
				EmptyDir: &corev1.EmptyDirVolumeSource{},
			},
		},
		{
			Name: volConfig,
			VolumeSource: corev1.VolumeSource{
				ConfigMap: &corev1.ConfigMapVolumeSource{
					LocalObjectReference: corev1.LocalObjectReference{Name: instanceName(b.crd, ordinal)},
					Items: []corev1.KeyToPath{
						{Key: configOverlayFile, Path: configOverlayFile},
						{Key: appOverlayFile, Path: appOverlayFile},
					},
				},
			},
		},
		{
			Name: volSystemTmp,
			VolumeSource: corev1.VolumeSource{
				EmptyDir: &corev1.EmptyDirVolumeSource{},
			},
		},
		{
			Name: volNodeKey,
			VolumeSource: corev1.VolumeSource{
				Secret: &corev1.SecretVolumeSource{
					SecretName: nodeKeySecretName(b.crd, ordinal),
					Items: []corev1.KeyToPath{
						{Key: nodeKeyFile, Path: nodeKeyFile},
					},
				},
			},
		},
	}

	// Mounts required by all containers.
	mounts := []corev1.VolumeMount{
		{Name: volChainHome, MountPath: ChainHomeDir(b.crd)},
		{Name: volSystemTmp, MountPath: systemTmpDir},
	}
	// Additional mounts only needed for init containers.
	for i := range pod.Spec.InitContainers {
		pod.Spec.InitContainers[i].VolumeMounts = append(mounts, []corev1.VolumeMount{
			{Name: volTmp, MountPath: tmpDir},
			{Name: volConfig, MountPath: tmpConfigDir},
		}...)
	}

	// At this point, guaranteed to have at least 2 containers.
	pod.Spec.Containers[0].VolumeMounts = append(mounts, corev1.VolumeMount{
		Name: volNodeKey, MountPath: path.Join(ChainHomeDir(b.crd), "config", nodeKeyFile), SubPath: nodeKeyFile,
	})
	pod.Spec.Containers[1].VolumeMounts = []corev1.VolumeMount{
		// The healthcheck sidecar needs access to the home directory so it can read disk usage.
		{Name: volChainHome, MountPath: ChainHomeDir(b.crd), ReadOnly: true},
	}
	if len(pod.Spec.Containers) > 2 {
		pod.Spec.Containers[2].VolumeMounts = mounts
	}

	b.pod = pod
	return b
}

const (
	workDir        = "/home/operator"
	tmpDir         = workDir + "/.tmp"
	tmpConfigDir   = workDir + "/.config"
	infraToolImage = "ghcr.io/strangelove-ventures/infra-toolkit:v0.0.1"

	// Necessary for statesync
	systemTmpDir = "/tmp"
)

// ChainHomeDir is the abs filepath for the chain's home directory.
func ChainHomeDir(crd *cosmosv1.CosmosFullNode) string {
	if home := crd.Spec.ChainSpec.HomeDir; home != "" {
		return path.Join(workDir, home)
	}
	return workDir + "/cosmos"
}

func envVars(crd *cosmosv1.CosmosFullNode) []corev1.EnvVar {
	home := ChainHomeDir(crd)
	return []corev1.EnvVar{
		{Name: "HOME", Value: workDir},
		{Name: "CHAIN_HOME", Value: home},
		{Name: "GENESIS_FILE", Value: path.Join(home, "config", "genesis.json")},
		{Name: "ADDRBOOK_FILE", Value: path.Join(home, "config", "addrbook.json")},
		{Name: "CONFIG_DIR", Value: path.Join(home, "config")},
		{Name: "DATA_DIR", Value: path.Join(home, "data")},
	}
}

func initContainers(crd *cosmosv1.CosmosFullNode, moniker string) []corev1.Container {
	tpl := crd.Spec.PodTemplate
	binary := crd.Spec.ChainSpec.Binary
	genesisCmd, genesisArgs := DownloadGenesisCommand(crd.Spec.ChainSpec)
	addrbookCmd, addrbookArgs := DownloadAddrbookCommand(crd.Spec.ChainSpec)
	env := envVars(crd)

	initCmd := fmt.Sprintf("%s init --chain-id %s %s", binary, crd.Spec.ChainSpec.ChainID, moniker)
	if len(crd.Spec.ChainSpec.AdditionalInitArgs) > 0 {
		initCmd += " " + strings.Join(crd.Spec.ChainSpec.AdditionalInitArgs, " ")
	}
	required := []corev1.Container{
		{
			Name:            "clean-init",
			Image:           infraToolImage,
			Command:         []string{"sh"},
			Args:            []string{"-c", `rm -rf "$HOME/.tmp/*"`},
			Env:             env,
			ImagePullPolicy: tpl.ImagePullPolicy,
			WorkingDir:      workDir,
		},
		{
			Name:    chainInitContainer,
			Image:   tpl.Image,
			Command: []string{"sh"},
			Args: []string{"-c",
				fmt.Sprintf(`
set -eu
if [ ! -d "$CHAIN_HOME/data" ]; then
	echo "Initializing chain..."
	%s --home "$CHAIN_HOME"
else
	echo "Skipping chain init; already initialized."
fi

echo "Initializing into tmp dir for downstream processing..."
%s --home "$HOME/.tmp"
`, initCmd, initCmd),
			},
			Env:             env,
			ImagePullPolicy: tpl.ImagePullPolicy,
			WorkingDir:      workDir,
		},

		{
			Name:            "genesis-init",
			Image:           infraToolImage,
			Command:         []string{genesisCmd},
			Args:            genesisArgs,
			Env:             env,
			ImagePullPolicy: tpl.ImagePullPolicy,
			WorkingDir:      workDir,
		},
		{
			Name:            "addrbook-init",
			Image:           infraToolImage,
			Command:         []string{addrbookCmd},
			Args:            addrbookArgs,
			Env:             env,
			ImagePullPolicy: tpl.ImagePullPolicy,
			WorkingDir:      workDir,
		},
		{
			Name:    "config-merge",
			Image:   infraToolImage,
			Command: []string{"sh"},
			Args: []string{"-c",
				`
set -eu
CONFIG_DIR="$CHAIN_HOME/config"
TMP_DIR="$HOME/.tmp/config"
OVERLAY_DIR="$HOME/.config"

# This is a hack to prevent adding another init container.
# Ideally, this step is not concerned with merging config, so it would live elsewhere.
# The node key is a secret mounted into the main "node" container, so we do not need this one.
echo "Removing node key from chain's init subcommand..."
rm -rf "$CONFIG_DIR/node_key.json"

echo "Merging config..."
set -x
config-merge -f toml "$TMP_DIR/config.toml" "$OVERLAY_DIR/config-overlay.toml" > "$CONFIG_DIR/config.toml"
config-merge -f toml "$TMP_DIR/app.toml" "$OVERLAY_DIR/app-overlay.toml" > "$CONFIG_DIR/app.toml"
`,
			},
			Env:             env,
			ImagePullPolicy: tpl.ImagePullPolicy,
			WorkingDir:      workDir,
		},
	}

	if willRestoreFromSnapshot(crd) {
		cmd, args := DownloadSnapshotCommand(crd.Spec.ChainSpec)
		required = append(required, corev1.Container{
			Name:            "snapshot-restore",
			Image:           infraToolImage,
			Command:         []string{cmd},
			Args:            args,
			Env:             env,
			ImagePullPolicy: tpl.ImagePullPolicy,
			WorkingDir:      workDir,
		})
	}

	versionCheckCmd := []string{"/manager", "versioncheck"}
	if crd.Spec.ChainSpec.DatabaseBackend != nil {
		versionCheckCmd = append(versionCheckCmd, "-b", *crd.Spec.ChainSpec.DatabaseBackend)
	}

	// Append version check after snapshot download, if applicable.
	// That way the version check will be after the database is initialized.
	// This initContainer will update the crd status with the current height for the pod,
	// And then panic if the image version is not correct for the current height.
	// After the status is patched, the pod will be restarted with the correct image.
	required = append(required, corev1.Container{
		Name:    "version-check",
		Image:   "ghcr.io/strangelove-ventures/cosmos-operator:" + version.DockerTag(),
		Command: versionCheckCmd,
		Resources: corev1.ResourceRequirements{
			Requests: corev1.ResourceList{
				corev1.ResourceCPU:    resource.MustParse("5m"),
				corev1.ResourceMemory: resource.MustParse("16Mi"),
			},
		},
		Env:             env,
		ImagePullPolicy: tpl.ImagePullPolicy,
		WorkingDir:      workDir,
		SecurityContext: &corev1.SecurityContext{},
	})

	return required
}

func startCmdAndArgs(crd *cosmosv1.CosmosFullNode) (string, []string) {
	var (
		binary             = crd.Spec.ChainSpec.Binary
		args               = startCommandArgs(crd)
		privvalSleep int32 = 10
	)
	if v := crd.Spec.ChainSpec.PrivvalSleepSeconds; v != nil {
		privvalSleep = *v
	}

	if crd.Spec.Type == cosmosv1.Sentry && privvalSleep > 0 {
		shellBody := fmt.Sprintf(`sleep %d
%s %s`, privvalSleep, binary, strings.Join(args, " "))
		return "sh", []string{"-c", shellBody}
	}

	return binary, args
}

func startCommandArgs(crd *cosmosv1.CosmosFullNode) []string {
	args := []string{"start", "--home", ChainHomeDir(crd)}
	cfg := crd.Spec.ChainSpec
	if cfg.SkipInvariants {
		args = append(args, "--x-crisis-skip-assert-invariants")
	}
	if lvl := cfg.LogLevel; lvl != nil {
		args = append(args, "--log_level", *lvl)
	}
	if format := cfg.LogFormat; format != nil {
		args = append(args, "--log_format", *format)
	}
	if len(crd.Spec.ChainSpec.AdditionalStartArgs) > 0 {
		args = append(args, crd.Spec.ChainSpec.AdditionalStartArgs...)
	}
	return args
}

func willRestoreFromSnapshot(crd *cosmosv1.CosmosFullNode) bool {
	return crd.Spec.ChainSpec.SnapshotURL != nil || crd.Spec.ChainSpec.SnapshotScript != nil
}

func podPatch(crd *cosmosv1.CosmosFullNode) *corev1.Pod {
	tpl := crd.Spec.PodTemplate
	// For fields with sliceOrDefault if you pass nil, the field is deleted.
	spec := corev1.PodSpec{
		Affinity:                      tpl.Affinity,
		Containers:                    sliceOrDefault(tpl.Containers, []corev1.Container{}),
		ImagePullSecrets:              sliceOrDefault(tpl.ImagePullSecrets, []corev1.LocalObjectReference{}),
		InitContainers:                sliceOrDefault(tpl.InitContainers, []corev1.Container{}),
		NodeSelector:                  tpl.NodeSelector,
		Priority:                      tpl.Priority,
		PriorityClassName:             tpl.PriorityClassName,
		TerminationGracePeriodSeconds: valOrDefault(tpl.TerminationGracePeriodSeconds, ptr(int64(30))),
		Tolerations:                   sliceOrDefault(tpl.Tolerations, []corev1.Toleration{}),
		Volumes:                       sliceOrDefault(tpl.Volumes, []corev1.Volume{}),
	}
	return &corev1.Pod{Spec: spec}
}

// PVCName returns the primary PVC holding the chain data associated with the pod.
func PVCName(pod *corev1.Pod) string {
	found, ok := lo.Find(pod.Spec.Volumes, func(v corev1.Volume) bool { return v.Name == volChainHome })
	if !ok {
		return ""
	}
	if found.PersistentVolumeClaim == nil {
		return ""
	}
	return found.PersistentVolumeClaim.ClaimName
}

'''
'''--- internal/fullnode/pod_builder_test.go ---
package fullnode

import (
	"strings"
	"testing"

	"github.com/samber/lo"
	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	"github.com/strangelove-ventures/cosmos-operator/internal/kube"
	"github.com/strangelove-ventures/cosmos-operator/internal/test"
	"github.com/stretchr/testify/require"
	corev1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/api/resource"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/util/intstr"
)

func defaultCRD() cosmosv1.CosmosFullNode {
	return cosmosv1.CosmosFullNode{
		ObjectMeta: metav1.ObjectMeta{
			Name:            "osmosis",
			Namespace:       "test",
			ResourceVersion: "_resource_version_",
		},
		Spec: cosmosv1.FullNodeSpec{
			ChainSpec: cosmosv1.ChainSpec{Network: "mainnet"},
			PodTemplate: cosmosv1.PodSpec{
				Image: "busybox:v1.2.3",
				Resources: corev1.ResourceRequirements{
					Limits: map[corev1.ResourceName]resource.Quantity{
						corev1.ResourceCPU:    resource.MustParse("5"),
						corev1.ResourceMemory: resource.MustParse("5Gi"),
					},
					Requests: map[corev1.ResourceName]resource.Quantity{
						corev1.ResourceCPU:    resource.MustParse("1"),
						corev1.ResourceMemory: resource.MustParse("500M"),
					},
				},
			},
			VolumeClaimTemplate: cosmosv1.PersistentVolumeClaimSpec{
				Resources: corev1.ResourceRequirements{
					Requests: corev1.ResourceList{corev1.ResourceStorage: resource.MustParse("100Gi")},
				},
			},
		},
	}
}

func TestPodBuilder(t *testing.T) {
	t.Parallel()

	t.Run("happy path - critical fields", func(t *testing.T) {
		crd := defaultCRD()
		builder := NewPodBuilder(&crd)
		pod, err := builder.WithOrdinal(5).Build()
		require.NoError(t, err)

		require.Equal(t, "Pod", pod.Kind)
		require.Equal(t, "v1", pod.APIVersion)

		require.Equal(t, "test", pod.Namespace)
		require.Equal(t, "osmosis-5", pod.Name)

		require.Equal(t, "osmosis", pod.Spec.Subdomain)
		require.Equal(t, "osmosis-5", pod.Spec.Hostname)

		wantLabels := map[string]string{
			"app.kubernetes.io/instance":   "osmosis-5",
			"app.kubernetes.io/component":  "CosmosFullNode",
			"app.kubernetes.io/created-by": "cosmos-operator",
			"app.kubernetes.io/name":       "osmosis",
			"app.kubernetes.io/version":    "v1.2.3",
			"cosmos.strange.love/network":  "mainnet",
			"cosmos.strange.love/type":     "FullNode",
		}
		require.Equal(t, wantLabels, pod.Labels)
		require.NotNil(t, pod.Annotations)
		require.Empty(t, pod.Annotations)

		require.EqualValues(t, 30, *pod.Spec.TerminationGracePeriodSeconds)

		sc := pod.Spec.SecurityContext
		require.EqualValues(t, 1025, *sc.RunAsUser)
		require.EqualValues(t, 1025, *sc.RunAsGroup)
		require.EqualValues(t, 1025, *sc.FSGroup)
		require.EqualValues(t, "OnRootMismatch", *sc.FSGroupChangePolicy)
		require.True(t, *sc.RunAsNonRoot)
		require.Equal(t, corev1.SeccompProfileTypeRuntimeDefault, sc.SeccompProfile.Type)

		// Test we don't share or leak data per invocation.
		pod, err = builder.Build()
		require.NoError(t, err)
		require.Empty(t, pod.Name)

		pod, err = builder.WithOrdinal(123).Build()
		require.NoError(t, err)
		require.Equal(t, "osmosis-123", pod.Name)

		crd.Spec.Type = cosmosv1.FullNode
		pod2, err := NewPodBuilder(&crd).WithOrdinal(123).Build()
		require.NoError(t, err)
		require.Equal(t, pod, pod2)
	})

	t.Run("happy path - ports", func(t *testing.T) {
		crd := defaultCRD()
		pod, err := NewPodBuilder(&crd).Build()
		require.NoError(t, err)
		ports := pod.Spec.Containers[0].Ports

		require.Equal(t, 7, len(ports))

		for i, tt := range []struct {
			Name string
			Port int32
		}{
			{"api", 1317},
			{"rosetta", 8080},
			{"grpc", 9090},
			{"prometheus", 26660},
			{"p2p", 26656},
			{"rpc", 26657},
			{"grpc-web", 9091},
			{"json-rpc", 8545},
			{"json-rpc-ws", 8545},
		} {
			port := ports[i]
			require.Equal(t, tt.Name, port.Name, tt)
			require.Equal(t, corev1.ProtocolTCP, port.Protocol)
			require.Equal(t, tt.Port, port.ContainerPort)
			require.Zero(t, port.HostPort)
		}
	})

	t.Run("ports - sentry", func(t *testing.T) {
		crd := defaultCRD()
		crd.Spec.Type = cosmosv1.Sentry

		pod, err := NewPodBuilder(&crd).Build()
		require.NoError(t, err)
		ports := pod.Spec.Containers[0].Ports

		require.Equal(t, 8, len(ports))

		got, _ := lo.Last(ports)

		require.Equal(t, "privval", got.Name)
		require.Equal(t, corev1.ProtocolTCP, got.Protocol)
		require.EqualValues(t, 1234, got.ContainerPort)
		require.Zero(t, got.HostPort)
	})

	t.Run("happy path - optional fields", func(t *testing.T) {
		optCrd := defaultCRD()

		optCrd.Spec.PodTemplate.Metadata.Labels = map[string]string{"custom": "label", kube.NameLabel: "should not see me"}
		optCrd.Spec.PodTemplate.Metadata.Annotations = map[string]string{"custom": "annotation"}

		optCrd.Spec.PodTemplate.Affinity = &corev1.Affinity{
			PodAffinity: &corev1.PodAffinity{
				RequiredDuringSchedulingIgnoredDuringExecution: []corev1.PodAffinityTerm{{TopologyKey: "affinity1"}},
			},
		}
		optCrd.Spec.PodTemplate.ImagePullPolicy = corev1.PullAlways
		optCrd.Spec.PodTemplate.ImagePullSecrets = []corev1.LocalObjectReference{{Name: "pullSecrets"}}
		optCrd.Spec.PodTemplate.NodeSelector = map[string]string{"node": "test"}
		optCrd.Spec.PodTemplate.Tolerations = []corev1.Toleration{{Key: "toleration1"}}
		optCrd.Spec.PodTemplate.PriorityClassName = "priority1"
		optCrd.Spec.PodTemplate.Priority = ptr(int32(55))
		optCrd.Spec.PodTemplate.TerminationGracePeriodSeconds = ptr(int64(40))

		builder := NewPodBuilder(&optCrd)
		pod, err := builder.WithOrdinal(9).Build()
		require.NoError(t, err)

		require.Equal(t, "label", pod.Labels["custom"])
		// Operator label takes precedence.
		require.Equal(t, "osmosis", pod.Labels[kube.NameLabel])

		require.Equal(t, "annotation", pod.Annotations["custom"])

		require.Equal(t, optCrd.Spec.PodTemplate.Affinity, pod.Spec.Affinity)
		require.Equal(t, optCrd.Spec.PodTemplate.Tolerations, pod.Spec.Tolerations)
		require.EqualValues(t, 40, *optCrd.Spec.PodTemplate.TerminationGracePeriodSeconds)
		require.Equal(t, optCrd.Spec.PodTemplate.NodeSelector, pod.Spec.NodeSelector)

		require.Equal(t, "priority1", pod.Spec.PriorityClassName)
		require.EqualValues(t, 55, *pod.Spec.Priority)
		require.Equal(t, optCrd.Spec.PodTemplate.ImagePullSecrets, pod.Spec.ImagePullSecrets)

		require.EqualValues(t, "Always", pod.Spec.Containers[0].ImagePullPolicy)
	})

	t.Run("long name", func(t *testing.T) {
		longCrd := defaultCRD()
		longCrd.Name = strings.Repeat("a", 253)

		builder := NewPodBuilder(&longCrd)
		pod, err := builder.WithOrdinal(125).Build()
		require.NoError(t, err)

		require.Regexp(t, `a.*-125`, pod.Name)

		test.RequireValidMetadata(t, pod)
	})

	t.Run("additional args", func(t *testing.T) {
		crd := defaultCRD()

		crd.Spec.ChainSpec.AdditionalStartArgs = []string{"--foo", "bar"}

		builder := NewPodBuilder(&crd)
		pod, err := builder.WithOrdinal(0).Build()
		require.NoError(t, err)

		test.RequireValidMetadata(t, pod)

		require.Equal(t, []string{"start", "--home", "/home/operator/cosmos", "--foo", "bar"}, pod.Spec.Containers[0].Args)
	})

	t.Run("containers", func(t *testing.T) {
		crd := defaultCRD()
		const wantWrkDir = "/home/operator"
		crd.Spec.ChainSpec.ChainID = "osmosis-123"
		crd.Spec.ChainSpec.Binary = "osmosisd"
		crd.Spec.ChainSpec.SnapshotURL = ptr("https://example.com/snapshot.tar")
		crd.Spec.PodTemplate.Image = "main-image:v1.2.3"
		builder := NewPodBuilder(&crd)
		pod, err := builder.WithOrdinal(6).Build()
		require.NoError(t, err)

		require.Len(t, pod.Spec.Containers, 2)

		startContainer := pod.Spec.Containers[0]
		require.Equal(t, "node", startContainer.Name)
		require.Empty(t, startContainer.ImagePullPolicy)
		require.Equal(t, crd.Spec.PodTemplate.Resources, startContainer.Resources)
		require.Equal(t, wantWrkDir, startContainer.WorkingDir)

		require.Equal(t, startContainer.Env[0].Name, "HOME")
		require.Equal(t, startContainer.Env[0].Value, "/home/operator")
		require.Equal(t, startContainer.Env[1].Name, "CHAIN_HOME")
		require.Equal(t, startContainer.Env[1].Value, "/home/operator/cosmos")
		require.Equal(t, startContainer.Env[2].Name, "GENESIS_FILE")
		require.Equal(t, startContainer.Env[2].Value, "/home/operator/cosmos/config/genesis.json")
		require.Equal(t, startContainer.Env[3].Name, "ADDRBOOK_FILE")
		require.Equal(t, startContainer.Env[3].Value, "/home/operator/cosmos/config/addrbook.json")
		require.Equal(t, startContainer.Env[4].Name, "CONFIG_DIR")
		require.Equal(t, startContainer.Env[4].Value, "/home/operator/cosmos/config")
		require.Equal(t, startContainer.Env[5].Name, "DATA_DIR")
		require.Equal(t, startContainer.Env[5].Value, "/home/operator/cosmos/data")
		require.Equal(t, envVars(&crd), startContainer.Env)

		healthContainer := pod.Spec.Containers[1]
		require.Equal(t, "healthcheck", healthContainer.Name)
		require.Equal(t, "ghcr.io/strangelove-ventures/cosmos-operator:latest", healthContainer.Image)
		require.Equal(t, []string{"/manager", "healthcheck"}, healthContainer.Command)
		require.Empty(t, healthContainer.Args)
		require.Empty(t, healthContainer.ImagePullPolicy)
		require.NotEmpty(t, healthContainer.Resources)
		require.Empty(t, healthContainer.Env)
		healthPort := corev1.ContainerPort{
			ContainerPort: 1251,
			Protocol:      "TCP",
		}
		require.Equal(t, healthPort, healthContainer.Ports[0])

		require.Len(t, lo.Map(pod.Spec.InitContainers, func(c corev1.Container, _ int) string { return c.Name }), 7)

		wantInitImages := []string{
			"ghcr.io/strangelove-ventures/infra-toolkit:v0.0.1",
			"main-image:v1.2.3",
			"ghcr.io/strangelove-ventures/infra-toolkit:v0.0.1",
			"ghcr.io/strangelove-ventures/infra-toolkit:v0.0.1",
			"ghcr.io/strangelove-ventures/infra-toolkit:v0.0.1",
			"ghcr.io/strangelove-ventures/infra-toolkit:v0.0.1",
			"ghcr.io/strangelove-ventures/cosmos-operator:latest",
		}
		require.Equal(t, wantInitImages, lo.Map(pod.Spec.InitContainers, func(c corev1.Container, _ int) string {
			return c.Image
		}))

		for _, c := range pod.Spec.InitContainers {
			require.Equal(t, envVars(&crd), startContainer.Env, c.Name)
			require.Equal(t, wantWrkDir, c.WorkingDir)
		}

		freshCont := pod.Spec.InitContainers[0]
		require.Contains(t, freshCont.Args[1], `rm -rf "$HOME/.tmp/*"`)

		initCont := pod.Spec.InitContainers[1]
		require.Contains(t, initCont.Args[1], `osmosisd init --chain-id osmosis-123 osmosis-6 --home "$CHAIN_HOME"`)
		require.Contains(t, initCont.Args[1], `osmosisd init --chain-id osmosis-123 osmosis-6 --home "$HOME/.tmp"`)

		mergeConfig1 := pod.Spec.InitContainers[3]
		// The order of config-merge arguments is important. Rightmost takes precedence.
		require.Contains(t, mergeConfig1.Args[1], `echo Using default address book`)

		mergeConfig := pod.Spec.InitContainers[4]
		// The order of config-merge arguments is important. Rightmost takes precedence.
		require.Contains(t, mergeConfig.Args[1], `config-merge -f toml "$TMP_DIR/config.toml" "$OVERLAY_DIR/config-overlay.toml" > "$CONFIG_DIR/config.toml"`)
		require.Contains(t, mergeConfig.Args[1], `config-merge -f toml "$TMP_DIR/app.toml" "$OVERLAY_DIR/app-overlay.toml" > "$CONFIG_DIR/app.toml`)
	})

	t.Run("containers - configured home dir", func(t *testing.T) {
		crd := defaultCRD()
		crd.Spec.ChainSpec.HomeDir = ".osmosisd"
		builder := NewPodBuilder(&crd)
		pod, err := builder.WithOrdinal(6).Build()
		require.NoError(t, err)

		require.Len(t, pod.Spec.Containers, 2)

		container := pod.Spec.Containers[0]
		require.Equal(t, "node", container.Name)
		require.Empty(t, container.ImagePullPolicy)
		require.Equal(t, crd.Spec.PodTemplate.Resources, container.Resources)

		require.Equal(t, container.Env[0].Name, "HOME")
		require.Equal(t, container.Env[0].Value, "/home/operator")
		require.Equal(t, container.Env[1].Name, "CHAIN_HOME")
		require.Equal(t, container.Env[1].Value, "/home/operator/.osmosisd")
		require.Equal(t, container.Env[2].Name, "GENESIS_FILE")
		require.Equal(t, container.Env[2].Value, "/home/operator/.osmosisd/config/genesis.json")
		require.Equal(t, container.Env[3].Name, "ADDRBOOK_FILE")
		require.Equal(t, container.Env[3].Value, "/home/operator/.osmosisd/config/addrbook.json")
		require.Equal(t, container.Env[4].Name, "CONFIG_DIR")
		require.Equal(t, container.Env[4].Value, "/home/operator/.osmosisd/config")
		require.Equal(t, container.Env[5].Name, "DATA_DIR")
		require.Equal(t, container.Env[5].Value, "/home/operator/.osmosisd/data")

		require.NotEmpty(t, pod.Spec.InitContainers)

		for _, c := range pod.Spec.InitContainers {
			require.Equal(t, container.Env, c.Env, c.Name)
		}
	})

	t.Run("volumes", func(t *testing.T) {
		crd := defaultCRD()
		builder := NewPodBuilder(&crd)
		pod, err := builder.WithOrdinal(5).Build()
		require.NoError(t, err)

		vols := pod.Spec.Volumes
		require.Equal(t, 5, len(vols))

		require.Equal(t, "vol-chain-home", vols[0].Name)
		require.Equal(t, "pvc-osmosis-5", vols[0].PersistentVolumeClaim.ClaimName)

		require.Equal(t, "vol-tmp", vols[1].Name)
		require.NotNil(t, vols[1].EmptyDir)

		require.Equal(t, "vol-config", vols[2].Name)
		require.Equal(t, "osmosis-5", vols[2].ConfigMap.Name)
		wantItems := []corev1.KeyToPath{
			{Key: "config-overlay.toml", Path: "config-overlay.toml"},
			{Key: "app-overlay.toml", Path: "app-overlay.toml"},
		}
		require.Equal(t, wantItems, vols[2].ConfigMap.Items)

		// Required for statesync
		require.Equal(t, "vol-system-tmp", vols[3].Name)
		require.NotNil(t, vols[3].EmptyDir)

		// Node key
		require.Equal(t, "vol-node-key", vols[4].Name)
		require.Equal(t, "osmosis-node-key-5", vols[4].Secret.SecretName)
		require.Equal(t, []corev1.KeyToPath{{Key: "node_key.json", Path: "node_key.json"}}, vols[4].Secret.Items)

		require.Equal(t, len(pod.Spec.Containers), 2)

		c := pod.Spec.Containers[0]
		require.Equal(t, "node", c.Name) // Sanity check

		require.Len(t, c.VolumeMounts, 3)
		mount := c.VolumeMounts[0]
		require.Equal(t, "vol-chain-home", mount.Name)
		require.Equal(t, "/home/operator/cosmos", mount.MountPath)
		require.False(t, mount.ReadOnly)

		mount = c.VolumeMounts[1]
		require.Equal(t, "vol-system-tmp", mount.Name)
		require.Equal(t, "/tmp", mount.MountPath)
		require.False(t, mount.ReadOnly)

		mount = c.VolumeMounts[2]
		require.Equal(t, "vol-node-key", mount.Name)
		require.Equal(t, "/home/operator/cosmos/config/node_key.json", mount.MountPath)
		require.Equal(t, "node_key.json", mount.SubPath)

		// healtcheck sidecar
		c = pod.Spec.Containers[1]
		require.Equal(t, 1, len(c.VolumeMounts))
		require.Equal(t, "healthcheck", c.Name) // Sanity check
		mount = c.VolumeMounts[0]
		require.Equal(t, "vol-chain-home", mount.Name)
		require.Equal(t, "/home/operator/cosmos", mount.MountPath)
		require.True(t, mount.ReadOnly)

		for _, c := range pod.Spec.InitContainers {
			require.Len(t, c.VolumeMounts, 4)
			mount := c.VolumeMounts[0]
			require.Equal(t, "vol-chain-home", mount.Name, c.Name)
			require.Equal(t, "/home/operator/cosmos", mount.MountPath, c.Name)

			mount = c.VolumeMounts[1]
			require.Equal(t, "vol-system-tmp", mount.Name, c.Name)
			require.Equal(t, "/tmp", mount.MountPath, c.Name)

			mount = c.VolumeMounts[2]
			require.Equal(t, "vol-tmp", mount.Name, c.Name)
			require.Equal(t, "/home/operator/.tmp", mount.MountPath, c.Name)

			mount = c.VolumeMounts[3]
			require.Equal(t, "vol-config", mount.Name, c.Name)
			require.Equal(t, "/home/operator/.config", mount.MountPath, c.Name)
		}
	})

	t.Run("start container command", func(t *testing.T) {
		const defaultHome = "/home/operator/cosmos"

		cmdCrd := defaultCRD()
		cmdCrd.Spec.ChainSpec.Binary = "gaiad"
		cmdCrd.Spec.PodTemplate.Image = "ghcr.io/cosmoshub:v1.2.3"

		pod, err := NewPodBuilder(&cmdCrd).WithOrdinal(1).Build()
		require.NoError(t, err)
		c := pod.Spec.Containers[0]

		require.Equal(t, "ghcr.io/cosmoshub:v1.2.3", c.Image)

		require.Equal(t, []string{"gaiad"}, c.Command)
		require.Equal(t, []string{"start", "--home", defaultHome}, c.Args)

		cmdCrd.Spec.ChainSpec.SkipInvariants = true
		pod, err = NewPodBuilder(&cmdCrd).WithOrdinal(1).Build()
		require.NoError(t, err)
		c = pod.Spec.Containers[0]

		require.Equal(t, []string{"gaiad"}, c.Command)
		require.Equal(t, []string{"start", "--home", defaultHome, "--x-crisis-skip-assert-invariants"}, c.Args)

		cmdCrd.Spec.ChainSpec.LogLevel = ptr("debug")
		cmdCrd.Spec.ChainSpec.LogFormat = ptr("json")
		pod, err = NewPodBuilder(&cmdCrd).WithOrdinal(1).Build()
		require.NoError(t, err)
		c = pod.Spec.Containers[0]

		require.Equal(t, []string{"start", "--home", defaultHome, "--x-crisis-skip-assert-invariants", "--log_level", "debug", "--log_format", "json"}, c.Args)

		cmdCrd.Spec.ChainSpec.HomeDir = ".other"
		pod, err = NewPodBuilder(&cmdCrd).WithOrdinal(1).Build()
		require.NoError(t, err)

		c = pod.Spec.Containers[0]
		require.Equal(t, []string{"start", "--home", "/home/operator/.other", "--x-crisis-skip-assert-invariants", "--log_level", "debug", "--log_format", "json"}, c.Args)
	})

	t.Run("sentry start container command ", func(t *testing.T) {
		cmdCrd := defaultCRD()
		cmdCrd.Spec.ChainSpec.Binary = "gaiad"
		cmdCrd.Spec.Type = cosmosv1.Sentry

		pod, err := NewPodBuilder(&cmdCrd).WithOrdinal(1).Build()
		require.NoError(t, err)
		c := pod.Spec.Containers[0]

		require.Equal(t, []string{"sh"}, c.Command)
		const wantBody1 = `sleep 10
gaiad start --home /home/operator/cosmos`
		require.Equal(t, []string{"-c", wantBody1}, c.Args)

		cmdCrd.Spec.ChainSpec.PrivvalSleepSeconds = ptr(int32(60))
		pod, err = NewPodBuilder(&cmdCrd).WithOrdinal(1).Build()
		require.NoError(t, err)
		c = pod.Spec.Containers[0]

		const wantBody2 = `sleep 60
gaiad start --home /home/operator/cosmos`
		require.Equal(t, []string{"-c", wantBody2}, c.Args)

		cmdCrd.Spec.ChainSpec.PrivvalSleepSeconds = ptr(int32(0))
		pod, err = NewPodBuilder(&cmdCrd).WithOrdinal(1).Build()
		require.NoError(t, err)
		c = pod.Spec.Containers[0]

		require.Equal(t, []string{"gaiad"}, c.Command)
	})

	t.Run("rpc probes", func(t *testing.T) {
		crd := defaultCRD()
		builder := NewPodBuilder(&crd)
		pod, err := builder.WithOrdinal(1).Build()
		require.NoError(t, err)

		want := &corev1.Probe{
			ProbeHandler: corev1.ProbeHandler{
				HTTPGet: &corev1.HTTPGetAction{
					Path:   "/health",
					Port:   intstr.FromInt(26657),
					Scheme: "HTTP",
				},
			},
			InitialDelaySeconds: 1,
			TimeoutSeconds:      10,
			PeriodSeconds:       10,
			SuccessThreshold:    1,
			FailureThreshold:    5,
		}
		got := pod.Spec.Containers[0].ReadinessProbe

		require.Equal(t, want, got)

		want = &corev1.Probe{
			ProbeHandler: corev1.ProbeHandler{
				HTTPGet: &corev1.HTTPGetAction{
					Path:   "/",
					Port:   intstr.FromInt(1251),
					Scheme: "HTTP",
				},
			},
			InitialDelaySeconds: 1,
			TimeoutSeconds:      10,
			PeriodSeconds:       10,
			SuccessThreshold:    1,
			FailureThreshold:    3,
		}
		got = pod.Spec.Containers[1].ReadinessProbe

		require.Equal(t, want, got)
	})

	t.Run("probe strategy", func(t *testing.T) {
		crd := defaultCRD()
		crd.Spec.PodTemplate.Probes = cosmosv1.FullNodeProbesSpec{Strategy: cosmosv1.FullNodeProbeStrategyNone}

		builder := NewPodBuilder(&crd)
		pod, err := builder.WithOrdinal(1).Build()
		require.NoError(t, err)

		for i, cont := range pod.Spec.Containers {
			require.Nilf(t, cont.ReadinessProbe, "container %d", i)
		}

		require.Equal(t, 2, len(pod.Spec.Containers))
		require.Equal(t, "node", pod.Spec.Containers[0].Name)

		sidecar := pod.Spec.Containers[1]
		require.Equal(t, "healthcheck", sidecar.Name)
		require.Nil(t, sidecar.ReadinessProbe)
	})

	t.Run("strategic merge fields", func(t *testing.T) {
		crd := defaultCRD()
		crd.Spec.PodTemplate.Volumes = []corev1.Volume{
			{Name: "foo-vol", VolumeSource: corev1.VolumeSource{EmptyDir: &corev1.EmptyDirVolumeSource{}}},
		}
		crd.Spec.PodTemplate.InitContainers = []corev1.Container{
			{Name: "chain-init", Image: "foo:latest", VolumeMounts: []corev1.VolumeMount{
				{Name: "foo-vol", MountPath: "/foo"}, // Should be merged with existing.
			}},
			{Name: "new-init", Image: "new-init:latest"}, // New container.
		}
		crd.Spec.PodTemplate.Containers = []corev1.Container{
			{Name: "node", VolumeMounts: []corev1.VolumeMount{
				{Name: "foo-vol", MountPath: "/foo"}, // Should be merged with existing.
			}},
			{Name: "new-sidecar", Image: "new-sidecar:latest"}, // New container.
		}

		builder := NewPodBuilder(&crd)
		pod, err := builder.WithOrdinal(0).Build()
		require.NoError(t, err)

		vols := lo.SliceToMap(pod.Spec.Volumes, func(v corev1.Volume) (string, corev1.Volume) { return v.Name, v })
		require.ElementsMatch(t, []string{"foo-vol", "vol-tmp", "vol-system-tmp", "vol-config", "vol-chain-home", "vol-node-key"}, lo.Keys(vols))
		require.Equal(t, &corev1.EmptyDirVolumeSource{}, vols["foo-vol"].VolumeSource.EmptyDir)

		containers := lo.SliceToMap(pod.Spec.Containers, func(c corev1.Container) (string, corev1.Container) { return c.Name, c })
		require.ElementsMatch(t, []string{"node", "new-sidecar", "healthcheck"}, lo.Keys(containers))

		extraVol := lo.Filter(containers["node"].VolumeMounts, func(vm corev1.VolumeMount, _ int) bool { return vm.Name == "foo-vol" })
		require.Equal(t, "/foo", extraVol[0].MountPath)

		initConts := lo.SliceToMap(pod.Spec.InitContainers, func(c corev1.Container) (string, corev1.Container) { return c.Name, c })
		require.ElementsMatch(t, []string{"clean-init", "chain-init", "new-init", "genesis-init", "addrbook-init", "config-merge", "version-check"}, lo.Keys(initConts))
		require.Equal(t, "foo:latest", initConts["chain-init"].Image)
	})

	t.Run("containers with chain spec versions", func(t *testing.T) {
		crd := defaultCRD()
		crd.Spec.PodTemplate.Volumes = []corev1.Volume{
			{Name: "foo-vol", VolumeSource: corev1.VolumeSource{EmptyDir: &corev1.EmptyDirVolumeSource{}}},
		}
		crd.Spec.PodTemplate.InitContainers = []corev1.Container{
			{Name: "chain-init", Image: "foo:latest", VolumeMounts: []corev1.VolumeMount{
				{Name: "foo-vol", MountPath: "/foo"}, // Should be merged with existing.
			}},
			{Name: "new-init", Image: "new-init:latest"}, // New container.
		}
		crd.Spec.PodTemplate.Containers = []corev1.Container{
			{Name: "node", VolumeMounts: []corev1.VolumeMount{
				{Name: "foo-vol", MountPath: "/foo"}, // Should be merged with existing.
			}},
			{Name: "new-sidecar", Image: "new-sidecar:latest"}, // New container.
		}
		crd.Spec.ChainSpec.Versions = []cosmosv1.ChainVersion{
			{
				UpgradeHeight: 1,
				Image:         "image:v1.0.0",
			},
			{
				UpgradeHeight: 100,
				Image:         "image:v2.0.0",
			},
		}

		builder := NewPodBuilder(&crd)
		pod, err := builder.WithOrdinal(0).Build()
		require.NoError(t, err)

		containers := lo.SliceToMap(pod.Spec.Containers, func(c corev1.Container) (string, corev1.Container) { return c.Name, c })
		require.ElementsMatch(t, []string{"node", "new-sidecar", "healthcheck", "version-check-interval"}, lo.Keys(containers))
	})

	test.HasTypeLabel(t, func(crd cosmosv1.CosmosFullNode) []map[string]string {
		builder := NewPodBuilder(&crd)
		pod, _ := builder.WithOrdinal(5).Build()
		return []map[string]string{pod.Labels}
	})
}

func TestChainHomeDir(t *testing.T) {
	crd := defaultCRD()
	require.Equal(t, "/home/operator/cosmos", ChainHomeDir(&crd))

	crd.Spec.ChainSpec.HomeDir = ".gaia"
	require.Equal(t, "/home/operator/.gaia", ChainHomeDir(&crd))
}

func TestPVCName(t *testing.T) {
	crd := defaultCRD()
	builder := NewPodBuilder(&crd)
	pod, err := builder.WithOrdinal(5).Build()
	require.NoError(t, err)

	require.Equal(t, "pvc-osmosis-5", PVCName(pod))

	pod.Spec.Volumes = append([]corev1.Volume{{Name: "foo"}}, pod.Spec.Volumes...)

	require.Equal(t, "pvc-osmosis-5", PVCName(pod))
}

'''
'''--- internal/fullnode/pod_control.go ---
package fullnode

import (
	"context"
	"fmt"

	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	"github.com/strangelove-ventures/cosmos-operator/internal/diff"
	"github.com/strangelove-ventures/cosmos-operator/internal/kube"
	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/util/intstr"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

// Client is a controller client. It is a subset of client.Client.
type Client interface {
	client.Reader
	client.Writer

	Scheme() *runtime.Scheme
}

type CacheInvalidator interface {
	Invalidate(controller client.ObjectKey, pods []string)
}

// PodControl reconciles pods for a CosmosFullNode.
type PodControl struct {
	client           Client
	cacheInvalidator CacheInvalidator
	computeRollout   func(maxUnavail *intstr.IntOrString, desired, ready int) int
}

// NewPodControl returns a valid PodControl.
func NewPodControl(client Client, cacheInvalidator CacheInvalidator) PodControl {
	return PodControl{
		client:           client,
		cacheInvalidator: cacheInvalidator,
		computeRollout:   kube.ComputeRollout,
	}
}

// Reconcile is the control loop for pods. The bool return value, if true, indicates the controller should requeue
// the request.
func (pc PodControl) Reconcile(
	ctx context.Context,
	reporter kube.Reporter,
	crd *cosmosv1.CosmosFullNode,
	cksums ConfigChecksums,
	syncInfo map[string]*cosmosv1.SyncInfoPodStatus,
) (bool, kube.ReconcileError) {
	var pods corev1.PodList
	if err := pc.client.List(ctx, &pods,
		client.InNamespace(crd.Namespace),
		client.MatchingFields{kube.ControllerOwnerField: crd.Name},
	); err != nil {
		return false, kube.TransientError(fmt.Errorf("list existing pods: %w", err))
	}

	wantPods, err := BuildPods(crd, cksums)
	if err != nil {
		return false, kube.UnrecoverableError(fmt.Errorf("build pods: %w", err))
	}
	diffed := diff.New(ptrSlice(pods.Items), wantPods)

	for _, pod := range diffed.Creates() {
		reporter.Info("Creating pod", "name", pod.Name)
		if err := ctrl.SetControllerReference(crd, pod, pc.client.Scheme()); err != nil {
			return true, kube.TransientError(fmt.Errorf("set controller reference on pod %q: %w", pod.Name, err))
		}
		if err := pc.client.Create(ctx, pod); kube.IgnoreAlreadyExists(err) != nil {
			return true, kube.TransientError(fmt.Errorf("create pod %q: %w", pod.Name, err))
		}
	}

	var invalidateCache []string

	defer func() {
		if pc.cacheInvalidator == nil {
			return
		}
		if len(invalidateCache) > 0 {
			pc.cacheInvalidator.Invalidate(client.ObjectKeyFromObject(crd), invalidateCache)
		}
	}()

	for _, pod := range diffed.Deletes() {
		reporter.Info("Deleting pod", "name", pod.Name)
		if err := pc.client.Delete(ctx, pod, client.PropagationPolicy(metav1.DeletePropagationForeground)); kube.IgnoreNotFound(err) != nil {
			return true, kube.TransientError(fmt.Errorf("delete pod %q: %w", pod.Name, err))
		}
		delete(syncInfo, pod.Name)
		invalidateCache = append(invalidateCache, pod.Name)
	}

	if len(diffed.Creates())+len(diffed.Deletes()) > 0 {
		// Scaling happens first; then updates. So requeue to handle updates after scaling finished.
		return true, nil
	}

	diffedUpdates := diffed.Updates()
	if len(diffedUpdates) > 0 {
		var (
			updatedPods      = 0
			rpcReachablePods = 0
			inSyncPods       = 0
			otherUpdates     = []*corev1.Pod{}
		)

		for _, existing := range pods.Items {
			podName := existing.Name

			if existing.DeletionTimestamp != nil {
				// Pod is being deleted, so we skip it.
				continue
			}

			var rpcReachable bool
			if ps, ok := syncInfo[podName]; ok {
				if ps.InSync != nil && *ps.InSync {
					inSyncPods++
				}
				rpcReachable = ps.Error == nil
				if rpcReachable {
					rpcReachablePods++
				}
			}
			for _, update := range diffedUpdates {
				if podName == update.Name {
					if existing.Spec.Containers[0].Image != update.Spec.Containers[0].Image {
						// awaiting upgrade
						if !rpcReachable {
							updatedPods++
							reporter.Info("Deleting pod for version upgrade", "name", podName)
							// Because we should watch for deletes, we get a re-queued request, detect pod is missing, and re-create it.
							if err := pc.client.Delete(ctx, update, client.PropagationPolicy(metav1.DeletePropagationForeground)); client.IgnoreNotFound(err) != nil {
								return true, kube.TransientError(fmt.Errorf("upgrade pod version %q: %w", podName, err))
							}
							syncInfo[podName].InSync = nil
							syncInfo[podName].Error = ptr("version upgrade in progress")
							invalidateCache = append(invalidateCache, podName)
						} else {
							otherUpdates = append(otherUpdates, update)
						}
					} else {
						otherUpdates = append(otherUpdates, update)
					}
					break
				}
			}
		}

		// If we don't have any pods in sync, we are down anyways, so we can use the number of RPC reachable pods for computing the rollout,
		// with the goal of recovering the pods as quickly as possible.
		ready := inSyncPods
		if ready == 0 {
			ready = rpcReachablePods
		}

		numUpdates := pc.computeRollout(crd.Spec.RolloutStrategy.MaxUnavailable, int(crd.Spec.Replicas), ready)

		if updatedPods == len(diffedUpdates) {
			// All pods are updated.
			return false, nil
		}

		if updatedPods >= numUpdates {
			// Signal requeue.
			return true, nil
		}

		for _, pod := range otherUpdates {
			podName := pod.Name
			reporter.Info("Deleting pod for update", "name", podName)
			// Because we should watch for deletes, we get a re-queued request, detect pod is missing, and re-create it.
			if err := pc.client.Delete(ctx, pod, client.PropagationPolicy(metav1.DeletePropagationForeground)); client.IgnoreNotFound(err) != nil {
				return true, kube.TransientError(fmt.Errorf("update pod %q: %w", podName, err))
			}
			syncInfo[podName].InSync = nil
			syncInfo[podName].Error = ptr("update in progress")
			invalidateCache = append(invalidateCache, podName)
			updatedPods++
			if updatedPods >= numUpdates {
				// done for this round
				break
			}
		}

		if len(diffedUpdates) == updatedPods {
			// All pods are updated.
			return false, nil
		}

		// Signal requeue.
		return true, nil
	}

	// Finished, pod state matches CRD.
	return false, nil
}

'''
'''--- internal/fullnode/pod_control_test.go ---
package fullnode

import (
	"context"
	"fmt"
	"testing"

	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	"github.com/strangelove-ventures/cosmos-operator/internal/diff"
	"github.com/strangelove-ventures/cosmos-operator/internal/kube"
	"github.com/stretchr/testify/require"
	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/util/intstr"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

type mockPodClient struct{ mockClient[*corev1.Pod] }

func newMockPodClient(pods []*corev1.Pod) *mockPodClient {
	return &mockPodClient{
		mockClient: mockClient[*corev1.Pod]{
			ObjectList: corev1.PodList{
				Items: valueSlice(pods),
			},
		},
	}
}

func (c *mockPodClient) setPods(pods []*corev1.Pod) {
	c.ObjectList = corev1.PodList{
		Items: valueSlice(pods),
	}
}

func (c *mockPodClient) upgradePods(
	t *testing.T,
	crdName string,
	ordinals ...int,
) {
	existing := ptrSlice(c.ObjectList.(corev1.PodList).Items)
	for _, ordinal := range ordinals {
		updatePod(t, crdName, ordinal, existing, newPodWithNewImage, true)
	}
	c.setPods(existing)
}

func (c *mockPodClient) deletePods(
	t *testing.T,
	crdName string,
	ordinals ...int,
) {
	existing := ptrSlice(c.ObjectList.(corev1.PodList).Items)
	for _, ordinal := range ordinals {
		updatePod(t, crdName, ordinal, existing, deletedPod, false)
	}
	c.setPods(existing)
}

func TestPodControl_Reconcile(t *testing.T) {
	t.Parallel()

	ctx := context.Background()
	const namespace = "test"

	t.Run("no changes", func(t *testing.T) {
		crd := defaultCRD()
		crd.Name = "hub"
		crd.Namespace = namespace
		crd.Spec.Replicas = 1

		pods, err := BuildPods(&crd, nil)
		require.NoError(t, err)
		existing := diff.New(nil, pods).Creates()

		require.Len(t, existing, 1)

		mClient := newMockPodClient(existing)

		syncInfo := map[string]*cosmosv1.SyncInfoPodStatus{
			"hub-0": {InSync: ptr(true)},
		}

		control := NewPodControl(mClient, nil)
		requeue, err := control.Reconcile(ctx, nopReporter, &crd, nil, syncInfo)
		require.NoError(t, err)
		require.False(t, requeue)

		require.Len(t, mClient.GotListOpts, 2)
		var listOpt client.ListOptions
		for _, opt := range mClient.GotListOpts {
			opt.ApplyToList(&listOpt)
		}
		require.Equal(t, namespace, listOpt.Namespace)
		require.Zero(t, listOpt.Limit)
		require.Equal(t, ".metadata.controller=hub", listOpt.FieldSelector.String())
	})

	t.Run("scale phase", func(t *testing.T) {
		crd := defaultCRD()
		crd.Name = "hub"
		crd.Namespace = namespace
		crd.Spec.Replicas = 3

		mClient := newMockPodClient([]*corev1.Pod{
			{ObjectMeta: metav1.ObjectMeta{Name: "hub-98"}},
			{ObjectMeta: metav1.ObjectMeta{Name: "hub-99"}},
		})

		control := NewPodControl(mClient, nil)
		requeue, err := control.Reconcile(ctx, nopReporter, &crd, nil, nil)
		require.NoError(t, err)
		require.True(t, requeue)

		require.Equal(t, 3, mClient.CreateCount)
		require.Equal(t, 2, mClient.DeleteCount)

		require.NotEmpty(t, mClient.LastCreateObject.OwnerReferences)
		require.Equal(t, crd.Name, mClient.LastCreateObject.OwnerReferences[0].Name)
		require.Equal(t, "CosmosFullNode", mClient.LastCreateObject.OwnerReferences[0].Kind)
		require.True(t, *mClient.LastCreateObject.OwnerReferences[0].Controller)
	})

	t.Run("rollout phase", func(t *testing.T) {
		crd := defaultCRD()
		crd.Name = "hub"
		crd.Namespace = namespace
		crd.Spec.Replicas = 5
		crd.Spec.RolloutStrategy = cosmosv1.RolloutStrategy{
			MaxUnavailable: ptr(intstr.FromInt(2)),
		}

		pods, err := BuildPods(&crd, nil)
		require.NoError(t, err)

		mClient := newMockPodClient(diff.New(nil, pods).Creates())

		syncInfo := map[string]*cosmosv1.SyncInfoPodStatus{
			"hub-0": {InSync: ptr(true)},
			"hub-1": {InSync: ptr(true)},
			"hub-2": {InSync: ptr(true)},
			"hub-3": {InSync: ptr(true)},
			"hub-4": {InSync: ptr(true)},
		}

		control := NewPodControl(mClient, nil)

		control.computeRollout = func(maxUnavail *intstr.IntOrString, desired, ready int) int {
			require.EqualValues(t, crd.Spec.Replicas, desired)
			require.Equal(t, 5, ready) // mockPodFilter only returns 1 candidate as ready
			return kube.ComputeRollout(maxUnavail, desired, ready)
		}

		// Trigger updates
		crd.Spec.PodTemplate.Image = "new-image"
		requeue, err := control.Reconcile(ctx, nopReporter, &crd, nil, syncInfo)
		require.NoError(t, err)
		require.True(t, requeue)

		require.Zero(t, mClient.CreateCount)

		mClient.deletePods(t, crd.Name, 0, 1)

		control.computeRollout = func(maxUnavail *intstr.IntOrString, desired, ready int) int {
			require.EqualValues(t, crd.Spec.Replicas, desired)
			require.Equal(t, 3, ready) // only 3 should be marked ready because 2 are in the deleting state.
			return kube.ComputeRollout(maxUnavail, desired, ready)
		}

		requeue, err = control.Reconcile(ctx, nopReporter, &crd, nil, syncInfo)
		require.NoError(t, err)

		require.True(t, requeue)

		// pod status has not changed, but 0 and 1 are now in deleting state.
		// should not delete any more.
		require.Equal(t, 2, mClient.DeleteCount)

		// once pod deletion is complete, new pods are created with new image.
		mClient.upgradePods(t, crd.Name, 0, 1)

		syncInfo["hub-0"].InSync = nil
		syncInfo["hub-0"].Error = ptr("upgrade in progress")

		syncInfo["hub-1"].InSync = nil
		syncInfo["hub-1"].Error = ptr("upgrade in progress")

		control.computeRollout = func(maxUnavail *intstr.IntOrString, desired, ready int) int {
			require.EqualValues(t, crd.Spec.Replicas, desired)
			require.Equal(t, 3, ready)
			return kube.ComputeRollout(maxUnavail, desired, ready)
		}

		requeue, err = control.Reconcile(ctx, nopReporter, &crd, nil, syncInfo)
		require.NoError(t, err)
		require.True(t, requeue)

		require.Zero(t, mClient.CreateCount)

		// should not delete any more yet.
		require.Equal(t, 2, mClient.DeleteCount)
	})

	t.Run("rollout version upgrade rolling", func(t *testing.T) {
		crd := defaultCRD()
		crd.Name = "hub"
		crd.Namespace = namespace
		crd.Spec.Replicas = 5
		crd.Spec.RolloutStrategy = cosmosv1.RolloutStrategy{
			MaxUnavailable: ptr(intstr.FromInt(2)),
		}
		crd.Spec.ChainSpec = cosmosv1.ChainSpec{
			Versions: []cosmosv1.ChainVersion{
				{
					Image: "image",
				},
				{
					UpgradeHeight: 100,
					Image:         "new-image",
				},
			},
		}
		crd.Status.Height = make(map[string]uint64)

		pods, err := BuildPods(&crd, nil)
		require.NoError(t, err)
		existing := diff.New(nil, pods).Creates()

		mClient := newMockPodClient(existing)

		// pods are at upgrade height and reachable
		syncInfo := map[string]*cosmosv1.SyncInfoPodStatus{
			"hub-0": {
				Height: ptr(uint64(100)),
				InSync: ptr(true),
			},
			"hub-1": {
				Height: ptr(uint64(100)),
				InSync: ptr(true),
			},
			"hub-2": {
				Height: ptr(uint64(100)),
				InSync: ptr(true),
			},
			"hub-3": {
				Height: ptr(uint64(100)),
				InSync: ptr(true),
			},
			"hub-4": {
				Height: ptr(uint64(100)),
				InSync: ptr(true),
			},
		}

		control := NewPodControl(mClient, nil)

		control.computeRollout = func(maxUnavail *intstr.IntOrString, desired, ready int) int {
			require.EqualValues(t, crd.Spec.Replicas, desired)
			require.Equal(t, 5, ready) // all are reachable and reporting ready, so we will maintain liveliness.
			return kube.ComputeRollout(maxUnavail, desired, ready)
		}

		// Trigger updates
		for _, pod := range existing {
			crd.Status.Height[pod.Name] = 100
		}

		// Reconcile 1, should update 0 and 1

		requeue, err := control.Reconcile(ctx, nopReporter, &crd, nil, syncInfo)
		require.NoError(t, err)

		// only handled 2 updates, so should requeue.
		require.True(t, requeue)

		require.Zero(t, mClient.CreateCount)
		require.Equal(t, 2, mClient.DeleteCount)

		mClient.deletePods(t, crd.Name, 0, 1)

		control.computeRollout = func(maxUnavail *intstr.IntOrString, desired, ready int) int {
			require.EqualValues(t, crd.Spec.Replicas, desired)
			require.Equal(t, 3, ready) // only 3 should be marked ready because 2 are in the deleting state.
			return kube.ComputeRollout(maxUnavail, desired, ready)
		}

		requeue, err = control.Reconcile(ctx, nopReporter, &crd, nil, syncInfo)
		require.NoError(t, err)

		require.True(t, requeue)

		// pod status has not changed, but 0 and 1 are now in deleting state.
		// should not delete any more.
		require.Equal(t, 2, mClient.DeleteCount)

		mClient.upgradePods(t, crd.Name, 0, 1)

		// 0 and 1 are now unavailable, working on upgrade
		syncInfo["hub-0"].InSync = nil
		syncInfo["hub-0"].Error = ptr("upgrade in progress")

		syncInfo["hub-1"].InSync = nil
		syncInfo["hub-1"].Error = ptr("upgrade in progress")

		// Reconcile 2, should not update anything because 0 and 1 are still in progress.

		control.computeRollout = func(maxUnavail *intstr.IntOrString, desired, ready int) int {
			require.EqualValues(t, crd.Spec.Replicas, desired)
			require.Equal(t, 3, ready)
			return kube.ComputeRollout(maxUnavail, desired, ready)
		}

		requeue, err = control.Reconcile(ctx, nopReporter, &crd, nil, syncInfo)
		require.NoError(t, err)

		// no further updates yet, should requeue.
		require.True(t, requeue)

		require.Zero(t, mClient.CreateCount)

		// should not delete any more yet.
		require.Equal(t, 2, mClient.DeleteCount)

		// mock out that one of the pods completed the upgrade. should begin upgrading one more
		syncInfo["hub-0"].InSync = ptr(true)
		syncInfo["hub-0"].Height = ptr(uint64(101))
		syncInfo["hub-0"].Error = nil

		// Reconcile 3, should update pod 2 (only one) because 1 is still in progress, but 0 is done.

		control.computeRollout = func(maxUnavail *intstr.IntOrString, desired, ready int) int {
			require.EqualValues(t, crd.Spec.Replicas, desired)
			require.Equal(t, 4, ready)
			return kube.ComputeRollout(maxUnavail, desired, ready)
		}

		requeue, err = control.Reconcile(ctx, nopReporter, &crd, nil, syncInfo)
		require.NoError(t, err)

		// only handled 1 updates, so should requeue.
		require.True(t, requeue)

		require.Zero(t, mClient.CreateCount)

		// should delete one more
		require.Equal(t, 3, mClient.DeleteCount)

		mClient.deletePods(t, crd.Name, 2)

		control.computeRollout = func(maxUnavail *intstr.IntOrString, desired, ready int) int {
			require.EqualValues(t, crd.Spec.Replicas, desired)
			require.Equal(t, 3, ready) // only 3 should be marked ready because 2 is in the deleting state and 1 is still in progress upgrading.
			return kube.ComputeRollout(maxUnavail, desired, ready)
		}

		requeue, err = control.Reconcile(ctx, nopReporter, &crd, nil, syncInfo)
		require.NoError(t, err)

		require.True(t, requeue)

		// pod status has not changed, but 2 is now in deleting state.
		// should not delete any more.
		require.Equal(t, 3, mClient.DeleteCount)

		mClient.upgradePods(t, crd.Name, 2)

		// mock out that both pods completed the upgrade. should begin upgrading the last 2
		syncInfo["hub-1"].InSync = ptr(true)
		syncInfo["hub-1"].Height = ptr(uint64(101))
		syncInfo["hub-1"].Error = nil

		syncInfo["hub-2"].InSync = ptr(true)
		syncInfo["hub-2"].Height = ptr(uint64(101))
		syncInfo["hub-2"].Error = nil

		// Reconcile 4, should update 3 and 4 because the rest are done.

		control.computeRollout = func(maxUnavail *intstr.IntOrString, desired, ready int) int {
			require.EqualValues(t, crd.Spec.Replicas, desired)
			require.Equal(t, 5, ready)
			return kube.ComputeRollout(maxUnavail, desired, ready)
		}

		requeue, err = control.Reconcile(ctx, nopReporter, &crd, nil, syncInfo)
		require.NoError(t, err)

		// all updates are now handled, no longer need requeue.
		require.False(t, requeue)

		require.Zero(t, mClient.CreateCount)

		// should delete the last 2
		require.Equal(t, 5, mClient.DeleteCount)
	})

	t.Run("rollout version upgrade halt", func(t *testing.T) {
		crd := defaultCRD()
		crd.Name = "hub"
		crd.Namespace = namespace
		crd.Spec.Replicas = 5
		crd.Spec.RolloutStrategy = cosmosv1.RolloutStrategy{
			MaxUnavailable: ptr(intstr.FromInt(2)),
		}
		crd.Spec.ChainSpec = cosmosv1.ChainSpec{
			Versions: []cosmosv1.ChainVersion{
				{
					Image: "image",
				},
				{
					UpgradeHeight: 100,
					Image:         "new-image",
					SetHaltHeight: true,
				},
			},
		}
		crd.Status.Height = make(map[string]uint64)

		pods, err := BuildPods(&crd, nil)
		require.NoError(t, err)
		existing := diff.New(nil, pods).Creates()

		mClient := newMockPodClient(existing)

		// pods are at upgrade height and reachable
		syncInfo := map[string]*cosmosv1.SyncInfoPodStatus{
			"hub-0": {
				Height: ptr(uint64(100)),
				Error:  ptr("panic at upgrade height"),
			},
			"hub-1": {
				Height: ptr(uint64(100)),
				Error:  ptr("panic at upgrade height"),
			},
			"hub-2": {
				Height: ptr(uint64(100)),
				Error:  ptr("panic at upgrade height"),
			},
			"hub-3": {
				Height: ptr(uint64(100)),
				Error:  ptr("panic at upgrade height"),
			},
			"hub-4": {
				Height: ptr(uint64(100)),
				Error:  ptr("panic at upgrade height"),
			},
		}

		control := NewPodControl(mClient, nil)

		control.computeRollout = func(maxUnavail *intstr.IntOrString, desired, ready int) int {
			require.EqualValues(t, crd.Spec.Replicas, desired)
			require.Equal(t, 0, ready) // mockPodFilter returns no pods as synced, but all are at the upgrade height.
			return kube.ComputeRollout(maxUnavail, desired, ready)
		}

		// Trigger updates
		for _, pod := range existing {
			crd.Status.Height[pod.Name] = 100
		}

		requeue, err := control.Reconcile(ctx, nopReporter, &crd, nil, syncInfo)
		require.NoError(t, err)

		// all updates are handled, so should not requeue
		require.False(t, requeue)

		require.Zero(t, mClient.CreateCount)
		require.Equal(t, 5, mClient.DeleteCount)
	})
}

// revision hash must be taken without the revision label and the ordinal annotation.
func recalculatePodRevision(pod *corev1.Pod, ordinal int) {
	delete(pod.Labels, "app.kubernetes.io/revision")
	delete(pod.Annotations, "app.kubernetes.io/ordinal")
	rev1 := diff.Adapt(pod, ordinal).Revision()
	pod.Labels["app.kubernetes.io/revision"] = rev1
	pod.Annotations["app.kubernetes.io/ordinal"] = fmt.Sprintf("%d", ordinal)
}

func newPodWithNewImage(pod *corev1.Pod) {
	pod.DeletionTimestamp = nil
	pod.Spec.Containers[0].Image = "new-image"
	pod.Spec.InitContainers[1].Image = "new-image"
}

func deletedPod(pod *corev1.Pod) {
	pod.DeletionTimestamp = ptr(metav1.Now())
}

func updatePod(t *testing.T, crdName string, ordinal int, pods []*corev1.Pod, updateFn func(pod *corev1.Pod), recalc bool) {
	podName := fmt.Sprintf("%s-%d", crdName, ordinal)
	for _, pod := range pods {
		if pod.Name == podName {
			updateFn(pod)
			if recalc {
				recalculatePodRevision(pod, ordinal)
			}
			return
		}
	}

	require.FailNow(t, "pod not found", podName)
}

'''
'''--- internal/fullnode/ports.go ---
package fullnode

import (
	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	corev1 "k8s.io/api/core/v1"
)

const (
	apiPort     = 1317
	grpcPort    = 9090
	grpcWebPort = 9091
	p2pPort     = 26656
	privvalPort = 1234
	promPort    = 26660
	rosettaPort = 8080
	rpcPort     = 26657
	jsonRpcPort = 8545
	jsonRpcWsPort = 8546
)

func buildPorts(nodeType cosmosv1.FullNodeType) []corev1.ContainerPort {
	switch nodeType {
	case cosmosv1.Sentry:
		return append(defaultPorts[:], corev1.ContainerPort{
			Name:          "privval",
			ContainerPort: privvalPort,
			Protocol:      corev1.ProtocolTCP,
		})
	default:
		return defaultPorts[:]
	}
}

var defaultPorts = [...]corev1.ContainerPort{
	{
		Name:          "api",
		Protocol:      corev1.ProtocolTCP,
		ContainerPort: apiPort,
	},
	{
		Name:          "rosetta",
		Protocol:      corev1.ProtocolTCP,
		ContainerPort: rosettaPort,
	},
	{
		Name:          "grpc",
		Protocol:      corev1.ProtocolTCP,
		ContainerPort: grpcPort,
	},
	{
		Name:          "prometheus",
		Protocol:      corev1.ProtocolTCP,
		ContainerPort: promPort,
	},
	{
		Name:          "p2p",
		Protocol:      corev1.ProtocolTCP,
		ContainerPort: p2pPort,
	},
	{
		Name:          "rpc",
		Protocol:      corev1.ProtocolTCP,
		ContainerPort: rpcPort,
	},
	{
		Name:          "grpc-web",
		Protocol:      corev1.ProtocolTCP,
		ContainerPort: grpcWebPort,
	},
	{
                Name:          "json-rpc",
                Protocol:      corev1.ProtocolTCP,
                ContainerPort: jsonRpcPort,
        },
	{
                Name:          "json-rpc-ws",
                Protocol:      corev1.ProtocolTCP,
                ContainerPort: jsonRpcWsPort,
        },
}

'''
'''--- internal/fullnode/ptr.go ---
package fullnode

import "github.com/samber/lo"

// ptr returns the pointer for any type.
// In k8s, many specs require a pointer to a scalar.
func ptr[T any](v T) *T {
	return &v
}

func ptrSlice[T any](s []T) []*T {
	return lo.Map(s, func(element T, _ int) *T { return &element })
}

func valueSlice[T any](s []*T) []T {
	return lo.Map(s, func(element *T, _ int) T { return *element })
}

func valOrDefault[T any](v *T, defaultVal *T) *T {
	if v == nil {
		return defaultVal
	}
	return v
}

func sliceOrDefault[T any](slice []T, defaultSlice []T) []T {
	if len(slice) == 0 {
		return defaultSlice
	}
	return slice
}

'''
'''--- internal/fullnode/pvc_auto_scaler.go ---
package fullnode

import (
	"context"
	"errors"
	"math"
	"time"

	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	"k8s.io/apimachinery/pkg/api/resource"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/util/intstr"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

type StatusSyncer interface {
	SyncUpdate(ctx context.Context, key client.ObjectKey, update func(status *cosmosv1.FullNodeStatus)) error
}

type PVCAutoScaler struct {
	client StatusSyncer
	now    func() time.Time
}

func NewPVCAutoScaler(client StatusSyncer) *PVCAutoScaler {
	return &PVCAutoScaler{
		client: client,
		now:    time.Now,
	}
}

// SignalPVCResize patches the CosmosFullNode.status.selfHealing with the new calculated PVC size as a resource quantity.
// Assumes CosmosfullNode.spec.selfHealing.pvcAutoScaling is set or else this method may panic.
// The CosmosFullNode controller is responsible for increasing the PVC disk size.
//
// Returns true if the status was patched.
//
// Returns false and does not patch if:
// 1. The PVCs do not need resizing
// 2. The status already has >= calculated size.
// 3. The maximum size has been reached. It will patch up to the maximum size.
//
// Returns an error if patching unsuccessful.
func (scaler PVCAutoScaler) SignalPVCResize(ctx context.Context, crd *cosmosv1.CosmosFullNode, results []PVCDiskUsage) (bool, error) {
	var (
		spec    = crd.Spec.SelfHeal.PVCAutoScale
		trigger = int(spec.UsedSpacePercentage)
	)

	var joinedErr error

	status := crd.Status.SelfHealing.PVCAutoScale

	patches := make(map[string]*cosmosv1.PVCAutoScaleStatus)

	now := metav1.NewTime(scaler.now())

	for _, pvc := range results {
		if pvc.PercentUsed < trigger {
			// no need to expand
			continue
		}

		newSize, err := scaler.calcNextCapacity(pvc.Capacity, spec.IncreaseQuantity)
		if err != nil {
			joinedErr = errors.Join(joinedErr, err)
			continue
		}

		if status != nil {
			if pvcStatus, ok := status[pvc.Name]; ok && pvcStatus.RequestedSize.Value() == newSize.Value() {
				// already requested
				continue
			}
		}

		if max := spec.MaxSize; !max.IsZero() {
			if pvc.Capacity.Cmp(max) >= 0 {
				// already at max size
				continue
			}

			if newSize.Cmp(max) >= 0 {
				// Cap new size to the max size
				newSize = max
			}
		}

		patches[pvc.Name] = &cosmosv1.PVCAutoScaleStatus{
			RequestedSize: newSize,
			RequestedAt:   now,
		}
	}

	if len(patches) == 0 {
		return false, joinedErr
	}

	return true, errors.Join(joinedErr, scaler.client.SyncUpdate(ctx, client.ObjectKeyFromObject(crd), func(status *cosmosv1.FullNodeStatus) {
		if status.SelfHealing.PVCAutoScale == nil {
			status.SelfHealing.PVCAutoScale = patches
			return
		}
		for k, v := range patches {
			status.SelfHealing.PVCAutoScale[k] = v
		}
	}))
}

func (scaler PVCAutoScaler) calcNextCapacity(current resource.Quantity, increase string) (resource.Quantity, error) {
	var (
		merr     error
		quantity resource.Quantity
	)

	// Try to calc by percentage first
	v := intstr.FromString(increase)
	percent, err := intstr.GetScaledValueFromIntOrPercent(&v, 100, false)
	if err == nil {
		addtl := math.Round(float64(current.Value()) * (float64(percent) / 100.0))
		quantity = *resource.NewQuantity(current.Value()+int64(addtl), current.Format)
		return quantity, nil
	}

	merr = errors.Join(merr, err)

	// Then try to calc by resource quantity
	addtl, err := resource.ParseQuantity(increase)
	if err != nil {
		return quantity, errors.Join(merr, err)
	}

	return *resource.NewQuantity(current.Value()+addtl.Value(), current.Format), nil
}

'''
'''--- internal/fullnode/pvc_auto_scaler_test.go ---
package fullnode

import (
	"context"
	"errors"
	"math/rand"
	"testing"
	"time"

	"github.com/samber/lo"
	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	"github.com/stretchr/testify/require"
	"k8s.io/apimachinery/pkg/api/resource"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

type mockStatusSyncer func(ctx context.Context, key client.ObjectKey, update func(status *cosmosv1.FullNodeStatus)) error

func (fn mockStatusSyncer) SyncUpdate(ctx context.Context, key client.ObjectKey, update func(status *cosmosv1.FullNodeStatus)) error {
	if ctx == nil {
		panic("nil context")
	}
	return fn(ctx, key, update)
}

func TestPVCAutoScaler_SignalPVCResize(t *testing.T) {
	t.Parallel()
	r := rand.New(rand.NewSource(time.Now().UnixNano()))

	ctx := context.Background()

	panicSyncer := mockStatusSyncer(func(ctx context.Context, key client.ObjectKey, update func(status *cosmosv1.FullNodeStatus)) error {
		panic("should not be called")
	})

	t.Run("happy path", func(t *testing.T) {
		var (
			capacity  = resource.MustParse("100Gi")
			stubNow   = time.Now()
			zeroQuant resource.Quantity
		)
		const (
			usedSpacePercentage = 80
			name                = "auto-scale-test"
			namespace           = "strangelove"
		)

		for _, tt := range []struct {
			Increase string
			Max      resource.Quantity
			Want     resource.Quantity
		}{
			{"20Gi", resource.MustParse("500Gi"), resource.MustParse("120Gi")},
			{"10%", zeroQuant, resource.MustParse("110Gi")},
			{"0.5Gi", zeroQuant, resource.MustParse("100.5Gi")},
			{"200%", zeroQuant, resource.MustParse("300Gi")},
			// Weird user input cases
			{"1", zeroQuant, *resource.NewQuantity(capacity.Value()+1, resource.BinarySI)},
		} {
			var crd cosmosv1.CosmosFullNode
			crd.APIVersion = "v1"
			crd.Name = name
			crd.Namespace = namespace
			crd.Spec.SelfHeal = &cosmosv1.SelfHealSpec{
				PVCAutoScale: &cosmosv1.PVCAutoScaleSpec{
					UsedSpacePercentage: usedSpacePercentage,
					IncreaseQuantity:    tt.Increase,
					MaxSize:             tt.Max,
				},
			}

			var patchCalled bool
			syncer := mockStatusSyncer(func(ctx context.Context, key client.ObjectKey, update func(status *cosmosv1.FullNodeStatus)) error {
				require.Equal(t, name, key.Name)
				require.Equal(t, namespace, key.Namespace)

				var got cosmosv1.FullNodeStatus
				update(&got)
				gotStatus := got.SelfHealing.PVCAutoScale
				require.Equal(t, stubNow, gotStatus["pvc-"+name+"-0"].RequestedAt.Time, tt)
				require.Truef(t, tt.Want.Equal(gotStatus["pvc-"+name+"-0"].RequestedSize), "%s:\nwant %+v\ngot  %+v", tt, tt.Want, gotStatus["pvc-"+name+"-0"].RequestedSize)

				patchCalled = true
				return nil
			})

			scaler := NewPVCAutoScaler(syncer)
			scaler.now = func() time.Time {
				return stubNow
			}

			trigger := 80 + r.Intn(20)
			usage := []PVCDiskUsage{
				{Name: "pvc-" + name + "-0", PercentUsed: trigger, Capacity: capacity},
				{Name: "pvc-" + name + "-1", PercentUsed: 10},
				{Name: "pvc-" + name + "-2", PercentUsed: 79},
			}
			got, err := scaler.SignalPVCResize(ctx, &crd, lo.Shuffle(usage))

			require.NoError(t, err, tt)
			require.True(t, got, tt)
			require.True(t, patchCalled, tt)
		}
	})

	t.Run("does not exceed max", func(t *testing.T) {
		var (
			capacity = resource.MustParse("100Ti")
			maxSize  = resource.MustParse("200Ti")
		)
		const usedSpacePercentage = 80

		var crd cosmosv1.CosmosFullNode
		name := "name"
		crd.Name = name
		crd.Spec.SelfHeal = &cosmosv1.SelfHealSpec{
			PVCAutoScale: &cosmosv1.PVCAutoScaleSpec{
				UsedSpacePercentage: usedSpacePercentage,
				IncreaseQuantity:    "300%",
				MaxSize:             maxSize,
			},
		}

		var patchCalled bool
		syncer := mockStatusSyncer(func(ctx context.Context, key client.ObjectKey, update func(status *cosmosv1.FullNodeStatus)) error {
			var got cosmosv1.FullNodeStatus
			update(&got)
			gotStatus := got.SelfHealing.PVCAutoScale
			require.Equal(t, maxSize.Value(), gotStatus["pvc-"+name+"-0"].RequestedSize.Value())
			require.Equal(t, maxSize.Format, gotStatus["pvc-"+name+"-0"].RequestedSize.Format)

			patchCalled = true
			return nil
		})
		scaler := NewPVCAutoScaler(syncer)

		usage := []PVCDiskUsage{
			{Name: "pvc-" + name + "-0", PercentUsed: 80, Capacity: capacity},
		}
		got, err := scaler.SignalPVCResize(ctx, &crd, lo.Shuffle(usage))

		require.NoError(t, err)
		require.True(t, got)
		require.True(t, patchCalled)
	})

	t.Run("capacity at or above max", func(t *testing.T) {
		for _, tt := range []struct {
			Max, Capacity resource.Quantity
		}{
			{resource.MustParse("5Ti"), resource.MustParse("5Ti")}, // the same
			{resource.MustParse("1G"), resource.MustParse("2G")},   // greater
		} {
			const usedSpacePercentage = 60

			var crd cosmosv1.CosmosFullNode
			name := "name"
			crd.Name = name
			crd.Spec.SelfHeal = &cosmosv1.SelfHealSpec{
				PVCAutoScale: &cosmosv1.PVCAutoScaleSpec{
					UsedSpacePercentage: usedSpacePercentage,
					IncreaseQuantity:    "10Gi",
					MaxSize:             tt.Max,
				},
			}

			scaler := NewPVCAutoScaler(panicSyncer)
			usage := []PVCDiskUsage{
				{Name: "pvc-" + name + "-0", PercentUsed: 80, Capacity: tt.Capacity},
			}
			got, err := scaler.SignalPVCResize(ctx, &crd, usage)

			require.NoError(t, err, tt)
			require.False(t, got, tt)
		}
	})

	t.Run("no patch needed", func(t *testing.T) {
		for _, tt := range []struct {
			DiskUsage []PVCDiskUsage
		}{
			{nil}, // tests zero state
			{[]PVCDiskUsage{
				{PercentUsed: 79},
				{PercentUsed: 1},
				{PercentUsed: 10},
			}},
		} {
			var crd cosmosv1.CosmosFullNode
			crd.Spec.SelfHeal = &cosmosv1.SelfHealSpec{
				PVCAutoScale: &cosmosv1.PVCAutoScaleSpec{
					UsedSpacePercentage: 80,
					IncreaseQuantity:    "10Gi",
				},
			}

			scaler := NewPVCAutoScaler(panicSyncer)
			got, err := scaler.SignalPVCResize(ctx, &crd, lo.Shuffle(tt.DiskUsage))

			require.NoError(t, err)
			require.False(t, got)
		}
	})

	t.Run("patch already signaled", func(t *testing.T) {
		const usedSpacePercentage = 90

		var crd cosmosv1.CosmosFullNode
		name := "name"
		crd.Name = name
		crd.Spec.SelfHeal = &cosmosv1.SelfHealSpec{
			PVCAutoScale: &cosmosv1.PVCAutoScaleSpec{
				UsedSpacePercentage: usedSpacePercentage,
				IncreaseQuantity:    "10Gi",
			},
		}
		crd.Status.SelfHealing.PVCAutoScale = map[string]*cosmosv1.PVCAutoScaleStatus{
			"pvc-" + name + "-0": {
				RequestedSize: resource.MustParse("100Gi"),
			},
		}

		scaler := NewPVCAutoScaler(panicSyncer)
		usage := []PVCDiskUsage{
			{Name: "pvc-" + name + "-0", PercentUsed: usedSpacePercentage, Capacity: resource.MustParse("90Gi")},
		}
		got, err := scaler.SignalPVCResize(ctx, &crd, usage)

		require.NoError(t, err)
		require.False(t, got)
	})

	t.Run("invalid increase quantity", func(t *testing.T) {
		const usedSpacePercentage = 80

		for _, tt := range []struct {
			Increase string
		}{
			{""}, // CRD validation should prevent this
			{"wut"},
		} {
			var crd cosmosv1.CosmosFullNode
			name := "name"
			crd.Name = name
			crd.Spec.SelfHeal = &cosmosv1.SelfHealSpec{
				PVCAutoScale: &cosmosv1.PVCAutoScaleSpec{
					UsedSpacePercentage: usedSpacePercentage,
					IncreaseQuantity:    tt.Increase,
				},
			}

			scaler := NewPVCAutoScaler(panicSyncer)
			usage := []PVCDiskUsage{
				{Name: "pvc-" + name + "-0", PercentUsed: usedSpacePercentage},
			}
			_, err := scaler.SignalPVCResize(ctx, &crd, lo.Shuffle(usage))

			require.Error(t, err)
			require.Contains(t, err.Error(), "invalid value for IntOrString: invalid type: string is not a percentage")
		}
	})

	t.Run("patch error", func(t *testing.T) {
		const usedSpacePercentage = 50

		var crd cosmosv1.CosmosFullNode
		crd.Spec.SelfHeal = &cosmosv1.SelfHealSpec{
			PVCAutoScale: &cosmosv1.PVCAutoScaleSpec{
				UsedSpacePercentage: usedSpacePercentage,
				IncreaseQuantity:    "10%",
			},
		}

		scaler := NewPVCAutoScaler(mockStatusSyncer(func(ctx context.Context, key client.ObjectKey, update func(status *cosmosv1.FullNodeStatus)) error {
			return errors.New("boom")
		}))
		usage := []PVCDiskUsage{
			{Name: "pvc-0", PercentUsed: usedSpacePercentage},
		}
		_, err := scaler.SignalPVCResize(ctx, &crd, lo.Shuffle(usage))

		require.Error(t, err)
		require.EqualError(t, err, "boom")
	})
}

'''
'''--- internal/fullnode/pvc_builder.go ---
package fullnode

import (
	"fmt"

	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	"github.com/strangelove-ventures/cosmos-operator/internal/diff"
	"github.com/strangelove-ventures/cosmos-operator/internal/kube"
	"gopkg.in/inf.v0"
	corev1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/api/resource"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

const (
	snapshotGrowthFactor = 102
)

var (
	defaultAccessModes = []corev1.PersistentVolumeAccessMode{corev1.ReadWriteOnce}
)

// BuildPVCs outputs desired PVCs given the crd.
func BuildPVCs(
	crd *cosmosv1.CosmosFullNode,
	dataSources map[int32]*dataSource,
	currentPVCs []*corev1.PersistentVolumeClaim,
) []diff.Resource[*corev1.PersistentVolumeClaim] {
	base := corev1.PersistentVolumeClaim{
		TypeMeta: metav1.TypeMeta{
			APIVersion: "v1",
			Kind:       "PersistentVolumeClaim",
		},
		ObjectMeta: metav1.ObjectMeta{
			Namespace:   crd.Namespace,
			Labels:      defaultLabels(crd),
			Annotations: make(map[string]string),
		},
	}

	var pvcs []diff.Resource[*corev1.PersistentVolumeClaim]
	for i := int32(0); i < crd.Spec.Replicas; i++ {
		if pvcDisabled(crd, i) {
			continue
		}

		pvc := base.DeepCopy()
		name := pvcName(crd, i)
		pvc.Name = name
		pvc.Labels[kube.InstanceLabel] = instanceName(crd, i)

		var dataSource *corev1.TypedLocalObjectReference
		var existingSize resource.Quantity
		if ds, ok := dataSources[i]; ok && ds != nil {
			dataSource = ds.ref
		} else {
			for _, pvc := range currentPVCs {
				if pvc.Name == name {
					if pvc.DeletionTimestamp == nil && pvc.Status.Phase == corev1.ClaimBound {
						existingSize = pvc.Status.Capacity[corev1.ResourceStorage]
					}
					break
				}
			}
		}

		tpl := crd.Spec.VolumeClaimTemplate
		if override, ok := crd.Spec.InstanceOverrides[instanceName(crd, i)]; ok {
			if overrideTpl := override.VolumeClaimTemplate; overrideTpl != nil {
				tpl = *overrideTpl
			}
		}

		pvc.Spec = corev1.PersistentVolumeClaimSpec{
			AccessModes:      sliceOrDefault(tpl.AccessModes, defaultAccessModes),
			Resources:        pvcResources(crd, name, dataSources[i], existingSize),
			StorageClassName: ptr(tpl.StorageClassName),
			VolumeMode:       valOrDefault(tpl.VolumeMode, ptr(corev1.PersistentVolumeFilesystem)),
		}

		preserveMergeInto(pvc.Labels, tpl.Metadata.Labels)
		preserveMergeInto(pvc.Annotations, tpl.Metadata.Annotations)
		kube.NormalizeMetadata(&pvc.ObjectMeta)

		pvcs = append(pvcs, diff.Adapt(pvc, i))
		pvc.Spec.DataSource = dataSource
	}
	return pvcs
}

func pvcDisabled(crd *cosmosv1.CosmosFullNode, ordinal int32) bool {
	name := instanceName(crd, ordinal)
	disable := crd.Spec.InstanceOverrides[name].DisableStrategy
	return disable != nil && *disable == cosmosv1.DisableAll
}

func pvcName(crd *cosmosv1.CosmosFullNode, ordinal int32) string {
	name := fmt.Sprintf("pvc-%s-%d", appName(crd), ordinal)
	return kube.ToName(name)
}

func pvcResources(
	crd *cosmosv1.CosmosFullNode,
	name string,
	dataSource *dataSource,
	existingSize resource.Quantity,
) corev1.ResourceRequirements {
	var reqs = crd.Spec.VolumeClaimTemplate.Resources.DeepCopy()

	if dataSource != nil {
		reqs.Requests[corev1.ResourceStorage] = dataSource.size
		return *reqs
	}

	if autoScale := crd.Status.SelfHealing.PVCAutoScale; autoScale != nil {
		if status, ok := autoScale[name]; ok {
			requestedSize := status.RequestedSize.DeepCopy()
			newSize := requestedSize.AsDec()
			sizeWithPadding := resource.NewDecimalQuantity(*newSize.Mul(newSize, inf.NewDec(snapshotGrowthFactor, 2)), resource.DecimalSI)
			if sizeWithPadding.Cmp(reqs.Requests[corev1.ResourceStorage]) > 0 {
				reqs.Requests[corev1.ResourceStorage] = *sizeWithPadding
			}
		}
	}

	if existingSize.Cmp(reqs.Requests[corev1.ResourceStorage]) > 0 {
		reqs.Requests[corev1.ResourceStorage] = existingSize
	}

	return *reqs
}

'''
'''--- internal/fullnode/pvc_builder_test.go ---
package fullnode

import (
	"fmt"
	"strings"
	"testing"

	"github.com/samber/lo"
	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	"github.com/strangelove-ventures/cosmos-operator/internal/diff"
	"github.com/strangelove-ventures/cosmos-operator/internal/kube"
	"github.com/strangelove-ventures/cosmos-operator/internal/test"
	"github.com/stretchr/testify/require"
	corev1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/api/resource"
)

func TestBuildPVCs(t *testing.T) {
	t.Parallel()

	t.Run("happy path", func(t *testing.T) {
		crd := defaultCRD()
		crd.Name = "juno"
		crd.Spec.Replicas = 3
		crd.Spec.VolumeClaimTemplate.StorageClassName = "test-storage-class"

		crd.Spec.InstanceOverrides = map[string]cosmosv1.InstanceOverridesSpec{
			"juno-0": {},
		}

		initial := BuildPVCs(&crd, map[int32]*dataSource{}, nil)
		for i, r := range initial {
			require.Equal(t, int64(i), r.Ordinal())
			require.NotEmpty(t, r.Revision())
		}

		initialPVCs := lo.Map(initial, func(r diff.Resource[*corev1.PersistentVolumeClaim], _ int) *corev1.PersistentVolumeClaim {
			return r.Object()
		})

		pvcs := lo.Map(BuildPVCs(&crd, map[int32]*dataSource{}, initialPVCs), func(r diff.Resource[*corev1.PersistentVolumeClaim], _ int) *corev1.PersistentVolumeClaim {
			return r.Object()
		})

		require.Len(t, pvcs, 3)

		gotNames := lo.Map(pvcs, func(pvc *corev1.PersistentVolumeClaim, _ int) string { return pvc.Name })
		require.Equal(t, []string{"pvc-juno-0", "pvc-juno-1", "pvc-juno-2"}, gotNames)

		for i, got := range pvcs {
			require.Equal(t, crd.Namespace, got.Namespace)
			require.Equal(t, "PersistentVolumeClaim", got.Kind)
			require.Equal(t, "v1", got.APIVersion)

			wantLabels := map[string]string{
				"app.kubernetes.io/created-by": "cosmos-operator",
				"app.kubernetes.io/component":  "CosmosFullNode",
				"app.kubernetes.io/name":       "juno",
				"app.kubernetes.io/instance":   fmt.Sprintf("juno-%d", i),
				"app.kubernetes.io/version":    "v1.2.3",
				"cosmos.strange.love/network":  "mainnet",
				"cosmos.strange.love/type":     "FullNode",
			}
			require.Equal(t, wantLabels, got.Labels)

			require.Len(t, got.Spec.AccessModes, 1)
			require.Equal(t, corev1.ReadWriteOnce, got.Spec.AccessModes[0])

			require.Equal(t, crd.Spec.VolumeClaimTemplate.Resources, got.Spec.Resources)
			require.Equal(t, "test-storage-class", *got.Spec.StorageClassName)
			require.Equal(t, corev1.PersistentVolumeFilesystem, *got.Spec.VolumeMode)
		}
	})

	t.Run("advanced configuration", func(t *testing.T) {
		crd := defaultCRD()
		crd.Spec.Replicas = 1
		crd.Spec.VolumeClaimTemplate.Metadata = cosmosv1.Metadata{
			Labels:      map[string]string{"label": "value", "app.kubernetes.io/created-by": "should not see me"},
			Annotations: map[string]string{"annot": "value"},
		}
		crd.Spec.VolumeClaimTemplate.AccessModes = []corev1.PersistentVolumeAccessMode{corev1.ReadWriteMany}
		crd.Spec.VolumeClaimTemplate.VolumeMode = ptr(corev1.PersistentVolumeBlock)
		crd.Spec.VolumeClaimTemplate.DataSource = &corev1.TypedLocalObjectReference{
			Kind: "TestKind",
			Name: "source-name",
		}

		pvcs := BuildPVCs(&crd, map[int32]*dataSource{
			0: {
				ref: crd.Spec.VolumeClaimTemplate.DataSource,
			},
		}, nil)
		require.NotEmpty(t, pvcs)

		got := pvcs[0].Object()
		require.Equal(t, []corev1.PersistentVolumeAccessMode{corev1.ReadWriteMany}, got.Spec.AccessModes)
		require.Equal(t, corev1.PersistentVolumeBlock, *got.Spec.VolumeMode)

		require.Equal(t, "value", got.Annotations["annot"])

		require.Equal(t, "cosmos-operator", got.Labels[kube.ControllerLabel])
		require.Equal(t, "value", got.Labels["label"])

		require.Equal(t, crd.Spec.VolumeClaimTemplate.DataSource, got.Spec.DataSource)
	})

	t.Run("instance override", func(t *testing.T) {
		crd := defaultCRD()
		crd.Name = "cosmoshub"
		crd.Spec.Replicas = 3
		crd.Spec.InstanceOverrides = map[string]cosmosv1.InstanceOverridesSpec{
			"cosmoshub-0": {
				VolumeClaimTemplate: &cosmosv1.PersistentVolumeClaimSpec{
					StorageClassName: "override",
				},
			},
			"cosmoshub-1": {
				DisableStrategy: ptr(cosmosv1.DisableAll),
			},
			"cosmoshub-2": {
				DisableStrategy: ptr(cosmosv1.DisablePod),
			},
			"does-not-exist": {
				VolumeClaimTemplate: &cosmosv1.PersistentVolumeClaimSpec{
					StorageClassName: "should never see me",
				},
			},
		}

		pvcs := BuildPVCs(&crd, map[int32]*dataSource{}, nil)
		require.Equal(t, 2, len(pvcs))

		got1, got2 := pvcs[0].Object(), pvcs[1].Object()

		require.NotEqual(t, got1.Spec, got2.Spec)
		require.Equal(t, []string{"pvc-cosmoshub-0", "pvc-cosmoshub-2"}, []string{got1.Name, got2.Name})
		require.Equal(t, "override", *got1.Spec.StorageClassName)
	})

	t.Run("long names", func(t *testing.T) {
		crd := defaultCRD()
		crd.Spec.Replicas = 3
		crd.Name = strings.Repeat("Y", 300)

		pvcs := BuildPVCs(&crd, map[int32]*dataSource{}, nil)
		require.NotEmpty(t, pvcs)

		for _, got := range pvcs {
			test.RequireValidMetadata(t, got.Object())
		}
	})

	t.Run("pvc auto scale with padding", func(t *testing.T) {
		t.Run("given auto scale size less then current size", func(t *testing.T) {
			for _, tt := range []struct {
				SpecQuant, AutoScaleQuant, WantQuant string
			}{
				{"100G", "97G", "100G"},
			} {
				crd := defaultCRD()
				crd.Spec.Replicas = 1
				crd.Spec.VolumeClaimTemplate.Resources.Requests[corev1.ResourceStorage] = resource.MustParse(tt.SpecQuant)

				crd.Status.SelfHealing.PVCAutoScale = map[string]*cosmosv1.PVCAutoScaleStatus{
					"pvc-osmosis-0": {
						RequestedSize: resource.MustParse(tt.AutoScaleQuant),
					},
				}

				pvcs := BuildPVCs(&crd, map[int32]*dataSource{}, nil)
				require.Len(t, pvcs, 1, tt)

				want := corev1.ResourceList{corev1.ResourceStorage: resource.MustParse(tt.WantQuant)}
				require.Equal(t, want.Storage().Value(), pvcs[0].Object().Spec.Resources.Requests.Storage().Value(), tt)
			}
		})

		t.Run("given auto scale size equal to current size", func(t *testing.T) {
			for _, tt := range []struct {
				SpecQuant, AutoScaleQuant, WantQuant string
			}{
				{"102G", "100G", "102G"},
			} {
				crd := defaultCRD()
				crd.Spec.Replicas = 1
				crd.Spec.VolumeClaimTemplate.Resources.Requests[corev1.ResourceStorage] = resource.MustParse(tt.SpecQuant)

				crd.Status.SelfHealing.PVCAutoScale = map[string]*cosmosv1.PVCAutoScaleStatus{
					"pvc-osmosis-0": {
						RequestedSize: resource.MustParse(tt.AutoScaleQuant),
					},
				}

				pvcs := BuildPVCs(&crd, map[int32]*dataSource{}, nil)
				require.Len(t, pvcs, 1, tt)

				want := corev1.ResourceList{corev1.ResourceStorage: resource.MustParse(tt.WantQuant)}
				require.Equal(t, want, pvcs[0].Object().Spec.Resources.Requests, tt)
			}
		})

		t.Run("given auto scale size greater than current size", func(t *testing.T) {
			for _, tt := range []struct {
				SpecQuant, AutoScaleQuant, WantQuant string
			}{
				{"100G", "100G", "102G"},
			} {
				crd := defaultCRD()
				crd.Spec.Replicas = 1
				crd.Spec.VolumeClaimTemplate.Resources.Requests[corev1.ResourceStorage] = resource.MustParse(tt.SpecQuant)

				crd.Status.SelfHealing.PVCAutoScale = map[string]*cosmosv1.PVCAutoScaleStatus{
					"pvc-osmosis-0": {
						RequestedSize: resource.MustParse(tt.AutoScaleQuant),
					},
				}

				pvcs := BuildPVCs(&crd, map[int32]*dataSource{}, nil)
				require.Len(t, pvcs, 1, tt)

				want := corev1.ResourceList{corev1.ResourceStorage: resource.MustParse(tt.WantQuant)}
				require.Equal(t, want.Storage().Value(), pvcs[0].Object().Spec.Resources.Requests.Storage().Value(), tt)
			}
		})
	})

	test.HasTypeLabel(t, func(crd cosmosv1.CosmosFullNode) []map[string]string {
		pvcs := BuildPVCs(&crd, map[int32]*dataSource{}, nil)
		labels := make([]map[string]string, 0)
		for _, pvc := range pvcs {
			labels = append(labels, pvc.Object().Labels)
		}
		return labels
	})
}

'''
'''--- internal/fullnode/pvc_control.go ---
package fullnode

import (
	"context"
	"fmt"

	snapshotv1 "github.com/kubernetes-csi/external-snapshotter/client/v6/apis/volumesnapshot/v1"
	"github.com/samber/lo"
	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	"github.com/strangelove-ventures/cosmos-operator/internal/diff"
	"github.com/strangelove-ventures/cosmos-operator/internal/kube"
	corev1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/api/resource"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

// PVCControl reconciles volumes for a CosmosFullNode.
// Unlike StatefulSet, PVCControl will update volumes by deleting and recreating volumes.
type PVCControl struct {
	client               Client
	recentVolumeSnapshot func(ctx context.Context, lister kube.Lister, namespace string, selector map[string]string) (*snapshotv1.VolumeSnapshot, error)
}

// NewPVCControl returns a valid PVCControl
func NewPVCControl(client Client) PVCControl {
	return PVCControl{
		client:               client,
		recentVolumeSnapshot: kube.RecentVolumeSnapshot,
	}
}

type PVCStatusChanges struct {
	Deleted []string
}

// Reconcile is the control loop for PVCs. The bool return value, if true, indicates the controller should requeue
// the request.
func (control PVCControl) Reconcile(ctx context.Context, reporter kube.Reporter, crd *cosmosv1.CosmosFullNode, pvcStatusChanges *PVCStatusChanges) (bool, kube.ReconcileError) {
	// Find any existing pvcs for this CRD.
	var vols corev1.PersistentVolumeClaimList
	if err := control.client.List(ctx, &vols,
		client.InNamespace(crd.Namespace),
		client.MatchingFields{kube.ControllerOwnerField: crd.Name},
	); err != nil {
		return false, kube.TransientError(fmt.Errorf("list existing pvcs: %w", err))
	}

	var currentPVCs = ptrSlice(vols.Items)

	dataSources := make(map[int32]*dataSource)
	if len(currentPVCs) < int(crd.Spec.Replicas) {
		for i := int32(0); i < crd.Spec.Replicas; i++ {
			name := pvcName(crd, i)
			found := false
			for _, pvc := range currentPVCs {
				if pvc.Name == name {
					found = true
					break
				}
			}
			if !found {
				ds := control.findDataSource(ctx, reporter, crd, i)
				if ds == nil {
					ds = &dataSource{
						size: crd.Spec.VolumeClaimTemplate.Resources.Requests[corev1.ResourceStorage],
					}
				}
				dataSources[i] = ds
			}
		}
	}

	var (
		wantPVCs = BuildPVCs(crd, dataSources, currentPVCs)
		diffed   = diff.New(currentPVCs, wantPVCs)
	)

	for _, pvc := range diffed.Creates() {
		size := pvc.Spec.Resources.Requests[corev1.ResourceStorage]

		reporter.Info(
			"Creating pvc",
			"name", pvc.Name,
			"size", size.String(),
		)
		if err := ctrl.SetControllerReference(crd, pvc, control.client.Scheme()); err != nil {
			return true, kube.TransientError(fmt.Errorf("set controller reference on pvc %q: %w", pvc.Name, err))
		}
		if err := control.client.Create(ctx, pvc); kube.IgnoreAlreadyExists(err) != nil {
			return true, kube.TransientError(fmt.Errorf("create pvc %q: %w", pvc.Name, err))
		}
		pvcStatusChanges.Deleted = append(pvcStatusChanges.Deleted, pvc.Name)
	}

	var deletes int
	if !control.shouldRetain(crd) {
		for _, pvc := range diffed.Deletes() {
			reporter.Info("Deleting pvc", "name", pvc.Name)
			if err := control.client.Delete(ctx, pvc, client.PropagationPolicy(metav1.DeletePropagationForeground)); client.IgnoreNotFound(err) != nil {
				return true, kube.TransientError(fmt.Errorf("delete pvc %q: %w", pvc.Name, err))
			}
			pvcStatusChanges.Deleted = append(pvcStatusChanges.Deleted, pvc.Name)
		}
		deletes = len(diffed.Deletes())
	}

	if deletes+len(diffed.Creates()) > 0 {
		// Scaling happens first; then updates. So requeue to handle updates after scaling finished.
		return true, nil
	}

	if len(diffed.Updates()) == 0 {
		return false, nil
	}

	if _, unbound := lo.Find(currentPVCs, func(pvc *corev1.PersistentVolumeClaim) bool {
		return pvc.Status.Phase != corev1.ClaimBound
	}); unbound {
		return true, nil
	}

	// PVCs have many immutable fields, so only update the storage size.
	for _, pvc := range diffed.Updates() {
		size := pvc.Spec.Resources.Requests[corev1.ResourceStorage]
		reporter.Info(
			"Patching pvc",
			"name", pvc.Name,
			"size", size.String(), // TODO remove expensive operation
		)
		patch := corev1.PersistentVolumeClaim{
			ObjectMeta: pvc.ObjectMeta,
			TypeMeta:   pvc.TypeMeta,
			Spec: corev1.PersistentVolumeClaimSpec{
				Resources: pvc.Spec.Resources,
			},
		}
		if err := control.client.Patch(ctx, &patch, client.Merge); err != nil {
			reporter.Error(err, "PVC patch failed", "name", pvc.Name)
			reporter.RecordError("PVCPatchFailed", err)
			continue
		}
	}

	return false, nil
}

func (control PVCControl) shouldRetain(crd *cosmosv1.CosmosFullNode) bool {
	if policy := crd.Spec.RetentionPolicy; policy != nil {
		return *policy == cosmosv1.RetentionPolicyRetain
	}
	return false
}

type dataSource struct {
	ref *corev1.TypedLocalObjectReference

	size resource.Quantity
}

func (control PVCControl) findDataSource(ctx context.Context, reporter kube.Reporter, crd *cosmosv1.CosmosFullNode, ordinal int32) *dataSource {
	if override, ok := crd.Spec.InstanceOverrides[instanceName(crd, ordinal)]; ok {
		if overrideTpl := override.VolumeClaimTemplate; overrideTpl != nil {
			return control.findDataSourceWithPvcSpec(ctx, reporter, crd, *overrideTpl, ordinal)
		}
	}

	return control.findDataSourceWithPvcSpec(ctx, reporter, crd, crd.Spec.VolumeClaimTemplate, ordinal)
}

func (control PVCControl) findDataSourceWithPvcSpec(
	ctx context.Context,
	reporter kube.Reporter,
	crd *cosmosv1.CosmosFullNode,
	pvcSpec cosmosv1.PersistentVolumeClaimSpec,
	ordinal int32,
) *dataSource {
	if ds := pvcSpec.DataSource; ds != nil {
		if ds.Kind == "VolumeSnapshot" && ds.APIGroup != nil && *ds.APIGroup == "snapshot.storage.k8s.io" {
			var vs snapshotv1.VolumeSnapshot
			if err := control.client.Get(ctx, client.ObjectKey{Namespace: crd.Namespace, Name: ds.Name}, &vs); err != nil {
				reporter.Error(err, "Failed to get VolumeSnapshot for DataSource")
				reporter.RecordError("DataSourceGetSnapshot", err)
				return nil
			}
			return &dataSource{
				ref:  ds,
				size: *vs.Status.RestoreSize,
			}
		} else if ds.Kind == "PersistentVolumeClaim" && (ds.APIGroup == nil || *ds.APIGroup == "") {
			var pvc corev1.PersistentVolumeClaim
			if err := control.client.Get(ctx, client.ObjectKey{Namespace: crd.Namespace, Name: ds.Name}, &pvc); err != nil {
				reporter.Error(err, "Failed to get PersistentVolumeClaim for DataSource")
				reporter.RecordError("DataSourceGetPVC", err)
				return nil
			}
			return &dataSource{
				ref:  ds,
				size: pvc.Status.Capacity["storage"],
			}
		} else {
			err := fmt.Errorf("unsupported DataSource %s", ds.Kind)
			reporter.Error(err, "Unsupported DataSource")
			reporter.RecordError("DataSourceUnsupported", err)
			return nil
		}
	}
	spec := pvcSpec.AutoDataSource
	if spec == nil {
		return nil
	}
	selector := spec.VolumeSnapshotSelector
	if len(selector) == 0 {
		return nil
	}
	if spec.MatchInstance {
		selector[kube.InstanceLabel] = instanceName(crd, ordinal)
	}
	found, err := control.recentVolumeSnapshot(ctx, control.client, crd.Namespace, selector)
	if err != nil {
		reporter.Error(err, "Failed to find VolumeSnapshot for AutoDataSource")
		reporter.RecordError("AutoDataSourceFindSnapshot", err)
		return nil
	}

	reporter.RecordInfo("AutoDataSource", "Using recent VolumeSnapshot for PVC data source")
	return &dataSource{
		ref: &corev1.TypedLocalObjectReference{
			APIGroup: ptr("snapshot.storage.k8s.io"),
			Kind:     "VolumeSnapshot",
			Name:     found.Name,
		},
		size: *found.Status.RestoreSize,
	}
}

'''
'''--- internal/fullnode/pvc_control_test.go ---
package fullnode

import (
	"context"
	"errors"
	"testing"

	snapshotv1 "github.com/kubernetes-csi/external-snapshotter/client/v6/apis/volumesnapshot/v1"
	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	"github.com/strangelove-ventures/cosmos-operator/internal/diff"
	"github.com/strangelove-ventures/cosmos-operator/internal/kube"
	"github.com/strangelove-ventures/cosmos-operator/internal/test"
	"github.com/stretchr/testify/require"
	corev1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/api/resource"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

var nopReporter test.NopReporter

func TestPVCControl_Reconcile(t *testing.T) {
	t.Parallel()

	type mockPVCClient = mockClient[*corev1.PersistentVolumeClaim]

	ctx := context.Background()
	const namespace = "test"

	testPVCControl := func(client Client) PVCControl {
		control := NewPVCControl(client)
		control.recentVolumeSnapshot = func(ctx context.Context, lister kube.Lister, namespace string, selector map[string]string) (*snapshotv1.VolumeSnapshot, error) {
			panic("recentVolumeSnapshot should not be called")
		}
		return control
	}

	t.Run("no changes", func(t *testing.T) {
		crd := defaultCRD()
		crd.Name = "hub"
		crd.Namespace = namespace
		crd.Spec.Replicas = 1
		existing := diff.New(nil, BuildPVCs(&crd, map[int32]*dataSource{}, nil)).Creates()[0]
		existing.Status.Phase = corev1.ClaimBound

		var mClient mockPVCClient
		mClient.ObjectList = corev1.PersistentVolumeClaimList{
			Items: []corev1.PersistentVolumeClaim{
				*existing,
			},
		}

		control := testPVCControl(&mClient)

		requeue, err := control.Reconcile(ctx, nopReporter, &crd, &PVCStatusChanges{})
		require.NoError(t, err)
		require.False(t, requeue)

		require.Len(t, mClient.GotListOpts, 2)
		var listOpt client.ListOptions
		for _, opt := range mClient.GotListOpts {
			opt.ApplyToList(&listOpt)
		}
		require.Equal(t, namespace, listOpt.Namespace)
		require.Zero(t, listOpt.Limit)
		require.Equal(t, ".metadata.controller=hub", listOpt.FieldSelector.String())

		require.Empty(t, mClient.LastPatchObject)
	})

	t.Run("scale phase", func(t *testing.T) {
		crd := defaultCRD()
		crd.Namespace = namespace
		crd.Name = "hub"
		crd.Spec.Replicas = 1
		existing := BuildPVCs(&crd, map[int32]*dataSource{}, nil)[0].Object()

		var mClient mockPVCClient
		mClient.ObjectList = corev1.PersistentVolumeClaimList{
			Items: []corev1.PersistentVolumeClaim{
				{ObjectMeta: metav1.ObjectMeta{Name: "pvc-hub-98", Namespace: namespace}}, // delete
				{ObjectMeta: metav1.ObjectMeta{Name: "pvc-hub-99", Namespace: namespace}}, // delete
				*existing,
			},
		}

		crd.Spec.Replicas = 4
		control := testPVCControl(&mClient)
		requeue, err := control.Reconcile(ctx, nopReporter, &crd, &PVCStatusChanges{})
		require.NoError(t, err)
		require.True(t, requeue)

		require.Equal(t, 3, mClient.CreateCount)
		require.Equal(t, 2, mClient.DeleteCount)
		require.Zero(t, mClient.UpdateCount)

		require.NotEmpty(t, mClient.LastCreateObject.OwnerReferences)
		require.Equal(t, crd.Name, mClient.LastCreateObject.OwnerReferences[0].Name)
		require.Equal(t, "CosmosFullNode", mClient.LastCreateObject.OwnerReferences[0].Kind)
		require.True(t, *mClient.LastCreateObject.OwnerReferences[0].Controller)
	})

	t.Run("create - autoDataSource", func(t *testing.T) {
		var (
			mClient mockPVCClient
			crd     = defaultCRD()
			control = testPVCControl(&mClient)
		)
		crd.Namespace = namespace
		crd.Spec.Replicas = 3
		crd.Spec.VolumeClaimTemplate.AutoDataSource = &cosmosv1.AutoDataSource{
			VolumeSnapshotSelector: map[string]string{"label": "vol-snapshot"},
		}

		var volCallCount int
		control.recentVolumeSnapshot = func(ctx context.Context, lister kube.Lister, namespace string, selector map[string]string) (*snapshotv1.VolumeSnapshot, error) {
			require.NotNil(t, ctx)
			require.Equal(t, &mClient, lister)
			require.Equal(t, namespace, namespace)
			require.Equal(t, map[string]string{"label": "vol-snapshot"}, selector)
			var stub snapshotv1.VolumeSnapshot
			stub.Name = "found-snapshot"
			stub.Status = &snapshotv1.VolumeSnapshotStatus{
				ReadyToUse:  ptr(true),
				RestoreSize: ptr(resource.MustParse("100Gi")),
			}
			volCallCount++
			return &stub, nil
		}
		requeue, err := control.Reconcile(ctx, nopReporter, &crd, &PVCStatusChanges{})
		require.NoError(t, err)
		require.True(t, requeue)

		require.Equal(t, 3, volCallCount)
		require.Equal(t, 3, mClient.CreateCount)

		want := corev1.TypedLocalObjectReference{
			APIGroup: ptr("snapshot.storage.k8s.io"),
			Kind:     "VolumeSnapshot",
			Name:     "found-snapshot",
		}
		for _, pvc := range mClient.CreatedObjects {
			ds := pvc.Spec.DataSource
			require.NotNil(t, ds)
			require.Equal(t, want, *ds)
		}
	})

	t.Run("create - autoDataSource dataSource already set", func(t *testing.T) {
		var (
			mClient mockPVCClient
			crd     = defaultCRD()
			control = testPVCControl(&mClient)
		)
		crdDataSource := &corev1.TypedLocalObjectReference{
			APIGroup: ptr("snapshot.storage.k8s.io"),
			Kind:     "VolumeSnapshot",
			Name:     "user-set-snapshot",
		}
		crd.Namespace = namespace
		crd.Spec.Replicas = 2
		crd.Spec.VolumeClaimTemplate.AutoDataSource = &cosmosv1.AutoDataSource{
			VolumeSnapshotSelector: map[string]string{"label": "vol-snapshot"},
		}
		crd.Spec.VolumeClaimTemplate.DataSource = crdDataSource

		control.recentVolumeSnapshot = func(ctx context.Context, lister kube.Lister, namespace string, selector map[string]string) (*snapshotv1.VolumeSnapshot, error) {
			panic("should not be called")
		}

		mClient.Object = snapshotv1.VolumeSnapshot{
			ObjectMeta: metav1.ObjectMeta{
				Name:      "user-set-snapshot",
				Namespace: namespace,
			},
			Status: &snapshotv1.VolumeSnapshotStatus{
				ReadyToUse:  ptr(true),
				RestoreSize: ptr(resource.MustParse("100Gi")),
			},
		}

		requeue, err := control.Reconcile(ctx, nopReporter, &crd, &PVCStatusChanges{})
		require.NoError(t, err)
		require.True(t, requeue)

		require.Equal(t, 2, mClient.CreateCount)

		for _, pvc := range mClient.CreatedObjects {
			ds := pvc.Spec.DataSource
			require.NotNil(t, ds)
			require.Equal(t, *crdDataSource, *ds)
		}
	})

	t.Run("create - autoDataSource error", func(t *testing.T) {
		var (
			mClient mockPVCClient
			crd     = defaultCRD()
			control = testPVCControl(&mClient)
		)
		crd.Namespace = namespace
		crd.Spec.Replicas = 1
		crd.Spec.VolumeClaimTemplate.AutoDataSource = &cosmosv1.AutoDataSource{
			VolumeSnapshotSelector: map[string]string{"label": "vol-snapshot"},
		}
		var volCallCount int
		control.recentVolumeSnapshot = func(ctx context.Context, lister kube.Lister, namespace string, selector map[string]string) (*snapshotv1.VolumeSnapshot, error) {
			volCallCount++
			return nil, errors.New("boom")
		}
		requeue, err := control.Reconcile(ctx, nopReporter, &crd, &PVCStatusChanges{})
		require.NoError(t, err)
		require.True(t, requeue)

		require.Equal(t, 1, mClient.CreateCount)
		require.Equal(t, 1, volCallCount)

		require.Nil(t, mClient.LastCreateObject.Spec.DataSource)
	})

	t.Run("updates", func(t *testing.T) {
		crd := defaultCRD()
		crd.Name = "hub"
		crd.Namespace = namespace
		crd.Spec.Replicas = 1

		var mClient mockPVCClient
		existing := BuildPVCs(&crd, map[int32]*dataSource{}, nil)[0].Object()
		existing.Status.Phase = corev1.ClaimBound
		mClient.ObjectList = corev1.PersistentVolumeClaimList{
			Items: []corev1.PersistentVolumeClaim{*existing},
		}

		// Cause a change
		crd.Spec.VolumeClaimTemplate.VolumeMode = ptr(corev1.PersistentVolumeMode("should not be in the patch"))
		crd.Spec.VolumeClaimTemplate.Resources.Requests["memory"] = resource.MustParse("1Gi")

		control := testPVCControl(&mClient)
		requeue, rerr := control.Reconcile(ctx, nopReporter, &crd, &PVCStatusChanges{})
		require.NoError(t, rerr)
		require.False(t, requeue)

		require.Empty(t, mClient.CreateCount)
		require.Empty(t, mClient.DeleteCount)

		require.Equal(t, 1, mClient.PatchCount)
		require.Equal(t, client.Merge, mClient.LastPatch)

		gotPatch := mClient.LastPatchObject.(*corev1.PersistentVolumeClaim)
		require.Equal(t, existing.Name, gotPatch.Name)
		require.Equal(t, namespace, gotPatch.Namespace)
		require.Empty(t, gotPatch.Spec.VolumeMode)
		require.Equal(t, crd.Spec.VolumeClaimTemplate.Resources, gotPatch.Spec.Resources)
	})

	t.Run("updates with unbound volumes", func(t *testing.T) {
		crd := defaultCRD()
		crd.Name = "hub"
		crd.Namespace = namespace
		crd.Spec.Replicas = 1

		existing := BuildPVCs(&crd, map[int32]*dataSource{}, nil)[0].Object()
		existing.Status.Phase = corev1.ClaimPending
		var mClient mockPVCClient
		mClient.ObjectList = corev1.PersistentVolumeClaimList{
			Items: []corev1.PersistentVolumeClaim{*existing},
		}

		// Cause a change
		crd.Spec.VolumeClaimTemplate.Resources.Requests[corev1.ResourceStorage] = resource.MustParse("1Ti")

		control := testPVCControl(&mClient)
		requeue, rerr := control.Reconcile(ctx, nopReporter, &crd, &PVCStatusChanges{})
		require.NoError(t, rerr)
		require.True(t, requeue)

		require.Zero(t, mClient.PatchCount)
	})

	t.Run("retention policy", func(t *testing.T) {
		crd := defaultCRD()
		crd.Namespace = namespace
		crd.Spec.Replicas = 0
		crd.Spec.RetentionPolicy = ptr(cosmosv1.RetentionPolicyRetain)

		var mClient mockPVCClient
		mClient.ObjectList = corev1.PersistentVolumeClaimList{
			Items: []corev1.PersistentVolumeClaim{
				{ObjectMeta: metav1.ObjectMeta{Name: "pvc-hub-98", Namespace: namespace}}, // delete
				{ObjectMeta: metav1.ObjectMeta{Name: "pvc-hub-99", Namespace: namespace}}, // delete
			},
		}

		control := testPVCControl(&mClient)
		requeue, err := control.Reconcile(ctx, nopReporter, &crd, &PVCStatusChanges{})
		require.NoError(t, err)
		require.False(t, requeue)

		require.Zero(t, mClient.DeleteCount)
	})
}

'''
'''--- internal/fullnode/pvc_disk_usage.go ---
package fullnode

import (
	"context"
	"errors"
	"fmt"
	"math"
	"time"

	"github.com/samber/lo"
	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	"github.com/strangelove-ventures/cosmos-operator/internal/healthcheck"
	"github.com/strangelove-ventures/cosmos-operator/internal/kube"
	"golang.org/x/sync/errgroup"
	corev1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/api/resource"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

// DiskUsager fetches disk usage statistics
type DiskUsager interface {
	DiskUsage(ctx context.Context, host, homeDir string) (healthcheck.DiskUsageResponse, error)
}

type PVCDiskUsage struct {
	Name        string // pvc name
	PercentUsed int
	Capacity    resource.Quantity
}

type DiskUsageCollector struct {
	diskClient DiskUsager
	client     Reader
}

func NewDiskUsageCollector(diskClient DiskUsager, lister Reader) *DiskUsageCollector {
	return &DiskUsageCollector{diskClient: diskClient, client: lister}
}

// CollectDiskUsage retrieves the disk usage information for all pods belonging to the specified CosmosFullNode.
//
// It returns a slice of PVCDiskUsage objects representing the disk usage information for each PVC or an error
// if fetching disk usage via all pods was unsuccessful.
func (c DiskUsageCollector) CollectDiskUsage(ctx context.Context, crd *cosmosv1.CosmosFullNode) ([]PVCDiskUsage, error) {
	var pods corev1.PodList
	if err := c.client.List(ctx, &pods,
		client.InNamespace(crd.Namespace),
		client.MatchingFields{kube.ControllerOwnerField: crd.Name},
	); err != nil {
		return nil, fmt.Errorf("list pods: %w", err)
	}

	if len(pods.Items) == 0 {
		return nil, errors.New("no pods found")
	}

	var (
		found = make([]PVCDiskUsage, len(pods.Items))
		errs  = make([]error, len(pods.Items))
		eg    errgroup.Group
	)

	for i := range pods.Items {
		i := i
		eg.Go(func() error {
			pod := pods.Items[i]
			cctx, cancel := context.WithTimeout(ctx, 10*time.Second)
			defer cancel()
			resp, err := c.diskClient.DiskUsage(cctx, "http://"+pod.Status.PodIP, ChainHomeDir(crd))
			if err != nil {
				errs[i] = fmt.Errorf("pod %s %s: %w", pod.Name, resp.Dir, err)
				return nil
			}

			// Find matching PVC to capture its actual capacity
			name := PVCName(&pod)
			key := client.ObjectKey{Namespace: pod.Namespace, Name: name}
			var pvc corev1.PersistentVolumeClaim
			if err = c.client.Get(ctx, key, &pvc); err != nil {
				errs[i] = fmt.Errorf("get pvc %s: %w", key, err)
			}

			found[i].Name = name
			found[i].Capacity = pvc.Status.Capacity[corev1.ResourceStorage]
			n := (float64(resp.AllBytes-resp.FreeBytes) / float64(resp.AllBytes)) * 100
			n = math.Round(n)
			found[i].PercentUsed = int(n)
			return nil
		})
	}

	_ = eg.Wait()

	errs = lo.Filter(errs, func(item error, _ int) bool {
		return item != nil
	})
	if len(errs) == len(pods.Items) {
		return nil, errors.Join(errs...)
	}

	return lo.Compact(found), nil
}

'''
'''--- internal/fullnode/pvc_disk_usage_test.go ---
package fullnode

import (
	"context"
	"errors"
	"fmt"
	"sort"
	"testing"

	"github.com/samber/lo"
	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	"github.com/strangelove-ventures/cosmos-operator/internal/healthcheck"
	"github.com/stretchr/testify/require"
	corev1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/api/resource"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

type mockDiskUsager func(ctx context.Context, host, homeDir string) (healthcheck.DiskUsageResponse, error)

func (fn mockDiskUsager) DiskUsage(ctx context.Context, host, homeDir string) (healthcheck.DiskUsageResponse, error) {
	return fn(ctx, host, homeDir)
}

func TestCollectDiskUsage(t *testing.T) {
	t.Parallel()

	type mockReader = mockClient[*corev1.Pod]

	ctx := context.Background()

	const namespace = "default"

	var crd cosmosv1.CosmosFullNode
	crd.Name = "cosmoshub"
	crd.Namespace = namespace

	builder := NewPodBuilder(&crd)
	validPods := lo.Map(lo.Range(3), func(_ int, index int) corev1.Pod {
		pod, err := builder.WithOrdinal(int32(index)).Build()
		if err != nil {
			panic(err)
		}
		pod.Status.PodIP = fmt.Sprintf("10.0.0.%d", index)
		return *pod
	})

	t.Run("happy path", func(t *testing.T) {
		var reader mockReader
		reader.ObjectList = corev1.PodList{Items: validPods}
		reader.Object = corev1.PersistentVolumeClaim{
			Status: corev1.PersistentVolumeClaimStatus{
				Capacity: corev1.ResourceList{corev1.ResourceStorage: resource.MustParse("500Gi")},
			},
		}

		diskClient := mockDiskUsager(func(ctx context.Context, host, homeDir string) (healthcheck.DiskUsageResponse, error) {
			if homeDir != "/home/operator/cosmos" {
				return healthcheck.DiskUsageResponse{}, fmt.Errorf("unexpected homeDir: %s", homeDir)
			}
			var free uint64
			switch host {
			case "http://10.0.0.0":
				free = 900
			case "http://10.0.0.1":
				free = 500
			case "http://10.0.0.2":
				free = 15 // Tests rounding up
			default:
				panic(fmt.Errorf("unknown host: %s", host))
			}
			return healthcheck.DiskUsageResponse{
				AllBytes:  1000,
				FreeBytes: free,
			}, nil
		})

		coll := NewDiskUsageCollector(diskClient, &reader)
		got, err := coll.CollectDiskUsage(ctx, &crd)

		require.NoError(t, err)
		require.Len(t, got, 3)

		require.Len(t, reader.GotListOpts, 2)
		var listOpt client.ListOptions
		for _, opt := range reader.GotListOpts {
			opt.ApplyToList(&listOpt)
		}
		require.Equal(t, "default", listOpt.Namespace)
		require.Zero(t, listOpt.Limit)
		require.Equal(t, ".metadata.controller=cosmoshub", listOpt.FieldSelector.String())

		require.Equal(t, namespace, reader.GetObjectKey.Namespace)
		require.Contains(t, []string{"pvc-cosmoshub-0", "pvc-cosmoshub-1", "pvc-cosmoshub-2"}, reader.GetObjectKey.Name)

		sort.Slice(got, func(i, j int) bool {
			return got[i].Name < got[j].Name
		})

		result := got[0]
		require.Equal(t, "pvc-cosmoshub-0", result.Name)
		require.Equal(t, 10, result.PercentUsed)
		require.Equal(t, resource.MustParse("500Gi"), result.Capacity)

		result = got[1]
		require.Equal(t, "pvc-cosmoshub-1", result.Name)
		require.Equal(t, 50, result.PercentUsed)
		require.Equal(t, resource.MustParse("500Gi"), result.Capacity)

		result = got[2]
		require.Equal(t, "pvc-cosmoshub-2", result.Name)
		require.Equal(t, 99, result.PercentUsed) // Tests rounding to be close to output of `df`
		require.Equal(t, resource.MustParse("500Gi"), result.Capacity)
	})

	t.Run("custom home dir", func(t *testing.T) {
		var reader mockReader
		reader.ObjectList = corev1.PodList{Items: validPods}
		reader.Object = corev1.PersistentVolumeClaim{
			Status: corev1.PersistentVolumeClaimStatus{
				Capacity: corev1.ResourceList{corev1.ResourceStorage: resource.MustParse("500Gi")},
			},
		}

		diskClient := mockDiskUsager(func(ctx context.Context, host, homeDir string) (healthcheck.DiskUsageResponse, error) {
			if homeDir != "/home/operator/.gaia" {
				return healthcheck.DiskUsageResponse{}, fmt.Errorf("unexpected homeDir: %s", homeDir)
			}
			return healthcheck.DiskUsageResponse{
				AllBytes:  1000,
				FreeBytes: 900,
			}, nil
		})

		coll := NewDiskUsageCollector(diskClient, &reader)

		ccrd := crd.DeepCopy()
		ccrd.Spec.ChainSpec.HomeDir = ".gaia"
		_, err := coll.CollectDiskUsage(ctx, ccrd)

		require.NoError(t, err)
	})

	t.Run("no pods found", func(t *testing.T) {
		var reader mockReader
		diskClient := mockDiskUsager(func(ctx context.Context, host, homeDir string) (healthcheck.DiskUsageResponse, error) {
			panic("should not be called")
		})

		coll := NewDiskUsageCollector(diskClient, &reader)
		_, err := coll.CollectDiskUsage(ctx, &crd)

		require.Error(t, err)
		require.EqualError(t, err, "no pods found")
	})

	t.Run("list error", func(t *testing.T) {
		var reader mockReader
		reader.ObjectList = corev1.PodList{Items: []corev1.Pod{
			{ObjectMeta: metav1.ObjectMeta{Name: "pod-1"}, Status: corev1.PodStatus{PodIP: "10.0.0.1"}},
		}}
		reader.ListErr = errors.New("boom")
		diskClient := mockDiskUsager(func(ctx context.Context, host, homeDir string) (healthcheck.DiskUsageResponse, error) {
			panic("should not be called")
		})

		coll := NewDiskUsageCollector(diskClient, &reader)
		_, err := coll.CollectDiskUsage(ctx, &crd)

		require.Error(t, err)
		require.EqualError(t, err, "list pods: boom")
	})

	t.Run("partial disk client errors", func(t *testing.T) {
		var reader mockReader
		reader.ObjectList = corev1.PodList{Items: validPods}

		diskClient := mockDiskUsager(func(ctx context.Context, host, homeDir string) (healthcheck.DiskUsageResponse, error) {
			if host == "http://10.0.0.1" {
				return healthcheck.DiskUsageResponse{}, errors.New("boom")
			}
			return healthcheck.DiskUsageResponse{
				AllBytes:  100,
				FreeBytes: 100,
			}, nil
		})

		coll := NewDiskUsageCollector(diskClient, &reader)
		got, err := coll.CollectDiskUsage(ctx, &crd)

		require.NoError(t, err)
		require.Len(t, got, 2)

		gotNames := lo.Map(got, func(item PVCDiskUsage, _ int) string {
			return item.Name
		})
		require.NotContains(t, gotNames, "pvc-cosmoshub-1")
	})

	t.Run("disk client error", func(t *testing.T) {
		var reader mockReader
		reader.ObjectList = corev1.PodList{Items: []corev1.Pod{
			{ObjectMeta: metav1.ObjectMeta{Name: "1"}, Status: corev1.PodStatus{PodIP: "10.0.0.1"}},
			{ObjectMeta: metav1.ObjectMeta{Name: "2"}, Status: corev1.PodStatus{PodIP: "10.0.0.2"}},
		}}

		diskClient := mockDiskUsager(func(ctx context.Context, host, homeDir string) (healthcheck.DiskUsageResponse, error) {
			return healthcheck.DiskUsageResponse{Dir: "/some/dir"}, errors.New("boom")
		})

		var crd cosmosv1.CosmosFullNode

		coll := NewDiskUsageCollector(diskClient, &reader)
		_, err := coll.CollectDiskUsage(ctx, &crd)

		require.Error(t, err)
		require.Contains(t, err.Error(), "pod 1 /some/dir: boom")
		require.Contains(t, err.Error(), "pod 2 /some/dir: boom")
	})
}

'''
'''--- internal/fullnode/rbac_builder.go ---
package fullnode

import (
	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	"github.com/strangelove-ventures/cosmos-operator/internal/diff"
	"github.com/strangelove-ventures/cosmos-operator/internal/kube"
	corev1 "k8s.io/api/core/v1"
	rbacv1 "k8s.io/api/rbac/v1"
	v1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

func serviceAccountName(crd *cosmosv1.CosmosFullNode) string {
	return crd.Name + "-vc-sa"
}

func roleName(crd *cosmosv1.CosmosFullNode) string {
	return crd.Name + "-vc-r"
}

func roleBindingName(crd *cosmosv1.CosmosFullNode) string {
	return crd.Name + "-vc-rb"
}

// BuildServiceAccounts returns a list of service accounts given the crd.
//
// Creates a single service account for the version check.
func BuildServiceAccounts(crd *cosmosv1.CosmosFullNode) []diff.Resource[*corev1.ServiceAccount] {
	diffSa := make([]diff.Resource[*corev1.ServiceAccount], 1)
	sa := corev1.ServiceAccount{
		TypeMeta: v1.TypeMeta{
			Kind:       "ServiceAccount",
			APIVersion: "v1",
		},
		ObjectMeta: v1.ObjectMeta{
			Name:      serviceAccountName(crd),
			Namespace: crd.Namespace,
		},
	}

	sa.Labels = defaultLabels(crd, kube.ComponentLabel, "vc")

	diffSa[0] = diff.Adapt(&sa, 0)

	return diffSa
}

// BuildRoles returns a list of role bindings given the crd.
//
// Creates a single role binding for the version check.
func BuildRoles(crd *cosmosv1.CosmosFullNode) []diff.Resource[*rbacv1.Role] {
	diffCr := make([]diff.Resource[*rbacv1.Role], 1)
	cr := rbacv1.Role{
		TypeMeta: v1.TypeMeta{
			Kind:       "Role",
			APIVersion: "rbac.authorization.k8s.io/v1",
		},
		ObjectMeta: v1.ObjectMeta{
			Name:      roleName(crd),
			Namespace: crd.Namespace,
		},
		Rules: []rbacv1.PolicyRule{
			{
				APIGroups: []string{""}, // core API group
				Resources: []string{"namespaces", "pods"},
				Verbs:     []string{"get", "list"},
			},
			{
				APIGroups: []string{"cosmos.strange.love"},
				Resources: []string{"cosmosfullnodes"},
				Verbs:     []string{"get"},
			},
			{
				APIGroups: []string{"cosmos.strange.love"},
				Resources: []string{"cosmosfullnodes/status"},
				Verbs:     []string{"patch"},
			},
		},
	}

	cr.Labels = defaultLabels(crd, kube.ComponentLabel, "vc")

	diffCr[0] = diff.Adapt(&cr, 0)

	return diffCr
}

// BuildRoles returns a list of role binding bindings given the crd.
//
// Creates a single role binding binding for the version check.
func BuildRoleBindings(crd *cosmosv1.CosmosFullNode) []diff.Resource[*rbacv1.RoleBinding] {
	diffCrb := make([]diff.Resource[*rbacv1.RoleBinding], 1)
	crb := rbacv1.RoleBinding{
		TypeMeta: v1.TypeMeta{
			Kind:       "RoleBinding",
			APIVersion: "rbac.authorization.k8s.io/v1",
		},
		ObjectMeta: v1.ObjectMeta{
			Name:      roleBindingName(crd),
			Namespace: crd.Namespace,
		},
		Subjects: []rbacv1.Subject{
			{
				Kind:      "ServiceAccount",
				Name:      serviceAccountName(crd),
				Namespace: crd.Namespace,
			},
		},
		RoleRef: rbacv1.RoleRef{
			Kind:     "Role",
			Name:     roleName(crd),
			APIGroup: "rbac.authorization.k8s.io",
		},
	}

	crb.Labels = defaultLabels(crd, kube.ComponentLabel, "vc")

	diffCrb[0] = diff.Adapt(&crb, 0)

	return diffCrb
}

'''
'''--- internal/fullnode/rbac_builder_test.go ---
package fullnode

import (
	"testing"

	"github.com/stretchr/testify/require"

	rbacv1 "k8s.io/api/rbac/v1"
)

func TestBuildRBAC(t *testing.T) {
	t.Parallel()

	t.Run("build rbac", func(t *testing.T) {
		crd := defaultCRD()
		crd.Spec.Replicas = 3
		crd.Name = "hub"
		crd.Namespace = "test"
		crd.Spec.ChainSpec.Network = "testnet"
		crd.Spec.PodTemplate.Image = "gaia:v6.0.0"

		sas := BuildServiceAccounts(&crd)

		require.Len(t, sas, 1) // 1 svc account in the namespace

		sa := sas[0].Object()

		require.Equal(t, "hub-vc-sa", sa.Name)
		require.Equal(t, "test", sa.Namespace)

		wantLabels := map[string]string{
			"app.kubernetes.io/created-by": "cosmos-operator",
			"app.kubernetes.io/name":       "hub",
			"app.kubernetes.io/component":  "vc",
			"app.kubernetes.io/version":    "v6.0.0",
			"cosmos.strange.love/network":  "testnet",
			"cosmos.strange.love/type":     "FullNode",
		}
		require.Equal(t, wantLabels, sa.Labels)

		roles := BuildRoles(&crd)

		require.Len(t, roles, 1) // 1 role in the namespace

		role := roles[0].Object()

		require.Equal(t, "hub-vc-r", role.Name)
		require.Equal(t, "test", role.Namespace)

		wantLabels = map[string]string{
			"app.kubernetes.io/created-by": "cosmos-operator",
			"app.kubernetes.io/name":       "hub",
			"app.kubernetes.io/component":  "vc",
			"app.kubernetes.io/version":    "v6.0.0",
			"cosmos.strange.love/network":  "testnet",
			"cosmos.strange.love/type":     "FullNode",
		}
		require.Equal(t, wantLabels, role.Labels)

		require.Equal(t, []rbacv1.PolicyRule{
			{
				APIGroups: []string{""}, // core API group
				Resources: []string{"namespaces", "pods"},
				Verbs:     []string{"get", "list"},
			},
			{
				APIGroups: []string{"cosmos.strange.love"},
				Resources: []string{"cosmosfullnodes"},
				Verbs:     []string{"get"},
			},
			{
				APIGroups: []string{"cosmos.strange.love"},
				Resources: []string{"cosmosfullnodes/status"},
				Verbs:     []string{"patch"},
			},
		}, role.Rules)

		rbs := BuildRoleBindings(&crd)

		require.Len(t, rbs, 1) // 1 role in the namespace

		rb := rbs[0].Object()

		require.Equal(t, "hub-vc-rb", rb.Name)
		require.Equal(t, "test", rb.Namespace)

		wantLabels = map[string]string{
			"app.kubernetes.io/created-by": "cosmos-operator",
			"app.kubernetes.io/name":       "hub",
			"app.kubernetes.io/component":  "vc",
			"app.kubernetes.io/version":    "v6.0.0",
			"cosmos.strange.love/network":  "testnet",
			"cosmos.strange.love/type":     "FullNode",
		}
		require.Equal(t, wantLabels, rb.Labels)

		require.Len(t, rb.Subjects, 1)
		require.Equal(t, rb.Subjects[0].Name, "hub-vc-sa")

		require.Equal(t, rb.RoleRef.Name, "hub-vc-r")
	})
}

'''
'''--- internal/fullnode/role_binding_control.go ---
package fullnode

import (
	"context"
	"fmt"

	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	"github.com/strangelove-ventures/cosmos-operator/internal/diff"
	"github.com/strangelove-ventures/cosmos-operator/internal/kube"
	rbacv1 "k8s.io/api/rbac/v1"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

// RoleBindingControl creates or updates RoleBindings.
type RoleBindingControl struct {
	client Client
}

func NewRoleBindingControl(client Client) RoleBindingControl {
	return RoleBindingControl{
		client: client,
	}
}

// Reconcile creates or updates role bindings.
func (sc RoleBindingControl) Reconcile(ctx context.Context, log kube.Logger, crd *cosmosv1.CosmosFullNode) kube.ReconcileError {
	var crs rbacv1.RoleBindingList
	if err := sc.client.List(ctx, &crs,
		client.InNamespace(crd.Namespace),
		client.MatchingLabels{
			kube.ControllerLabel: "cosmos-operator",
			kube.ComponentLabel:  "vc",
			kube.NameLabel:       appName(crd),
		},
	); err != nil {
		return kube.TransientError(fmt.Errorf("list existing role bindings: %w", err))
	}

	current := ptrSlice(crs.Items)
	want := BuildRoleBindings(crd)
	diffed := diff.New(current, want)

	for _, cr := range diffed.Creates() {
		log.Info("Creating role binding", "name", cr.Name)
		if err := ctrl.SetControllerReference(crd, cr, sc.client.Scheme()); err != nil {
			return kube.TransientError(fmt.Errorf("set controller reference on role binding %q: %w", cr.Name, err))
		}
		// CreateOrUpdate (vs. only create) fixes a bug with current deployments where updating would remove the owner reference.
		// This ensures we update the service with the owner reference.
		if err := kube.CreateOrUpdate(ctx, sc.client, cr); err != nil {
			return kube.TransientError(fmt.Errorf("create role binding %q: %w", cr.Name, err))
		}
	}

	for _, cr := range diffed.Updates() {
		log.Info("Updating role binding", "name", cr.Name)
		if err := sc.client.Update(ctx, cr); err != nil {
			return kube.TransientError(fmt.Errorf("update role binding %q: %w", cr.Name, err))
		}
	}

	return nil
}

'''
'''--- internal/fullnode/role_binding_control_test.go ---
package fullnode

import (
	"context"
	"testing"

	"github.com/stretchr/testify/require"
	rbacv1 "k8s.io/api/rbac/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

func TestRoleBindingControl_Reconcile(t *testing.T) {
	t.Parallel()

	type mockRbClient = mockClient[*rbacv1.RoleBinding]

	ctx := context.Background()

	t.Run("happy path", func(t *testing.T) {
		crd := defaultCRD()
		crd.Namespace = "test"
		crd.Spec.Replicas = 3

		var mClient mockRbClient

		control := NewRoleBindingControl(&mClient)
		err := control.Reconcile(ctx, nopReporter, &crd)
		require.NoError(t, err)

		require.Len(t, mClient.GotListOpts, 2)
		var listOpt client.ListOptions
		for _, opt := range mClient.GotListOpts {
			opt.ApplyToList(&listOpt)
		}
		require.Equal(t, "test", listOpt.Namespace)
		require.Zero(t, listOpt.Limit)

		require.Equal(t, 1, mClient.CreateCount) // Created 1 role binding.
		require.Equal(t, "osmosis-vc-rb", mClient.LastCreateObject.Name)
		require.NotEmpty(t, mClient.LastCreateObject.OwnerReferences)
		require.Equal(t, crd.Name, mClient.LastCreateObject.OwnerReferences[0].Name)
		require.Equal(t, "CosmosFullNode", mClient.LastCreateObject.OwnerReferences[0].Kind)
		require.True(t, *mClient.LastCreateObject.OwnerReferences[0].Controller)

		mClient.ObjectList = rbacv1.RoleBindingList{Items: []rbacv1.RoleBinding{
			{
				ObjectMeta: metav1.ObjectMeta{Name: "osmosis-vc-rb", Namespace: crd.Namespace},
				Subjects:   nil, // different to force update
			},
		}}

		mClient.GotListOpts = nil // reset for next reconcile

		err = control.Reconcile(ctx, nopReporter, &crd)
		require.NoError(t, err)

		require.Len(t, mClient.GotListOpts, 2)
		listOpt = client.ListOptions{}
		for _, opt := range mClient.GotListOpts {
			opt.ApplyToList(&listOpt)
		}
		require.Equal(t, "test", listOpt.Namespace)
		require.Zero(t, listOpt.Limit)

		require.Equal(t, 1, mClient.UpdateCount) // Updated 1 role binding.

		require.Zero(t, mClient.DeleteCount) // Role bindings are never deleted.
	})
}

'''
'''--- internal/fullnode/role_control.go ---
package fullnode

import (
	"context"
	"fmt"

	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	"github.com/strangelove-ventures/cosmos-operator/internal/diff"
	"github.com/strangelove-ventures/cosmos-operator/internal/kube"
	rbacv1 "k8s.io/api/rbac/v1"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

// RoleControl creates or updates Roles.
type RoleControl struct {
	client Client
}

func NewRoleControl(client Client) RoleControl {
	return RoleControl{
		client: client,
	}
}

// Reconcile creates or updates roles.
func (sc RoleControl) Reconcile(ctx context.Context, log kube.Logger, crd *cosmosv1.CosmosFullNode) kube.ReconcileError {
	var crs rbacv1.RoleList
	if err := sc.client.List(ctx, &crs,
		client.InNamespace(crd.Namespace),
		client.MatchingLabels{
			kube.ControllerLabel: "cosmos-operator",
			kube.ComponentLabel:  "vc",
			kube.NameLabel:       appName(crd),
		},
	); err != nil {
		return kube.TransientError(fmt.Errorf("list existing roles: %w", err))
	}

	current := ptrSlice(crs.Items)
	want := BuildRoles(crd)
	diffed := diff.New(current, want)

	for _, cr := range diffed.Creates() {
		log.Info("Creating role", "name", cr.Name)
		if err := ctrl.SetControllerReference(crd, cr, sc.client.Scheme()); err != nil {
			return kube.TransientError(fmt.Errorf("set controller reference on role %q: %w", cr.Name, err))
		}
		// CreateOrUpdate (vs. only create) fixes a bug with current deployments where updating would remove the owner reference.
		// This ensures we update the service with the owner reference.
		if err := kube.CreateOrUpdate(ctx, sc.client, cr); err != nil {
			return kube.TransientError(fmt.Errorf("create role %q: %w", cr.Name, err))
		}
	}

	for _, cr := range diffed.Updates() {
		log.Info("Updating role", "name", cr.Name)
		if err := sc.client.Update(ctx, cr); err != nil {
			return kube.TransientError(fmt.Errorf("update role %q: %w", cr.Name, err))
		}
	}

	return nil
}

'''
'''--- internal/fullnode/role_control_test.go ---
package fullnode

import (
	"context"
	"testing"

	"github.com/stretchr/testify/require"
	rbacv1 "k8s.io/api/rbac/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

func TestRoleControl_Reconcile(t *testing.T) {
	t.Parallel()

	type mockRoleClient = mockClient[*rbacv1.Role]

	ctx := context.Background()

	t.Run("happy path", func(t *testing.T) {
		crd := defaultCRD()
		crd.Namespace = "test"
		crd.Spec.Replicas = 3

		var mClient mockRoleClient

		control := NewRoleControl(&mClient)
		err := control.Reconcile(ctx, nopReporter, &crd)
		require.NoError(t, err)

		require.Len(t, mClient.GotListOpts, 2)
		var listOpt client.ListOptions
		for _, opt := range mClient.GotListOpts {
			opt.ApplyToList(&listOpt)
		}
		require.Equal(t, "test", listOpt.Namespace)
		require.Zero(t, listOpt.Limit)

		require.Equal(t, 1, mClient.CreateCount) // Created 1 role.
		require.Equal(t, "osmosis-vc-r", mClient.LastCreateObject.Name)
		require.NotEmpty(t, mClient.LastCreateObject.OwnerReferences)
		require.Equal(t, crd.Name, mClient.LastCreateObject.OwnerReferences[0].Name)
		require.Equal(t, "CosmosFullNode", mClient.LastCreateObject.OwnerReferences[0].Kind)
		require.True(t, *mClient.LastCreateObject.OwnerReferences[0].Controller)

		mClient.ObjectList = rbacv1.RoleList{Items: []rbacv1.Role{
			{
				ObjectMeta: metav1.ObjectMeta{Name: "osmosis-vc-r", Namespace: crd.Namespace},
				Rules:      nil, // added to force update
			},
		}}

		mClient.GotListOpts = nil // reset for next reconcile

		err = control.Reconcile(ctx, nopReporter, &crd)
		require.NoError(t, err)

		require.Len(t, mClient.GotListOpts, 2)
		listOpt = client.ListOptions{}
		for _, opt := range mClient.GotListOpts {
			opt.ApplyToList(&listOpt)
		}
		require.Equal(t, "test", listOpt.Namespace)
		require.Zero(t, listOpt.Limit)

		require.Equal(t, 1, mClient.UpdateCount) // Updated 1 role.

		require.Zero(t, mClient.DeleteCount) // Roles are never deleted.
	})
}

'''
'''--- internal/fullnode/script/download-addrbook.sh ---
set -eu

# $ADDRBOOK_FILE and $CONFIG_DIR already set via pod env vars.

ADDRBOOK_URL="$1"

echo "Downloading address book file $ADDRBOOK_URL to $ADDRBOOK_FILE..."

download_json() {
  echo "Downloading plain json..."
  wget -c -O "$ADDRBOOK_FILE" "$ADDRBOOK_URL"
}

download_jsongz() {
  echo "Downloading json.gz..."
  wget -c -O - "$ADDRBOOK_URL" | gunzip -c >"$ADDRBOOK_FILE"
}

download_tar() {
  echo "Downloading and extracting tar..."
  wget -c -O - "$ADDRBOOK_URL" | tar -x -C "$CONFIG_DIR"
}

download_targz() {
  echo "Downloading and extracting compressed tar..."
  wget -c -O - "$ADDRBOOK_URL" | tar -xz -C "$CONFIG_DIR"
}

download_zip() {
  echo "Downloading and extracting zip..."
  wget -c -O tmp_genesis.zip "$ADDRBOOK_URL"
  unzip tmp_genesis.zip
  rm tmp_genesis.zip
  mv genesis.json "$ADDRBOOK_FILE"
}

rm -f "$ADDRBOOK_FILE"

case "$ADDRBOOK_URL" in
*.json.gz) download_jsongz ;;
*.json) download_json ;;
*.tar.gz) download_targz ;;
*.tar.gzip) download_targz ;;
*.tar) download_tar ;;
*.zip) download_zip ;;
*)
  echo "Unable to handle file extension for $ADDRBOOK_URL"
  exit 1
  ;;
esac

echo "Saved address book file to $ADDRBOOK_FILE."
echo "Download address book file complete."

'''
'''--- internal/fullnode/script/download-genesis.sh ---
set -eu

# $GENESIS_FILE and $CONFIG_DIR already set via pod env vars.

GENESIS_URL="$1"

echo "Downloading genesis file $GENESIS_URL to $GENESIS_FILE..."

download_json() {
  echo "Downloading plain json..."
  wget -c -O "$GENESIS_FILE" "$GENESIS_URL"
}

download_jsongz() {
  echo "Downloading json.gz..."
  wget -c -O - "$GENESIS_URL" | gunzip -c >"$GENESIS_FILE"
}

download_tar() {
  echo "Downloading and extracting tar..."
  wget -c -O - "$GENESIS_URL" | tar -x -C "$CONFIG_DIR"
}

download_targz() {
  echo "Downloading and extracting compressed tar..."
  wget -c -O - "$GENESIS_URL" | tar -xz -C "$CONFIG_DIR"
}

download_zip() {
  echo "Downloading and extracting zip..."
  wget -c -O tmp_genesis.zip "$GENESIS_URL"
  unzip tmp_genesis.zip
  rm tmp_genesis.zip
  mv genesis.json "$GENESIS_FILE"
}

rm -f "$GENESIS_FILE"

case "$GENESIS_URL" in
*.json.gz) download_jsongz ;;
*.json) download_json ;;
*.tar.gz) download_targz ;;
*.tar.gzip) download_targz ;;
*.tar) download_tar ;;
*.zip) download_zip ;;
*)
  echo "Unable to handle file extension for $GENESIS_URL"
  exit 1
  ;;
esac

echo "Saved genesis file to $GENESIS_FILE."
echo "Download genesis file complete."

'''
'''--- internal/fullnode/script/download-snapshot.sh ---
set -eu

# $CHAIN_HOME already set via pod env vars.

SNAPSHOT_URL="$1"

echo "Downloading snapshot archive $SNAPSHOT_URL to $CHAIN_HOME..."

download_tar() {
  echo "Downloading and extracting tar..."
  wget -c -O - "$SNAPSHOT_URL" | tar -x -C "$CHAIN_HOME"
}

download_targz() {
  echo "Downloading and extracting compressed tar..."
  wget -c -O - "$SNAPSHOT_URL" | tar -xz -C "$CHAIN_HOME"
}

download_lz4() {
  echo "Downloading and extracting lz4..."
  wget -c -O - "$SNAPSHOT_URL" | lz4 -c -d | tar -x -C "$CHAIN_HOME"
}

case "$SNAPSHOT_URL" in
*.tar.lz4) download_lz4 ;;
*.tar.gzip) download_targz ;;
*.tar.gz) download_targz ;;
*.tar) download_tar ;;
*)
  echo "Unable to handle file extension for $SNAPSHOT_URL"
  exit 1
  ;;
esac

echo "Download and extract snapshot complete."

'''
'''--- internal/fullnode/script/use-init-genesis.sh ---
set -eu

# $GENESIS_FILE and $CONFIG_DIR already set via pod env vars.

INIT_GENESIS_FILE="$HOME/.tmp/config/genesis.json"

echo "Using initialized genesis file $INIT_GENESIS_FILE..."

set -x

mv "$INIT_GENESIS_FILE" "$GENESIS_FILE"

set +x

echo "Move complete."

'''
'''--- internal/fullnode/service_account_control.go ---
package fullnode

import (
	"context"
	"fmt"

	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	"github.com/strangelove-ventures/cosmos-operator/internal/diff"
	"github.com/strangelove-ventures/cosmos-operator/internal/kube"
	corev1 "k8s.io/api/core/v1"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

// ServiceControl creates or updates Services.
type ServiceAccountControl struct {
	client Client
}

func NewServiceAccountControl(client Client) ServiceAccountControl {
	return ServiceAccountControl{
		client: client,
	}
}

// Reconcile creates or updates service accounts.
func (sc ServiceAccountControl) Reconcile(ctx context.Context, log kube.Logger, crd *cosmosv1.CosmosFullNode) kube.ReconcileError {
	var sas corev1.ServiceAccountList
	if err := sc.client.List(ctx, &sas,
		client.InNamespace(crd.Namespace),
		client.MatchingLabels{
			kube.ControllerLabel: "cosmos-operator",
			kube.ComponentLabel:  "vc",
			kube.NameLabel:       appName(crd),
		},
	); err != nil {
		return kube.TransientError(fmt.Errorf("list existing service accounts: %w", err))
	}

	current := ptrSlice(sas.Items)
	want := BuildServiceAccounts(crd)
	diffed := diff.New(current, want)

	for _, sa := range diffed.Creates() {
		log.Info("Creating service account", "name", sa.Name)
		if err := ctrl.SetControllerReference(crd, sa, sc.client.Scheme()); err != nil {
			return kube.TransientError(fmt.Errorf("set controller reference on service account %q: %w", sa.Name, err))
		}
		// CreateOrUpdate (vs. only create) fixes a bug with current deployments where updating would remove the owner reference.
		// This ensures we update the service with the owner reference.
		if err := kube.CreateOrUpdate(ctx, sc.client, sa); err != nil {
			return kube.TransientError(fmt.Errorf("create service account %q: %w", sa.Name, err))
		}
	}

	for _, sa := range diffed.Updates() {
		log.Info("Updating service account", "name", sa.Name)
		if err := sc.client.Update(ctx, sa); err != nil {
			return kube.TransientError(fmt.Errorf("update service account %q: %w", sa.Name, err))
		}
	}

	return nil
}

'''
'''--- internal/fullnode/service_account_control_test.go ---
package fullnode

import (
	"context"
	"testing"

	"github.com/stretchr/testify/require"
	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

func TestServiceAccountControl_Reconcile(t *testing.T) {
	t.Parallel()

	type mockSaClient = mockClient[*corev1.ServiceAccount]

	ctx := context.Background()

	t.Run("happy path", func(t *testing.T) {
		crd := defaultCRD()
		crd.Namespace = "test"
		crd.Spec.Replicas = 3

		var mClient mockSaClient

		control := NewServiceAccountControl(&mClient)
		err := control.Reconcile(ctx, nopReporter, &crd)
		require.NoError(t, err)

		require.Len(t, mClient.GotListOpts, 2)
		var listOpt client.ListOptions
		for _, opt := range mClient.GotListOpts {
			opt.ApplyToList(&listOpt)
		}
		require.Equal(t, "test", listOpt.Namespace)
		require.Zero(t, listOpt.Limit)

		require.Equal(t, 1, mClient.CreateCount) // Created 1 service account.
		require.Equal(t, "osmosis-vc-sa", mClient.LastCreateObject.Name)
		require.NotEmpty(t, mClient.LastCreateObject.OwnerReferences)
		require.Equal(t, crd.Name, mClient.LastCreateObject.OwnerReferences[0].Name)
		require.Equal(t, "CosmosFullNode", mClient.LastCreateObject.OwnerReferences[0].Kind)
		require.True(t, *mClient.LastCreateObject.OwnerReferences[0].Controller)

		mClient.ObjectList = corev1.ServiceAccountList{Items: []corev1.ServiceAccount{
			{
				ObjectMeta:                   metav1.ObjectMeta{Name: "osmosis-vc-sa", Namespace: crd.Namespace},
				AutomountServiceAccountToken: ptr(true), // added to force update
			},
		}}

		mClient.GotListOpts = nil // reset for next reconcile

		err = control.Reconcile(ctx, nopReporter, &crd)
		require.NoError(t, err)

		require.Len(t, mClient.GotListOpts, 2)
		listOpt = client.ListOptions{}
		for _, opt := range mClient.GotListOpts {
			opt.ApplyToList(&listOpt)
		}
		require.Equal(t, "test", listOpt.Namespace)
		require.Zero(t, listOpt.Limit)

		require.Equal(t, 1, mClient.UpdateCount) // Updated 1 service account.

		require.Zero(t, mClient.DeleteCount) // Service accounts are never deleted.
	})
}

'''
'''--- internal/fullnode/service_builder.go ---
package fullnode

import (
	"fmt"

	"github.com/samber/lo"
	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	"github.com/strangelove-ventures/cosmos-operator/internal/diff"
	"github.com/strangelove-ventures/cosmos-operator/internal/kube"
	corev1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/util/intstr"
)

const maxP2PServiceDefault = int32(1)

// BuildServices returns a list of services given the crd.
//
// Creates a single RPC service, likely for use with an Ingress.
//
// Creates 1 p2p service per pod. P2P diverges from traditional web and kubernetes architecture which calls for a single
// p2p service backed by multiple pods.
// Pods may be in various states even with proper readiness probes.
// Therefore, we do not want to confuse or disrupt peer exchange (PEX) within CometBFT.
// If using a single p2p service, an outside peer discovering a pod out of sync it could be
// interpreted as byzantine behavior if the peer previously connected to a pod that was in sync through the same
// external address.
func BuildServices(crd *cosmosv1.CosmosFullNode) []diff.Resource[*corev1.Service] {
	max := maxP2PServiceDefault
	if v := crd.Spec.Service.MaxP2PExternalAddresses; v != nil {
		max = *v
	}
	maxExternal := lo.Clamp(max, 0, crd.Spec.Replicas)
	p2ps := make([]diff.Resource[*corev1.Service], crd.Spec.Replicas)

	for i := int32(0); i < crd.Spec.Replicas; i++ {
		ordinal := i
		var svc corev1.Service
		svc.Name = p2pServiceName(crd, ordinal)
		svc.Namespace = crd.Namespace
		svc.Kind = "Service"
		svc.APIVersion = "v1"

		svc.Labels = defaultLabels(crd,
			kube.InstanceLabel, instanceName(crd, ordinal),
			kube.ComponentLabel, "p2p",
		)
		svc.Annotations = map[string]string{}

		svc.Spec.Ports = []corev1.ServicePort{
			{
				Name:       "p2p",
				Protocol:   corev1.ProtocolTCP,
				Port:       p2pPort,
				TargetPort: intstr.FromString("p2p"),
			},
		}
		svc.Spec.Selector = map[string]string{kube.InstanceLabel: instanceName(crd, ordinal)}

		if i < maxExternal {
			preserveMergeInto(svc.Labels, crd.Spec.Service.P2PTemplate.Metadata.Labels)
			preserveMergeInto(svc.Annotations, crd.Spec.Service.P2PTemplate.Metadata.Annotations)
			svc.Spec.Type = *valOrDefault(crd.Spec.Service.P2PTemplate.Type, ptr(corev1.ServiceTypeLoadBalancer))
			svc.Spec.ExternalTrafficPolicy = *valOrDefault(crd.Spec.Service.P2PTemplate.ExternalTrafficPolicy, ptr(corev1.ServiceExternalTrafficPolicyTypeLocal))
		} else {
			svc.Spec.Type = corev1.ServiceTypeClusterIP
		}

		p2ps[i] = diff.Adapt(&svc, i)
	}

	rpc := rpcService(crd)
	jsonRpc := jsonRpcService(crd)
	jsonRpcWs := jsonRpcWsService(crd)

	services := append(p2ps, diff.Adapt(rpc, len(p2ps)))
	services = append(services, diff.Adapt(jsonRpc, len(services)))
	services = append(services, diff.Adapt(jsonRpcWs, len(services)))
	return services
}

func rpcService(crd *cosmosv1.CosmosFullNode) *corev1.Service {
	var svc corev1.Service
	svc.Name = rpcServiceName(crd)
	svc.Namespace = crd.Namespace
	svc.Kind = "Service"
	svc.APIVersion = "v1"
	svc.Labels = defaultLabels(crd,
		kube.ComponentLabel, "rpc",
	)
	svc.Annotations = map[string]string{}

	svc.Spec.Ports = []corev1.ServicePort{
		{
			Name:       "api",
			Protocol:   corev1.ProtocolTCP,
			Port:       apiPort,
			TargetPort: intstr.FromString("api"),
		},
		{
			Name:       "rosetta",
			Protocol:   corev1.ProtocolTCP,
			Port:       rosettaPort,
			TargetPort: intstr.FromString("rosetta"),
		},
		{
			Name:       "grpc",
			Protocol:   corev1.ProtocolTCP,
			Port:       grpcPort,
			TargetPort: intstr.FromString("grpc"),
		},
		{
			Name:       "rpc",
			Protocol:   corev1.ProtocolTCP,
			Port:       rpcPort,
			TargetPort: intstr.FromString("rpc"),
		},
		{
			Name:       "grpc-web",
			Protocol:   corev1.ProtocolTCP,
			Port:       grpcWebPort,
			TargetPort: intstr.FromString("grpc-web"),
		},
	}

	svc.Spec.Selector = map[string]string{kube.NameLabel: appName(crd)}
	svc.Spec.Type = corev1.ServiceTypeClusterIP

	rpcSpec := crd.Spec.Service.RPCTemplate
	preserveMergeInto(svc.Labels, rpcSpec.Metadata.Labels)
	preserveMergeInto(svc.Annotations, rpcSpec.Metadata.Annotations)
	kube.NormalizeMetadata(&svc.ObjectMeta)

	if v := rpcSpec.ExternalTrafficPolicy; v != nil {
		svc.Spec.ExternalTrafficPolicy = *v
	}
	if v := rpcSpec.Type; v != nil {
		svc.Spec.Type = *v
	}

	return &svc
}

func jsonRpcService(crd *cosmosv1.CosmosFullNode) *corev1.Service {
	var svc corev1.Service
	svc.Name = jsonRpcServiceName(crd)
	svc.Namespace = crd.Namespace
	svc.Kind = "Service"
	svc.APIVersion = "v1"
	svc.Labels = defaultLabels(crd,
		kube.ComponentLabel, "json-rpc",
	)
	svc.Annotations = map[string]string{}

	svc.Spec.Ports = []corev1.ServicePort{
		{
			Name:       "json-rpc",
			Protocol:   corev1.ProtocolTCP,
			Port:       jsonRpcPort,
			TargetPort: intstr.FromString("json-rpc"),
		},
	}

	svc.Spec.Selector = map[string]string{kube.NameLabel: appName(crd)}
	svc.Spec.Type = corev1.ServiceTypeClusterIP

	rpcSpec := crd.Spec.Service.RPCTemplate
	preserveMergeInto(svc.Labels, rpcSpec.Metadata.Labels)
	preserveMergeInto(svc.Annotations, rpcSpec.Metadata.Annotations)
	kube.NormalizeMetadata(&svc.ObjectMeta)

	if v := rpcSpec.ExternalTrafficPolicy; v != nil {
		svc.Spec.ExternalTrafficPolicy = *v
	}
	if v := rpcSpec.Type; v != nil {
		svc.Spec.Type = *v
	}

	return &svc
}

func jsonRpcWsService(crd *cosmosv1.CosmosFullNode) *corev1.Service {
	var svc corev1.Service
	svc.Name = jsonRpcWsServiceName(crd)
	svc.Namespace = crd.Namespace
	svc.Kind = "Service"
	svc.APIVersion = "v1"
	svc.Labels = defaultLabels(crd,
		kube.ComponentLabel, "json-rpc-ws",
	)
	svc.Annotations = map[string]string{}

	svc.Spec.Ports = []corev1.ServicePort{
		{
			Name:       "json-rpc-ws",
			Protocol:   corev1.ProtocolTCP,
			Port:       jsonRpcWsPort,
			TargetPort: intstr.FromString("json-rpc-ws"),
		},
	}

	svc.Spec.Selector = map[string]string{kube.NameLabel: appName(crd)}
	svc.Spec.Type = corev1.ServiceTypeClusterIP

	rpcSpec := crd.Spec.Service.RPCTemplate
	preserveMergeInto(svc.Labels, rpcSpec.Metadata.Labels)
	preserveMergeInto(svc.Annotations, rpcSpec.Metadata.Annotations)
	kube.NormalizeMetadata(&svc.ObjectMeta)

	if v := rpcSpec.ExternalTrafficPolicy; v != nil {
		svc.Spec.ExternalTrafficPolicy = *v
	}
	if v := rpcSpec.Type; v != nil {
		svc.Spec.Type = *v
	}

	return &svc
}

func p2pServiceName(crd *cosmosv1.CosmosFullNode, ordinal int32) string {
	return fmt.Sprintf("%s-p2p-%d", appName(crd), ordinal)
}

func rpcServiceName(crd *cosmosv1.CosmosFullNode) string {
	return fmt.Sprintf("%s-rpc", appName(crd))
}

func jsonRpcServiceName(crd *cosmosv1.CosmosFullNode) string {
	return fmt.Sprintf("%s-json-rpc", appName(crd))
}

func jsonRpcWsServiceName(crd *cosmosv1.CosmosFullNode) string {
	return fmt.Sprintf("%s-json-rpc-ws", appName(crd))
}

'''
'''--- internal/fullnode/service_builder_test.go ---
package fullnode

import (
	"fmt"
	"strings"
	"testing"

	"github.com/samber/lo"
	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	"github.com/strangelove-ventures/cosmos-operator/internal/diff"
	"github.com/strangelove-ventures/cosmos-operator/internal/kube"
	"github.com/strangelove-ventures/cosmos-operator/internal/test"
	"github.com/stretchr/testify/require"
	corev1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/util/intstr"
)

func TestBuildServices(t *testing.T) {
	t.Parallel()

	t.Run("p2p services", func(t *testing.T) {
		crd := defaultCRD()
		crd.Spec.Replicas = 3
		crd.Name = "terra"
		crd.Namespace = "test"
		crd.Spec.ChainSpec.Network = "testnet"
		crd.Spec.PodTemplate.Image = "terra:v6.0.0"

		svcs := BuildServices(&crd)

		require.Equal(t, 4, len(svcs)) // 3 p2p services + 1 rpc service

		for i, svc := range svcs[:3] {
			p2p := svc.Object()
			require.Equal(t, fmt.Sprintf("terra-p2p-%d", i), p2p.Name)
			require.Equal(t, "test", p2p.Namespace)

			wantLabels := map[string]string{
				"app.kubernetes.io/created-by": "cosmos-operator",
				"app.kubernetes.io/name":       "terra",
				"app.kubernetes.io/component":  "p2p",
				"app.kubernetes.io/version":    "v6.0.0",
				"app.kubernetes.io/instance":   fmt.Sprintf("terra-%d", i),
				"cosmos.strange.love/network":  "testnet",
				"cosmos.strange.love/type":     "FullNode",
			}
			require.Equal(t, wantLabels, p2p.Labels)

			wantSpec := corev1.ServiceSpec{
				Ports: []corev1.ServicePort{
					{
						Name:       "p2p",
						Protocol:   corev1.ProtocolTCP,
						Port:       26656,
						TargetPort: intstr.FromString("p2p"),
					},
				},
				Selector: map[string]string{"app.kubernetes.io/instance": fmt.Sprintf("terra-%d", i)},
				Type:     corev1.ServiceTypeClusterIP,
			}
			// By default, expose the first p2p service publicly.
			if i == 0 {
				wantSpec.Type = corev1.ServiceTypeLoadBalancer
				wantSpec.ExternalTrafficPolicy = corev1.ServiceExternalTrafficPolicyTypeLocal
			}

			require.Equal(t, wantSpec, p2p.Spec)
		}
	})

	t.Run("p2p max external addresses", func(t *testing.T) {
		crd := defaultCRD()
		crd.Spec.Replicas = 3
		crd.Spec.Service.MaxP2PExternalAddresses = ptr(int32(2))

		svcs := BuildServices(&crd)

		gotP2P := lo.Filter(svcs, func(s diff.Resource[*corev1.Service], _ int) bool {
			return s.Object().Labels[kube.ComponentLabel] == "p2p"
		})

		require.Equal(t, 3, len(gotP2P))
		for i, svc := range gotP2P[:2] {
			p2p := svc.Object()
			require.Equal(t, corev1.ServiceTypeLoadBalancer, p2p.Spec.Type, i)
			require.Equal(t, corev1.ServiceExternalTrafficPolicyTypeLocal, p2p.Spec.ExternalTrafficPolicy, i)
		}

		got := gotP2P[2].Object()
		require.Equal(t, corev1.ServiceTypeClusterIP, got.Spec.Type)
		require.Empty(t, got.Spec.ExternalTrafficPolicy)
	})

	t.Run("zero p2p max external addresses", func(t *testing.T) {
		crd := defaultCRD()
		crd.Spec.Replicas = 3
		crd.Spec.Service.MaxP2PExternalAddresses = ptr(int32(0))
		// These overrides should be ignored.
		crd.Spec.Service.P2PTemplate = cosmosv1.ServiceOverridesSpec{
			Metadata: cosmosv1.Metadata{
				Labels: map[string]string{"test": "should not see me"},
			},
			Type:                  ptr(corev1.ServiceTypeNodePort),
			ExternalTrafficPolicy: ptr(corev1.ServiceExternalTrafficPolicyTypeLocal),
		}

		svcs := BuildServices(&crd)

		gotP2P := lo.Filter(svcs, func(s diff.Resource[*corev1.Service], _ int) bool {
			return s.Object().Labels[kube.ComponentLabel] == "p2p"
		})

		require.Equal(t, 3, len(gotP2P))
		for i, svc := range gotP2P {
			p2p := svc.Object()
			require.Empty(t, p2p.Labels["test"])
			require.Equal(t, corev1.ServiceTypeClusterIP, p2p.Spec.Type, i)
			require.Empty(t, p2p.Spec.ExternalTrafficPolicy, i)
		}
	})

	t.Run("p2p services with overrides", func(t *testing.T) {
		crd := defaultCRD()
		crd.Spec.Replicas = 2
		crd.Name = "terra"
		crd.Spec.Service.MaxP2PExternalAddresses = ptr(int32(2))
		crd.Spec.Service.P2PTemplate = cosmosv1.ServiceOverridesSpec{
			Metadata: cosmosv1.Metadata{
				Labels:      map[string]string{"test": "value1", "app.kubernetes.io/name": "should not see me"},
				Annotations: map[string]string{"test": "value2", "app.kubernetes.io/ordinal": "should not see me"},
			},
			Type:                  ptr(corev1.ServiceTypeNodePort),
			ExternalTrafficPolicy: ptr(corev1.ServiceExternalTrafficPolicyTypeLocal),
		}
		svcs := BuildServices(&crd)

		require.Equal(t, 3, len(svcs)) // 2 p2p services + 1 rpc service

		for i, svc := range svcs[:2] {
			p2p := svc.Object()
			require.Equal(t, fmt.Sprintf("terra-p2p-%d", i), p2p.Name)

			require.Equal(t, "value1", p2p.Labels["test"])
			require.NotEqual(t, "should not see me", p2p.Labels["app.kubernetes.io/name"])

			require.Equal(t, "value2", p2p.Annotations["test"])
			require.NotEqual(t, "should not see me", p2p.Labels["app.kubernetes.io/ordinal"])

			wantSpec := corev1.ServiceSpec{
				Ports: []corev1.ServicePort{
					{
						Name:       "p2p",
						Protocol:   corev1.ProtocolTCP,
						Port:       26656,
						TargetPort: intstr.FromString("p2p"),
					},
				},
				Selector:              map[string]string{"app.kubernetes.io/instance": fmt.Sprintf("terra-%d", i)},
				Type:                  corev1.ServiceTypeNodePort,
				ExternalTrafficPolicy: corev1.ServiceExternalTrafficPolicyTypeLocal,
			}

			require.Equal(t, wantSpec, p2p.Spec)
		}
	})

	t.Run("rpc service", func(t *testing.T) {
		crd := defaultCRD()
		crd.Spec.Replicas = 1
		crd.Name = "terra"
		crd.Namespace = "test"
		crd.Spec.ChainSpec.Network = "testnet"
		crd.Spec.PodTemplate.Image = "terra:v6.0.0"
		svcs := BuildServices(&crd)

		require.Equal(t, 2, len(svcs)) // Includes single p2p service.

		rpc := svcs[1].Object()
		require.Equal(t, "terra-rpc", rpc.Name)
		require.Equal(t, "test", rpc.Namespace)
		require.Equal(t, corev1.ServiceTypeClusterIP, rpc.Spec.Type)
		require.Equal(t, map[string]string{"app.kubernetes.io/name": "terra"}, rpc.Spec.Selector)

		wantLabels := map[string]string{
			"app.kubernetes.io/created-by": "cosmos-operator",
			"app.kubernetes.io/name":       "terra",
			"app.kubernetes.io/component":  "rpc",
			"app.kubernetes.io/version":    "v6.0.0",
			"cosmos.strange.love/network":  "testnet",
			"cosmos.strange.love/type":     "FullNode",
		}
		require.Equal(t, wantLabels, rpc.Labels)

		require.Equal(t, 5, len(rpc.Spec.Ports))
		// All ports minus prometheus and p2p.
		want := []corev1.ServicePort{
			{
				Name:       "api",
				Protocol:   corev1.ProtocolTCP,
				Port:       1317,
				TargetPort: intstr.FromString("api"),
			},
			{
				Name:       "rosetta",
				Protocol:   corev1.ProtocolTCP,
				Port:       8080,
				TargetPort: intstr.FromString("rosetta"),
			},
			{
				Name:       "grpc",
				Protocol:   corev1.ProtocolTCP,
				Port:       9090,
				TargetPort: intstr.FromString("grpc"),
			},
			{
				Name:       "rpc",
				Protocol:   corev1.ProtocolTCP,
				Port:       26657,
				TargetPort: intstr.FromString("rpc"),
			},
			{
				Name:       "grpc-web",
				Protocol:   corev1.ProtocolTCP,
				Port:       9091,
				TargetPort: intstr.FromString("grpc-web"),
			},
			{
                                Name:       "json-rpc",
                                Protocol:   corev1.ProtocolTCP,
                                Port:       8545,
                                TargetPort: intstr.FromString("json-rpc"),
                        },
			{
                                Name:       "json-rpc-ws",
                                Protocol:   corev1.ProtocolTCP,
                                Port:       8546,
                                TargetPort: intstr.FromString("json-rpc-ws"),
                        },
		}

		require.Equal(t, want, rpc.Spec.Ports)
	})

	t.Run("rpc service with overrides", func(t *testing.T) {
		crd := defaultCRD()
		crd.Spec.Replicas = 0
		crd.Name = "terra"
		crd.Namespace = "test"
		crd.Spec.ChainSpec.Network = "testnet"
		crd.Spec.PodTemplate.Image = "terra:v6.0.0"
		crd.Spec.Service.RPCTemplate = cosmosv1.ServiceOverridesSpec{
			Metadata: cosmosv1.Metadata{
				Labels:      map[string]string{"label": "value", "app.kubernetes.io/name": "should not see me"},
				Annotations: map[string]string{"test": "value"},
			},
			Type:                  ptr(corev1.ServiceTypeNodePort),
			ExternalTrafficPolicy: ptr(corev1.ServiceExternalTrafficPolicyTypeLocal),
		}
		svcs := BuildServices(&crd)

		rpc := svcs[0].Object()
		require.Equal(t, map[string]string{"test": "value"}, rpc.Annotations)

		require.Equal(t, "value", rpc.Labels["label"])
		require.Equal(t, "terra", rpc.Labels["app.kubernetes.io/name"])

		require.Equal(t, corev1.ServiceExternalTrafficPolicyTypeLocal, rpc.Spec.ExternalTrafficPolicy)
		require.Equal(t, corev1.ServiceTypeNodePort, rpc.Spec.Type)
	})

	t.Run("long name", func(t *testing.T) {
		crd := defaultCRD()
		name := strings.Repeat("Long", 500)
		crd.Name = name

		svcs := BuildServices(&crd)

		for _, svc := range svcs {
			test.RequireValidMetadata(t, svc.Object())
		}
	})

	test.HasTypeLabel(t, func(crd cosmosv1.CosmosFullNode) []map[string]string {
		svcs := BuildServices(&crd)
		labels := make([]map[string]string, 0)
		for _, svc := range svcs {
			labels = append(labels, svc.Object().Labels)
		}
		return labels
	})
}

'''
'''--- internal/fullnode/service_control.go ---
package fullnode

import (
	"context"
	"fmt"

	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	"github.com/strangelove-ventures/cosmos-operator/internal/diff"
	"github.com/strangelove-ventures/cosmos-operator/internal/kube"
	corev1 "k8s.io/api/core/v1"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

// ServiceControl creates or updates Services.
type ServiceControl struct {
	client Client
}

func NewServiceControl(client Client) ServiceControl {
	return ServiceControl{
		client: client,
	}
}

// Reconcile creates or updates services.
// Some services, like P2P, reserve public addresses of which should not change.
// Therefore, services are never deleted unless the CRD itself is deleted.
func (sc ServiceControl) Reconcile(ctx context.Context, log kube.Logger, crd *cosmosv1.CosmosFullNode) kube.ReconcileError {
	var svcs corev1.ServiceList
	if err := sc.client.List(ctx, &svcs,
		client.InNamespace(crd.Namespace),
		client.MatchingFields{kube.ControllerOwnerField: crd.Name},
	); err != nil {
		return kube.TransientError(fmt.Errorf("list existing services: %w", err))
	}

	current := ptrSlice(svcs.Items)
	want := BuildServices(crd)
	diffed := diff.New(current, want)

	for _, svc := range diffed.Creates() {
		log.Info("Creating service", "svcName", svc.Name)
		if err := ctrl.SetControllerReference(crd, svc, sc.client.Scheme()); err != nil {
			return kube.TransientError(fmt.Errorf("set controller reference on service %q: %w", svc.Name, err))
		}
		// CreateOrUpdate (vs. only create) fixes a bug with current deployments where updating would remove the owner reference.
		// This ensures we update the service with the owner reference.
		if err := kube.CreateOrUpdate(ctx, sc.client, svc); err != nil {
			return kube.TransientError(fmt.Errorf("create service %q: %w", svc.Name, err))
		}
	}

	for _, svc := range diffed.Updates() {
		log.Info("Updating service", "svcName", svc.Name)
		if err := sc.client.Update(ctx, svc); err != nil {
			return kube.TransientError(fmt.Errorf("update service %q: %w", svc.Name, err))
		}
	}

	return nil
}

'''
'''--- internal/fullnode/service_control_test.go ---
package fullnode

import (
	"context"
	"testing"

	"github.com/stretchr/testify/require"
	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

func TestServiceControl_Reconcile(t *testing.T) {
	t.Parallel()

	type mockSvcClient = mockClient[*corev1.Service]

	ctx := context.Background()

	t.Run("happy path", func(t *testing.T) {
		crd := defaultCRD()
		crd.Namespace = "test"
		crd.Spec.Replicas = 3
		crd.Spec.Service.MaxP2PExternalAddresses = ptr(int32(2)) // Causes 1 p2p service to be created.

		var mClient mockSvcClient
		mClient.ObjectList = corev1.ServiceList{Items: []corev1.Service{
			{ObjectMeta: metav1.ObjectMeta{Name: "osmosis-p2p-0", Namespace: crd.Namespace}}, // update
			{ObjectMeta: metav1.ObjectMeta{Name: "osmosis-rpc", Namespace: crd.Namespace}},   // update
			{ObjectMeta: metav1.ObjectMeta{Name: "osmosis-99", Namespace: crd.Namespace}},    // Tests we never delete services.
		}}

		control := NewServiceControl(&mClient)
		err := control.Reconcile(ctx, nopReporter, &crd)
		require.NoError(t, err)

		require.Len(t, mClient.GotListOpts, 2)
		var listOpt client.ListOptions
		for _, opt := range mClient.GotListOpts {
			opt.ApplyToList(&listOpt)
		}
		require.Equal(t, "test", listOpt.Namespace)
		require.Zero(t, listOpt.Limit)
		require.Equal(t, ".metadata.controller=osmosis", listOpt.FieldSelector.String())

		require.Equal(t, 2, mClient.CreateCount) // Created 2 p2p services.
		require.Equal(t, "osmosis-p2p-2", mClient.LastCreateObject.Name)
		require.NotEmpty(t, mClient.LastCreateObject.OwnerReferences)
		require.Equal(t, crd.Name, mClient.LastCreateObject.OwnerReferences[0].Name)
		require.Equal(t, "CosmosFullNode", mClient.LastCreateObject.OwnerReferences[0].Kind)
		require.True(t, *mClient.LastCreateObject.OwnerReferences[0].Controller)

		require.Equal(t, 2, mClient.UpdateCount)
		require.Zero(t, mClient.DeleteCount) // Services are never deleted.
	})
}

'''
'''--- internal/fullnode/snapshot.go ---
package fullnode

import (
	_ "embed"
	"errors"
	"fmt"

	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
)

var (
	//go:embed script/download-snapshot.sh
	scriptDownloadSnapshot string
)

// There are other files in the DATA_DIR that we must not touch, so we only test for the existence of database files.
const snapshotScriptWrapper = `set -eu
if test -n "$(find $DATA_DIR -maxdepth 1 -name '*.db' -print -quit)"; then
	echo "Databases in $DATA_DIR already exists; skipping initialization."
	exit 0
fi

%s

echo "$DATA_DIR initialized."
`

// DownloadSnapshotCommand returns a command and args for downloading and restoring from a snapshot.
func DownloadSnapshotCommand(cfg cosmosv1.ChainSpec) (string, []string) {
	args := []string{"-c"}
	switch {
	case cfg.SnapshotScript != nil:
		args = append(args, fmt.Sprintf(snapshotScriptWrapper, *cfg.SnapshotScript))
	case cfg.SnapshotURL != nil:
		args = append(args, fmt.Sprintf(snapshotScriptWrapper, scriptDownloadSnapshot), "-s", *cfg.SnapshotURL)
	default:
		panic(errors.New("attempted to restore from a snapshot but snapshots are not configured"))
	}

	return "sh", args
}

'''
'''--- internal/fullnode/snapshot_test.go ---
package fullnode

import (
	"testing"

	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	"github.com/stretchr/testify/require"
)

func TestDownloadSnapshotCommand(t *testing.T) {
	t.Parallel()

	const (
		testURL         = "https://example.com/archive.tar"
		wantIfStatement = `if test -n "$(find $DATA_DIR -maxdepth 1 -name '*.db' -print -quit)"; then
	echo "Databases in $DATA_DIR already exists; skipping initialization."
	exit 0
fi`
	)
	t.Run("snapshot url", func(t *testing.T) {
		var cfg cosmosv1.ChainSpec
		cfg.SnapshotURL = ptr(testURL)

		cmd, args := DownloadSnapshotCommand(cfg)
		require.Equal(t, "sh", cmd)

		require.Len(t, args, 4)

		require.Equal(t, "-c", args[0])

		script := args[1]
		require.Contains(t, script, wantIfStatement)
		require.Contains(t, script, `SNAPSHOT_URL`)

		require.Equal(t, "-s", args[2])
		require.Equal(t, testURL, args[3])
	})

	t.Run("snapshot script", func(t *testing.T) {
		var cfg cosmosv1.ChainSpec
		cfg.SnapshotURL = ptr(testURL) // Asserts SnapshotScript takes precedence.
		cfg.SnapshotScript = ptr("echo hello")

		_, args := DownloadSnapshotCommand(cfg)
		require.Len(t, args, 2)

		require.Equal(t, "-c", args[0])

		got := args[1]
		require.Contains(t, got, wantIfStatement)
		require.NotContains(t, got, "SNAPSHOT_URL")
		require.Contains(t, got, "echo hello")
	})

	t.Run("zero state", func(t *testing.T) {
		var cfg cosmosv1.ChainSpec
		require.Panics(t, func() {
			DownloadSnapshotCommand(cfg)
		})
	})
}

'''
'''--- internal/fullnode/status.go ---
package fullnode

import (
	"context"

	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	"github.com/strangelove-ventures/cosmos-operator/internal/cosmos"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

// ResetStatus is used at the beginning of the reconcile loop.
// It resets the crd's status to a fresh state.
func ResetStatus(crd *cosmosv1.CosmosFullNode) {
	crd.Status.ObservedGeneration = crd.Generation
	crd.Status.Phase = cosmosv1.FullNodePhaseProgressing
	crd.Status.StatusMessage = nil
}

type StatusCollector interface {
	Collect(ctx context.Context, controller client.ObjectKey) cosmos.StatusCollection
}

// SyncInfoStatus returns the status of the full node's sync info.
func SyncInfoStatus(
	ctx context.Context,
	crd *cosmosv1.CosmosFullNode,
	collector StatusCollector,
) map[string]*cosmosv1.SyncInfoPodStatus {
	status := make(map[string]*cosmosv1.SyncInfoPodStatus, crd.Spec.Replicas)

	coll := collector.Collect(ctx, client.ObjectKeyFromObject(crd))

	for _, item := range coll {
		var stat cosmosv1.SyncInfoPodStatus
		podName := item.GetPod().Name
		stat.Timestamp = metav1.NewTime(item.Timestamp())
		comet, err := item.GetStatus()
		if err != nil {
			stat.Error = ptr(err.Error())
			status[podName] = &stat
			continue
		}
		stat.Height = ptr(comet.LatestBlockHeight())
		stat.InSync = ptr(!comet.Result.SyncInfo.CatchingUp)
		status[podName] = &stat
	}

	return status
}

'''
'''--- internal/fullnode/status_client.go ---
package fullnode

import (
	"context"
	"sync"

	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

type semaphore chan struct{}

func newSem() semaphore {
	return make(semaphore, 1)
}

func (s semaphore) Acquire() {
	s <- struct{}{}
}

func (s semaphore) Release() {
	<-s
}

type StatusClient struct {
	sems   sync.Map
	client client.Client
}

func NewStatusClient(c client.Client) *StatusClient {
	return &StatusClient{client: c}
}

// SyncUpdate synchronizes updates to a CosmosFullNode's status subresource per client.ObjectKey.
// There are several controllers that update a fullnode's status to signal the fullnode controller to take action
// and update the cluster state.
//
// This method minimizes accidentally overwriting status fields by several actors.
//
// Server-side-apply, in theory, would be a solution. During testing, however, it resulted in many conflict errors
// and would require non-trivial migration to clear existing deployment's metadata.managedFields.
func (client *StatusClient) SyncUpdate(ctx context.Context, key client.ObjectKey, update func(status *cosmosv1.FullNodeStatus)) error {
	sem, _ := client.sems.LoadOrStore(key, newSem())
	sem.(semaphore).Acquire()
	defer sem.(semaphore).Release()

	var crd cosmosv1.CosmosFullNode
	if err := client.client.Get(ctx, key, &crd); err != nil {
		return err
	}

	update(&crd.Status)

	return client.client.Status().Update(ctx, &crd)
}

'''
'''--- internal/fullnode/status_client_test.go ---
package fullnode

import (
	"context"
	"errors"
	"testing"

	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	"github.com/stretchr/testify/require"
	"golang.org/x/sync/errgroup"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

type threadUnsafeClient struct {
	client.Client
	UpdateCount int
}

func (t *threadUnsafeClient) Get(ctx context.Context, key client.ObjectKey, obj client.Object, opts ...client.GetOption) error {
	return nil
}

func (t *threadUnsafeClient) Update(ctx context.Context, obj client.Object, opts ...client.UpdateOption) error {
	t.UpdateCount++
	return nil
}

func (t *threadUnsafeClient) Status() client.StatusWriter { return t }

func TestStatusClient_SyncUpdate(t *testing.T) {
	type mClient = mockClient[*cosmosv1.CosmosFullNode]

	ctx := context.Background()

	t.Run("happy path", func(t *testing.T) {
		var (
			mock    mClient
			stubCRD cosmosv1.CosmosFullNode
		)
		stubCRD.Status.Phase = "test-phase"
		stubCRD.Name = "test"
		stubCRD.Namespace = "default"
		mock.Object = stubCRD

		c := NewStatusClient(&mock)
		key := client.ObjectKey{Name: "test", Namespace: "default"}
		msg := ptr("Here's test message")
		err := c.SyncUpdate(ctx, key, func(status *cosmosv1.FullNodeStatus) {
			status.StatusMessage = msg
		})

		require.NoError(t, err)

		require.Equal(t, key, mock.GetObjectKey)
		require.Equal(t, 1, mock.UpdateCount)

		updated := mock.LastUpdateObject
		want := stubCRD.DeepCopy()
		want.Status.StatusMessage = msg
		require.Equal(t, want.ObjectMeta, updated.ObjectMeta)
		require.Equal(t, want.Status, updated.Status)
	})

	t.Run("concurrency", func(t *testing.T) {
		var mock threadUnsafeClient
		c := NewStatusClient(&mock)
		key := client.ObjectKey{Name: "test", Namespace: "default"}
		const total = 10
		var eg errgroup.Group
		for i := 0; i < total; i++ {
			eg.Go(func() error {
				return c.SyncUpdate(ctx, key, func(status *cosmosv1.FullNodeStatus) {})
			})
		}

		require.NoError(t, eg.Wait())
		require.Equal(t, 10, mock.UpdateCount)
	})

	t.Run("get error", func(t *testing.T) {
		var (
			mock mClient
		)
		mock.GetObjectErr = errors.New("get boom")

		c := NewStatusClient(&mock)
		key := client.ObjectKey{Name: "test", Namespace: "default"}
		err := c.SyncUpdate(ctx, key, nil)

		require.Error(t, err)
		require.EqualError(t, err, "get boom")
		require.Nil(t, mock.LastUpdateObject)
	})

	t.Run("update error", func(t *testing.T) {
		var (
			mock    mClient
			stubCRD cosmosv1.CosmosFullNode
		)
		mock.Object = stubCRD
		mock.UpdateErr = errors.New("update boom")

		c := NewStatusClient(&mock)
		key := client.ObjectKey{Name: "test", Namespace: "default"}
		err := c.SyncUpdate(ctx, key, func(status *cosmosv1.FullNodeStatus) {})

		require.Error(t, err)
		require.EqualError(t, err, "update boom")
	})
}

'''
'''--- internal/fullnode/status_test.go ---
package fullnode

import (
	"context"
	"errors"
	"testing"
	"time"

	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	"github.com/strangelove-ventures/cosmos-operator/internal/cosmos"
	"github.com/stretchr/testify/require"
	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

func TestResetStatus(t *testing.T) {
	t.Parallel()

	var crd cosmosv1.CosmosFullNode
	crd.Generation = 123
	crd.Status.StatusMessage = ptr("should not see me")
	crd.Status.Phase = "should not see me"
	ResetStatus(&crd)

	require.EqualValues(t, 123, crd.Status.ObservedGeneration)
	require.Nil(t, crd.Status.StatusMessage)
	require.Equal(t, cosmosv1.FullNodePhaseProgressing, crd.Status.Phase)
}

type mockStatusCollector struct {
	CollectFn func(ctx context.Context, controller client.ObjectKey) cosmos.StatusCollection
}

func (m mockStatusCollector) Collect(ctx context.Context, controller client.ObjectKey) cosmos.StatusCollection {
	return m.CollectFn(ctx, controller)
}

func TestSyncInfoStatus(t *testing.T) {
	t.Parallel()

	const (
		name      = "agoric"
		namespace = "default"
	)

	var crd cosmosv1.CosmosFullNode
	crd.Name = name
	crd.Namespace = namespace

	ts := time.Now()

	var collector mockStatusCollector
	collector.CollectFn = func(ctx context.Context, controller client.ObjectKey) cosmos.StatusCollection {
		require.NotNil(t, ctx)
		require.Equal(t, name, controller.Name)
		require.Equal(t, namespace, controller.Namespace)

		var notInSync cosmos.CometStatus
		notInSync.Result.SyncInfo.CatchingUp = true
		notInSync.Result.SyncInfo.LatestBlockHeight = "9999"

		var inSync cosmos.CometStatus
		inSync.Result.SyncInfo.LatestBlockHeight = "10000"

		return cosmos.StatusCollection{
			// Purposefully out of order to test sorting.
			{Pod: &corev1.Pod{ObjectMeta: metav1.ObjectMeta{Name: "pod-0"}}, Status: notInSync, TS: ts},
			{Pod: &corev1.Pod{ObjectMeta: metav1.ObjectMeta{Name: "pod-1"}}, Status: inSync, TS: ts},
			{Pod: &corev1.Pod{ObjectMeta: metav1.ObjectMeta{Name: "pod-2"}}, Err: errors.New("some error"), TS: ts},
		}
	}

	wantTS := metav1.NewTime(ts)
	want := map[string]*cosmosv1.SyncInfoPodStatus{
		"pod-0": {
			Timestamp: wantTS,
			Height:    ptr(uint64(9999)),
			InSync:    ptr(false),
		},
		"pod-1": {
			Timestamp: wantTS,
			Height:    ptr(uint64(10000)),
			InSync:    ptr(true),
		},
		"pod-2": {
			Timestamp: wantTS,
			Error:     ptr("some error"),
		},
	}

	status := SyncInfoStatus(context.Background(), &crd, collector)
	require.Equal(t, want, status)
}

'''
'''--- internal/fullnode/testdata/app.toml ---
minimum-gas-prices = "0.123token"

halt-height = 34567

pruning = "custom"
pruning-keep-recent = "444"
pruning-keep-every = "333"
pruning-interval = "222"
min-retain-blocks = 271500

[api]
enable = true
swagger = true
address = "tcp://0.0.0.0:1317"
enabled-unsafe-cors = true

[grpc]
enable = true
address = "0.0.0.0:9090"

[grpc-web]
enable = true
address = "0.0.0.0:9091"
enable-unsafe-cors = true

'''
'''--- internal/fullnode/testdata/app_defaults.toml ---
minimum-gas-prices = "0.123token"

[api]
enable = true
swagger = true
address = "tcp://0.0.0.0:1317"
enabled-unsafe-cors = false

[grpc]
enable = true
address = "0.0.0.0:9090"

[grpc-web]
enable = true
address = "0.0.0.0:9091"
enable-unsafe-cors = false

'''
'''--- internal/fullnode/testdata/app_overrides.toml ---
minimum-gas-prices = "0.1override"
new-base = "new base value"

[api]
enable = false
swagger = true
address = "tcp://0.0.0.0:1317"
enabled-unsafe-cors = false
new-field = "test"

[grpc]
enable = true
address = "0.0.0.0:9090"

[grpc-web]
enable = true
address = "0.0.0.0:9091"
enable-unsafe-cors = false

'''
'''--- internal/fullnode/testdata/comet.toml ---
log_format = "json"
log-format = "json"
log_level = "debug"
log-level = "debug"
priv_validator_laddr = ""

[p2p]
laddr = "tcp://0.0.0.0:26656"
persistent_peers = "peer1@1.2.2.2:789,peer2@2.2.2.2:789,peer3@3.2.2.2:789"
persistent-peers = "peer1@1.2.2.2:789,peer2@2.2.2.2:789,peer3@3.2.2.2:789"
seeds = "seed1@1.1.1.1:456,seed2@1.1.1.1:456"
max_num_inbound_peers = 5
max-num-inbound-peers = 5
max_num_outbound_peers = 15
max-num-outbound-peers = 15

[rpc]
laddr = "tcp://0.0.0.0:26657"
cors_allowed_origins = ["*"]
cors-allowed-origins = ["*"]

[tx_index]
indexer = "kv"

[instrumentation]
prometheus = true
prometheus_listen_addr = ":26660"

'''
'''--- internal/fullnode/testdata/comet_defaults.toml ---
log_format = "plain"
log_level = "info"
priv_validator_laddr = ""

[p2p]
laddr = "tcp://0.0.0.0:26656"
persistent_peers = "peer1@1.2.2.2:789,peer2@2.2.2.2:789,peer3@3.2.2.2:789"
persistent-peers = "peer1@1.2.2.2:789,peer2@2.2.2.2:789,peer3@3.2.2.2:789"
seeds = "seed1@1.1.1.1:456,seed2@1.1.1.1:456"
max_num_inbound_peers = 20
max_num_outbound_peers = 20

[rpc]
laddr = "tcp://0.0.0.0:26657"
cors_allowed_origins = []

[tx_index]
indexer = "kv"

[instrumentation]
prometheus = true
prometheus_listen_addr = ":26660"

'''
'''--- internal/fullnode/testdata/comet_overrides.toml ---
log_format = "json"
log_level = "info"
priv_validator_laddr = ""
new_base = "new base value"

[p2p]
external-address = "override.example.com"
external_address = "override.example.com"
laddr = "tcp://0.0.0.0:26656"
persistent_peers = "peer1@1.2.2.2:789,peer2@2.2.2.2:789,peer3@3.2.2.2:789"
persistent-peers = "peer1@1.2.2.2:789,peer2@2.2.2.2:789,peer3@3.2.2.2:789"
seeds = "override@seed"
max_num_inbound_peers = 20
max_num_outbound_peers = 20
new_field = "p2p"

[rpc]
laddr = "tcp://0.0.0.0:26657"
cors_allowed_origins = ["override"]
cors-allowed-origins = ["override"]

[tx_index]
indexer = "null"

[new_section]
test = "value"

[instrumentation]
prometheus = true
prometheus_listen_addr = ":26660"
'''
'''--- internal/fullnode/toml/app_default_config.toml ---
[api]
enable = true
# Swagger defines if swagger documentation should automatically be registered.
swagger = true
address = "tcp://0.0.0.0:1317"
# EnableUnsafeCORS defines if CORS should be enabled (unsafe - use it at your own risk).
enabled-unsafe-cors = false

[grpc]
# Enable defines if the gRPC server should be enabled.
enable = true
# Address defines the gRPC server address to bind to.
address = "0.0.0.0:9090"

[grpc-web]
# GRPCWebEnable defines if the gRPC-web should be enabled.
# NOTE: gRPC must also be enabled, otherwise, this configuration is a no-op.
enable = true

# Address defines the gRPC-web server address to bind to.
address = "0.0.0.0:9091"

# EnableUnsafeCORS defines if CORS should be enabled (unsafe - use it at your own risk).
# Note the name discrepency between enabled-unsafe-cors in api ("enabled" with a "d").
enable-unsafe-cors = false

'''
'''--- internal/fullnode/toml/comet_default_config.toml ---
# Output level for logging, including package level options
log_level = "info"

# Output format: 'plain' (colored text) or 'json'
log_format = "plain"

priv_validator_laddr = ""

[p2p]

# Address to listen for incoming connections
laddr = "tcp://0.0.0.0:26656"

max_num_inbound_peers = 20
max_num_outbound_peers = 20

[rpc]

# TCP or UNIX socket address for the RPC server to listen on
laddr = "tcp://0.0.0.0:26657"

cors_allowed_origins = []

[tx_index]

# What indexer to use for transactions
#
# The application will set which txs to index. In some cases a node operator will be able
# to decide which txs to index based on configuration set in the application.
#
# Options:
#   1) "null"
#   2) "kv" (default) - the simplest possible indexer, backed by key-value storage (defaults to levelDB; see DBBackend).
# 		- When "kv" is chosen "tx.height" and "tx.hash" will always be indexed.
indexer = "kv"

[instrumentation]

# When true, Prometheus metrics are served under /metrics on
# PrometheusListenAddr.
# Check out the documentation for the list of available metrics.
prometheus = true

# Address to listen for Prometheus collector(s) connections
prometheus_listen_addr = ":26660"

'''
'''--- internal/healthcheck/client.go ---
package healthcheck

import (
	"context"
	"encoding/json"
	"errors"
	"fmt"
	"net"
	"net/http"
	"net/url"
	"strconv"
)

// Client can be used to query healthcheck information.
type Client struct {
	httpDo func(req *http.Request) (*http.Response, error)
}

func NewClient(client *http.Client) *Client {
	return &Client{
		httpDo: client.Do,
	}
}

// DiskUsage returns disk usage statistics or an error if unable to obtain.
// Do not include the port in the host.
func (c Client) DiskUsage(ctx context.Context, host, homeDir string) (DiskUsageResponse, error) {
	var diskResp DiskUsageResponse
	u, err := url.Parse(host)
	if err != nil {
		return diskResp, fmt.Errorf("url parse: %w", err)
	}
	u.Host = net.JoinHostPort(u.Host, strconv.Itoa(Port))
	u.Path = "/disk"

	req, err := http.NewRequestWithContext(ctx, "GET", u.String(), nil)
	if err != nil {
		return diskResp, fmt.Errorf("new request: %w", err)
	}

	q := req.URL.Query()
	q.Set("dir", homeDir)
	req.URL.RawQuery = q.Encode()

	resp, err := c.httpDo(req)
	if err != nil {
		return diskResp, fmt.Errorf("http do: %w", err)
	}
	defer resp.Body.Close()
	if err = json.NewDecoder(resp.Body).Decode(&diskResp); err != nil {
		return diskResp, fmt.Errorf("malformed json: %w", err)
	}
	if diskResp.Error != "" {
		return diskResp, errors.New(diskResp.Error)
	}
	if diskResp.AllBytes == 0 {
		return diskResp, errors.New("invalid response: 0 free bytes")
	}
	return diskResp, nil
}

'''
'''--- internal/healthcheck/client_test.go ---
package healthcheck

import (
	"bytes"
	"context"
	"encoding/json"
	"errors"
	"io"
	"net/http"
	"strings"
	"testing"

	"github.com/stretchr/testify/require"
)

func TestClient_DiskUsage(t *testing.T) {
	var (
		ctx        = context.Background()
		httpClient = &http.Client{}
	)

	const (
		host    = "http://10.1.1.1"
		homeDir = "/home/test"
	)

	t.Run("happy path", func(t *testing.T) {
		client := NewClient(httpClient)
		require.NotNil(t, client.httpDo)

		want := DiskUsageResponse{
			Dir:       "/test",
			AllBytes:  100,
			FreeBytes: 10,
		}

		client.httpDo = func(req *http.Request) (*http.Response, error) {
			require.Equal(t, "http://10.1.1.1:1251/disk?dir=%2Fhome%2Ftest", req.URL.String())
			require.Equal(t, "GET", req.Method)
			require.Equal(t, homeDir, req.URL.Query().Get("dir"))

			b, err := json.Marshal(want)
			if err != nil {
				panic(err)
			}
			return &http.Response{Body: io.NopCloser(bytes.NewReader(b))}, nil
		}

		got, err := client.DiskUsage(ctx, host, homeDir)

		require.NoError(t, err)
		require.Equal(t, want, got)
	})

	t.Run("request error", func(t *testing.T) {
		client := NewClient(httpClient)
		client.httpDo = func(req *http.Request) (*http.Response, error) {
			return nil, errors.New("boom")
		}
		_, err := client.DiskUsage(ctx, host, "")

		require.Error(t, err)
		require.EqualError(t, err, "http do: boom")
	})

	t.Run("error in response", func(t *testing.T) {
		client := NewClient(httpClient)

		stub := DiskUsageResponse{Error: "something bad happened"}
		client.httpDo = func(req *http.Request) (*http.Response, error) {
			b, err := json.Marshal(stub)
			if err != nil {
				panic(err)
			}
			return &http.Response{
				Body: io.NopCloser(bytes.NewReader(b)),
			}, nil
		}

		_, err := client.DiskUsage(ctx, host, "")

		require.Error(t, err)
		require.EqualError(t, err, "something bad happened")
	})

	t.Run("invalid JSON", func(t *testing.T) {
		client := NewClient(httpClient)

		client.httpDo = func(req *http.Request) (*http.Response, error) {
			return &http.Response{
				Body: io.NopCloser(strings.NewReader("{")),
			}, nil
		}

		_, err := client.DiskUsage(ctx, host, "")

		require.Error(t, err)
		require.EqualError(t, err, "malformed json: unexpected EOF")
	})

	t.Run("zero values", func(t *testing.T) {
		client := NewClient(httpClient)

		client.httpDo = func(req *http.Request) (*http.Response, error) {
			return &http.Response{
				Body: io.NopCloser(strings.NewReader(`{}`)),
			}, nil
		}

		_, err := client.DiskUsage(ctx, host, "")

		require.Error(t, err)
		require.EqualError(t, err, "invalid response: 0 free bytes")
	})
}

'''
'''--- internal/healthcheck/comet.go ---
// Package healthcheck typically enables readiness or liveness probes within kubernetes.
// IMPORTANT: If you update this behavior, be sure to update internal/fullnode/pod_builder.go with the new
// cosmos operator image in the "healthcheck" container.
package healthcheck

import (
	"context"
	"net/http"
	"sync/atomic"
	"time"

	"github.com/go-logr/logr"
	"github.com/strangelove-ventures/cosmos-operator/internal/cosmos"
)

// Statuser can query the Comet status endpoint.
type Statuser interface {
	Status(ctx context.Context, rpcHost string) (cosmos.CometStatus, error)
}

type healthResponse struct {
	Address string `json:"address"`
	InSync  bool   `json:"in_sync"`
	Error   string `json:"error,omitempty"`
}

// Comet checks the CometBFT status endpoint to determine if the node is in-sync or not.
type Comet struct {
	client     Statuser
	lastStatus int32
	logger     logr.Logger
	rpcHost    string
	timeout    time.Duration
}

func NewComet(logger logr.Logger, client Statuser, rpcHost string, timeout time.Duration) *Comet {
	return &Comet{
		client:  client,
		logger:  logger,
		rpcHost: rpcHost,
		timeout: timeout,
	}
}

// ServeHTTP implements http.Handler.
func (h *Comet) ServeHTTP(w http.ResponseWriter, r *http.Request) {
	var resp healthResponse
	resp.Address = h.rpcHost

	ctx, cancel := context.WithTimeout(r.Context(), h.timeout)
	defer cancel()

	status, err := h.client.Status(ctx, h.rpcHost)
	if err != nil {
		resp.Error = err.Error()
		h.writeResponse(http.StatusServiceUnavailable, w, resp)
		return
	}

	resp.InSync = !status.Result.SyncInfo.CatchingUp
	if !resp.InSync {
		h.writeResponse(http.StatusUnprocessableEntity, w, resp)
		return
	}

	h.writeResponse(http.StatusOK, w, resp)
}

func (h *Comet) writeResponse(code int, w http.ResponseWriter, resp healthResponse) {
	w.WriteHeader(code)
	w.Header().Set("Content-Type", "application/json")
	mustJSONEncode(resp, w)
	// Only log when status code changes, so we don't spam logs.
	if atomic.SwapInt32(&h.lastStatus, int32(code)) != int32(code) {
		h.logger.Info("Health state change", "statusCode", code, "response", resp)
	}
}

'''
'''--- internal/healthcheck/comet_test.go ---
package healthcheck

import (
	"context"
	"encoding/json"
	"errors"
	"net/http"
	"net/http/httptest"
	"testing"
	"time"

	"github.com/go-logr/logr"
	"github.com/strangelove-ventures/cosmos-operator/internal/cosmos"
	"github.com/stretchr/testify/require"
)

type mockClient func(ctx context.Context, rpcHost string) (cosmos.CometStatus, error)

func (fn mockClient) Status(ctx context.Context, rpcHost string) (cosmos.CometStatus, error) {
	return fn(ctx, rpcHost)
}

var nopLogger = logr.Discard()

func TestComet_ServeHTTP(t *testing.T) {
	t.Parallel()

	var (
		stubReq = httptest.NewRequest("GET", "/", nil)
	)
	const testRPC = "http://my-rpc:25567"

	t.Run("happy path", func(t *testing.T) {
		client := mockClient(func(ctx context.Context, rpcHost string) (cosmos.CometStatus, error) {
			require.NotNil(t, ctx)
			require.Equal(t, testRPC, rpcHost)
			return cosmos.CometStatus{}, nil
		})

		h := NewComet(nopLogger, client, testRPC, 10*time.Second)
		w := httptest.NewRecorder()
		h.ServeHTTP(w, stubReq)

		require.Equal(t, http.StatusOK, w.Code)
		var got healthResponse
		err := json.NewDecoder(w.Body).Decode(&got)
		require.NoError(t, err)

		want := healthResponse{
			Address: testRPC,
			InSync:  true,
		}
		require.Equal(t, want, got)
	})

	t.Run("still catching up", func(t *testing.T) {
		client := mockClient(func(ctx context.Context, rpcHost string) (cosmos.CometStatus, error) {
			var stub cosmos.CometStatus
			stub.Result.SyncInfo.CatchingUp = true
			return stub, nil
		})

		h := NewComet(nopLogger, client, testRPC, 10*time.Second)
		w := httptest.NewRecorder()
		h.ServeHTTP(w, stubReq)

		require.Equal(t, http.StatusUnprocessableEntity, w.Code)
		var got healthResponse
		err := json.NewDecoder(w.Body).Decode(&got)
		require.NoError(t, err)

		want := healthResponse{
			Address: testRPC,
			InSync:  false,
		}
		require.Equal(t, want, got)
	})

	t.Run("status error", func(t *testing.T) {
		client := mockClient(func(ctx context.Context, rpcHost string) (cosmos.CometStatus, error) {
			return cosmos.CometStatus{}, errors.New("boom")
		})

		h := NewComet(nopLogger, client, testRPC, 10*time.Second)
		w := httptest.NewRecorder()
		h.ServeHTTP(w, stubReq)

		require.Equal(t, http.StatusServiceUnavailable, w.Code)
		var got healthResponse
		err := json.NewDecoder(w.Body).Decode(&got)
		require.NoError(t, err)

		want := healthResponse{
			Address: testRPC,
			Error:   "boom",
		}
		require.Equal(t, want, got)
	})

	t.Run("times out", func(t *testing.T) {
		var gotCtx context.Context
		client := mockClient(func(ctx context.Context, rpcHost string) (cosmos.CometStatus, error) {
			gotCtx = ctx
			return cosmos.CometStatus{}, nil
		})

		h := NewComet(nopLogger, client, testRPC, time.Nanosecond)
		w := httptest.NewRecorder()
		h.ServeHTTP(w, stubReq)

		select {
		case <-gotCtx.Done():
		// Test passes.
		case <-time.After(3 * time.Second):
			require.Fail(t, "context did not time out")
		}
	})
}

'''
'''--- internal/healthcheck/disk_usage.go ---
package healthcheck

import (
	"encoding/json"
	"io"
	"net/http"
	"path/filepath"
	"syscall"
)

// DiskUsageResponse returns disk statistics in bytes.
type DiskUsageResponse struct {
	Dir       string `json:"dir"`
	AllBytes  uint64 `json:"all_bytes,omitempty"`
	FreeBytes uint64 `json:"free_bytes,omitempty"`
	Error     string `json:"error,omitempty"`
}

// DiskUsage returns a handler which responds with disk statistics in JSON.
// Path is the filesystem path from which to check disk usage.
func DiskUsage(w http.ResponseWriter, r *http.Request) {
	var resp DiskUsageResponse
	dir := r.URL.Query().Get("dir")
	if dir == "" {
		w.WriteHeader(http.StatusBadRequest)
		resp.Error = "query param dir must be specified"
		mustJSONEncode(resp, w)
		return
	}
	resp.Dir = dir
	var fs syscall.Statfs_t
	// Purposefully not adding test hook, so tests may catch OS issues.
	err := syscall.Statfs(filepath.Clean(dir), &fs)
	if err != nil {
		w.WriteHeader(http.StatusInternalServerError)
		resp.Error = err.Error()
		mustJSONEncode(resp, w)
		return
	}

	w.WriteHeader(http.StatusOK)
	var (
		all  = fs.Blocks * uint64(fs.Bsize)
		free = fs.Bfree * uint64(fs.Bsize)
	)
	resp.AllBytes = all
	resp.FreeBytes = free
	mustJSONEncode(resp, w)
}

func mustJSONEncode(v interface{}, w io.Writer) {
	if err := json.NewEncoder(w).Encode(v); err != nil {
		panic(err)
	}
}

'''
'''--- internal/healthcheck/disk_usage_test.go ---
package healthcheck

import (
	"encoding/json"
	"net/http/httptest"
	"testing"

	"github.com/stretchr/testify/require"
)

func TestDiskUsage(t *testing.T) {
	t.Run("happy path", func(t *testing.T) {
		var (
			w = httptest.NewRecorder()
			r = httptest.NewRequest("GET", "/ignored", nil)
		)
		q := r.URL.Query()
		q.Set("dir", "/")
		r.URL.RawQuery = q.Encode()

		DiskUsage(w, r)

		require.Equal(t, 200, w.Code)

		var got DiskUsageResponse
		err := json.Unmarshal(w.Body.Bytes(), &got)
		require.NoError(t, err)

		require.Equal(t, "/", got.Dir)
		require.NotZero(t, got.AllBytes)
		require.NotZero(t, got.FreeBytes)
		require.True(t, got.AllBytes >= got.FreeBytes, "free bytes should not be more than all bytes")

		require.NotContains(t, w.Body.String(), "error")
	})

	t.Run("statfs error", func(t *testing.T) {
		const dir = "/this-directory-had-better-not-be-present-in-any-sort-of-test-environment"
		var (
			w = httptest.NewRecorder()
			r = httptest.NewRequest("GET", "/ignored", nil)
		)
		q := r.URL.Query()
		q.Set("dir", dir)
		r.URL.RawQuery = q.Encode()

		DiskUsage(w, r)

		require.Equal(t, 500, w.Code)

		var got DiskUsageResponse
		err := json.Unmarshal(w.Body.Bytes(), &got)
		require.NoError(t, err)

		require.Equal(t, dir, got.Dir)
		require.Equal(t, "no such file or directory", got.Error)
		require.NotContains(t, w.Body.String(), "all_bytes")
		require.NotContains(t, w.Body.String(), "free_bytes")
	})

	t.Run("missing dir", func(t *testing.T) {
		var (
			w = httptest.NewRecorder()
			r = httptest.NewRequest("GET", "/", nil)
		)

		DiskUsage(w, r)

		require.Equal(t, 400, w.Code)

		var got DiskUsageResponse
		err := json.Unmarshal(w.Body.Bytes(), &got)
		require.NoError(t, err)

		require.Equal(t, "query param dir must be specified", got.Error)
		require.NotContains(t, w.Body.String(), "all_bytes")
		require.NotContains(t, w.Body.String(), "free_bytes")
	})
}

'''
'''--- internal/healthcheck/healtchcheck.go ---
package healthcheck

// Port is the port for the healthcheck sidecar.
const Port = 1251

'''
'''--- internal/kube/create_or_update.go ---
package kube

import (
	"context"

	"sigs.k8s.io/controller-runtime/pkg/client"
)

// CreateOrUpdate first attempts to create the obj. If it already exists, it makes a second
// call to update the obj.
func CreateOrUpdate(ctx context.Context, client client.Writer, obj client.Object) error {
	err := client.Create(ctx, obj)
	if IsAlreadyExists(err) {
		err = client.Update(ctx, obj)
	}
	return err
}

'''
'''--- internal/kube/doc.go ---
// Package kube contains utility types and methods for managing kubernetes state.
package kube

'''
'''--- internal/kube/error.go ---
package kube

import (
	"strings"

	"github.com/samber/lo"
	apierrors "k8s.io/apimachinery/pkg/api/errors"
)

// ReconcileError is a controller-specific error.
type ReconcileError interface {
	error
	// IsTransient returns true if the error is temporary.
	IsTransient() bool
}

type reconcileError struct {
	error
	isTransient bool
}

func (e reconcileError) IsTransient() bool { return e.isTransient }

func (e reconcileError) Unwrap() error { return e.error }

// TransientError can be recovered or retried.
func TransientError(err error) ReconcileError {
	return reconcileError{err, true}
}

// UnrecoverableError cannot be recovered and should not be retried.
func UnrecoverableError(err error) ReconcileError {
	return reconcileError{err, false}
}

// IsNotFound returns true if the err reason is "not found".
func IsNotFound(err error) bool {
	return apierrors.IsNotFound(err)
}

// IgnoreNotFound returns nil if err reason is "not found".
func IgnoreNotFound(err error) error {
	if apierrors.IsNotFound(err) {
		return nil
	}
	return err
}

// IgnoreAlreadyExists returns nil if err reason is "already exists".
func IgnoreAlreadyExists(err error) error {
	if apierrors.IsAlreadyExists(err) {
		return nil
	}
	return err
}

// IsAlreadyExists determines if the error indicates that a specified resource already exists.
// It supports wrapped errors and returns false when the error is nil.
func IsAlreadyExists(err error) bool {
	return apierrors.IsAlreadyExists(err)
}

// ReconcileErrors is a collection of ReconcileError
type ReconcileErrors struct {
	errs []ReconcileError
}

func (errs *ReconcileErrors) Error() string {
	all := lo.Map(errs.errs, func(err ReconcileError, i int) string { return err.Error() })
	return strings.Join(all, "; ")
}

// IsTransient returns true if all errors are transient. False if at least one is not transient.
func (errs *ReconcileErrors) IsTransient() bool {
	for _, err := range errs.errs {
		if !err.IsTransient() {
			return false
		}
	}
	return true
}

// Append adds the ReconcileError.
func (errs *ReconcileErrors) Append(err ReconcileError) {
	errs.errs = append(errs.errs, err)
}

// Any returns true if any errors were collected.
func (errs *ReconcileErrors) Any() bool {
	return len(errs.errs) > 0
}

'''
'''--- internal/kube/error_test.go ---
package kube

import (
	"errors"
	"testing"

	"github.com/stretchr/testify/require"
)

func TestReconcileError(t *testing.T) {
	t.Parallel()

	err := errors.New("boom")

	terr := TransientError(err)
	require.True(t, terr.IsTransient())
	require.ErrorIs(t, terr, err)
	require.EqualError(t, terr, "boom")

	rerr := UnrecoverableError(err)
	require.False(t, rerr.IsTransient())
	require.ErrorIs(t, rerr, err)
	require.EqualError(t, rerr, "boom")
}

func TestReconcileErrors(t *testing.T) {
	t.Parallel()

	t.Run("transient", func(t *testing.T) {
		errs := &ReconcileErrors{}
		require.False(t, errs.Any())

		errs.Append(TransientError(errors.New("boom1")))
		errs.Append(TransientError(errors.New("boom2")))

		require.True(t, errs.Any())

		require.EqualError(t, errs, "boom1; boom2")
		require.True(t, errs.IsTransient())
	})

	t.Run("unrecoverable", func(t *testing.T) {
		errs := &ReconcileErrors{}
		errs.Append(TransientError(errors.New("boom1")))
		errs.Append(UnrecoverableError(errors.New("boom2")))
		errs.Append(TransientError(errors.New("boom3")))

		require.EqualError(t, errs, "boom1; boom2; boom3")
		require.False(t, errs.IsTransient())
	})
}

'''
'''--- internal/kube/find.go ---
package kube

import (
	"github.com/samber/lo"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

// FindOrDefaultCopy returns a deep copy of the object if it exists in the collection, otherwise it
// returns a deep copy of the comparator.
// Also defaults .metadata.labels and .metadata.annotations to an empty map if they are nil.
func FindOrDefaultCopy[T client.Object](existing []T, comparator T) T {
	found := lo.FindOrElse(existing, comparator, func(item T) bool {
		return item.GetName() == comparator.GetName() && item.GetNamespace() == comparator.GetNamespace()
	})
	found = found.DeepCopyObject().(T)
	if found.GetLabels() == nil {
		found.SetLabels(map[string]string{})
	}
	if found.GetAnnotations() == nil {
		found.SetAnnotations(map[string]string{})
	}
	return found
}

'''
'''--- internal/kube/find_test.go ---
package kube

import (
	"fmt"
	"testing"

	"github.com/samber/lo"
	"github.com/stretchr/testify/require"
	corev1 "k8s.io/api/core/v1"
)

func TestFindOrDefault(t *testing.T) {
	t.Parallel()

	pods := lo.Map(lo.Range(3), func(i int, _ int) *corev1.Pod {
		var pod corev1.Pod
		pod.Name = fmt.Sprintf("pod-%d", i)
		pod.Namespace = "default"
		pod.Spec.Volumes = []corev1.Volume{
			{Name: fmt.Sprintf("vol-%d", i)},
		}
		return &pod
	})

	t.Run("found", func(t *testing.T) {
		var cmp corev1.Pod
		cmp.Name = "pod-1"
		cmp.Namespace = "default"

		got := FindOrDefaultCopy(pods, &cmp)

		var want corev1.Pod
		want.Name = "pod-1"
		want.Namespace = "default"
		want.Annotations = map[string]string{}
		want.Labels = map[string]string{}
		want.Spec.Volumes = []corev1.Volume{
			{Name: "vol-1"},
		}

		require.Equal(t, &want, got)
		require.NotSame(t, pods[1], got)

		require.NotNil(t, got.Labels)
		require.NotNil(t, got.Annotations)
	})

	t.Run("not found", func(t *testing.T) {
		var cmp corev1.Pod
		cmp.Name = "pod-1"
		cmp.Namespace = "notsame"

		got := FindOrDefaultCopy(pods, &cmp)

		want := cmp.DeepCopy()
		want.Annotations = map[string]string{}
		want.Labels = map[string]string{}

		require.Equal(t, want, got)
		require.NotSame(t, &cmp, got)
	})
}

'''
'''--- internal/kube/image.go ---
package kube

import "strings"

// ParseImageVersion parses the version (aka tag) out of imageRef.
// As an example, "busybox:stable" would return "stable".
// If no tag, defaults to "latest".
func ParseImageVersion(imageRef string) string {
	parts := strings.Split(imageRef, ":")
	if len(parts) != 2 {
		return "latest"
	}
	v := parts[1]
	if v == "" {
		v = "latest"
	}
	return v
}

'''
'''--- internal/kube/image_test.go ---
package kube

import (
	"testing"

	"github.com/stretchr/testify/require"
)

func TestParseImageVersion(t *testing.T) {
	for _, tt := range []struct {
		ImageRef string
		Want     string
	}{
		{"", "latest"},
		{"busybox", "latest"},
		{"busybox:stable", "stable"},
		{"ghcr.io/strangelove-ventures/heighliner/osmosis:v9.0.0", "v9.0.0"},
		{"busybox:", "latest"},
	} {
		got := ParseImageVersion(tt.ImageRef)

		require.Equal(t, tt.Want, got, tt)
	}
}

'''
'''--- internal/kube/indexer.go ---
package kube

import (
	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

// IndexOwner returns field values for indexing "child" resources that are "owned" by a controller
// (typically the CRD such as CosmosFullNode).
// Indexing is required for client.Client methods such as listing resources.
//
// It returns a field to index only if all are true:
// 1) resource is part of cosmosv1.GroupVersion.
// 2) resource is owned by a controller equal to "kind".
func IndexOwner[T client.Object](kind string) client.IndexerFunc {
	return func(object client.Object) []string {
		resource := object.(T)
		owner := metav1.GetControllerOf(resource)
		if owner == nil {
			return nil
		}
		if owner.APIVersion != cosmosv1.GroupVersion.String() || owner.Kind != kind {
			return nil
		}
		return []string{owner.Name}
	}
}

'''
'''--- internal/kube/indexer_test.go ---
package kube

import (
	"testing"

	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	"github.com/stretchr/testify/require"
	corev1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/runtime"
	ctrl "sigs.k8s.io/controller-runtime"
)

func TestIndexOwner(t *testing.T) {
	scheme := runtime.NewScheme()
	err := cosmosv1.AddToScheme(scheme)
	if err != nil {
		panic(err)
	}

	t.Parallel()

	t.Run("happy path", func(t *testing.T) {
		resource := &corev1.Pod{}
		crd := &cosmosv1.CosmosFullNode{}
		crd.Name = "test"

		err = ctrl.SetControllerReference(crd, resource, scheme)
		require.NoError(t, err)

		index := IndexOwner[*corev1.Pod]("CosmosFullNode")
		got := index(resource)

		require.Equal(t, []string{"test"}, got)
	})

	t.Run("no controller", func(t *testing.T) {
		index := IndexOwner[*corev1.Pod]("CosmosFullNode")
		got := index(&corev1.Pod{})

		require.Nil(t, got)
	})

	t.Run("kind mismatch", func(t *testing.T) {
		resource := &corev1.Pod{}
		crd := &cosmosv1.CosmosFullNode{}
		crd.Name = "test"

		err = ctrl.SetControllerReference(crd, resource, scheme)
		require.NoError(t, err)

		index := IndexOwner[*corev1.Pod]("SomeOtherCRD")
		got := index(&corev1.Pod{})

		require.Nil(t, got)
	})
}

'''
'''--- internal/kube/job.go ---
package kube

import (
	batchv1 "k8s.io/api/batch/v1"
	corev1 "k8s.io/api/core/v1"
)

// IsJobFinished checks whether the given Job has finished execution.
// It does not discriminate between successful and failed terminations.
func IsJobFinished(job *batchv1.Job) bool {
	for _, c := range job.Status.Conditions {
		if (c.Type == batchv1.JobComplete || c.Type == batchv1.JobFailed) && c.Status == corev1.ConditionTrue {
			return true
		}
	}
	return false
}

'''
'''--- internal/kube/job_test.go ---
package kube

import (
	"testing"

	"github.com/stretchr/testify/require"
	batchv1 "k8s.io/api/batch/v1"
	corev1 "k8s.io/api/core/v1"
)

func TestIsJobFinished(t *testing.T) {
	for _, tt := range []struct {
		Job  batchv1.JobCondition
		Want bool
	}{
		{batchv1.JobCondition{Type: batchv1.JobComplete, Status: corev1.ConditionTrue}, true},
		{batchv1.JobCondition{Type: batchv1.JobFailed, Status: corev1.ConditionTrue}, true},

		{batchv1.JobCondition{Type: batchv1.JobComplete, Status: corev1.ConditionUnknown}, false},
		{batchv1.JobCondition{Type: batchv1.JobComplete, Status: corev1.ConditionFalse}, false},
		{batchv1.JobCondition{Type: batchv1.JobSuspended, Status: corev1.ConditionTrue}, false},
	} {
		var job batchv1.Job
		job.Status.Conditions = []batchv1.JobCondition{{Type: "test", Status: "ignored"}, tt.Job}

		require.Equal(t, tt.Want, IsJobFinished(&job), tt)
	}
}

'''
'''--- internal/kube/labels.go ---
package kube

import (
	"bytes"
	"strconv"

	"golang.org/x/exp/constraints"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

// Recommended labels. See: https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/
const (
	ControllerLabel = "app.kubernetes.io/created-by"
	InstanceLabel   = "app.kubernetes.io/instance"
	NameLabel       = "app.kubernetes.io/name"
	VersionLabel    = "app.kubernetes.io/version"
	ComponentLabel  = "app.kubernetes.io/component"

	// OrdinalAnnotation is used to order resources. The value must be a base 10 integer string.
	OrdinalAnnotation = "app.kubernetes.io/ordinal"
)

// Fields.
const (
	ControllerOwnerField = ".metadata.controller"
)

// ToIntegerValue converts n to a base 10 integer string.
func ToIntegerValue[T constraints.Signed](n T) string {
	return strconv.FormatInt(int64(n), 10)
}

// MustToInt converts s to int64 or panics on failure.
func MustToInt(s string) int64 {
	n, err := strconv.ParseInt(s, 10, 64)
	if err != nil {
		panic(err)
	}
	return n
}

// ToLabelKey normalizes val per kubernetes label constraints to a max of 63 characters.
// See: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set.
func ToLabelKey(val string) string {
	return normalizeValue(val, 63, '-', '_', '.', '/')
}

// ToName normalizes val per kubernetes name constraints to a max of 253 characters.
// See: https://unofficial-kubernetes.readthedocs.io/en/latest/concepts/overview/working-with-objects/names/
func ToName(val string) string {
	return normalizeValue(val, 253, '-', '.')
}

// NormalizeMetadata normalizes name, labels, and annotations.
// See: https://unofficial-kubernetes.readthedocs.io/en/latest/concepts/overview/working-with-objects/names/
// See: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set.
func NormalizeMetadata(obj *metav1.ObjectMeta) {
	obj.Name = ToName(obj.Name)

	annots := make(map[string]string)
	for k, v := range obj.Annotations {
		annots[ToLabelKey(k)] = trimMiddle(v, 63)
	}
	obj.Annotations = annots

	labels := make(map[string]string)
	for k, v := range obj.Labels {
		labels[ToLabelKey(k)] = trimMiddle(v, 63)
	}
	obj.Labels = labels
}

func normalizeValue(val string, limit int, allowed ...byte) string {
	// Select only alphanumeric and allowed characters.
	result := []byte(val)
	j := 0
	for _, char := range []byte(val) {
		if (char >= 'a' && char <= 'z') ||
			(char >= 'A' && char <= 'Z') ||
			(char >= '0' && char <= '9') ||
			(bytes.IndexByte(allowed, char) != -1) {
			result[j] = char
			j++
		}
	}
	result = result[:j]

	// Start and end with alphanumeric only
	result = bytes.TrimLeftFunc(result, func(r rune) bool {
		return bytes.ContainsRune(allowed, r)
	})
	result = bytes.TrimRightFunc(result, func(r rune) bool {
		return bytes.ContainsRune(allowed, r)
	})

	return trimMiddle(string(result), limit)
}

// Truncates the middle, trying to preserve prefix and suffix.
func trimMiddle(val string, limit int) string {
	if len(val) <= limit {
		return val
	}

	// Truncate the middle, trying to preserve prefix and suffix.
	left, right := limit/2, limit/2
	if limit%2 != 0 {
		right++
	}
	b := []byte(val)
	return string(append(b[:left], b[len(b)-right:]...))
}

'''
'''--- internal/kube/labels_test.go ---
package kube

import (
	"fmt"
	"math/rand"
	"strings"
	"testing"
	"time"

	"github.com/strangelove-ventures/cosmos-operator/internal/test"
	"github.com/stretchr/testify/require"
	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

func TestToLabelKey(t *testing.T) {
	t.Parallel()

	for _, tt := range []struct {
		Input string
		Want  string
	}{
		{"hub-0", "hub-0"},
		{"", ""},

		{"HUB!@+=_.0", "HUB_.0"},
		{strings.Repeat("abcde^&*", 60) + "-suffix", "abcdeabcdeabcdeabcdeabcdeabcdeaabcdeabcdeabcdeabcdeabcde-suffix"},

		// Must start and end with alphanumeric character.
		{"#..abc1-_@!", "abc1"},
	} {
		got := ToLabelKey(tt.Input)

		require.LessOrEqual(t, len(got), 63)
		require.Equal(t, tt.Want, got, tt)
	}
}

func TestToName(t *testing.T) {
	t.Parallel()

	for _, tt := range []struct {
		Input string
		Want  string
	}{
		{"hub-0", "hub-0"},
		{"", ""},

		{"HUB!@+=_.0", "HUB.0"},
		{strings.Repeat("abcde^&*", 100) + "-suffix", "abcdeabcdeabcdeabcdeabcdeabcdeabcdeabcdeabcdeabcdeabcdeabcdeabcdeabcdeabcdeabcdeabcdeabcdeabcdeabcdeabcdeabcdeabcdeabcdeabcdeaabcdeabcdeabcdeabcdeabcdeabcdeabcdeabcdeabcdeabcdeabcdeabcdeabcdeabcdeabcdeabcdeabcdeabcdeabcdeabcdeabcdeabcdeabcdeabcde-suffix"},

		// Must start and end with alphanumeric character.
		{"#..abc2-_@!", "abc2"},
	} {
		got := ToName(tt.Input)

		require.LessOrEqual(t, len(got), 253)
		require.Equal(t, tt.Want, got, tt)
	}
}

func TestToIntegerValue(t *testing.T) {
	t.Parallel()

	require.Equal(t, "123", ToIntegerValue(123))
	require.Equal(t, "-1", ToIntegerValue(-1))
}

func TestMustToInt(t *testing.T) {
	t.Parallel()

	require.EqualValues(t, 123, MustToInt(ToIntegerValue(123)))

	r := rand.New(rand.NewSource(time.Now().UnixNano()))
	n := r.Intn(1000)
	require.EqualValues(t, n, MustToInt(fmt.Sprintf("%d", n)))

	for _, badValue := range []string{"", "1.2", "1-2"} {
		require.Panics(t, func() {
			MustToInt(badValue)
		})
	}
}

func TestNormalizeMetadata(t *testing.T) {
	obj := &corev1.Pod{
		ObjectMeta: metav1.ObjectMeta{
			Name:        strings.Repeat(" name ", 500),
			Annotations: map[string]string{strings.Repeat("annot-key", 500): strings.Repeat("value", 500), "cloud.google.com/neg": `{"ingress": true}`},
			Labels:      map[string]string{strings.Repeat("label-key", 500): strings.Repeat("value", 500)},
		},
	}

	NormalizeMetadata(&obj.ObjectMeta)

	test.RequireValidMetadata(t, obj)
	require.Equal(t, `{"ingress": true}`, obj.Annotations["cloud.google.com/neg"])
}

'''
'''--- internal/kube/patch.go ---
package kube

import (
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/util/strategicpatch"
)

var converter = runtime.DefaultUnstructuredConverter

// ApplyStrategicMergePatch applies a strategic merge patch to a target object.
// Inspired by: https://github.com/kubernetes/apiserver/blob/45f55ded302a02ed2023e8b45bd241cf7d81169e/pkg/endpoints/handlers/patch.go
func ApplyStrategicMergePatch[T any](target, patch T) error {
	targetMap, err := converter.ToUnstructured(target)
	if err != nil {
		return err
	}
	patchMap, err := converter.ToUnstructured(patch)
	if err != nil {
		return err
	}
	schema, err := strategicpatch.NewPatchMetaFromStruct(target)
	if err != nil {
		return err
	}
	result, err := strategicpatch.StrategicMergeMapPatchUsingLookupPatchMeta(targetMap, patchMap, schema)
	if err != nil {
		return err
	}
	return runtime.DefaultUnstructuredConverter.FromUnstructured(result, target)
}

'''
'''--- internal/kube/patch_test.go ---
package kube

import (
	"testing"

	"github.com/stretchr/testify/require"
	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

func TestApplyStrategicMergePatch(t *testing.T) {
	t.Parallel()

	t.Run("happy path", func(t *testing.T) {
		target := &corev1.PodTemplateSpec{
			ObjectMeta: metav1.ObjectMeta{
				Name: "test",
				Labels: map[string]string{
					"app": "myapp",
					"foo": "bar",
				},
			},
			Spec: corev1.PodSpec{
				NodeSelector:  map[string]string{"test": "value"},
				RestartPolicy: corev1.RestartPolicyAlways,
				Containers: []corev1.Container{
					{
						Name:  "app",
						Image: "myapp:v1",
					},
					{
						Name:  "second",
						Image: "v2",
					},
				},
			},
		}

		patch := &corev1.PodTemplateSpec{
			ObjectMeta: metav1.ObjectMeta{
				Labels: map[string]string{
					"app": "CHANGED",
				},
			},
			Spec: corev1.PodSpec{
				RestartPolicy: corev1.RestartPolicyNever,
				Containers: []corev1.Container{
					{
						Name:  "app",
						Image: "myapp:CHANGED",
					},
				},
			},
		}

		err := ApplyStrategicMergePatch(target, patch)

		require.NoError(t, err)

		want := &corev1.PodTemplateSpec{
			ObjectMeta: metav1.ObjectMeta{
				Name: "test",
				Labels: map[string]string{
					"app": "CHANGED",
					"foo": "bar",
				},
			},
			Spec: corev1.PodSpec{
				NodeSelector:  map[string]string{"test": "value"},
				RestartPolicy: corev1.RestartPolicyNever,
				Containers: []corev1.Container{
					{
						Name:  "app",
						Image: "myapp:CHANGED",
					},
					{
						Name:  "second",
						Image: "v2",
					},
				},
			},
		}
		require.Equal(t, want, target)
	})

	t.Run("identity", func(t *testing.T) {
		obj := &corev1.PodTemplateSpec{
			ObjectMeta: metav1.ObjectMeta{
				Name: "test",
				Labels: map[string]string{
					"foo": "bar",
				},
			},
			Spec: corev1.PodSpec{
				NodeSelector:  map[string]string{"test": "value"},
				RestartPolicy: corev1.RestartPolicyAlways,
				Containers: []corev1.Container{
					{
						Name:  "app",
						Image: "myapp:v1",
					},
					{
						Name:  "second",
						Image: "v2",
					},
				},
			},
		}

		want := obj.DeepCopy()

		err := ApplyStrategicMergePatch(obj, obj)
		require.NoError(t, err)
		require.Equal(t, want, obj)
	})
}

'''
'''--- internal/kube/pod.go ---
package kube

import (
	"time"

	"github.com/samber/lo"
	corev1 "k8s.io/api/core/v1"
)

// IsPodAvailable returns true if a pod is available; false otherwise.
// Precondition for an available pod is that it must be ready.
// Additionally, there are two cases when a pod can be considered available:
// 1. minReady == 0, or
// 2. LastTransitionTime (is set) + minReadySeconds < current time
//
// Much of this code was vendored from the kubernetes codebase.
func IsPodAvailable(pod *corev1.Pod, minReady time.Duration, now time.Time) bool {
	if !isPodReadyConditionTrue(pod.Status) {
		return false
	}

	c := getPodReadyCondition(pod.Status)
	if minReady == 0 || (!c.LastTransitionTime.IsZero() && c.LastTransitionTime.Add(minReady).Before(now)) {
		return true
	}
	return false
}

// isPodReadyConditionTrue returns true if a pod is ready; false otherwise.
func isPodReadyConditionTrue(status corev1.PodStatus) bool {
	condition := getPodReadyCondition(status)
	return condition != nil && condition.Status == corev1.ConditionTrue
}

// getPodReadyCondition extracts the pod ready condition from the given status and returns that.
// Returns nil if the condition is not present.
func getPodReadyCondition(status corev1.PodStatus) *corev1.PodCondition {
	_, condition := getPodCondition(&status, corev1.PodReady)
	return condition
}

// getPodCondition extracts the provided condition from the given status and returns that.
// Returns nil and -1 if the condition is not present, and the index of the located condition.
func getPodCondition(status *corev1.PodStatus, conditionType corev1.PodConditionType) (int, *corev1.PodCondition) {
	if status == nil {
		return -1, nil
	}
	return getPodConditionFromList(status.Conditions, conditionType)
}

// getPodConditionFromList extracts the provided condition from the given list of condition and
// returns the index of the condition and the condition. Returns -1 and nil if the condition is not present.
func getPodConditionFromList(conditions []corev1.PodCondition, conditionType corev1.PodConditionType) (int, *corev1.PodCondition) {
	for i := range conditions {
		if conditions[i].Type == conditionType {
			return i, &conditions[i]
		}
	}
	return -1, nil
}

// AvailablePods returns pods which are available as defined in IsPodAvailable.
func AvailablePods(pods []*corev1.Pod, minReady time.Duration, now time.Time) []*corev1.Pod {
	return lo.Filter(pods, func(p *corev1.Pod, _ int) bool { return IsPodAvailable(p, minReady, now) })
}

'''
'''--- internal/kube/pod_test.go ---
package kube

import (
	"testing"
	"time"

	"github.com/stretchr/testify/require"
	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

func TestIsPodAvailable(t *testing.T) {
	now := time.Now()

	for _, tt := range []struct {
		Condition corev1.PodCondition
		MinReady  time.Duration
		Want      bool
	}{
		// Not available
		{corev1.PodCondition{}, 0, false},
		{corev1.PodCondition{Type: corev1.PodReady, Status: corev1.ConditionFalse}, 0, false},
		{corev1.PodCondition{Type: corev1.PodReady, Status: corev1.ConditionUnknown}, 0, false},
		{corev1.PodCondition{Type: corev1.PodReady, Status: corev1.ConditionTrue, LastTransitionTime: metav1.NewTime(now)}, time.Second, false},

		// Available
		{corev1.PodCondition{Type: corev1.PodReady, Status: corev1.ConditionTrue}, 0, true},
		{corev1.PodCondition{Type: corev1.PodReady, Status: corev1.ConditionTrue, LastTransitionTime: metav1.NewTime(now.Add(-5 * time.Second))}, 2 * time.Second, true},
	} {
		pod := &corev1.Pod{
			Status: corev1.PodStatus{
				Conditions: []corev1.PodCondition{
					{Type: corev1.PodScheduled, Status: corev1.ConditionTrue},
					{Type: corev1.ContainersReady, Status: corev1.ConditionTrue},
					{Type: corev1.PodInitialized, Status: corev1.ConditionTrue},
				},
			},
		}
		pod.Status.Conditions = append(pod.Status.Conditions, tt.Condition)

		got := IsPodAvailable(pod, tt.MinReady, now)
		require.Equal(t, tt.Want, got, tt)
	}
}

'''
'''--- internal/kube/ptr.go ---
package kube

func ptr[T any](v T) *T {
	return &v
}

'''
'''--- internal/kube/reporter.go ---
package kube

import (
	"github.com/go-logr/logr"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/client-go/tools/record"
)

// The two accepted event types for recording events.
const (
	EventWarning = "Warning"
	EventNormal  = "Normal"
)

// Logger is a structured logger
type Logger interface {
	Info(msg string, keysAndValues ...interface{})
	Debug(msg string, keysAndValues ...interface{})
	Error(err error, msg string, keysAndValues ...interface{})
}

// Reporter logs and reports various events.
type Reporter interface {
	Logger
	RecordInfo(reason, msg string)
	RecordError(reason string, err error)
}

// ToLogger converts a logr.Logger to a Logger.
func ToLogger(log logr.Logger) Logger {
	return logger{log}
}

type logger struct {
	logr.Logger
}

func (l logger) Debug(msg string, keysAndValues ...interface{}) {
	l.V(1).Info(msg, keysAndValues...)
}

// EventReporter both logs and records events.
type EventReporter struct {
	log      Logger
	recorder record.EventRecorder
	resource runtime.Object
}

func NewEventReporter(logger logr.Logger, recorder record.EventRecorder, resource runtime.Object) EventReporter {
	return EventReporter{log: ToLogger(logger), recorder: recorder, resource: resource}
}

// Error logs as an error log entry.
func (r EventReporter) Error(err error, msg string, keysAndValues ...interface{}) {
	r.log.Error(err, msg, keysAndValues...)
}

// Info logs as an info log entry.
func (r EventReporter) Info(msg string, keysAndValues ...interface{}) {
	r.log.Info(msg, keysAndValues...)
}

// Debug logs as a debug log entry.
func (r EventReporter) Debug(msg string, keysAndValues ...interface{}) {
	r.log.Debug(msg, keysAndValues...)
}

// RecordError records a warning event.
func (r EventReporter) RecordError(reason string, err error) {
	r.recorder.Event(r.resource, EventWarning, reason, err.Error())
}

// RecordInfo records a normal event.
func (r EventReporter) RecordInfo(reason, msg string) {
	r.recorder.Event(r.resource, EventNormal, reason, msg)
}

'''
'''--- internal/kube/rollout.go ---
package kube

import (
	"errors"

	"k8s.io/apimachinery/pkg/util/intstr"
)

var defaultMaxUnavail = intstr.FromString("25%")

// ComputeRollout returns the number of replicas allowed to be updated to keep within
// a max unavailable threshold. Example: If max unavailable is 5 with a desired replica count of 10,
// that means rollout cannot happen until 6 or more replicas are ready. The replicas must stay
// within the minimum threshold of 5 ready replicas.
//
// If "maxUnavail" is nil, defaults to 25% and string value must be a percentage.
// "desired" must be >= 1 and "ready" must be >= 0 or else this function panics.
func ComputeRollout(maxUnavail *intstr.IntOrString, desired, ready int) int {
	if desired < 0 {
		panic(errors.New("desired must be >= 0"))
	}
	if ready < 0 {
		panic(errors.New("ready must be >= 0"))
	}

	if maxUnavail == nil {
		maxUnavail = &defaultMaxUnavail
	}
	unavail, err := intstr.GetScaledValueFromIntOrPercent(maxUnavail, desired, false)
	if err != nil {
		panic(err)
	}
	// At least 1 resource is allowed to be unavailable.
	if unavail < 1 {
		unavail = 1
	}
	minAvail := desired - unavail
	if ready <= minAvail {
		return 0
	}

	target := unavail - (desired - ready)
	if target > desired {
		target = desired
	}
	return target
}

'''
'''--- internal/kube/rollout_test.go ---
package kube

import (
	"fmt"
	"testing"

	"github.com/stretchr/testify/require"
	"k8s.io/apimachinery/pkg/util/intstr"
)

func TestComputeRollout(t *testing.T) {
	t.Parallel()

	t.Run("happy path", func(t *testing.T) {
		for _, tt := range []struct {
			Unavail        intstr.IntOrString
			Desired, Ready int
			Want           int
		}{
			// All ready
			{intstr.FromInt(1), 1, 1, 1},
			{intstr.FromInt(1), 10, 10, 1},
			{intstr.FromInt(5), 10, 10, 5},

			// None ready
			{intstr.FromInt(5), 10, 0, 0},
			{intstr.FromString("44%"), 10, 0, 0},

			// Partial ready
			{intstr.FromString("50%"), 3, 3, 1},

			{intstr.FromInt(3), 9, 3, 0},
			{intstr.FromInt(3), 9, 5, 0},
			{intstr.FromInt(3), 9, 7, 1},
			{intstr.FromInt(3), 9, 8, 2},

			{intstr.FromString("35%"), 9, 4, 0},
			{intstr.FromString("35%"), 9, 6, 0},
			{intstr.FromString("35%"), 9, 7, 1},
			{intstr.FromString("35%"), 9, 8, 2},

			// Rounding down
			{intstr.FromString("33%"), 9, 8, 1},

			// Aggressive
			{intstr.FromInt(10), 10, 10, 10},
			{intstr.FromInt(20), 10, 10, 10},
			{intstr.FromInt(20), 10, 0, 10},
			{intstr.FromString("100%"), 10, 10, 10},
			{intstr.FromString("200%"), 10, 10, 10},
			{intstr.FromString("200%"), 10, 0, 10},

			// Zero max unavailable
			{intstr.FromInt(0), 100, 100, 1},
			{intstr.FromInt(0), 100, 99, 0},
			{intstr.FromString("0%"), 100, 100, 1},
			{intstr.FromString("0%"), 100, 99, 0},

			// Zero state
			{intstr.FromInt(10), 0, 0, 0},
			{intstr.FromInt(10), 0, 10, 0},
		} {
			got := ComputeRollout(&tt.Unavail, tt.Desired, tt.Ready)

			require.Equal(t, tt.Want, got, tt)
		}
	})

	t.Run("defaults to 25%", func(t *testing.T) {
		require.Equal(t, 1, ComputeRollout(nil, 4, 4))
		require.Equal(t, 0, ComputeRollout(nil, 4, 3))
		require.Equal(t, 25, ComputeRollout(nil, 100, 100))
		require.Equal(t, 1, ComputeRollout(nil, 10, 9))
		require.Equal(t, 0, ComputeRollout(nil, 10, 8))
	})
}

func FuzzComputeRollout(f *testing.F) {
	f.Add(uint(1), uint(2), uint(1))

	f.Fuzz(func(t *testing.T, maxUnavail, desired, ready uint) {
		unavail := intstr.FromInt(int(maxUnavail))
		if desired == 0 {
			desired = 1
		}
		got := ComputeRollout(&unavail, int(desired), int(ready))

		msg := fmt.Sprintf("got: %v, seeds: %v %v %v", got, maxUnavail, desired, ready)
		require.True(t, got <= int(desired), msg)
	})
}

'''
'''--- internal/kube/volume_snapshot.go ---
package kube

import (
	"context"
	"errors"
	"sort"
	"time"

	snapshotv1 "github.com/kubernetes-csi/external-snapshotter/client/v6/apis/volumesnapshot/v1"
	"github.com/samber/lo"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

// VolumeSnapshotIsReady returns true if the snapshot is ready to use.
func VolumeSnapshotIsReady(status *snapshotv1.VolumeSnapshotStatus) bool {
	if status == nil {
		return false
	}
	if status.ReadyToUse == nil {
		return false
	}
	return *status.ReadyToUse
}

// Lister can list resources, subset of client.Client.
type Lister interface {
	List(ctx context.Context, list client.ObjectList, opts ...client.ListOption) error
}

// RecentVolumeSnapshot finds the most recent, ready to use VolumeSnapshot.
// This function may not work well given very large lists and therefore assumes a reasonable number of VolumeSnapshots.
// If you must search among many VolumeSnapshots, consider refactoring to use Limit and Continue features of listing.
func RecentVolumeSnapshot(ctx context.Context, lister Lister, namespace string, selector map[string]string) (*snapshotv1.VolumeSnapshot, error) {
	var snapshots snapshotv1.VolumeSnapshotList
	err := lister.List(ctx,
		&snapshots,
		client.InNamespace(namespace),
		client.MatchingLabels(selector),
	)
	if err != nil {
		return nil, err
	}

	filtered := lo.Filter(snapshots.Items, func(s snapshotv1.VolumeSnapshot, _ int) bool {
		return VolumeSnapshotIsReady(s.Status)
	})
	if len(filtered) == 0 {
		return nil, errors.New("no ready to use VolumeSnapshots found")
	}

	sort.Slice(filtered, func(i, j int) bool {
		lhs := statusCreationTime(filtered[i].Status)
		rhs := statusCreationTime(filtered[j].Status)
		return !lhs.Before(rhs)
	})

	found := &filtered[0]
	return found, nil
}

func statusCreationTime(status *snapshotv1.VolumeSnapshotStatus) (zero time.Time) {
	if status == nil {
		return zero
	}
	if status.CreationTime == nil {
		return zero
	}
	return status.CreationTime.Time
}

'''
'''--- internal/kube/volume_snapshot_test.go ---
package kube

import (
	"context"
	"errors"
	"testing"
	"time"

	snapshotv1 "github.com/kubernetes-csi/external-snapshotter/client/v6/apis/volumesnapshot/v1"
	"github.com/samber/lo"
	"github.com/stretchr/testify/require"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

func TestVolumeSnapshotIsReady(t *testing.T) {
	t.Parallel()

	var (
		isReady  = true
		notReady bool
	)

	for _, tt := range []struct {
		Status *snapshotv1.VolumeSnapshotStatus
		Want   bool
	}{
		{nil, false},
		{new(snapshotv1.VolumeSnapshotStatus), false},
		{&snapshotv1.VolumeSnapshotStatus{ReadyToUse: &notReady}, false},

		{&snapshotv1.VolumeSnapshotStatus{ReadyToUse: &isReady}, true},
	} {
		require.Equal(t, tt.Want, VolumeSnapshotIsReady(tt.Status), tt)
	}
}

type mockLister func(ctx context.Context, list client.ObjectList, opts ...client.ListOption) error

func (fn mockLister) List(ctx context.Context, list client.ObjectList, opts ...client.ListOption) error {
	return fn(ctx, list, opts...)
}

func TestRecentVolumeSnapshot(t *testing.T) {
	t.Parallel()

	var (
		ctx      = context.Background()
		selector = map[string]string{"test": "selector"}
	)

	const namespace = "testns"

	t.Run("happy path", func(t *testing.T) {
		now := metav1.Now()
		var snap1 snapshotv1.VolumeSnapshot
		snap1.Name = "snap1"
		snap1.Status = &snapshotv1.VolumeSnapshotStatus{
			CreationTime: ptr(now),
			ReadyToUse:   ptr(true),
		}

		snap2 := *snap1.DeepCopy()
		snap2.Name = "snap2"
		snap2.Status.CreationTime = ptr(metav1.NewTime(now.Add(-time.Hour)))

		snap3 := *snap1.DeepCopy()
		snap3.Name = "snap3"
		snap3.Status = nil

		snap4 := *snap1.DeepCopy()
		snap4.Name = "snap4"
		snap4.Status = &snapshotv1.VolumeSnapshotStatus{} // empty

		var list snapshotv1.VolumeSnapshotList
		list.Items = lo.Shuffle(append(list.Items, snap2, snap1, snap4, snap3))

		lister := mockLister(func(ctx context.Context, inList client.ObjectList, opts ...client.ListOption) error {
			require.NotNil(t, ctx)
			require.NotNil(t, inList)
			require.Equal(t, 2, len(opts))
			require.Equal(t, client.InNamespace("testns"), opts[0])
			require.Equal(t, client.MatchingLabels{"test": "selector"}, opts[1])

			ref := inList.(*snapshotv1.VolumeSnapshotList)
			*ref = list
			return nil
		})

		got, err := RecentVolumeSnapshot(ctx, lister, namespace, selector)
		require.NoError(t, err)
		require.Equal(t, snap1.Name, got.Name)
	})

	t.Run("no items", func(t *testing.T) {
		var list snapshotv1.VolumeSnapshotList
		lister := mockLister(func(ctx context.Context, inList client.ObjectList, opts ...client.ListOption) error {
			ref := inList.(*snapshotv1.VolumeSnapshotList)
			*ref = list
			return nil
		})

		_, err := RecentVolumeSnapshot(ctx, lister, namespace, selector)
		require.Error(t, err)
		require.EqualError(t, err, "no ready to use VolumeSnapshots found")
	})

	t.Run("error", func(t *testing.T) {
		lister := mockLister(func(ctx context.Context, list client.ObjectList, opts ...client.ListOption) error {
			return errors.New("boom")
		})

		_, err := RecentVolumeSnapshot(ctx, lister, namespace, selector)

		require.Error(t, err)
		require.EqualError(t, err, "boom")
	})

	t.Run("not ready to use", func(t *testing.T) {
		for _, tt := range []struct {
			Status *snapshotv1.VolumeSnapshotStatus
		}{
			{nil},
			{&snapshotv1.VolumeSnapshotStatus{}},
			{&snapshotv1.VolumeSnapshotStatus{ReadyToUse: ptr(false)}},
		} {
			var snap1 snapshotv1.VolumeSnapshot
			snap1.Name = "not-ready"
			snap1.Status = tt.Status

			var list snapshotv1.VolumeSnapshotList
			list.Items = append(list.Items, snap1)

			lister := mockLister(func(ctx context.Context, inList client.ObjectList, opts ...client.ListOption) error {
				ref := inList.(*snapshotv1.VolumeSnapshotList)
				*ref = list
				return nil
			})

			_, err := RecentVolumeSnapshot(ctx, lister, namespace, selector)
			require.Error(t, err)
			require.EqualError(t, err, "no ready to use VolumeSnapshots found")

			snap2 := snap1.DeepCopy()
			snap2.Name = "found"
			snap2.Status = &snapshotv1.VolumeSnapshotStatus{
				ReadyToUse: ptr(true),
			}
			list.Items = append(list.Items, *snap2)

			got, err := RecentVolumeSnapshot(ctx, lister, namespace, selector)

			require.NoError(t, err)
			require.Equal(t, "found", got.Name)
		}
	})
}

'''
'''--- internal/statefuljob/active_job.go ---
package statefuljob

import (
	"context"

	cosmosalpha "github.com/strangelove-ventures/cosmos-operator/api/v1alpha1"
	"github.com/strangelove-ventures/cosmos-operator/internal/kube"
	batchv1 "k8s.io/api/batch/v1"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

type Getter interface {
	Get(ctx context.Context, key client.ObjectKey, obj client.Object, opts ...client.GetOption) error
}

var isNotFoundErr = kube.IsNotFound

// FindActiveJob finds the currently active job in any state. A job is considered inactive if it cannot
// be found.
func FindActiveJob(ctx context.Context, getter Getter, crd *cosmosalpha.StatefulJob) (bool, *batchv1.Job, error) {
	job := new(batchv1.Job)
	job.Name = ResourceName(crd)
	job.Namespace = crd.Namespace
	err := getter.Get(ctx, client.ObjectKeyFromObject(job), job)
	switch {
	case isNotFoundErr(err):
		return false, nil, nil
	case err != nil:
		return false, nil, err
	}
	return true, job, nil
}

'''
'''--- internal/statefuljob/active_job_test.go ---
package statefuljob

import (
	"context"
	"errors"
	"testing"

	cosmosalpha "github.com/strangelove-ventures/cosmos-operator/api/v1alpha1"
	"github.com/strangelove-ventures/cosmos-operator/internal/kube"
	"github.com/stretchr/testify/require"
	batchv1 "k8s.io/api/batch/v1"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

type mockGetter func(ctx context.Context, key client.ObjectKey, obj client.Object, opts ...client.GetOption) error

func (fn mockGetter) Get(ctx context.Context, key client.ObjectKey, obj client.Object, opts ...client.GetOption) error {
	return fn(ctx, key, obj, opts...)
}

func TestFindActiveJob(t *testing.T) {
	t.Parallel()

	var (
		ctx = context.Background()
		crd cosmosalpha.StatefulJob
	)
	crd.Namespace = "test-ns"
	crd.Name = "test"

	var foundJob batchv1.Job
	foundJob.Name = "found-me"

	t.Run("happy path", func(t *testing.T) {
		getter := mockGetter(func(ctx context.Context, key client.ObjectKey, obj client.Object, opts ...client.GetOption) error {
			require.NotNil(t, ctx)
			require.Equal(t, "test-ns", key.Namespace)
			require.Equal(t, "test", key.Name)
			require.Empty(t, opts)

			ref := obj.(*batchv1.Job)
			*ref = foundJob

			return nil
		})

		found, job, err := FindActiveJob(ctx, getter, &crd)

		require.NoError(t, err)
		require.True(t, found)
		require.Equal(t, foundJob, *job)
	})

	t.Run("not found", func(t *testing.T) {
		isNotFoundErr = func(err error) bool { return true }

		getter := mockGetter(func(ctx context.Context, key client.ObjectKey, obj client.Object, opts ...client.GetOption) error {
			return errors.New("stub not found")
		})

		found, _, err := FindActiveJob(ctx, getter, &crd)

		require.NoError(t, err)
		require.False(t, found)
	})

	t.Run("error", func(t *testing.T) {
		isNotFoundErr = kube.IsNotFound

		getter := mockGetter(func(ctx context.Context, key client.ObjectKey, obj client.Object, opts ...client.GetOption) error {
			return errors.New("boom")
		})

		_, _, err := FindActiveJob(ctx, getter, &crd)

		require.Error(t, err)
		require.EqualError(t, err, "boom")
	})
}

'''
'''--- internal/statefuljob/create.go ---
package statefuljob

import (
	"context"
	"fmt"

	cosmosalpha "github.com/strangelove-ventures/cosmos-operator/api/v1alpha1"
	"k8s.io/apimachinery/pkg/runtime"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/log"
)

// CreateClient creates and sets owner reference.
type CreateClient interface {
	Create(ctx context.Context, obj client.Object, opts ...client.CreateOption) error
	Scheme() *runtime.Scheme
}

// Creator creates objects and assigns the owner reference.
type Creator[T client.Object] struct {
	builder func() ([]T, error)
	client  CreateClient
}

// NewCreator returns a valid Creator.
func NewCreator[T client.Object](client CreateClient, builder func() ([]T, error)) Creator[T] {
	return Creator[T]{
		builder: builder,
		client:  client,
	}
}

// Create builds the resources, creates them, and assigns owner reference.
func (c Creator[T]) Create(ctx context.Context, crd *cosmosalpha.StatefulJob) error {
	resources, err := c.builder()
	if err != nil {
		return fmt.Errorf("build resources: %w", err)
	}

	logger := log.FromContext(ctx)
	for _, r := range resources {
		gk := r.GetObjectKind().GroupVersionKind().GroupKind().String()
		logger.Info("Creating resource", "groupKind", gk, "resource", r.GetName())
		if err = c.client.Create(ctx, r); err != nil {
			return err
		}
		err = ctrl.SetControllerReference(crd, r, c.client.Scheme())
		if err != nil {
			return err
		}
	}

	return nil
}

'''
'''--- internal/statefuljob/create_test.go ---
package statefuljob

import (
	"context"
	"errors"
	"fmt"
	"testing"

	cosmosalpha "github.com/strangelove-ventures/cosmos-operator/api/v1alpha1"
	"github.com/stretchr/testify/require"
	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

func requireOwner(t *testing.T, crd metav1.Object, obj client.Object) {
	t.Helper()
	require.NotEmpty(t, crd.GetName())
	require.Equal(t, crd.GetName(), obj.GetOwnerReferences()[0].Name)
	require.Equal(t, "StatefulJob", obj.GetOwnerReferences()[0].Kind)
	require.True(t, *obj.GetOwnerReferences()[0].Controller)
}

type mockCreateClient struct {
	GotObjects []client.Object
	Err        error
}

func (m *mockCreateClient) Create(ctx context.Context, obj client.Object, opts ...client.CreateOption) error {
	if ctx == nil {
		panic("nil context")
	}
	if len(opts) > 0 {
		panic(fmt.Errorf("expected 0 opts, got %d", len(opts)))
	}
	m.GotObjects = append(m.GotObjects, obj)
	return m.Err
}

func (m *mockCreateClient) Scheme() *runtime.Scheme {
	scheme := runtime.NewScheme()
	if err := cosmosalpha.AddToScheme(scheme); err != nil {
		panic(err)
	}
	return scheme
}

func TestCreator_Create(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	t.Run("happy path", func(t *testing.T) {
		var (
			crd     cosmosalpha.StatefulJob
			mClient mockCreateClient
			pods    = []*corev1.Pod{new(corev1.Pod), new(corev1.Pod)}
		)
		crd.Name = "create-test"

		creator := NewCreator(&mClient, func() ([]*corev1.Pod, error) {
			return pods, nil
		})

		err := creator.Create(ctx, &crd)

		require.NoError(t, err)
		require.Equal(t, 2, len(mClient.GotObjects))

		for _, pod := range mClient.GotObjects {
			requireOwner(t, &crd, pod)
		}
	})

	t.Run("no resources", func(t *testing.T) {
		var (
			crd     cosmosalpha.StatefulJob
			mClient mockCreateClient
		)
		creator := NewCreator(&mClient, func() ([]*corev1.Pod, error) {
			return nil, nil
		})
		err := creator.Create(ctx, &crd)

		require.NoError(t, err)
		require.Empty(t, mClient.GotObjects)
	})

	t.Run("builder error", func(t *testing.T) {
		var (
			crd     cosmosalpha.StatefulJob
			mClient mockCreateClient
		)
		crd.Name = "create-test"
		creator := NewCreator(&mClient, func() ([]*corev1.Pod, error) {
			return nil, errors.New("boom")
		})
		err := creator.Create(ctx, &crd)
		require.Error(t, err)
		require.EqualError(t, err, "build resources: boom")
	})

	t.Run("create error", func(t *testing.T) {
		var (
			crd     cosmosalpha.StatefulJob
			mClient mockCreateClient
			pods    = []*corev1.Pod{new(corev1.Pod), new(corev1.Pod)}
		)

		creator := NewCreator(&mClient, func() ([]*corev1.Pod, error) {
			return pods, nil
		})

		mClient.Err = errors.New("boom")
		err := creator.Create(ctx, &crd)

		require.Error(t, err)
		require.EqualError(t, err, "boom")
	})
}

'''
'''--- internal/statefuljob/job.go ---
package statefuljob

import (
	"time"

	cosmosalpha "github.com/strangelove-ventures/cosmos-operator/api/v1alpha1"
	batchv1 "k8s.io/api/batch/v1"
	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

// BuildJobs returns jobs to compress and upload data to an object storage.
func BuildJobs(crd *cosmosalpha.StatefulJob) []*batchv1.Job {
	job := batchv1.Job{
		TypeMeta: metav1.TypeMeta{
			Kind:       "Job",
			APIVersion: batchv1.SchemeGroupVersion.String(),
		},
		Spec: batchv1.JobSpec{
			// Set defaults
			ActiveDeadlineSeconds:   ptr(int64(24 * time.Hour.Seconds())),
			BackoffLimit:            ptr(int32(5)),
			TTLSecondsAfterFinished: ptr(int32(15 * time.Minute.Seconds())),

			Template: crd.Spec.PodTemplate,
		},
	}
	job.Labels = defaultLabels()
	job.Namespace = crd.Namespace
	job.Name = ResourceName(crd)

	if v := crd.Spec.JobTemplate.ActiveDeadlineSeconds; v != nil {
		job.Spec.ActiveDeadlineSeconds = v
	}
	if v := crd.Spec.JobTemplate.BackoffLimit; v != nil {
		job.Spec.BackoffLimit = v
	}
	if v := crd.Spec.JobTemplate.TTLSecondsAfterFinished; v != nil {
		job.Spec.TTLSecondsAfterFinished = v
	}

	job.Spec.Template.Spec.Volumes = append(job.Spec.Template.Spec.Volumes, corev1.Volume{
		Name: "snapshot",
		VolumeSource: corev1.VolumeSource{PersistentVolumeClaim: &corev1.PersistentVolumeClaimVolumeSource{
			ClaimName: ResourceName(crd),
		}},
	})

	if job.Spec.Template.Spec.RestartPolicy == "" {
		job.Spec.Template.Spec.RestartPolicy = corev1.RestartPolicyNever
	}

	return []*batchv1.Job{&job}
}

'''
'''--- internal/statefuljob/job_list.go ---
package statefuljob

import batchv1 "k8s.io/api/batch/v1"

// AddJobStatus adds the status to the head of the list.
func AddJobStatus(existing []batchv1.JobStatus, status batchv1.JobStatus) []batchv1.JobStatus {
	list := append([]batchv1.JobStatus{status}, existing...)
	const maxSize = 5
	if len(list) > maxSize {
		list = list[:maxSize]
	}
	return list
}

// UpdateJobStatus updates the most recent status (at the head).
// If the list is empty, this operation is a no-op.
func UpdateJobStatus(existing []batchv1.JobStatus, status batchv1.JobStatus) []batchv1.JobStatus {
	if len(existing) == 0 {
		return existing
	}
	return append([]batchv1.JobStatus{status}, existing[1:]...)
}

'''
'''--- internal/statefuljob/job_list_test.go ---
package statefuljob

import (
	"testing"

	"github.com/stretchr/testify/require"
	batchv1 "k8s.io/api/batch/v1"
)

func TestAddJobStatus(t *testing.T) {
	t.Parallel()

	list := AddJobStatus(nil, batchv1.JobStatus{})
	require.Len(t, list, 1)

	for i := 0; i < 15; i++ {
		list = AddJobStatus(list, batchv1.JobStatus{})
	}

	require.Len(t, list, 5)

	status := batchv1.JobStatus{Active: 1}
	list = AddJobStatus(list, status)

	require.Equal(t, status, list[0])
}

func TestJobList_Update(t *testing.T) {
	t.Parallel()

	list := UpdateJobStatus(nil, batchv1.JobStatus{})

	require.Empty(t, list)

	status := batchv1.JobStatus{Active: 1}
	list = UpdateJobStatus(make([]batchv1.JobStatus, 2), status)

	require.Len(t, list, 2)
	require.Equal(t, status, list[0])
}

'''
'''--- internal/statefuljob/job_test.go ---
package statefuljob

import (
	"testing"

	"github.com/samber/lo"
	cosmosalpha "github.com/strangelove-ventures/cosmos-operator/api/v1alpha1"
	"github.com/stretchr/testify/require"
	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

func TestBuildJobs(t *testing.T) {
	t.Run("happy path", func(t *testing.T) {
		crd := cosmosalpha.StatefulJob{
			ObjectMeta: metav1.ObjectMeta{
				Name:      "axelar",
				Namespace: "test",
			},
			Spec: cosmosalpha.StatefulJobSpec{
				JobTemplate: cosmosalpha.JobTemplateSpec{
					ActiveDeadlineSeconds:   ptr(int64(20)),
					BackoffLimit:            ptr(int32(1)),
					TTLSecondsAfterFinished: ptr(int32(10)),
				},
				PodTemplate: corev1.PodTemplateSpec{
					Spec: corev1.PodSpec{
						RestartPolicy: corev1.RestartPolicyAlways,
					},
				},
			},
		}

		jobs := BuildJobs(&crd)
		require.Len(t, jobs, 1)
		got := jobs[0]

		require.Equal(t, "test", got.Namespace)
		require.Equal(t, "axelar", got.Name)

		wantLabels := map[string]string{
			"app.kubernetes.io/created-by": "cosmos-operator",
			"app.kubernetes.io/component":  "StatefulJob",
		}
		require.Equal(t, wantLabels, got.Labels)

		require.EqualValues(t, 20, *got.Spec.ActiveDeadlineSeconds)
		require.EqualValues(t, 1, *got.Spec.BackoffLimit)
		require.EqualValues(t, 10, *got.Spec.TTLSecondsAfterFinished)

		require.Nil(t, got.Spec.Parallelism)
		require.Equal(t, corev1.RestartPolicyAlways, got.Spec.Template.Spec.RestartPolicy)
	})

	t.Run("defaults", func(t *testing.T) {
		var crd cosmosalpha.StatefulJob

		jobs := BuildJobs(&crd)
		require.Len(t, jobs, 1)

		got := jobs[0]

		require.EqualValues(t, 900, *got.Spec.TTLSecondsAfterFinished)
		require.EqualValues(t, 5, *got.Spec.BackoffLimit)
		require.EqualValues(t, 86_400, *got.Spec.ActiveDeadlineSeconds)

		require.Equal(t, corev1.RestartPolicyNever, got.Spec.Template.Spec.RestartPolicy)
		require.Len(t, got.Spec.Template.Spec.Volumes, 1)
	})

	t.Run("volumes", func(t *testing.T) {
		crd := cosmosalpha.StatefulJob{
			ObjectMeta: metav1.ObjectMeta{
				Name: "cosmoshub",
			},
			Spec: cosmosalpha.StatefulJobSpec{
				PodTemplate: corev1.PodTemplateSpec{
					Spec: corev1.PodSpec{
						Volumes: make([]corev1.Volume, 2),
					},
				},
			},
		}

		jobs := BuildJobs(&crd)
		require.Len(t, jobs, 1)
		got := jobs[0]

		require.Len(t, got.Spec.Template.Spec.Volumes, 3)
		gotVol, err := lo.Last(got.Spec.Template.Spec.Volumes)
		require.NoError(t, err)
		require.Equal(t, "snapshot", gotVol.Name)
		require.Equal(t, "cosmoshub", gotVol.VolumeSource.PersistentVolumeClaim.ClaimName)
	})
}

'''
'''--- internal/statefuljob/labels.go ---
package statefuljob

import (
	cosmosalpha "github.com/strangelove-ventures/cosmos-operator/api/v1alpha1"
	"github.com/strangelove-ventures/cosmos-operator/internal/kube"
)

func defaultLabels() map[string]string {
	return map[string]string{
		kube.ControllerLabel: "cosmos-operator",
		kube.ComponentLabel:  cosmosalpha.StatefulJobController,
	}
}

'''
'''--- internal/statefuljob/predicate.go ---
package statefuljob

import (
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"sigs.k8s.io/controller-runtime/pkg/event"
	"sigs.k8s.io/controller-runtime/pkg/predicate"
)

// DeletePredicate watches all delete events.
func DeletePredicate() predicate.Predicate {
	return &predicate.Funcs{
		DeleteFunc: func(ev event.DeleteEvent) bool {
			return true
		},
	}
}

// LabelSelectorPredicate returns a predicate matching default labels created by the operator.
func LabelSelectorPredicate() predicate.Predicate {
	pred, err := predicate.LabelSelectorPredicate(metav1.LabelSelector{MatchLabels: defaultLabels()})
	if err != nil {
		panic(err)
	}
	return pred
}

'''
'''--- internal/statefuljob/ptr.go ---
package statefuljob

func ptr[T any](v T) *T {
	return &v
}

'''
'''--- internal/statefuljob/pvc.go ---
package statefuljob

import (
	"fmt"

	snapshotv1 "github.com/kubernetes-csi/external-snapshotter/client/v6/apis/volumesnapshot/v1"
	cosmosalpha "github.com/strangelove-ventures/cosmos-operator/api/v1alpha1"
	corev1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/api/resource"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

// BuildPVCs builds PVCs given the crd and VolumeSnapshot.
func BuildPVCs(crd *cosmosalpha.StatefulJob, vs *snapshotv1.VolumeSnapshot) ([]*corev1.PersistentVolumeClaim, error) {
	storage, err := findStorage(vs)
	if err != nil {
		return nil, err
	}

	pvc := corev1.PersistentVolumeClaim{
		TypeMeta: metav1.TypeMeta{
			Kind:       "PersistentVolumeClaim",
			APIVersion: "v1",
		},
		Spec: corev1.PersistentVolumeClaimSpec{
			StorageClassName: ptr(crd.Spec.VolumeClaimTemplate.StorageClassName),
			AccessModes:      crd.Spec.VolumeClaimTemplate.AccessModes,
			DataSource: &corev1.TypedLocalObjectReference{
				APIGroup: ptr(vs.GroupVersionKind().Group),
				Kind:     vs.Kind,
				Name:     vs.Name,
			},
			Resources: corev1.ResourceRequirements{
				Requests: corev1.ResourceList{corev1.ResourceStorage: storage},
			},
		},
	}
	pvc.Namespace = crd.Namespace
	pvc.Name = ResourceName(crd)
	pvc.Labels = defaultLabels()

	return []*corev1.PersistentVolumeClaim{&pvc}, nil
}

func findStorage(vs *snapshotv1.VolumeSnapshot) (zero resource.Quantity, _ error) {
	if vs.Status == nil {
		return zero, fmt.Errorf("%s %s: missing status subresource", vs.Kind, vs.Name)
	}
	if vs.Status.RestoreSize == nil {
		return zero, fmt.Errorf("%s %s: missing status.restoreSize", vs.Kind, vs.Name)
	}
	return *vs.Status.RestoreSize, nil
}

'''
'''--- internal/statefuljob/pvc_test.go ---
package statefuljob

import (
	"testing"

	snapshotv1 "github.com/kubernetes-csi/external-snapshotter/client/v6/apis/volumesnapshot/v1"
	cosmosalpha "github.com/strangelove-ventures/cosmos-operator/api/v1alpha1"
	"github.com/stretchr/testify/require"
	corev1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/api/resource"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

func TestBuildPVCs(t *testing.T) {
	t.Run("happy path", func(t *testing.T) {
		crd := cosmosalpha.StatefulJob{
			Spec: cosmosalpha.StatefulJobSpec{
				VolumeClaimTemplate: cosmosalpha.StatefulJobVolumeClaimTemplate{
					StorageClassName: "primo",
					AccessModes:      []corev1.PersistentVolumeAccessMode{corev1.ReadWriteOncePod},
				},
			},
		}
		crd.Name = "my-test"
		crd.Namespace = "test"

		vs := snapshotv1.VolumeSnapshot{
			TypeMeta: metav1.TypeMeta{
				Kind:       "VolumeSnapshot",
				APIVersion: "snapshot.storage.k8s.io/v1",
			},
			ObjectMeta: metav1.ObjectMeta{
				Name: "my-snapshot",
			},
			Status: &snapshotv1.VolumeSnapshotStatus{
				RestoreSize: ptr(resource.MustParse("10Gi")),
			},
		}

		pvcs, err := BuildPVCs(&crd, &vs)

		require.NoError(t, err)
		require.Len(t, pvcs, 1)

		got := pvcs[0]

		require.Equal(t, "my-test", got.Name)
		require.Equal(t, "test", got.Namespace)

		require.Equal(t, "my-snapshot", got.Spec.DataSource.Name)
		require.Equal(t, "VolumeSnapshot", got.Spec.DataSource.Kind)
		require.Equal(t, "snapshot.storage.k8s.io", *got.Spec.DataSource.APIGroup)
		require.Equal(t, "primo", *got.Spec.StorageClassName)
		require.Equal(t, corev1.ReadWriteOncePod, got.Spec.AccessModes[0])

		require.EqualValues(t, "10Gi", got.Spec.Resources.Requests.Storage().String())

		wantLabels := map[string]string{
			"app.kubernetes.io/created-by": "cosmos-operator",
			"app.kubernetes.io/component":  "StatefulJob",
		}
		require.Equal(t, wantLabels, got.Labels)
	})

	t.Run("no storage size", func(t *testing.T) {
		for _, tt := range []struct {
			Status *snapshotv1.VolumeSnapshotStatus
		}{
			{nil},
			{&snapshotv1.VolumeSnapshotStatus{}},
		} {
			crd := cosmosalpha.StatefulJob{}

			vs := snapshotv1.VolumeSnapshot{
				Status: tt.Status,
			}

			_, err := BuildPVCs(&crd, &vs)
			require.Error(t, err)
		}
	})
}

'''
'''--- internal/statefuljob/resource.go ---
package statefuljob

import (
	cosmosalpha "github.com/strangelove-ventures/cosmos-operator/api/v1alpha1"
	"github.com/strangelove-ventures/cosmos-operator/internal/kube"
)

// ResourceName is the name of all resources created by the controller.
func ResourceName(crd *cosmosalpha.StatefulJob) string {
	return kube.ToName(crd.Name)
}

'''
'''--- internal/statefuljob/resource_test.go ---
package statefuljob

import (
	"strings"
	"testing"

	cosmosalpha "github.com/strangelove-ventures/cosmos-operator/api/v1alpha1"
	"github.com/stretchr/testify/require"
)

func TestResourceName(t *testing.T) {
	t.Parallel()

	var crd cosmosalpha.StatefulJob
	crd.Name = "test"

	require.Equal(t, "test", ResourceName(&crd))

	crd.Name = strings.Repeat("long", 100)
	name := ResourceName(&crd)
	require.LessOrEqual(t, 253, len(name))
}

'''
'''--- internal/statefuljob/schedule.go ---
package statefuljob

import (
	"time"

	cosmosalpha "github.com/strangelove-ventures/cosmos-operator/api/v1alpha1"
)

// ReadyForSnapshot returns true if enough time has passed to create a new snapshot.
func ReadyForSnapshot(crd *cosmosalpha.StatefulJob, now time.Time) bool {
	history := crd.Status.JobHistory
	if len(history) == 0 {
		return true
	}

	dur := crd.Spec.Interval.Duration
	if dur <= 0 {
		dur = 24 * time.Hour
	}

	// JobHistory should always have most recent first.
	status := history[0]
	return now.Sub(status.StartTime.Time) >= dur
}

'''
'''--- internal/statefuljob/schedule_test.go ---
package statefuljob

import (
	"testing"
	"time"

	cosmosalpha "github.com/strangelove-ventures/cosmos-operator/api/v1alpha1"
	"github.com/stretchr/testify/require"
	batchv1 "k8s.io/api/batch/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

func TestReadyForSnapshot(t *testing.T) {
	t.Run("happy path", func(t *testing.T) {
		const duration = time.Hour
		now := time.Now()
		crd := cosmosalpha.StatefulJob{
			Spec: cosmosalpha.StatefulJobSpec{
				Interval: metav1.Duration{Duration: duration},
			},
			Status: cosmosalpha.StatefulJobStatus{
				JobHistory: []batchv1.JobStatus{
					{StartTime: ptr(metav1.NewTime(now))},
					{StartTime: ptr(metav1.NewTime(now.Add(-2 * duration)))},
				},
			},
		}

		require.True(t, ReadyForSnapshot(&crd, now.Add(duration)))
		require.True(t, ReadyForSnapshot(&crd, now.Add(duration+1)))
		require.False(t, ReadyForSnapshot(&crd, now.Add(duration-1)))
		require.False(t, ReadyForSnapshot(&crd, now))
	})

	t.Run("default", func(t *testing.T) {
		const duration = 24 * time.Hour
		now := time.Now()
		crd := cosmosalpha.StatefulJob{
			Status: cosmosalpha.StatefulJobStatus{
				JobHistory: []batchv1.JobStatus{
					{StartTime: ptr(metav1.NewTime(now))},
					{StartTime: ptr(metav1.NewTime(now.Add(-duration)))},
				},
			},
		}

		require.True(t, ReadyForSnapshot(&crd, now.Add(duration)))
		require.True(t, ReadyForSnapshot(&crd, now.Add(duration+1)))
		require.False(t, ReadyForSnapshot(&crd, now))
	})

	t.Run("zero state", func(t *testing.T) {
		now := time.Now()
		var crd cosmosalpha.StatefulJob

		require.True(t, ReadyForSnapshot(&crd, now))
		require.True(t, ReadyForSnapshot(&crd, now.Add(24*time.Hour)))
		require.True(t, ReadyForSnapshot(&crd, now.Add(-24*time.Hour)))
	})
}

'''
'''--- internal/test/assertions.go ---
package test

import (
	"testing"

	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	"github.com/stretchr/testify/require"
)

func HasTypeLabel(t *testing.T, builder func(crd cosmosv1.CosmosFullNode) []map[string]string) {
	t.Run("sets labels for", func(t *testing.T) {
		var crd cosmosv1.CosmosFullNode
		crd.Spec.Replicas = 3

		t.Run("type", func(t *testing.T) {
			t.Run("given unspecified type sets type to FullNode", func(t *testing.T) {
				resources := builder(crd)

				for _, resource := range resources {
					require.Equal(t, "FullNode", resource["cosmos.strange.love/type"])
				}
			})

			t.Run("given Sentry type", func(t *testing.T) {
				crd.Spec.Type = "Sentry"
				resources := builder(crd)

				for _, resource := range resources {
					require.Equal(t, "Sentry", resource["cosmos.strange.love/type"])
				}
			})

			t.Run("given FullNode type", func(t *testing.T) {
				crd.Spec.Type = "FullNode"
				resources := builder(crd)

				for _, resource := range resources {
					require.Equal(t, "FullNode", resource["cosmos.strange.love/type"])
				}
			})
		})
	})
}

'''
'''--- internal/test/doc.go ---
// Package test defines custom test helpers.
package test

'''
'''--- internal/test/metadata.go ---
package test

import (
	"testing"

	"github.com/stretchr/testify/require"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

// RequireValidMetadata asserts valid metadata properties such as name and label length.
func RequireValidMetadata(t *testing.T, obj client.Object) {
	t.Helper()

	require.LessOrEqual(t, len(obj.GetName()), 253)
	for k, v := range obj.GetLabels() {
		require.LessOrEqual(t, len(k), 63)
		require.LessOrEqual(t, len(v), 63, k)
	}
	for k, v := range obj.GetAnnotations() {
		require.LessOrEqual(t, len(k), 63)
		require.LessOrEqual(t, len(v), 63, k)
	}
}

'''
'''--- internal/test/mock_reporter.go ---
package test

// NopReporter is a no-op kube.Reporter.
type NopReporter struct{}

func (n NopReporter) Info(msg string, keysAndValues ...interface{})             {}
func (n NopReporter) Debug(msg string, keysAndValues ...interface{})            {}
func (n NopReporter) Error(err error, msg string, keysAndValues ...interface{}) {}
func (n NopReporter) RecordInfo(reason, msg string)                             {}
func (n NopReporter) RecordError(reason string, err error)                      {}

'''
'''--- internal/version/version.go ---
package version

// version is the version of the build.
// Set via ldflags at build time.
// Used for docker image.
// See Dockerfile, Makefile, and .github/workflows/release.yaml.
var version = ""

// DockerTag returns the version of the build or "latest" if unknown.
func DockerTag() string {
	if version == "" {
		return "latest"
	}
	return version
}

// AppVersion returns the version of the build or "(devel)" if unknown.
func AppVersion() string {
	if version == "" {
		return "(devel)"
	}
	return version
}

'''
'''--- internal/volsnapshot/fullnode_control.go ---
package volsnapshot

import (
	"context"
	"fmt"
	"strings"

	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	cosmosalpha "github.com/strangelove-ventures/cosmos-operator/api/v1alpha1"
	"github.com/strangelove-ventures/cosmos-operator/internal/kube"
	corev1 "k8s.io/api/core/v1"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

type StatusSyncer interface {
	SyncUpdate(ctx context.Context, key client.ObjectKey, update func(status *cosmosv1.FullNodeStatus)) error
}

// FullNodeControl manages a ScheduledVolumeSnapshot's spec.fullNodeRef.
type FullNodeControl struct {
	client       client.Reader
	statusClient StatusSyncer
}

func NewFullNodeControl(statusClient StatusSyncer, client client.Reader) *FullNodeControl {
	return &FullNodeControl{client: client, statusClient: statusClient}
}

// SignalPodDeletion updates the LocalFullNodeRef's status to indicate it should delete the pod candidate.
// The pod is gracefully removed to ensure the highest data integrity while taking a VolumeSnapshot.
// Assumes crd's status.candidate is set, otherwise this method panics.
// Any error returned can be treated as transient and retried.
func (control FullNodeControl) SignalPodDeletion(ctx context.Context, crd *cosmosalpha.ScheduledVolumeSnapshot) error {
	key := control.sourceKey(crd)
	objKey := client.ObjectKey{Name: crd.Spec.FullNodeRef.Name, Namespace: crd.Namespace}
	return control.statusClient.SyncUpdate(ctx, objKey, func(status *cosmosv1.FullNodeStatus) {
		if status.ScheduledSnapshotStatus == nil {
			status.ScheduledSnapshotStatus = make(map[string]cosmosv1.FullNodeSnapshotStatus)
		}
		status.ScheduledSnapshotStatus[key] = cosmosv1.FullNodeSnapshotStatus{PodCandidate: crd.Status.Candidate.PodName}
	})
}

// SignalPodRestoration updates the LocalFullNodeRef's status to indicate it should recreate the pod candidate.
// Any error returned can be treated as transient and retried.
func (control FullNodeControl) SignalPodRestoration(ctx context.Context, crd *cosmosalpha.ScheduledVolumeSnapshot) error {
	key := control.sourceKey(crd)
	objKey := client.ObjectKey{Name: crd.Spec.FullNodeRef.Name, Namespace: crd.Namespace}
	return control.statusClient.SyncUpdate(ctx, objKey, func(status *cosmosv1.FullNodeStatus) {
		delete(status.ScheduledSnapshotStatus, key)
	})
}

// ConfirmPodRestoration verifies the pod has been restored.
func (control FullNodeControl) ConfirmPodRestoration(ctx context.Context, crd *cosmosalpha.ScheduledVolumeSnapshot) error {
	var (
		fullnode cosmosv1.CosmosFullNode
		getKey   = client.ObjectKey{Name: crd.Spec.FullNodeRef.Name, Namespace: crd.Namespace}
	)

	if err := control.client.Get(ctx, getKey, &fullnode); err != nil {
		return fmt.Errorf("get CosmosFullNode: %w", err)
	}

	if _, exists := fullnode.Status.ScheduledSnapshotStatus[control.sourceKey(crd)]; exists {
		return fmt.Errorf("pod %s not restored yet", crd.Status.Candidate.PodName)
	}

	return nil
}

// ConfirmPodDeletion returns a nil error if the pod is deleted.
// Any non-nil error is transient, including if the pod has not been deleted yet.
// Assumes crd's status.candidate is set, otherwise this method panics.
func (control FullNodeControl) ConfirmPodDeletion(ctx context.Context, crd *cosmosalpha.ScheduledVolumeSnapshot) error {
	var pods corev1.PodList
	if err := control.client.List(ctx, &pods,
		client.InNamespace(crd.Namespace),
		client.MatchingFields{kube.ControllerOwnerField: crd.Spec.FullNodeRef.Name},
	); err != nil {
		return fmt.Errorf("list pods: %w", err)
	}
	for _, pod := range pods.Items {
		if pod.Name == crd.Status.Candidate.PodName {
			return fmt.Errorf("pod %s not deleted yet", pod.Name)
		}
	}
	return nil
}

func (control FullNodeControl) sourceKey(crd *cosmosalpha.ScheduledVolumeSnapshot) string {
	key := strings.Join([]string{crd.Namespace, crd.Name, cosmosalpha.GroupVersion.Version, cosmosalpha.GroupVersion.Group}, ".")
	// Remove all slashes because key is used in JSONPatch where slash "/" is a reserved character.
	return strings.ReplaceAll(key, "/", "")
}

'''
'''--- internal/volsnapshot/fullnode_control_test.go ---
package volsnapshot

import (
	"context"
	"errors"
	"testing"

	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	cosmosalpha "github.com/strangelove-ventures/cosmos-operator/api/v1alpha1"
	"github.com/stretchr/testify/require"
	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

type mockStatusSyncer func(ctx context.Context, key client.ObjectKey, update func(status *cosmosv1.FullNodeStatus)) error

func (fn mockStatusSyncer) SyncUpdate(ctx context.Context, key client.ObjectKey, update func(status *cosmosv1.FullNodeStatus)) error {
	if ctx == nil {
		panic("nil context")
	}
	return fn(ctx, key, update)
}

var nopSyncer = mockStatusSyncer(func(ctx context.Context, key client.ObjectKey, update func(status *cosmosv1.FullNodeStatus)) error {
	return nil
})

type mockReader struct {
	Lister func(ctx context.Context, list client.ObjectList, opts ...client.ListOption) error
	Getter func(ctx context.Context, key client.ObjectKey, obj client.Object, opts ...client.GetOption) error
}

func (m mockReader) Get(ctx context.Context, key client.ObjectKey, obj client.Object, opts ...client.GetOption) error {
	if ctx == nil {
		panic("nil context")
	}
	if len(opts) > 0 {
		panic("unexpected opts")
	}
	if m.Getter == nil {
		panic("get called with no implementation")
	}
	return m.Getter(ctx, key, obj, opts...)
}

func (m mockReader) List(ctx context.Context, list client.ObjectList, opts ...client.ListOption) error {
	if ctx == nil {
		panic("nil context")
	}
	if m.Lister == nil {
		panic("list called with no implementation")
	}
	return m.Lister(ctx, list, opts...)
}

var nopReader = mockReader{
	Lister: func(ctx context.Context, list client.ObjectList, opts ...client.ListOption) error { return nil },
	Getter: func(ctx context.Context, key client.ObjectKey, obj client.Object, opts ...client.GetOption) error {
		return nil
	},
}

func TestFullNodeControl_SignalPodDeletion(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	var crd cosmosalpha.ScheduledVolumeSnapshot
	crd.Namespace = "default/" // Tests for slash stripping.
	crd.Name = "my-snapshot"
	crd.Spec.FullNodeRef.Name = "my-node"
	crd.Status.Candidate = &cosmosalpha.SnapshotCandidate{
		PodName: "target-pod",
	}

	t.Run("happy path", func(t *testing.T) {
		var didSync bool
		syncer := mockStatusSyncer(func(ctx context.Context, key client.ObjectKey, update func(status *cosmosv1.FullNodeStatus)) error {
			require.Equal(t, "my-node", key.Name)
			require.Equal(t, "default/", key.Namespace)

			var got cosmosv1.FullNodeStatus
			update(&got)
			want := map[string]cosmosv1.FullNodeSnapshotStatus{
				"default.my-snapshot.v1alpha1.cosmos.strange.love": {PodCandidate: "target-pod"},
			}
			require.Equal(t, want, got.ScheduledSnapshotStatus)

			didSync = true
			return nil
		})

		control := NewFullNodeControl(syncer, nopReader)
		err := control.SignalPodDeletion(ctx, &crd)

		require.NoError(t, err)
		require.True(t, didSync)
	})

	t.Run("patch failed", func(t *testing.T) {
		syncer := mockStatusSyncer(func(ctx context.Context, key client.ObjectKey, update func(status *cosmosv1.FullNodeStatus)) error {
			return errors.New("boom")
		})

		control := NewFullNodeControl(syncer, nopReader)
		err := control.SignalPodDeletion(ctx, &crd)

		require.Error(t, err)
		require.EqualError(t, err, "boom")
	})
}

func TestFullNodeControl_SignalPodRestoration(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	var crd cosmosalpha.ScheduledVolumeSnapshot
	crd.Namespace = "default/" // Tests for slash stripping.
	crd.Name = "my-snapshot"
	crd.Spec.FullNodeRef.Name = "my-node"
	crd.Status.Candidate = &cosmosalpha.SnapshotCandidate{
		PodName: "target-pod",
	}

	t.Run("happy path", func(t *testing.T) {
		var didSync bool
		syncer := mockStatusSyncer(func(ctx context.Context, key client.ObjectKey, update func(status *cosmosv1.FullNodeStatus)) error {
			require.Equal(t, "my-node", key.Name)
			require.Equal(t, "default/", key.Namespace)

			var got cosmosv1.FullNodeStatus
			got.ScheduledSnapshotStatus = map[string]cosmosv1.FullNodeSnapshotStatus{
				"default.my-snapshot.v1alpha1.cosmos.strange.love": {PodCandidate: "target-pod"},
			}
			update(&got)
			require.Empty(t, got.ScheduledSnapshotStatus)

			got.ScheduledSnapshotStatus = map[string]cosmosv1.FullNodeSnapshotStatus{
				"default.my-snapshot.v1alpha1.cosmos.strange.love":      {PodCandidate: "target-pod"},
				"default.another-snapshot.v1alpha1.cosmos.strange.love": {PodCandidate: "another-pod"},
			}
			update(&got)
			want := map[string]cosmosv1.FullNodeSnapshotStatus{
				"default.another-snapshot.v1alpha1.cosmos.strange.love": {PodCandidate: "another-pod"},
			}
			require.Equal(t, want, got.ScheduledSnapshotStatus)

			didSync = true
			return nil
		})

		control := NewFullNodeControl(syncer, nopReader)
		err := control.SignalPodRestoration(ctx, &crd)

		require.NoError(t, err)
		require.True(t, didSync)
	})

	t.Run("patch failed", func(t *testing.T) {
		syncer := mockStatusSyncer(func(ctx context.Context, key client.ObjectKey, update func(status *cosmosv1.FullNodeStatus)) error {
			return errors.New("boom")
		})

		control := NewFullNodeControl(syncer, nopReader)
		err := control.SignalPodRestoration(ctx, &crd)

		require.Error(t, err)
		require.EqualError(t, err, "boom")
	})
}

func TestFullNodeControl_ConfirmPodRestoration(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	var crd cosmosalpha.ScheduledVolumeSnapshot
	crd.Name = "snapshot"
	crd.Namespace = "default"
	crd.Spec.FullNodeRef.Name = "cosmoshub"
	crd.Status.Candidate = &cosmosalpha.SnapshotCandidate{
		PodName: "target-pod",
	}

	t.Run("happy path", func(t *testing.T) {
		for _, tt := range []struct {
			Status map[string]cosmosv1.FullNodeSnapshotStatus
		}{
			{nil},
			{map[string]cosmosv1.FullNodeSnapshotStatus{
				"should-not-be-a-match": {PodCandidate: "target-pod"},
			}},
		} {
			var reader mockReader
			reader.Getter = func(_ context.Context, key client.ObjectKey, obj client.Object, _ ...client.GetOption) error {
				require.Equal(t, "cosmoshub", key.Name)
				require.Equal(t, "default", key.Namespace)
				require.IsType(t, &cosmosv1.CosmosFullNode{}, obj)
				obj.(*cosmosv1.CosmosFullNode).Status.ScheduledSnapshotStatus = map[string]cosmosv1.FullNodeSnapshotStatus{
					"should-not-be-a-match": {PodCandidate: "target-pod"},
				}
				return nil
			}

			control := NewFullNodeControl(nopSyncer, reader)

			err := control.ConfirmPodRestoration(ctx, &crd)
			require.NoError(t, err, tt)
		}
	})

	t.Run("fullnode status not updated yet", func(t *testing.T) {
		var reader mockReader
		reader.Getter = func(_ context.Context, key client.ObjectKey, obj client.Object, _ ...client.GetOption) error {
			obj.(*cosmosv1.CosmosFullNode).Status.ScheduledSnapshotStatus = map[string]cosmosv1.FullNodeSnapshotStatus{
				"default.snapshot.v1alpha1.cosmos.strange.love": {PodCandidate: "target-pod"},
			}
			return nil
		}

		control := NewFullNodeControl(nopSyncer, reader)
		err := control.ConfirmPodRestoration(ctx, &crd)

		require.Error(t, err)
		require.EqualError(t, err, "pod target-pod not restored yet")
	})

	t.Run("get error", func(t *testing.T) {
		var reader mockReader
		reader.Getter = func(_ context.Context, key client.ObjectKey, obj client.Object, _ ...client.GetOption) error {
			return errors.New("boom")
		}

		control := NewFullNodeControl(nopSyncer, reader)
		err := control.ConfirmPodRestoration(ctx, &crd)

		require.Error(t, err)
		require.EqualError(t, err, "get CosmosFullNode: boom")
	})
}

func TestFullNodeControl_ConfirmPodDeletion(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	var crd cosmosalpha.ScheduledVolumeSnapshot
	crd.Namespace = "default"
	crd.Spec.FullNodeRef.Name = "cosmoshub"
	crd.Status.Candidate = &cosmosalpha.SnapshotCandidate{
		PodName: "target-pod",
	}

	t.Run("happy path", func(t *testing.T) {
		var didList bool
		var reader mockReader
		reader.Lister = func(_ context.Context, list client.ObjectList, opts ...client.ListOption) error {
			list.(*corev1.PodList).Items = []corev1.Pod{
				{ObjectMeta: metav1.ObjectMeta{Name: "pod-1"}},
				{ObjectMeta: metav1.ObjectMeta{Name: "pod-2"}},
			}

			require.Len(t, opts, 2)
			var listOpt client.ListOptions
			for _, opt := range opts {
				opt.ApplyToList(&listOpt)
			}
			require.Equal(t, "default", listOpt.Namespace)
			require.Zero(t, listOpt.Limit)
			require.Equal(t, ".metadata.controller=cosmoshub", listOpt.FieldSelector.String())

			didList = true
			return nil
		}

		control := NewFullNodeControl(nopSyncer, reader)

		err := control.ConfirmPodDeletion(ctx, &crd)
		require.NoError(t, err)

		require.True(t, didList)
	})

	t.Run("happy path - no items", func(t *testing.T) {
		control := NewFullNodeControl(nopSyncer, nopReader)
		err := control.ConfirmPodDeletion(ctx, &crd)

		require.NoError(t, err)
	})

	t.Run("pod not deleted yet", func(t *testing.T) {
		var reader mockReader
		reader.Lister = func(_ context.Context, list client.ObjectList, opts ...client.ListOption) error {
			list.(*corev1.PodList).Items = []corev1.Pod{
				{ObjectMeta: metav1.ObjectMeta{Name: "pod-1"}},
				{ObjectMeta: metav1.ObjectMeta{Name: "target-pod"}},
				{ObjectMeta: metav1.ObjectMeta{Name: "pod-2"}},
			}
			return nil
		}

		control := NewFullNodeControl(nopSyncer, reader)
		err := control.ConfirmPodDeletion(ctx, &crd)

		require.Error(t, err)
		require.EqualError(t, err, "pod target-pod not deleted yet")
	})

	t.Run("list error", func(t *testing.T) {
		var reader mockReader
		reader.Lister = func(_ context.Context, list client.ObjectList, opts ...client.ListOption) error {
			return errors.New("boom")
		}

		control := NewFullNodeControl(nopSyncer, reader)
		err := control.ConfirmPodDeletion(ctx, &crd)

		require.Error(t, err)
		require.EqualError(t, err, "list pods: boom")
	})
}

'''
'''--- internal/volsnapshot/ptr.go ---
package volsnapshot

import "github.com/samber/lo"

func ptr[T any](v T) *T {
	return &v
}

func ptrSlice[T any](s []T) []*T {
	return lo.Map(s, func(element T, _ int) *T { return &element })
}

'''
'''--- internal/volsnapshot/scheduler.go ---
package volsnapshot

import (
	"context"
	"fmt"
	"time"

	snapshotv1 "github.com/kubernetes-csi/external-snapshotter/client/v6/apis/volumesnapshot/v1"
	"github.com/robfig/cron/v3"
	"github.com/samber/lo"
	cosmosalpha "github.com/strangelove-ventures/cosmos-operator/api/v1alpha1"
	"github.com/strangelove-ventures/cosmos-operator/internal/kube"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

// Getter is a subset of client.Client.
type Getter interface {
	Get(ctx context.Context, key client.ObjectKey, obj client.Object, opts ...client.GetOption) error
}

// Scheduler calculates schedules using crontabs and currently running VolumeSnapshots.
type Scheduler struct {
	getter Getter
	now    func() time.Time
}

func NewScheduler(getter Getter) *Scheduler {
	return &Scheduler{
		getter: getter,
		now:    time.Now,
	}
}

// CalcNext the duration until it's time to take the next snapshot.
// A zero duration without an error indicates a VolumeSnapshot should be created.
// Updates crd.status with the last VolumeSnapshot status.
func (s Scheduler) CalcNext(crd *cosmosalpha.ScheduledVolumeSnapshot) (time.Duration, error) {
	sched, err := cron.ParseStandard(crd.Spec.Schedule)
	if err != nil {
		return 0, fmt.Errorf("invalid spec.schedule: %w", err)
	}

	refDate := crd.Status.CreatedAt.Time
	if snapStatus := crd.Status.LastSnapshot; snapStatus != nil {
		refDate = snapStatus.StartedAt.Time
	}

	next := sched.Next(refDate)
	return lo.Max([]time.Duration{next.Sub(s.now()), 0}), nil
}

// IsSnapshotReady returns true if the status.LastSnapshot is ready for use and updates the crd.status.lastSnapshot.
// A non-nil error can be treated as transient.
// If VolumeSnapshot is not found, this indicates a rare case where something deleted the VolumeSnapshot before
// detecting if it's ready. In that case, this method returns that the snapshot is ready.
func (s Scheduler) IsSnapshotReady(ctx context.Context, crd *cosmosalpha.ScheduledVolumeSnapshot) (bool, error) {
	var snapshot snapshotv1.VolumeSnapshot
	snapshot.Name = crd.Status.LastSnapshot.Name
	snapshot.Namespace = crd.Namespace

	err := s.getter.Get(ctx, client.ObjectKeyFromObject(&snapshot), &snapshot)
	switch {
	case kube.IsNotFound(err):
		return true, nil
	case err != nil:
		return false, err
	}

	crd.Status.LastSnapshot.Status = snapshot.Status
	return kube.VolumeSnapshotIsReady(snapshot.Status), nil
}

'''
'''--- internal/volsnapshot/scheduler_test.go ---
package volsnapshot

import (
	"context"
	"errors"
	"fmt"
	"testing"
	"time"

	snapshotv1 "github.com/kubernetes-csi/external-snapshotter/client/v6/apis/volumesnapshot/v1"
	cosmosalpha "github.com/strangelove-ventures/cosmos-operator/api/v1alpha1"
	"github.com/stretchr/testify/require"
	apierrors "k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

type mockGet func(ctx context.Context, key client.ObjectKey, obj client.Object, opts ...client.GetOption) error

func (fn mockGet) Get(ctx context.Context, key client.ObjectKey, obj client.Object, opts ...client.GetOption) error {
	if ctx == nil {
		panic("nil context")
	}
	if len(opts) > 0 {
		panic("got unexpected opts")
	}
	return fn(ctx, key, obj)
}

var panicGetter = mockGet(func(ctx context.Context, key client.ObjectKey, obj client.Object, opts ...client.GetOption) error {
	panic("should not be called")
})

func TestScheduler_CalcNext(t *testing.T) {
	t.Parallel()

	t.Run("happy path - first snapshot", func(t *testing.T) {
		var crd cosmosalpha.ScheduledVolumeSnapshot
		refDate := time.Date(2022, time.December, 1, 0, 0, 0, 0, time.UTC)

		for _, scenario := range []struct {
			Status cosmosalpha.ScheduledVolumeSnapshotStatus
			Name   string
		}{
			{cosmosalpha.ScheduledVolumeSnapshotStatus{CreatedAt: metav1.NewTime(refDate)}, "createdAt"},
			{cosmosalpha.ScheduledVolumeSnapshotStatus{LastSnapshot: &cosmosalpha.VolumeSnapshotStatus{StartedAt: metav1.NewTime(refDate)}}, "lastSnapshot.startedAt"},
		} {
			for _, tt := range []struct {
				Schedule     string
				Now          time.Time
				WantDuration time.Duration
			}{
				// Wait
				{
					"0 * * * *", // hourly
					refDate,
					time.Hour,
				},
				{
					"0 * * * *", // hourly
					refDate.Add(30 * time.Minute),
					30 * time.Minute,
				},
				{
					"0 0 * * *", // daily at midnight
					refDate.Add(1 * time.Hour),
					23 * time.Hour,
				},
				{
					"* * * * *", // every minute
					refDate,
					time.Minute,
				},
				{
					"0 */3 * * *", // At minute 0 past every 3rd hour
					refDate,
					3 * time.Hour,
				},

				// Ready
				{
					"0 * * * *", // hourly
					refDate.Add(1 * time.Hour),
					0,
				},
				{
					"0 * * * *", // hourly
					refDate.Add(1 * time.Hour),
					0,
				},
				{
					"0 0 * * *", // daily at midnight
					refDate.Add(24*time.Hour + time.Minute),
					0,
				},
			} {
				crd.Status = scenario.Status
				crd.Spec.Schedule = tt.Schedule
				sched := NewScheduler(panicGetter)
				sched.now = func() time.Time {
					return tt.Now
				}
				got, err := sched.CalcNext(&crd)

				msg := fmt.Sprintf("%s: %+v", scenario.Name, tt)
				require.NoError(t, err, scenario.Name, msg)
				require.Equal(t, tt.WantDuration, got, msg)
			}
		}
	})
}

func TestScheduler_IsSnapshotReady(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	t.Run("happy path - not ready", func(t *testing.T) {
		var crd cosmosalpha.ScheduledVolumeSnapshot
		crd.Namespace = "strangelove"
		crd.Status.LastSnapshot = &cosmosalpha.VolumeSnapshotStatus{
			Name: "my-snapshot-123",
		}

		notReadyStatus := &snapshotv1.VolumeSnapshotStatus{ReadyToUse: ptr(false)}
		sched := NewScheduler(mockGet(func(_ context.Context, key client.ObjectKey, obj client.Object, _ ...client.GetOption) error {
			require.Equal(t, "strangelove", key.Namespace)
			require.Equal(t, "my-snapshot-123", key.Name)

			ref := obj.(*snapshotv1.VolumeSnapshot)
			*ref = snapshotv1.VolumeSnapshot{
				Status: notReadyStatus,
			}
			return nil
		}))

		got, err := sched.IsSnapshotReady(ctx, &crd)

		require.NoError(t, err)
		require.False(t, got)

		require.Equal(t, notReadyStatus, crd.Status.LastSnapshot.Status)
	})

	t.Run("happy path - ready for use", func(t *testing.T) {
		var crd cosmosalpha.ScheduledVolumeSnapshot
		crd.Status.LastSnapshot = new(cosmosalpha.VolumeSnapshotStatus)

		readyStatus := &snapshotv1.VolumeSnapshotStatus{ReadyToUse: ptr(true)}
		sched := NewScheduler(mockGet(func(_ context.Context, key client.ObjectKey, obj client.Object, _ ...client.GetOption) error {
			ref := obj.(*snapshotv1.VolumeSnapshot)
			*ref = snapshotv1.VolumeSnapshot{
				Status: readyStatus,
			}
			return nil
		}))

		got, err := sched.IsSnapshotReady(ctx, &crd)

		require.NoError(t, err)
		require.True(t, got)

		require.Equal(t, readyStatus, crd.Status.LastSnapshot.Status)
	})

	t.Run("not found error", func(t *testing.T) {
		var crd cosmosalpha.ScheduledVolumeSnapshot
		status := new(cosmosalpha.VolumeSnapshotStatus)
		crd.Status.LastSnapshot = status

		sched := NewScheduler(mockGet(func(_ context.Context, key client.ObjectKey, obj client.Object, _ ...client.GetOption) error {
			return &apierrors.StatusError{ErrStatus: metav1.Status{Reason: metav1.StatusReasonNotFound}}
		}))

		got, err := sched.IsSnapshotReady(ctx, &crd)

		require.NoError(t, err)
		require.True(t, got)

		require.Same(t, status, crd.Status.LastSnapshot)
	})

	t.Run("error", func(t *testing.T) {
		var crd cosmosalpha.ScheduledVolumeSnapshot
		crd.Status.LastSnapshot = new(cosmosalpha.VolumeSnapshotStatus)

		sched := NewScheduler(mockGet(func(_ context.Context, key client.ObjectKey, obj client.Object, _ ...client.GetOption) error {
			return errors.New("boom")
		}))

		_, err := sched.IsSnapshotReady(ctx, &crd)

		require.Error(t, err)
		require.EqualError(t, err, "boom")
	})
}

'''
'''--- internal/volsnapshot/status.go ---
package volsnapshot

import (
	"time"

	cosmosalpha "github.com/strangelove-ventures/cosmos-operator/api/v1alpha1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

// ResetStatus resets the CRD's status to appropriate values for the start of a reconcile loop.
func ResetStatus(crd *cosmosalpha.ScheduledVolumeSnapshot) {
	crd.Status.ObservedGeneration = crd.Generation
	crd.Status.StatusMessage = nil
	if crd.Status.CreatedAt.IsZero() {
		crd.Status.CreatedAt = metav1.NewTime(time.Now())
	}
	switch {
	case crd.Spec.Suspend:
		// Restore any temporarily deleted pod and suspend
		crd.Status.Phase = cosmosalpha.SnapshotPhaseRestorePod
	case !crd.Spec.Suspend && crd.Status.Phase == cosmosalpha.SnapshotPhaseSuspended:
		// If user reactivates, reset to beginning.
		crd.Status.Phase = cosmosalpha.SnapshotPhaseWaitingForNext
	case crd.Status.Phase == "":
		// CRD was just created.
		crd.Status.Phase = cosmosalpha.SnapshotPhaseWaitingForNext
	}
}

'''
'''--- internal/volsnapshot/status_test.go ---
package volsnapshot

import (
	"testing"
	"time"

	cosmosalpha "github.com/strangelove-ventures/cosmos-operator/api/v1alpha1"
	"github.com/stretchr/testify/require"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

func TestResetStatus(t *testing.T) {
	t.Run("happy path", func(t *testing.T) {
		var crd cosmosalpha.ScheduledVolumeSnapshot
		crd.Generation = 456
		crd.Status.StatusMessage = ptr("should not see me")
		createdAt := metav1.NewTime(time.Now())
		crd.Status.CreatedAt = createdAt
		crd.Status.Phase = "Test"

		ResetStatus(&crd)

		require.EqualValues(t, 456, crd.Status.ObservedGeneration)
		require.Nil(t, crd.Status.StatusMessage)
		require.Equal(t, createdAt, crd.Status.CreatedAt)
		require.EqualValues(t, "Test", crd.Status.Phase)
	})

	t.Run("fields not set", func(t *testing.T) {
		var crd cosmosalpha.ScheduledVolumeSnapshot
		ResetStatus(&crd)

		require.WithinDuration(t, time.Now(), crd.Status.CreatedAt.Time, 10*time.Second)
		require.Equal(t, cosmosalpha.SnapshotPhaseWaitingForNext, crd.Status.Phase)
	})

	t.Run("suspended", func(t *testing.T) {
		var crd cosmosalpha.ScheduledVolumeSnapshot

		crd.Status.Phase = cosmosalpha.SnapshotPhaseWaitingForPodDeletion
		crd.Spec.Suspend = true
		ResetStatus(&crd)
		require.Equal(t, cosmosalpha.SnapshotPhaseRestorePod, crd.Status.Phase)

		crd.Status.Phase = cosmosalpha.SnapshotPhaseSuspended
		crd.Spec.Suspend = false
		ResetStatus(&crd)
		require.Equal(t, cosmosalpha.SnapshotPhaseWaitingForNext, crd.Status.Phase)

		crd.Status.Phase = cosmosalpha.SnapshotPhaseDeletingPod
		ResetStatus(&crd)
		require.Equal(t, cosmosalpha.SnapshotPhaseDeletingPod, crd.Status.Phase)
	})
}

'''
'''--- internal/volsnapshot/vol_snapshot_control.go ---
package volsnapshot

import (
	"context"
	"errors"
	"fmt"
	"sort"
	"time"

	"github.com/go-logr/logr"
	snapshotv1 "github.com/kubernetes-csi/external-snapshotter/client/v6/apis/volumesnapshot/v1"
	"github.com/samber/lo"
	cosmosalpha "github.com/strangelove-ventures/cosmos-operator/api/v1alpha1"
	"github.com/strangelove-ventures/cosmos-operator/internal/fullnode"
	"github.com/strangelove-ventures/cosmos-operator/internal/kube"
	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

const cosmosSourceLabel = "cosmos.strange.love/source"

// Client is a subset of client.Client.
type Client interface {
	Create(ctx context.Context, obj client.Object, opts ...client.CreateOption) error
	List(ctx context.Context, list client.ObjectList, opts ...client.ListOption) error
	Delete(ctx context.Context, obj client.Object, opts ...client.DeleteOption) error
}

type PodFilter interface {
	SyncedPods(ctx context.Context, controller client.ObjectKey) []*corev1.Pod
}

// VolumeSnapshotControl manages VolumeSnapshots
type VolumeSnapshotControl struct {
	client    Client
	podFilter PodFilter
	now       func() time.Time
}

func NewVolumeSnapshotControl(client Client, filter PodFilter) *VolumeSnapshotControl {
	return &VolumeSnapshotControl{
		client:    client,
		podFilter: filter,
		now:       time.Now,
	}
}

type Candidate = cosmosalpha.SnapshotCandidate

// FindCandidate finds a suitable candidate for creating a volume snapshot.
// Only selects a pod that is in-sync.
// Any errors returned can be treated as transient; worth a retry.
func (control VolumeSnapshotControl) FindCandidate(ctx context.Context, crd *cosmosalpha.ScheduledVolumeSnapshot) (Candidate, error) {
	cctx, cancel := context.WithTimeout(ctx, 10*time.Second)
	defer cancel()
	var (
		synced     = control.podFilter.SyncedPods(cctx, client.ObjectKey{Namespace: crd.Namespace, Name: crd.Spec.FullNodeRef.Name})
		availCount = int32(len(synced))
		minAvail   = crd.Spec.MinAvailable
	)
	if minAvail <= 0 {
		minAvail = 2
	}

	if availCount < minAvail {
		return Candidate{}, fmt.Errorf("%d or more pods must be in-sync to prevent downtime, found %d in-sync", minAvail, availCount)
	}

	var pod *corev1.Pod

	if crd.Spec.FullNodeRef.Ordinal != nil {
		podIndex := *crd.Spec.FullNodeRef.Ordinal
		podIndexStr := fmt.Sprintf("%d", podIndex)
		for _, p := range synced {
			if p.Annotations["app.kubernetes.io/ordinal"] == podIndexStr {
				pod = p
				break
			}
		}
		if pod == nil {
			return Candidate{}, fmt.Errorf("in-sync pod with index %d not found", podIndex)
		}
	} else {
		pod = synced[0]
	}

	return Candidate{
		PodLabels: pod.Labels,
		PodName:   pod.Name,
		PVCName:   fullnode.PVCName(pod),
	}, nil
}

// CreateSnapshot creates VolumeSnapshot from the Candidate.PVCName and updates crd.status to reflect the created VolumeSnapshot.
// CreateSnapshot does not set an owner reference to avoid garbage collection if the CRD is deleted.
// Any error returned is considered transient and can be retried.
func (control VolumeSnapshotControl) CreateSnapshot(ctx context.Context, crd *cosmosalpha.ScheduledVolumeSnapshot, candidate Candidate) error {
	snapshot := snapshotv1.VolumeSnapshot{
		Spec: snapshotv1.VolumeSnapshotSpec{
			Source: snapshotv1.VolumeSnapshotSource{
				PersistentVolumeClaimName: ptr(candidate.PVCName),
			},
			VolumeSnapshotClassName: ptr(crd.Spec.VolumeSnapshotClassName),
		},
	}
	snapshot.Namespace = crd.Namespace
	ts := control.now().UTC().Format("200601021504")
	name := kube.ToName(fmt.Sprintf("%s-%s", crd.Name, ts))
	snapshot.Name = name

	snapshot.Labels = lo.Assign(candidate.PodLabels)
	snapshot.Labels[kube.ComponentLabel] = cosmosalpha.ScheduledVolumeSnapshotController
	snapshot.Labels[kube.ControllerLabel] = "cosmos-operator"
	snapshot.Labels[cosmosSourceLabel] = crd.Name

	if err := control.client.Create(ctx, &snapshot); err != nil {
		return err
	}

	crd.Status.LastSnapshot = &cosmosalpha.VolumeSnapshotStatus{
		Name:      name,
		StartedAt: metav1.NewTime(control.now()),
	}

	return nil
}

// DeleteOldSnapshots deletes old VolumeSnapshots given crd's spec.limit.
// If limit not set, defaults to keeping the 3 most recent.
func (control VolumeSnapshotControl) DeleteOldSnapshots(ctx context.Context, log logr.Logger, crd *cosmosalpha.ScheduledVolumeSnapshot) error {
	limit := int(crd.Spec.Limit)
	if limit <= 0 {
		limit = 3
	}
	var snapshots snapshotv1.VolumeSnapshotList
	err := control.client.List(ctx,
		&snapshots,
		client.InNamespace(crd.Namespace),
		client.MatchingLabels(map[string]string{cosmosSourceLabel: crd.Name}),
	)
	if err != nil {
		return fmt.Errorf("list volume snapshots: %w", err)
	}

	filtered := lo.Filter(snapshots.Items, func(item snapshotv1.VolumeSnapshot, _ int) bool {
		return item.Status != nil && item.Status.CreationTime != nil
	})

	if len(filtered) <= limit {
		return nil
	}

	// Sort by time descending
	sort.Slice(filtered, func(i, j int) bool {
		lhs := filtered[i].Status.CreationTime
		rhs := filtered[j].Status.CreationTime
		return !lhs.Before(rhs)
	})

	toDelete := filtered[limit:]

	var merr error
	for _, vs := range toDelete {
		vs := vs
		log.Info("Deleting volume snapshot", "volumeSnapshotName", vs.Name, "limit", limit)
		if err := control.client.Delete(ctx, &vs); kube.IgnoreNotFound(err) != nil {
			merr = errors.Join(merr, fmt.Errorf("delete %s: %w", vs.Name, err))
		}
	}
	return merr
}

'''
'''--- internal/volsnapshot/vol_snapshot_control_test.go ---
package volsnapshot

import (
	"context"
	"errors"
	"fmt"
	"strconv"
	"testing"
	"time"

	"github.com/go-logr/logr"
	snapshotv1 "github.com/kubernetes-csi/external-snapshotter/client/v6/apis/volumesnapshot/v1"
	"github.com/samber/lo"
	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	cosmosalpha "github.com/strangelove-ventures/cosmos-operator/api/v1alpha1"
	"github.com/strangelove-ventures/cosmos-operator/internal/fullnode"
	"github.com/strangelove-ventures/cosmos-operator/internal/kube"
	"github.com/stretchr/testify/require"
	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

type mockPodClient struct {
	GotListOpts []client.ListOption
	Items       []corev1.Pod
	ListErr     error

	GotCreateObj client.Object
	CreateErr    error
}

func (m *mockPodClient) Create(ctx context.Context, obj client.Object, opts ...client.CreateOption) error {
	if ctx == nil {
		panic("nil context")
	}
	if len(opts) > 0 {
		panic(fmt.Errorf("expected 0 opts, got %d", len(opts)))
	}
	m.GotCreateObj = obj
	return m.CreateErr
}

func (m *mockPodClient) List(ctx context.Context, list client.ObjectList, opts ...client.ListOption) error {
	if ctx == nil {
		panic("nil context")
	}
	m.GotListOpts = opts
	list.(*corev1.PodList).Items = m.Items
	return m.ListErr
}

func (m *mockPodClient) Delete(ctx context.Context, obj client.Object, opts ...client.DeleteOption) error {
	panic("delete should not be called")
}

type mockPodFilter struct {
	SyncedPodsFn func(ctx context.Context, controller client.ObjectKey) []*corev1.Pod
}

func (fn mockPodFilter) SyncedPods(ctx context.Context, controller client.ObjectKey) []*corev1.Pod {
	if ctx == nil {
		panic("nil context")
	}
	if fn.SyncedPodsFn == nil {
		panic("SyncedPods not implemented")
	}
	return fn.SyncedPodsFn(ctx, controller)
}

var (
	panicFilter mockPodFilter
	nopLogger   = logr.Discard()
)

func TestVolumeSnapshotControl_FindCandidate(t *testing.T) {
	t.Parallel()

	var (
		ctx            = context.Background()
		readyCondition = corev1.PodCondition{Type: corev1.PodReady, Status: corev1.ConditionTrue}
	)

	const (
		fullNodeName = "cosmoshub"
		namespace    = "strangelove"
	)

	var crd cosmosalpha.ScheduledVolumeSnapshot
	crd.Name = "test"
	crd.Namespace = namespace
	crd.Spec.FullNodeRef.Name = fullNodeName

	t.Run("happy path", func(t *testing.T) {
		pods := make([]corev1.Pod, 3)
		for i := range pods {
			pods[i].Status.Conditions = []corev1.PodCondition{readyCondition}
		}
		var mClient mockPodClient
		mClient.Items = pods

		var fullnodeCRD cosmosv1.CosmosFullNode
		fullnodeCRD.Name = fullNodeName
		// Purposefully using PodBuilder to cross-test any breaking changes in PodBuilder which affects
		// finding the PVC name.
		candidate, err := fullnode.NewPodBuilder(&fullnodeCRD).WithOrdinal(1).Build()
		require.NoError(t, err)

		control := NewVolumeSnapshotControl(&mClient, mockPodFilter{
			SyncedPodsFn: func(ctx context.Context, controller client.ObjectKey) []*corev1.Pod {
				require.Equal(t, namespace, controller.Namespace)
				require.Equal(t, fullNodeName, controller.Name)
				return []*corev1.Pod{candidate, new(corev1.Pod), new(corev1.Pod)}
			},
		})

		got, err := control.FindCandidate(ctx, &crd)
		require.NoError(t, err)

		require.Equal(t, "cosmoshub-1", got.PodName)
		require.Equal(t, "pvc-cosmoshub-1", got.PVCName)
		require.NotEmpty(t, got.PodLabels)
		require.Equal(t, candidate.Labels, got.PodLabels)
	})

	t.Run("happy path with index", func(t *testing.T) {
		pods := make([]corev1.Pod, 3)
		for i := range pods {
			pods[i].Status.Conditions = []corev1.PodCondition{readyCondition}
		}
		var mClient mockPodClient
		mClient.Items = pods

		var fullnodeCRD cosmosv1.CosmosFullNode
		fullnodeCRD.Name = fullNodeName
		// Purposefully using PodBuilder to cross-test any breaking changes in PodBuilder which affects
		// finding the PVC name.
		candidate, err := fullnode.NewPodBuilder(&fullnodeCRD).WithOrdinal(1).Build()
		require.NoError(t, err)

		candidate.Annotations["app.kubernetes.io/ordinal"] = "1"

		control := NewVolumeSnapshotControl(&mClient, mockPodFilter{
			SyncedPodsFn: func(ctx context.Context, controller client.ObjectKey) []*corev1.Pod {
				require.Equal(t, namespace, controller.Namespace)
				require.Equal(t, fullNodeName, controller.Name)
				return []*corev1.Pod{candidate, new(corev1.Pod), new(corev1.Pod)}
			},
		})

		indexCRD := crd.DeepCopy()
		index := int32(1)
		indexCRD.Spec.FullNodeRef.Ordinal = &index

		got, err := control.FindCandidate(ctx, indexCRD)
		require.NoError(t, err)

		require.Equal(t, "cosmoshub-1", got.PodName)
		require.Equal(t, "pvc-cosmoshub-1", got.PVCName)
		require.NotEmpty(t, got.PodLabels)
		require.Equal(t, candidate.Labels, got.PodLabels)
	})

	t.Run("index not available", func(t *testing.T) {
		pods := make([]corev1.Pod, 3)
		for i := range pods {
			pods[i].Status.Conditions = []corev1.PodCondition{readyCondition}
		}
		var mClient mockPodClient
		mClient.Items = pods

		var fullnodeCRD cosmosv1.CosmosFullNode
		fullnodeCRD.Name = fullNodeName
		// Purposefully using PodBuilder to cross-test any breaking changes in PodBuilder which affects
		// finding the PVC name.
		candidate, err := fullnode.NewPodBuilder(&fullnodeCRD).WithOrdinal(1).Build()
		require.NoError(t, err)

		control := NewVolumeSnapshotControl(&mClient, mockPodFilter{
			SyncedPodsFn: func(ctx context.Context, controller client.ObjectKey) []*corev1.Pod {
				require.Equal(t, namespace, controller.Namespace)
				require.Equal(t, fullNodeName, controller.Name)
				return []*corev1.Pod{candidate, new(corev1.Pod), new(corev1.Pod)}
			},
		})

		indexCRD := crd.DeepCopy()
		index := int32(2)
		indexCRD.Spec.FullNodeRef.Ordinal = &index

		_, err = control.FindCandidate(ctx, indexCRD)
		require.ErrorContains(t, err, "in-sync pod with index 2 not found")
	})

	t.Run("custom min available", func(t *testing.T) {
		var pod corev1.Pod
		pod.Name = "found-me"
		pod.Status.Conditions = []corev1.PodCondition{readyCondition}
		var mClient mockPodClient
		mClient.Items = []corev1.Pod{pod}

		control := NewVolumeSnapshotControl(&mClient, mockPodFilter{
			SyncedPodsFn: func(context.Context, client.ObjectKey) []*corev1.Pod {
				return []*corev1.Pod{&pod}
			},
		})

		availCRD := crd.DeepCopy()
		availCRD.Spec.MinAvailable = 1

		got, err := control.FindCandidate(ctx, availCRD)

		require.NoError(t, err)
		require.Equal(t, "found-me", got.PodName)
	})

	t.Run("not enough ready pods", func(t *testing.T) {
		var readyPod corev1.Pod
		readyPod.Status.Conditions = []corev1.PodCondition{readyCondition}

		for i, tt := range []struct {
			Pods    []corev1.Pod
			WantErr string
		}{
			{nil, "2 or more pods must be in-sync to prevent downtime, found 0 in-sync"},
			{make([]corev1.Pod, 0), "2 or more pods must be in-sync to prevent downtime, found 0 in-sync"},
			{make([]corev1.Pod, 1), "2 or more pods must be in-sync to prevent downtime, found 1 in-sync"}, // no pods in-sync
		} {
			var mClient mockPodClient
			control := NewVolumeSnapshotControl(&mClient, mockPodFilter{
				SyncedPodsFn: func(context.Context, client.ObjectKey) []*corev1.Pod {
					return ptrSlice(tt.Pods)
				},
			})

			_, err := control.FindCandidate(ctx, &crd)

			require.Errorf(t, err, "test case %d", i)
			require.EqualErrorf(t, err, tt.WantErr, "test case %d", i)
		}
	})
}

func TestVolumeSnapshotControl_CreateSnapshot(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	t.Run("happy path", func(t *testing.T) {
		var mClient mockPodClient
		control := NewVolumeSnapshotControl(&mClient, panicFilter)
		// Use time.Local to ensure we format with UTC.
		now := time.Date(2022, time.September, 1, 2, 3, 0, 0, time.UTC)
		control.now = func() time.Time {
			return now
		}

		var crd cosmosalpha.ScheduledVolumeSnapshot
		crd.Name = "my-snapshot"
		crd.Namespace = "strangelove"
		crd.Spec.VolumeSnapshotClassName = "my-snap-class"

		labels := map[string]string{
			"test":               "labels",
			kube.ControllerLabel: "should not see me",
			kube.ComponentLabel:  "should not see me",
		}
		candidate := Candidate{
			PodLabels: labels,
			PodName:   "chain-1",
			PVCName:   "pvc-chain-1",
		}
		err := control.CreateSnapshot(ctx, &crd, candidate)

		require.NoError(t, err)
		require.NotNil(t, mClient.GotCreateObj)

		got := mClient.GotCreateObj.(*snapshotv1.VolumeSnapshot)
		require.Equal(t, "strangelove", got.Namespace)
		const wantName = "my-snapshot-202209010203"
		require.Equal(t, wantName, got.Name)

		require.Equal(t, "my-snap-class", *got.Spec.VolumeSnapshotClassName)
		require.Equal(t, "pvc-chain-1", *got.Spec.Source.PersistentVolumeClaimName)
		require.Nil(t, got.Spec.Source.VolumeSnapshotContentName)

		wantLabels := map[string]string{
			"test":                       "labels",
			kube.ControllerLabel:         "cosmos-operator",
			kube.ComponentLabel:          "ScheduledVolumeSnapshot",
			"cosmos.strange.love/source": "my-snapshot",
		}
		require.Equal(t, wantLabels, got.Labels)

		wantStatus := &cosmosalpha.VolumeSnapshotStatus{
			Name:      wantName,
			StartedAt: metav1.NewTime(now),
		}
		require.Equal(t, wantStatus, crd.Status.LastSnapshot)
	})

	t.Run("nil pod labels", func(t *testing.T) {
		var mClient mockPodClient
		control := NewVolumeSnapshotControl(&mClient, panicFilter)
		var crd cosmosalpha.ScheduledVolumeSnapshot
		crd.Name = "cosmoshub"

		err := control.CreateSnapshot(ctx, &crd, Candidate{})

		require.NoError(t, err)
		require.NotNil(t, mClient.GotCreateObj)

		got := mClient.GotCreateObj.(*snapshotv1.VolumeSnapshot)

		wantLabels := map[string]string{
			kube.ControllerLabel: "cosmos-operator",
			kube.ComponentLabel:  "ScheduledVolumeSnapshot",
			cosmosSourceLabel:    "cosmoshub",
		}
		require.Equal(t, wantLabels, got.Labels)
	})

	t.Run("create error", func(t *testing.T) {
		var mClient mockPodClient
		mClient.CreateErr = errors.New("boom")
		control := NewVolumeSnapshotControl(&mClient, panicFilter)
		var crd cosmosalpha.ScheduledVolumeSnapshot
		err := control.CreateSnapshot(ctx, &crd, Candidate{})

		require.Error(t, err)
		require.EqualError(t, err, "boom")
	})
}

type mockVolumeSnapshotClient struct {
	GotListOpts []client.ListOption
	Items       []snapshotv1.VolumeSnapshot
	ListErr     error

	DeletedObjs []*snapshotv1.VolumeSnapshot
	DeleteErr   error
}

func (m *mockVolumeSnapshotClient) Create(ctx context.Context, obj client.Object, opts ...client.CreateOption) error {
	panic("create should not be called")
}

func (m *mockVolumeSnapshotClient) List(ctx context.Context, list client.ObjectList, opts ...client.ListOption) error {
	if ctx == nil {
		panic("nil context")
	}
	m.GotListOpts = opts
	list.(*snapshotv1.VolumeSnapshotList).Items = m.Items
	return m.ListErr
}

func (m *mockVolumeSnapshotClient) Delete(ctx context.Context, obj client.Object, opts ...client.DeleteOption) error {
	if ctx == nil {
		panic("nil context")
	}
	m.DeletedObjs = append(m.DeletedObjs, obj.(*snapshotv1.VolumeSnapshot))
	return m.DeleteErr
}

func TestVolumeSnapshotControl_DeleteOldSnapshots(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	t.Run("happy path - custom limit", func(t *testing.T) {
		now := time.Now()
		const (
			limit      = 5
			additional = 3
		)

		var mClient mockVolumeSnapshotClient
		for i := 0; i < limit+additional; i++ {
			creation := metav1.NewTime(now.Add(time.Duration(i) * time.Second))
			mClient.Items = append(mClient.Items, snapshotv1.VolumeSnapshot{
				ObjectMeta: metav1.ObjectMeta{Name: strconv.Itoa(i)},
				Status: &snapshotv1.VolumeSnapshotStatus{
					CreationTime: &creation,
				},
			})
		}

		// Nil status should be ignored.
		mClient.Items = append(mClient.Items, snapshotv1.VolumeSnapshot{
			ObjectMeta: metav1.ObjectMeta{Name: "should be filtered out 1"},
			Status:     nil,
		})
		// Nil status.creationTime should be ignored.
		mClient.Items = append(mClient.Items, snapshotv1.VolumeSnapshot{
			ObjectMeta: metav1.ObjectMeta{Name: "should be filtered out 2"},
			Status:     &snapshotv1.VolumeSnapshotStatus{},
		})

		lo.Shuffle(mClient.Items)

		var crd cosmosalpha.ScheduledVolumeSnapshot
		crd.Name = "agoric"
		crd.Namespace = "default"
		crd.Spec.Limit = int32(limit)

		control := NewVolumeSnapshotControl(&mClient, panicFilter)
		err := control.DeleteOldSnapshots(ctx, nopLogger, &crd)

		require.NoError(t, err)

		require.Len(t, mClient.GotListOpts, 2)
		var listOpt client.ListOptions
		for _, opt := range mClient.GotListOpts {
			opt.ApplyToList(&listOpt)
		}
		require.Zero(t, listOpt.Limit)
		require.Equal(t, "default", listOpt.Namespace)
		require.Equal(t, "cosmos.strange.love/source=agoric", listOpt.LabelSelector.String())

		require.EqualValues(t, additional, len(mClient.DeletedObjs))

		got := lo.Map(mClient.DeletedObjs, func(item *snapshotv1.VolumeSnapshot, _ int) string {
			return item.Name
		})
		require.ElementsMatch(t, []string{"0", "1", "2"}, got)
	})

	t.Run("happy path - default limit", func(t *testing.T) {
		now := time.Now()
		const total = 5

		var mClient mockVolumeSnapshotClient
		for i := 0; i < total; i++ {
			creation := metav1.NewTime(now.Add(time.Duration(i) * time.Second))
			mClient.Items = append(mClient.Items, snapshotv1.VolumeSnapshot{
				ObjectMeta: metav1.ObjectMeta{Name: strconv.Itoa(i)},
				Status: &snapshotv1.VolumeSnapshotStatus{
					CreationTime: &creation,
				},
			})
		}

		lo.Shuffle(mClient.Items)

		var crd cosmosalpha.ScheduledVolumeSnapshot
		control := NewVolumeSnapshotControl(&mClient, panicFilter)
		err := control.DeleteOldSnapshots(ctx, nopLogger, &crd)

		require.NoError(t, err)

		require.EqualValues(t, 2, len(mClient.DeletedObjs))

		got := lo.Map(mClient.DeletedObjs, func(item *snapshotv1.VolumeSnapshot, _ int) string {
			return item.Name
		})
		require.ElementsMatch(t, []string{"0", "1"}, got)
	})

	t.Run("happy path - under limit", func(t *testing.T) {
		now := metav1.Now()
		const total = 5

		var mClient mockVolumeSnapshotClient
		for i := 0; i < total; i++ {
			mClient.Items = append(mClient.Items, snapshotv1.VolumeSnapshot{
				ObjectMeta: metav1.ObjectMeta{Name: strconv.Itoa(i)},
				Status: &snapshotv1.VolumeSnapshotStatus{
					CreationTime: &now,
				},
			})
		}

		var crd cosmosalpha.ScheduledVolumeSnapshot
		crd.Spec.Limit = total + 1
		control := NewVolumeSnapshotControl(&mClient, panicFilter)
		err := control.DeleteOldSnapshots(ctx, nopLogger, &crd)

		require.NoError(t, err)
		require.Empty(t, mClient.DeletedObjs)
	})

	t.Run("happy path - no items", func(t *testing.T) {
		var mClient mockVolumeSnapshotClient
		var crd cosmosalpha.ScheduledVolumeSnapshot
		control := NewVolumeSnapshotControl(&mClient, panicFilter)
		err := control.DeleteOldSnapshots(ctx, nopLogger, &crd)

		require.NoError(t, err)
		require.Empty(t, mClient.DeletedObjs)
	})

	t.Run("list error", func(t *testing.T) {
		var mClient mockVolumeSnapshotClient
		mClient.ListErr = errors.New("boom")
		var crd cosmosalpha.ScheduledVolumeSnapshot
		control := NewVolumeSnapshotControl(&mClient, panicFilter)
		err := control.DeleteOldSnapshots(ctx, nopLogger, &crd)

		require.Error(t, err)
		require.EqualError(t, err, "list volume snapshots: boom")
	})

	t.Run("delete errors", func(t *testing.T) {
		now := metav1.Now()
		const total = 3

		var mClient mockVolumeSnapshotClient
		mClient.DeleteErr = errors.New("oops")
		for i := 0; i < total; i++ {
			mClient.Items = append(mClient.Items, snapshotv1.VolumeSnapshot{
				ObjectMeta: metav1.ObjectMeta{Name: strconv.Itoa(i)},
				Status: &snapshotv1.VolumeSnapshotStatus{
					CreationTime: &now,
				},
			})
		}

		var crd cosmosalpha.ScheduledVolumeSnapshot
		crd.Spec.Limit = 1
		control := NewVolumeSnapshotControl(&mClient, panicFilter)
		err := control.DeleteOldSnapshots(ctx, nopLogger, &crd)

		require.Error(t, err)
		require.EqualError(t, err, "delete 1: oops\ndelete 0: oops")
	})
}

'''
'''--- main.go ---
/*
Copyright 2022 Strangelove Ventures LLC.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package main

import (
	"fmt"
	"net/http"
	"os"
	"time"

	"github.com/go-logr/zapr"
	"github.com/pkg/profile"
	"github.com/spf13/cobra"
	"github.com/spf13/viper"
	opcmd "github.com/strangelove-ventures/cosmos-operator/cmd"
	"github.com/strangelove-ventures/cosmos-operator/controllers"
	"github.com/strangelove-ventures/cosmos-operator/internal/cosmos"
	"github.com/strangelove-ventures/cosmos-operator/internal/fullnode"
	"github.com/strangelove-ventures/cosmos-operator/internal/version"
	"sigs.k8s.io/controller-runtime/pkg/healthz"

	// Import all Kubernetes client auth plugins (e.g. Azure, GCP, OIDC, etc.)
	// to ensure that exec-entrypoint and run can make use of them.
	_ "k8s.io/client-go/plugin/pkg/client/auth"

	// Add Pprof endpoints.
	_ "net/http/pprof"

	snapshotv1 "github.com/kubernetes-csi/external-snapshotter/client/v6/apis/volumesnapshot/v1"
	cosmosv1 "github.com/strangelove-ventures/cosmos-operator/api/v1"
	cosmosv1alpha1 "github.com/strangelove-ventures/cosmos-operator/api/v1alpha1"
	"k8s.io/apimachinery/pkg/runtime"
	utilruntime "k8s.io/apimachinery/pkg/util/runtime"
	clientgoscheme "k8s.io/client-go/kubernetes/scheme"
	ctrl "sigs.k8s.io/controller-runtime"
	//+kubebuilder:scaffold:imports
)

var (
	scheme   = runtime.NewScheme()
	setupLog = ctrl.Log.WithName("setup")
)

func init() {
	utilruntime.Must(clientgoscheme.AddToScheme(scheme))
	utilruntime.Must(snapshotv1.AddToScheme(scheme))

	utilruntime.Must(cosmosv1.AddToScheme(scheme))
	utilruntime.Must(cosmosv1alpha1.AddToScheme(scheme))
	//+kubebuilder:scaffold:scheme
}

func main() {
	root := rootCmd()

	ctx := ctrl.SetupSignalHandler()

	if err := root.ExecuteContext(ctx); err != nil {
		os.Exit(1)
	}
}

// root command flags
var (
	metricsAddr          string
	enableLeaderElection bool
	probeAddr            string
	profileMode          string
	logLevel             string
	logFormat            string
)

func rootCmd() *cobra.Command {
	root := &cobra.Command{
		Short:        "Run the operator",
		Use:          "manager",
		Version:      version.AppVersion(),
		RunE:         startManager,
		SilenceUsage: true,
	}

	root.Flags().StringVar(&metricsAddr, "metrics-bind-address", ":8080", "The address the metric endpoint binds to.")
	root.Flags().StringVar(&probeAddr, "health-probe-bind-address", ":8081", "The address the probe endpoint binds to.")
	root.Flags().BoolVar(&enableLeaderElection, "leader-elect", false,
		"Enable leader election for controller manager. "+
			"Enabling this will ensure there is only one active controller manager.")
	root.Flags().StringVar(&profileMode, "profile", "", "Enable profiling and save profile to working dir. (Must be one of 'cpu', or 'mem'.)")
	root.Flags().StringVar(&logLevel, "log-level", "info", "Logging level one of 'error', 'info', 'debug'")
	root.Flags().StringVar(&logFormat, "log-format", "console", "Logging format one of 'console' or 'json'")

	if err := viper.BindPFlags(root.Flags()); err != nil {
		panic(err)
	}

	// Add subcommands here
	root.AddCommand(opcmd.HealthCheckCmd())
	root.AddCommand(opcmd.VersionCheckCmd(scheme))
	root.AddCommand(&cobra.Command{
		Short: "Print the version",
		Use:   "version",
		Run: func(cmd *cobra.Command, args []string) {
			fmt.Println("App Version:", version.AppVersion())
			fmt.Println("Docker Tag:", version.DockerTag())
		},
	})

	return root
}

func startManager(cmd *cobra.Command, args []string) error {
	go func() {
		setupLog.Info("Serving pprof endpoints at localhost:6060/debug/pprof")
		if err := http.ListenAndServe("localhost:6060", nil); err != nil {
			setupLog.Error(err, "Pprof server exited with error")
		}
	}()

	logger := opcmd.ZapLogger(logLevel, logFormat)
	defer func() { _ = logger.Sync() }()
	ctrl.SetLogger(zapr.NewLogger(logger))

	if profileMode != "" {
		defer profile.Start(profileOpts(profileMode)...).Stop()
	}

	mgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{
		Scheme:                 scheme,
		MetricsBindAddress:     metricsAddr,
		Port:                   9443,
		HealthProbeBindAddress: probeAddr,
		LeaderElection:         enableLeaderElection,
		LeaderElectionID:       "16e1bc09.strange.love",
		// LeaderElectionReleaseOnCancel defines if the leader should step down voluntarily
		// when the Manager ends. This requires the binary to immediately end when the
		// Manager is stopped, otherwise, this setting is unsafe. Setting this significantly
		// speeds up voluntary leader transitions as the new leader don't have to wait
		// LeaseDuration time first.
		//
		// In the default scaffold provided, the program ends immediately after
		// the manager stops, so would be fine to enable this option. However,
		// if you are doing or is intended to do any operation such as perform cleanups
		// after the manager stops then its usage might be unsafe.
		// LeaderElectionReleaseOnCancel: true,
	})
	if err != nil {
		return fmt.Errorf("unable to start manager: %w", err)
	}

	ctx := cmd.Context()

	// CacheController which fetches CometBFT status in the background.
	httpClient := &http.Client{Timeout: 30 * time.Second}
	statusClient := fullnode.NewStatusClient(mgr.GetClient())
	cometClient := cosmos.NewCometClient(httpClient)
	cacheController := cosmos.NewCacheController(
		cosmos.NewStatusCollector(cometClient, 5*time.Second),
		mgr.GetClient(),
		mgr.GetEventRecorderFor(cosmos.CacheControllerName),
	)
	defer func() { _ = cacheController.Close() }()
	if err = cacheController.SetupWithManager(ctx, mgr); err != nil {
		return fmt.Errorf("unable to create CosmosCache controller: %w", err)
	}

	// The primary controller for CosmosFullNode.
	if err = controllers.NewFullNode(
		mgr.GetClient(),
		mgr.GetEventRecorderFor(cosmosv1.CosmosFullNodeController),
		statusClient,
		cacheController,
	).SetupWithManager(ctx, mgr); err != nil {
		return fmt.Errorf("unable to create CosmosFullNode controller: %w", err)
	}

	// An ancillary controller that supports CosmosFullNode.
	if err = controllers.NewSelfHealing(
		mgr.GetClient(),
		mgr.GetEventRecorderFor(cosmosv1.SelfHealingController),
		statusClient,
		httpClient,
		cacheController,
	).SetupWithManager(ctx, mgr); err != nil {
		return fmt.Errorf("unable to create SelfHealing controller: %w", err)
	}

	// Test for presence of VolumeSnapshot CRD.
	snapshotErr := controllers.IndexVolumeSnapshots(ctx, mgr)
	if snapshotErr != nil {
		setupLog.Info("Warning: VolumeSnapshot CRD not found, StatefulJob and ScheduledVolumeSnapshot controllers will be disabled")
	}

	// StatefulJobs
	jobCtl := controllers.NewStatefulJob(
		mgr.GetClient(),
		mgr.GetEventRecorderFor(cosmosv1alpha1.StatefulJobController),
		snapshotErr != nil,
	)

	if err = jobCtl.SetupWithManager(ctx, mgr); err != nil {
		return fmt.Errorf("unable to create StatefulJob controller: %w", err)
	}

	// ScheduledVolumeSnapshots
	if err = controllers.NewScheduledVolumeSnapshotReconciler(
		mgr.GetClient(),
		mgr.GetEventRecorderFor(cosmosv1alpha1.ScheduledVolumeSnapshotController),
		statusClient,
		cacheController,
		snapshotErr != nil,
	).SetupWithManager(ctx, mgr); err != nil {
		return fmt.Errorf("unable to create ScheduledVolumeSnapshot controller: %w", err)
	}

	//+kubebuilder:scaffold:builder

	if err := mgr.AddHealthzCheck("healthz", healthz.Ping); err != nil {
		return fmt.Errorf("unable to set up health check: %w", err)
	}
	if err := mgr.AddReadyzCheck("readyz", healthz.Ping); err != nil {
		return fmt.Errorf("unable to set up ready check: %w", err)
	}

	setupLog.Info("Starting Cosmos Operator manager", "version", version.AppVersion())
	if err := mgr.Start(ctx); err != nil {
		return fmt.Errorf("problem running manager: %w", err)
	}

	return nil
}

func profileOpts(mode string) []func(*profile.Profile) {
	opts := []func(*profile.Profile){profile.ProfilePath("."), profile.NoShutdownHook}
	switch mode {
	case "cpu":
		return append(opts, profile.CPUProfile)
	case "mem":
		return append(opts, profile.MemProfile)
	default:
		panic(fmt.Errorf("unknown profile mode %q", mode))
	}
}

'''
'''--- rocksdb/README.md ---
# RocksDB Static Build

This Dockerfile produces cross-architecture (amd64 and arm64) docker images with a static rocksdb library.

## Reason

This static rocksdb build takes a while, and it is not necessary to build every time the cosmos-operator docker image is built, so this image caches the required artifacts to link rocksdb into the operator build.

## Build and push to Github Container Registry

```
ROCKSDB_VERSION=v7.10.2
docker buildx build --platform linux/arm64,linux/amd64 --build-arg "ROCKSDB_VERSION=$ROCKSDB_VERSION" --push -t ghcr.io/strangelove-ventures/rocksdb:$ROCKSDB_VERSION .
```

After publishing a new version, import that version in the `Dockerfile` and `local.Dockerfile` in the root of the cosmos-operator repository
'''