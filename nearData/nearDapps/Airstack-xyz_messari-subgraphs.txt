*GitHub Repository "Airstack-xyz/messari-subgraphs"*

'''--- apps/curve-pool-depeg/app.py ---
import streamlit as st
import altair as alt
import pandas as pd
from subgrounds import Subgrounds
from itertools import cycle
from datetime import datetime, timedelta

sg = Subgrounds()
subgraphs_urls = {
        'gnosis': "https://api.thegraph.com/subgraphs/name/messari/curve-finance-gnosis",
        'optimism': "https://api.thegraph.com/subgraphs/name/messari/curve-finance-optimism",
        'fantom': "https://api.thegraph.com/subgraphs/name/messari/curve-finance-fantom",
        'ethereum': "https://api.thegraph.com/subgraphs/name/messari/curve-finance-ethereum",
}
network_list = ['ethereum', 'gnosis', 'optimism', 'fantom']

def load_subgraph(url):
    subgraph = sg.load_subgraph(url)
    subgraph._transforms = []
    return subgraph

def get_data(subgraph, network, startTime, numberPools):
    #Define Query Fieldpath
    liquidity_pools = subgraph.Query.liquidityPools(
        orderBy=subgraph.LiquidityPool.totalValueLockedUSD,
        orderDirection="desc",
        where={"totalValueLockedUSD_gt": 500000, "dailySnapshots_": {"timestamp_gt": int(startTime.timestamp())}},
        first=numberPools
    )

    lp_snapshots = liquidity_pools.dailySnapshots(
        orderBy=liquidity_pools.dailySnapshots.timestamp,
        where=[liquidity_pools.dailySnapshots.timestamp > int(startTime.timestamp())],
    )

    #Query
    df = sg.query_df([
        liquidity_pools.symbol,
        liquidity_pools.inputTokens.name,
        liquidity_pools.totalValueLockedUSD,
        lp_snapshots.timestamp,
        lp_snapshots.totalValueLockedUSD,
        lp_snapshots.inputTokenWeights,
        lp_snapshots.inputTokenBalances,
    ])

    dec = sg.query_df([
        liquidity_pools.inputTokens.name,
        liquidity_pools.inputTokens.decimals
    ])

    dec_dict = (pd.Series(dec.liquidityPools_inputTokens_decimals.values, index=dec.liquidityPools_inputTokens_name).to_dict())
    
    snapshot_cols = df[0].columns.tolist()
    snapshot_cols.remove("liquidityPools_inputTokens_name")
    df_0 = df[0].groupby(snapshot_cols, axis=0)['liquidityPools_inputTokens_name'].apply(list).reset_index()

    snapshot_cols = df[1].columns.tolist()
    snapshot_cols.remove("liquidityPools_dailySnapshots_inputTokenWeights")
    df_1 = df[1].groupby(snapshot_cols, axis=0)['liquidityPools_dailySnapshots_inputTokenWeights'].apply(list).reset_index()

    snapshot_cols = df[2].columns.tolist()
    snapshot_cols.remove("liquidityPools_dailySnapshots_inputTokenBalances")
    df_2 = df[2].groupby(snapshot_cols, axis=0)['liquidityPools_dailySnapshots_inputTokenBalances'].apply(list).reset_index()

    df = pd.merge(df_0, pd.merge(df_1, df_2))
    df = df.rename(columns={'liquidityPools_inputTokens_name': 'input_tokens_name', 
                            'liquidityPools_dailySnapshots_inputTokenWeights': 'daily_input_tokens_weight',
                            'liquidityPools_dailySnapshots_inputTokenBalances': 'daily_input_tokens_balance', 
                            'liquidityPools_dailySnapshots_totalValueLockedUSD': 'daily_tvl', 
                            'liquidityPools_totalValueLockedUSD': 'TVL'})
    df['daily_input_tokens_price'] = df.apply(lambda x: calc_input_token_price(x, dec_dict), axis=1)
    df['%depeg'] = df.apply(lambda x: calc_depeg(x), axis=1)
    df["network"] = network
    df["date"] = pd.to_datetime(df['liquidityPools_dailySnapshots_timestamp'], unit="s")

    return df

def calc_depeg(df):

    return [round((1 - float(df['daily_input_tokens_price'][i])), 5) for i in range(len(df['input_tokens_name']))]

def calc_input_token_price(df, dict):
    input_token_prices = []
    # This isn't the best solution as its looping in python but given how there is a small number of tokens in a pool shouldn't be to big a hit to performance
    for i in range(len(df['input_tokens_name'])):
        if(float(df['daily_input_tokens_balance'][i]) != 0):
            input_token_price = (float(df['daily_tvl']) * float(df['daily_input_tokens_weight'][i])
                                / (int(df['daily_input_tokens_balance'][i]) / 10**dict[df['input_tokens_name'][i]]))
        else:
            input_token_price = 0
        input_token_prices.append(input_token_price)

    return input_token_prices

def plot_pools(sort, data):
    grouped = data.groupby('liquidityPools_symbol')
    pool_snapshots = [group for _, group in grouped]
    sorted_pool_snapshots = sorted(pool_snapshots, key=lambda x:x[sort].max(axis=0), reverse=True)
    st.header(data['network'].iloc[0])
    cols = st.columns(2)
    for col, pool in zip(cycle(cols), sorted_pool_snapshots):
        with col: 
            frame = pool.explode(['input_tokens_name', '%depeg', 'daily_input_tokens_price', 'daily_input_tokens_weight'])
            tvl_chart = (alt.Chart(frame).mark_bar().encode(
                    x='date:T', 
                    y=alt.Y('sum(daily_input_tokens_weight):Q',
                            title="Input Token Weight"),
                    color='input_tokens_name'
            ).configure_legend(
                    strokeColor='gray',
                    fillColor='#EEEEEE',
                    padding=2,
                    cornerRadius=5,
                    orient='bottom'
            ))

            chart1 = alt.Chart(frame).mark_line().encode(
                x='date:T',
                y=alt.Y('daily_input_tokens_price:Q',  
                        title='Input Token Price (USD)'),
                color='input_tokens_name',
            ).properties(
                height=70,
                width=250
            )

            chart2 = alt.Chart(frame).mark_line().encode(
                x='date:T',
                y='%depeg:Q',
                color='input_tokens_name',
            ).properties(
                height=70,
                width=250
            )

            fig = alt.vconcat(chart1, chart2).configure_legend(
                    strokeColor='gray',
                    fillColor='#EEEEEE',
                    padding=2,
                    cornerRadius=5,
                    orient='bottom'
            )

            st.subheader(pool['liquidityPools_symbol'].iloc[0])
            st.altair_chart(tvl_chart, use_container_width=True)
            st.altair_chart(fig, use_container_width=True)

st.title("Curve Pool Composition Dashboard")

#Sidebar Form 
st.sidebar.header("Select Curve Pools")
form = st.sidebar.form('select')
sort = form.selectbox("Sort By", ("TVL", "%depeg"))
network = form.radio('Network:', network_list)
days = form.slider("Timeframe: (Past x days)", min_value=1, max_value=30, step=1)
full_history = form.checkbox("Plot Complete Pool History")
number_pools = form.slider('Number Pools:', min_value=1, max_value=200, step=1)
submitted = form.form_submit_button("Submit")

if submitted:
    start_time = datetime.today() - timedelta(days=days)
    if full_history:
        #Set to Unix Epoch to source entire history
        start_time = datetime(1970, 1, 1)
    pools = get_data(load_subgraph(subgraphs_urls[network]), network, start_time, number_pools)
    plot_pools(sort, pools)

'''
'''--- apps/curve-pool-depeg/requirements.txt ---
subgrounds
pandas
streamlit
altair

'''
'''--- apps/dex-dashboard/CustomCharts/Bar.py ---
import copy 
import config
from utils import *
from pyecharts import options as opts
from pyecharts.charts import Bar, Line

class CustomBarChart:
    def __init__(
        self,
        chart_title,
        yaxis_name,
        xaxis_name,
        height="1000px",
        xaxis_namegap=20,
        yaxis_namegap=40,
        logo_position=70
    ):
        self.LINE_CHART = Line()
        self.BAR_CHART = Bar(
            init_opts=opts.InitOpts(
                height=height, 
                width="100%", 
                bg_color="#232329"
            )
        )
        
        self.DEFAULT_TITLE_OPTS = copy.deepcopy(config.DEFAULT_TITLE_OPTS)
        self.DEFAULT_LEGEND_OPTS = copy.deepcopy(config.DEFAULT_LEGEND_OPTS)
        self.DEFAULT_TOOLTIP_OPTS = copy.deepcopy(config.DEFAULT_TOOLTIP_OPTS)
        self.DEFAULT_TOOLBOX_OPTS = copy.deepcopy(config.DEFAULT_TOOLBOX_OPTS)
        self.DEFAULT_XAXIS_OPTS = copy.deepcopy(config.DEFAULT_XAXIS_OPTS)
        self.DEFAULT_YAXIS_OPTS = copy.deepcopy(config.DEFAULT_YAXIS_OPTS)
        self.DEFAULT_DATAZOOM_OPTS = copy.deepcopy(config.DEFAULT_DATAZOOM_OPTS)
        
        
        self.DEFAULT_TITLE_OPTS.opts[0]['text'] = chart_title
        self.DEFAULT_LEGEND_OPTS.update(
            show=False
        )
        self.DEFAULT_XAXIS_OPTS.update(
            name=xaxis_name,
            nameGap=xaxis_namegap
        )
        self.DEFAULT_YAXIS_OPTS.update(
            name=yaxis_name,
            nameGap=yaxis_namegap
        )
        
        self.BAR_CHART.set_global_opts(
            title_opts=self.DEFAULT_TITLE_OPTS,
            legend_opts=self.DEFAULT_LEGEND_OPTS,
            tooltip_opts=self.DEFAULT_TOOLTIP_OPTS,
            toolbox_opts=self.DEFAULT_TOOLBOX_OPTS,
            xaxis_opts=self.DEFAULT_XAXIS_OPTS,
            yaxis_opts=self.DEFAULT_YAXIS_OPTS,
            datazoom_opts=self.DEFAULT_DATAZOOM_OPTS
        )

    def add_xaxis_line_chart(self, xaxis_data):
        self.LINE_CHART.add_xaxis(xaxis_data)
    
    def add_xaxis_bar_chart(self, xaxis_data):
        self.BAR_CHART.add_xaxis(xaxis_data)
    
    def add_yaxis_bar_chart(self, series_name, color, yaxis_data):
        self.BAR_CHART.add_yaxis(
            series_name=series_name,
            y_axis=yaxis_data,
            itemstyle_opts=opts.ItemStyleOpts(color=color),
            label_opts=opts.LabelOpts(is_show=False)
        )
    
    def add_yaxis_line_chart(self, series_name, color, yaxis_data):
        self.LINE_CHART.add_yaxis(
            series_name=series_name,
            y_axis=yaxis_data,
            yaxis_index=1,
            itemstyle_opts=opts.ItemStyleOpts(color=color),
            label_opts=opts.LabelOpts(is_show=False)
        )
    
    def extend_axis(self, name):
        self.BAR_CHART.extend_axis(
            yaxis=opts.AxisOpts(
                name=name,
                type_="value",
                name_location="middle",
                name_gap=40,
                name_rotate=-90,
                name_textstyle_opts=opts.TextStyleOpts(
                    font_size=15,
                ),
                axislabel_opts=opts.LabelOpts(
                    formatter=yaxis_label_formatter()
                )
            )
        )

'''
'''--- apps/dex-dashboard/CustomCharts/Line.py ---
import copy 
import config
from utils import *
from pyecharts.charts import Line
from pyecharts import options as opts

class CustomLineChart:
    def __init__(
        self,
        chart_title,
        xaxis_name,
        yaxis_name,
        height="1000px",
        xaxis_namegap=30,
        yaxis_namegap=40,
        logo_position=70
    ):
        self.LINE_CHART = Line(
            init_opts=opts.InitOpts(
                width="100%", 
                height=height, 
                bg_color="#232329"
            )
        )
        
        self.DEFAULT_TITLE_OPTS = copy.deepcopy(config.DEFAULT_TITLE_OPTS)
        self.DEFAULT_LEGEND_OPTS = copy.deepcopy(config.DEFAULT_LEGEND_OPTS)
        self.DEFAULT_TOOLTIP_OPTS = copy.deepcopy(config.DEFAULT_TOOLTIP_OPTS)
        self.DEFAULT_TOOLBOX_OPTS = copy.deepcopy(config.DEFAULT_TOOLBOX_OPTS)
        self.DEFAULT_XAXIS_OPTS = copy.deepcopy(config.DEFAULT_XAXIS_OPTS)
        self.DEFAULT_YAXIS_OPTS = copy.deepcopy(config.DEFAULT_YAXIS_OPTS)
        self.DEFAULT_DATAZOOM_OPTS = copy.deepcopy(config.DEFAULT_DATAZOOM_OPTS)
        
        self.DEFAULT_TITLE_OPTS.opts[0]['text'] = chart_title
        self.DEFAULT_LEGEND_OPTS.update(
            show=False
        )
        self.DEFAULT_XAXIS_OPTS.update(
            name=xaxis_name,
            nameGap=xaxis_namegap
        )
        self.DEFAULT_YAXIS_OPTS.update(
            name=yaxis_name,
            nameGap=yaxis_namegap
        )
        
        self.LINE_CHART.set_global_opts(
            title_opts=self.DEFAULT_TITLE_OPTS,
            legend_opts=self.DEFAULT_LEGEND_OPTS,
            tooltip_opts=self.DEFAULT_TOOLTIP_OPTS,
            toolbox_opts=self.DEFAULT_TOOLBOX_OPTS,
            xaxis_opts=self.DEFAULT_XAXIS_OPTS,
            yaxis_opts=self.DEFAULT_YAXIS_OPTS,
            datazoom_opts=self.DEFAULT_DATAZOOM_OPTS,
        )
    
    def add_xaxis(self, xaxis_data):
        self.LINE_CHART.add_xaxis(xaxis_data)
    
    def add_yaxis(self, series_name, color, yaxis_data):
        self.LINE_CHART.add_yaxis(
            y_axis=yaxis_data,
            series_name=series_name,
            label_opts=opts.LabelOpts(is_show=False),
            itemstyle_opts=opts.ItemStyleOpts(color=color)
        )

'''
'''--- apps/dex-dashboard/CustomCharts/Pie.py ---
import copy
import config 
from utils import *
from pyecharts.charts import Pie
from pyecharts import options as opts

class CustomPieChart:
    def __init__(
        self,
        chart_title,
        yaxis_name='',
        xaxis_name='',
        height="1000px",
        xaxis_namegap=50,
        yaxis_namegap=50
    ):
        self.PIE_CHART = Pie(
            init_opts=opts.InitOpts(
                height=height, 
                width="100%", 
                bg_color="#232329"
            )
        )
        
        self.DEFAULT_TITLE_OPTS = copy.deepcopy(config.DEFAULT_TITLE_OPTS)
        self.DEFAULT_LEGEND_OPTS = copy.deepcopy(config.DEFAULT_LEGEND_OPTS)
        self.DEFAULT_TOOLTIP_OPTS = copy.deepcopy(config.DEFAULT_TOOLTIP_OPTS)
        self.DEFAULT_TOOLBOX_OPTS = copy.deepcopy(config.DEFAULT_TOOLBOX_OPTS)
        self.DEFAULT_XAXIS_OPTS = copy.deepcopy(config.DEFAULT_XAXIS_OPTS)
        self.DEFAULT_YAXIS_OPTS = copy.deepcopy(config.DEFAULT_YAXIS_OPTS)
        self.DEFAULT_DATAZOOM_OPTS = copy.deepcopy(config.DEFAULT_DATAZOOM_OPTS)
        
        self.DEFAULT_LEGEND_OPTS.update(
            show=True
        )
        self.DEFAULT_TOOLTIP_OPTS.update(
            show=True,
            trigger="item",
            formatter="{b}: {d}%"
        )
        
        self.DEFAULT_TITLE_OPTS.opts[0]['text'] = chart_title
        self.DEFAULT_XAXIS_OPTS.opts.update(
            name=xaxis_name,
            nameGap=xaxis_namegap
        )
        self.DEFAULT_YAXIS_OPTS.update(
            name=yaxis_name,
            nameGap=yaxis_namegap
        )

        self.PIE_CHART.set_global_opts(
            title_opts=self.DEFAULT_TITLE_OPTS,
            legend_opts=self.DEFAULT_LEGEND_OPTS,
            tooltip_opts=self.DEFAULT_TOOLTIP_OPTS,
            toolbox_opts=self.DEFAULT_TOOLBOX_OPTS,
            xaxis_opts=self.DEFAULT_XAXIS_OPTS,
            yaxis_opts=self.DEFAULT_YAXIS_OPTS,
            datazoom_opts=self.DEFAULT_DATAZOOM_OPTS
        )

    def add(
        self, series_name, data):
        self.PIE_CHART.add(
            data_pair=data,
            center=['30%', '50%'],
            radius=["40%", "65%"],
            series_name=series_name, 
            label_opts=opts.LabelOpts(is_show=False),
        )

'''
'''--- apps/dex-dashboard/CustomCharts/__init__.py ---
from .Pie import CustomPieChart
from .Bar import CustomBarChart
from .Line import CustomLineChart

'''
'''--- apps/dex-dashboard/app.py ---
from config import *
from st_aggrid import AgGrid
from subgrounds.subgrounds import Subgrounds
from tables import DepositTransactions, SwapTransactions, WithdawTransactions
from metrics import FinancialsDailySnapshots, LiquidityPools, MetricsDailySnapshots

import streamlit as st
from streamlit_echarts import st_pyecharts
st.set_page_config(layout="wide")

st.title("DEX Subgraphs Dashboard")

subgraph_name = st.selectbox(
    label='',
    options=[
        'Sushiswap (Ethereum)', 'Sushiswap (Avax)', 'Uniswap v3 (Ethereum)', 
        'Balancer v2 (Ethereum)', 'Curve (Ethereum)', 'Saddle Finance (Ethereum)'
    ]
)

SUBGROUND = Subgrounds()
SUBGRAPH = SUBGROUND.load_subgraph(SUBGRAPH_API_URL[subgraph_name])

FinancialsSnapshot = FinancialsDailySnapshots(SUBGRAPH, SUBGROUND, initial_timestamp=1601322741)

col1, col2 = st.columns(2)

with col1:
    st_pyecharts(
        chart=FinancialsSnapshot.tvl_chart(),
        height="450px",
        key="TVLChart",
    )

with col2:
    st_pyecharts(
        chart=FinancialsSnapshot.volume_chart(),
        height="450px",
        key="VolumeChart",
    )

col1, col2 = st.columns(2)

with col1:
    st_pyecharts(
        chart=FinancialsSnapshot.revenue_chart(),
        height="450px",
        key="RevenueChart",
    )

with col2:
    st_pyecharts(
        chart=FinancialsSnapshot.cumulative_revenue_chart(),
        height="450px",
        key="CumulativeRevenueChart",
    )

MetricsSnapshot = MetricsDailySnapshots(SUBGRAPH, SUBGROUND, initial_timestamp=1601322741)

with st.container():
    st_pyecharts(
        chart=MetricsSnapshot.transactions_count_chart(),
        height="450px",
        key="TransactionChart",
    )

with st.container():
    st_pyecharts(
        chart=MetricsSnapshot.active_users_chart(),
        height="450px",
        key="ActiveUsersChart",
    )

liquidity_pool = LiquidityPools(SUBGRAPH, SUBGROUND, initial_timestamp=1601322741)

col1, col2 = st.columns(2)

with col1:
    st_pyecharts(
        chart=liquidity_pool.top_10_pools_by_tvl(),
        height="450px",
        key="Top10ByTVL",
    )

with col2:
    st_pyecharts(
        chart=liquidity_pool.top_10_pools_by_volume(),
        height="450px",
        key="Top10ByVolume",
    )

swap = SwapTransactions(SUBGRAPH, SUBGROUND)

if not swap.dataframe.empty:
    st.header("Swap Transactions")
    
    with st.container():
        AgGrid(
            swap.dataframe, 
            editable=True,
            data_return_mode="filtered_and_sorted",
            update_mode="no_update",
            fit_columns_on_grid_load=True, 
            theme="streamlit"
        )

deposits = DepositTransactions(SUBGRAPH, SUBGROUND)

if not deposits.dataframe.empty:
    st.header("Deposit Transactions")
    
    with st.container():
        AgGrid(
            deposits.dataframe, 
            editable=True,
            data_return_mode="filtered_and_sorted",
            update_mode="no_update",
            fit_columns_on_grid_load=True, 
            theme="streamlit"
        )

withdraws = WithdawTransactions(SUBGRAPH, SUBGROUND)

if not withdraws.dataframe.empty:
    st.header("Withdraw Transactions")
    
    with st.container():
        AgGrid(
            withdraws.dataframe, 
            editable=True,
            data_return_mode="filtered_and_sorted",
            update_mode="no_update",
            fit_columns_on_grid_load=True, 
            theme="streamlit"
        )
'''
'''--- apps/dex-dashboard/config.py ---
from utils import *
from pyecharts import options as opts

SUBGRAPH_API_URL = {
    'Balancer v2 (Ethereum)': "https://api.thegraph.com/subgraphs/name/messari/balancer-v2-ethereum", 
    'Curve (Ethereum)': "https://api.thegraph.com/subgraphs/name/messari/curve-finance-ethereum", 
    'Saddle Finance (Ethereum)': "https://api.thegraph.com/subgraphs/name/messari/saddle-finance-ethereum",
    'Sushiswap (Ethereum)': "https://api.thegraph.com/subgraphs/name/messari/sushiswap-ethereum", 
    'Sushiswap (Avax)': "https://api.thegraph.com/subgraphs/name/messari/sushiswap-avalanche",
    'Uniswap v3 (Ethereum)': "https://api.thegraph.com/subgraphs/name/messari/uniswap-v3-ethereum"
}

DEFAULT_TITLE_OPTS = opts.TitleOpts(
    padding=10,
    item_gap=0,
    pos_left="10",
    pos_right="0",
    pos_top="10",
    pos_bottom="0",
    title_textstyle_opts=opts.TextStyleOpts(
        color="white",
    )
)

DEFAULT_LEGEND_OPTS = opts.LegendOpts(
    is_show=True,
    type_="scroll", 
    pos_left="55%", 
    pos_top = "20%",
    orient="vertical",
    textstyle_opts=opts.TextStyleOpts(
        color='#FFFFFF'
    )
)

DEFAULT_TOOLTIP_OPTS = opts.TooltipOpts(
    is_show=True,
    padding=12,
    trigger="axis",
    axis_pointer_type="line",
    background_color="#3C2E48",
    textstyle_opts=opts.TextStyleOpts(color="#FFFFFF", font_size=14),
)

DEFAULT_TOOLBOX_OPTS = opts.ToolboxOpts(
    is_show=True,
    pos_left="82%",
    feature=opts.ToolBoxFeatureOpts(
        save_as_image=opts.ToolBoxFeatureSaveAsImageOpts(
            is_show=True, title="Save"
        ),
        restore=opts.ToolBoxFeatureRestoreOpts(
            is_show=True, title="Refresh"
        ),
        data_view=opts.ToolBoxFeatureDataViewOpts(
            is_show=False
        ),
        data_zoom=opts.ToolBoxFeatureDataZoomOpts(
            is_show=False
        ),
        magic_type=opts.ToolBoxFeatureMagicTypeOpts(
            is_show=True,
            line_title="Line",
            bar_title="Bar",
            stack_title="Stack",
            tiled_title="Tiled",
        ),
        brush=opts.ToolBoxFeatureBrushOpts(type_=False),
    )
)

DEFAULT_XAXIS_OPTS = opts.AxisOpts(
    type_="category",
    is_show=True,
    name_location="start",
    min_interval=5,
    axislabel_opts=opts.LabelOpts(
        is_show=True,
        formatter=xaxis_label_formatter()
    )
)

DEFAULT_YAXIS_OPTS = opts.AxisOpts(
    is_show=True,
    type_="value",
    name_location="middle",
    offset=5,
    split_number=5,
    name_textstyle_opts=opts.TextStyleOpts(
        font_size=15,
    ),
    axistick_opts=opts.AxisTickOpts(is_show=True),
    axisline_opts=opts.AxisLineOpts(
        is_show=True,
        is_on_zero=False,
        on_zero_axis_index=0,
        symbol=None,
        linestyle_opts=opts.LineStyleOpts(
            is_show=True,
            width=1,
            opacity=1,
            curve=0,
            type_="dash",
            color=None,
        ),
    ),
    axislabel_opts=opts.LabelOpts(
        formatter=yaxis_label_formatter()
    ),
    axispointer_opts=opts.AxisPointerOpts(),
    splitline_opts=opts.SplitLineOpts(
        is_show=True,
        linestyle_opts=opts.LineStyleOpts(
            type_="dashed", opacity=0.2, color="#FFFFFF"
        ),
    ),
    splitarea_opts=opts.SplitAreaOpts(),
    minor_tick_opts=opts.MinorTickOpts(),
    minor_split_line_opts=opts.MinorSplitLineOpts()
)

DEFAULT_DATAZOOM_OPTS = [
    opts.DataZoomOpts(
        range_start=0, 
        range_end=100
    ),
    opts.DataZoomOpts(
        type_="inside"
    ),
]

DEFAULT_GRAPHIC_OPTS = opts.GraphicImage(
    graphic_item=opts.GraphicItem(
        id_="Messari_Logo", 
        z=-10, 
        top=70, 
        right=70, 
        bounding="raw", 
        origin=[75, 75]
    ),
    graphic_imagestyle_opts=opts.GraphicImageStyleOpts(
        image="https://messari.io/images/Messari_horizontal_white-03.svg",
        width=165,
        height=30,
        opacity=0.2,
    )
)

'''
'''--- apps/dex-dashboard/metrics.py ---
from utils import *
from config import *

from CustomCharts import CustomLineChart, CustomBarChart, CustomPieChart

class FinancialsDailySnapshots:
    def __init__(self, subgraph, subground, initial_timestamp):
        self.subgraph = subgraph
        self.subground = subground
        self.timestamp = initial_timestamp

        self.dataframe = self.query()

    def query(self):
        financial_daily_snapshot = self.subgraph.Query.financialsDailySnapshots(
            first=1000,
            where=[self.subgraph.FinancialsDailySnapshot.timestamp > self.timestamp],
        )

        dataframe = self.subground.query_df([financial_daily_snapshot])

        return dataframe

    def tvl_chart(self):
        chart = CustomLineChart(
            chart_title="Total Value Locked (USD)", xaxis_name="UTC", yaxis_name="Daily TVL"
        )

        # x_axis --> timestamp
        chart.add_xaxis(format_xaxis(self.dataframe.financialsDailySnapshots_id))

        # y_axis -->
        chart.add_yaxis(
            color="#12b8ff",
            series_name="TotalValueLocked",
            yaxis_data=self.dataframe.financialsDailySnapshots_totalValueLockedUSD.round(
                1
            ).to_list(),
        )

        return chart.LINE_CHART

    def volume_chart(self):
        chart = CustomLineChart(
            chart_title="Volume (USD)", xaxis_name="UTC", yaxis_name="Daily Volume"
        )

        xaxis_data = format_xaxis(self.dataframe.financialsDailySnapshots_id)
        # x_axis --> timestamp
        chart.add_xaxis(xaxis_data)

        # y_axis -->
        chart.add_yaxis(
            color="#12b8ff",
            series_name="New",
            yaxis_data=self.dataframe.financialsDailySnapshots_dailyVolumeUSD.round(
                1
            ).to_list(),
        )

        return chart.LINE_CHART

    def revenue_chart(self):
        chart = CustomBarChart(
            chart_title="Revenue (USD)",
            xaxis_name="UTC",
            yaxis_name="Revenue",
        )

        xaxis_data = format_xaxis(self.dataframe.financialsDailySnapshots_id)

        chart.add_xaxis_bar_chart(xaxis_data=xaxis_data)
        chart.add_xaxis_line_chart(xaxis_data=xaxis_data)

        chart.add_yaxis_bar_chart(
            series_name="Daily Supply Side Revenue (USD)",
            color="#5a66f9",
            yaxis_data=self.dataframe.financialsDailySnapshots_dailySupplySideRevenueUSD.round(
                1
            ).to_list(),
        )
        chart.add_yaxis_bar_chart(
            series_name="Daily Protocol Side Revenue (USD)",
            color="#6ac5c8",
            yaxis_data=self.dataframe.financialsDailySnapshots_dailyProtocolSideRevenueUSD.round(
                1
            ).to_list(),
        )

        chart.extend_axis(name="Total Revenue")

        chart.add_yaxis_line_chart(
            series_name="Daily Total Revenue (USD)",
            color="#fc03f8",
            yaxis_data=self.dataframe.financialsDailySnapshots_dailyTotalRevenueUSD.round(
                1
            ).to_list(),
        )

        return chart.BAR_CHART.overlap(chart.LINE_CHART)

    def cumulative_revenue_chart(self):
        chart = CustomLineChart(
            chart_title="Cumulative Revenue (USD)",
            xaxis_name="UTC",
            yaxis_name="Daily Cumulative Revenue",
            yaxis_namegap=45,
        )

        xaxis_data = format_xaxis(self.dataframe.financialsDailySnapshots_id)

        chart.add_xaxis(xaxis_data)

        chart.add_yaxis(
            color="#5a66f9",
            series_name="Supply Side Revenue (USD)",
            yaxis_data=self.dataframe.financialsDailySnapshots_cumulativeSupplySideRevenueUSD.round(
                1
            ).to_list(),
        )

        chart.add_yaxis(
            color="#fc03f8",
            series_name="Protocol Side Revenue (USD)",
            yaxis_data=self.dataframe.financialsDailySnapshots_cumulativeProtocolSideRevenueUSD.round(
                1
            ).to_list(),
        )

        chart.add_yaxis(
            color="#12b8ff",
            series_name="Total Side Revenue (USD)",
            yaxis_data=self.dataframe.financialsDailySnapshots_cumulativeTotalRevenueUSD.round(
                1
            ).to_list(),
        )

        return chart.LINE_CHART

class MetricsDailySnapshots:
    def __init__(self, subgraph, subground, initial_timestamp):
        self.subgraph = subgraph
        self.subground = subground
        self.timestamp = initial_timestamp

        self.dataframe = self.query()

    def query(self):
        metrics_daily_snapshot = self.subgraph.Query.usageMetricsDailySnapshots(
            first=1000,
            where=[self.subgraph.UsageMetricsDailySnapshot.timestamp > self.timestamp],
        )

        dataframe = self.subground.query_df([metrics_daily_snapshot])

        return dataframe

    def transactions_count_chart(self):
        chart = CustomBarChart(
            chart_title="Transactions",
            xaxis_name="UTC",
            yaxis_name="Count Of Transactions",
            logo_position=130
        )

        xaxis_data = format_xaxis(self.dataframe.usageMetricsDailySnapshots_id)

        chart.add_xaxis_bar_chart(xaxis_data=xaxis_data)
        chart.add_xaxis_line_chart(xaxis_data=xaxis_data)

        chart.add_yaxis_bar_chart(
            series_name="Daily Deposit Count",
            color="#5a66f9",
            yaxis_data=self.dataframe.usageMetricsDailySnapshots_dailyDepositCount.round(
                1
            ).to_list(),
        )
        chart.add_yaxis_bar_chart(
            series_name="Daily Withdraw Count",
            color="#6ac5c8",
            yaxis_data=self.dataframe.usageMetricsDailySnapshots_dailyWithdrawCount.round(
                1
            ).to_list(),
        )
        chart.add_yaxis_bar_chart(
            series_name="Daily Swap Count",
            color="#F2AA4CFF",
            yaxis_data=self.dataframe.usageMetricsDailySnapshots_dailySwapCount.round(
                1
            ).to_list(),
        )

        chart.extend_axis(name="Total Daily Transactions")

        chart.add_yaxis_line_chart(
            series_name="Daily Total Transactions",
            color="#fc03f8",
            yaxis_data=self.dataframe.usageMetricsDailySnapshots_dailyTransactionCount.round(
                1
            ).to_list(),
        )

        return chart.BAR_CHART.overlap(chart.LINE_CHART)

    def active_users_chart(self):
        chart = CustomLineChart(
            chart_title="Active Users",
            xaxis_name="UTC",
            yaxis_name="Count Of Users",
            logo_position=135
        )

        # x_axis --> timestamp
        chart.add_xaxis(format_xaxis(self.dataframe.usageMetricsDailySnapshots_id))

        # y_axis -->
        chart.add_yaxis(
            color="#12b8ff",
            series_name="Daily Active Users",
            yaxis_data=self.dataframe.usageMetricsDailySnapshots_dailyActiveUsers.round(
                1
            ).to_list(),
        )

        return chart.LINE_CHART

class LiquidityPools:
    def __init__(self, subgraph, subground, initial_timestamp=None):
        self.subgraph = subgraph
        self.subground = subground
        self.timestamp = initial_timestamp

        self.dataframe_tvl = self.query(orderBy=self.subgraph.LiquidityPool.totalValueLockedUSD)
        self.dataframe_volume = self.query(orderBy=self.subgraph.LiquidityPool.cumulativeVolumeUSD)

    def query(self, orderBy):
        pools = self.subgraph.Query.liquidityPools(
            first=10,
            orderBy=orderBy,
            orderDirection="desc",
        )

        dataframe = self.subground.query_df(
            [pools.id, pools.name, pools.totalValueLockedUSD, pools.cumulativeVolumeUSD]
        )

        return dataframe

    def top_10_pools_by_tvl(self):
        chart = CustomPieChart(
            chart_title="Top 10 Pools (By TVL)", 
        )
        chart.add(
            series_name="",
            data=[
                list(z)
                for z in zip(
                    self.dataframe_tvl.liquidityPools_name.to_list(),
                    self.dataframe_tvl.liquidityPools_totalValueLockedUSD.round(1).to_list(),
                )
            ],
        )
        return chart.PIE_CHART

    def top_10_pools_by_volume(self):
        chart = CustomPieChart(
            chart_title="Top 10 Pools (By Volume)", 
        )

        chart.add(
            series_name="",
            data=[
                list(z)
                for z in zip(
                    self.dataframe_volume.liquidityPools_name.to_list(),
                    self.dataframe_volume.liquidityPools_cumulativeVolumeUSD.round(1).to_list(),
                )
            ],
        )
        return chart.PIE_CHART

'''
'''--- apps/dex-dashboard/requirements.txt ---
streamlit==1.9.0
streamlit-echarts==0.4.0
subgrounds==0.1.1
protobuf~=3.19.0
streamlit-aggrid
'''
'''--- apps/dex-dashboard/tables.py ---
import pandas as pd

class SwapTransactions:
    def __init__(self, subgraph, subground, initial_timestamp=None):
        self.subgraph = subgraph
        self.subground = subground
        self.timestamp = initial_timestamp
        self.columns_order = [
            'Date', 'blockNumber', 'from', 'tokenIn', 'amountIn',
            'amountInUSD', 'tokenOut', 'amountOut', 'amountOutUSD'
        ]
        self.dataframe = self.query()

    def query(self):
        swaps = self.subgraph.Query.swaps(
            first=15,
            orderBy=self.subgraph.Swap.timestamp,
            orderDirection="desc",
        )

        dataframe = self.subground.query_df([swaps])
        
        if dataframe.empty:
            return dataframe
        
        dataframe.columns = [
            "id", "hash", "logIndex", "protocol_id", "to", "from", "blockNumber", "Date", "tokenIn",
            "amountIn", "amountInUSD", "tokenOut", "amountOut", "amountOutUSD", "pool_id",
        ]

        dataframe.drop(
            ["id", "to", "logIndex", "protocol_id", "pool_id"], axis=1, inplace=True
        )
        dataframe['hash'] = dataframe['hash'].map(str)
        dataframe['blockNumber'] = dataframe['blockNumber'].map(int)
        dataframe['Date'] = pd.to_datetime(dataframe['Date'].map(int), unit='s')
        dataframe['tokenIn'] = dataframe['tokenIn'].map(str)
        dataframe['amountIn'] = dataframe['amountIn'].map(str)
        dataframe['amountInUSD'] = dataframe['amountInUSD'].map(float)
        dataframe['tokenOut'] = dataframe['tokenOut'].map(str)
        dataframe['amountOut'] = dataframe['amountOut'].map(str)
        dataframe['amountOutUSD'] = dataframe['amountOutUSD'].map(float)

        dataframe = dataframe.reindex(columns=self.columns_order)
        
        return dataframe

class DepositTransactions:
    def __init__(self, subgraph, subground, initial_timestamp=None):
        self.subgraph = subgraph
        self.subground = subground
        self.timestamp = initial_timestamp
        self.columns_order = [
            'Date', 'blockNumber', 'from', 'outputToken', 'outputTokenAmount',
            'outputTokenAmountUSD'
        ]
        self.dataframe = self.query()

    def query(self):
        swaps = self.subgraph.Query.deposits(
            first=15,
            orderBy=self.subgraph.Deposit.timestamp,
            orderDirection="desc",
        )

        dataframe = self.subground.query_df([swaps])

        if dataframe.empty:
            return dataframe
        
        dataframe.columns = [
            'id', 'hash', 'logIndex','protocol_id', 'to', 'from', 'blockNumber', 
            'Date', 'outputToken', 'outputTokenAmount', 'outputTokenAmountUSD', 'pool_id'
        ]

        dataframe.drop(
            ["id", "to", "logIndex", "protocol_id", "pool_id"], axis=1, inplace=True
        )
        dataframe['hash'] = dataframe['hash'].map(str)
        dataframe['blockNumber'] = dataframe['blockNumber'].map(int)
        dataframe['Date'] = pd.to_datetime(dataframe['Date'].map(int), unit='s')
        dataframe['outputToken'] = dataframe['outputToken'].map(str)
        dataframe['outputTokenAmount'] = dataframe['outputTokenAmount'].map(float)
        dataframe['outputTokenAmountUSD'] = dataframe['outputTokenAmountUSD'].map(str)

        dataframe = dataframe.reindex(columns=self.columns_order)
        
        return dataframe

class WithdawTransactions:
    def __init__(self, subgraph, subground, initial_timestamp=None):
        self.subgraph = subgraph
        self.subground = subground
        self.timestamp = initial_timestamp
        self.columns_order = [
            'Date', 'blockNumber', 'from', 'outputToken', 'outputTokenAmount',
            'outputTokenAmountUSD'
        ]
        self.dataframe = self.query()

    def query(self):
        swaps = self.subgraph.Query.withdraws(
            first=15,
            orderBy=self.subgraph.Withdraw.timestamp,
            orderDirection="desc",
        )

        dataframe = self.subground.query_df([swaps])

        if dataframe.empty:
            return dataframe
        
        dataframe.columns = [
            'id', 'hash', 'logIndex', 'protocol_id', 'to', 'from', 'blockNumber', 
            'Date', 'outputToken', 'outputTokenAmount', 'outputTokenAmountUSD', 'pool_id'
        ]

        dataframe.drop(
            ["id", "to", "logIndex", "protocol_id", "pool_id"], axis=1, inplace=True
        )
        dataframe['hash'] = dataframe['hash'].map(str)
        dataframe['blockNumber'] = dataframe['blockNumber'].map(int)
        dataframe['Date'] = pd.to_datetime(dataframe['Date'].map(int), unit='s')
        dataframe['outputToken'] = dataframe['outputToken'].map(str)
        dataframe['outputTokenAmount'] = dataframe['outputTokenAmount'].map(float)
        dataframe['outputTokenAmountUSD'] = dataframe['outputTokenAmountUSD'].map(str)

        dataframe = dataframe.reindex(columns=self.columns_order)
        
        return dataframe

'''
'''--- apps/dex-dashboard/utils.py ---
import pytz
import datetime
from pyecharts.types import JsCode

def format_xaxis(series: list[int], Multiplier=60 * 60 * 24, format: str = "%B %d, %Y"):
    return [
        datetime.datetime.fromtimestamp(i).astimezone(pytz.utc).strftime(format)
        for i in list(map(lambda x: int(x) * Multiplier, series))
    ]

def xaxis_label_formatter():
    return JsCode(
        """
        function Formatter(n) {
            let word = n.split(',');
            
            return word[0];
        };
        """
    )

def yaxis_label_formatter():
    return JsCode(
        """
        function Formatter(n) {
            if (n < 1e3) return n;
            if (n >= 1e3 && n < 1e6) return +(n / 1e3).toFixed(1) + "K";
            if (n >= 1e6 && n < 1e9) return +(n / 1e6).toFixed(1) + "M";
            if (n >= 1e9 && n < 1e12) return +(n / 1e9).toFixed(1) + "B";
            if (n >= 1e12) return +(n / 1e12).toFixed(1) + "T";
        };
        """
    )

'''
'''--- apps/erc20-analytics/app.py ---
import streamlit as st
from streamlit_autorefresh import st_autorefresh
import altair as alt
import pandas as pd
from subgrounds.subgrounds import Subgrounds

# Refresh every 30 seconds
REFRESH_INTERVAL_SEC = 30

sg = Subgrounds()
erc20Subgraph = sg.load_subgraph("https://api.thegraph.com/subgraphs/name/corerouter/erc20")

def fetch_tokens(subgraph):
    tokens = subgraph.Query.tokens(
        orderBy='id', 
        orderDirection='desc',
        first=10
    )

    df = sg.query_df(
        [
            tokens.id,
            tokens.symbol,
            tokens.totalSupply,
            tokens.holderCount,
            tokens.transferCount
        ]
    )

    df = df.rename(
        columns=lambda x: x[len("tokens_") :]
    )
   
    df["address"] = df["id"]
    df["totalSupply"] = df["totalSupply"].map("{:,.0f}".format)
    df["holderCount"] = df["holderCount"].map("{:,.0f}".format)
    df["transferCount"] = df["transferCount"].map("{:,.0f}".format)

    return df[["symbol", "totalSupply", "holderCount", "transferCount", "address"]]

def fetch_account_token(subgraph, tokenAddress):
    accountBalances = subgraph.Query.accountBalances(
        orderBy='amount', 
        orderDirection='desc',
        first=10,
        where={
            'token' : tokenAddress
        }
    )
    account_df = sg.query_df(
        [
            accountBalances.account,
            accountBalances.amount
        ]
    )

    tokenDailySnapshots = subgraph.Query.tokenDailySnapshots(
        orderBy='id', 
        orderDirection='desc',
        first=100,
        where={
            'token' : tokenAddress
        }
    )

    token_df = sg.query_df(
        [
            tokenDailySnapshots.id,
            tokenDailySnapshots.dailyTransferCount,
            tokenDailySnapshots.dailyTransferAmount
        ]
    )

    if account_df.empty | token_df.empty:
        return account_df,token_df

    account_df = account_df.rename(
        columns=lambda x: x[len("accountBalances_") :]
    )
    account_df["amount"] = account_df["amount"].map("{:,.2f}".format)

    token_df = token_df.rename(
        columns=lambda x: x[len("tokenDailySnapshots_") :]
    )
    token_df["Date"] = pd.to_datetime(token_df["id"].str.split("-").apply(lambda x: x[1]), unit="d").dt.date
    token_df = token_df.drop(columns="id")

    return account_df,token_df

st.set_page_config(layout="wide")
ticker = st_autorefresh(interval=REFRESH_INTERVAL_SEC * 1000, key="ticker")
st.title("ERC20 Token Analytics")

data_loading = st.text(f"[Every {REFRESH_INTERVAL_SEC} seconds] Loading data...")
df = fetch_tokens(erc20Subgraph)
data_loading.text(f"[Every {REFRESH_INTERVAL_SEC} seconds] Loading data... done!")

st.header("Token list")
st.markdown(df.to_markdown())

st.header("Specific Token Metrics")
symbol = st.selectbox(
    "Select token",
    df["symbol"]
)

tokenAddress = df.loc[df['symbol'] == symbol, 'address'].iloc[0]
data_loading = st.text(f"[Every {REFRESH_INTERVAL_SEC} seconds] Loading data...")
account_df,token_df = fetch_account_token(erc20Subgraph, tokenAddress)
data_loading.text(f"[Every {REFRESH_INTERVAL_SEC} seconds] Loading data... done!")

if(account_df.empty | token_df.empty):
    st.write("No data yet.")
else:
    st.header("Top Account")
    st.markdown(account_df.to_markdown())

    st.header("Token Daily Snapshot")
    token_snapshot_dailyTransferCount_line_chart = (
        alt.Chart(token_df)
        .mark_line()
        .encode(
            x="Date:T",
            y="dailyTransferCount:Q"
        )
    )
    st.altair_chart(token_snapshot_dailyTransferCount_line_chart, use_container_width=True)

    token_snapshot_dailyTransferAmount_line_chart = (
        alt.Chart(token_df)
        .mark_line()
        .encode(
            x="Date:T",
            y="dailyTransferAmount:Q"
        )
    )
    st.altair_chart(token_snapshot_dailyTransferAmount_line_chart, use_container_width=True)

'''
'''--- apps/erc20-analytics/requirements.txt ---
streamlit
streamlit-autorefresh
subgrounds
altair
tabulate

'''
'''--- apps/makerdao-analytics/makerdao.py ---
from utilities.coingecko import get_coin_market_cap, get_market_data
from subgrounds.subgrounds import Subgrounds
from streamlit_autorefresh import st_autorefresh
from datetime import datetime
import streamlit as st
import altair as alt
import pandas as pd

# Refresh every 30 seconds
REFRESH_INTERVAL_SEC = 30

# Initialize Subgrounds
SUBGRAPH_URL = "https://api.thegraph.com/subgraphs/name/messari/makerdao-ethereum" # messari/makerdao-ethereum
sg = Subgrounds()
makerdao = sg.load_subgraph(SUBGRAPH_URL)
#  python3.10 -m streamlit run protocols/makerdao.py 

x= ["0xF72beaCc6fD334E14a7DDAC25c3ce1Eb8a827E10",
 "0xb0D2EB3C2cA3c6916FAb8DCbf9d9c165649231AE",
 "0x065f44cd602cc6680e82e516125839b9bbbbe57e",
 "0x850c7cc8757ce1fa8ced709f297d842e12e61759",
 "0xaea2e71b631fa93683bcf256a8689dfa0e094fcd",
 "0x6041631c566eb8dc6258a75fa5370761d4873990",
 "0xf92c2a3c91bf869f77f9cb221c5ab1b1ada8a586",
 "0xe9dcf2d2a17ead11fab8b198578b20535370be6a",
 "0x30df229cefa463e991e29d42db0bae2e122b2ac7"]

x = [a.lower() for a in x]
#####################
##### Streamlit #####
#####################

st.set_page_config(layout="wide")
ticker = st_autorefresh(interval=REFRESH_INTERVAL_SEC * 1000, key="ticker")
st.title("MakerDao Analytics")

data_loading = st.text(f"[Every {REFRESH_INTERVAL_SEC} seconds] Loading data...")

def format_currency(x):
    return "${:.1f}K".format(x/1000)

def get_financial_snapshots(subgraph):
    financialSnapshot = subgraph.Query.financialsDailySnapshots(
    orderBy=subgraph.FinancialsDailySnapshot.timestamp,
    orderDirection='desc',
    first=100
    )
    df = sg.query_df([
    financialSnapshot.id,
    financialSnapshot.totalValueLockedUSD,
    financialSnapshot.dailyProtocolSideRevenueUSD,
    financialSnapshot.dailySupplySideRevenueUSD,
    financialSnapshot.dailyTotalRevenueUSD,
    financialSnapshot.totalDepositBalanceUSD,
    financialSnapshot.totalBorrowBalanceUSD,
    financialSnapshot.dailyDepositUSD,
    financialSnapshot.dailyBorrowUSD,
    financialSnapshot.dailyLiquidateUSD,
    financialSnapshot.cumulativeBorrowUSD,
    financialSnapshot.cumulativeLiquidateUSD,
    financialSnapshot.mintedTokenSupplies,
    financialSnapshot.timestamp,
    ])
    df['Date'] = df['financialsDailySnapshots_id'].apply(lambda x: datetime.utcfromtimestamp(int(x)*86400))
    df['Collateralization Ratio'] = df['financialsDailySnapshots_totalBorrowBalanceUSD'] / df['financialsDailySnapshots_totalDepositBalanceUSD']
    df['financialsDailySnapshots_mintedTokenSupplies'] = df['financialsDailySnapshots_mintedTokenSupplies'].apply(lambda x: float(x)/1e18)
    df['Dai Supply'] = [row['financialsDailySnapshots_mintedTokenSupplies'] for i,row in df.iterrows()]
    df = df.rename(columns={
        'financialsDailySnapshots_cumulativeBorrowUSD':'Loan Origination',
        'financialsDailySnapshots_cumulativeLiquidateUSD':'Cumulative Liquidations',
        'financialsDailySnapshots_dailyBorrowUSD':'Daily Borrows USD',
        'financialsDailySnapshots_dailyLiquidateUSD':'Daily Liquidations USD',
        'financialsDailySnapshots_dailyDepositUSD':'Daily Deposits USD',
        'financialsDailySnapshots_dailyTotalRevenueUSD':'Daily Total Revenue',
        'financialsDailySnapshots_dailySupplySideRevenueUSD':'Daily Supply Revenue',
        'financialsDailySnapshots_dailyProtocolSideRevenueUSD':'Daily Protocol Revenue',
        'financialsDailySnapshots_totalValueLockedUSD':'Total Value Locked',
        'financialsDailySnapshots_totalDepositBalanceUSD':'Total Deposit Balance',
        'financialsDailySnapshots_totalBorrowBalanceUSD':'Total Borrow Balance'
        })
    print(df)
    return df

def get_usage_metrics_df(subgraph):
    usageMetrics = subgraph.Query.usageMetricsDailySnapshots(
    orderBy=subgraph.UsageMetricsDailySnapshot.timestamp,
    orderDirection='desc',
    first=100
    )
    df = sg.query_df([
    usageMetrics.id,
    usageMetrics.dailyDepositCount,
    usageMetrics.dailyWithdrawCount,
    usageMetrics.dailyBorrowCount,
    usageMetrics.dailyRepayCount,
    usageMetrics.dailyLiquidateCount,
    usageMetrics.dailyActiveUsers,
    usageMetrics.cumulativeUniqueUsers,
    ])
    df['Date'] = df['usageMetricsDailySnapshots_id'].apply(lambda x: datetime.utcfromtimestamp(int(x)*86400))
    df = df.rename(columns={
        'usageMetricsDailySnapshots_dailyDepositCount':'Daily Deposit Count',
        'usageMetricsDailySnapshots_dailyWithdrawCount':'Daily Withdraw Count',
        'usageMetricsDailySnapshots_dailyBorrowCount':'Daily Borrow Count',
        'usageMetricsDailySnapshots_dailyRepayCount':'Daily Repay Count',
        'usageMetricsDailySnapshots_dailyLiquidateCount':'Daily Liquidation Count',
        'usageMetricsDailySnapshots_dailyActiveUsers':'Daily Active Users',
        'usageMetricsDailySnapshots_cumulativeUniqueUsers':'Cumulative New Users'
        })
    return df

def get_markets_df(subgraph):
    markets = subgraph.Query.markets(
        first=100,
        where=[subgraph.Market.id != '0x0000000000000000000000000000000000000000']
    )
    markets_df = sg.query_df([
        markets.id,
        markets.name,
        markets.totalValueLockedUSD,
        markets.inputToken
    ])
    return markets_df

def get_events_df(subgraph,event_name='Deposit'):
    slug = event_name.lower()+'s'
    event = subgraph.Query.__getattribute__(slug)(
        orderBy=subgraph.__getattribute__(event_name).timestamp,
        orderDirection='desc',
        first=10
    )
    df = sg.query_df([
        event.timestamp,
        event.hash,
        event.__getattribute__('from'),
        event.to,
        event.market.name,
        event.asset.symbol,
        event.amountUSD
    ])
    df = df.rename(columns={
        slug+'_timestamp':'Date',
        slug+'_hash':'Transaction Hash',
        slug+'_from':'From',
        slug+'_to':'To',
        slug+'_market_name':'Market',
        slug+'_asset_symbol':'Asset',
        slug+'_amountUSD':'Amount'
    })
    df['Date'] = df['Date'].apply(lambda x: datetime.utcfromtimestamp(int(x)))
    df['Amount'] = df['Amount'].apply(lambda x: "${:.1f}k".format((x/1000)))
    return df

def get_revenue_df(df):
    mcap_df = get_coin_market_cap('maker')
    revenue_df = df.merge(mcap_df, how='inner', on='Date')
    revenue_df = revenue_df[(revenue_df['Daily Protocol Revenue']>0) | (revenue_df['Daily Total Revenue']>0)]
    revenue_df['P/E Ratio'] = (revenue_df['mcap'] / revenue_df['Daily Protocol Revenue'])/1000
    revenue_df['P/S Ratio'] = (revenue_df['mcap'] / revenue_df['Daily Total Revenue'])/1000
    revenue_df = revenue_df.sort_values(by='Date')[:-1]
    return revenue_df

def get_top_10_markets_tvl(markets_df):
    top_10 = markets_df.sort_values(by='markets_totalValueLockedUSD',ascending=False)[:10]
    top_10 = top_10.rename(columns={'markets_totalValueLockedUSD':'Total Value Locked', 'markets_name':'Market'})
    return top_10

def get_asset_tvl(markets_df):
    assets_df = markets_df.copy()
    for i, row in assets_df.iterrows():
        if row['markets_inputToken_name'] == 'Uniswap V2':
            assets_df.loc[i, 'markets_inputToken_symbol'] = row['markets_name'].split('-')[0]
    assets_df = assets_df.groupby(['markets_inputToken_id', 'markets_inputToken_symbol'])['markets_totalValueLockedUSD'].sum().reset_index()
    assets_df = assets_df[assets_df['markets_totalValueLockedUSD'] >= 1.0]
    assets_df = assets_df.rename(columns={'markets_totalValueLockedUSD': 'Total Value Locked', 'markets_inputToken_symbol': 'Token'})
    return assets_df

def get_financial_statement_df(df):
    financial_df = df[['Date', 'Daily Deposits USD', 'Daily Borrows USD', 'Daily Liquidations USD', 'Total Deposit Balance', 'Total Borrow Balance']]
    return financial_df

def get_stable_ratio(assets_df):
    stablecoins = ['TUSD','GUSD','USDC','PAX','USDT']
    stable_ratio = assets_df[assets_df['Token'].isin(stablecoins)]['Total Value Locked'].sum() / assets_df['Total Value Locked'].sum()
    stable_ratio_df = pd.DataFrame({'ratio': [stable_ratio, 1-stable_ratio], 'Collateral Type': ['STABLE', 'NON-STABLE']})
    return stable_ratio_df

df = get_financial_snapshots(makerdao)
usage_df = get_usage_metrics_df(makerdao)
markets_df = get_markets_df(makerdao)
revenue_df = get_revenue_df(df)

top_10 = get_top_10_markets_tvl(markets_df)
assets_df = get_asset_tvl(markets_df)

data_loading.text(f"[Every {REFRESH_INTERVAL_SEC} seconds] Loading data... done!")

scales = alt.selection_interval(bind='scales')
# Create a selection that chooses the nearest point & selects based on x-value

date_axis = alt.X("Date:T", axis=alt.Axis(title=None, format="%Y-%m-%d", labelAngle=45, tickCount=20))
nearest = alt.selection(type='single', nearest=True, on='mouseover',
                        fields=['Date'], empty='none')

def build_financial_chart(df, column, title=None, y_axis_format='$,.2f',color=None):
    title = column if not title else title
    y_axis = alt.Y(column+":Q", axis=alt.Axis(format=y_axis_format)) if y_axis_format else alt.Y(column+":Q")
    line = alt.Chart(df).mark_line().encode(x=date_axis, y=y_axis, tooltip=[alt.Tooltip("Date")])
    if color:
        line = alt.Chart(df).mark_line().encode(x=date_axis, y=y_axis, tooltip=[alt.Tooltip("Date")],color=color)
    selectors = alt.Chart(df).mark_point().encode(x=date_axis, opacity=alt.value(0),).add_selection(nearest)
    points = line.mark_point().encode(opacity=alt.condition(nearest, alt.value(1), alt.value(0)))
    text = line.mark_text(align='left', dx=5, dy=-5, color='black').encode(text=alt.condition(nearest, column+":Q", alt.value(' ')))
    rules = alt.Chart(df).mark_rule(color='gray').encode(x=date_axis,).transform_filter(nearest)
    line_chart = alt.layer(line, selectors, points, rules, text).interactive().properties(title=title)
    return line_chart

def build_pie_chart(df, theta, color):
    base = alt.Chart(df).encode(
        theta=alt.Theta(theta+':Q', stack=True),
        color=alt.Color(color+':N', legend=None),
        tooltip=[alt.Tooltip(theta),
                 alt.Tooltip(color)]
    )
    pie = base.mark_arc(outerRadius=110)
    text = base.mark_text(radius=130, size=8).encode(text=color)
    return pie + text

def build_tvl_per_asset_pie(assets_df):
    selection = alt.selection_multi(fields=['Token'], bind='legend')
    pie_chart = alt.Chart(assets_df).mark_arc().encode(
        theta=alt.Theta(field="Total Value Locked", type="quantitative"),
        tooltip=[alt.Tooltip("Total Value Locked"), alt.Tooltip("Token")],
        color=alt.condition(selection, 'Token:N', alt.value('white'))
    ).add_selection(selection).configure_legend(
        titleFontSize=14,
        labelFontSize=10,
        columns=2
    )
    return pie_chart

def build_multi_line_rev_chart(revenue_df):
    sub_df = revenue_df[['Date','Daily Protocol Revenue','Daily Supply Revenue']]
    sub_df = sub_df.rename(columns={'Daily Protocol Revenue': 'Protocol', 'Daily Supply Revenue': 'Supply'})
    formatted_df = sub_df.melt(id_vars=['Date'], var_name='Side', value_name='Revenue')
    chart = build_financial_chart(formatted_df,  'Revenue', 'Daily Revenue', color='Side')
    return chart

st.header('Protocol Snapshot')
col1, col2, col3, col4, col5, col6 = st.columns(6)

market_data = get_market_data('maker')

def has_percent(val):
    return True if '%' in val else False

def format_percent_to_float(val):
    return float(val.strip('%'))

def which_color(val):
    if isinstance(val, str):
        val = format_percent_to_float(val) if has_percent(val) else val
    return 'green' if val > 0 else 'red'

def get_colored_text(val):
    text = '<span id="bottom-right" class="is-display-inline-block is-text-align-right" style="font-family:sans-serif; color:{}; font-size: 18px;">{}</span>'.format(which_color(val),val)
    return text

def annualize_value(val_list):
    num_vals = len(val_list)
    annual_val = (sum(val_list) / num_vals) * 365
    return annual_val

with col1:
    st.subheader(market_data["price"])
    st.markdown('24h: {}'.format(get_colored_text(market_data['24hr_change'])) + '$~~~~~$ 7d: {}'.format(get_colored_text(market_data['7d_change'])), unsafe_allow_html=True)
    st.markdown('30d: {}'.format(get_colored_text(market_data['30d_change'])) + '$~~~~$ 1y: {}'.format(get_colored_text(market_data['1y_change'])), unsafe_allow_html=True)

with col2:
    st.header('')
    text = '<span style="color:gray;">Circulating market cap:</span><br><span style="color:black;">{}</span>'.format(market_data['circ_market_cap'])
    st.markdown(text, unsafe_allow_html=True)
    text = '<span style="color:gray;">Fully-diluted market cap:</span><br><span style="color:black;">{}</span>'.format(market_data['fdv_market_cap'])
    st.markdown(text, unsafe_allow_html=True)

with col3:
    st.header('')
    rate_change_rev = (sum(df['Daily Total Revenue'][:30])-sum(df['Daily Total Revenue'][31:60]))/sum(df['Daily Total Revenue'][:30])
    text = '<span style="color:gray;">Total revenue 30d:</span><br>' \
           '<span style="color:black;">{}</span>' \
           '<span style="color:{};"> ({})</span>'.format("${:,.2f}".format(sum(df['Daily Total Revenue'][:30])),which_color(rate_change_rev), '{:.2%}'.format(rate_change_rev))
    st.markdown(text, unsafe_allow_html=True)
    rate_change_rev = (sum(df['Daily Protocol Revenue'][:30])-sum(df['Daily Protocol Revenue'][31:60]))/sum(df['Daily Protocol Revenue'][:30])
    text = '<span style="color:gray;">Total protocol revenue 30d:</span><br>' \
           '<span style="color:black;">{}</span>' \
           '<span style="color:{};"> ({})</span>'.format("${:,.2f}".format(sum(df['Daily Protocol Revenue'][:30])),which_color(rate_change_rev), '{:.2%}'.format(rate_change_rev))
    st.markdown(text, unsafe_allow_html=True)

with col4:
    st.header('')
    text = '<span style="color:gray;">Annualized total revenue:</span><br><span style="color:black;">{}</span>'.format("${:,.2f}".format(annualize_value(df['Daily Total Revenue'])))
    st.markdown(text, unsafe_allow_html=True)
    text = '<span style="color:gray;">Annualized protocol revenue:</span><br><span style="color:black;">{}</span>'.format("${:,.2f}".format(annualize_value(df['Daily Protocol Revenue'])))
    st.markdown(text, unsafe_allow_html=True)

with col5:
    st.header('')
    text = '<span style="color:gray;">P/S Ratio:</span><br><span style="color:black;">{}</span>'.format("{:,.2f}".format(revenue_df.iloc[-1]['P/S Ratio']))
    st.markdown(text, unsafe_allow_html=True)
    text = '<span style="color:gray;">P/E Ratio:</span><br><span style="color:black;">{}</span>'.format("{:,.2f}".format(revenue_df.iloc[-1]['P/E Ratio']))
    st.markdown(text, unsafe_allow_html=True)

with col6:
    st.header('')
    text = '<span style="color:gray;">Annualized borrowing volume:</span><br><span style="color:black;">{}</span>'.format("${:,.2f}".format(annualize_value(df['Daily Borrows USD'])))
    st.markdown(text, unsafe_allow_html=True)
    text = '<span style="color:gray;">Total value locked:</span><br><span style="color:black;">{}</span>'.format(market_data['tvl'])
    st.markdown(text, unsafe_allow_html=True)

st.header('Key Metrics')

with st.container():
    tvl = build_financial_chart(df, 'Total Value Locked')
    rev = build_financial_chart(df, 'Daily Total Revenue')
    dai_supply = build_financial_chart(df, 'Dai Supply')

    st.altair_chart(tvl | rev | dai_supply, use_container_width=False)

col1, col2, col3 = st.columns(3)

with col1:
    st.subheader("Top 10 Markets by TVL")
    top_10_markets = build_pie_chart(top_10, "Total Value Locked", "Market")
    st.altair_chart(top_10_markets, use_container_width=False)

with col2:
    st.subheader("TVL by Asset")
    tvl_per_asset = build_tvl_per_asset_pie(assets_df)
    st.altair_chart(tvl_per_asset, use_container_width=False)

with col3:
    st.subheader("DAI Collateral Split")
    stable_ratio_df = get_stable_ratio(assets_df)
    stable_ratio_pie = alt.Chart(stable_ratio_df).mark_arc(innerRadius=50).encode(
        theta=alt.Theta(field="ratio", type="quantitative"),
        color=alt.Color(field="Collateral Type", type="nominal"),
        tooltip=[alt.Tooltip("ratio")],
    )
    st.altair_chart(stable_ratio_pie, use_container_width=False)

st.header('Financial Statement')

statement_df = get_financial_statement_df(df)
st.table(data=statement_df[:10])

st.header('Financial Metrics')

col1, col2, col3 = st.columns(3)

with col1:
    protocol_rev = build_multi_line_rev_chart(revenue_df)
    st.altair_chart(protocol_rev, use_container_width=False)

with col2:
    ps_ratio = build_financial_chart(revenue_df, 'P/S Ratio', y_axis_format=None)
    st.altair_chart(ps_ratio, use_container_width=False)

with col3:
    pe_ratio = build_financial_chart(revenue_df, 'P/E Ratio', y_axis_format=None)
    st.altair_chart(pe_ratio, use_container_width=False)

col_ratio = build_financial_chart(revenue_df, 'Collateralization Ratio','Dai Collateralization Ratio', y_axis_format=None)
st.altair_chart(col_ratio, use_container_width=False)

st.header('Usage Metrics')

with st.container():
    active = build_financial_chart(usage_df, 'Daily Active Users', y_axis_format=None)
    new = build_financial_chart(usage_df, 'Cumulative New Users', y_axis_format=None)

    st.altair_chart(active | new, use_container_width=False)

st.header('Live Transactions')

col1, col2 = st.columns(2)

with col1:
    st.subheader('Deposits')
    deposits_df = get_events_df(makerdao)
    st.dataframe(data=deposits_df)

with col2:
    st.subheader('Withdrawals')
    withdrawals_df = get_events_df(makerdao, 'Withdraw')
    st.dataframe(withdrawals_df)
'''
'''--- apps/makerdao-analytics/requirements.txt ---
subgrounds
streamlit
streamlit-autorefresh
black
altair

'''
'''--- apps/makerdao-analytics/utilities/__init__.py ---

'''
'''--- apps/makerdao-analytics/utilities/coingecko.py ---
from datetime import datetime
import pandas as pd
import requests

BASE_URL = "https://api.coingecko.com/api/v3/"

def get_coin_market_chart(token_name):
    url = BASE_URL+"coins/{}/market_chart?vs_currency=usd&days=max&interval=daily".format(token_name)
    resp = requests.get(url)
    if resp.status_code == 200:
        result = resp.json()
    else:
        print('Request Error: {}: invalid token name'.format(resp.status_code))
        result = {}
    return result

def get_coin_market_cap(token_name):
    market_chart = get_coin_market_chart(token_name)
    market_caps = [{
        'Date':datetime.utcfromtimestamp(int(item[0]/1000)),
        'mcap':item[1]
        } for item in market_chart['market_caps']]
    df = pd.DataFrame(market_caps)
    return df

def get_price(token_name):
    url = 'https://api.coingecko.com/api/v3/simple/price?ids={}&vs_currencies=usd'.format(token_name)
    resp = requests.get(url)
    if resp.status_code == 200:
        result = resp.json()
        return result[token_name]['usd']
    else:
        print('Request Error: {}: invalid token name'.format(resp.status_code))
        return 0

def get_market_data(token_name):
    url = 'https://api.coingecko.com/api/v3/coins/{}?market_data=true&community_data=false&developer_data=false'.format(token_name)
    resp = requests.get(url)
    result_dict = {}
    if resp.status_code == 200:
        result = resp.json()
        result_dict['price'] = "${:,.2f}".format(result['market_data']['current_price']['usd'])
        result_dict['ath'] = "${:,.2f}".format(result['market_data']['ath']['usd'])
        result_dict['atl'] = "${:,.2f}".format(result['market_data']['atl']['usd'])
        result_dict['24hr_change'] = "{:,.2f}%".format(result['market_data']['price_change_percentage_24h'])
        result_dict['7d_change'] = "{:,.2f}%".format(result['market_data']['price_change_percentage_7d'])
        result_dict['30d_change'] = "{:,.2f}%".format(result['market_data']['price_change_percentage_30d'])
        result_dict['1y_change'] = "{:,.2f}%".format(result['market_data']['price_change_percentage_1y'])
        result_dict['circ_market_cap'] = "${:,.2f}".format(result['market_data']['circulating_supply'] * result['market_data']['current_price']['usd'])
        result_dict['fdv_market_cap'] = "${:,.2f}".format(result['market_data']['fully_diluted_valuation']['usd'])
        result_dict['tvl'] = "${:,.2f}".format(result['market_data']['total_value_locked']['usd'])
    else:
        print('Request Error: {}: invalid token name'.format(resp.status_code))
    return result_dict
'''
'''--- apps/uniswap-analytics/app.py ---
import streamlit as st
from streamlit_autorefresh import st_autorefresh
import altair as alt
import pandas as pd
from subgrounds.subgrounds import Subgrounds

# Refresh every 30 seconds
REFRESH_INTERVAL_SEC = 30

sg = Subgrounds()
subgraphs = {
    "matic": sg.load_subgraph(
        "https://api.thegraph.com/subgraphs/name/messari/uniswap-v3-polygon"
    ),
    "optimism": sg.load_subgraph(
        "https://api.thegraph.com/subgraphs/name/messari/uniswap-v3-optimism"
    ),
    "arbitrum_one": sg.load_subgraph(
        "https://api.thegraph.com/subgraphs/name/messari/uniswap-v3-arbitrum"
    ),
}

def fetch_data(network, subgraph):
    financial_metrics = subgraph.Query.financialsDailySnapshots(
        orderBy=subgraph.FinancialsDailySnapshot.id,
        orderDirection="desc",
        first=100,
    )
    usage_metrics = subgraph.Query.usageMetricsDailySnapshots(
        orderBy=subgraph.UsageMetricsDailySnapshot.id,
        orderDirection="desc",
        first=100,
    )
    financial_df = sg.query_df(
        [
            financial_metrics.id,
            financial_metrics.totalValueLockedUSD,
            financial_metrics.cumulativeVolumeUSD,
            financial_metrics.cumulativeTotalRevenueUSD,
        ]
    )
    usage_df = sg.query_df(
        [
            usage_metrics.id,
            usage_metrics.dailyActiveUsers,
        ]
    )
    financial_df = financial_df.rename(
        columns=lambda x: x[len("financialsDailySnapshots_") :]
    )
    usage_df = usage_df.rename(
        columns=lambda x: x[len("usageMetricsDailySnapshots_") :]
    )
    df = pd.merge(financial_df, usage_df)
    df["network"] = network
    df["date"] = pd.to_datetime(df["id"], unit="d")
    df = df.drop(columns="id")
    return df

st.set_page_config(page_icon="🦄", layout="wide")
ticker = st_autorefresh(interval=REFRESH_INTERVAL_SEC * 1000, key="ticker")
st.title("🦄 Uniswap Analytics")

data_loading = st.text(f"[Every {REFRESH_INTERVAL_SEC} seconds] Loading data...")
df = pd.concat(
    map(lambda x: fetch_data(x, subgraphs[x]), ["matic", "optimism", "arbitrum_one"]),
    axis=0,
)
data_loading.text(f"[Every {REFRESH_INTERVAL_SEC} seconds] Loading data... done!")

# Plot charts with altair is like a breeze
st.header("Revenue")
rev_stacked_bar_chart = (
    alt.Chart(df)
    .mark_bar()
    .encode(x="date:T", y="cumulativeTotalRevenueUSD:Q", color="network:N")
)
st.altair_chart(rev_stacked_bar_chart, use_container_width=True)

st.header("TVL")
tvl_stacked_bar_chart = (
    alt.Chart(df)
    .mark_bar()
    .encode(x="date:T", y="totalValueLockedUSD:Q", color="network:N")
)
st.altair_chart(tvl_stacked_bar_chart, use_container_width=True)

st.header("Volume")
volume_norm_stacked_area_chart = (
    alt.Chart(df)
    .mark_area()
    .encode(
        x="date:T",
        y=alt.Y("cumulativeVolumeUSD:Q", stack="normalize"),
        color="network:N",
    )
)
st.altair_chart(volume_norm_stacked_area_chart, use_container_width=True)

st.header("DAU")
dau_line_chart = (
    alt.Chart(df)
    .mark_line()
    .encode(
        x="date:T",
        y="dailyActiveUsers:Q",
        color="network:N",
    )
)
st.altair_chart(dau_line_chart, use_container_width=True)

'''
'''--- apps/uniswap-analytics/requirements.txt ---
streamlit
streamlit-autorefresh
subgrounds
black
altair

'''
'''--- apps/whale-watcher/app.py ---
import streamlit as st
from streamlit_autorefresh import st_autorefresh
import pandas as pd
from datetime import datetime
from subgrounds.subgrounds import Subgrounds

# Refresh every 10 seconds
REFRESH_INTERVAL_SEC = 10

EXPLORERS = {
    "mainnet": "etherscan.io",
    "matic": "polygonscan.com",
    "optimism": "optimistic.etherscan.io",
    "arbitrum_one": "arbiscan.io",
}

sg = Subgrounds()
subgraphs = {
    "mainnet": sg.load_subgraph(
        "https://api.thegraph.com/subgraphs/name/messari/uniswap-v3-ethereum"
    ),
    "matic": sg.load_subgraph(
        "https://api.thegraph.com/subgraphs/name/messari/uniswap-v3-polygon"
    ),
    "optimism": sg.load_subgraph(
        "https://api.thegraph.com/subgraphs/name/messari/uniswap-v3-optimism"
    ),
    "arbitrum_one": sg.load_subgraph(
        "https://api.thegraph.com/subgraphs/name/messari/uniswap-v3-arbitrum"
    ),
}

def fetch_data(subgraph, amount_in_usd_gte):
    latest_swaps = subgraph.Query.swaps(
        where=[subgraph.Swap.amountInUSD >= amount_in_usd_gte],
        orderBy=subgraph.Swap.timestamp,
        orderDirection="desc",
        first=10,
    )
    df = sg.query_df(
        [
            latest_swaps.hash,
            latest_swaps.protocol.name,
            latest_swaps.protocol.network,
            latest_swaps.timestamp,
            latest_swaps.tokenIn.symbol,
            latest_swaps.amountInUSD,
            latest_swaps.tokenOut.symbol,
            latest_swaps.amountOutUSD,
        ]
    )
    df = df.rename(columns=lambda x: x[len("swaps_") :])
    df["time"] = df["timestamp"].apply(
        lambda x: datetime.fromtimestamp(x).strftime("%H:%M:%S")
    )
    df["dex"] = df["protocol_name"]
    df["network"] = df["protocol_network"]

    df["amountInUSD"] = df["amountInUSD"].map("{:,.2f}".format)
    df["amountOutUSD"] = df["amountOutUSD"].map("{:,.2f}".format)
    df["swap"] = df.apply(
        lambda x: f"""\${x["amountInUSD"]} {x["tokenIn_symbol"]} 💸 \${x["amountOutUSD"]} {x["tokenOut_symbol"]}""",
        axis=1,
    )
    df["txn"] = df.apply(
        lambda x: f"""[🔗](https://{EXPLORERS[x["protocol_network"].lower()]}/tx/{x["hash"]})""",
        axis=1,
    )
    return df[["time", "dex", "network", "swap", "txn"]]

st.set_page_config(page_icon="🐋")
ticker = st_autorefresh(interval=REFRESH_INTERVAL_SEC * 1000, key="ticker")
st.title("🐋 Whale Watcher")

networks = st.multiselect(
    "Select networks",
    ["mainnet", "matic", "optimism", "arbitrum_one"],
    ["mainnet", "matic"],
)

amount_in_usd_gte = st.select_slider(
    "Only display swaps with amount >=",
    value=100,
    options=[100, 1000, 10000, 100000],
    key="amount_in_usd_gte",
)

data_loading = st.text(f"[Every {REFRESH_INTERVAL_SEC} seconds] Loading data...")
df = pd.concat(
    map(lambda x: fetch_data(subgraphs[x], amount_in_usd_gte), networks), axis=0
)
df = df.sort_values(by=["time"], ascending=False)
data_loading.text(f"[Every {REFRESH_INTERVAL_SEC} seconds] Loading data... done!")
st.markdown(df.to_markdown())

'''
'''--- apps/whale-watcher/requirements.txt ---
streamlit
streamlit-autorefresh
subgrounds
black
tabulate

'''
'''--- dashboard/public/index.html ---
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <link rel="icon" href="%PUBLIC_URL%/favicon.ico" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="stylesheet"
          href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700&display=swap"
    />
    <meta name="theme-color" content="#000000" />
    <meta
      name="description"
      content="Subgraph validation dashboard"
    />
    <link rel="apple-touch-icon" href="%PUBLIC_URL%/logo192.png" />
    <!--
      manifest.json provides metadata used when your web app is installed on a
      user's mobile device or desktop. See https://developers.google.com/web/fundamentals/web-app-manifest/
    -->
    <link rel="manifest" href="%PUBLIC_URL%/manifest.json" />
    <!--
      Notice the use of %PUBLIC_URL% in the tags above.
      It will be replaced with the URL of the `public` folder during the build.
      Only files inside the `public` folder can be referenced from the HTML.

      Unlike "/favicon.ico" or "favicon.ico", "%PUBLIC_URL%/favicon.ico" will
      work correctly both with client-side routing and a non-root public URL.
      Learn how to configure a non-root public URL by running `npm run build`.
    -->
    <title>Subgraphs</title>
  </head>
  <body>
    <noscript>You need to enable JavaScript to run this app.</noscript>
    <div id="root"></div>
    <!--
      This HTML file is a template.
      If you open it directly in the browser, you will see an empty page.

      You can add webfonts, meta tags, or analytics to this file.
      The build step will place the bundled scripts into the <body> tag.

      To begin the development, run `npm start` or `yarn start`.
      To create a production bundle, use `npm run build` or `yarn build`.
    -->
  </body>
</html>

'''
'''--- dashboard/public/robots.txt ---
# https://www.robotstxt.org/robotstxt.html
User-agent: *
Disallow:

'''
'''--- deployment/buildDeploymentMap.js ---
const https = require('https')
const fs = require('fs')
var path = require('path');

// This script takes the deployment.json generated by CI/CD and fetches the IPFS hashes of the current deployments of each subgraph
// Pulls the deployment.json from "https://raw.githubusercontent.com/messari/subgraphs/master/deployment/deployment.json"
// Assumes the structure is "subgraphs">{FORK}>{PROTOCOL}>{NETWORK}>"messari"
// The name comes from the string on the "messari" key under the network. If this key is not available, attempts to assume the name as "messari/{PROTOCOL}-{NETWORK}"

const url = "https://raw.githubusercontent.com/messari/subgraphs/master/deployment/deployment.json";
const liveDeployments = {};
const listedNoName = {};
https.get(url, res => {
    let data = '';
    res.on('data', chunk => {
        data += chunk;
    });
    res.on('end', () => {
        data = JSON.parse(data);
        const firstLevelKeys = Object.keys(data.subgraphs)
        firstLevelKeys.forEach(x => {
            Object.keys(data.subgraphs[x]).forEach(y => {
                Object.keys(data.subgraphs[x][y]).forEach(network => {
                    if (!data.subgraphs[x][y][network].messari) {
                        listedNoName['messari' + '/' + y + '-' + network] = true;
                        liveDeployments['messari' + '/' + y + '-' + network] = "";
                    } else {
                        liveDeployments[data.subgraphs[x][y][network].messari] = "";
                    }
                })
            })
        })

        const depIdPromArr = Object.keys(liveDeployments).map(key => {
            const options = {
                hostname: 'api.thegraph.com',
                path: '/subgraphs/name/' + key,
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                },
            };
            const dataQuery = JSON.stringify({
                query: `query DeploymentIDs {
                    _meta {
                      deployment
                    }
                  }`,
            });
            return getData(key, dataQuery, options)
        })

        Promise.all(depIdPromArr).then(val => {
            const formattedDeployments = {}
            val.forEach((dep, idx) => {
                if (dep) {
                    if (dep._meta) {
                        formattedDeployments[Object.keys(liveDeployments)[idx]] = {
                            IPFS_Hash: dep._meta.deployment || 'N/A',
                            issues: []
                        }
                    }
                }
                if (!formattedDeployments[Object.keys(liveDeployments)[idx]]) {
                    formattedDeployments[Object.keys(liveDeployments)[idx]] = {
                        IPFS_Hash: 'N/A',
                        issues: ["Could not pull either current subgraph IPFS hash given subgraph name. Query attempted on _meta entity at endpoint https://api.thegraph.com/subgraphs/name/" + Object.keys(liveDeployments)[idx]]
                    }
                }
                if (listedNoName[Object.keys(liveDeployments)[idx]]) {
                    formattedDeployments[Object.keys(liveDeployments)[idx]].issues.push("deployment.json file does not have subgraph name string under the 'messari' key. Attempted to construct subgraph name from 'messari/(PROTOCOL)-(NETWORK)");
                }
                // if (!dep._meta.deployment) {
                //     formattedDeployments[Object.keys(liveDeployments)[idx]].issues.push("Could not pull either current subgraph IPFS hash given subgraph name. Query attempted on _meta entity at endpoint https://api.thegraph.com/subgraphs/name/" + Object.keys(liveDeployments)[idx])
                // }
            })

            const storeData = () => {
                try {
                    const nowDate = new Date().getMonth().toString() + '-' + new Date().getDate().toString() + '-' + new Date().getFullYear().toString()
                    const jsonPath = path.join(__dirname, 'Deployments_' + nowDate + '.json');
                    fs.writeFileSync(jsonPath, JSON.stringify(formattedDeployments, null, '\t'))

                } catch (err) {
                    console.error(err)
                }
            }
            storeData()
        })
    })
}).on('error', err => {
    console.log(err.message);
}).end()

async function getData(url, dataQuery, options) {
    return new Promise((resolve, reject) => {
        const req = https.request(options, (res) => {
            let data = '';
            res.on('data', (d) => {
                data += d;
            });
            res.on('end', () => {
                try {
                    const parsedData = { ...JSON.parse(data).data, subgraphName: url }
                    resolve(parsedData)
                } catch (e) {
                    reject(e.message)
                }
            });
        });
        req.write(dataQuery);
        req.end();
    })
}

'''
'''--- deployment/deployment.js ---
const {
  getDeploymentNetwork,
  runCommands,
  scripts,
} = require("./execution.js");
const protocolNetworkData = require("./deployment.json");
const args = require("minimist")(process.argv.slice(2));

const protocolNetworkMap = JSON.parse(JSON.stringify(protocolNetworkData))[
  "subgraphs"
];

let allScripts = new Map();
let results = "RESULTS:\n";

if (
  args.subgraph === undefined ||
  args.protocol === undefined ||
  args.network === undefined ||
  args.location === undefined ||
  args.printlogs === undefined ||
  args.merge === undefined
) {
  console.log(
    "Usage: node deployment.js --subgraph=" +
      args.subgraph +
      " --protocol=" +
      args.protocol +
      " --network=" +
      args.network +
      " --location=" +
      args.location +
      " --printlogs=" +
      args.printlogs +
      " --merge=" +
      args.merge
  );
  console.log(
    "Please check subgraph:deploy script in package.json. Make sure it matches example script in the deployments folder. "
  );
} else if (!args.subgraph || !args.location) {
  console.log("Please provide at least --SUBGRAPH and --LOCATION");
} else if (args.subgraph && args.protocol && args.network && args.location) {
  if (args.subgraph in protocolNetworkMap == false) {
    console.log(
      "Error: please specify a a valid subgraph directory or add to configurations (e.g. uniswap-forks, compound-forks, qidao, etc"
    );
  } else if (args.protocol in protocolNetworkMap[args.subgraph] == false) {
    console.log(
      "Error: please specify a valid protocol as 1st argument or add to configurations  (i.e. uniswap-v2, sushiswap, etc.)"
    );
    console.log(
      "To deploy a protocol to a specific network, pass 3 arguements (protocol/network/location)"
    );
  } else if (
    args.network in protocolNetworkMap[args.subgraph][args.protocol] ==
    false
  ) {
    console.log(
      "Error: please specify a valid network as 2nd argument or add to configurations  (i.e. mainnet, ropsten, etc.)"
    );
    console.log(
      "To deploy a protocol to a specific network, pass 3 arguements (protocol/network/location)"
    );
  } else {
    let protocol = args.protocol;
    let network = args.network;
    let template =
      protocolNetworkMap[args.subgraph][protocol][network]["template"];
    let location = "";
    let prepareConstants =
      protocolNetworkMap[args.subgraph][protocol][network]["prepare:constants"];

    // Get location for configurations or derive using standard naming convention
    if (args.location in protocolNetworkMap[args.subgraph][protocol][network]) {
      location =
        protocolNetworkMap[args.subgraph][protocol][network][args.location];
    } else {
      location =
        args.location + "/" + protocol + "-" + getDeploymentNetwork(network);
    }

    // Don't execute the script if the location is messari, you have specified not to deploy on merge, and this script is excuted by Github Action upon merge
    if (
      args.location == "messari" &&
      [false, undefined].includes(
        protocolNetworkMap[args.subgraph][protocol][network]["deploy-on-merge"]
      ) &&
      ["true", "t"].includes(args.merge.toLowerCase())
    ) {
      results += "Ignored in Deployment Configurations: " + location + "\n";
    } else {
      allScripts.set(
        location,
        scripts(protocol, network, template, location, prepareConstants)
      );
    }
    runCommands(allScripts, results, args, function (results) {});
  }
} else if (args.subgraph && args.protocol && args.location) {
  if (args.subgraph in protocolNetworkMap == false) {
    console.log(
      "Error: please specify a a valid subgraph directory or add to configurations  (e.g. uniswap-forks, compound-forks, qidao, etc"
    );
  } else if (args.protocol in protocolNetworkMap[args.subgraph] == false) {
    console.log(
      "Error: please specify a valid protocol as 1st argument or add to configurations  (i.e. uniswap-v2, sushiswap, etc.)"
    );
    console.log(
      "To deploy all networks of a specified protocol, pass 2 arguements (protocol/location)"
    );
  } else {
    let protocol = args.protocol;

    for (const network in protocolNetworkMap[args.subgraph][protocol]) {
      let template =
        protocolNetworkMap[args.subgraph][protocol][network]["template"];
      let location = "";
      let prepareConstants =
        protocolNetworkMap[args.subgraph][protocol][network][
          "prepare:constants"
        ];

      // Get location for configurations or derive using standard naming convention
      if (
        args.location in protocolNetworkMap[args.subgraph][protocol][network]
      ) {
        location =
          protocolNetworkMap[args.subgraph][protocol][network][args.location];
      } else {
        location =
          args.location + "/" + protocol + "-" + getDeploymentNetwork(network);
      }

      // Don't execute the script if the location is messari, you have specified not to deploy on merge, and this script is excuted by Github Action upon merge
      if (
        args.location == "messari" &&
        [false, undefined].includes(
          protocolNetworkMap[args.subgraph][protocol][network][
            "deploy-on-merge"
          ]
        ) &&
        ["true", "t"].includes(args.merge.toLowerCase())
      ) {
        results += "Ignored in Deployment Configurations: " + location + "\n";
      } else {
        allScripts.set(
          location,
          scripts(protocol, network, template, location, prepareConstants)
        );
      }
    }

    runCommands(allScripts, results, args, function (results) {});
  }
} else if (args.subgraph && args.location) {
  if (args.subgraph in protocolNetworkMap == false) {
    console.log(
      "Error: please specify a a valid subgraph directory or add to configurations  (e.g. uniswap-forks, compound-forks, qidao, etc"
    );
  } else {
    for (const protocol in protocolNetworkMap[args.subgraph]) {
      for (const network in protocolNetworkMap[args.subgraph][protocol]) {
        let template =
          protocolNetworkMap[args.subgraph][protocol][network]["template"];
        let location = "";
        let prepareConstants =
          protocolNetworkMap[args.subgraph][protocol][network][
            "prepare:constants"
          ];

        // Get location for configurations or derive using standard naming convention
        if (
          args.location in protocolNetworkMap[args.subgraph][protocol][network]
        ) {
          location =
            protocolNetworkMap[args.subgraph][protocol][network][args.location];
        } else {
          location =
            args.location +
            "/" +
            protocol +
            "-" +
            getDeploymentNetwork(network);
        }

        // Don't execute the script if the location is messari, you have specified not to deploy on merge, and this script is excuted by Github Action upon merge
        if (
          args.location == "messari" &&
          [false, undefined].includes(
            protocolNetworkMap[args.subgraph][protocol][network][
              "deploy-on-merge"
            ]
          ) &&
          ["true", "t"].includes(args.merge.toLowerCase())
        ) {
          results += "Ignored in Deployment Configurations: " + location + "\n";
        } else {
          allScripts.set(
            location,
            scripts(protocol, network, template, location, prepareConstants)
          );
        }
      }
    }

    runCommands(allScripts, results, args, function (results) {});
  }
} else {
  console.log("UNKOWN - Please post issue on github.");
}

'''
'''--- deployment/execution.js ---
const exec = require("child_process").exec;
const fs = require("fs");

/**
 * @param {string} protocol - Protocol that is being deployed
 * @param {string} network - Network that the protocol is being deployed to
 * @param {string} template - Template location that will be used to create subgraph.yaml
 * @param {string} location - Location in the subgraph will be deployed to {e.g. messari/uniswap-v2-ethereum}
 */
function scripts(protocol, network, template, location, constants) {
  let scripts = [];
  let removeResults = "rm -rf results.txt";
  let removeConfig = "rm -rf configurations/configure.ts";
  let removeSubgraphYaml = "rm -rf subgraph.yaml";
  let prepareYaml =
    "npm run prepare:yaml --PROTOCOL=" +
    protocol +
    " --NETWORK=" +
    network +
    " --TEMPLATE=" +
    template;
  let prepareConstants =
    "npm run prepare:constants --PROTOCOL=" +
    protocol +
    " --NETWORK=" +
    network;
  let codegen = "graph codegen";
  let deployment = "npm run deploy:subgraph --LOCATION=" + location;

  scripts.push(removeResults);
  scripts.push(removeConfig);
  scripts.push(removeSubgraphYaml);
  scripts.push(prepareYaml);
  if (constants == true) {
    scripts.push(prepareConstants);
  }
  scripts.push(codegen);
  scripts.push(deployment);

  return scripts;
}

function getDeploymentNetwork(network) {
  let deployNetwork = "";
  switch (network) {
    case "mainnet":
      deployNetwork = "ethereum";
      break;
    case "xdai":
      deployNetwork = "gnosis";
      break;
    case "matic":
      deployNetwork = "polygon";
      break;
    default:
      deployNetwork = network;
  }
  return deployNetwork;
}

/**
 * @param {string[]} array - Protocol that is being deployed
 * @param {string} callback
 */
async function runCommands(allScripts, results, args, callback) {
  let logs = "";
  var deploymentIndex = 0;
  var scriptIndex = 0;
  var httpCounter = 1;
  let allDeployments = Array.from(allScripts.keys());

  function next() {
    if (deploymentIndex < allDeployments.length) {
      exec(
        allScripts.get(allDeployments[deploymentIndex])[scriptIndex++],
        function (error, stdout, stderr) {
          logs = logs + "stdout: " + stdout;
          logs = logs + "stderr: " + stderr;
          if (stderr.includes("HTTP error")) {
            if (httpCounter >= 2) {
              deploymentIndex++;
              scriptIndex = 0;
            }
            httpCounter++;
          }
          if (error !== null) {
            if (
              stderr.includes("HTTP error deploying the subgraph") &&
              httpCounter <= 3
            ) {
              httpCounter++;
              console.log(
                "HTTP error on deployment " +
                  httpCounter.toString() +
                  " for " +
                  allDeployments[deploymentIndex] +
                  ". Trying Again..."
              );
            } else {
              logs = logs + "Exec error: " + error;
              results +=
                "Deployment Failed: " + allDeployments[deploymentIndex] + "\n";
              console.log(error);
              deploymentIndex++;
              scriptIndex = 0;
              httpCounter = 1;
            }
          } else if (
            scriptIndex ==
            allScripts.get(allDeployments[deploymentIndex]).length
          ) {
            results +=
              "Deployment Successful: " + allDeployments[deploymentIndex] + "\n";
            deploymentIndex++;
            scriptIndex = 0;
            httpCounter = 1;
          }

          // do the next iteration
          next();
        }
      );
    } else {
      // all done here
      fs.writeFile(
        "results.txt",
        logs.replace(/\u001b[^m]*?m/g, ""),
        function (err) {
          if (err) throw err;
        }
      );

      // Print the logs if printlogs is 't' or 'true'
      if (["true", "t"].includes(args.printlogs.toLowerCase())) {
        console.log(logs);
      }
      console.log("\n" + results + "END" + "\n\n");
      callback(results);
    }
  }

  // start the first iteration
  next();
}

module.exports = { scripts, getDeploymentNetwork, runCommands };

'''
'''--- mscutil/src/index.js ---
const path = require('path')
//const yargs = require('yargs')
const { exec } = require('child_process');
const fs = require ('fs');
const fse = require('fs-extra');
const yaml = require('js-yaml');
const directoryTree = require('directory-tree');
    
var cmds = [
    "price-oracle",
    "abis",
    "common",
    "mappings",
    "ethereum-abis",
    "subgraph-manifest",
    "schema",
    "package-cfg",
];
var appRoot = process.env.PWD;
var modRoot = path.dirname(__dirname);
var refRoot = modRoot+"/_reference_";
var sgRefRoot = path.dirname(path.dirname(__dirname))+'/subgraphs/_reference_';

const run = async argv => {
    var args = process.argv.slice(2);
    var cmd = args.shift();
    switch (cmd) {
      case 'install':
        install(args.shift());
        break;
      case 'remove':
        remove(args.shift());	
        break;
      case 'install-all':
        cmds.forEach(element => {
          install(element)
        })
        break;
      case 'remove-all':
        cmds.forEach(element => {
          remove(element)
        })
        break;
      case 'update':
        // Everything
        cmds.forEach(element => {
          remove(element);
          install(element)
        })
        break;
      case 'debug':
        console.log('appRoot: '+appRoot)
        console.log('modRoot: '+modRoot)
        console.log('refRoot: '+refRoot)
        console.log('sgRefRoot: '+sgRefRoot)
        break;
      default:
        dispUsage();
        break;
    }
}

function dispUsage() {
        console.error("Usage: mscutil [install | remove] ["+cmds.join(" | ")+"]\n       mscutil [install-all | remove-all]")
}

function symlink(src,dest) {
  var res = fs.symlink(src, dest, function (err) {
    if(err) {
      console.error(err);
      return false;
    } else {
      console.log("Linking "+src+" "+dest)
      return true;
    }
  });
  return res;
}

function installDefaultManifest() {
  fse.copySync(
    refRoot+"/subgraph.yaml",
    appRoot+"/subgraph.yaml",
  )
}

function installDefaultSchema() {
  fse.copySync(
    sgRefRoot+"/schema.graphql",
    appRoot+"/schema.graphql",
  )
}

function installDefaultPackage() {
  fse.copySync(
    refRoot+"/package.json",
    appRoot+"/package.json",
  )
  // Consider running installation automatically.
}

function install(submod) {    
  switch (submod) {
    case 'subgraph-manifest':
      try {
        fs.readFileSync(appRoot+'/subgraph.yaml')
      } catch (e) {
        installDefaultManifest();
      }
      break;
    case 'price-oracle':
      // ./abis/Prices/ - Create directory if nonexistent
      if (!fse.existsSync(appRoot+"/abis/")) {
        fse.mkdirSync(appRoot+"/abis/");
      }
      symlink(sgRefRoot+"/abis/Prices/",appRoot+"/abis/Prices");
      // ./src/prices/ - Create directory if nonexistent
      if (!fs.existsSync(appRoot+"/src/")) {
        fs.mkdirSync(appRoot+"/src/");
      }
      symlink(sgRefRoot+"/src/prices/",appRoot+"/src/prices");
      // ./subgraph.yaml insertion
      j = JSON.parse(fs.readFileSync(refRoot+'/config/priceOracleABIs.json'))
      abis = j.priceOracleABIs
      jy = yaml.load(abis);

      try {
        y = yaml.load(fs.readFileSync(appRoot+'/subgraph.yaml'))
      } catch (e) {
        y = null;
      }
      if(y) {
        jy.forEach(item => function(){
          y.dataSources.forEach(source => function() {
            sma = source.mapping.abis;
            var match = sma.filter(function(val) { return val.file === item.file});
            if(!match.length) {
              sma.push(item)
            }
          }());
        }())
        var output = yaml.dump(y);
        fs.closeSync(fs.openSync(appRoot+'/subgraph.yaml', "w"));
        fs.writeFileSync(appRoot+'/subgraph.yaml',output,function(err) { console.error(err); });
      } else {
        //Use the default.
        installDefaultManifest();
      }
      break;
    case 'abis':
      // ./abis/*.json
      if (!fs.existsSync(appRoot+"/abis/")) {
        fs.mkdirSync(appRoot+"/abis/");
      }
      var tree = directoryTree(sgRefRoot+'/abis',{extensions:/\.json/});
      tree.children.forEach(file => function() {
        if(file.children) return
        symlink(file.path,appRoot+'/abis/'+file.name);
      }());
      break;
    case 'common':
      if (!fs.existsSync(appRoot+"/src/")) {
        fs.mkdirSync(appRoot+"/src/");
      }
      // ./src/common/
      symlink(sgRefRoot+"/src/common/",appRoot+"/src/common");
      break;
    case 'mappings':
      // ./src/common/mappings
      if (!fs.existsSync(appRoot+"/src/")) {
        fs.mkdirSync(appRoot+"/src/");
      }
      symlink(sgRefRoot+"/src/mappings",appRoot+"/src/mappings");
      break;
    case 'ethereum-abis':
      if (!fse.existsSync(appRoot+"/abis/")) {
        fse.mkdirSync(appRoot+"/abis/");
      }
      symlink(refRoot+"/abis/Ethereum/",appRoot+"/abis/Ethereum");
      break;
    case 'schema':
      try {
        y = yaml.load(fs.readFileSync(appRoot+'/schema.graphql'))
      } catch (e) {
        y = null;
      }
      if(y) {
        // Ostensibly already exists.
      } else {
        //Use the default.
        installDefaultSchema();
      }
      break;
    case 'package-cfg':
      try {
        y = yaml.load(fs.readFileSync(appRoot+'/package.json'))
      } catch (e) {
        y = null;
      }
      if(y) {
        // Ostensibly already exists.
      } else {
        //Use the default.
        installDefaultPackage();
      }
      break;
    default:
      dispUsage();
      break;
  }
}

function remove(submod) {
  // Always ask for a confirmation before proceeding.  Later, check git.
  switch (submod) {
    //Note: Removal is non-recursive, or otherwise uses _reference_ specific paths.
    //Confirmation not needed, but unique pathing strongly recommended.
      case 'price-oracle':
        // ./abis/Prices/
        fse.removeSync(appRoot+"/abis/Prices");
        // ./src/prices/
        fse.removeSync(appRoot+"/src/prices");
        // ./subgraph.yaml insertion
        j = JSON.parse(fs.readFileSync(refRoot+'/config/priceOracleABIs.json'))
        abis = j.priceOracleABIs
        jy = yaml.load(abis);
        
        try {
          y = yaml.load(fs.readFileSync(appRoot+'/subgraph.yaml'))
        } catch (e) {
          y = null;
        }
        if(y) {
          jy.forEach(item => function(){
            y.dataSources.forEach(source => function() {
              sma = source.mapping.abis;
              sma = sma.filter(function(val) { return val.file !== item.file});
              source.mapping.abis = sma;
            }());
          }())
          var output = yaml.dump(y);
          fs.closeSync(fs.openSync(appRoot+'/subgraph.yaml', "w"));
          fs.writeFileSync(appRoot+'/subgraph.yaml',output,function(err) { console.error(err); });
        } else {
          //???
        }
        break;
      case 'abis':
        // ./abis/*.json
        var tree = directoryTree(sgRefRoot+'/abis',{extensions:/\.json/});
        tree.children.forEach(file => function() {
          if(file.children) return;
          fse.unlinkSync(appRoot+'/abis/'+file.name);
        }());
        break;
      case 'common':
        // ./src/common/
        fse.removeSync(appRoot+"/src/common");
        break;
      case 'mappings':
        // ./src/mappings
        fse.removeSync(appRoot+"/src/mappings");
        break;
      case 'ethereum-abis':
        fse.removeSync(appRoot+"/abis/Ethereum");
        break;
      case 'schema':
        fse.removeSync(appRoot+"/schema.graphql");
        break;
      case 'package-cfg':
        fse.removeSync(appRoot+"/package.json");
        break;
      default:
        dispUsage();
        break;
    }
}

module.exports = { run }

'''
'''--- subgraphs/beefy-finance/setup/scripts/getStartBlock.js ---
const fs = require("fs");
const ethers = require("ethers");
const { vaults } = require("../vaults");
const { providers } = require("./publicRpcs");
const EthDater = require("ethereum-block-by-date");
const moment = require("moment");
require("dotenv").config();

async function getStartBlock(timestamp, network) {
  const provider = new ethers.providers.JsonRpcProvider(providers[0][network]);
  const dater = new EthDater(provider);
  return (await dater.getDate(moment.unix(timestamp).format(), false, false))
    .block;
}

async function writeStartBlock() {
  let startBlock;
  let string = "const vaults = [";
  for (let i = 0; i < vaults.length; i++) {
    if (vaults[i].startBlock > 1) {
      startBlock = vaults[i].startBlock;
    } else {
      console.log(
        "Getting start block for vault " +
          vaults[i].id +
          " on network " +
          vaults[i].chain
      );
      try {
        startBlock = await getStartBlock(vaults[i].createdAt, vaults[i].chain);
      } catch (error) {
        try {
          startBlock = await getStartBlock(
            vaults[i].createdAt,
            vaults[i].chain
          );
        } catch (error) {
          console.log(error);
          startBlock = 1;
        }
      }
      console.log(
        "Start block found " +
          startBlock +
          " current progress: " +
          (i + 1) +
          "/" +
          vaults.length
      );
    }
    vaults[i].startBlock = startBlock;
    string = string.concat(JSON.stringify(vaults[i]) + ",");
  }
  console.log("Writing start blocks to file...");
  fs.writeFileSync(
    `./setup/vaults.js`,
    string + "]\nmodule.exports = {vaults}"
  );
}

writeStartBlock();

'''
'''--- subgraphs/beefy-finance/setup/scripts/publicRpcs.js ---
const providers = [
  {
    bsc: "https://rpc.ankr.com/bsc",
    avalanche: "https://ava-mainnet.public.blastapi.io/ext/bc/C/rpc	",
    heco: "https://http-mainnet.hecochain.com",
    matic: "https://matic-mainnet-full-rpc.bwarelabs.com",
    fantom: "https://rpc.ankr.com/fantom",
    harmony: "https://api.harmony.one",
    arbitrum: "https://arb1.arbitrum.io/rpc	",
    celo: "https://forno.celo.org	",
    moonriver: "https://moonriver.public.blastapi.io",
    cronos: "https://evm.cronos.org	",
    fuse: "https://rpc.fuse.io",
    metis: "https://andromeda.metis.io/?owner=1088",
    aurora: "https://mainnet.aurora.dev",
    moonbeam: "https://rpc.ankr.com/moonbeam",
    oasis: "https://emerald.oasis.dev",
  },
];

module.exports = { providers };

'''
'''--- subgraphs/beefy-finance/setup/setup.js ---
const writeYamlFile = require("write-yaml-file");
const { vaults } = require("./vaults");
require("dotenv").config();

let monitoredVaults = [];

const chains = [
  "bsc",
  "avalanche",
  "heco", //not supported yet by theGraph
  "polygon",
  "fantom",
  "one", //not supported yet by theGraph
  "arbitrum-one",
  "celo",
  "moonriver",
  "cronos", //not supported yet by theGraph
  "fuse",
  "metis", //not supported yet by theGraph
  "aurora",
  "moonbeam", //not supported yet by theGraph
  "oasis", //not supported yet by theGraph
];

// STEP 1: Build the subgraph.yaml file
function createDataSource(contractName, contractAddress, network, startBlock) {
  let dataSource = {
    kind: "ethereum/contract",
    name: contractName,
    network: network,
    source: {
      address: contractAddress,
      abi: "BeefyStrategy",
      startBlock: startBlock,
    },
    mapping: {
      kind: "ethereum/events",
      apiVersion: "0.0.6",
      language: "wasm/assemblyscript",
      entities: [
        "Vault",
        "Token",
        "Deposit",
        "Withdraw",
        "YieldAggregator",
        "VaultDailySnapshot",
        "VaultHourlySnapshot",
        "VaultFee",
      ],
      abis: [
        {
          name: "BeefyStrategy",
          file: "./abis/BeefyStrategy.json",
        },
        {
          name: "BeefyVault",
          file: "./abis/BeefyVault.json",
        },
        {
          name: "ChainlinkOracle",
          file: "./abis/Chainlink.json",
        },
        {
          name: "ERC20",
          file: "./abis/ERC20.json",
        },
        {
          name: "CurveRegistry",
          file: "./abis/Prices/Curve/Registry.json",
        },
        {
          name: "CurvePoolRegistry",
          file: "./abis/Prices/Curve/PoolRegistry.json",
        },
        {
          name: "CalculationsCurve",
          file: "./abis/Prices/Calculations/Curve.json",
        },
        {
          name: "YearnLensContract",
          file: "./abis/Prices/YearnLens.json",
        },
        {
          name: "ChainLinkContract",
          file: "./abis/Prices/ChainLink.json",
        },
        {
          name: "UniswapRouter",
          file: "./abis/Prices/Uniswap/Router.json",
        },
        {
          name: "UniswapFeeRouter",
          file: "./abis/Prices/Uniswap/FeeRouter.json",
        },
        {
          name: "UniswapFactory",
          file: "./abis/Prices/Uniswap/Factory.json",
        },
        {
          name: "UniswapPair",
          file: "./abis/Prices/Uniswap/Pair.json",
        },
        {
          name: "SushiSwapRouter",
          file: "./abis/Prices/SushiSwap/Router.json",
        },
        {
          name: "SushiSwapFactory",
          file: "./abis/Prices/SushiSwap/Factory.json",
        },
        {
          name: "SushiSwapPair",
          file: "./abis/Prices/SushiSwap/Pair.json",
        },
        {
          name: "CalculationsSushiSwap",
          file: "./abis/Prices/Calculations/SushiSwap.json",
        },
      ],
      eventHandlers: [
        {
          event: "Deposit(uint256)",
          handler: "handleDeposit",
        },
        {
          event: "Withdraw(uint256)",
          handler: "handleWithdraw",
        },
        {
          event: "StratHarvest(indexed address,uint256,uint256)",
          handler: "handleStratHarvestWithAmount",
        },
        {
          event: "StratHarvest(indexed address)",
          handler: "handleStratHarvest",
        },
        {
          event: "StratHarvest(indexed address,indexed uint256)",
          handler: "handleStratHarvest",
        },
        {
          event: "ChargedFees(uint256,uint256,uint256)",
          handler: "handleChargedFees",
        },
      ],
      file: "./src/mappings/vault.ts",
    },
  };

  return dataSource;
}

function bootstrap(network) {
  //setup provider to get contract addresses
  let subgraphYamlDoc = {
    specVersion: "0.0.5",
    schema: {
      file: "./schema.graphql",
    },
    dataSources: [],
  };
  //loop through all the pools and get the strategy addresses
  let strategyAddress, vaultName, startBlock;
  for (let i = 0; i < vaults.length; i++) {
    vaultNetwork = vaults[i].chain;
    if (vaultNetwork === network) {
      strategyAddress = vaults[i].strategy;
      vaultName = vaults[i].id;
      startBlock = vaults[i].startBlock;

      // Add the datasource
      subgraphYamlDoc["dataSources"].push(
        createDataSource(vaultName, strategyAddress, network, startBlock)
      );

      //add the strategy address to the list of monitored contracts
      monitoredVaults.push(vaultName);
    }
  }

  writeYamlFile("subgraph.yaml", subgraphYamlDoc).then(() => {});

  console.log(
    `
    Bootstrap done !
    Your subgraph will be indexing all vaults at address:
        MONITORED_VAULTS
    You can now deploy your subgraph and start indexing :)
    Run: 
        yarn deploy yourgithubusername/yourrepositoryname
`.replace("MONITORED_VAULTS", monitoredVaults.join(", "))
  );
}

bootstrap(process.argv.slice(2)[0]);

'''
'''--- subgraphs/beefy-finance/src/prices/oracles/ChainLinkAddresses.js ---
const { constants } = require("ethers");

const polygonChainLinkOracles = [
  "0x443c5116cdf663eb387e72c688d276e702135c87",
  "0x72484b12719e23115761d5da1646945632979bb6",
  "0x882554df528115a743c4537828da8d5b58e52544",
  "0x3fd911749fce21a38704b76ffabcb6bef2567f2e",
  "0x9b88d07b2354ef5f4579690356818e07371c7bed",
  "0x5db6e61b6159b20f068dc15a47df2e5931b14f29",
  "0x03bc6d9efed65708d35fdaefb25e87631a0a3437",
  "0x289833f252eab98582d62db94bd75ab48ad9cf0d",
  "0xf9184b8e5da48c19fa4e06f83f77742e748cca96",
  "0x2ac3f3bfac8fc9094bc3f0f9041a51375235b992",
  "0x062df9c4efd2030e243ffcc398b652e8b8f95c6f",
  "0xe01ea2fbd8d76ee323fbed03eb9a8625ec981a10",
  "0x9c371ae34509590e10ab98205d2df5936a1ad875",
  "0x82c9d4e88862f194c2bd874a106a90ddd0d35aab",
  "0xf626964ba5e81405f47e8004f0b276bb974742b5",
  "0x03cd157746c61f44597dd54c6f6702105258c722",
  "0xd106b538f2a868c28ca1ec7e298c3325e0251d66",
  "0x2346ce62bd732c62618944e51cbfa09d985d86d2",
  "0x327d9822e9932996f55b39f557aec838313da8b7",
  "0xc5c770ae2efdf0dbc2fb366fb3833dac2a20bf2f",
  "0x82a6c4af830caa6c97bb504425f6a66165c2c26e",
  "0xf5724884b6e99257cc003375e6b844bc776183f9",
  "0x58527c2dcc755297bb81f9334b80b2b6032d8524",
  "0x8803dd6705f0d38e79790b02a2c43594a0538d22",
  "0x19b0f0833c78c0848109e3842d34d2fdf2ca69ba",
  "0xc907e116054ad103354f2d350fd2514433d57f6f",
  "0x18e4058491c3f58bc2f747a9e64ca256ed6b318d",
  "0x2f2c605f28de314bc579a7c0fdf85536529e9825",
  "0xe0dc07d5ed74741ceeda61284ee56a2a0f7a4cc9",
  "0xaca44abb8b04d07d883202f99fa5e3c53ed57fb5",
  "0xc9ecf45956f576681bdc01f79602a79bc2667b0c",
  "0xc76f762cedf0f78a439727861628e0fdfe1e70c2",
  "0x2409987e514ad8b0973c2b90ee1d95051df0ecb9",
  "0xf238a5fb3a15ab4b063b3894fab30442620b70b9",
  "0x04bb437aa63e098236fa47365f0268547f6eab32",
  "0x2a8758b7257102461bc958279054e372c2b1bde6",
  "0xde6302dfa0ac45b2b1b1a23304469da630b2f59b",
  "0x1cf68c76803c9a415be301f50e82e44c64b7f1d4",
  "0x336584c8e6dc19637a5b36206b1c79923111b405",
  "0xe039d4aa72a0c0d6d0218e650c1ebd6b2675a575",
  "0x5ec151834040b4d453a1ea46aa634c1773b36084",
  "0x6a03b6f7a833a8305a532e7f4fc161f470910058",
  "0x99ca59cab16767f40afe385bdd423a309f6ac2c2",
  "0xb3dbea967a5411b8abfb79e4e9d2138b7a7de2e7",
  "0xb4d962106206d88372c542c8ffecacaefb728a60",
  "0xb527769842f997d56dd1ff73c34192141b69077e",
  "0xfc539a559e170f848323e19dfd66007520510085",
  "0x4746dec9e833a82ec7c2c1356372ccf2cfcd2f3d",
  "0xd94427edee70e4991b4b8ddcc848f2b58ed01c0b",
  "0x4205ec5fd179a843caa7b0860a8ec7d980013359",
  "0x59161117086a4c7a9beda16c66e40bdaa1c5a8b6",
  "0xbaf9327b6564454f4a3364c33efeef032b4b4444",
  "0xbd238a35fb47ae22f0cc551f14ffb8e8f04fca21",
  "0xacb51f1a83922632ca02b25a8164c10748001bde",
  "0xc70aaf9092de3a4e5000956e672cdf5e996b4610",
  "0x9be99a22be3a702172da3980aa760365ad8cd7fd",
  "0x392acaa165a882dfc63d3aeb4c446b95fa7013b0",
  "0x440a341bbc9fa86aa60a195e2409a547e48d4c0c",
  "0xd6285f06203d938ab713fa6a315e7d23247dde95",
  "0xdf3f72be10d194b58b1bb56f2c4183e661cb2114",
  "0xf9680d99d6c9589e2a93a78a04a279e509205945",
  "0x67935f65d1577ced9f4929d3679a157e95c1c02c",
  "0x73366fe0aa0ded304479862808e02506fe556a98",
  "0xe7ef3246654ac0fd0e22fc30dce40466cfdf597c",
  "0xdfb138ba3a6cce675a6f5961323be31ee42e40ff",
  "0x5b4586c911144a947d7814fd71fe0872b8334748",
  "0x18617d05ee1692ad7eafee9839459da16097afd8",
  "0x00dbeb1e45485d53df7c2f0df1aa0b6dc30311d3",
  "0x58326c0f831b2dbf7234a4204f28bba79aa06d5f",
  "0x817a7d43f0277ca480ae03ec76fc63a2ec7114ba",
  "0x6c0fe985d3cacbcde428b84fc9431792694d0f51",
  "0x099a2540848573e94fb1ca0fa420b00acbbc845a",
  "0xe638249af9642cda55a92245525268482ee4c67b",
  "0xdd229ce42f11d8ee7fff29bdb71c7b81352e11be",
  "0x432fa0899cf1bcdb98592d7eaa23c372b8b8ddf2",
  "0x1b32682c033b2dd7efdc615fa82d353e254f39b5",
  "0x3fabbfb300b1e2d7c9b84512fe9d30aedf24c410",
  "0xc5878bdf8a89fa3bf0dc8389ae8ee6de601d01bc",
  "0x82d43b72573f902f960126a19581bcbba5b014f5",
  "0x6f8f9e75c0285aece30adfe1bcc1955f145d971a",
  "0x84227a76a04289473057bef706646199d7c58c34",
  "0x80a5cb83ce268ed11a6efc4bbf0bec39df35db21",
  "0x8d5eb34c509261533235b91350d359edcb969d33",
  "0x0052f461a5dc0feef70cabc312c9acb1e4afc0a2",
  "0xd647a6fc9bc6402301583c91decc5989d8bc382d",
  "0x5438e60a06c7447432512264fa57e2fed3224b33",
  "0x86f87cb74238a6f24606534a2fcc05469eb2bcf5",
  "0x10e5f3dfc81b3e5ef4e648c4454d04e79e1e41e2",
  "0x24b820870f726da9b0d83b0b28a93885061dbf50",
  "0x90711d545915f8e99a22bb1f86eb8c0403e3358f",
  "0xb77fa460604b9c6435a235d057f7d319ac83cb53",
  "0x5787befdc0ecd210dfa948264631cd53e68f7802",
  "0xd9ffdb71ebe7496cc440152d43986aae0ab76665",
  "0xbaaf11ceda1d1ca9cf01748f8196653c9656a400",
  "0xeb99f173cf7d9a6dc4d889c2ad7103e8383b6efa",
  "0xa1cbf3fe43bc3501e3fc4b573e822c70e76a7512",
  "0x327e23a4855b6f663a28c5161541d69af8973302",
  "0xab594600376ec9fd91f8e885dadf0ce036862de0",
  "0x6e53c1c22427258be55ae985a65c0c87bb631f9c",
  "0xd133f916e04ed5d67b231183d85be12eaa018320",
  "0xd8d483d813547cfb624b8dc33a00f2fcbcd2d428",
  "0x7d620d05c317a426903596432a5ca83375dc8d2a",
  "0x807b59d12520830d1864286fa0271c27baa94197",
  "0xa070427bf5ba5709f70e98b94cb2f435a242c46c",
  "0xb89d583b72abf9c3a7e6e093251c2fcad3365312",
  "0xc43081d9ea6d1c53f1f0e525504d47dd60de12da",
  "0x171b16562ea3476f5c61d1b8dad031dba0768545",
  "0xee47cf6bf2e58b276b565f73d5c6de5d405a33f5",
  "0x74b3587a23ee786a43c8529c2e98d3c05a8fb1fb",
  "0x666bb13b3ed3816504e8c30d0f9b9c16b371774b",
  "0x0df812c4d675d155815b1216ce1da9e68f1b7050",
  "0x1342a7a1d7264daf8ae790712266c7be19f71211",
  "0xa302a0b8a499fd0f00449df0a490dede21105955",
  "0xdcda79097c44353dee65684328793695bd34a629",
  "0xa8b05b6337040c0529919bdb51f6b40a684eb08c",
  "0xc08f70c26ab8c659eaf259c51a0f7ae22758c6ac",
  "0x4ce90f28c6357a7d3f47d680723d18af3684cd00",
  "0x93ffee768f74208a7b9f2a4426f0f6bcbb1d09de",
  "0x0e12b79a6e5c919f89246ededb2d6413a8890a54",
  "0x56d55d34ecc616e71ae998accba79f236ff2ff46",
  "0x0f6914d8e7e1214cdb3a4c6fbf729b75c69df608",
  "0x218231089bebb2a31970c3b77e96ecfb3ba006d1",
  "0xd3963855b73979b617455fc38a7355563a289948",
  "0x24c0e0fc8ccb21e2fb3e1a8a4ec4b29458664f79",
  "0xb34bce11040702f71c11529d00179b2959bce6c0",
  "0xc741f7752bae936fce97933b755884af66fb69c1",
  "0xdcc714619e59a626fde5f082d42f314e9fb832fb",
  "0x836a579b39d22b2147c1c229920d27880c915578",
  "0xa058689f4bca95208bba3f265674ae95ded75b6d",
  "0x7f45273fd7c644714825345670414ea649b50b16",
  "0x634b084372f88848ac8f8006dc178aa810a58e89",
  "0x3fcef3edf17f515d9c0fa72020fcfc6c0001f876",
  "0x2e5b04adc0a3b7db5fd34ae817c7d0993315a8a6",
  "0x3d49406edd4d52fb7ffd25485f32e073b529c924",
  "0x5047cdcf17aa5a0bb77217497142657b27a1e228",
  "0xcc73e00db7a6fd589a30bbe2e957086b8d7d3331",
  "0xbd92b4919ae82be8473859295def0e778a626302",
  "0x8ce3cac0e6635ce04783709ca3cc4f5fc5304299",
  "0x3710abeb1a0fc7c2ec59c26c8daa7a448ff6125a",
  "0xbb3ef70953fc3766bec4ab7a9bf05b6e4caf89c6",
  "0xbf90a5d9b6ee9019028dbfc2a9e50056d5252894",
  "0x10c8264c0935b3b9870013e057f330ff3e9c56dc",
  "0xd8f8a7a38a1ac326312000d0a0218bf3216bfabb",
  "0x17414eb5159a082e8d41d243c1601c2944401431",
  "0x49b0c695039243bbfeb8ecd054eb70061fd54aa0",
  "0x80fdfcd7b923d4332dc053bf2e23e3e36d12bba4",
  "0x38611b09f8f2d520c14ea973765c225bf57b9eac",
  "0x9bce696fb0dce1ed4ddb94305757dedc745f3786",
  "0x346c7d75e315b54129eac38cc4e2b9f9b0250e3e",
  "0x307ccf7cbd17b69a487b9c3dbe483931cf3e1833",
  "0xd78325dca0f90f0ffe53ccea1b02bb12e1bf8fdb",
  "0x567e67f456c7453c583b6efa6f18452cdee1f5a8",
  "0x7c5d415b64312d38c56b54358449d0a4058339d2",
  "0xe6d13ef6fb49230791c0f21927f091f2b8e2c566",
  "0xbb9749b5ad68574c106ac4f9cd5e1c400dbb88c3",
  "0x33d9b1baadcf4b26ab6f8e83e9cb8a611b2b3956",
  "0x162d8c5bf15eb6bee003a1ffc4049c92114bc931",
  "0xdf0fb4e4f928d2dcb76f438575fdd8682386e13c",
  "0xefb7e6be8356ccc6827799b6a7348ee674a80eae",
  "0xfe4a8cc5b5b2366c1b58bea3858e81843581b2f7",
  "0xf9d5aac6e5572aefa6bd64108ff86a222f69b64d",
  "0x0a6513e40db6eb1b165753ad52e80663aea50545",
  "0xd78bc11ef3256e3ce9dc0df0fa7be9e9afc07f95",
  "0x0cf1d8c6651f4188e55fce6ab25261948108f197",
  "0xa338e0492b2f944e9f8c0653d3ad1484f2657a37",
  "0xde31f8bfbd8c84b5360cfacca3539b938dd78ae6",
  "0x6a99ec84819fb7007dd5d032068742604e755c56",
  "0x461c7b8d370a240ddb46b402748381c3210136b3",
  "0x0c466540b2ee1a31b441671eac0ca886e051e410",
  "0x692ae5510ca9070095a496dbcfbcda99d4024cd9",
  "0xbe6fb0ab6302b693368d0e9001faf77ecc6571db",
  "0x34108d18dcb16cfe4ea8930a5fec16deb7b4e383",
  "0x785ba89291f676b5386652eb12b30cf361020694",
  "0xc16cb62cdde46f43fd73257b957bf527f07b51c0",
  "0x691e26ab58ff05800e028b0876a41b720b26fc65",
  "0x9896a1ea7a00f5f32ab131ebbee07487b0af31d0",
  "0x9d3a43c111e7b2c6601705d9fcf7a70c95b1dc55",
  "0xd4a120c26d57b910c56c910cdd13eebfa3135502",
  "0xbc08c639e579a391c4228f20d0c29d0690092df0",
  "0x6ea4d89474d9410939d429b786208c74853a5b47",
];

exports.constants = { polygonChainLinkOracles };

'''
'''--- subgraphs/beefy-finance/src/prices/oracles/scripts/asset_finder.js ---
const oracles = require("../ChainLinkAddresses");
const ethers = require("ethers");
const chainLinkAbi = require("../../../../abis/Chainlink.json");
const fs = require("fs");
require("dotenv").config();

async function findAssets() {
  const provider = new ethers.providers.JsonRpcProvider(
    process.env.ALCHEMY_POLYGON
  );
  const signer = new ethers.Wallet(
    process.env.TEST_PRIVATE_KEY || "",
    provider
  );

  let contract, asset, denominator;
  let oraclesJson = { polygon: [] };

  for (let i = 0; i < oracles.constants.polygonChainLinkOracles.length; i++) {
    contract = new ethers.Contract(
      oracles.constants.polygonChainLinkOracles[i],
      chainLinkAbi,
      signer
    );
    [asset, , denominator] = (await contract.description()).split(" ");
    if (denominator === "USD") {
      oraclesJson.polygon.push(
        oracleTemplate(
          oracles.constants.polygonChainLinkOracles[i].toString(),
          asset
        )
      );
      console.log("saving data for " + asset + " oracle");
    }
  }
  fs.writeFileSync(
    "./src/prices/oracles/oracles.json",
    JSON.stringify(oraclesJson)
  );
}

function oracleTemplate(address, asset) {
  template = [asset, address];
  return template;
}

findAssets();

'''