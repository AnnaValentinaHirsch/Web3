*GitHub Repository "near/abomonation"*

'''--- .github/ISSUE_TEMPLATE/BOUNTY.yml ---
name: "Simple Bounty"
description: "Use this template to create a HEROES Simple Bounty via Github bot"
title: "Bounty: "
labels: ["bounty"]
assignees: heroes-bot-test
body:
  - type: markdown
    attributes:
      value: |
        Hi! Let's set up your bounty! Please don't change the template - @heroes-bot-test won't be able to help you.

  - type: dropdown
    id: type
    attributes:
      label: What talent are you looking for?
      options:
        - Marketing
        - Development
        - Design
        - Other
        - Content
        - Research
        - Audit

  - type: textarea
    id: description
    attributes:
      label: What you need to be done?

  - type: dropdown
    id: tags
    attributes:
      label: Tags
      description: Add tags that match the topic of the work
      multiple: true
      options:
        - API
        - Blockchain
        - Community
        - CSS
        - DAO
        - dApp
        - DeFi
        - Design
        - Documentation
        - HTML
        - Javascript
        - NFT
        - React
        - Rust
        - Smart contract
        - Typescript
        - UI/UX
        - web3
        - Translation
        - Illustration
        - Branding
        - Copywriting
        - Blogging
        - Editing
        - Video Creation
        - Social Media
        - Graphic Design
        - Transcription
        - Product Design
        - Artificial Intelligence
        - Quality Assurance
        - Risk Assessment
        - Security Audit
        - Bug Bounty
        - Code Review
        - Blockchain Security
        - Smart Contract Testing
        - Penetration Testing
        - Vulnerability Assessment
        - BOS
        - News
        - Hackathon
        - NEARCON2023
        - NEARWEEK

  - type: input
    id: deadline
    attributes:
      label: Deadline
      description: "Set a deadline for your bounty. Please enter the date in format: DD.MM.YYYY"
      placeholder: "19.05.2027"

  - type: dropdown
    id: currencyType
    attributes:
      label: Currency
      description: What is the currency you want to pay?
      options:
        - USDC.e
        - USDT.e
        - DAI
        - wNEAR
        - USDt
        - XP
        - marmaj
        - NEKO
        - JUMP
        - USDC
        - NEARVIDIA
      default: 0
    validations:
      required: true

  - type: input
    id: currencyAmount
    attributes:
      label: Amount
      description: How much it will be cost?

  - type: markdown
    attributes:
      value: "## Advanced settings"

  - type: checkboxes
    id: kyc
    attributes:
      label: KYC
      description: "Use HEROES' KYC Verification, only applicants who passed HEROES' KYC can apply and work on this bounty!"
      options:
        - label: Use KYC Verification

  - type: markdown
    attributes:
      value: |
        ### This cannot be changed once the bounty is live!

'''
'''--- .travis.yml ---
language: rust
sudo: required
rust:
- stable
before_script:
- pip install 'travis-cargo<0.2' --user && export PATH=$HOME/.local/bin:$PATH
script:
- |
  travis-cargo build &&
  travis-cargo test &&
  travis-cargo bench &&
  travis-cargo doc
after_success:
- travis-cargo --only stable doc-upload
env:
  global:
    secure: kYa+xSF8ladJiN/6fK9qEdFtvmeWsvlBdRFXqeFdPFTvkNhKqMg9SxSlZfhhQw03x20uNDbo1o/wWEWWjr99lc/drVCdD6phbfvus7DRmmPEXqr3SLfnNrFVkfYUv3WkqoVIcmtxyqw12eJE7KKxLCZkXeImamFDyMSOb5ceNFo=

'''
'''--- Cargo.toml ---
[package]
name = "abomonation"
version = "0.7.3"
authors = ["Frank McSherry <fmcsherry@me.com>"]

description = "A high performance and very unsafe serialization library"

# These URLs point to more information about the repository
documentation = "https://docs.rs/abomonation/"
homepage = "https://github.com/TimelyDataflow/abomonation"
repository = "https://github.com/TimelyDataflow/abomonation.git"
keywords = ["abomonation"]
license = "MIT"

[dependencies]
cranelift-entity = {version = "0.68", optional = true}
indexmap = { version = "1.4", optional = true}

[dev-dependencies]
recycler="0.1.4"

[features]
default = ["wasmer"]
wasmer = ["cranelift-entity", "indexmap"]
'''
'''--- README.md ---
# Abomonation
A mortifying serialization library for Rust

Abomonation (spelling intentional) is a serialization library for Rust based on the very simple idea that if someone presents data for serialization it will copy those exact bits, and then follow any pointers and copy those bits, and so on. When deserializing it recovers the exact bits, and then corrects pointers to aim at the serialized forms of the chased data.

**Warning**: Abomonation should not be used on any data you care strongly about, or from any computer you value the data on. The `encode` and `decode` methods do things that may be undefined behavior, and you shouldn't stand for that. Specifically, `encode` exposes padding bytes to `memcpy`, and `decode` doesn't much respect alignment.

Please consult the [abomonation documentation](https://frankmcsherry.github.com/abomonation) for more specific information.

Here is an example of using Abomonation. It is very easy to use. Frighteningly easy.

```rust
extern crate abomonation;
use abomonation::{encode, decode};

// create some test data out of abomonation-approved types
let vector = (0..256u64).map(|i| (i, format!("{}", i))).collect();

// encode vector into a Vec<u8>
let mut bytes = Vec::new();
unsafe { encode(&vector, &mut bytes); }

// unsafely decode a &Vec<(u64, String)> from binary data (maybe your utf8 are lies!).
if let Some((result, remaining) = unsafe { decode::<Vec<(u64, String)>>(&mut bytes) } {
    assert!(result == &vector);
    assert!(remaining.len() == 0);
}
```

When you use Abomonation things may go really fast. That is because it does so little work, and mostly just copies large hunks of memory. Typing

    cargo bench

will trigger Rust's benchmarking infrastructure (or an error if you are not using nightly. bad luck). The tests repeatedly encode `Vec<u64>`, `Vec<String>`, and `Vec<Vec<(u64, String)>>` giving numbers like:

    test u64_enc        ... bench:         131 ns/iter (+/- 58) = 62717 MB/s
    test string10_enc   ... bench:       8,784 ns/iter (+/- 2,791) = 3966 MB/s
    test vec_u_s_enc    ... bench:       8,964 ns/iter (+/- 1,439) = 4886 MB/s

They also repeatedly decode the same data, giving numbers like:

    test u64_dec        ... bench:           2 ns/iter (+/- 1) = 4108000 MB/s
    test string10_dec   ... bench:       1,058 ns/iter (+/- 349) = 32930 MB/s
    test vec_u_s_dec    ... bench:       1,232 ns/iter (+/- 223) = 35551 MB/s

These throughputs are so high because there is very little to do: internal pointers need to be corrected, but in their absence (*e.g.* `u64`) there is literally nothing to do.

Be warned that these numbers are not *goodput*, but rather the total number of bytes moved, which is equal to the in-memory representation of the data. On a 64bit system, a `String` requires 24 bytes plus one byte per character, which can be a lot of overhead for small strings.

## unsafe_abomonate!

Abomonation comes with the `unsafe_abomonate!` macro implementing `Abomonation` for structs which are essentially equivalent to a tuple of other `Abomonable` types. To use the macro, you must put the `#[macro_use]` modifier before `extern crate abomonation;`.

Please note that `unsafe_abomonate!` synthesizes unsafe implementations of `Abomonation`, and it is should be considered unsafe to invoke.

```rust
#[macro_use]
extern crate abomonation;
use abomonation::{encode, decode};

#[derive(Eq, PartialEq)]
struct MyStruct {
    pub a: String,
    pub b: u64,
    pub c: Vec<u8>,
}

// (type : field1, field2 .. )
unsafe_abomonate!(MyStruct : a, b, c);

// create some test data out of abomonation-approved types
let record = MyStruct{ a: "test".to_owned(), b: 0, c: vec![0, 1, 2] };

// encode vector into a Vec<u8>
let mut bytes = Vec::new();
unsafe { encode(&record, &mut bytes); }

// decode a &Vec<(u64, String)> from binary data
if let Some((result, remaining)) = unsafe { decode::<MyStruct>(&mut bytes) } {
    assert!(result == &record);
    assert!(remaining.len() == 0);
}
```

Be warned that implementing `Abomonable` for types can be a giant disaster and is entirely discouraged.

'''
'''--- benches/bench.rs ---
#![feature(test)]

extern crate abomonation;
extern crate test;

use abomonation::*;
use test::Bencher;

#[bench] fn empty_enc(bencher: &mut Bencher) { _bench_enc(bencher, vec![(); 1024]); }
#[bench] fn empty_dec(bencher: &mut Bencher) { _bench_dec(bencher, vec![(); 1024]); }

#[bench] fn u64_enc(bencher: &mut Bencher) { _bench_enc(bencher, vec![0u64; 1024]); }
#[bench] fn u64_dec(bencher: &mut Bencher) { _bench_dec(bencher, vec![0u64; 1024]); }

#[bench] fn u32x2_enc(bencher: &mut Bencher) { _bench_enc(bencher, vec![(0u32,0u32); 1024]); }
#[bench] fn u32x2_dec(bencher: &mut Bencher) { _bench_dec(bencher, vec![(0u32,0u32); 1024]); }

#[bench] fn u8_u64_enc(bencher: &mut Bencher) { _bench_enc(bencher, vec![(0u8, 0u64); 512]); }
#[bench] fn u8_u64_dec(bencher: &mut Bencher) { _bench_dec(bencher, vec![(0u8, 0u64); 512]); }

#[bench] fn string10_enc(bencher: &mut Bencher) { _bench_enc(bencher, vec![format!("grawwwwrr!"); 1024]); }
#[bench] fn string10_dec(bencher: &mut Bencher) { _bench_dec(bencher, vec![format!("grawwwwrr!"); 1024]); }

#[bench] fn string20_enc(bencher: &mut Bencher) { _bench_enc(bencher, vec![format!("grawwwwrr!!!!!!!!!!!"); 512]); }
#[bench] fn string20_dec(bencher: &mut Bencher) { _bench_dec(bencher, vec![format!("grawwwwrr!!!!!!!!!!!"); 512]); }

#[bench] fn vec_u_s_enc(bencher: &mut Bencher) { _bench_enc(bencher, vec![vec![(0u64, format!("grawwwwrr!")); 32]; 32]); }
#[bench] fn vec_u_s_dec(bencher: &mut Bencher) { _bench_dec(bencher, vec![vec![(0u64, format!("grawwwwrr!")); 32]; 32]); }

#[bench] fn vec_u_vn_s_enc(bencher: &mut Bencher) { _bench_enc(bencher, vec![vec![(0u64, vec![(); 1 << 40], format!("grawwwwrr!")); 32]; 32]); }
#[bench] fn vec_u_vn_s_dec(bencher: &mut Bencher) { _bench_dec(bencher, vec![vec![(0u64, vec![(); 1 << 40], format!("grawwwwrr!")); 32]; 32]); }

fn _bench_enc<T: Abomonation>(bencher: &mut Bencher, record: T) {

    // prepare encoded data for bencher.bytes
    let mut bytes = Vec::new();
    unsafe { encode(&record, &mut bytes).unwrap(); }

    // repeatedly encode this many bytes
    bencher.bytes = bytes.len() as u64;
    bencher.iter(|| {
        bytes.clear();
        unsafe { encode(&record, &mut bytes).unwrap(); }
    });
}

fn _bench_dec<T: Abomonation+Eq>(bencher: &mut Bencher, record: T) {

    // prepare encoded data
    let mut bytes = Vec::new();
    unsafe { encode(&record, &mut bytes).unwrap(); }

    // repeatedly decode (and validate)
    bencher.bytes = bytes.len() as u64;
    bencher.iter(|| {
        unsafe { decode::<T>(&mut bytes) }.is_some()
        // assert!(&record == result);
    });
}

'''
'''--- benches/clone.rs ---
#![feature(test)]

extern crate abomonation;
extern crate test;

use abomonation::*;
use test::Bencher;

#[bench] fn empty_e_d(bencher: &mut Bencher) { _bench_e_d(bencher, vec![(); 1024]); }
#[bench] fn empty_cln(bencher: &mut Bencher) { _bench_cln(bencher, vec![(); 1024]); }

#[bench] fn u64_e_d(bencher: &mut Bencher) { _bench_e_d(bencher, vec![0u64; 1024]); }
#[bench] fn u64_cln(bencher: &mut Bencher) { _bench_cln(bencher, vec![0u64; 1024]); }

#[bench] fn u8_u64_e_d(bencher: &mut Bencher) { _bench_e_d(bencher, vec![(0u8, 0u64); 512]); }
#[bench] fn u8_u64_cln(bencher: &mut Bencher) { _bench_cln(bencher, vec![(0u8, 0u64); 512]); }

#[bench] fn string10_e_d(bencher: &mut Bencher) { _bench_e_d(bencher, vec![format!("grawwwwrr!"); 1024]); }
#[bench] fn string10_cln(bencher: &mut Bencher) { _bench_cln(bencher, vec![format!("grawwwwrr!"); 1024]); }

#[bench] fn string20_e_d(bencher: &mut Bencher) { _bench_e_d(bencher, vec![format!("grawwwwrr!!!!!!!!!!!"); 512]); }
#[bench] fn string20_cln(bencher: &mut Bencher) { _bench_cln(bencher, vec![format!("grawwwwrr!!!!!!!!!!!"); 512]); }

#[bench] fn vec_u_s_e_d(bencher: &mut Bencher) { _bench_e_d(bencher, vec![vec![(0u64, format!("grawwwwrr!")); 32]; 32]); }
#[bench] fn vec_u_s_cln(bencher: &mut Bencher) { _bench_cln(bencher, vec![vec![(0u64, format!("grawwwwrr!")); 32]; 32]); }

#[bench] fn vec_u_vn_s_e_d(bencher: &mut Bencher) { _bench_e_d(bencher, vec![vec![(0u64, vec![(); 1 << 40], format!("grawwwwrr!")); 32]; 32]); }
#[bench] fn vec_u_vn_s_cln(bencher: &mut Bencher) { _bench_cln(bencher, vec![vec![(0u64, vec![(); 1 << 40], format!("grawwwwrr!")); 32]; 32]); }

fn _bench_e_d<T: Abomonation>(bencher: &mut Bencher, record: T) {

    // prepare encoded data for bencher.bytes
    let mut bytes = Vec::new();
    unsafe { encode(&record, &mut bytes).unwrap(); }

    // repeatedly encode this many bytes
    bencher.bytes = bytes.len() as u64;
    bencher.iter(|| {
        bytes = vec![];
        unsafe { encode(&record, &mut bytes).unwrap(); }
        unsafe { decode::<T>(&mut bytes) }.is_some()
    });
}

fn _bench_cln<T: Abomonation+Clone>(bencher: &mut Bencher, record: T) {

    // prepare encoded data
    let mut bytes = Vec::new();
    unsafe { encode(&record, &mut bytes).unwrap(); }

    // repeatedly decode (and validate)
    bencher.bytes = bytes.len() as u64;
    bencher.iter(|| {
        record.clone()
    });
}

'''
'''--- benches/recycler.rs ---
#![feature(test)]

extern crate recycler;
extern crate abomonation;
extern crate test;

use recycler::{Recyclable, Recycler, make_recycler};
use abomonation::*;
use test::Bencher;
// use std::io::Read;

#[bench] fn empty_own(bencher: &mut Bencher) { _bench_own(bencher, vec![(); 1024]); }
#[bench] fn u64_own(bencher: &mut Bencher) { _bench_own(bencher, vec![0u64; 1024]); }
#[bench] fn u8_u64_own(bencher: &mut Bencher) { _bench_own(bencher, vec![(0u8, 0u64); 512]); }
#[bench] fn string10_own(bencher: &mut Bencher) { _bench_own(bencher, vec![format!("grawwwwrr!"); 1024]); }
#[bench] fn string20_own(bencher: &mut Bencher) { _bench_own(bencher, vec![format!("grawwwwrr!!!!!!!!!!!"); 512]); }
#[bench] fn vec_u_s_own(bencher: &mut Bencher) { _bench_own(bencher, vec![vec![(0u64, format!("grawwwwrr!")); 32]; 32]); }
#[bench] fn vec_u_vn_s_own(bencher: &mut Bencher) { _bench_own(bencher, vec![vec![(0u64, vec![(); 1 << 40], format!("grawwwwrr!")); 32]; 32]); }

#[bench] fn empty_rec(bencher: &mut Bencher) { _bench_rec(bencher, vec![(); 1024]); }
#[bench] fn u64_rec(bencher: &mut Bencher) { _bench_rec(bencher, vec![0u64; 1024]); }
#[bench] fn u8_u64_rec(bencher: &mut Bencher) { _bench_rec(bencher, vec![(0u8, 0u64); 512]); }
#[bench] fn string10_rec(bencher: &mut Bencher) { _bench_rec(bencher, vec![format!("grawwwwrr!"); 1024]); }
#[bench] fn string20_rec(bencher: &mut Bencher) { _bench_rec(bencher, vec![format!("grawwwwrr!!!!!!!!!!!"); 512]); }
#[bench] fn vec_u_s_rec(bencher: &mut Bencher) { _bench_rec(bencher, vec![vec![(0u64, format!("grawwwwrr!")); 32]; 32]); }

// TODO : this reveals that working with a `vec![(); 1 << 40]` does not get optimized away.
// #[bench] fn vec_u_vn_s_rec(bencher: &mut Bencher) { _bench_rec(bencher, vec![vec![(0u64, vec![(); 1 << 40], format!("grawwwwrr!")); 32]; 32]); }

fn _bench_own<T: Abomonation+Clone>(bencher: &mut Bencher, record: T) {

    // prepare encoded data
    let mut bytes = Vec::new();
    unsafe { encode(&record, &mut bytes).unwrap(); }

    // repeatedly decode (and validate)
    bencher.bytes = bytes.len() as u64;
    bencher.iter(|| {
        (*unsafe {decode::<T>(&mut bytes[..]) }.unwrap().0).clone()
        // assert!(record == result);
    });
}

fn _bench_rec<T: Abomonation+Recyclable>(bencher: &mut Bencher, record: T) {

    // prepare encoded data
    let mut bytes = Vec::new();
    unsafe { encode(&record, &mut bytes).unwrap(); }
    let mut recycler = make_recycler::<T>();
    recycler.recycle(record);

    // repeatedly decode (and validate)
    bencher.bytes = bytes.len() as u64;
    bencher.iter(|| {
        let result = recycler.recreate(unsafe { decode::<T>(&mut bytes[..]) }.unwrap().0);
        recycler.recycle(result);
    });
}

'''
'''--- benches/serde.rs ---
#![feature(test)]
#[macro_use]
extern crate abomonation;
extern crate test;

use test::Bencher;
use abomonation::{Abomonation, encode, decode};

#[bench]
fn bench_populate(b: &mut Bencher) {
    b.iter(|| {
        Log::new()
    });
}

#[bench]
fn bench_serialize(b: &mut Bencher) {
    let log = Log::new();
    let mut bytes = vec![];
    unsafe { encode(&log, &mut bytes).unwrap(); }
    b.bytes = bytes.len() as u64;
    b.iter(|| {
        bytes.clear();
        unsafe { encode(&log, &mut bytes).unwrap(); }
        test::black_box(&bytes);
    });
}

#[bench]
fn bench_deserialize(b: &mut Bencher) {
    let log = Log::new();
    let mut bytes = vec![];
    unsafe { encode(&log, &mut bytes).unwrap(); }
    b.bytes = bytes.len() as u64;
    b.iter(|| {
        test::black_box(unsafe { decode::<Log>(&mut bytes) });
    });
}

#[bench]
fn bench_deserialize_assert(b: &mut Bencher) {
    let log = Log::new();
    let mut bytes = vec![];
    unsafe { encode(&log, &mut bytes).unwrap(); }
    b.bytes = bytes.len() as u64;
    b.iter(|| {
        assert!(unsafe { decode::<Log>(&mut bytes) }.unwrap().0 == &log);
    });
}

unsafe_abomonate!(Http : content_type, user_agent, referer, request_uri);
unsafe_abomonate!(Origin : ip, hostname);
unsafe_abomonate!(Log : http, origin, server_ip, server_name, remote_ip, ray_id);

#[derive(Eq, PartialEq)]
pub struct Http {
    protocol: HttpProtocol,
    status: u32,
    host_status: u32,
    up_status: u32,
    method: HttpMethod,
    content_type: String,
    user_agent: String,
    referer: String,
    request_uri: String,
}

#[allow(non_camel_case_types)]
#[derive(Eq, PartialEq)]
pub enum HttpProtocol {
    HTTP_PROTOCOL_UNKNOWN,
    HTTP10,
    HTTP11,
}

#[allow(non_camel_case_types)]
#[derive(Eq, PartialEq)]
pub enum HttpMethod {
    METHOD_UNKNOWN,
    GET,
    POST,
    DELETE,
    PUT,
    HEAD,
    PURGE,
    OPTIONS,
    PROPFIND,
    MKCOL,
    PATCH,
}

#[allow(non_camel_case_types)]
#[derive(Eq, PartialEq)]
pub enum CacheStatus {
    CACHESTATUS_UNKNOWN,
    Miss,
    Expired,
    Hit,
}

#[derive(Eq, PartialEq)]
pub struct Origin {
    ip: String,
    port: u32,
    hostname: String,
    protocol: OriginProtocol,
}

#[allow(non_camel_case_types)]
#[derive(Eq, PartialEq)]
pub enum OriginProtocol {
    ORIGIN_PROTOCOL_UNKNOWN,
    HTTP,
    HTTPS,
}

#[allow(non_camel_case_types)]
#[derive(Eq, PartialEq)]
pub enum ZonePlan {
    ZONEPLAN_UNKNOWN,
    FREE,
    PRO,
    BIZ,
    ENT,
}

#[derive(Eq, PartialEq)]
pub enum Country {
	UNKNOWN,
	A1,
	A2,
	O1,
	AD,
	AE,
	AF,
	AG,
	AI,
	AL,
	AM,
	AO,
	AP,
	AQ,
	AR,
	AS,
	AT,
	AU,
	AW,
	AX,
	AZ,
	BA,
	BB,
	BD,
	BE,
	BF,
	BG,
	BH,
	BI,
	BJ,
	BL,
	BM,
	BN,
	BO,
	BQ,
	BR,
	BS,
	BT,
	BV,
	BW,
	BY,
	BZ,
	CA,
	CC,
	CD,
	CF,
	CG,
	CH,
	CI,
	CK,
	CL,
	CM,
	CN,
	CO,
	CR,
	CU,
	CV,
	CW,
	CX,
	CY,
	CZ,
	DE,
	DJ,
	DK,
	DM,
	DO,
	DZ,
	EC,
	EE,
	EG,
	EH,
	ER,
	ES,
	ET,
	EU,
	FI,
	FJ,
	FK,
	FM,
	FO,
	FR,
	GA,
	GB,
	GD,
	GE,
	GF,
	GG,
	GH,
	GI,
	GL,
	GM,
	GN,
	GP,
	GQ,
	GR,
	GS,
	GT,
	GU,
	GW,
	GY,
	HK,
	HM,
	HN,
	HR,
	HT,
	HU,
	ID,
	IE,
	IL,
	IM,
	IN,
	IO,
	IQ,
	IR,
	IS,
	IT,
	JE,
	JM,
	JO,
	JP,
	KE,
	KG,
	KH,
	KI,
	KM,
	KN,
	KP,
	KR,
	KW,
	KY,
	KZ,
	LA,
	LB,
	LC,
	LI,
	LK,
	LR,
	LS,
	LT,
	LU,
	LV,
	LY,
	MA,
	MC,
	MD,
	ME,
	MF,
	MG,
	MH,
	MK,
	ML,
	MM,
	MN,
	MO,
	MP,
	MQ,
	MR,
	MS,
	MT,
	MU,
	MV,
	MW,
	MX,
	MY,
	MZ,
	NA,
	NC,
	NE,
	NF,
	NG,
	NI,
	NL,
	NO,
	NP,
	NR,
	NU,
	NZ,
	OM,
	PA,
	PE,
	PF,
	PG,
	PH,
	PK,
	PL,
	PM,
	PN,
	PR,
	PS,
	PT,
	PW,
	PY,
	QA,
	RE,
	RO,
	RS,
	RU,
	RW,
	SA,
	SB,
	SC,
	SD,
	SE,
	SG,
	SH,
	SI,
	SJ,
	SK,
	SL,
	SM,
	SN,
	SO,
	SR,
	SS,
	ST,
	SV,
	SX,
	SY,
	SZ,
	TC,
	TD,
	TF,
	TG,
	TH,
	TJ,
	TK,
	TL,
	TM,
	TN,
	TO,
	TR,
	TT,
	TV,
	TW,
	TZ,
	UA,
	UG,
	UM,
	US,
	UY,
	UZ,
	VA,
	VC,
	VE,
	VG,
	VI,
	VN,
	VU,
	WF,
	WS,
	XX,
	YE,
	YT,
	ZA,
	ZM,
	ZW,
}

#[derive(Eq, PartialEq)]
pub struct Log {
    timestamp: i64,
    zone_id: u32,
    zone_plan: ZonePlan,
    http: Http,
    origin: Origin,
    country: Country,
    cache_status: CacheStatus,
    server_ip: String,
    server_name: String,
    remote_ip: String,
    bytes_dlv: u64,
    ray_id: String,
}

impl Log {
    pub fn new() -> Log {
        Log {
            timestamp: 2837513946597,
            zone_id: 123456,
            zone_plan: ZonePlan::FREE,
            http: Http {
                protocol: HttpProtocol::HTTP11,
                status: 200,
                host_status: 503,
                up_status: 520,
                method: HttpMethod::GET,
                content_type: "text/html".to_owned(),
                user_agent: "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/33.0.1750.146 Safari/537.36".to_owned(),
                referer: "https://www.cloudflare.com/".to_owned(),
                request_uri: "/cdn-cgi/trace".to_owned(),
            },
            origin: Origin {
                ip: "1.2.3.4".to_owned(),
                port: 8000,
                hostname: "www.example.com".to_owned(),
                protocol: OriginProtocol::HTTPS,
            },
            country: Country::US,
            cache_status: CacheStatus::Hit,
            server_ip: "192.168.1.1".to_owned(),
            server_name: "metal.cloudflare.com".to_owned(),
            remote_ip: "10.1.2.3".to_owned(),
            bytes_dlv: 123456,
            ray_id: "10c73629cce30078-LAX".to_owned(),
        }
    }
}

'''
'''--- src/abomonated.rs ---

use std::mem::transmute;
use std::marker::PhantomData;
use std::ops::{Deref, DerefMut};

use super::{Abomonation, decode};

/// A type wrapping owned decoded abomonated data.
///
/// This type ensures that decoding and pointer correction has already happened,
/// and implements `Deref<Target=T>` using a pointer cast (transmute).
///
/// #Safety
///
/// The safety of this type, and in particular its `transute` implementation of
/// the `Deref` trait, relies on the owned bytes not being externally mutated
/// once provided. You could imagine a new type implementing `DerefMut` as required,
/// but which also retains the ability (e.g. through `RefCell`) to mutate the bytes.
/// This would be very bad, but seems hard to prevent in the type system. Please
/// don't do this.
///
/// You must also use a type `S` whose bytes have a fixed location in memory.
/// Otherwise moving an instance of `Abomonated<T, S>` may invalidate decoded
/// pointers, and everything goes badly.
///
/// #Examples
///
/// ```
/// use std::ops::Deref;
/// use abomonation::{encode, decode};
/// use abomonation::abomonated::Abomonated;
///
/// // create some test data out of abomonation-approved types
/// let vector = (0..256u64).map(|i| (i, format!("{}", i)))
///                         .collect::<Vec<_>>();
///
/// // encode a Vec<(u64, String)> into a Vec<u8>
/// let mut bytes = Vec::new();
/// unsafe { encode(&vector, &mut bytes); }
///
/// // attempt to decode `bytes` into a `&Vec<(u64, String)>`.
/// let maybe_decoded = unsafe { Abomonated::<Vec<(u64, String)>,_>::new(bytes) };
///
/// if let Some(decoded) = maybe_decoded {
///     // each `deref()` call is now just a pointer cast.
///     assert!(decoded.deref() == &vector);
/// }
/// else {
///     panic!("failed to decode");
/// }
/// ```
pub struct Abomonated<T, S: DerefMut<Target=[u8]>> {
    phantom: PhantomData<T>,
    decoded: S,
}

impl<T: Abomonation, S: DerefMut<Target=[u8]>> Abomonated<T, S> {

    /// Attempts to create decoded data from owned mutable bytes.
    ///
    /// This method will return `None` if it is unable to decode the data with
    /// type `T`.
    ///
    /// #Examples
    ///
    /// ```
    /// use std::ops::Deref;
    /// use abomonation::{encode, decode};
    /// use abomonation::abomonated::Abomonated;
    ///
    /// // create some test data out of abomonation-approved types
    /// let vector = (0..256u64).map(|i| (i, format!("{}", i)))
    ///                         .collect::<Vec<_>>();
    ///
    /// // encode a Vec<(u64, String)> into a Vec<u8>
    /// let mut bytes = Vec::new();
    /// unsafe { encode(&vector, &mut bytes); }
    ///
    /// // attempt to decode `bytes` into a `&Vec<(u64, String)>`.
    /// let maybe_decoded = unsafe { Abomonated::<Vec<(u64, String)>,_>::new(bytes) };
    ///
    /// if let Some(decoded) = maybe_decoded {
    ///     // each `deref()` call is now just a pointer cast.
    ///     assert!(decoded.deref() == &vector);
    /// }
    /// else {
    ///     panic!("failed to decode");
    /// }
    /// ```
    ///
    /// #Safety
    ///
    /// The type `S` must have its bytes at a fixed location, which will
    /// not change if the `bytes: S` instance is moved. Good examples are
    /// `Vec<u8>` whereas bad examples are `[u8; 16]`.
    pub unsafe fn new(mut bytes: S) -> Option<Self> {

        // performs the underlying pointer correction, indicates success.
        let decoded = decode::<T>(bytes.deref_mut()).is_some();

        if decoded {
            Some(Abomonated {
                phantom: PhantomData,
                decoded: bytes,
            })
        }
        else {
            None
        }
    }
}

impl<T, S: DerefMut<Target=[u8]>> Abomonated<T, S> {
    pub fn as_bytes(&self) -> &[u8] {
        &self.decoded
    }
}

impl<T, S: DerefMut<Target=[u8]>> Deref for Abomonated<T, S> {
    type Target = T;
    #[inline]
    fn deref(&self) -> &T {
        let result: &T = unsafe { transmute(self.decoded.get_unchecked(0)) };
        result
    }
}

'''
'''--- src/lib.rs ---
//! Abomonation (spelling intentional) is a fast serialization / deserialization crate.
//!
//! Abomonation takes typed elements and simply writes their contents as binary.
//! It then gives the element the opportunity to serialize more data, which is
//! useful for types with owned memory such as `String` and `Vec`.
//! The result is effectively a copy of reachable memory.
//! Deserialization results in a shared reference to the type, pointing at the binary data itself.
//!
//! Abomonation does several unsafe things, and should ideally be used only through the methods
//! `encode` and `decode` on types implementing the `Abomonation` trait. Implementing the
//! `Abomonation` trait is highly discouraged; instead, you can use the [`abomonation_derive` crate](https://crates.io/crates/abomonation_derive).
//!
//! **Very important**: Abomonation reproduces the memory as laid out by the serializer, which will
//! reveal architectural variations. Data encoded on a 32bit big-endian machine will not decode
//! properly on a 64bit little-endian machine. Moreover, it could result in undefined behavior if
//! the deserialization results in invalid typed data. Please do not do this.
//!
//!
//! # Examples
//! ```
//! use abomonation::{encode, decode};
//!
//! // create some test data out of abomonation-approved types
//! let vector = (0..256u64).map(|i| (i, format!("{}", i)))
//!                         .collect::<Vec<_>>();
//!
//! // encode a Vec<(u64, String)> into a Vec<u8>
//! let mut bytes = Vec::new();
//! unsafe { encode(&vector, &mut bytes); }
//!
//! // decode a &Vec<(u64, String)> from &mut [u8] binary data
//! if let Some((result, remaining)) = unsafe { decode::<Vec<(u64, String)>>(&mut bytes) } {
//!     assert!(result == &vector);
//!     assert!(remaining.len() == 0);
//! }
//! ```

use std::mem;       // yup, used pretty much everywhere.
use std::io::Write; // for bytes.write_all; push_all is unstable and extend is slow.
use std::io::Result as IOResult;
use std::marker::PhantomData;
use std::num::*;

pub mod abomonated;

/// Encodes a typed reference into a binary buffer.
///
/// # Safety
///
/// This method is unsafe because it is unsafe to transmute typed allocations to binary.
/// Furthermore, Rust currently indicates that it is undefined behavior to observe padding
/// bytes, which will happen when we `memmcpy` structs which contain padding bytes.
///
/// # Examples
/// ```
/// use abomonation::{encode, decode};
///
/// // create some test data out of abomonation-approved types
/// let vector = (0..256u64).map(|i| (i, format!("{}", i)))
///                         .collect::<Vec<_>>();
///
/// // encode a Vec<(u64, String)> into a Vec<u8>
/// let mut bytes = Vec::new();
/// unsafe { encode(&vector, &mut bytes); }
///
/// // decode a &Vec<(u64, String)> from &mut [u8] binary data
/// if let Some((result, remaining)) = unsafe { decode::<Vec<(u64, String)>>(&mut bytes) } {
///     assert!(result == &vector);
///     assert!(remaining.len() == 0);
/// }
/// ```
///
#[inline]
pub unsafe fn encode<T: Abomonation, W: Write>(typed: &T, write: &mut W) -> IOResult<()> {
    let slice = std::slice::from_raw_parts(mem::transmute(typed), mem::size_of::<T>());
    write.write_all(slice)?;
    typed.entomb(write)?;
    Ok(())
}

/// Decodes a mutable binary slice into an immutable typed reference.
///
/// `decode` treats the first `mem::size_of::<T>()` bytes as a `T`, and will then `exhume` the
/// element, offering it the ability to consume prefixes of `bytes` to back any owned data.
/// The return value is either a pair of the typed reference `&T` and the remaining `&mut [u8]`
/// binary data, or `None` if decoding failed due to lack of data.
///
/// # Safety
///
/// The `decode` method is unsafe due to a number of unchecked invariants.
///
/// Decoding arbitrary `&[u8]` data can
/// result in invalid utf8 strings, enums with invalid discriminants, etc. `decode` *does*
/// perform bounds checks, as part of determining if enough data are present to completely decode,
/// and while it should only write within the bounds of its `&mut [u8]` argument, the use of invalid
/// utf8 and enums are undefined behavior.
///
/// Please do not decode data that was not encoded by the corresponding implementation.
///
/// In addition, `decode` does not ensure that the bytes representing types will be correctly aligned.
/// On several platforms unaligned reads are undefined behavior, but on several other platforms they
/// are only a performance penalty.
///
/// # Examples
/// ```
/// use abomonation::{encode, decode};
///
/// // create some test data out of abomonation-approved types
/// let vector = (0..256u64).map(|i| (i, format!("{}", i)))
///                         .collect::<Vec<_>>();
///
/// // encode a Vec<(u64, String)> into a Vec<u8>
/// let mut bytes = Vec::new();
/// unsafe { encode(&vector, &mut bytes); }
///
/// // decode a &Vec<(u64, String)> from &mut [u8] binary data
/// if let Some((result, remaining)) = unsafe { decode::<Vec<(u64, String)>>(&mut bytes) } {
///     assert!(result == &vector);
///     assert!(remaining.len() == 0);
/// }
/// ```
#[inline]
pub unsafe fn decode<T: Abomonation>(bytes: &mut [u8]) -> Option<(&T, &mut [u8])> {
    if bytes.len() < mem::size_of::<T>() { None }
    else {
        let (split1, split2) = bytes.split_at_mut(mem::size_of::<T>());
        let result: &mut T = mem::transmute(split1.get_unchecked_mut(0));
        if let Some(remaining) = result.exhume(split2) {
            Some((result, remaining))
        }
        else {
            None
        }
    }
}

/// Reports the number of bytes required to encode `self`.
///
/// # Safety
///
/// The `measure` method is safe. It neither produces nor consults serialized representations.
#[inline]
pub fn measure<T: Abomonation>(typed: &T) -> usize {
    mem::size_of::<T>() + typed.extent()
}

/// Abomonation provides methods to serialize any heap data the implementor owns.
///
/// The default implementations for Abomonation's methods are all empty. Many types have no owned
/// data to transcribe. Some do, however, and need to carefully implement these unsafe methods.
///
/// # Safety
///
/// Abomonation has no safe methods. Please do not call them. They should be called only by
/// `encode` and `decode`, each of which impose restrictions on ownership and lifetime of the data
/// they take as input and return as output.
///
/// If you are concerned about safety, it may be best to avoid Abomonation all together. It does
/// several things that may be undefined behavior, depending on how undefined behavior is defined.
pub trait Abomonation {

    /// Write any additional information about `&self` beyond its binary representation.
    ///
    /// Most commonly this is owned data on the other end of pointers in `&self`. The return value
    /// reports any failures in writing to `write`.
    #[inline(always)] unsafe fn entomb<W: Write>(&self, _write: &mut W) -> IOResult<()> { Ok(()) }

    /// Recover any information for `&mut self` not evident from its binary representation.
    ///
    /// Most commonly this populates pointers with valid references into `bytes`.
    #[inline(always)] unsafe fn exhume<'a,'b>(&'a mut self, bytes: &'b mut [u8]) -> Option<&'b mut [u8]> { Some(bytes) }

    /// Reports the number of further bytes required to entomb `self`.
    #[inline(always)] fn extent(&self) -> usize { 0 }
}

/// The `unsafe_abomonate!` macro takes a type name with an optional list of fields, and implements
/// `Abomonation` for the type, following the pattern of the tuple implementations: each method
/// calls the equivalent method on each of its fields.
///
/// It is strongly recommended that you use the `abomonation_derive` crate instead of this macro.
///
/// # Safety
/// `unsafe_abomonate` is unsafe because if you fail to specify a field it will not be properly
/// re-initialized from binary data. This can leave you with a dangling pointer, or worse.
///
/// # Examples
/// ```
/// #[macro_use]
/// extern crate abomonation;
/// use abomonation::{encode, decode, Abomonation};
///
/// #[derive(Eq, PartialEq)]
/// struct MyStruct {
///     a: String,
///     b: u64,
///     c: Vec<u8>,
/// }
///
/// unsafe_abomonate!(MyStruct : a, b, c);
///
/// fn main() {
///
///     // create some test data out of recently-abomonable types
///     let my_struct = MyStruct { a: "grawwwwrr".to_owned(), b: 0, c: vec![1,2,3] };
///
///     // encode a &MyStruct into a Vec<u8>
///     let mut bytes = Vec::new();
///     unsafe { encode(&my_struct, &mut bytes); }
///
///     // decode a &MyStruct from &mut [u8] binary data
///     if let Some((result, remaining)) = unsafe { decode::<MyStruct>(&mut bytes) } {
///         assert!(result == &my_struct);
///         assert!(remaining.len() == 0);
///     }
/// }
/// ```
#[macro_export]
#[deprecated(since="0.5", note="please use the abomonation_derive crate")]
macro_rules! unsafe_abomonate {
    ($t:ty) => {
        impl Abomonation for $t { }
    };
    ($t:ty : $($field:ident),*) => {
        impl Abomonation for $t {
            #[inline] unsafe fn entomb<W: ::std::io::Write>(&self, write: &mut W) -> ::std::io::Result<()> {
                $( self.$field.entomb(write)?; )*
                Ok(())
            }
            #[inline] unsafe fn exhume<'a,'b>(&'a mut self, mut bytes: &'b mut [u8]) -> Option<&'b mut [u8]> {
                $( let temp = bytes; bytes = self.$field.exhume(temp)?; )*
                Some(bytes)
            }
            #[inline] fn extent(&self) -> usize {
                let mut size = 0;
                $( size += self.$field.extent(); )*
                size
            }
        }
    };
}

// general code for tuples (can't use '0', '1', ... as field identifiers)
macro_rules! tuple_abomonate {
    ( $($name:ident)+) => (
        impl<$($name: Abomonation),*> Abomonation for ($($name,)*) {
            #[allow(non_snake_case)]
            #[inline(always)] unsafe fn entomb<WRITE: Write>(&self, write: &mut WRITE) -> IOResult<()> {
                let ($(ref $name,)*) = *self;
                $($name.entomb(write)?;)*
                Ok(())
            }
            #[allow(non_snake_case)]
            #[inline(always)] unsafe fn exhume<'a,'b>(&'a mut self, mut bytes: &'b mut [u8]) -> Option<&'b mut [u8]> {
                let ($(ref mut $name,)*) = *self;
                $( let temp = bytes; bytes = $name.exhume(temp)?; )*
                Some(bytes)
            }
            #[allow(non_snake_case)]
            #[inline(always)] fn extent(&self) -> usize {
                let mut size = 0;
                let ($(ref $name,)*) = *self;
                $( size += $name.extent(); )*
                size
            }
        }
    );
}

impl Abomonation for u8 { }
impl Abomonation for u16 { }
impl Abomonation for u32 { }
impl Abomonation for u64 { }
impl Abomonation for u128 { }
impl Abomonation for usize { }

impl Abomonation for i8 { }
impl Abomonation for i16 { }
impl Abomonation for i32 { }
impl Abomonation for i64 { }
impl Abomonation for i128 { }
impl Abomonation for isize { }

impl Abomonation for NonZeroU8 { }
impl Abomonation for NonZeroU16 { }
impl Abomonation for NonZeroU32 { }
impl Abomonation for NonZeroU64 { }
impl Abomonation for NonZeroU128 { }
impl Abomonation for NonZeroUsize { }

impl Abomonation for NonZeroI8 { }
impl Abomonation for NonZeroI16 { }
impl Abomonation for NonZeroI32 { }
impl Abomonation for NonZeroI64 { }
impl Abomonation for NonZeroI128 { }
impl Abomonation for NonZeroIsize { }

impl Abomonation for f32 { }
impl Abomonation for f64 { }

impl Abomonation for bool { }
impl Abomonation for () { }

impl Abomonation for char { }

impl Abomonation for ::std::time::Duration { }

impl<T> Abomonation for PhantomData<T> {}

impl<T: Abomonation> Abomonation for std::ops::Range<T> {
    #[inline(always)] unsafe fn entomb<W: Write>(&self, write: &mut W) -> IOResult<()> {
        self.start.entomb(write)?;
        self.end.entomb(write)?;
        Ok(())
    }
    #[inline(always)] unsafe fn exhume<'a, 'b>(&'a mut self, mut bytes: &'b mut[u8]) -> Option<&'b mut [u8]> {
        let tmp = bytes; bytes = self.start.exhume(tmp)?;
        let tmp = bytes; bytes = self.end.exhume(tmp)?;
        Some(bytes)
    }
    #[inline] fn extent(&self) -> usize {
        self.start.extent() << 1
    }
}

impl<T: Abomonation> Abomonation for Option<T> {
    #[inline(always)] unsafe fn entomb<W: Write>(&self, write: &mut W) -> IOResult<()> {
        if let &Some(ref inner) = self {
            inner.entomb(write)?;
        }
        Ok(())
    }
    #[inline(always)] unsafe fn exhume<'a, 'b>(&'a mut self, mut bytes: &'b mut[u8]) -> Option<&'b mut [u8]> {
        if let &mut Some(ref mut inner) = self {
            let tmp = bytes; bytes = inner.exhume(tmp)?;
        }
        Some(bytes)
    }
    #[inline] fn extent(&self) -> usize {
        self.as_ref().map(|inner| inner.extent()).unwrap_or(0)
    }
}

impl<T: Abomonation, E: Abomonation> Abomonation for Result<T, E> {
    #[inline(always)] unsafe fn entomb<W: Write>(&self, write: &mut W) -> IOResult<()> {
        match self {
            &Ok(ref inner) => inner.entomb(write)?,
            &Err(ref inner) => inner.entomb(write)?,
        };
        Ok(())
    }
    #[inline(always)] unsafe fn exhume<'a, 'b>(&'a mut self, bytes: &'b mut[u8]) -> Option<&'b mut [u8]> {
        match self {
            &mut Ok(ref mut inner) => inner.exhume(bytes),
            &mut Err(ref mut inner) => inner.exhume(bytes),
        }
    }
    #[inline] fn extent(&self) -> usize {
        match self {
            &Ok(ref inner) => inner.extent(),
            &Err(ref inner) => inner.extent(),
        }
    }
}

tuple_abomonate!(A);
tuple_abomonate!(A B);
tuple_abomonate!(A B C);
tuple_abomonate!(A B C D);
tuple_abomonate!(A B C D E);
tuple_abomonate!(A B C D E F);
tuple_abomonate!(A B C D E F G);
tuple_abomonate!(A B C D E F G H);
tuple_abomonate!(A B C D E F G H I);
tuple_abomonate!(A B C D E F G H I J);
tuple_abomonate!(A B C D E F G H I J K);
tuple_abomonate!(A B C D E F G H I J K L);
tuple_abomonate!(A B C D E F G H I J K L M);
tuple_abomonate!(A B C D E F G H I J K L M N);
tuple_abomonate!(A B C D E F G H I J K L M N O);
tuple_abomonate!(A B C D E F G H I J K L M N O P);
tuple_abomonate!(A B C D E F G H I J K L M N O P Q);
tuple_abomonate!(A B C D E F G H I J K L M N O P Q R);
tuple_abomonate!(A B C D E F G H I J K L M N O P Q R S);
tuple_abomonate!(A B C D E F G H I J K L M N O P Q R S T);
tuple_abomonate!(A B C D E F G H I J K L M N O P Q R S T U);
tuple_abomonate!(A B C D E F G H I J K L M N O P Q R S T U V);
tuple_abomonate!(A B C D E F G H I J K L M N O P Q R S T U V W);
tuple_abomonate!(A B C D E F G H I J K L M N O P Q R S T U V W X);
tuple_abomonate!(A B C D E F G H I J K L M N O P Q R S T U V W X Y);
tuple_abomonate!(A B C D E F G H I J K L M N O P Q R S T U V W X Y Z);
tuple_abomonate!(A B C D E F G H I J K L M N O P Q R S T U V W X Y Z AA);
tuple_abomonate!(A B C D E F G H I J K L M N O P Q R S T U V W X Y Z AA AB);
tuple_abomonate!(A B C D E F G H I J K L M N O P Q R S T U V W X Y Z AA AB AC);
tuple_abomonate!(A B C D E F G H I J K L M N O P Q R S T U V W X Y Z AA AB AC AD);
tuple_abomonate!(A B C D E F G H I J K L M N O P Q R S T U V W X Y Z AA AB AC AD AE);
tuple_abomonate!(A B C D E F G H I J K L M N O P Q R S T U V W X Y Z AA AB AC AD AE AF);

macro_rules! array_abomonate {
    ($size:expr) => (
        impl<T: Abomonation> Abomonation for [T; $size] {
            #[inline(always)]
            unsafe fn entomb<W: Write>(&self, write: &mut W) ->  IOResult<()> {
                for element in self { element.entomb(write)?; }
                Ok(())
            }
            #[inline(always)]
            unsafe fn exhume<'a, 'b>(&'a mut self, mut bytes: &'b mut[u8]) -> Option<&'b mut [u8]> {
                for element in self {
                    let tmp = bytes; bytes = element.exhume(tmp)?;
                }
                Some(bytes)
            }
            #[inline(always)] fn extent(&self) -> usize {
                let mut size = 0;
                for element in self {
                    size += element.extent();
                }
                size
            }
        }
    )
}

array_abomonate!(0);
array_abomonate!(1);
array_abomonate!(2);
array_abomonate!(3);
array_abomonate!(4);
array_abomonate!(5);
array_abomonate!(6);
array_abomonate!(7);
array_abomonate!(8);
array_abomonate!(9);
array_abomonate!(10);
array_abomonate!(11);
array_abomonate!(12);
array_abomonate!(13);
array_abomonate!(14);
array_abomonate!(15);
array_abomonate!(16);
array_abomonate!(17);
array_abomonate!(18);
array_abomonate!(19);
array_abomonate!(20);
array_abomonate!(21);
array_abomonate!(22);
array_abomonate!(23);
array_abomonate!(24);
array_abomonate!(25);
array_abomonate!(26);
array_abomonate!(27);
array_abomonate!(28);
array_abomonate!(29);
array_abomonate!(30);
array_abomonate!(31);
array_abomonate!(32);

impl Abomonation for String {
    #[inline]
    unsafe fn entomb<W: Write>(&self, write: &mut W) -> IOResult<()> {
        write.write_all(self.as_bytes())?;
        Ok(())
    }
    #[inline]
    unsafe fn exhume<'a,'b>(&'a mut self, bytes: &'b mut [u8]) -> Option<&'b mut [u8]> {
        if self.len() > bytes.len() { None }
        else {
            let (mine, rest) = bytes.split_at_mut(self.len());
            std::ptr::write(self, String::from_raw_parts(mem::transmute(mine.as_ptr()), self.len(), self.len()));
            Some(rest)
        }
    }
    #[inline] fn extent(&self) -> usize {
        self.len()
    }
}

impl<T: Abomonation> Abomonation for Vec<T> {
    #[inline]
    unsafe fn entomb<W: Write>(&self, write: &mut W) -> IOResult<()> {
        write.write_all(typed_to_bytes(&self[..]))?;
        for element in self.iter() { element.entomb(write)?; }
        Ok(())
    }
    #[inline]
    unsafe fn exhume<'a,'b>(&'a mut self, bytes: &'b mut [u8]) -> Option<&'b mut [u8]> {

        // extract memory from bytes to back our vector
        let binary_len = self.len() * mem::size_of::<T>();
        if binary_len > bytes.len() { None }
        else {
            let (mine, mut rest) = bytes.split_at_mut(binary_len);
            let slice = std::slice::from_raw_parts_mut(mine.as_mut_ptr() as *mut T, self.len());
            std::ptr::write(self, Vec::from_raw_parts(slice.as_mut_ptr(), self.len(), self.len()));
            for element in self.iter_mut() {
                let temp = rest;             // temp variable explains lifetimes (mysterious!)
                rest = element.exhume(temp)?;
            }
            Some(rest)
        }
    }
    #[inline]
    fn extent(&self) -> usize {
        let mut sum = mem::size_of::<T>() * self.len();
        for element in self.iter() {
            sum += element.extent();
        }
        sum
    }
}

impl<T: Abomonation> Abomonation for Box<T> {
    #[inline]
    unsafe fn entomb<W: Write>(&self, bytes: &mut W) -> IOResult<()> {
        bytes.write_all(std::slice::from_raw_parts(mem::transmute(&**self), mem::size_of::<T>()))?;
        (**self).entomb(bytes)?;
        Ok(())
    }
    #[inline]
    unsafe fn exhume<'a,'b>(&'a mut self, bytes: &'b mut [u8]) -> Option<&'b mut [u8]> {
        let binary_len = mem::size_of::<T>();
        if binary_len > bytes.len() { None }
        else {
            let (mine, mut rest) = bytes.split_at_mut(binary_len);
            std::ptr::write(self, mem::transmute(mine.as_mut_ptr() as *mut T));
            let temp = rest; rest = (**self).exhume(temp)?;
            Some(rest)
        }
    }
    #[inline] fn extent(&self) -> usize {
        mem::size_of::<T>() + (&**self).extent()
    }
}

// This method currently enables undefined behavior, by exposing padding bytes.
#[inline] unsafe fn typed_to_bytes<T>(slice: &[T]) -> &[u8] {
    std::slice::from_raw_parts(slice.as_ptr() as *const u8, slice.len() * mem::size_of::<T>())
}

mod network {
    use Abomonation;
    use std::net::{SocketAddr, SocketAddrV4, SocketAddrV6, IpAddr, Ipv4Addr, Ipv6Addr};

    impl Abomonation for IpAddr { }
    impl Abomonation for Ipv4Addr { }
    impl Abomonation for Ipv6Addr { }

    impl Abomonation for SocketAddr { }
    impl Abomonation for SocketAddrV4 { }
    impl Abomonation for SocketAddrV6 { }
}

mod std_collections {
    use Abomonation;
    use std::collections::HashMap;
    use std::io::Write;
    use std::io::Result as IOResult;
    use std::ptr::NonNull;
    use std::mem;
    use std::marker::PhantomData;
    use std::alloc::Layout;
    use std::hint;
    use encode;
    use decode;

    unsafe fn offset_from<T>(to: *const T, from: *const T) -> usize {
        to.offset_from(from) as usize
    }

    #[inline]
    fn calculate_layout<T>(buckets: usize) -> Option<(Layout, usize)> {
//        debug_assert!(buckets.is_power_of_two());

        // Manual layout calculation since Layout methods are not yet stable.
        let ctrl_align = usize::max(mem::align_of::<T>(), Group::WIDTH);
        let ctrl_offset = mem::size_of::<T>()
            .checked_mul(buckets)?
            .checked_add(ctrl_align - 1)?
            & !(ctrl_align - 1);
        let len = ctrl_offset.checked_add(buckets + Group::WIDTH)?;

        Some((
            unsafe { Layout::from_size_align_unchecked(len, ctrl_align) },
            ctrl_offset,
        ))
    }

    struct RandomState {
        k0: u64,
        k1: u64,
    }
    impl Abomonation for RandomState {}

    struct StdHashMap<K, V> {
        base: HashbrownHashMap<K, V>,
    }

    struct HashbrownHashMap<K, V> {
        hash_builder: RandomState,
        table: RawTable<(K, V)>,
    }

    struct RawTable<T> {
        bucket_mask: usize,
        ctrl: NonNull<u8>,
        growth_left: usize,
        items: usize,
        marker: PhantomData<T>
    }

    impl<T> RawTable<T> {
        #[inline]
        fn num_ctrl_bytes(&self) -> usize {
            self.bucket_mask + 1 + 16
        }

        #[inline]
        pub fn buckets(&self) -> usize {
            self.bucket_mask + 1
        }

        #[inline]
        pub unsafe fn data_end(&self) -> NonNull<T> {
            NonNull::new_unchecked(self.ctrl.as_ptr() as *mut T)
        }

        #[inline]
        unsafe fn ctrl(&self, index: usize) -> *mut u8 {
            debug_assert!(index < self.num_ctrl_bytes());
            self.ctrl.as_ptr().add(index)
        }

        #[inline]
        pub unsafe fn iter(&self) -> RawIter<T> {
            let data = Bucket::from_base_index(self.data_end(), 0);
            RawIter {
                iter: RawIterRange::new(self.ctrl.as_ptr(), data, self.buckets()),
                items: self.items,
            }
        }

        #[inline]
        fn is_empty_singleton(&self) -> bool {
            self.bucket_mask == 0
        }

        #[inline]
        pub(crate) fn into_alloc(self) -> Option<(NonNull<u8>, Layout)> {
            let alloc = if self.is_empty_singleton() {
                None
            } else {
                // Avoid `Option::unwrap_or_else` because it bloats LLVM IR.
                let (layout, ctrl_offset) = match calculate_layout::<T>(self.buckets()) {
                    Some(lco) => lco,
                    None => unsafe { hint::unreachable_unchecked() },
                };
                Some((
                    unsafe { NonNull::new_unchecked(self.ctrl.as_ptr().sub(ctrl_offset)) },
                    layout,
                ))
            };
            mem::forget(self);
            alloc
        }

        #[inline]
        pub unsafe fn into_iter_from(self, iter: RawIter<T>) -> RawIntoIter<T> {
//            debug_assert_eq!(iter.len(), self.len());

            let alloc = self.into_alloc();
            RawIntoIter {
                iter,
                alloc,
                marker: PhantomData,
            }
        }

        #[inline]
        pub unsafe fn bucket_index(&self, bucket: &Bucket<T>) -> usize {
            bucket.to_base_index(self.data_end())
        }

        #[inline]
        pub unsafe fn bucket(&self, index: usize) -> Bucket<T> {
            debug_assert_ne!(self.bucket_mask, 0);
            debug_assert!(index < self.buckets());
            Bucket::from_base_index(self.data_end(), index)
        }
    }

    pub struct RawIntoIter<T> {
        iter: RawIter<T>,
        alloc: Option<(NonNull<u8>, Layout)>,
        marker: PhantomData<T>,
    }

    impl<T> IntoIterator for RawTable<T> {
        type Item = T;
        type IntoIter = RawIntoIter<T>;

        #[inline]
        fn into_iter(self) -> RawIntoIter<T> {
            unsafe {
                let iter = self.iter();
                self.into_iter_from(iter)
            }
        }
    }

    impl<T> Iterator for RawIntoIter<T> {
        type Item = T;

        #[inline]
        fn next(&mut self) -> Option<T> {
            unsafe { Some(self.iter.next()?.read()) }
        }

        #[inline]
        fn size_hint(&self) -> (usize, Option<usize>) {
            self.iter.size_hint()
        }
    }

    #[cfg(target_arch = "x86_64")]
    use std::arch::x86_64 as x86;
    pub type BitMaskWord = u16;
    #[derive(Copy, Clone)]
    pub struct BitMask(pub BitMaskWord);
    pub const BITMASK_MASK: BitMaskWord = 0xffff;
    pub const BITMASK_STRIDE: usize = 1;
    impl BitMask {
        /// Returns a new `BitMask` with all bits inverted.
        #[inline]
        pub fn invert(self) -> Self {
            BitMask(self.0 ^ BITMASK_MASK)
        }
        #[inline]
        pub fn lowest_set_bit(self) -> Option<usize> {
            if self.0 == 0 {
                None
            } else {
                Some(unsafe { self.lowest_set_bit_nonzero() })
            }
        }
        #[inline]
        pub unsafe fn lowest_set_bit_nonzero(self) -> usize {
            self.trailing_zeros()
        }

        #[inline]
        pub fn trailing_zeros(self) -> usize {
            self.0.trailing_zeros() as usize / BITMASK_STRIDE
        }

        #[inline]
        pub fn remove_lowest_bit(self) -> Self {
            BitMask(self.0 & (self.0 - 1))
        }
    }

    #[derive(Copy, Clone)]
    pub struct Group(x86::__m128i);
    impl Group {
        /// Number of bytes in the group.
        pub const WIDTH: usize = mem::size_of::<Self>();
        #[inline]
        pub unsafe fn load_aligned(ptr: *const u8) -> Self {
            // FIXME: use align_offset once it stabilizes
            debug_assert_eq!(ptr as usize & (mem::align_of::<Self>() - 1), 0);
            Group(x86::_mm_load_si128(ptr as *const _))
        }
        #[inline]
        pub fn match_full(&self) -> BitMask {
            self.match_empty_or_deleted().invert()
        }
        #[inline]
        pub fn match_empty_or_deleted(self) -> BitMask {
            unsafe {
                // A byte is EMPTY or DELETED iff the high bit is set
                BitMask(x86::_mm_movemask_epi8(self.0) as u16)
            }
        }
    }

    pub(crate) struct RawIterRange<T> {
        current_group: BitMask,
        data: Bucket<T>,
        next_ctrl: *const u8,
        end: *const u8,
    }

    impl<T> RawIterRange<T> {
        #[inline]
        unsafe fn new(ctrl: *const u8, data: Bucket<T>, len: usize) -> Self {
            debug_assert_ne!(len, 0);
            debug_assert_eq!(ctrl as usize % Group::WIDTH, 0);
            let end = ctrl.add(len);

            // Load the first group and advance ctrl to point to the next group
            let current_group = Group::load_aligned(ctrl).match_full();
            let next_ctrl = ctrl.add(Group::WIDTH);

            Self {
                current_group,
                data,
                next_ctrl,
                end,
            }
        }
    }

    impl<T> Iterator for RawIterRange<T> {
        type Item = Bucket<T>;

        fn next(&mut self) -> Option<Bucket<T>> {
            unsafe {
                loop {
                    if let Some(index) = self.current_group.lowest_set_bit() {
                        self.current_group = self.current_group.remove_lowest_bit();
                        return Some(self.data.next_n(index));
                    }

                    if self.next_ctrl >= self.end {
                        return None;
                    }

                    // We might read past self.end up to the next group boundary,
                    // but this is fine because it only occurs on tables smaller
                    // than the group size where the trailing control bytes are all
                    // EMPTY. On larger tables self.end is guaranteed to be aligned
                    // to the group size (since tables are power-of-two sized).
                    self.current_group = Group::load_aligned(self.next_ctrl).match_full();
                    self.data = self.data.next_n(Group::WIDTH);
                    self.next_ctrl = self.next_ctrl.add(Group::WIDTH);
                }
            }
        }

        #[inline]
        fn size_hint(&self) -> (usize, Option<usize>) {
            // We don't have an item count, so just guess based on the range size.
            (
                0,
                Some(unsafe { offset_from(self.end, self.next_ctrl) + Group::WIDTH }),
            )
        }
    }

    #[derive(Debug)]
    pub struct Bucket<T> {
        ptr: NonNull<T>,
    }

    impl<T> Clone for Bucket<T> {
        #[inline]
        fn clone(&self) -> Self {
            Self { ptr: self.ptr }
        }
    }

    impl<T> Bucket<T> {
        #[inline]
        unsafe fn from_base_index(base: NonNull<T>, index: usize) -> Self {
            let ptr = if std::mem::size_of::<T>() == 0 {
                // won't overflow because index must be less than length
                (index + 1) as *mut T
            } else {
                base.as_ptr().sub(index)
            };
            Self {
                ptr: NonNull::new_unchecked(ptr),
            }
        }

        #[inline]
        unsafe fn next_n(&self, offset: usize) -> Self {
            let ptr = if mem::size_of::<T>() == 0 {
                (self.ptr.as_ptr() as usize + offset) as *mut T
            } else {
                self.ptr.as_ptr().sub(offset)
            };
            Self {
                ptr: NonNull::new_unchecked(ptr),
            }
        }

        #[inline]
        pub unsafe fn read(&self) -> T {
            self.as_ptr().read()
        }
        #[inline]
        pub unsafe fn write(&self, val: T) {
            self.as_ptr().write(val);
        }
        #[inline]
        pub unsafe fn as_ref<'a>(&self) -> &'a T {
            &*self.as_ptr()
        }

        #[inline]
        pub unsafe fn as_mut<'a>(&self) -> &'a mut T {
            &mut *self.as_ptr()
        }

        #[inline]
        pub unsafe fn as_ptr(&self) -> *mut T {
            if mem::size_of::<T>() == 0 {
                // Just return an arbitrary ZST pointer which is properly aligned
                mem::align_of::<T>() as *mut T
            } else {
                self.ptr.as_ptr().sub(1)
            }
        }

        #[inline]
        unsafe fn to_base_index(&self, base: NonNull<T>) -> usize {
            if mem::size_of::<T>() == 0 {
                self.ptr.as_ptr() as usize - 1
            } else {
                offset_from(base.as_ptr(), self.ptr.as_ptr())
            }
        }
    }

    pub struct RawIter<T> {
        iter: RawIterRange<T>,
        items: usize,
    }

    impl<T> Iterator for RawIter<T> {
        type Item = Bucket<T>;

        #[inline]
        fn next(&mut self) -> Option<Bucket<T>> {
            if let Some(b) = self.iter.next() {
                self.items -= 1;
                Some(b)
            } else {
                // We don't check against items == 0 here to allow the
                // compiler to optimize away the item count entirely if the
                // iterator length is never queried.
                debug_assert_eq!(self.items, 0);
                None
            }
        }

        #[inline]
        fn size_hint(&self) -> (usize, Option<usize>) {
            (self.items, Some(self.items))
        }
    }

    impl<T: Abomonation + std::fmt::Debug + Default> Abomonation for RawTable<T> {
        #[inline]
        unsafe fn entomb<W: Write>(&self, write: &mut W) -> IOResult<()> {
            let size = mem::size_of::<T>();
            let buckets = self.buckets();
            let mut write2: Vec<u8> = vec![0; size*buckets];
            let mut write3: Vec<u8> = Vec::new();

            for from in self.iter() {
                let index = self.bucket_index(&from);
                let slice = std::slice::from_raw_parts(mem::transmute::<_, *const u8>(from.as_ref()), size);
                write2[(buckets-index-1)*size..(buckets-index)*size].copy_from_slice(slice);
                encode(&index, &mut write3)?;
                from.as_ref().entomb(&mut write3)?;
            }
            write.write_all(&write2);
            write.write_all(std::slice::from_raw_parts(self.ctrl(0), self.num_ctrl_bytes()));
            write.write_all(&write3);
            Ok(())
        }

        #[inline]
        unsafe fn exhume<'a,'b>(&'a mut self, bytes: &'b mut [u8]) -> Option<&'b mut [u8]> {
            if self.num_ctrl_bytes() > bytes.len() {
                return None;
            } else {
                let size = mem::size_of::<T>();
                let (_buckets, mut rest) = bytes.split_at_mut(self.buckets() * size);
                let (ctrl_bytes, temp) = rest.split_at_mut(self.num_ctrl_bytes());
                rest = temp;
                self.ctrl = NonNull::new_unchecked(ctrl_bytes.as_ptr() as *mut u8);
                for i in 0..self.items {
                    let (index, temp) = decode::<usize>(rest)?;
                    rest = temp;
                    let temp = self.bucket(*index).as_mut().exhume(rest)?;
                    rest = temp;
                }

                // below is essentially return Some(rest), but Some(rest) doesn't pass lifetime checker
                let l = rest.len();
                let total_len = bytes.len();
                Some(&mut bytes[total_len-l..])
            }
        }

        #[inline]
        fn extent(&self) -> usize {
            let mut ret = self.num_ctrl_bytes();
            unsafe {
                for from in self.iter() {
                    ret += mem::size_of::<usize>();
                    ret += from.as_ref().extent();
                }
            }
            ret
        }
    }

    impl<K: Abomonation + std::fmt::Debug + Default, V: Abomonation + std::fmt::Debug + Default> Abomonation for StdHashMap<K, V> {
        #[inline]
        unsafe fn entomb<W: Write>(&self, write: &mut W) -> IOResult<()> {
            self.base.table.entomb(write)?;
            Ok(())
        }

        #[inline]
        unsafe fn exhume<'a,'b>(&'a mut self, mut bytes: &'b mut [u8]) -> Option<&'b mut [u8]> {
            let temp = bytes;
            bytes = self.base.table.exhume(temp)?;
            Some(bytes)
        }

        #[inline]
        fn extent(&self) -> usize {
            self.base.table.extent()
        }
    }

    impl<K: Abomonation + std::fmt::Debug + Default, V: Abomonation + std::fmt::Debug + Default> Abomonation for HashMap<K, V> {
        #[inline]
        unsafe fn entomb<W: Write>(&self, write: &mut W) -> IOResult<()> {
            let hash_map: &StdHashMap<K, V> = std::mem::transmute(self);
            (*hash_map).entomb(write)?;
            Ok(())
        }

        #[inline]
        unsafe fn exhume<'a,'b>(&'a mut self, mut bytes: &'b mut [u8]) -> Option<&'b mut [u8]> {
            let temp = bytes;
            let hash_map: &mut StdHashMap<K, V> = std::mem::transmute(self);
            bytes = (*hash_map).base.table.exhume(temp)?;
            Some(bytes)
        }

        #[inline]
        fn extent(&self) -> usize {
            let hash_map: &StdHashMap<K, V> = unsafe { std::mem::transmute(self) };
            (*hash_map).base.table.extent()
        }
    }
}

'''
'''--- tests/tests.rs ---
extern crate abomonation;

use abomonation::*;

#[test] fn test_array() { _test_pass(vec![[0, 1, 2]; 1024]); }
#[test] fn test_nonzero() { _test_pass(vec![[std::num::NonZeroI32::new(1)]; 1024]); }
#[test] fn test_opt_vec() { _test_pass(vec![Some(vec![0,1,2]), None]); }
#[test] fn test_alignment() { _test_pass(vec![(format!("x"), vec![1,2,3]); 1024]); }
#[test] fn test_alignment_128() { _test_pass(vec![(format!("x"), vec![1u128,2,3]); 1024]); }
#[test] fn test_option_box_u64() { _test_pass(vec![Some(Box::new(0u64))]); }
#[test] fn test_option_vec() { _test_pass(vec![Some(vec![0, 1, 2])]); }
#[test] fn test_u32x4_pass() { _test_pass(vec![((1,2,3),vec![(0u32, 0u32, 0u32, 0u32); 1024])]); }
#[test] fn test_u64_pass() { _test_pass(vec![0u64; 1024]); }
#[test] fn test_u128_pass() { _test_pass(vec![0u128; 1024]); }
#[test] fn test_string_pass() { _test_pass(vec![format!("grawwwwrr!"); 1024]); }
#[test] fn test_vec_u_s_pass() { _test_pass(vec![vec![(0u64, format!("grawwwwrr!")); 32]; 32]); }

#[test] fn test_u64_fail() { _test_fail(vec![0u64; 1024]); }
#[test] fn test_u128_fail() { _test_fail(vec![0u128; 1024]); }
#[test] fn test_string_fail() { _test_fail(vec![format!("grawwwwrr!"); 1024]); }
#[test] fn test_vec_u_s_fail() { _test_fail(vec![vec![(0u64, format!("grawwwwrr!")); 32]; 32]); }

#[test] fn test_array_size() { _test_size(vec![[0, 1, 2]; 1024]); }
#[test] fn test_opt_vec_size() { _test_size(vec![Some(vec![0,1,2]), None]); }
#[test] fn test_alignment_size() { _test_size(vec![(format!("x"), vec![1,2,3]); 1024]); }
#[test] fn test_option_box_u64_size() { _test_size(vec![Some(Box::new(0u64))]); }
#[test] fn test_option_vec_size() { _test_size(vec![Some(vec![0, 1, 2])]); }
#[test] fn test_u32x4_size() { _test_size(vec![((1,2,3),vec![(0u32, 0u32, 0u32, 0u32); 1024])]); }
#[test] fn test_u64_size() { _test_size(vec![0u64; 1024]); }
#[test] fn test_u128_size() { _test_size(vec![0u128; 1024]); }
#[test] fn test_string_size() { _test_size(vec![format!("grawwwwrr!"); 1024]); }
#[test] fn test_vec_u_s_size() { _test_size(vec![vec![(0u64, format!("grawwwwrr!")); 32]; 32]); }

#[test]
fn test_phantom_data_for_non_abomonatable_type() {
    use std::marker::PhantomData;
    struct NotAbomonatable;
    _test_pass(PhantomData::<NotAbomonatable>::default());
}

fn _test_pass<T: Abomonation+Eq>(record: T) {
    let mut bytes = Vec::new();
    unsafe { encode(&record, &mut bytes).unwrap(); }
    {
        let (result, rest) = unsafe { decode::<T>(&mut bytes[..]) }.unwrap();
        assert!(&record == result);
        assert!(rest.len() == 0);
    }
}

fn _test_fail<T: Abomonation>(record: T) {
    let mut bytes = Vec::new();
    unsafe { encode(&record, &mut bytes).unwrap(); }
    bytes.pop();
    assert!(unsafe { decode::<T>(&mut bytes[..]) }.is_none());
}

fn _test_size<T: Abomonation>(record: T) {
    let mut bytes = Vec::new();
    unsafe { encode(&record, &mut bytes).unwrap(); }
    assert_eq!(bytes.len(), measure(&record));
}

#[derive(Eq, PartialEq)]
struct MyStruct {
    a: String,
    b: u64,
    c: Vec<u8>,
}

unsafe_abomonate!(MyStruct : a, b, c);

#[test]
fn test_macro() {
    // create some test data out of abomonation-approved types
    let record = MyStruct{ a: "test".to_owned(), b: 0, c: vec![0, 1, 2] };

    // encode vector into a Vec<u8>
    let mut bytes = Vec::new();
    unsafe { encode(&record, &mut bytes).unwrap(); }

    // decode a &Vec<(u64, String)> from binary data
    if let Some((result, rest)) = unsafe { decode::<MyStruct>(&mut bytes) } {
        assert!(result == &record);
        assert!(rest.len() == 0);
    }
}

use crate::abomonated::Abomonated;
enum MyStructWrap {
    Original(MyStruct),
    Abomonated(Abomonated<MyStruct, Vec<u8>>),
}
use std::ops::Deref;

impl Deref for MyStructWrap {
    type Target = MyStruct;
    fn deref(&self) -> &Self::Target {
        match self {
            Self::Original(m) => m,
            Self::Abomonated(a) => &*a,
        }
    }
}

fn de<'a>(bytes: Vec<u8>) -> MyStructWrap {
    MyStructWrap::Abomonated(unsafe { Abomonated::new(bytes) }.unwrap())
}

#[test]
fn test_abominated_enum() {
    // create some test data out of abomonation-approved types
    let record = MyStruct{ a: "test".to_owned(), b: 0, c: vec![0, 1, 2] };

    // encode vector into a Vec<u8>
    let mut bytes = Vec::new();
    unsafe { encode(&record, &mut bytes).unwrap(); }
    let b = bytes.clone();

    let result = de(bytes);
    assert!(*result == record);
}

#[test]
fn test_macro_size() {
    // create some test data out of abomonation-approved types
    let record = MyStruct{ a: "test".to_owned(), b: 0, c: vec![0, 1, 2] };

    // encode vector into a Vec<u8>
    let mut bytes = Vec::new();
    unsafe { encode(&record, &mut bytes).unwrap(); }
    assert_eq!(bytes.len(), measure(&record));
}

#[test]
fn test_multiple_encode_decode() {
    let mut bytes = Vec::new();
    unsafe { encode(&0u32, &mut bytes).unwrap(); }
    unsafe { encode(&7u64, &mut bytes).unwrap(); }
    unsafe { encode(&vec![1,2,3], &mut bytes).unwrap(); }
    unsafe { encode(&"grawwwwrr".to_owned(), &mut bytes).unwrap(); }

    let (t, r) = unsafe { decode::<u32>(&mut bytes) }.unwrap(); assert!(*t == 0);
    let (t, r) = unsafe { decode::<u64>(r) }.unwrap(); assert!(*t == 7);
    let (t, r) = unsafe { decode::<Vec<i32>>(r) }.unwrap(); assert!(*t == vec![1,2,3]);
    let (t, _r) = unsafe { decode::<String>(r) }.unwrap(); assert!(*t == "grawwwwrr".to_owned());
}

#[test]
fn test_net_types() {

    use std::net::{SocketAddr, IpAddr, Ipv4Addr, Ipv6Addr};

    let socket_addr4 = SocketAddr::new(IpAddr::V4(Ipv4Addr::new(128, 0, 0, 1)), 1234);
    let socket_addr6 = SocketAddr::new(IpAddr::V6(Ipv6Addr::LOCALHOST), 1234);

    let mut bytes = Vec::new();

    unsafe { encode(&socket_addr4, &mut bytes).unwrap(); }
    unsafe { encode(&socket_addr6, &mut bytes).unwrap(); }

    let (t, r) = unsafe { decode::<SocketAddr>(&mut bytes) }.unwrap(); assert!(*t == socket_addr4);
    let (t, _r) = unsafe { decode::<SocketAddr>(r) }.unwrap(); assert!(*t == socket_addr6);
}

#[test]
fn test_hash_map() {
    use std::collections::HashMap;

    let mut h = HashMap::new();
    h.insert("aaaaa".to_string(), "3".to_string());
    h.insert("bbbbbb".to_string(), "4".to_string());
    let mut bytes = Vec::new();
    unsafe { encode(&h, &mut bytes).unwrap(); }
    let (t, r) = unsafe { decode::<HashMap<String, String>>(&mut bytes) }.unwrap();
    std::mem::forget(h);
    assert!(r.len() == 0);
    assert!(t.len() == 2);
    assert!(t.get("aaaaa") == Some(&"3".to_string()));
    assert!(t.get("bbbbbb") == Some(&"4".to_string()));

    // Test re-encode
    let mut bytes2 = Vec::new();
    unsafe { encode(t, &mut bytes2).unwrap(); }
    assert!(bytes == bytes2);
}

'''