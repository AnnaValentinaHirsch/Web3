*GitHub Repository "nftdw/near-processor"*

'''--- README.md ---
# Near NFT Processor

Near NFT Minter using Arweave and Near Protocol

## Architecture
![](./docs/assets/nft-minter-infra.png)

## Jobs

- [Arweave Processor](./jobs/arweave_processor/README.md) - Service responsible for uploading media to the permaweb. Check [arweave processor local demo](./docs/assets/arweave_processor/DEMO.md) for instructions on how to simulate the service on local machine.
- [Near Processor](./jobs/near_processor/README.md) - Service responsible for minting the nfts and sending them to their respective owners.

## Contracts
- [NFT Contract](./contracts/nft/README.md)

## Production
### Requirements
- [docker/docker-compose](https://docs.docker.com/get-docker/)

### Configuration
Before running, a dotenv file is required to configure each service.
Kindly check the documentation for available keys to configure.

- [arweave_processor.env](./jobs/arweave_processor/README.md#environment-configuration)
- [near_processor.env](./jobs/near_processor/README.md#environment-configuration)

### Build the images

Build the production images with docker-compose
```bash
    docker-compose -f docker-compose-production.yaml build 
```

### Run
Once arweave_processor.env and near_processor.env is configured on the working directory, run the service using docker-compose.
```bash
    docker-compose -f docker-compose-production.yaml up -d
    # OR run each service with additional replica
    docker-compose -f docker-compose-production.yaml up -d --scale arweave_processor=<REPLICA_COUNT> --scale near_processor=<REPLICA_COUNT>
```
 
'''
'''--- conf/clickhouse/clickhouse-server/config.d/docker_related_config.xml ---
<clickhouse>
     <!-- Listen wildcard address to allow accepting connections from other containers and host network. -->
    <listen_host>::</listen_host>
    <listen_host>0.0.0.0</listen_host>
    <listen_try>1</listen_try>

    <!--
    <logger>
        <console>1</console>
    </logger>
    -->
</clickhouse>

'''
'''--- conf/clickhouse/clickhouse-server/config.d/prometheus.xml ---
<clickhouse>
    <prometheus>
        <endpoint>/metrics</endpoint>
        <port>9363</port>
        <metrics>true</metrics>
        <events>true</events>
        <asynchronous_metrics>true</asynchronous_metrics>
        <status_info>true</status_info>
    </prometheus>
</clickhouse>
'''
'''--- conf/clickhouse/clickhouse-server/config.xml ---
<!--
  NOTE: User and query level settings are set up in "users.xml" file.
  If you have accidentally specified user-level settings here, server won't start.
  You can either move the settings to the right place inside "users.xml" file
   or add <skip_check_for_incorrect_settings>1</skip_check_for_incorrect_settings> here.
-->
<clickhouse>
    <logger>
        <!-- Possible levels [1]:

          - none (turns off logging)
          - fatal
          - critical
          - error
          - warning
          - notice
          - information
          - debug
          - trace
          - test (not for production usage)

            [1]: https://github.com/pocoproject/poco/blob/poco-1.9.4-release/Foundation/include/Poco/Logger.h#L105-L114
        -->
        <level>trace</level>
        <log>/var/log/clickhouse-server/clickhouse-server.log</log>
        <errorlog>/var/log/clickhouse-server/clickhouse-server.err.log</errorlog>
        <!-- Rotation policy
             See https://github.com/pocoproject/poco/blob/poco-1.9.4-release/Foundation/include/Poco/FileChannel.h#L54-L85
          -->
        <size>1000M</size>
        <count>10</count>

        <!-- <console>1</console> --> <!-- Default behavior is autodetection (log to console if not daemon mode and is tty) -->

        <!-- Per level overrides (legacy):

        For example to suppress logging of the ConfigReloader you can use:
        NOTE: levels.logger is reserved, see below.
        -->
        <!--
        <levels>
          <ConfigReloader>none</ConfigReloader>
        </levels>
        -->

        <!-- Per level overrides:

        For example to suppress logging of the RBAC for default user you can use:
        (But please note that the logger name maybe changed from version to version, even after minor upgrade)
        -->
        <!--
        <levels>
          <logger>
            <name>ContextAccess (default)</name>
            <level>none</level>
          </logger>
          <logger>
            <name>DatabaseOrdinary (test)</name>
            <level>none</level>
          </logger>
        </levels>
        -->
        <!-- Structured log formatting:
        You can specify log format(for now, JSON only). In that case, the console log will be printed
        in specified format like JSON.
        For example, as below:
        {"date_time":"1650918987.180175","thread_name":"#1","thread_id":"254545","level":"Trace","query_id":"","logger_name":"BaseDaemon","message":"Received signal 2","source_file":"../base/daemon/BaseDaemon.cpp; virtual void SignalListener::run()","source_line":"192"}
        To enable JSON logging support, please uncomment the entire <formatting> tag below.

        a) You can modify key names by changing values under tag values inside <names> tag.
        For example, to change DATE_TIME to MY_DATE_TIME, you can do like:
            <date_time>MY_DATE_TIME</date_time>
        b) You can stop unwanted log properties to appear in logs. To do so, you can simply comment out (recommended)
        that property from this file.
        For example, if you do not want your log to print query_id, you can comment out only <query_id> tag.
        However, if you comment out all the tags under <names>, the program will print default values for as
        below.
        -->
        <!-- <formatting>
            <type>json</type>
            <names>
                <date_time>date_time</date_time>
                <thread_name>thread_name</thread_name>
                <thread_id>thread_id</thread_id>
                <level>level</level>
                <query_id>query_id</query_id>
                <logger_name>logger_name</logger_name>
                <message>message</message>
                <source_file>source_file</source_file>
                <source_line>source_line</source_line>
            </names>
        </formatting> -->
    </logger>

    <!-- Add headers to response in options request. OPTIONS method is used in CORS preflight requests. -->
    <!-- It is off by default. Next headers are obligate for CORS.-->
    <!-- http_options_response>
        <header>
            <name>Access-Control-Allow-Origin</name>
            <value>*</value>
        </header>
        <header>
            <name>Access-Control-Allow-Headers</name>
            <value>origin, x-requested-with</value>
        </header>
        <header>
            <name>Access-Control-Allow-Methods</name>
            <value>POST, GET, OPTIONS</value>
        </header>
        <header>
            <name>Access-Control-Max-Age</name>
            <value>86400</value>
        </header>
    </http_options_response -->

    <!-- It is the name that will be shown in the clickhouse-client.
         By default, anything with "production" will be highlighted in red in query prompt.
    -->
    <!--display_name>production</display_name-->

    <!-- Port for HTTP API. See also 'https_port' for secure connections.
         This interface is also used by ODBC and JDBC drivers (DataGrip, Dbeaver, ...)
         and by most of web interfaces (embedded UI, Grafana, Redash, ...).
      -->
    <http_port>8123</http_port>

    <!-- Port for interaction by native protocol with:
         - clickhouse-client and other native ClickHouse tools (clickhouse-benchmark, clickhouse-copier);
         - clickhouse-server with other clickhouse-servers for distributed query processing;
         - ClickHouse drivers and applications supporting native protocol
         (this protocol is also informally called as "the TCP protocol");
         See also 'tcp_port_secure' for secure connections.
    -->
    <tcp_port>9000</tcp_port>
    

    <!-- Compatibility with MySQL protocol.
         ClickHouse will pretend to be MySQL for applications connecting to this port.
    -->
    <mysql_port>9004</mysql_port>

    <!-- Compatibility with PostgreSQL protocol.
         ClickHouse will pretend to be PostgreSQL for applications connecting to this port.
    -->
    <postgresql_port>9005</postgresql_port>

    <!-- HTTP API with TLS (HTTPS).
         You have to configure certificate to enable this interface.
         See the openSSL section below.
    -->
    <!-- <https_port>8443</https_port> -->

    <!-- Native interface with TLS.
         You have to configure certificate to enable this interface.
         See the openSSL section below.
    -->
    <!-- <tcp_port_secure>9440</tcp_port_secure> -->

    <!-- Native interface wrapped with PROXYv1 protocol
         PROXYv1 header sent for every connection.
         ClickHouse will extract information about proxy-forwarded client address from the header.
    -->
    <!-- <tcp_with_proxy_port>9011</tcp_with_proxy_port> -->

    <!-- Port for communication between replicas. Used for data exchange.
         It provides low-level data access between servers.
         This port should not be accessible from untrusted networks.
         See also 'interserver_http_credentials'.
         Data transferred over connections to this port should not go through untrusted networks.
         See also 'interserver_https_port'.
      -->
    <interserver_http_port>9009</interserver_http_port>

    <!-- Port for communication between replicas with TLS.
         You have to configure certificate to enable this interface.
         See the openSSL section below.
         See also 'interserver_http_credentials'.
      -->
    <!-- <interserver_https_port>9010</interserver_https_port> -->

    <!-- Hostname that is used by other replicas to request this server.
         If not specified, then it is determined analogous to 'hostname -f' command.
         This setting could be used to switch replication to another network interface
         (the server may be connected to multiple networks via multiple addresses)
      -->

    <!--
    <interserver_http_host>example.clickhouse.com</interserver_http_host>
    -->

    <!-- You can specify credentials for authenthication between replicas.
         This is required when interserver_https_port is accessible from untrusted networks,
         and also recommended to avoid SSRF attacks from possibly compromised services in your network.
      -->
    <!--<interserver_http_credentials>
        <user>interserver</user>
        <password></password>
    </interserver_http_credentials>-->

    <!-- Listen specified address.
         Use :: (wildcard IPv6 address), if you want to accept connections both with IPv4 and IPv6 from everywhere.
         Notes:
         If you open connections from wildcard address, make sure that at least one of the following measures applied:
         - server is protected by firewall and not accessible from untrusted networks;
         - all users are restricted to subset of network addresses (see users.xml);
         - all users have strong passwords, only secure (TLS) interfaces are accessible, or connections are only made via TLS interfaces.
         - users without password have readonly access.
         See also: https://www.shodan.io/search?query=clickhouse
      -->
    <!-- <listen_host>::</listen_host> -->

    <!-- Same for hosts without support for IPv6: -->
    <!-- <listen_host>0.0.0.0</listen_host> -->

    <!-- Default values - try listen localhost on IPv4 and IPv6. -->
    <!--
    <listen_host>::1</listen_host>
    <listen_host>127.0.0.1</listen_host>
    -->

    <!-- <interserver_listen_host>::</interserver_listen_host> -->
    <!-- Listen host for communication between replicas. Used for data exchange -->
    <!-- Default values - equal to listen_host -->

    <!-- Don't exit if IPv6 or IPv4 networks are unavailable while trying to listen. -->
    <!-- <listen_try>0</listen_try> -->

    <!-- Allow multiple servers to listen on the same address:port. This is not recommended.
      -->
    <!-- <listen_reuse_port>0</listen_reuse_port> -->

    <!-- <listen_backlog>4096</listen_backlog> -->

    <max_connections>4096</max_connections>

    <!-- For 'Connection: keep-alive' in HTTP 1.1 -->
    <keep_alive_timeout>3</keep_alive_timeout>

    <!-- gRPC protocol (see src/Server/grpc_protos/clickhouse_grpc.proto for the API) -->
    <!-- <grpc_port>9100</grpc_port> -->
    <grpc>
        <enable_ssl>false</enable_ssl>

        <!-- The following two files are used only if enable_ssl=1 -->
        <ssl_cert_file>/path/to/ssl_cert_file</ssl_cert_file>
        <ssl_key_file>/path/to/ssl_key_file</ssl_key_file>

        <!-- Whether server will request client for a certificate -->
        <ssl_require_client_auth>false</ssl_require_client_auth>

        <!-- The following file is used only if ssl_require_client_auth=1 -->
        <ssl_ca_cert_file>/path/to/ssl_ca_cert_file</ssl_ca_cert_file>

        <!-- Default transport compression type (can be overridden by client, see the transport_compression_type field in QueryInfo).
             Supported algorithms: none, deflate, gzip, stream_gzip -->
        <transport_compression_type>none</transport_compression_type>

        <!-- Default transport compression level. Supported levels: 0..3 -->
        <transport_compression_level>0</transport_compression_level>

        <!-- Send/receive message size limits in bytes. -1 means unlimited -->
        <max_send_message_size>-1</max_send_message_size>
        <max_receive_message_size>-1</max_receive_message_size>

        <!-- Enable if you want very detailed logs -->
        <verbose_logs>false</verbose_logs>
    </grpc>

    <!-- Used with https_port and tcp_port_secure. Full ssl options list: https://github.com/ClickHouse-Extras/poco/blob/master/NetSSL_OpenSSL/include/Poco/Net/SSLManager.h#L71 -->
    <openSSL>
        <server> <!-- Used for https server AND secure tcp port -->
            <!-- openssl req -subj "/CN=localhost" -new -newkey rsa:2048 -days 365 -nodes -x509 -keyout /etc/clickhouse-server/server.key -out /etc/clickhouse-server/server.crt -->
            <!-- <certificateFile>/etc/clickhouse-server/server.crt</certificateFile>
            <privateKeyFile>/etc/clickhouse-server/server.key</privateKeyFile> -->
            <!-- dhparams are optional. You can delete the <dhParamsFile> element.
                 To generate dhparams, use the following command:
                  openssl dhparam -out /etc/clickhouse-server/dhparam.pem 4096
                 Only file format with BEGIN DH PARAMETERS is supported.
              -->
            <!-- <dhParamsFile>/etc/clickhouse-server/dhparam.pem</dhParamsFile>-->
            <verificationMode>none</verificationMode>
            <loadDefaultCAFile>true</loadDefaultCAFile>
            <cacheSessions>true</cacheSessions>
            <disableProtocols>sslv2,sslv3</disableProtocols>
            <preferServerCiphers>true</preferServerCiphers>
        </server>

        <client> <!-- Used for connecting to https dictionary source and secured Zookeeper communication -->
            <loadDefaultCAFile>true</loadDefaultCAFile>
            <cacheSessions>true</cacheSessions>
            <disableProtocols>sslv2,sslv3</disableProtocols>
            <preferServerCiphers>true</preferServerCiphers>
            <!-- Use for self-signed: <verificationMode>none</verificationMode> -->
            <invalidCertificateHandler>
                <!-- Use for self-signed: <name>AcceptCertificateHandler</name> -->
                <name>RejectCertificateHandler</name>
            </invalidCertificateHandler>
        </client>
    </openSSL>

    <!-- Default root page on http[s] server. For example load UI from https://tabix.io/ when opening http://localhost:8123 -->
    <!--
    <http_server_default_response><![CDATA[<html ng-app="SMI2"><head><base href="http://ui.tabix.io/"></head><body><div ui-view="" class="content-ui"></div><script src="http://loader.tabix.io/master.js"></script></body></html>]]></http_server_default_response>
    -->

    <!-- The maximum number of query processing threads, excluding threads for retrieving data from remote servers, allowed to run all queries.
         This is not a hard limit. In case if the limit is reached the query will still get at least one thread to run.
         Query can upscale to desired number of threads during execution if more threads become available.
    -->
    <concurrent_threads_soft_limit_num>0</concurrent_threads_soft_limit_num>
    <concurrent_threads_soft_limit_ratio_to_cores>0</concurrent_threads_soft_limit_ratio_to_cores>

    <!-- Maximum number of concurrent queries. -->
    <max_concurrent_queries>100</max_concurrent_queries>

    <!-- Maximum memory usage (resident set size) for server process.
         Zero value or unset means default. Default is "max_server_memory_usage_to_ram_ratio" of available physical RAM.
         If the value is larger than "max_server_memory_usage_to_ram_ratio" of available physical RAM, it will be cut down.

         The constraint is checked on query execution time.
         If a query tries to allocate memory and the current memory usage plus allocation is greater
          than specified threshold, exception will be thrown.

         It is not practical to set this constraint to small values like just a few gigabytes,
          because memory allocator will keep this amount of memory in caches and the server will deny service of queries.
      -->
    <max_server_memory_usage>0</max_server_memory_usage>

    <!-- Maximum number of threads in the Global thread pool.
    This will default to a maximum of 10000 threads if not specified.
    This setting will be useful in scenarios where there are a large number
    of distributed queries that are running concurrently but are idling most
    of the time, in which case a higher number of threads might be required.
    -->

    <max_thread_pool_size>10000</max_thread_pool_size>

    <!-- Number of workers to recycle connections in background (see also drain_timeout).
         If the pool is full, connection will be drained synchronously. -->
    <!-- <max_threads_for_connection_collector>10</max_threads_for_connection_collector> -->

    <!-- On memory constrained environments you may have to set this to value larger than 1.
      -->
    <max_server_memory_usage_to_ram_ratio>0.9</max_server_memory_usage_to_ram_ratio>

    <!-- Simple server-wide memory profiler. Collect a stack trace at every peak allocation step (in bytes).
         Data will be stored in system.trace_log table with query_id = empty string.
         Zero means disabled.
      -->
    <total_memory_profiler_step>4194304</total_memory_profiler_step>

    <!-- Collect random allocations and deallocations and write them into system.trace_log with 'MemorySample' trace_type.
         The probability is for every alloc/free regardless to the size of the allocation.
         Note that sampling happens only when the amount of untracked memory exceeds the untracked memory limit,
          which is 4 MiB by default but can be lowered if 'total_memory_profiler_step' is lowered.
         You may want to set 'total_memory_profiler_step' to 1 for extra fine grained sampling.
      -->
    <total_memory_tracker_sample_probability>0</total_memory_tracker_sample_probability>

    <!-- Set limit on number of open files (default: maximum). This setting makes sense on Mac OS X because getrlimit() fails to retrieve
         correct maximum value. -->
    <!-- <max_open_files>262144</max_open_files> -->

    <!-- Size of cache of uncompressed blocks of data, used in tables of MergeTree family.
         In bytes. Cache is single for server. Memory is allocated only on demand.
         Cache is used when 'use_uncompressed_cache' user setting turned on (off by default).
         Uncompressed cache is advantageous only for very short queries and in rare cases.

         Note: uncompressed cache can be pointless for lz4, because memory bandwidth
         is slower than multi-core decompression on some server configurations.
         Enabling it can sometimes paradoxically make queries slower.
      -->
    <uncompressed_cache_size>8589934592</uncompressed_cache_size>

    <!-- Approximate size of mark cache, used in tables of MergeTree family.
         In bytes. Cache is single for server. Memory is allocated only on demand.
         You should not lower this value.
      -->
    <mark_cache_size>5368709120</mark_cache_size>

    <!-- If you enable the `min_bytes_to_use_mmap_io` setting,
         the data in MergeTree tables can be read with mmap to avoid copying from kernel to userspace.
         It makes sense only for large files and helps only if data reside in page cache.
         To avoid frequent open/mmap/munmap/close calls (which are very expensive due to consequent page faults)
         and to reuse mappings from several threads and queries,
         the cache of mapped files is maintained. Its size is the number of mapped regions (usually equal to the number of mapped files).
         The amount of data in mapped files can be monitored
         in system.metrics, system.metric_log by the MMappedFiles, MMappedFileBytes metrics
         and in system.asynchronous_metrics, system.asynchronous_metrics_log by the MMapCacheCells metric,
         and also in system.events, system.processes, system.query_log, system.query_thread_log, system.query_views_log by the
         CreatedReadBufferMMap, CreatedReadBufferMMapFailed, MMappedFileCacheHits, MMappedFileCacheMisses events.
         Note that the amount of data in mapped files does not consume memory directly and is not accounted
         in query or server memory usage - because this memory can be discarded similar to OS page cache.
         The cache is dropped (the files are closed) automatically on removal of old parts in MergeTree,
         also it can be dropped manually by the SYSTEM DROP MMAP CACHE query.
      -->
    <mmap_cache_size>1000</mmap_cache_size>

    <!-- Cache size in bytes for compiled expressions.-->
    <compiled_expression_cache_size>134217728</compiled_expression_cache_size>

    <!-- Cache size in elements for compiled expressions.-->
    <compiled_expression_cache_elements_size>10000</compiled_expression_cache_elements_size>

    <!-- Path to data directory, with trailing slash. -->
    <path>/var/lib/clickhouse/</path>

    <!-- Multi-disk configuration example: -->
    <!--
    <storage_configuration>
        <disks>
            <default>
                <keep_free_space_bytes>0</keep_free_space_bytes>
            </default>
            <data>
                <path>/data/</path>
                <keep_free_space_bytes>0</keep_free_space_bytes>
            </data>
            <s3>
                <type>s3</type>
                <endpoint>http://path/to/endpoint</endpoint>
                <access_key_id>your_access_key_id</access_key_id>
                <secret_access_key>your_secret_access_key</secret_access_key>
            </s3>
            <blob_storage_disk>
                <type>azure_blob_storage</type>
                <storage_account_url>http://account.blob.core.windows.net</storage_account_url>
                <container_name>container</container_name>
                <account_name>account</account_name>
                <account_key>pass123</account_key>
                <metadata_path>/var/lib/clickhouse/disks/blob_storage_disk/</metadata_path>
                <cache_enabled>true</cache_enabled>
                <cache_path>/var/lib/clickhouse/disks/blob_storage_disk/cache/</cache_path>
                <skip_access_check>false</skip_access_check>
            </blob_storage_disk>
        </disks>

        <policies>
            <all>
                <volumes>
                    <main>
                        <disk>default</disk>
                        <disk>data</disk>
                        <disk>s3</disk>
                        <disk>blob_storage_disk</disk>

                        <max_data_part_size_bytes></max_data_part_size_bytes>
                        <max_data_part_size_ratio></max_data_part_size_ratio>
                        <perform_ttl_move_on_insert>true</perform_ttl_move_on_insert>
                        <prefer_not_to_merge>false</prefer_not_to_merge>
                        <load_balancing>round_robin</load_balancing>
                    </main>
                </volumes>
                <move_factor>0.2</move_factor>
            </all>
        </policies>
    </storage_configuration>
    -->

    <!-- Path to temporary data for processing hard queries. -->
    <tmp_path>/var/lib/clickhouse/tmp/</tmp_path>

    <!-- Disable AuthType plaintext_password and no_password for ACL. -->
    <!-- <allow_plaintext_password>0</allow_plaintext_password> -->
    <!-- <allow_no_password>0</allow_no_password> -->`

    <!-- Policy from the <storage_configuration> for the temporary files.
         If not set <tmp_path> is used, otherwise <tmp_path> is ignored.

         Notes:
         - move_factor              is ignored
         - keep_free_space_bytes    is ignored
         - max_data_part_size_bytes is ignored
         - you must have exactly one volume in that policy
    -->
    <!-- <tmp_policy>tmp</tmp_policy> -->

    <!-- Directory with user provided files that are accessible by 'file' table function. -->
    <user_files_path>/var/lib/clickhouse/user_files/</user_files_path>

    <!-- LDAP server definitions. -->
    <ldap_servers>
        <!-- List LDAP servers with their connection parameters here to later 1) use them as authenticators for dedicated local users,
              who have 'ldap' authentication mechanism specified instead of 'password', or to 2) use them as remote user directories.
             Parameters:
                host - LDAP server hostname or IP, this parameter is mandatory and cannot be empty.
                port - LDAP server port, default is 636 if enable_tls is set to true, 389 otherwise.
                bind_dn - template used to construct the DN to bind to.
                        The resulting DN will be constructed by replacing all '{user_name}' substrings of the template with the actual
                         user name during each authentication attempt.
                user_dn_detection - section with LDAP search parameters for detecting the actual user DN of the bound user.
                        This is mainly used in search filters for further role mapping when the server is Active Directory. The
                         resulting user DN will be used when replacing '{user_dn}' substrings wherever they are allowed. By default,
                         user DN is set equal to bind DN, but once search is performed, it will be updated with to the actual detected
                         user DN value.
                    base_dn - template used to construct the base DN for the LDAP search.
                            The resulting DN will be constructed by replacing all '{user_name}' and '{bind_dn}' substrings
                             of the template with the actual user name and bind DN during the LDAP search.
                    scope - scope of the LDAP search.
                            Accepted values are: 'base', 'one_level', 'children', 'subtree' (the default).
                    search_filter - template used to construct the search filter for the LDAP search.
                            The resulting filter will be constructed by replacing all '{user_name}', '{bind_dn}', and '{base_dn}'
                             substrings of the template with the actual user name, bind DN, and base DN during the LDAP search.
                            Note, that the special characters must be escaped properly in XML.
                verification_cooldown - a period of time, in seconds, after a successful bind attempt, during which a user will be assumed
                         to be successfully authenticated for all consecutive requests without contacting the LDAP server.
                        Specify 0 (the default) to disable caching and force contacting the LDAP server for each authentication request.
                enable_tls - flag to trigger use of secure connection to the LDAP server.
                        Specify 'no' for plain text (ldap://) protocol (not recommended).
                        Specify 'yes' for LDAP over SSL/TLS (ldaps://) protocol (recommended, the default).
                        Specify 'starttls' for legacy StartTLS protocol (plain text (ldap://) protocol, upgraded to TLS).
                tls_minimum_protocol_version - the minimum protocol version of SSL/TLS.
                        Accepted values are: 'ssl2', 'ssl3', 'tls1.0', 'tls1.1', 'tls1.2' (the default).
                tls_require_cert - SSL/TLS peer certificate verification behavior.
                        Accepted values are: 'never', 'allow', 'try', 'demand' (the default).
                tls_cert_file - path to certificate file.
                tls_key_file - path to certificate key file.
                tls_ca_cert_file - path to CA certificate file.
                tls_ca_cert_dir - path to the directory containing CA certificates.
                tls_cipher_suite - allowed cipher suite (in OpenSSL notation).
             Example:
                <my_ldap_server>
                    <host>localhost</host>
                    <port>636</port>
                    <bind_dn>uid={user_name},ou=users,dc=example,dc=com</bind_dn>
                    <verification_cooldown>300</verification_cooldown>
                    <enable_tls>yes</enable_tls>
                    <tls_minimum_protocol_version>tls1.2</tls_minimum_protocol_version>
                    <tls_require_cert>demand</tls_require_cert>
                    <tls_cert_file>/path/to/tls_cert_file</tls_cert_file>
                    <tls_key_file>/path/to/tls_key_file</tls_key_file>
                    <tls_ca_cert_file>/path/to/tls_ca_cert_file</tls_ca_cert_file>
                    <tls_ca_cert_dir>/path/to/tls_ca_cert_dir</tls_ca_cert_dir>
                    <tls_cipher_suite>ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:AES256-GCM-SHA384</tls_cipher_suite>
                </my_ldap_server>
             Example (typical Active Directory with configured user DN detection for further role mapping):
                <my_ad_server>
                    <host>localhost</host>
                    <port>389</port>
                    <bind_dn>EXAMPLE\{user_name}</bind_dn>
                    <user_dn_detection>
                        <base_dn>CN=Users,DC=example,DC=com</base_dn>
                        <search_filter>(&amp;(objectClass=user)(sAMAccountName={user_name}))</search_filter>
                    </user_dn_detection>
                    <enable_tls>no</enable_tls>
                </my_ad_server>
        -->
    </ldap_servers>

    <!-- To enable Kerberos authentication support for HTTP requests (GSS-SPNEGO), for those users who are explicitly configured
          to authenticate via Kerberos, define a single 'kerberos' section here.
         Parameters:
            principal - canonical service principal name, that will be acquired and used when accepting security contexts.
                    This parameter is optional, if omitted, the default principal will be used.
                    This parameter cannot be specified together with 'realm' parameter.
            realm - a realm, that will be used to restrict authentication to only those requests whose initiator's realm matches it.
                    This parameter is optional, if omitted, no additional filtering by realm will be applied.
                    This parameter cannot be specified together with 'principal' parameter.
         Example:
            <kerberos />
         Example:
            <kerberos>
                <principal>HTTP/clickhouse.example.com@EXAMPLE.COM</principal>
            </kerberos>
         Example:
            <kerberos>
                <realm>EXAMPLE.COM</realm>
            </kerberos>
    -->

    <!-- Sources to read users, roles, access rights, profiles of settings, quotas. -->
    <user_directories>
        <users_xml>
            <!-- Path to configuration file with predefined users. -->
            <path>users.xml</path>
        </users_xml>
        <local_directory>
            <!-- Path to folder where users created by SQL commands are stored. -->
            <path>/var/lib/clickhouse/access/</path>
        </local_directory>

        <!-- To add an LDAP server as a remote user directory of users that are not defined locally, define a single 'ldap' section
              with the following parameters:
                server - one of LDAP server names defined in 'ldap_servers' config section above.
                        This parameter is mandatory and cannot be empty.
                roles - section with a list of locally defined roles that will be assigned to each user retrieved from the LDAP server.
                        If no roles are specified here or assigned during role mapping (below), user will not be able to perform any
                         actions after authentication.
                role_mapping - section with LDAP search parameters and mapping rules.
                        When a user authenticates, while still bound to LDAP, an LDAP search is performed using search_filter and the
                         name of the logged in user. For each entry found during that search, the value of the specified attribute is
                         extracted. For each attribute value that has the specified prefix, the prefix is removed, and the rest of the
                         value becomes the name of a local role defined in ClickHouse, which is expected to be created beforehand by
                         CREATE ROLE command.
                        There can be multiple 'role_mapping' sections defined inside the same 'ldap' section. All of them will be
                         applied.
                    base_dn - template used to construct the base DN for the LDAP search.
                            The resulting DN will be constructed by replacing all '{user_name}', '{bind_dn}', and '{user_dn}'
                             substrings of the template with the actual user name, bind DN, and user DN during each LDAP search.
                    scope - scope of the LDAP search.
                            Accepted values are: 'base', 'one_level', 'children', 'subtree' (the default).
                    search_filter - template used to construct the search filter for the LDAP search.
                            The resulting filter will be constructed by replacing all '{user_name}', '{bind_dn}', '{user_dn}', and
                             '{base_dn}' substrings of the template with the actual user name, bind DN, user DN, and base DN during
                             each LDAP search.
                            Note, that the special characters must be escaped properly in XML.
                    attribute - attribute name whose values will be returned by the LDAP search. 'cn', by default.
                    prefix - prefix, that will be expected to be in front of each string in the original list of strings returned by
                             the LDAP search. Prefix will be removed from the original strings and resulting strings will be treated
                             as local role names. Empty, by default.
             Example:
                <ldap>
                    <server>my_ldap_server</server>
                    <roles>
                        <my_local_role1 />
                        <my_local_role2 />
                    </roles>
                    <role_mapping>
                        <base_dn>ou=groups,dc=example,dc=com</base_dn>
                        <scope>subtree</scope>
                        <search_filter>(&amp;(objectClass=groupOfNames)(member={bind_dn}))</search_filter>
                        <attribute>cn</attribute>
                        <prefix>clickhouse_</prefix>
                    </role_mapping>
                </ldap>
             Example (typical Active Directory with role mapping that relies on the detected user DN):
                <ldap>
                    <server>my_ad_server</server>
                    <role_mapping>
                        <base_dn>CN=Users,DC=example,DC=com</base_dn>
                        <attribute>CN</attribute>
                        <scope>subtree</scope>
                        <search_filter>(&amp;(objectClass=group)(member={user_dn}))</search_filter>
                        <prefix>clickhouse_</prefix>
                    </role_mapping>
                </ldap>
        -->
    </user_directories>

    <access_control_improvements>
        <!-- Enables logic that users without permissive row policies can still read rows using a SELECT query.
             For example, if there two users A, B and a row policy is defined only for A, then
             if this setting is true the user B will see all rows, and if this setting is false the user B will see no rows.
             By default this setting is false for compatibility with earlier access configurations. -->
        <users_without_row_policies_can_read_rows>false</users_without_row_policies_can_read_rows>

        <!-- By default, for backward compatibility ON CLUSTER queries ignore CLUSTER grant,
             however you can change this behaviour by setting this to true -->
        <on_cluster_queries_require_cluster_grant>false</on_cluster_queries_require_cluster_grant>

        <!-- By default, for backward compatibility "SELECT * FROM system.<table>" doesn't require any grants and can be executed
             by any user. You can change this behaviour by setting this to true.
             If it's set to true then this query requires "GRANT SELECT ON system.<table>" just like as for non-system tables.
             Exceptions: a few system tables ("tables", "columns", "databases", and some constant tables like "one", "contributors")
             are still accessible for everyone; and if there is a SHOW privilege (e.g. "SHOW USERS") granted the corresponding system
             table (i.e. "system.users") will be accessible. -->
        <select_from_system_db_requires_grant>false</select_from_system_db_requires_grant>

        <!-- By default, for backward compatibility "SELECT * FROM information_schema.<table>" doesn't require any grants and can be
             executed by any user. You can change this behaviour by setting this to true.
             If it's set to true then this query requires "GRANT SELECT ON information_schema.<table>" just like as for ordinary tables. -->
        <select_from_information_schema_requires_grant>false</select_from_information_schema_requires_grant>

        <!-- By default, for backward compatibility a settings profile constraint for a specific setting inherit every not set field from
             previous profile. You can change this behaviour by setting this to true.
             If it's set to true then if settings profile has a constraint for a specific setting, then this constraint completely cancels all
             actions of previous constraint (defined in other profiles) for the same specific setting, including fields that are not set by new constraint.
             It also enables 'changeable_in_readonly' constraint type -->
        <settings_constraints_replace_previous>false</settings_constraints_replace_previous>
    </access_control_improvements>

    <!-- Default profile of settings. -->
    <default_profile>default</default_profile>

    <!-- Comma-separated list of prefixes for user-defined settings. -->
    <custom_settings_prefixes></custom_settings_prefixes>

    <!-- System profile of settings. This settings are used by internal processes (Distributed DDL worker and so on). -->
    <!-- <system_profile>default</system_profile> -->

    <!-- Buffer profile of settings.
         This settings are used by Buffer storage to flush data to the underlying table.
         Default: used from system_profile directive.
    -->
    <!-- <buffer_profile>default</buffer_profile> -->

    <!-- Default database. -->
    <default_database>default</default_database>

    <!-- Server time zone could be set here.

         Time zone is used when converting between String and DateTime types,
          when printing DateTime in text formats and parsing DateTime from text,
          it is used in date and time related functions, if specific time zone was not passed as an argument.

         Time zone is specified as identifier from IANA time zone database, like UTC or Africa/Abidjan.
         If not specified, system time zone at server startup is used.

         Please note, that server could display time zone alias instead of specified name.
         Example: Zulu is an alias for UTC.
    -->
    <!-- <timezone>UTC</timezone> -->

    <!-- You can specify umask here (see "man umask"). Server will apply it on startup.
         Number is always parsed as octal. Default umask is 027 (other users cannot read logs, data files, etc; group can only read).
    -->
    <!-- <umask>022</umask> -->

    <!-- Perform mlockall after startup to lower first queries latency
          and to prevent clickhouse executable from being paged out under high IO load.
         Enabling this option is recommended but will lead to increased startup time for up to a few seconds.
    -->
    <mlock_executable>true</mlock_executable>

    <!-- Reallocate memory for machine code ("text") using huge pages. Highly experimental. -->
    <remap_executable>false</remap_executable>

    <![CDATA[
         Uncomment below in order to use JDBC table engine and function.

         To install and run JDBC bridge in background:
         * [Debian/Ubuntu]
           export MVN_URL=https://repo1.maven.org/maven2/com/clickhouse/clickhouse-jdbc-bridge/
           export PKG_VER=$(curl -sL $MVN_URL/maven-metadata.xml | grep '<release>' | sed -e 's|.*>\(.*\)<.*|\1|')
           wget https://github.com/ClickHouse/clickhouse-jdbc-bridge/releases/download/v$PKG_VER/clickhouse-jdbc-bridge_$PKG_VER-1_all.deb
           apt install --no-install-recommends -f ./clickhouse-jdbc-bridge_$PKG_VER-1_all.deb
           clickhouse-jdbc-bridge &

         * [CentOS/RHEL]
           export MVN_URL=https://repo1.maven.org/maven2/com/clickhouse/clickhouse-jdbc-bridge/
           export PKG_VER=$(curl -sL $MVN_URL/maven-metadata.xml | grep '<release>' | sed -e 's|.*>\(.*\)<.*|\1|')
           wget https://github.com/ClickHouse/clickhouse-jdbc-bridge/releases/download/v$PKG_VER/clickhouse-jdbc-bridge-$PKG_VER-1.noarch.rpm
           yum localinstall -y clickhouse-jdbc-bridge-$PKG_VER-1.noarch.rpm
           clickhouse-jdbc-bridge &

         Please refer to https://github.com/ClickHouse/clickhouse-jdbc-bridge#usage for more information.
    ]]>
    <!--
    <jdbc_bridge>
        <host>127.0.0.1</host>
        <port>9019</port>
    </jdbc_bridge>
    -->

    <!-- Configuration of clusters that could be used in Distributed tables.
         https://clickhouse.com/docs/en/operations/table_engines/distributed/
      -->
    <remote_servers>
        <!-- Test only shard config for testing distributed storage -->
        <test_shard_localhost>
            <!-- Inter-server per-cluster secret for Distributed queries
                 default: no secret (no authentication will be performed)

                 If set, then Distributed queries will be validated on shards, so at least:
                 - such cluster should exist on the shard,
                 - such cluster should have the same secret.

                 And also (and which is more important), the initial_user will
                 be used as current user for the query.

                 Right now the protocol is pretty simple and it only takes into account:
                 - cluster name
                 - query

                 Also it will be nice if the following will be implemented:
                 - source hostname (see interserver_http_host), but then it will depends from DNS,
                   it can use IP address instead, but then the you need to get correct on the initiator node.
                 - target hostname / ip address (same notes as for source hostname)
                 - time-based security tokens
            -->
            <!-- <secret></secret> -->

            <shard>
                <!-- Optional. Whether to write data to just one of the replicas. Default: false (write data to all replicas). -->
                <!-- <internal_replication>false</internal_replication> -->
                <!-- Optional. Shard weight when writing data. Default: 1. -->
                <!-- <weight>1</weight> -->
                <replica>
                    <host>localhost</host>
                    <port>9000</port>
                    <!-- Optional. Priority of the replica for load_balancing. Default: 1 (less value has more priority). -->
                    <!-- <priority>1</priority> -->
                </replica>
            </shard>
        </test_shard_localhost>
        <test_cluster_one_shard_three_replicas_localhost>
            <shard>
                <internal_replication>false</internal_replication>
                <replica>
                    <host>127.0.0.1</host>
                    <port>9000</port>
                </replica>
                <replica>
                    <host>127.0.0.2</host>
                    <port>9000</port>
                </replica>
                <replica>
                    <host>127.0.0.3</host>
                    <port>9000</port>
                </replica>
            </shard>
            <!--shard>
                <internal_replication>false</internal_replication>
                <replica>
                    <host>127.0.0.1</host>
                    <port>9000</port>
                </replica>
                <replica>
                    <host>127.0.0.2</host>
                    <port>9000</port>
                </replica>
                <replica>
                    <host>127.0.0.3</host>
                    <port>9000</port>
                </replica>
            </shard-->
        </test_cluster_one_shard_three_replicas_localhost>
        <test_cluster_two_shards_localhost>
             <shard>
                 <replica>
                     <host>localhost</host>
                     <port>9000</port>
                 </replica>
             </shard>
             <shard>
                 <replica>
                     <host>localhost</host>
                     <port>9000</port>
                 </replica>
             </shard>
        </test_cluster_two_shards_localhost>
        <test_cluster_two_shards>
            <shard>
                <replica>
                    <host>127.0.0.1</host>
                    <port>9000</port>
                </replica>
            </shard>
            <shard>
                <replica>
                    <host>127.0.0.2</host>
                    <port>9000</port>
                </replica>
            </shard>
        </test_cluster_two_shards>
        <test_cluster_two_shards_internal_replication>
            <shard>
                <internal_replication>true</internal_replication>
                <replica>
                    <host>127.0.0.1</host>
                    <port>9000</port>
                </replica>
            </shard>
            <shard>
                <internal_replication>true</internal_replication>
                <replica>
                    <host>127.0.0.2</host>
                    <port>9000</port>
                </replica>
            </shard>
        </test_cluster_two_shards_internal_replication>
        <test_shard_localhost_secure>
            <shard>
                <replica>
                    <host>localhost</host>
                    <port>9440</port>
                    <secure>1</secure>
                </replica>
            </shard>
        </test_shard_localhost_secure>
        <test_unavailable_shard>
            <shard>
                <replica>
                    <host>localhost</host>
                    <port>9000</port>
                </replica>
            </shard>
            <shard>
                <replica>
                    <host>localhost</host>
                    <port>1</port>
                </replica>
            </shard>
        </test_unavailable_shard>
    </remote_servers>

    <!-- The list of hosts allowed to use in URL-related storage engines and table functions.
        If this section is not present in configuration, all hosts are allowed.
    -->
    <!--<remote_url_allow_hosts>-->
        <!-- Host should be specified exactly as in URL. The name is checked before DNS resolution.
            Example: "clickhouse.com", "clickhouse.com." and "www.clickhouse.com" are different hosts.
                    If port is explicitly specified in URL, the host:port is checked as a whole.
                    If host specified here without port, any port with this host allowed.
                    "clickhouse.com" -> "clickhouse.com:443", "clickhouse.com:80" etc. is allowed, but "clickhouse.com:80" -> only "clickhouse.com:80" is allowed.
            If the host is specified as IP address, it is checked as specified in URL. Example: "[2a02:6b8:a::a]".
            If there are redirects and support for redirects is enabled, every redirect (the Location field) is checked.
            Host should be specified using the host xml tag:
                    <host>clickhouse.com</host>
        -->

        <!-- Regular expression can be specified. RE2 engine is used for regexps.
            Regexps are not aligned: don't forget to add ^ and $. Also don't forget to escape dot (.) metacharacter
            (forgetting to do so is a common source of error).
        -->
    <!--</remote_url_allow_hosts>-->

    <!-- If element has 'incl' attribute, then for it's value will be used corresponding substitution from another file.
         By default, path to file with substitutions is /etc/metrika.xml. It could be changed in config in 'include_from' element.
         Values for substitutions are specified in /clickhouse/name_of_substitution elements in that file.
      -->

    <!-- ZooKeeper is used to store metadata about replicas, when using Replicated tables.
         Optional. If you don't use replicated tables, you could omit that.

         See https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/replication/
      -->

    <!--
    <zookeeper>
        <node>
            <host>example1</host>
            <port>2181</port>
        </node>
        <node>
            <host>example2</host>
            <port>2181</port>
        </node>
        <node>
            <host>example3</host>
            <port>2181</port>
        </node>
    </zookeeper>
    -->

    <!-- Substitutions for parameters of replicated tables.
          Optional. If you don't use replicated tables, you could omit that.

         See https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/replication/#creating-replicated-tables
      -->
    <!--
    <macros>
        <shard>01</shard>
        <replica>example01-01-1</replica>
    </macros>
    -->

    <!-- Reloading interval for embedded dictionaries, in seconds. Default: 3600. -->
    <builtin_dictionaries_reload_interval>3600</builtin_dictionaries_reload_interval>

    <!-- Maximum session timeout, in seconds. Default: 3600. -->
    <max_session_timeout>3600</max_session_timeout>

    <!-- Default session timeout, in seconds. Default: 60. -->
    <default_session_timeout>60</default_session_timeout>

    <!-- Sending data to Graphite for monitoring. Several sections can be defined. -->
    <!--
        interval - send every X second
        root_path - prefix for keys
        hostname_in_path - append hostname to root_path (default = true)
        metrics - send data from table system.metrics
        events - send data from table system.events
        asynchronous_metrics - send data from table system.asynchronous_metrics
    -->
    <!--
    <graphite>
        <host>localhost</host>
        <port>42000</port>
        <timeout>0.1</timeout>
        <interval>60</interval>
        <root_path>one_min</root_path>
        <hostname_in_path>true</hostname_in_path>

        <metrics>true</metrics>
        <events>true</events>
        <events_cumulative>false</events_cumulative>
        <asynchronous_metrics>true</asynchronous_metrics>
    </graphite>
    <graphite>
        <host>localhost</host>
        <port>42000</port>
        <timeout>0.1</timeout>
        <interval>1</interval>
        <root_path>one_sec</root_path>

        <metrics>true</metrics>
        <events>true</events>
        <events_cumulative>false</events_cumulative>
        <asynchronous_metrics>false</asynchronous_metrics>
    </graphite>
    -->

    <!-- Serve endpoint for Prometheus monitoring. -->
    <!--
        endpoint - mertics path (relative to root, statring with "/")
        port - port to setup server. If not defined or 0 than http_port used
        metrics - send data from table system.metrics
        events - send data from table system.events
        asynchronous_metrics - send data from table system.asynchronous_metrics
        status_info - send data from different component from CH, ex: Dictionaries status
    -->
    <!--
    <prometheus>
        <endpoint>/metrics</endpoint>
        <port>9363</port>

        <metrics>true</metrics>
        <events>true</events>
        <asynchronous_metrics>true</asynchronous_metrics>
        <status_info>true</status_info>
    </prometheus>
    -->

    <!-- Query log. Used only for queries with setting log_queries = 1. -->
    <query_log>
        <!-- What table to insert data. If table is not exist, it will be created.
             When query log structure is changed after system update,
              then old table will be renamed and new table will be created automatically.
        -->
        <database>system</database>
        <table>query_log</table>
        <!--
            PARTITION BY expr: https://clickhouse.com/docs/en/table_engines/mergetree-family/custom_partitioning_key/
            Example:
                event_date
                toMonday(event_date)
                toYYYYMM(event_date)
                toStartOfHour(event_time)
        -->
        <partition_by>toYYYYMM(event_date)</partition_by>
        <!--
            Table TTL specification: https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/mergetree/#mergetree-table-ttl
            Example:
                event_date + INTERVAL 1 WEEK
                event_date + INTERVAL 7 DAY DELETE
                event_date + INTERVAL 2 WEEK TO DISK 'bbb'

        <ttl>event_date + INTERVAL 30 DAY DELETE</ttl>
        -->

        <!-- Instead of partition_by, you can provide full engine expression (starting with ENGINE = ) with parameters,
             Example: <engine>ENGINE = MergeTree PARTITION BY toYYYYMM(event_date) ORDER BY (event_date, event_time) SETTINGS index_granularity = 1024</engine>
          -->

        <!-- Interval of flushing data. -->
        <flush_interval_milliseconds>7500</flush_interval_milliseconds>
    </query_log>

    <!-- Trace log. Stores stack traces collected by query profilers.
         See query_profiler_real_time_period_ns and query_profiler_cpu_time_period_ns settings. -->
    <trace_log>
        <database>system</database>
        <table>trace_log</table>

        <partition_by>toYYYYMM(event_date)</partition_by>
        <flush_interval_milliseconds>7500</flush_interval_milliseconds>
    </trace_log>

    <!-- Query thread log. Has information about all threads participated in query execution.
         Used only for queries with setting log_query_threads = 1. -->
    <query_thread_log>
        <database>system</database>
        <table>query_thread_log</table>
        <partition_by>toYYYYMM(event_date)</partition_by>
        <flush_interval_milliseconds>7500</flush_interval_milliseconds>
    </query_thread_log>

    <!-- Query views log. Has information about all dependent views associated with a query.
         Used only for queries with setting log_query_views = 1. -->
    <query_views_log>
        <database>system</database>
        <table>query_views_log</table>
        <partition_by>toYYYYMM(event_date)</partition_by>
        <flush_interval_milliseconds>7500</flush_interval_milliseconds>
    </query_views_log>

    <!-- Uncomment if use part log.
         Part log contains information about all actions with parts in MergeTree tables (creation, deletion, merges, downloads).-->
    <part_log>
        <database>system</database>
        <table>part_log</table>
        <partition_by>toYYYYMM(event_date)</partition_by>
        <flush_interval_milliseconds>7500</flush_interval_milliseconds>
    </part_log>

    <!-- Uncomment to write text log into table.
         Text log contains all information from usual server log but stores it in structured and efficient way.
         The level of the messages that goes to the table can be limited (<level>), if not specified all messages will go to the table.
    <text_log>
        <database>system</database>
        <table>text_log</table>
        <flush_interval_milliseconds>7500</flush_interval_milliseconds>
        <level></level>
    </text_log>
    -->

    <!-- Metric log contains rows with current values of ProfileEvents, CurrentMetrics collected with "collect_interval_milliseconds" interval. -->
    <metric_log>
        <database>system</database>
        <table>metric_log</table>
        <flush_interval_milliseconds>7500</flush_interval_milliseconds>
        <collect_interval_milliseconds>1000</collect_interval_milliseconds>
    </metric_log>

    <!--
        Asynchronous metric log contains values of metrics from
        system.asynchronous_metrics.
    -->
    <asynchronous_metric_log>
        <database>system</database>
        <table>asynchronous_metric_log</table>
        <!--
            Asynchronous metrics are updated once a minute, so there is
            no need to flush more often.
        -->
        <flush_interval_milliseconds>7000</flush_interval_milliseconds>
    </asynchronous_metric_log>

    <!--
        OpenTelemetry log contains OpenTelemetry trace spans.
    -->
    <opentelemetry_span_log>
        <!--
            The default table creation code is insufficient, this <engine> spec
            is a workaround. There is no 'event_time' for this log, but two times,
            start and finish. It is sorted by finish time, to avoid inserting
            data too far away in the past (probably we can sometimes insert a span
            that is seconds earlier than the last span in the table, due to a race
            between several spans inserted in parallel). This gives the spans a
            global order that we can use to e.g. retry insertion into some external
            system.
        -->
        <engine>
            engine MergeTree
            partition by toYYYYMM(finish_date)
            order by (finish_date, finish_time_us, trace_id)
        </engine>
        <database>system</database>
        <table>opentelemetry_span_log</table>
        <flush_interval_milliseconds>7500</flush_interval_milliseconds>
    </opentelemetry_span_log>

    <!-- Crash log. Stores stack traces for fatal errors.
         This table is normally empty. -->
    <crash_log>
        <database>system</database>
        <table>crash_log</table>

        <partition_by />
        <flush_interval_milliseconds>1000</flush_interval_milliseconds>
    </crash_log>

    <!-- Session log. Stores user log in (successful or not) and log out events.

        Note: session log has known security issues and should not be used in production.
    -->
    <!-- <session_log>
        <database>system</database>
        <table>session_log</table>

        <partition_by>toYYYYMM(event_date)</partition_by>
        <flush_interval_milliseconds>7500</flush_interval_milliseconds>
    </session_log> -->

    <!-- Profiling on Processors level. -->
    <processors_profile_log>
        <database>system</database>
        <table>processors_profile_log</table>

        <partition_by>toYYYYMM(event_date)</partition_by>
        <flush_interval_milliseconds>7500</flush_interval_milliseconds>
    </processors_profile_log>

    <!-- <top_level_domains_path>/var/lib/clickhouse/top_level_domains/</top_level_domains_path> -->
    <!-- Custom TLD lists.
         Format: <name>/path/to/file</name>

         Changes will not be applied w/o server restart.
         Path to the list is under top_level_domains_path (see above).
    -->
    <top_level_domains_lists>
        <!--
        <public_suffix_list>/path/to/public_suffix_list.dat</public_suffix_list>
        -->
    </top_level_domains_lists>

    <!-- Configuration of external dictionaries. See:
         https://clickhouse.com/docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts
    -->
    <dictionaries_config>*_dictionary.xml</dictionaries_config>

    <!-- Configuration of user defined executable functions -->
    <user_defined_executable_functions_config>*_function.xml</user_defined_executable_functions_config>

    <!-- Uncomment if you want data to be compressed 30-100% better.
         Don't do that if you just started using ClickHouse.
      -->
    <!--
    <compression>
        <!- - Set of variants. Checked in order. Last matching case wins. If nothing matches, lz4 will be used. - ->
        <case>

            <!- - Conditions. All must be satisfied. Some conditions may be omitted. - ->
            <min_part_size>10000000000</min_part_size>        <!- - Min part size in bytes. - ->
            <min_part_size_ratio>0.01</min_part_size_ratio>   <!- - Min size of part relative to whole table size. - ->

            <!- - What compression method to use. - ->
            <method>zstd</method>
        </case>
    </compression>
    -->

    <!-- Configuration of encryption. The server executes a command to
         obtain an encryption key at startup if such a command is
         defined, or encryption codecs will be disabled otherwise. The
         command is executed through /bin/sh and is expected to write
         a Base64-encoded key to the stdout. -->
    <encryption_codecs>
        <!-- aes_128_gcm_siv -->
            <!-- Example of getting hex key from env -->
            <!-- the code should use this key and throw an exception if its length is not 16 bytes -->
            <!--key_hex from_env="..."></key_hex -->

            <!-- Example of multiple hex keys. They can be imported from env or be written down in config-->
            <!-- the code should use these keys and throw an exception if their length is not 16 bytes -->
            <!-- key_hex id="0">...</key_hex -->
            <!-- key_hex id="1" from_env=".."></key_hex -->
            <!-- key_hex id="2">...</key_hex -->
            <!-- current_key_id>2</current_key_id -->

            <!-- Example of getting hex key from config -->
            <!-- the code should use this key and throw an exception if its length is not 16 bytes -->
            <!-- key>...</key -->

            <!-- example of adding nonce -->
            <!-- nonce>...</nonce -->

        <!-- /aes_128_gcm_siv -->
    </encryption_codecs>

    <!-- Allow to execute distributed DDL queries (CREATE, DROP, ALTER, RENAME) on cluster.
         Works only if ZooKeeper is enabled. Comment it if such functionality isn't required. -->
    <distributed_ddl>
        <!-- Path in ZooKeeper to queue with DDL queries -->
        <path>/clickhouse/task_queue/ddl</path>

        <!-- Settings from this profile will be used to execute DDL queries -->
        <!-- <profile>default</profile> -->

        <!-- Controls how much ON CLUSTER queries can be run simultaneously. -->
        <!-- <pool_size>1</pool_size> -->

        <!--
             Cleanup settings (active tasks will not be removed)
        -->

        <!-- Controls task TTL (default 1 week) -->
        <!-- <task_max_lifetime>604800</task_max_lifetime> -->

        <!-- Controls how often cleanup should be performed (in seconds) -->
        <!-- <cleanup_delay_period>60</cleanup_delay_period> -->

        <!-- Controls how many tasks could be in the queue -->
        <!-- <max_tasks_in_queue>1000</max_tasks_in_queue> -->
    </distributed_ddl>

    <!-- Settings to fine tune MergeTree tables. See documentation in source code, in MergeTreeSettings.h -->
    <!--
    <merge_tree>
        <max_suspicious_broken_parts>5</max_suspicious_broken_parts>
    </merge_tree>
    -->

    <!-- Protection from accidental DROP.
         If size of a MergeTree table is greater than max_table_size_to_drop (in bytes) than table could not be dropped with any DROP query.
         If you want do delete one table and don't want to change clickhouse-server config, you could create special file <clickhouse-path>/flags/force_drop_table and make DROP once.
         By default max_table_size_to_drop is 50GB; max_table_size_to_drop=0 allows to DROP any tables.
         The same for max_partition_size_to_drop.
         Uncomment to disable protection.
    -->
    <!-- <max_table_size_to_drop>0</max_table_size_to_drop> -->
    <!-- <max_partition_size_to_drop>0</max_partition_size_to_drop> -->

    <!-- Example of parameters for GraphiteMergeTree table engine -->
    <graphite_rollup_example>
        <pattern>
            <regexp>click_cost</regexp>
            <function>any</function>
            <retention>
                <age>0</age>
                <precision>3600</precision>
            </retention>
            <retention>
                <age>86400</age>
                <precision>60</precision>
            </retention>
        </pattern>
        <default>
            <function>max</function>
            <retention>
                <age>0</age>
                <precision>60</precision>
            </retention>
            <retention>
                <age>3600</age>
                <precision>300</precision>
            </retention>
            <retention>
                <age>86400</age>
                <precision>3600</precision>
            </retention>
        </default>
    </graphite_rollup_example>

    <!-- Directory in <clickhouse-path> containing schema files for various input formats.
         The directory will be created if it doesn't exist.
      -->
    <format_schema_path>/var/lib/clickhouse/format_schemas/</format_schema_path>

    <!-- Default query masking rules, matching lines would be replaced with something else in the logs
        (both text logs and system.query_log).
        name - name for the rule (optional)
        regexp - RE2 compatible regular expression (mandatory)
        replace - substitution string for sensitive data (optional, by default - six asterisks)
    -->
    <query_masking_rules>
        <rule>
            <name>hide encrypt/decrypt arguments</name>
            <regexp>((?:aes_)?(?:encrypt|decrypt)(?:_mysql)?)\s*\(\s*(?:'(?:\\'|.)+'|.*?)\s*\)</regexp>
            <!-- or more secure, but also more invasive:
                (aes_\w+)\s*\(.*\)
            -->
            <replace>\1(???)</replace>
        </rule>
    </query_masking_rules>

    <!-- Uncomment to use custom http handlers.
        rules are checked from top to bottom, first match runs the handler
            url - to match request URL, you can use 'regex:' prefix to use regex match(optional)
            methods - to match request method, you can use commas to separate multiple method matches(optional)
            headers - to match request headers, match each child element(child element name is header name), you can use 'regex:' prefix to use regex match(optional)
        handler is request handler
            type - supported types: static, dynamic_query_handler, predefined_query_handler
            query - use with predefined_query_handler type, executes query when the handler is called
            query_param_name - use with dynamic_query_handler type, extracts and executes the value corresponding to the <query_param_name> value in HTTP request params
            status - use with static type, response status code
            content_type - use with static type, response content-type
            response_content - use with static type, Response content sent to client, when using the prefix 'file://' or 'config://', find the content from the file or configuration send to client.

    <http_handlers>
        <rule>
            <url>/</url>
            <methods>POST,GET</methods>
            <headers><pragma>no-cache</pragma></headers>
            <handler>
                <type>dynamic_query_handler</type>
                <query_param_name>query</query_param_name>
            </handler>
        </rule>

        <rule>
            <url>/predefined_query</url>
            <methods>POST,GET</methods>
            <handler>
                <type>predefined_query_handler</type>
                <query>SELECT * FROM system.settings</query>
            </handler>
        </rule>

        <rule>
            <handler>
                <type>static</type>
                <status>200</status>
                <content_type>text/plain; charset=UTF-8</content_type>
                <response_content>config://http_server_default_response</response_content>
            </handler>
        </rule>
    </http_handlers>
    -->

    <send_crash_reports>
        <!-- Changing <enabled> to true allows sending crash reports to -->
        <!-- the ClickHouse core developers team via Sentry https://sentry.io -->
        <!-- Doing so at least in pre-production environments is highly appreciated -->
        <enabled>false</enabled>
        <!-- Change <anonymize> to true if you don't feel comfortable attaching the server hostname to the crash report -->
        <anonymize>false</anonymize>
        <!-- Default endpoint should be changed to different Sentry DSN only if you have -->
        <!-- some in-house engineers or hired consultants who're going to debug ClickHouse issues for you -->
        <endpoint>https://6f33034cfe684dd7a3ab9875e57b1c8d@o388870.ingest.sentry.io/5226277</endpoint>
    </send_crash_reports>

    <!-- Uncomment to disable ClickHouse internal DNS caching. -->
    <!-- <disable_internal_dns_cache>1</disable_internal_dns_cache> -->

    <!-- You can also configure rocksdb like this: -->
    <!--
    <rocksdb>
        <options>
            <max_background_jobs>8</max_background_jobs>
        </options>
        <column_family_options>
            <num_levels>2</num_levels>
        </column_family_options>
        <tables>
            <table>
                <name>TABLE</name>
                <options>
                    <max_background_jobs>8</max_background_jobs>
                </options>
                <column_family_options>
                    <num_levels>2</num_levels>
                </column_family_options>
            </table>
        </tables>
    </rocksdb>
    -->

    <!-- Uncomment if enable merge tree metadata cache -->
    <!--merge_tree_metadata_cache>
        <lru_cache_size>268435456</lru_cache_size>
        <continue_if_corrupted>true</continue_if_corrupted>
    </merge_tree_metadata_cache-->

    <!-- This allows to disable exposing addresses in stack traces for security reasons.
         Please be aware that it does not improve security much, but makes debugging much harder.
         The addresses that are small offsets from zero will be displayed nevertheless to show nullptr dereferences.
         Regardless of this configuration, the addresses are visible in the system.stack_trace and system.trace_log tables
         if the user has access to these tables.
         I don't recommend to change this setting.
    <show_addresses_in_stack_traces>false</show_addresses_in_stack_traces>
    -->
</clickhouse>

'''
'''--- conf/clickhouse/clickhouse-server/users.d/operator.xml ---
<clickhouse>
    <users>
        <!-- If user name was not specified, 'default' user is used. -->
        <operator>
            <!-- See also the files in users.d directory where the password can be overridden.

                 Password could be specified in plaintext or in SHA256 (in hex format).

                 If you want to specify password in plaintext (not recommended), place it in 'password' element.
                 Example: <password>qwerty</password>.
                 Password could be empty.

                 If you want to specify SHA256, place it in 'password_sha256_hex' element.
                 Example: <password_sha256_hex>65e84be33532fb784c48129675f9eff3a682b27168c0ea744b2cf58ee02337c5</password_sha256_hex>
                 Restrictions of SHA256: impossibility to connect to ClickHouse using MySQL JS client (as of July 2019).

                 If you want to specify double SHA1, place it in 'password_double_sha1_hex' element.
                 Example: <password_double_sha1_hex>e395796d6546b1b65db9d665cd43f0e858dd4303</password_double_sha1_hex>

                 If you want to specify a previously defined LDAP server (see 'ldap_servers' in the main config) for authentication,
                  place its name in 'server' element inside 'ldap' element.
                 Example: <ldap><server>my_ldap_server</server></ldap>

                 If you want to authenticate the user via Kerberos (assuming Kerberos is enabled, see 'kerberos' in the main config),
                  place 'kerberos' element instead of 'password' (and similar) elements.
                 The name part of the canonical principal name of the initiator must match the user name for authentication to succeed.
                 You can also place 'realm' element inside 'kerberos' element to further restrict authentication to only those requests
                  whose initiator's realm matches it.
                 Example: <kerberos />
                 Example: <kerberos><realm>EXAMPLE.COM</realm></kerberos>

                 How to generate decent password:
                 Execute: PASSWORD=$(base64 < /dev/urandom | head -c8); echo "$PASSWORD"; echo -n "$PASSWORD" | sha256sum | tr -d '-'
                 In first line will be password and in second - corresponding SHA256.

                 How to generate double SHA1:
                 Execute: PASSWORD=$(base64 < /dev/urandom | head -c8); echo "$PASSWORD"; echo -n "$PASSWORD" | sha1sum | tr -d '-' | xxd -r -p | sha1sum | tr -d '-'
                 In first line will be password and in second - corresponding double SHA1.
            -->
            <password></password>

            <!-- List of networks with open access.

                 To open access from everywhere, specify:
                    <ip>::/0</ip>

                 To open access only from localhost, specify:
                    <ip>::1</ip>
                    <ip>127.0.0.1</ip>

                 Each element of list has one of the following forms:
                 <ip> IP-address or network mask. Examples: 213.180.204.3 or 10.0.0.1/8 or 10.0.0.1/255.255.255.0
                     2a02:6b8::3 or 2a02:6b8::3/64 or 2a02:6b8::3/ffff:ffff:ffff:ffff::.
                 <host> Hostname. Example: server01.clickhouse.com.
                     To check access, DNS query is performed, and all received addresses compared to peer address.
                 <host_regexp> Regular expression for host names. Example, ^server\d\d-\d\d-\d\.clickhouse\.com$
                     To check access, DNS PTR query is performed for peer address and then regexp is applied.
                     Then, for result of PTR query, another DNS query is performed and all received addresses compared to peer address.
                     Strongly recommended that regexp is ends with $
                 All results of DNS requests are cached till server restart.
            -->
            <networks>
                <ip>::1</ip>
                <ip>127.0.0.1</ip>
            </networks>

            <!-- Settings profile for user. -->
            <profile>default</profile>

            <!-- Quota for user. -->
            <quota>default</quota>

            <!-- User can create other users and grant rights to them. -->
            <access_management>1</access_management>
        </operator>
    </users>
</clickhouse>
'''
'''--- conf/clickhouse/clickhouse-server/users.xml ---
<clickhouse>
    <!-- See also the files in users.d directory where the settings can be overridden. -->

    <!-- Profiles of settings. -->
    <profiles>
        <!-- Default settings. -->
        <default>
        </default>

        <!-- Profile that allows only read queries. -->
        <readonly>
            <readonly>1</readonly>
        </readonly>
    </profiles>

    <!-- Users and ACL. -->
    <users>
        <!-- If user name was not specified, 'default' user is used. -->
        <default>
            <!-- See also the files in users.d directory where the password can be overridden.

                 Password could be specified in plaintext or in SHA256 (in hex format).

                 If you want to specify password in plaintext (not recommended), place it in 'password' element.
                 Example: <password>qwerty</password>.
                 Password could be empty.

                 If you want to specify SHA256, place it in 'password_sha256_hex' element.
                 Example: <password_sha256_hex>65e84be33532fb784c48129675f9eff3a682b27168c0ea744b2cf58ee02337c5</password_sha256_hex>
                 Restrictions of SHA256: impossibility to connect to ClickHouse using MySQL JS client (as of July 2019).

                 If you want to specify double SHA1, place it in 'password_double_sha1_hex' element.
                 Example: <password_double_sha1_hex>e395796d6546b1b65db9d665cd43f0e858dd4303</password_double_sha1_hex>

                 If you want to specify a previously defined LDAP server (see 'ldap_servers' in the main config) for authentication,
                  place its name in 'server' element inside 'ldap' element.
                 Example: <ldap><server>my_ldap_server</server></ldap>

                 If you want to authenticate the user via Kerberos (assuming Kerberos is enabled, see 'kerberos' in the main config),
                  place 'kerberos' element instead of 'password' (and similar) elements.
                 The name part of the canonical principal name of the initiator must match the user name for authentication to succeed.
                 You can also place 'realm' element inside 'kerberos' element to further restrict authentication to only those requests
                  whose initiator's realm matches it.
                 Example: <kerberos />
                 Example: <kerberos><realm>EXAMPLE.COM</realm></kerberos>

                 How to generate decent password:
                 Execute: PASSWORD=$(base64 < /dev/urandom | head -c8); echo "$PASSWORD"; echo -n "$PASSWORD" | sha256sum | tr -d '-'
                 In first line will be password and in second - corresponding SHA256.

                 How to generate double SHA1:
                 Execute: PASSWORD=$(base64 < /dev/urandom | head -c8); echo "$PASSWORD"; echo -n "$PASSWORD" | sha1sum | tr -d '-' | xxd -r -p | sha1sum | tr -d '-'
                 In first line will be password and in second - corresponding double SHA1.
            -->
            <password></password>

            <!-- List of networks with open access.

                 To open access from everywhere, specify:
                    <ip>::/0</ip>

                 To open access only from localhost, specify:
                    <ip>::1</ip>
                    <ip>127.0.0.1</ip>

                 Each element of list has one of the following forms:
                 <ip> IP-address or network mask. Examples: 213.180.204.3 or 10.0.0.1/8 or 10.0.0.1/255.255.255.0
                     2a02:6b8::3 or 2a02:6b8::3/64 or 2a02:6b8::3/ffff:ffff:ffff:ffff::.
                 <host> Hostname. Example: server01.clickhouse.com.
                     To check access, DNS query is performed, and all received addresses compared to peer address.
                 <host_regexp> Regular expression for host names. Example, ^server\d\d-\d\d-\d\.clickhouse\.com$
                     To check access, DNS PTR query is performed for peer address and then regexp is applied.
                     Then, for result of PTR query, another DNS query is performed and all received addresses compared to peer address.
                     Strongly recommended that regexp is ends with $
                 All results of DNS requests are cached till server restart.
            -->
            <networks>
                <ip>::1</ip>
                <ip>127.0.0.1</ip>
            </networks>

            <!-- Settings profile for user. -->
            <profile>default</profile>

            <!-- Quota for user. -->
            <quota>default</quota>

            <!-- User can create other users and grant rights to them. -->
            <access_management>1</access_management>
        </default>
    </users>

    <!-- Quotas. -->
    <quotas>
        <!-- Name of quota. -->
        <default>
            <!-- Limits for time interval. You could specify many intervals with different limits. -->
            <interval>
                <!-- Length of interval. -->
                <duration>3600</duration>

                <!-- No limits. Just calculate resource usage for time interval. -->
                <queries>0</queries>
                <errors>0</errors>
                <result_rows>0</result_rows>
                <read_rows>0</read_rows>
                <execution_time>0</execution_time>
            </interval>
        </default>
    </quotas>
</clickhouse>

'''
'''--- conf/clickhouse/docker-entrypoint-initdb.d/init.sql ---
CREATE DATABASE IF NOT EXISTS grafana;

CREATE USER IF NOT EXISTS grafana@'%' IDENTIFIED WITH plaintext_password BY 'password123';
CREATE USER IF NOT EXISTS grafana_insert@'%' IDENTIFIED WITH plaintext_password BY 'password123';

GRANT SELECT ON grafana.* TO grafana;
GRANT SELECT, INSERT, CREATE TABLE ON grafana.* TO grafana_insert;

USE grafana;

CREATE TABLE IF NOT EXISTS t_docker_logs(
    `time` DateTime,
    `log` String,
    `log_path` String,
    `stream` String
) ENGINE = MergeTree()
ORDER BY (time, log_path);

CREATE TABLE IF NOT EXISTS t_near_processor_logs(
    `time` DateTime,
    `message` String,
    `level` String,
    `log_type` String,
    `job_id` String,
    `service_name` String,
    `hostname` String
) ENGINE = MergeTree()
ORDER BY (time, service_name)
'''
'''--- conf/fluent-bit/schema.json ---
{"fluent-bit":{"version":"1.9.8","schema_version":"1","os":"linux"},"customs":[{"type":"custom","name":"calyptia","description":"Calyptia Cloud","properties":{"options":[{"name":"api_key","description":"Calyptia Cloud API Key.","default":null,"type":"string"},{"name":"store_path","description":"","default":null,"type":"string"},{"name":"calyptia_host","description":"","default":null,"type":"string"},{"name":"calyptia_port","description":"","default":null,"type":"string"},{"name":"calyptia_tls","description":"","default":"true","type":"boolean"},{"name":"calyptia_tls.verify","description":"","default":"true","type":"boolean"},{"name":"add_label","description":"Label to append to the generated metric.","default":null,"type":"space delimited strings (minimum 1)"},{"name":"machine_id","description":"Custom machine_id to be used when registering agent","default":null,"type":"string"}]}}],"inputs":[{"type":"input","name":"cpu","description":"CPU Usage","properties":{"options":[{"name":"pid","description":"Configure a single process to measure usage via their PID","default":"-1","type":"integer"},{"name":"interval_sec","description":"Set the collector interval","default":"1","type":"integer"},{"name":"interval_nsec","description":"Set the collector interval (sub seconds)","default":"0","type":"integer"}]}},{"type":"input","name":"mem","description":"Memory Usage","properties":{"options":[{"name":"interval_sec","description":"Set the collector interval","default":"1","type":"integer"},{"name":"interval_nsec","description":"Set the collector interval (subseconds)","default":"0","type":"integer"},{"name":"pid","description":"Set the PID of the process to measure","default":"0","type":"integer"}]}},{"type":"input","name":"thermal","description":"Thermal","properties":{"options":[{"name":"interval_sec","description":"Set the collector interval","default":"1","type":"integer"},{"name":"interval_nsec","description":"Set the collector interval (nanoseconds)","default":"0","type":"integer"},{"name":"name_regex","description":"Set thermal name regular expression filter","default":null,"type":"string"},{"name":"type_regex","description":"Set thermal type regular expression filter","default":null,"type":"string"}]}},{"type":"input","name":"kmsg","description":"Kernel Log Buffer","properties":{"options":[]}},{"type":"input","name":"proc","description":"Check Process health","properties":{"options":[{"name":"interval_sec","description":"Set the collector interval","default":"1","type":"integer"},{"name":"interval_nsec","description":"Set the collector interval (nanoseconds)","default":"0","type":"integer"},{"name":"alert","description":"Only generate alerts if process is down","default":"false","type":"boolean"},{"name":"mem","description":"Append memory usage to record","default":"true","type":"boolean"},{"name":"fd","description":"Append fd count to record","default":"true","type":"boolean"},{"name":"proc_name","description":"Define process name to health check","default":null,"type":"string"}]}},{"type":"input","name":"disk","description":"Diskstats","properties":{"options":[{"name":"interval_sec","description":"Set the collector interval","default":"1","type":"integer"},{"name":"interval_nsec","description":"Set the collector interval (nanoseconds)","default":"0","type":"integer"},{"name":"dev_name","description":"Set the device name","default":null,"type":"string"}]}},{"type":"input","name":"systemd","description":"Systemd (Journal) reader","properties":{"options":[{"name":"path","description":"Set the systemd journal path","default":null,"type":"string"},{"name":"max_fields","description":"Set the maximum fields per notification","default":"8000","type":"integer"},{"name":"max_entries","description":"Set the maximum entries per notification","default":"5000","type":"integer"},{"name":"systemd_filter_type","description":"Set the systemd filter type to either 'and' or 'or'","default":null,"type":"string"},{"name":"systemd_filter","description":"Add a systemd filter, can be set multiple times","default":null,"type":"string"},{"name":"read_from_tail","description":"Read the journal from the end (tail)","default":"false","type":"boolean"},{"name":"lowercase","description":"Lowercase the fields","default":"false","type":"boolean"},{"name":"strip_underscores","description":"Strip undersecores from fields","default":"false","type":"boolean"},{"name":"db.sync","description":"Set the database sync mode: extra, full, normal or off","default":null,"type":"string"},{"name":"db","description":"Set the database path","default":null,"type":"string"}]}},{"type":"input","name":"netif","description":"Network Interface Usage","properties":{"options":[{"name":"interface","description":"Set the interface, eg: eth0 or enp1s0","default":null,"type":"string"},{"name":"interval_sec","description":"Set the collector interval","default":"1","type":"integer"},{"name":"interval_nsec","description":"Set the collector interval (nanoseconds)","default":"0","type":"integer"},{"name":"verbose","description":"Enable verbosity","default":"false","type":"boolean"},{"name":"test_at_init","description":"Testing interface at initialization","default":"false","type":"boolean"}]}},{"type":"input","name":"docker","description":"Docker containers metrics","properties":{"options":[{"name":"interval_sec","description":"Set the collector interval","default":"1","type":"integer"},{"name":"interval_nsec","description":"Set the collector interval (nanoseconds)","default":"0","type":"integer"},{"name":"include","description":"A space-separated list of containers to include","default":null,"type":"string"},{"name":"exclude","description":"A space-separated list of containers to exclude","default":null,"type":"string"}]}},{"type":"input","name":"docker_events","description":"Docker events","properties":{"options":[{"name":"unix_path","description":"Define Docker unix socket path to read events","default":"/var/run/docker.sock","type":"string"},{"name":"buffer_size","description":"Set buffer size to read events","default":"8k","type":"size"},{"name":"parser","description":"Optional parser for records, if not set, records are packages under 'key'","default":null,"type":"string"},{"name":"key","description":"Set the key name to store unparsed Docker events","default":"message","type":"string"},{"name":"reconnect.retry_limits","description":"Maximum number to retry to connect docker socket","default":"5","type":"integer"},{"name":"reconnect.retry_interval","description":"Retry interval to connect docker socket","default":"1","type":"integer"}]}},{"type":"input","name":"node_exporter_metrics","description":"Node Exporter Metrics (Prometheus Compatible)","properties":{"options":[{"name":"scrape_interval","description":"scrape interval to collect metrics from the node.","default":"5","type":"time"},{"name":"path.procfs","description":"procfs mount point","default":"/proc","type":"string"},{"name":"path.sysfs","description":"sysfs mount point","default":"/sys","type":"string"}]}},{"type":"input","name":"fluentbit_metrics","description":"Fluent Bit internal metrics","properties":{"options":[{"name":"scrape_interval","description":"scrape interval to collect the internal metrics of Fluent Bit.","default":"2","type":"time"},{"name":"scrape_on_start","description":"scrape metrics upon start, useful to avoid waiting for 'scrape_interval' for the first round of metrics.","default":"false","type":"boolean"}]}},{"type":"input","name":"prometheus_scrape","description":"Scrape metrics from Prometheus Endpoint","properties":{"options":[{"name":"scrape_interval","description":"Scraping interval.","default":"10s","type":"time"},{"name":"metrics_path","description":"Set the metrics URI endpoint, it must start with a forward slash.","default":"/metrics","type":"string"}]}},{"type":"input","name":"tail","description":"Tail files","properties":{"options":[{"name":"path","description":"pattern specifying log files or multiple ones through the use of common wildcards.","default":null,"type":"multiple comma delimited strings"},{"name":"exclude_path","description":"Set one or multiple shell patterns separated by commas to exclude files matching a certain criteria, e.g: 'exclude_path *.gz,*.zip'","default":null,"type":"multiple comma delimited strings"},{"name":"key","description":"when a message is unstructured (no parser applied), it's appended as a string under the key name log. This option allows to define an alternative name for that key.","default":"log","type":"string"},{"name":"read_from_head","description":"For new discovered files on start (without a database offset/position), read the content from the head of the file, not tail.","default":"false","type":"boolean"},{"name":"refresh_interval","description":"interval to refresh the list of watched files expressed in seconds.","default":"60","type":"string"},{"name":"watcher_interval","description":"","default":"2s","type":"time"},{"name":"rotate_wait","description":"specify the number of extra time in seconds to monitor a file once is rotated in case some pending data is flushed.","default":"5","type":"time"},{"name":"docker_mode","description":"If enabled, the plugin will recombine split Docker log lines before passing them to any parser as configured above. This mode cannot be used at the same time as Multiline.","default":"false","type":"boolean"},{"name":"docker_mode_flush","description":"wait period time in seconds to flush queued unfinished split lines.","default":"4","type":"integer"},{"name":"docker_mode_parser","description":"specify the parser name to fetch log first line for muliline log","default":null,"type":"string"},{"name":"path_key","description":"set the 'key' name where the name of monitored file will be appended.","default":null,"type":"string"},{"name":"offset_key","description":"set the 'key' name where the offset of monitored file will be appended.","default":null,"type":"string"},{"name":"ignore_older","description":"ignore records older than 'ignore_older'. Supports m,h,d (minutes, hours, days) syntax. Default behavior is to read all records. Option only available when a Parser is specified and it can parse the time of a record.","default":"0","type":"time"},{"name":"buffer_chunk_size","description":"set the initial buffer size to read data from files. This value is used too to increase buffer size.","default":"32768","type":"size"},{"name":"buffer_max_size","description":"set the limit of the buffer size per monitored file. When a buffer needs to be increased (e.g: very long lines), this value is used to restrict how much the memory buffer can grow. If reading a file exceed this limit, the file is removed from the monitored file list.","default":"32768","type":"size"},{"name":"static_batch_size","description":"On start, Fluent Bit might process files which already contains data, these files are called 'static' files. The configuration property in question set's the maximum number of bytes to process per iteration for the static files monitored.","default":"50M","type":"size"},{"name":"event_batch_size","description":"When Fluent Bit is processing files in event based mode the amount ofdata available for consumption could be too much and cause the input plugin to over extend and smother other pluginsThe configuration property sets the maximum number of bytes to process per iteration for the files monitored (in event mode).","default":"50M","type":"size"},{"name":"skip_long_lines","description":"if a monitored file reach it buffer capacity due to a very long line (buffer_max_size), the default behavior is to stop monitoring that file. This option alter that behavior and instruct Fluent Bit to skip long lines and continue processing other lines that fits into the buffer.","default":"false","type":"boolean"},{"name":"exit_on_eof","description":"exit Fluent Bit when reaching EOF on a monitored file.","default":"false","type":"boolean"},{"name":"skip_empty_lines","description":"Allows to skip empty lines.","default":"false","type":"boolean"},{"name":"inotify_watcher","description":"set to false to use file stat watcher instead of inotify.","default":"true","type":"boolean"},{"name":"parser","description":"specify the parser name to process an unstructured message.","default":null,"type":"string"},{"name":"tag_regex","description":"set a regex to extract fields from the file name and use them later to compose the Tag.","default":null,"type":"string"},{"name":"db","description":"set a database file to keep track of monitored files and it offsets.","default":null,"type":"string"},{"name":"db.sync","description":"set a database sync method. values: extra, full, normal and off.","default":"normal","type":"string"},{"name":"db.locking","description":"set exclusive locking mode, increase performance but don't allow external connections to the database file.","default":"false","type":"boolean"},{"name":"db.journal_mode","description":"Option to provide WAL configuration for Work Ahead Logging mechanism (WAL). Enabling WAL provides higher performance. Note that WAL is not compatible with shared network file systems.","default":"WAL","type":"string"},{"name":"multiline","description":"if enabled, the plugin will try to discover multiline messages and use the proper parsers to compose the outgoing messages. Note that when this option is enabled the Parser option is not used.","default":"false","type":"boolean"},{"name":"multiline_flush","description":"wait period time in seconds to process queued multiline messages.","default":"4","type":"time"},{"name":"parser_firstline","description":"name of the parser that matches the beginning of a multiline message. Note that the regular expression defined in the parser must include a group name (named capture).","default":null,"type":"string"},{"name":"parser_","description":"optional extra parser to interpret and structure multiline entries. This option can be used to define multiple parsers, e.g: Parser_1 ab1, Parser_2 ab2, Parser_N abN.","default":null,"type":"prefixed string"},{"name":"multiline.parser","description":"specify one or multiple multiline parsers: docker, cri, go, java, etc.","default":null,"type":"multiple comma delimited strings"}]}},{"type":"input","name":"dummy","description":"Generate dummy data","properties":{"options":[{"name":"samples","description":"set a number of times to generate event.","default":"0","type":"integer"},{"name":"dummy","description":"set the sample record to be generated. It should be a JSON object.","default":"{\"message\":\"dummy\"}","type":"string"},{"name":"rate","description":"set a number of events per second.","default":"1","type":"integer"},{"name":"start_time_sec","description":"set a dummy base timestamp in seconds.","default":"-1","type":"integer"},{"name":"start_time_nsec","description":"set a dummy base timestamp in nanoseconds.","default":"-1","type":"integer"},{"name":"fixed_timestamp","description":"used a fixed timestamp, allows the message to pre-generated once.","default":"off","type":"boolean"}]}},{"type":"input","name":"dummy_thread","description":"Generate dummy data in a separate thread","properties":{"options":[{"name":"message","description":"Define dummy message","default":"thready dummy","type":"string"},{"name":"samples","description":"Define the number of samples to send","default":"1000000","type":"integer"}]}},{"type":"input","name":"head","description":"Head Input","properties":{"options":[{"name":"file","description":"Set the file","default":null,"type":"string"},{"name":"key","description":"Set the record key","default":"head","type":"string"},{"name":"buf_size","description":"Set the read buffer size","default":"256","type":"integer"},{"name":"split_line","description":"generate key/value pair per line","default":"false","type":"boolean"},{"name":"lines","description":"Line number to read","default":"0","type":"integer"},{"name":"add_path","description":"append filepath to records","default":"false","type":"boolean"},{"name":"interval_sec","description":"Set the collector interval","default":"1","type":"integer"},{"name":"interval_nsec","description":"Set the collector interval (nanoseconds)","default":"0","type":"integer"}]}},{"type":"input","name":"health","description":"Check TCP server health","properties":{"options":[{"name":"alert","description":"Only generate records when the port is down","default":"false","type":"boolean"},{"name":"add_host","description":"Append hostname to each record","default":"false","type":"boolean"},{"name":"add_port","description":"Append port to each record","default":"false","type":"boolean"},{"name":"interval_sec","description":"Set the collector interval","default":"1","type":"integer"},{"name":"interval_nsec","description":"Set the collector interval (nanoseconds)","default":"0","type":"integer"}]}},{"type":"input","name":"http","description":"HTTP","properties":{"options":[{"name":"buffer_max_size","description":"","default":"4M","type":"size"},{"name":"buffer_chunk_size","description":"","default":"512K","type":"size"},{"name":"tag_key","description":"","default":null,"type":"string"},{"name":"successful_response_code","description":"Set successful response code. 200, 201 and 204 are supported.","default":"201","type":"integer"}]}},{"type":"input","name":"collectd","description":"collectd input plugin","properties":{"options":[{"name":"typesdb","description":"Set the types database filename","default":"/usr/share/collectd/types.db","type":"string"}]}},{"type":"input","name":"statsd","description":"StatsD input plugin","properties":{"options":[]}},{"type":"input","name":"opentelemetry","description":"OpenTelemetry","properties":{"options":[{"name":"buffer_max_size","description":"","default":"4M","type":"size"},{"name":"buffer_chunk_size","description":"","default":"512K","type":"size"},{"name":"tag_key","description":"","default":null,"type":"string"},{"name":"successful_response_code","description":"Set successful response code. 200, 201 and 204 are supported.","default":"201","type":"integer"}]}},{"type":"input","name":"nginx_metrics","description":"Nginx status metrics","properties":{"options":[{"name":"status_url","description":"Define URL of stub status handler","default":"/status","type":"string"},{"name":"nginx_plus","description":"Turn on NGINX plus mode","default":"true","type":"boolean"}]}},{"type":"input","name":"serial","description":"Serial input","properties":{"options":[{"name":"file","description":"Set the serial character device file name","default":null,"type":"string"},{"name":"bitrate","description":"Set the serial bitrate (baudrate)","default":null,"type":"string"},{"name":"separator","description":"Set the record separator","default":null,"type":"string"},{"name":"format","description":"Set the serial format: json or none","default":null,"type":"string"},{"name":"min_bytes","description":"Set the serial minimum bytes","default":"0","type":"integer"}]}},{"type":"input","name":"stdin","description":"Standard Input","properties":{"options":[{"name":"parser","description":"Set and use a fluent-bit parser","default":null,"type":"string"},{"name":"buffer_size","description":"Set the read buffer size","default":null,"type":"size"}]}},{"type":"input","name":"syslog","description":"Syslog","properties":{"options":[{"name":"mode","description":"Set the socket mode: unix_tcp, unix_udp, tcp or udp","default":null,"type":"string"},{"name":"path","description":"Set the path for the UNIX socket","default":null,"type":"string"},{"name":"unix_perm","description":"Set the permissions for the UNIX socket","default":null,"type":"string"},{"name":"buffer_chunk_size","description":"Set the buffer chunk size","default":"32768","type":"size"},{"name":"buffer_max_size","description":"Set the buffer chunk size","default":null,"type":"size"},{"name":"parser","description":"Set the parser","default":null,"type":"string"}]}},{"type":"input","name":"tcp","description":"TCP","properties":{"options":[{"name":"format","description":"Set the format: json or none","default":null,"type":"string"},{"name":"separator","description":"Set separator","default":null,"type":"string"},{"name":"chunk_size","description":"Set the chunk size","default":null,"type":"string"},{"name":"buffer_size","description":"Set the buffer size","default":null,"type":"string"}]}},{"type":"input","name":"mqtt","description":"MQTT, listen for Publish messages","properties":{"options":[]}},{"type":"input","name":"lib","description":"Library mode Input","properties":{}},{"type":"input","name":"forward","description":"Fluentd in-forward","properties":{"options":[{"name":"tag_prefix","description":"Prefix incoming tag with the defined value.","default":null,"type":"string"},{"name":"unix_path","description":"The path to unix socket to receive a Forward message.","default":null,"type":"string"},{"name":"unix_perm","description":"Set the permissions for the UNIX socket","default":null,"type":"string"},{"name":"buffer_chunk_size","description":"The buffer memory size used to receive a Forward message.","default":"1024000","type":"size"},{"name":"buffer_max_size","description":"The maximum buffer memory size used to receive a Forward message.","default":"6144000","type":"size"}]}},{"type":"input","name":"random","description":"Random","properties":{"options":[{"name":"samples","description":"Number of samples to send, -1 for infinite","default":"-1","type":"integer"},{"name":"interval_sec","description":"Set the collector interval","default":"1","type":"integer"},{"name":"interval_nsec","description":"Set the collector interval (sub seconds)","default":"0","type":"integer"}]}}],"filters":[{"type":"filter","name":"alter_size","description":"Alter incoming chunk size","properties":{"options":[{"name":"add","description":"add N records to the chunk","default":"0","type":"integer"},{"name":"remove","description":"remove N records from the chunk","default":"0","type":"integer"}]}},{"type":"filter","name":"aws","description":"Add AWS Metadata","properties":{"options":[{"name":"imds_version","description":"Specifies which version of the EC2 instance metadata service will be used: 'v1' or 'v2'. 'v2' may not work if you run Fluent Bit in a container.","default":"v2","type":"string"},{"name":"az","description":"Enable EC2 instance availability zone","default":"true","type":"boolean"},{"name":"ec2_instance_id","description":"Enable EC2 instance ID","default":"true","type":"boolean"},{"name":"ec2_instance_type","description":"Enable EC2 instance type","default":"false","type":"boolean"},{"name":"private_ip","description":"Enable EC2 instance private IP","default":"false","type":"boolean"},{"name":"vpc_id","description":"Enable EC2 instance VPC ID","default":"false","type":"boolean"},{"name":"ami_id","description":"Enable EC2 instance Image ID","default":"false","type":"boolean"},{"name":"account_id","description":"Enable EC2 instance Account ID","default":"false","type":"boolean"},{"name":"hostname","description":"Enable EC2 instance hostname","default":"false","type":"boolean"}]}},{"type":"filter","name":"checklist","description":"Check records and flag them","properties":{"options":[{"name":"file","description":"Specify the file that contains the patterns to lookup.","default":null,"type":"string"},{"name":"mode","description":"Set the check mode: 'exact' or 'partial'.","default":"exact","type":"string"},{"name":"print_query_time","description":"Print to stdout the elapseed query time for every matched record","default":"false","type":"boolean"},{"name":"ignore_case","description":"Compare strings by ignoring case.","default":"false","type":"boolean"},{"name":"lookup_key","description":"Name of the key to lookup.","default":"log","type":"string"},{"name":"record","description":"Name of record key to add and its value, it accept two values,e.g 'record mykey my val'. You can add many 'record' entries as needed.","default":null,"type":"space delimited strings (minimum 2)"}]}},{"type":"filter","name":"record_modifier","description":"modify record","properties":{"options":[{"name":"record","description":"Append fields. This parameter needs key and value pair.","default":null,"type":"space delimited strings (minimum 2)"},{"name":"remove_key","description":"If the key is matched, that field is removed.","default":null,"type":"string"},{"name":"allowlist_key","description":"If the key is not matched, that field is removed.","default":null,"type":"string"},{"name":"whitelist_key","description":"(Alias of allowlist_key)","default":null,"type":"string"}]}},{"type":"filter","name":"throttle","description":"Throttle messages using sliding window algorithm","properties":{"options":[{"name":"rate","description":"Set throttle rate","default":"1","type":"double"},{"name":"window","description":"Set throttle window","default":"5","type":"integer"},{"name":"print_status","description":"Set whether or not to print status information","default":"false","type":"boolean"},{"name":"interval","description":"Set the slide interval","default":"1","type":"string"}]}},{"type":"filter","name":"type_converter","description":"Data type converter","properties":{"options":[{"name":"int_key","description":"Convert integer to other type. e.g. int_key id id_str string","default":null,"type":"space delimited strings (minimum 3)"},{"name":"uint_key","description":"Convert unsinged integer to other type. e.g. uint_key id id_str string","default":null,"type":"space delimited strings (minimum 3)"},{"name":"float_key","description":"Convert float to other type. e.g. float_key ratio id_str string","default":null,"type":"space delimited strings (minimum 3)"},{"name":"str_key","description":"Convert string to other type. e.g. str_key id id_val integer","default":null,"type":"space delimited strings (minimum 3)"}]}},{"type":"filter","name":"kubernetes","description":"Filter to append Kubernetes metadata","properties":{"options":[{"name":"buffer_size","description":"buffer size to receive response from API server","default":"32K","type":"size"},{"name":"tls.debug","description":"set TLS debug level: 0 (no debug), 1 (error), 2 (state change), 3 (info) and 4 (verbose)","default":"0","type":"integer"},{"name":"tls.verify","description":"enable or disable verification of TLS peer certificate","default":"true","type":"boolean"},{"name":"tls.vhost","description":"set optional TLS virtual host","default":null,"type":"string"},{"name":"merge_log","description":"merge 'log' key content as individual keys","default":"false","type":"boolean"},{"name":"merge_parser","description":"specify a 'parser' name to parse the 'log' key content","default":null,"type":"string"},{"name":"merge_log_key","description":"set the 'key' name where the content of 'key' will be placed. Only used if the option 'merge_log' is enabled","default":null,"type":"string"},{"name":"merge_log_trim","description":"remove ending '\\n' or '\\r' characters from the log content","default":"true","type":"boolean"},{"name":"keep_log","description":"keep original log content if it was successfully parsed and merged","default":"true","type":"boolean"},{"name":"kube_url","description":"Kubernetes API server URL","default":"https://kubernetes.default.svc","type":"string"},{"name":"kube_meta_preload_cache_dir","description":"set directory with metadata files","default":null,"type":"string"},{"name":"kube_ca_file","description":"Kubernetes TLS CA file","default":"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt","type":"string"},{"name":"kube_ca_path","description":"Kubernetes TLS ca path","default":null,"type":"string"},{"name":"kube_tag_prefix","description":"prefix used in tag by the input plugin","default":"kube.var.log.containers.","type":"string"},{"name":"kube_token_file","description":"Kubernetes authorization token file","default":"/var/run/secrets/kubernetes.io/serviceaccount/token","type":"string"},{"name":"kube_token_command","description":"command to get Kubernetes authorization token","default":null,"type":"string"},{"name":"labels","description":"include Kubernetes labels on every record","default":"true","type":"boolean"},{"name":"annotations","description":"include Kubernetes annotations on every record","default":"true","type":"boolean"},{"name":"k8s-logging.parser","description":"allow Pods to suggest a parser","default":"false","type":"boolean"},{"name":"k8s-logging.exclude","description":"allow Pods to exclude themselves from the logging pipeline","default":"false","type":"boolean"},{"name":"use_journal","description":"use Journald (Systemd) mode","default":"false","type":"boolean"},{"name":"regex_parser","description":"optional regex parser to extract metadata from container name or container log file name","default":null,"type":"string"},{"name":"dummy_meta","description":"use 'dummy' metadata, do not talk to API server","default":"false","type":"boolean"},{"name":"dns_retries","description":"dns lookup retries N times until the network start working","default":"6","type":"integer"},{"name":"dns_wait_time","description":"dns interval between network status checks","default":"30","type":"time"},{"name":"cache_use_docker_id","description":"fetch K8s meta when docker_id is changed","default":"false","type":"boolean"},{"name":"use_tag_for_meta","description":"use tag associated to retrieve metadata instead of kube-server","default":"false","type":"boolean"},{"name":"use_kubelet","description":"use kubelet to get metadata instead of kube-server","default":"false","type":"boolean"},{"name":"kubelet_host","description":"kubelet host to connect with when using kubelet","default":"127.0.0.1","type":"string"},{"name":"kubelet_port","description":"kubelet port to connect with when using kubelet","default":"10250","type":"integer"},{"name":"kube_token_ttl","description":"kubernetes token ttl, until it is reread from the token file. Default: 10m","default":"10m","type":"time"},{"name":"kube_meta_cache_ttl","description":"configurable TTL for K8s cached metadata. By default, it is set to 0 which means TTL for cache entries is disabled and cache entries are evicted at random when capacity is reached. In order to enable this option, you should set the number to a time interval. For example, set this value to 60 or 60s and cache entries which have been created more than 60s will be evicted","default":"0","type":"time"}]}},{"type":"filter","name":"modify","description":"modify records by applying rules","properties":{"options":[{"name":"Set","description":"Add a key/value pair with key KEY and value VALUE. If KEY already exists, this field is overwritten.","default":null,"type":"string"},{"name":"Add","description":"Add a key/value pair with key KEY and value VALUE if KEY does not exist","default":null,"type":"string"},{"name":"Remove","description":"Remove a key/value pair with key KEY if it exists","default":null,"type":"string"},{"name":"Remove_wildcard","description":"Remove all key/value pairs with key matching wildcard KEY","default":null,"type":"string"},{"name":"Remove_regex","description":"Remove all key/value pairs with key matching regexp KEY","default":null,"type":"string"},{"name":"Rename","description":"Rename a key/value pair with key KEY to RENAMED_KEY if KEY exists AND RENAMED_KEY does not exist","default":null,"type":"string"},{"name":"Hard_Rename","description":"Rename a key/value pair with key KEY to RENAMED_KEY if KEY exists. If RENAMED_KEY already exists, this field is overwritten","default":null,"type":"string"},{"name":"Copy","description":"Copy a key/value pair with key KEY to COPIED_KEY if KEY exists AND COPIED_KEY does not exist","default":null,"type":"string"},{"name":"Hard_copy","description":"Copy a key/value pair with key KEY to COPIED_KEY if KEY exists. If COPIED_KEY already exists, this field is overwritten","default":null,"type":"string"},{"name":"Condition","description":"Set the condition to modify. Key_exists, Key_does_not_exist, A_key_matches, No_key_matches, Key_value_equals, Key_value_does_not_equal, Key_value_matches, Key_value_does_not_match, Matching_keys_have_matching_values and Matching_keys_do_not_have_matching_values are supported.","default":null,"type":"string"}]}},{"type":"filter","name":"multiline","description":"Concatenate multiline messages","properties":{"options":[{"name":"debug_flush","description":"enable debugging for concatenation flush to stdout","default":"false","type":"boolean"},{"name":"buffer","description":"Enable buffered mode. In buffered mode, the filter can concatenate multilines from inputs that ingest records one by one (ex: Forward), rather than in chunks, re-emitting them into the beggining of the pipeline using the in_emitter instance. With buffer off, this filter will not work with most inputs, except tail.","default":"true","type":"boolean"},{"name":"mode","description":"Mode can be 'parser' for regex concat, or 'partial_message' to concat split docker logs.","default":"parser","type":"string"},{"name":"flush_ms","description":"Flush time for pending multiline records","default":"2000","type":"integer"},{"name":"multiline.parser","description":"specify one or multiple multiline parsers: docker, cri, go, java, etc.","default":null,"type":"multiple comma delimited strings"},{"name":"multiline.key_content","description":"specify the key name that holds the content to process.","default":null,"type":"string"},{"name":"emitter_name","description":"","default":null,"type":"string"},{"name":"emitter_storage.type","description":"","default":"memory","type":"string"},{"name":"emitter_mem_buf_limit","description":"set a memory buffer limit to restrict memory usage of emitter","default":"10M","type":"size"}]}},{"type":"filter","name":"nest","description":"nest events by specified field values","properties":{"options":[{"name":"Operation","description":"Select the operation nest or lift","default":null,"type":"string"},{"name":"Wildcard","description":"Nest records which field matches the wildcard","default":null,"type":"string"},{"name":"Nest_under","description":"Nest records matching the Wildcard under this key","default":null,"type":"string"},{"name":"Nested_under","description":"Lift records nested under the Nested_under key","default":null,"type":"string"},{"name":"Add_prefix","description":"Prefix affected keys with this string","default":null,"type":"string"},{"name":"Remove_prefix","description":"Remove prefix from affected keys if it matches this string","default":null,"type":"string"}]}},{"type":"filter","name":"parser","description":"Parse events","properties":{"options":[{"name":"Key_Name","description":"Specify field name in record to parse.","default":null,"type":"string"},{"name":"Parser","description":"Specify the parser name to interpret the field. Multiple Parser entries are allowed (one per line).","default":null,"type":"string"},{"name":"Preserve_Key","description":"Keep original Key_Name field in the parsed result. If false, the field will be removed.","default":null,"type":"boolean"},{"name":"Reserve_Data","description":"Keep all other original fields in the parsed result. If false, all other original fields will be removed.","default":null,"type":"boolean"},{"name":"Unescape_key","description":"(deprecated)","default":null,"type":"deprecated"}]}},{"type":"filter","name":"expect","description":"Validate expected keys and values","properties":{"options":[{"name":"key_exists","description":"check that the given key name exists in the record","default":null,"type":"string"},{"name":"key_not_exists","description":"check that the given key name do not exists in the record","default":null,"type":"string"},{"name":"key_val_is_null","description":"check that the value of the key is NULL","default":null,"type":"string"},{"name":"key_val_is_not_null","description":"check that the value of the key is NOT NULL","default":null,"type":"string"},{"name":"key_val_eq","description":"check that the value of the key equals the given value","default":null,"type":"space delimited strings (minimum 1)"},{"name":"action","description":"action to take when a rule does not match: 'warn', 'exit' or 'result_key'.","default":"warn","type":"string"},{"name":"result_key","description":"specify the key name to append a boolean that indicates rule is matched or not. This key is to be used only when 'action' is 'result_key'.","default":"matched","type":"string"}]}},{"type":"filter","name":"grep","description":"grep events by specified field values","properties":{"options":[{"name":"regex","description":"Keep records in which the content of KEY matches the regular expression.","default":null,"type":"string"},{"name":"exclude","description":"Exclude records in which the content of KEY matches the regular expression.","default":null,"type":"string"}]}},{"type":"filter","name":"rewrite_tag","description":"Rewrite records tags","properties":{"options":[{"name":"rule","description":"","default":null,"type":"space delimited strings (minimum 4)"},{"name":"emitter_name","description":"","default":null,"type":"string"},{"name":"emitter_storage.type","description":"","default":"memory","type":"string"},{"name":"emitter_mem_buf_limit","description":"set a memory buffer limit to restrict memory usage of emitter","default":"10M","type":"size"}]}},{"type":"filter","name":"lua","description":"Lua Scripting Filter","properties":{"options":[{"name":"script","description":"The path of lua script.","default":null,"type":"string"},{"name":"code","description":"String that contains the Lua script source code","default":null,"type":"string"},{"name":"call","description":"Lua function name that will be triggered to do filtering.","default":null,"type":"string"},{"name":"type_int_key","description":"If these keys are matched, the fields are converted to integer. If more than one key, delimit by space.","default":null,"type":"string"},{"name":"type_array_key","description":"If these keys are matched, the fields are converted to array. If more than one key, delimit by space.","default":null,"type":"string"},{"name":"protected_mode","description":"If enabled, Lua script will be executed in protected mode. It prevents to crash when invalid Lua script is executed.","default":"true","type":"boolean"},{"name":"time_as_table","description":"If enabled, Fluent-bit will pass the timestamp as a Lua table with keys \"sec\" for seconds since epoch and \"nsec\" for nanoseconds.","default":"false","type":"boolean"}]}},{"type":"filter","name":"stdout","description":"Filter events to STDOUT","properties":{"options":[]}},{"type":"filter","name":"geoip2","description":"add geoip information to records","properties":{"options":[{"name":"database","description":"Set the geoip2 database path","default":null,"type":"string"},{"name":"lookup_key","description":"Add a lookup_key","default":null,"type":"string"},{"name":"record","description":"Add a record to the output base on geoip2","default":null,"type":"string"}]}},{"type":"filter","name":"nightfall","description":"scans records for sensitive content","properties":{"options":[{"name":"nightfall_api_key","description":"The Nightfall API key to scan your logs with.","default":null,"type":"string"},{"name":"policy_id","description":"The Nightfall policy ID to scan your logs with.","default":null,"type":"string"},{"name":"sampling_rate","description":"The sampling rate for scanning, must be (0,1]. 1 means all logs will be scanned.","default":"1","type":"double"},{"name":"tls.debug","description":"Set TLS debug level: 0 (no debug), 1 (error), 2 (state change), 3 (info) and 4 (verbose)","default":"0","type":"integer"},{"name":"tls.verify","description":"Enable or disable verification of TLS peer certificate","default":"true","type":"boolean"},{"name":"tls.vhost","description":"Set optional TLS virtual host","default":null,"type":"string"},{"name":"tls.ca_path","description":"Path to root certificates on the system","default":null,"type":"string"}]}}],"outputs":[{"type":"output","name":"azure","description":"Send events to Azure HTTP Event Collector","properties":{"options":[{"name":"customer_id","description":"Customer ID or WorkspaceID string.","default":null,"type":"string"},{"name":"shared_key","description":"The primary or the secondary Connected Sources client authentication key.","default":null,"type":"string"},{"name":"log_type","description":"The name of the event type.","default":"fluentbit","type":"string"},{"name":"time_key","description":"Optional parameter to specify the key name where the timestamp will be stored.","default":"@timestamp","type":"string"},{"name":"time_generated","description":"If enabled, the HTTP request header 'time-generated-field' will be included so Azure can override the timestamp with the key specified by 'time_key' option.","default":"false","type":"boolean"}],"networking":[{"name":"net.dns.mode","description":"Select the primary DNS connection type (TCP or UDP)","default":null,"type":"string"},{"name":"net.dns.resolver","description":"Select the primary DNS resolver type (LEGACY or ASYNC)","default":null,"type":"string"},{"name":"net.dns.prefer_ipv4","description":"Prioritize IPv4 DNS results when trying to establish a connection","default":"false","type":"boolean"},{"name":"net.keepalive","description":"Enable or disable Keepalive support","default":"true","type":"boolean"},{"name":"net.keepalive_idle_timeout","description":"Set maximum time allowed for an idle Keepalive connection","default":"30s","type":"time"},{"name":"net.connect_timeout","description":"Set maximum time allowed to establish a connection, this time includes the TLS handshake","default":"10s","type":"time"},{"name":"net.connect_timeout_log_error","description":"On connection timeout, specify if it should log an error. When disabled, the timeout is logged as a debug message","default":"true","type":"boolean"},{"name":"net.source_address","description":"Specify network address to bind for data traffic","default":null,"type":"string"},{"name":"net.keepalive_max_recycle","description":"Set maximum number of times a keepalive connection can be used before it is retired.","default":"2000","type":"integer"}],"network_tls":[{"name":"tls","description":"Enable or disable TLS/SSL support","default":"off","type":"boolean"},{"name":"tls.verify","description":"Force certificate validation","default":"on","type":"boolean"},{"name":"tls.debug","description":"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose","default":"1","type":"integer"},{"name":"tls.ca_file","description":"Absolute path to CA certificate file","default":null,"type":"string"},{"name":"tls.ca_path","description":"Absolute path to scan for certificate files","default":null,"type":"string"},{"name":"tls.crt_file","description":"Absolute path to Certificate file","default":null,"type":"string"},{"name":"tls.key_file","description":"Absolute path to private Key file","default":null,"type":"string"},{"name":"tls.key_passwd","description":"Optional password for tls.key_file file","default":null,"type":"string"},{"name":"tls.vhost","description":"Hostname to be used for TLS SNI extension","default":null,"type":"string"}]}},{"type":"output","name":"azure_blob","description":"Azure Blob Storage","properties":{"options":[{"name":"account_name","description":"Azure account name (mandatory)","default":null,"type":"string"},{"name":"container_name","description":"Container name (mandatory)","default":null,"type":"string"},{"name":"auto_create_container","description":"Auto create container if it don't exists","default":"true","type":"boolean"},{"name":"blob_type","description":"Set the block type: appendblob or blockblob","default":"appendblob","type":"string"},{"name":"compress","description":"Set payload compression in network transfer. Option available is 'gzip'","default":null,"type":"string"},{"name":"compress_blob","description":"Enable block blob GZIP compression in the final blob file. This option is not compatible with 'appendblob' block type","default":"false","type":"boolean"},{"name":"emulator_mode","description":"Use emulator mode, enable it if you want to use Azurite","default":"false","type":"boolean"},{"name":"shared_key","description":"Azure shared key","default":null,"type":"string"},{"name":"endpoint","description":"Custom full URL endpoint to use an emulator","default":null,"type":"string"},{"name":"path","description":"Set a path for your blob","default":null,"type":"string"},{"name":"date_key","description":"Name of the key that will have the record timestamp","default":"@timestamp","type":"string"}],"networking":[{"name":"net.dns.mode","description":"Select the primary DNS connection type (TCP or UDP)","default":null,"type":"string"},{"name":"net.dns.resolver","description":"Select the primary DNS resolver type (LEGACY or ASYNC)","default":null,"type":"string"},{"name":"net.dns.prefer_ipv4","description":"Prioritize IPv4 DNS results when trying to establish a connection","default":"false","type":"boolean"},{"name":"net.keepalive","description":"Enable or disable Keepalive support","default":"true","type":"boolean"},{"name":"net.keepalive_idle_timeout","description":"Set maximum time allowed for an idle Keepalive connection","default":"30s","type":"time"},{"name":"net.connect_timeout","description":"Set maximum time allowed to establish a connection, this time includes the TLS handshake","default":"10s","type":"time"},{"name":"net.connect_timeout_log_error","description":"On connection timeout, specify if it should log an error. When disabled, the timeout is logged as a debug message","default":"true","type":"boolean"},{"name":"net.source_address","description":"Specify network address to bind for data traffic","default":null,"type":"string"},{"name":"net.keepalive_max_recycle","description":"Set maximum number of times a keepalive connection can be used before it is retired.","default":"2000","type":"integer"}],"network_tls":[{"name":"tls","description":"Enable or disable TLS/SSL support","default":"off","type":"boolean"},{"name":"tls.verify","description":"Force certificate validation","default":"on","type":"boolean"},{"name":"tls.debug","description":"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose","default":"1","type":"integer"},{"name":"tls.ca_file","description":"Absolute path to CA certificate file","default":null,"type":"string"},{"name":"tls.ca_path","description":"Absolute path to scan for certificate files","default":null,"type":"string"},{"name":"tls.crt_file","description":"Absolute path to Certificate file","default":null,"type":"string"},{"name":"tls.key_file","description":"Absolute path to private Key file","default":null,"type":"string"},{"name":"tls.key_passwd","description":"Optional password for tls.key_file file","default":null,"type":"string"},{"name":"tls.vhost","description":"Hostname to be used for TLS SNI extension","default":null,"type":"string"}]}},{"type":"output","name":"bigquery","description":"Send events to BigQuery via streaming insert","properties":{"options":[{"name":"google_service_credentials","description":"Set the path for the google service credentials file","default":null,"type":"string"},{"name":"enable_identity_federation","description":"Enable identity federation","default":"false","type":"boolean"},{"name":"aws_region","description":"Enable identity federation","default":null,"type":"string"},{"name":"project_number","description":"Set project number","default":null,"type":"string"},{"name":"pool_id","description":"Set the pool id","default":null,"type":"string"},{"name":"provider_id","description":"Set the provider id","default":null,"type":"string"},{"name":"google_service_account","description":"Set the google service account","default":null,"type":"string"},{"name":"service_account_email","description":"Set the service account email","default":null,"type":"string"},{"name":"service_account_secret","description":"Set the service account secret","default":null,"type":"string"},{"name":"project_id","description":"Set the project id","default":null,"type":"string"},{"name":"dataset_id","description":"Set the dataset id","default":null,"type":"string"},{"name":"table_id","description":"Set the table id","default":null,"type":"string"},{"name":"skip_invalid_rows","description":"Enable skipping of invalid rows","default":"false","type":"boolean"},{"name":"ignore_unknown_values","description":"Enable ignoring unknown value","default":"false","type":"boolean"}],"networking":[{"name":"net.dns.mode","description":"Select the primary DNS connection type (TCP or UDP)","default":null,"type":"string"},{"name":"net.dns.resolver","description":"Select the primary DNS resolver type (LEGACY or ASYNC)","default":null,"type":"string"},{"name":"net.dns.prefer_ipv4","description":"Prioritize IPv4 DNS results when trying to establish a connection","default":"false","type":"boolean"},{"name":"net.keepalive","description":"Enable or disable Keepalive support","default":"true","type":"boolean"},{"name":"net.keepalive_idle_timeout","description":"Set maximum time allowed for an idle Keepalive connection","default":"30s","type":"time"},{"name":"net.connect_timeout","description":"Set maximum time allowed to establish a connection, this time includes the TLS handshake","default":"10s","type":"time"},{"name":"net.connect_timeout_log_error","description":"On connection timeout, specify if it should log an error. When disabled, the timeout is logged as a debug message","default":"true","type":"boolean"},{"name":"net.source_address","description":"Specify network address to bind for data traffic","default":null,"type":"string"},{"name":"net.keepalive_max_recycle","description":"Set maximum number of times a keepalive connection can be used before it is retired.","default":"2000","type":"integer"}],"network_tls":[{"name":"tls","description":"Enable or disable TLS/SSL support","default":"off","type":"boolean"},{"name":"tls.verify","description":"Force certificate validation","default":"on","type":"boolean"},{"name":"tls.debug","description":"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose","default":"1","type":"integer"},{"name":"tls.ca_file","description":"Absolute path to CA certificate file","default":null,"type":"string"},{"name":"tls.ca_path","description":"Absolute path to scan for certificate files","default":null,"type":"string"},{"name":"tls.crt_file","description":"Absolute path to Certificate file","default":null,"type":"string"},{"name":"tls.key_file","description":"Absolute path to private Key file","default":null,"type":"string"},{"name":"tls.key_passwd","description":"Optional password for tls.key_file file","default":null,"type":"string"},{"name":"tls.vhost","description":"Hostname to be used for TLS SNI extension","default":null,"type":"string"}]}},{"type":"output","name":"counter","description":"Records counter","properties":{"options":[]}},{"type":"output","name":"datadog","description":"Send events to DataDog HTTP Event Collector","properties":{"options":[{"name":"compress","description":"compresses the payload in GZIP format, Datadog supports and recommends setting this to 'gzip'.","default":"false","type":"string"},{"name":"apikey","description":"Datadog API key","default":null,"type":"string"},{"name":"dd_service","description":"The human readable name for your service generating the logs - the name of your application or database.","default":null,"type":"string"},{"name":"dd_source","description":"A human readable name for the underlying technology of your service. For example, 'postgres' or 'nginx'.","default":null,"type":"string"},{"name":"dd_tags","description":"The tags you want to assign to your logs in Datadog.","default":null,"type":"string"},{"name":"proxy","description":"Specify an HTTP Proxy. The expected format of this value is http://host:port. Note that https is not supported yet.","default":null,"type":"string"},{"name":"include_tag_key","description":"If enabled, tag is appended to output. The key name is used 'tag_key' property.","default":"false","type":"boolean"},{"name":"tag_key","description":"The key name of tag. If 'include_tag_key' is false, This property is ignored","default":"tagkey","type":"string"},{"name":"dd_message_key","description":"By default, the plugin searches for the key 'log' and remap the value to the key 'message'. If the property is set, the plugin will search the property name key.","default":null,"type":"string"},{"name":"provider","description":"To activate the remapping, specify configuration flag provider with value 'ecs'","default":null,"type":"string"},{"name":"json_date_key","description":"Date key name for output.","default":"timestamp","type":"string"}],"networking":[{"name":"net.dns.mode","description":"Select the primary DNS connection type (TCP or UDP)","default":null,"type":"string"},{"name":"net.dns.resolver","description":"Select the primary DNS resolver type (LEGACY or ASYNC)","default":null,"type":"string"},{"name":"net.dns.prefer_ipv4","description":"Prioritize IPv4 DNS results when trying to establish a connection","default":"false","type":"boolean"},{"name":"net.keepalive","description":"Enable or disable Keepalive support","default":"true","type":"boolean"},{"name":"net.keepalive_idle_timeout","description":"Set maximum time allowed for an idle Keepalive connection","default":"30s","type":"time"},{"name":"net.connect_timeout","description":"Set maximum time allowed to establish a connection, this time includes the TLS handshake","default":"10s","type":"time"},{"name":"net.connect_timeout_log_error","description":"On connection timeout, specify if it should log an error. When disabled, the timeout is logged as a debug message","default":"true","type":"boolean"},{"name":"net.source_address","description":"Specify network address to bind for data traffic","default":null,"type":"string"},{"name":"net.keepalive_max_recycle","description":"Set maximum number of times a keepalive connection can be used before it is retired.","default":"2000","type":"integer"}],"network_tls":[{"name":"tls","description":"Enable or disable TLS/SSL support","default":"off","type":"boolean"},{"name":"tls.verify","description":"Force certificate validation","default":"on","type":"boolean"},{"name":"tls.debug","description":"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose","default":"1","type":"integer"},{"name":"tls.ca_file","description":"Absolute path to CA certificate file","default":null,"type":"string"},{"name":"tls.ca_path","description":"Absolute path to scan for certificate files","default":null,"type":"string"},{"name":"tls.crt_file","description":"Absolute path to Certificate file","default":null,"type":"string"},{"name":"tls.key_file","description":"Absolute path to private Key file","default":null,"type":"string"},{"name":"tls.key_passwd","description":"Optional password for tls.key_file file","default":null,"type":"string"},{"name":"tls.vhost","description":"Hostname to be used for TLS SNI extension","default":null,"type":"string"}]}},{"type":"output","name":"es","description":"Elasticsearch","properties":{"options":[{"name":"index","description":"Set an index name","default":"fluent-bit","type":"string"},{"name":"type","description":"Set the document type property","default":"_doc","type":"string"},{"name":"suppress_type_name","description":"If true, mapping types is removed. (for v7.0.0 or later)","default":"false","type":"boolean"},{"name":"http_user","description":"Optional username credential for Elastic X-Pack access","default":null,"type":"string"},{"name":"http_passwd","description":"Password for user defined in HTTP_User","default":"","type":"string"},{"name":"cloud_id","description":"Elastic cloud ID of the cluster to connect to","default":null,"type":"string"},{"name":"cloud_auth","description":"Elastic cloud authentication credentials","default":null,"type":"string"},{"name":"aws_auth","description":"Enable AWS Sigv4 Authentication","default":"false","type":"boolean"},{"name":"aws_region","description":"AWS Region of your Amazon OpenSearch Service cluster","default":null,"type":"string"},{"name":"aws_sts_endpoint","description":"Custom endpoint for the AWS STS API, used with the AWS_Role_ARN option","default":null,"type":"string"},{"name":"aws_role_arn","description":"AWS IAM Role to assume to put records to your Amazon OpenSearch cluster","default":null,"type":"string"},{"name":"aws_external_id","description":"External ID for the AWS IAM Role specified with `aws_role_arn`","default":null,"type":"string"},{"name":"logstash_format","description":"Enable Logstash format compatibility","default":"false","type":"boolean"},{"name":"logstash_prefix","description":"When Logstash_Format is enabled, the Index name is composed using a prefix and the date, e.g: If Logstash_Prefix is equals to 'mydata' your index will become 'mydata-YYYY.MM.DD'. The last string appended belongs to the date when the data is being generated","default":"logstash","type":"string"},{"name":"logstash_prefix_key","description":"When included: the value in the record that belongs to the key will be looked up and over-write the Logstash_Prefix for index generation. If the key/value is not found in the record then the Logstash_Prefix option will act as a fallback. Nested keys are supported through record accessor pattern","default":null,"type":"string"},{"name":"logstash_dateformat","description":"Time format (based on strftime) to generate the second part of the Index name","default":"%Y.%m.%d","type":"string"},{"name":"time_key","description":"When Logstash_Format is enabled, each record will get a new timestamp field. The Time_Key property defines the name of that field","default":"@timestamp","type":"string"},{"name":"time_key_format","description":"When Logstash_Format is enabled, this property defines the format of the timestamp","default":"%Y-%m-%dT%H:%M:%S","type":"string"},{"name":"time_key_nanos","description":"When Logstash_Format is enabled, enabling this property sends nanosecond precision timestamps","default":"false","type":"boolean"},{"name":"include_tag_key","description":"When enabled, it append the Tag name to the record","default":"false","type":"boolean"},{"name":"tag_key","description":"When Include_Tag_Key is enabled, this property defines the key name for the tag","default":"flb-key","type":"string"},{"name":"buffer_size","description":"Specify the buffer size used to read the response from the Elasticsearch HTTP service. This option is useful for debugging purposes where is required to read full responses, note that response size grows depending of the number of records inserted. To set an unlimited amount of memory set this value to 'false', otherwise the value must be according to the Unit Size specification","default":"512k","type":"size"},{"name":"path","description":"Elasticsearch accepts new data on HTTP query path '/_bulk'. But it is also possible to serve Elasticsearch behind a reverse proxy on a subpath. This option defines such path on the fluent-bit side. It simply adds a path prefix in the indexing HTTP POST URI","default":null,"type":"string"},{"name":"pipeline","description":"Newer versions of Elasticsearch allows to setup filters called pipelines. This option allows to define which pipeline the database should use. For performance reasons is strongly suggested to do parsing and filtering on Fluent Bit side, avoid pipelines","default":null,"type":"string"},{"name":"generate_id","description":"When enabled, generate _id for outgoing records. This prevents duplicate records when retrying ES","default":"false","type":"boolean"},{"name":"write_operation","description":"Operation to use to write in bulk requests","default":"create","type":"string"},{"name":"id_key","description":"If set, _id will be the value of the key from incoming record.","default":null,"type":"string"},{"name":"replace_dots","description":"When enabled, replace field name dots with underscore, required by Elasticsearch 2.0-2.3.","default":"false","type":"boolean"},{"name":"current_time_index","description":"Use current time for index generation instead of message record","default":"false","type":"boolean"},{"name":"trace_output","description":"When enabled print the Elasticsearch API calls to stdout (for diag only)","default":"false","type":"boolean"},{"name":"trace_error","description":"When enabled print the Elasticsearch exception to stderr (for diag only)","default":"false","type":"boolean"}],"networking":[{"name":"net.dns.mode","description":"Select the primary DNS connection type (TCP or UDP)","default":null,"type":"string"},{"name":"net.dns.resolver","description":"Select the primary DNS resolver type (LEGACY or ASYNC)","default":null,"type":"string"},{"name":"net.dns.prefer_ipv4","description":"Prioritize IPv4 DNS results when trying to establish a connection","default":"false","type":"boolean"},{"name":"net.keepalive","description":"Enable or disable Keepalive support","default":"true","type":"boolean"},{"name":"net.keepalive_idle_timeout","description":"Set maximum time allowed for an idle Keepalive connection","default":"30s","type":"time"},{"name":"net.connect_timeout","description":"Set maximum time allowed to establish a connection, this time includes the TLS handshake","default":"10s","type":"time"},{"name":"net.connect_timeout_log_error","description":"On connection timeout, specify if it should log an error. When disabled, the timeout is logged as a debug message","default":"true","type":"boolean"},{"name":"net.source_address","description":"Specify network address to bind for data traffic","default":null,"type":"string"},{"name":"net.keepalive_max_recycle","description":"Set maximum number of times a keepalive connection can be used before it is retired.","default":"2000","type":"integer"}],"network_tls":[{"name":"tls","description":"Enable or disable TLS/SSL support","default":"off","type":"boolean"},{"name":"tls.verify","description":"Force certificate validation","default":"on","type":"boolean"},{"name":"tls.debug","description":"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose","default":"1","type":"integer"},{"name":"tls.ca_file","description":"Absolute path to CA certificate file","default":null,"type":"string"},{"name":"tls.ca_path","description":"Absolute path to scan for certificate files","default":null,"type":"string"},{"name":"tls.crt_file","description":"Absolute path to Certificate file","default":null,"type":"string"},{"name":"tls.key_file","description":"Absolute path to private Key file","default":null,"type":"string"},{"name":"tls.key_passwd","description":"Optional password for tls.key_file file","default":null,"type":"string"},{"name":"tls.vhost","description":"Hostname to be used for TLS SNI extension","default":null,"type":"string"}]}},{"type":"output","name":"exit","description":"Exit after a number of flushes (test purposes)","properties":{"options":[{"name":"flush_count","description":"","default":"1","type":"integer"}]}},{"type":"output","name":"file","description":"Generate log file","properties":{"options":[{"name":"path","description":"Absolute path to store the files. This parameter is optional","default":null,"type":"string"},{"name":"file","description":"Name of the target file to write the records. If 'path' is specified, the value is prefixed","default":null,"type":"string"},{"name":"format","description":"Specify the output data format, the available options are: plain (json), csv, ltsv and template. If no value is set the outgoing data is formatted using the tag and the record in json","default":null,"type":"string"},{"name":"delimiter","description":"Set a custom delimiter for the records","default":null,"type":"string"},{"name":"label_delimiter","description":"Set a custom label delimiter, to be used with 'ltsv' format","default":null,"type":"string"},{"name":"template","description":"Set a custom template format for the data","default":"{time} {message}","type":"string"},{"name":"csv_column_names","description":"Add column names (keys) in the first line of the target file","default":"false","type":"boolean"},{"name":"mkdir","description":"Recursively create output directory if it does not exist. Permissions set to 0755","default":"false","type":"boolean"}]}},{"type":"output","name":"forward","description":"Forward (Fluentd protocol)","properties":{"options":[{"name":"time_as_integer","description":"Set timestamp in integer format (compat mode for old Fluentd v0.12)","default":"false","type":"boolean"},{"name":"shared_key","description":"Shared key for authentication","default":null,"type":"string"},{"name":"self_hostname","description":"Hostname","default":null,"type":"string"},{"name":"empty_shared_key","description":"Set an empty shared key for authentication","default":"false","type":"boolean"},{"name":"send_options","description":"Send 'forward protocol options' to remote endpoint","default":"false","type":"boolean"},{"name":"require_ack_response","description":"Require that remote endpoint confirms data reception","default":"false","type":"boolean"},{"name":"username","description":"Username for authentication","default":"","type":"string"},{"name":"password","description":"Password for authentication","default":"","type":"string"},{"name":"unix_path","description":"Path to unix socket. It is ignored when 'upstream' property is set","default":null,"type":"string"},{"name":"upstream","description":"Path to 'upstream' configuration file (define multiple nodes)","default":null,"type":"string"},{"name":"tag","description":"Set a custom Tag for the outgoing records","default":null,"type":"string"},{"name":"compress","description":"Compression mode","default":null,"type":"string"}],"networking":[{"name":"net.dns.mode","description":"Select the primary DNS connection type (TCP or UDP)","default":null,"type":"string"},{"name":"net.dns.resolver","description":"Select the primary DNS resolver type (LEGACY or ASYNC)","default":null,"type":"string"},{"name":"net.dns.prefer_ipv4","description":"Prioritize IPv4 DNS results when trying to establish a connection","default":"false","type":"boolean"},{"name":"net.keepalive","description":"Enable or disable Keepalive support","default":"true","type":"boolean"},{"name":"net.keepalive_idle_timeout","description":"Set maximum time allowed for an idle Keepalive connection","default":"30s","type":"time"},{"name":"net.connect_timeout","description":"Set maximum time allowed to establish a connection, this time includes the TLS handshake","default":"10s","type":"time"},{"name":"net.connect_timeout_log_error","description":"On connection timeout, specify if it should log an error. When disabled, the timeout is logged as a debug message","default":"true","type":"boolean"},{"name":"net.source_address","description":"Specify network address to bind for data traffic","default":null,"type":"string"},{"name":"net.keepalive_max_recycle","description":"Set maximum number of times a keepalive connection can be used before it is retired.","default":"2000","type":"integer"}],"network_tls":[{"name":"tls","description":"Enable or disable TLS/SSL support","default":"off","type":"boolean"},{"name":"tls.verify","description":"Force certificate validation","default":"on","type":"boolean"},{"name":"tls.debug","description":"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose","default":"1","type":"integer"},{"name":"tls.ca_file","description":"Absolute path to CA certificate file","default":null,"type":"string"},{"name":"tls.ca_path","description":"Absolute path to scan for certificate files","default":null,"type":"string"},{"name":"tls.crt_file","description":"Absolute path to Certificate file","default":null,"type":"string"},{"name":"tls.key_file","description":"Absolute path to private Key file","default":null,"type":"string"},{"name":"tls.key_passwd","description":"Optional password for tls.key_file file","default":null,"type":"string"},{"name":"tls.vhost","description":"Hostname to be used for TLS SNI extension","default":null,"type":"string"}]}},{"type":"output","name":"http","description":"HTTP Output","properties":{"options":[{"name":"proxy","description":"Specify an HTTP Proxy. The expected format of this value is http://host:port. ","default":null,"type":"string"},{"name":"allow_duplicated_headers","description":"Specify if duplicated headers are allowed or not","default":"true","type":"boolean"},{"name":"log_response_payload","description":"Specify if the response paylod should be logged or not","default":"true","type":"boolean"},{"name":"http_user","description":"Set HTTP auth user","default":null,"type":"string"},{"name":"http_passwd","description":"Set HTTP auth password","default":"","type":"string"},{"name":"aws_auth","description":"Enable AWS SigV4 authentication","default":"false","type":"boolean"},{"name":"aws_service","description":"AWS destination service code, used by SigV4 authentication","default":null,"type":"string"},{"name":"aws_region","description":"AWS region of your service","default":null,"type":"string"},{"name":"aws_sts_endpoint","description":"Custom endpoint for the AWS STS API, used with the `aws_role_arn` option","default":null,"type":"string"},{"name":"aws_role_arn","description":"ARN of an IAM role to assume (ex. for cross account access)","default":null,"type":"string"},{"name":"aws_external_id","description":"Specify an external ID for the STS API, can be used with the `aws_role_arn` parameter if your role requires an external ID.","default":null,"type":"string"},{"name":"header_tag","description":"Set a HTTP header which value is the Tag","default":null,"type":"string"},{"name":"format","description":"Set desired payload format: json, json_stream, json_lines, gelf or msgpack","default":null,"type":"string"},{"name":"json_date_format","description":"Specify the format of the date, supported formats: double, iso8601 (e.g: 2018-05-30T09:39:52.000681Z), java_sql_timestamp (e.g: 2018-05-30 09:39:52.000681, useful for AWS Athena), and epoch.","default":null,"type":"string"},{"name":"json_date_key","description":"Specify the name of the date field in output","default":"date","type":"string"},{"name":"compress","description":"Set payload compression mechanism. Option available is 'gzip'","default":null,"type":"string"},{"name":"header","description":"Add a HTTP header key/value pair. Multiple headers can be set","default":null,"type":"space delimited strings (minimum 1)"},{"name":"uri","description":"Specify an optional HTTP URI for the target web server, e.g: /something","default":null,"type":"string"},{"name":"gelf_timestamp_key","description":"Specify the key to use for 'timestamp' in gelf format","default":null,"type":"string"},{"name":"gelf_host_key","description":"Specify the key to use for the 'host' in gelf format","default":null,"type":"string"},{"name":"gelf_short_message_key","description":"Specify the key to use as the 'short' message in gelf format","default":null,"type":"string"},{"name":"gelf_full_message_key","description":"Specify the key to use for the 'full' message in gelf format","default":null,"type":"string"},{"name":"gelf_level_key","description":"Specify the key to use for the 'level' in gelf format","default":null,"type":"string"},{"name":"body_key","description":"Specify the key which contains the body","default":null,"type":"string"},{"name":"headers_key","description":"Specify the key which contains the headers","default":null,"type":"string"}],"networking":[{"name":"net.dns.mode","description":"Select the primary DNS connection type (TCP or UDP)","default":null,"type":"string"},{"name":"net.dns.resolver","description":"Select the primary DNS resolver type (LEGACY or ASYNC)","default":null,"type":"string"},{"name":"net.dns.prefer_ipv4","description":"Prioritize IPv4 DNS results when trying to establish a connection","default":"false","type":"boolean"},{"name":"net.keepalive","description":"Enable or disable Keepalive support","default":"true","type":"boolean"},{"name":"net.keepalive_idle_timeout","description":"Set maximum time allowed for an idle Keepalive connection","default":"30s","type":"time"},{"name":"net.connect_timeout","description":"Set maximum time allowed to establish a connection, this time includes the TLS handshake","default":"10s","type":"time"},{"name":"net.connect_timeout_log_error","description":"On connection timeout, specify if it should log an error. When disabled, the timeout is logged as a debug message","default":"true","type":"boolean"},{"name":"net.source_address","description":"Specify network address to bind for data traffic","default":null,"type":"string"},{"name":"net.keepalive_max_recycle","description":"Set maximum number of times a keepalive connection can be used before it is retired.","default":"2000","type":"integer"}],"network_tls":[{"name":"tls","description":"Enable or disable TLS/SSL support","default":"off","type":"boolean"},{"name":"tls.verify","description":"Force certificate validation","default":"on","type":"boolean"},{"name":"tls.debug","description":"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose","default":"1","type":"integer"},{"name":"tls.ca_file","description":"Absolute path to CA certificate file","default":null,"type":"string"},{"name":"tls.ca_path","description":"Absolute path to scan for certificate files","default":null,"type":"string"},{"name":"tls.crt_file","description":"Absolute path to Certificate file","default":null,"type":"string"},{"name":"tls.key_file","description":"Absolute path to private Key file","default":null,"type":"string"},{"name":"tls.key_passwd","description":"Optional password for tls.key_file file","default":null,"type":"string"},{"name":"tls.vhost","description":"Hostname to be used for TLS SNI extension","default":null,"type":"string"}]}},{"type":"output","name":"influxdb","description":"InfluxDB Time Series","properties":{"options":[{"name":"database","description":"Set the database name.","default":"fluentbit","type":"string"},{"name":"bucket","description":"Specify the bucket name, used on InfluxDB API v2.","default":null,"type":"string"},{"name":"org","description":"Set the Organization name.","default":"fluent","type":"string"},{"name":"sequence_tag","description":"Specify the sequence tag.","default":null,"type":"string"},{"name":"uri","description":"Specify a custom URI endpoint (must start with '/').","default":null,"type":"string"},{"name":"http_user","description":"HTTP Basic Auth username.","default":null,"type":"string"},{"name":"http_passwd","description":"HTTP Basic Auth password.","default":"","type":"string"},{"name":"http_token","description":"Set InfluxDB HTTP Token API v2.","default":null,"type":"string"},{"name":"http_header","description":"Add a HTTP header key/value pair. Multiple headers can be set","default":null,"type":"space delimited strings (minimum 1)"},{"name":"auto_tags","description":"Automatically tag keys where value is string.","default":"false","type":"boolean"},{"name":"tag_keys","description":"Space separated list of keys that needs to be tagged.","default":null,"type":"boolean"}],"networking":[{"name":"net.dns.mode","description":"Select the primary DNS connection type (TCP or UDP)","default":null,"type":"string"},{"name":"net.dns.resolver","description":"Select the primary DNS resolver type (LEGACY or ASYNC)","default":null,"type":"string"},{"name":"net.dns.prefer_ipv4","description":"Prioritize IPv4 DNS results when trying to establish a connection","default":"false","type":"boolean"},{"name":"net.keepalive","description":"Enable or disable Keepalive support","default":"true","type":"boolean"},{"name":"net.keepalive_idle_timeout","description":"Set maximum time allowed for an idle Keepalive connection","default":"30s","type":"time"},{"name":"net.connect_timeout","description":"Set maximum time allowed to establish a connection, this time includes the TLS handshake","default":"10s","type":"time"},{"name":"net.connect_timeout_log_error","description":"On connection timeout, specify if it should log an error. When disabled, the timeout is logged as a debug message","default":"true","type":"boolean"},{"name":"net.source_address","description":"Specify network address to bind for data traffic","default":null,"type":"string"},{"name":"net.keepalive_max_recycle","description":"Set maximum number of times a keepalive connection can be used before it is retired.","default":"2000","type":"integer"}],"network_tls":[{"name":"tls","description":"Enable or disable TLS/SSL support","default":"off","type":"boolean"},{"name":"tls.verify","description":"Force certificate validation","default":"on","type":"boolean"},{"name":"tls.debug","description":"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose","default":"1","type":"integer"},{"name":"tls.ca_file","description":"Absolute path to CA certificate file","default":null,"type":"string"},{"name":"tls.ca_path","description":"Absolute path to scan for certificate files","default":null,"type":"string"},{"name":"tls.crt_file","description":"Absolute path to Certificate file","default":null,"type":"string"},{"name":"tls.key_file","description":"Absolute path to private Key file","default":null,"type":"string"},{"name":"tls.key_passwd","description":"Optional password for tls.key_file file","default":null,"type":"string"},{"name":"tls.vhost","description":"Hostname to be used for TLS SNI extension","default":null,"type":"string"}]}},{"type":"output","name":"logdna","description":"LogDNA","properties":{"options":[{"name":"logdna_host","description":"LogDNA Host address","default":"logs.logdna.com","type":"string"},{"name":"logdna_port","description":"LogDNA TCP port","default":"443","type":"integer"},{"name":"api_key","description":"Logdna API key","default":null,"type":"string"},{"name":"hostname","description":"Local Server or device host name","default":null,"type":"string"},{"name":"mac","description":"MAC address (optional)","default":"","type":"string"},{"name":"ip","description":"IP address (optional)","default":"","type":"string"},{"name":"tags","description":"Tags (optional)","default":"","type":"multiple comma delimited strings"},{"name":"file","description":"Name of the monitored file (optional)","default":null,"type":"string"},{"name":"app","description":"Name of the application generating the data (optional)","default":"Fluent Bit","type":"string"}],"networking":[{"name":"net.dns.mode","description":"Select the primary DNS connection type (TCP or UDP)","default":null,"type":"string"},{"name":"net.dns.resolver","description":"Select the primary DNS resolver type (LEGACY or ASYNC)","default":null,"type":"string"},{"name":"net.dns.prefer_ipv4","description":"Prioritize IPv4 DNS results when trying to establish a connection","default":"false","type":"boolean"},{"name":"net.keepalive","description":"Enable or disable Keepalive support","default":"true","type":"boolean"},{"name":"net.keepalive_idle_timeout","description":"Set maximum time allowed for an idle Keepalive connection","default":"30s","type":"time"},{"name":"net.connect_timeout","description":"Set maximum time allowed to establish a connection, this time includes the TLS handshake","default":"10s","type":"time"},{"name":"net.connect_timeout_log_error","description":"On connection timeout, specify if it should log an error. When disabled, the timeout is logged as a debug message","default":"true","type":"boolean"},{"name":"net.source_address","description":"Specify network address to bind for data traffic","default":null,"type":"string"},{"name":"net.keepalive_max_recycle","description":"Set maximum number of times a keepalive connection can be used before it is retired.","default":"2000","type":"integer"}],"network_tls":[{"name":"tls","description":"Enable or disable TLS/SSL support","default":"off","type":"boolean"},{"name":"tls.verify","description":"Force certificate validation","default":"on","type":"boolean"},{"name":"tls.debug","description":"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose","default":"1","type":"integer"},{"name":"tls.ca_file","description":"Absolute path to CA certificate file","default":null,"type":"string"},{"name":"tls.ca_path","description":"Absolute path to scan for certificate files","default":null,"type":"string"},{"name":"tls.crt_file","description":"Absolute path to Certificate file","default":null,"type":"string"},{"name":"tls.key_file","description":"Absolute path to private Key file","default":null,"type":"string"},{"name":"tls.key_passwd","description":"Optional password for tls.key_file file","default":null,"type":"string"},{"name":"tls.vhost","description":"Hostname to be used for TLS SNI extension","default":null,"type":"string"}]}},{"type":"output","name":"loki","description":"Loki","properties":{"options":[{"name":"tenant_id","description":"Tenant ID used by default to push logs to Loki. If omitted or empty it assumes Loki is running in single-tenant mode and no X-Scope-OrgID header is sent.","default":null,"type":"string"},{"name":"tenant_id_key","description":"If set, X-Scope-OrgID will be the value of the key from incoming record. It is useful to set X-Scode-OrgID dynamically.","default":null,"type":"string"},{"name":"labels","description":"labels for API requests. If no value is set, the default label is 'job=fluent-bit'","default":null,"type":"multiple comma delimited strings"},{"name":"auto_kubernetes_labels","description":"If set to true, it will add all Kubernetes labels to Loki labels.","default":"false","type":"boolean"},{"name":"drop_single_key","description":"If set to true and only a single key remains, the log line sent to Loki will be the value of that key.","default":"false","type":"boolean"},{"name":"label_keys","description":"Comma separated list of keys to use as stream labels.","default":null,"type":"multiple comma delimited strings"},{"name":"remove_keys","description":"Comma separated list of keys to remove.","default":null,"type":"multiple comma delimited strings"},{"name":"line_format","description":"Format to use when flattening the record to a log line. Valid values are 'json' or 'key_value'. If set to 'json' the log line sent to Loki will be the Fluent Bit record dumped as json. If set to 'key_value', the log line will be each item in the record concatenated together (separated by a single space) in the format '='.","default":"json","type":"string"},{"name":"http_user","description":"Set HTTP auth user","default":null,"type":"string"},{"name":"http_passwd","description":"Set HTTP auth password","default":"","type":"string"}],"networking":[{"name":"net.dns.mode","description":"Select the primary DNS connection type (TCP or UDP)","default":null,"type":"string"},{"name":"net.dns.resolver","description":"Select the primary DNS resolver type (LEGACY or ASYNC)","default":null,"type":"string"},{"name":"net.dns.prefer_ipv4","description":"Prioritize IPv4 DNS results when trying to establish a connection","default":"false","type":"boolean"},{"name":"net.keepalive","description":"Enable or disable Keepalive support","default":"true","type":"boolean"},{"name":"net.keepalive_idle_timeout","description":"Set maximum time allowed for an idle Keepalive connection","default":"30s","type":"time"},{"name":"net.connect_timeout","description":"Set maximum time allowed to establish a connection, this time includes the TLS handshake","default":"10s","type":"time"},{"name":"net.connect_timeout_log_error","description":"On connection timeout, specify if it should log an error. When disabled, the timeout is logged as a debug message","default":"true","type":"boolean"},{"name":"net.source_address","description":"Specify network address to bind for data traffic","default":null,"type":"string"},{"name":"net.keepalive_max_recycle","description":"Set maximum number of times a keepalive connection can be used before it is retired.","default":"2000","type":"integer"}],"network_tls":[{"name":"tls","description":"Enable or disable TLS/SSL support","default":"off","type":"boolean"},{"name":"tls.verify","description":"Force certificate validation","default":"on","type":"boolean"},{"name":"tls.debug","description":"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose","default":"1","type":"integer"},{"name":"tls.ca_file","description":"Absolute path to CA certificate file","default":null,"type":"string"},{"name":"tls.ca_path","description":"Absolute path to scan for certificate files","default":null,"type":"string"},{"name":"tls.crt_file","description":"Absolute path to Certificate file","default":null,"type":"string"},{"name":"tls.key_file","description":"Absolute path to private Key file","default":null,"type":"string"},{"name":"tls.key_passwd","description":"Optional password for tls.key_file file","default":null,"type":"string"},{"name":"tls.vhost","description":"Hostname to be used for TLS SNI extension","default":null,"type":"string"}]}},{"type":"output","name":"kafka","description":"Kafka","properties":{"options":[{"name":"topic_key","description":"Which record to use as the kafka topic.","default":null,"type":"string"},{"name":"dynamic_topic","description":"Activate dynamic topics.","default":"false","type":"boolean"},{"name":"format","description":"Set the record output format.","default":null,"type":"string"},{"name":"message_key","description":"Which record key to use as the message data.","default":null,"type":"string"},{"name":"message_key_field","description":"Which record key field to use as the message data.","default":null,"type":"string"},{"name":"timestamp_key","description":"Set the key for the the timestamp.","default":"@timestamp","type":"string"},{"name":"timestamp_format","description":"Set the format the timestamp is in.","default":null,"type":"string"},{"name":"queue_full_retries","description":"Set the number of local retries to enqueue the data.","default":"10","type":"integer"},{"name":"gelf_timestamp_key","description":"Set the timestamp key for gelf  output.","default":null,"type":"string"},{"name":"gelf_host_key","description":"Set the host key for gelf  output.","default":null,"type":"string"},{"name":"gelf_short_message_key","description":"Set the short message key for gelf  output.","default":null,"type":"string"},{"name":"gelf_full_message_key","description":"Set the full message key for gelf  output.","default":null,"type":"string"},{"name":"gelf_level_key","description":"Set the level key for gelf  output.","default":null,"type":"string"},{"name":"topics","description":"Set the kafka topics, delimited by commas.","default":null,"type":"string"},{"name":"brokers","description":"Set the kafka brokers, delimited by commas.","default":null,"type":"string"},{"name":"client_id","description":"Set the kafka client_id.","default":null,"type":"string"},{"name":"group_id","description":"Set the kafka group_id.","default":null,"type":"string"},{"name":"rdkafka.","description":"Set the kafka group_id.","default":null,"type":"prefixed string"}]}},{"type":"output","name":"kafka-rest","description":"Kafka REST Proxy","properties":{"options":[{"name":"message_key","description":"Specify a message key. ","default":null,"type":"string"},{"name":"time_key","description":"Specify the name of the field that holds the record timestamp. ","default":null,"type":"string"},{"name":"topic","description":"Specify the kafka topic. ","default":"fluent-bit","type":"string"},{"name":"url_path","description":"Specify an optional HTTP URL path for the target web server, e.g: /something","default":null,"type":"string"},{"name":"partition","description":"Specify kafka partition number. ","default":"-1","type":"double"},{"name":"time_key_format","description":"Specify the format of the timestamp. ","default":"%Y-%m-%dT%H:%M:%S","type":"string"},{"name":"include_tag_key","description":"Specify whether to append tag name to final record. ","default":"false","type":"boolean"},{"name":"tag_key","description":"Specify the key name of the record if include_tag_key is enabled. ","default":"_flb-key","type":"string"},{"name":"avro_http_header","description":"Specify if the format has avro header in http request","default":"false","type":"boolean"}],"networking":[{"name":"net.dns.mode","description":"Select the primary DNS connection type (TCP or UDP)","default":null,"type":"string"},{"name":"net.dns.resolver","description":"Select the primary DNS resolver type (LEGACY or ASYNC)","default":null,"type":"string"},{"name":"net.dns.prefer_ipv4","description":"Prioritize IPv4 DNS results when trying to establish a connection","default":"false","type":"boolean"},{"name":"net.keepalive","description":"Enable or disable Keepalive support","default":"true","type":"boolean"},{"name":"net.keepalive_idle_timeout","description":"Set maximum time allowed for an idle Keepalive connection","default":"30s","type":"time"},{"name":"net.connect_timeout","description":"Set maximum time allowed to establish a connection, this time includes the TLS handshake","default":"10s","type":"time"},{"name":"net.connect_timeout_log_error","description":"On connection timeout, specify if it should log an error. When disabled, the timeout is logged as a debug message","default":"true","type":"boolean"},{"name":"net.source_address","description":"Specify network address to bind for data traffic","default":null,"type":"string"},{"name":"net.keepalive_max_recycle","description":"Set maximum number of times a keepalive connection can be used before it is retired.","default":"2000","type":"integer"}],"network_tls":[{"name":"tls","description":"Enable or disable TLS/SSL support","default":"off","type":"boolean"},{"name":"tls.verify","description":"Force certificate validation","default":"on","type":"boolean"},{"name":"tls.debug","description":"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose","default":"1","type":"integer"},{"name":"tls.ca_file","description":"Absolute path to CA certificate file","default":null,"type":"string"},{"name":"tls.ca_path","description":"Absolute path to scan for certificate files","default":null,"type":"string"},{"name":"tls.crt_file","description":"Absolute path to Certificate file","default":null,"type":"string"},{"name":"tls.key_file","description":"Absolute path to private Key file","default":null,"type":"string"},{"name":"tls.key_passwd","description":"Optional password for tls.key_file file","default":null,"type":"string"},{"name":"tls.vhost","description":"Hostname to be used for TLS SNI extension","default":null,"type":"string"}]}},{"type":"output","name":"nats","description":"NATS Server","properties":{"options":[],"networking":[{"name":"net.dns.mode","description":"Select the primary DNS connection type (TCP or UDP)","default":null,"type":"string"},{"name":"net.dns.resolver","description":"Select the primary DNS resolver type (LEGACY or ASYNC)","default":null,"type":"string"},{"name":"net.dns.prefer_ipv4","description":"Prioritize IPv4 DNS results when trying to establish a connection","default":"false","type":"boolean"},{"name":"net.keepalive","description":"Enable or disable Keepalive support","default":"true","type":"boolean"},{"name":"net.keepalive_idle_timeout","description":"Set maximum time allowed for an idle Keepalive connection","default":"30s","type":"time"},{"name":"net.connect_timeout","description":"Set maximum time allowed to establish a connection, this time includes the TLS handshake","default":"10s","type":"time"},{"name":"net.connect_timeout_log_error","description":"On connection timeout, specify if it should log an error. When disabled, the timeout is logged as a debug message","default":"true","type":"boolean"},{"name":"net.source_address","description":"Specify network address to bind for data traffic","default":null,"type":"string"},{"name":"net.keepalive_max_recycle","description":"Set maximum number of times a keepalive connection can be used before it is retired.","default":"2000","type":"integer"}]}},{"type":"output","name":"nrlogs","description":"New Relic","properties":{"options":[{"name":"base_uri","description":"New Relic Host address","default":"https://log-api.newrelic.com/log/v1","type":"string"},{"name":"api_key","description":"New Relic API Key","default":null,"type":"string"},{"name":"license_key","description":"New Relic License Key","default":null,"type":"string"},{"name":"compress","description":"Set payload compression mechanism","default":"gzip","type":"string"}],"networking":[{"name":"net.dns.mode","description":"Select the primary DNS connection type (TCP or UDP)","default":null,"type":"string"},{"name":"net.dns.resolver","description":"Select the primary DNS resolver type (LEGACY or ASYNC)","default":null,"type":"string"},{"name":"net.dns.prefer_ipv4","description":"Prioritize IPv4 DNS results when trying to establish a connection","default":"false","type":"boolean"},{"name":"net.keepalive","description":"Enable or disable Keepalive support","default":"true","type":"boolean"},{"name":"net.keepalive_idle_timeout","description":"Set maximum time allowed for an idle Keepalive connection","default":"30s","type":"time"},{"name":"net.connect_timeout","description":"Set maximum time allowed to establish a connection, this time includes the TLS handshake","default":"10s","type":"time"},{"name":"net.connect_timeout_log_error","description":"On connection timeout, specify if it should log an error. When disabled, the timeout is logged as a debug message","default":"true","type":"boolean"},{"name":"net.source_address","description":"Specify network address to bind for data traffic","default":null,"type":"string"},{"name":"net.keepalive_max_recycle","description":"Set maximum number of times a keepalive connection can be used before it is retired.","default":"2000","type":"integer"}],"network_tls":[{"name":"tls","description":"Enable or disable TLS/SSL support","default":"off","type":"boolean"},{"name":"tls.verify","description":"Force certificate validation","default":"on","type":"boolean"},{"name":"tls.debug","description":"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose","default":"1","type":"integer"},{"name":"tls.ca_file","description":"Absolute path to CA certificate file","default":null,"type":"string"},{"name":"tls.ca_path","description":"Absolute path to scan for certificate files","default":null,"type":"string"},{"name":"tls.crt_file","description":"Absolute path to Certificate file","default":null,"type":"string"},{"name":"tls.key_file","description":"Absolute path to private Key file","default":null,"type":"string"},{"name":"tls.key_passwd","description":"Optional password for tls.key_file file","default":null,"type":"string"},{"name":"tls.vhost","description":"Hostname to be used for TLS SNI extension","default":null,"type":"string"}]}},{"type":"output","name":"null","description":"Throws away events","properties":{"options":[{"name":"format","description":"Specifies the data format to be printed. Supported formats are msgpack json, json_lines and json_stream.","default":null,"type":"string"},{"name":"json_date_format","description":"Specifies the name of the date field in output.","default":null,"type":"string"},{"name":"json_date_key","description":"Specify the format of the date, supported formats: double, iso8601 (e.g: 2018-05-30T09:39:52.000681Z), java_sql_timestamp (e.g: 2018-05-30 09:39:52.000681, useful for AWS Athena), and epoch.","default":"date","type":"string"}]}},{"type":"output","name":"opensearch","description":"OpenSearch","properties":{"options":[{"name":"index","description":"Set an index name","default":"fluent-bit","type":"string"},{"name":"type","description":"Set the document type property","default":"_doc","type":"string"},{"name":"suppress_type_name","description":"If true, mapping types is removed. (for v7.0.0 or later)","default":"false","type":"boolean"},{"name":"http_user","description":"Optional username credential for access","default":null,"type":"string"},{"name":"http_passwd","description":"Password for user defined in 'http_user'","default":"","type":"string"},{"name":"aws_auth","description":"Enable AWS Sigv4 Authentication","default":"false","type":"boolean"},{"name":"aws_region","description":"AWS Region of your Amazon OpenSearch Service cluster","default":null,"type":"string"},{"name":"aws_sts_endpoint","description":"Custom endpoint for the AWS STS API, used with the AWS_Role_ARN option","default":null,"type":"string"},{"name":"aws_role_arn","description":"AWS IAM Role to assume to put records to your Amazon OpenSearch cluster","default":null,"type":"string"},{"name":"aws_external_id","description":"External ID for the AWS IAM Role specified with `aws_role_arn`","default":null,"type":"string"},{"name":"logstash_format","description":"Enable Logstash format compatibility","default":"false","type":"boolean"},{"name":"logstash_prefix","description":"When Logstash_Format is enabled, the Index name is composed using a prefix and the date, e.g: If Logstash_Prefix is equals to 'mydata' your index will become 'mydata-YYYY.MM.DD'. The last string appended belongs to the date when the data is being generated","default":"logstash","type":"string"},{"name":"logstash_prefix_key","description":"When included: the value in the record that belongs to the key will be looked up and over-write the Logstash_Prefix for index generation. If the key/value is not found in the record then the Logstash_Prefix option will act as a fallback. Nested keys are supported through record accessor pattern","default":null,"type":"string"},{"name":"logstash_dateformat","description":"Time format (based on strftime) to generate the second part of the Index name","default":"%Y.%m.%d","type":"string"},{"name":"time_key","description":"When Logstash_Format is enabled, each record will get a new timestamp field. The Time_Key property defines the name of that field","default":"@timestamp","type":"string"},{"name":"time_key_format","description":"When Logstash_Format is enabled, this property defines the format of the timestamp","default":"%Y-%m-%dT%H:%M:%S","type":"string"},{"name":"time_key_nanos","description":"When Logstash_Format is enabled, enabling this property sends nanosecond precision timestamps","default":"false","type":"boolean"},{"name":"include_tag_key","description":"When enabled, it append the Tag name to the record","default":"false","type":"boolean"},{"name":"tag_key","description":"When Include_Tag_Key is enabled, this property defines the key name for the tag","default":"flb-key","type":"string"},{"name":"buffer_size","description":"Specify the buffer size used to read the response from the OpenSearch HTTP service. This option is useful for debugging purposes where is required to read full responses, note that response size grows depending of the number of records inserted. To set an unlimited amount of memory set this value to 'false', otherwise the value must be according to the Unit Size specification","default":"512k","type":"size"},{"name":"path","description":"OpenSearch accepts new data on HTTP query path '/_bulk'. But it is also possible to serve OpenSearch behind a reverse proxy on a subpath. This option defines such path on the fluent-bit side. It simply adds a path prefix in the indexing HTTP POST URI","default":null,"type":"string"},{"name":"pipeline","description":"OpenSearch allows to setup filters called pipelines. This option allows to define which pipeline the database should use. For performance reasons is strongly suggested to do parsing and filtering on Fluent Bit side, avoid pipelines","default":null,"type":"string"},{"name":"generate_id","description":"When enabled, generate _id for outgoing records. This prevents duplicate records when retrying","default":"false","type":"boolean"},{"name":"write_operation","description":"Operation to use to write in bulk requests","default":"create","type":"string"},{"name":"id_key","description":"If set, _id will be the value of the key from incoming record.","default":null,"type":"string"},{"name":"replace_dots","description":"When enabled, replace field name dots with underscore.","default":"false","type":"boolean"},{"name":"current_time_index","description":"Use current time for index generation instead of message record","default":"false","type":"boolean"},{"name":"trace_output","description":"When enabled print the OpenSearch API calls to stdout (for diag only)","default":"false","type":"boolean"},{"name":"trace_error","description":"When enabled print the OpenSearch exception to stderr (for diag only)","default":"false","type":"boolean"}],"networking":[{"name":"net.dns.mode","description":"Select the primary DNS connection type (TCP or UDP)","default":null,"type":"string"},{"name":"net.dns.resolver","description":"Select the primary DNS resolver type (LEGACY or ASYNC)","default":null,"type":"string"},{"name":"net.dns.prefer_ipv4","description":"Prioritize IPv4 DNS results when trying to establish a connection","default":"false","type":"boolean"},{"name":"net.keepalive","description":"Enable or disable Keepalive support","default":"true","type":"boolean"},{"name":"net.keepalive_idle_timeout","description":"Set maximum time allowed for an idle Keepalive connection","default":"30s","type":"time"},{"name":"net.connect_timeout","description":"Set maximum time allowed to establish a connection, this time includes the TLS handshake","default":"10s","type":"time"},{"name":"net.connect_timeout_log_error","description":"On connection timeout, specify if it should log an error. When disabled, the timeout is logged as a debug message","default":"true","type":"boolean"},{"name":"net.source_address","description":"Specify network address to bind for data traffic","default":null,"type":"string"},{"name":"net.keepalive_max_recycle","description":"Set maximum number of times a keepalive connection can be used before it is retired.","default":"2000","type":"integer"}],"network_tls":[{"name":"tls","description":"Enable or disable TLS/SSL support","default":"off","type":"boolean"},{"name":"tls.verify","description":"Force certificate validation","default":"on","type":"boolean"},{"name":"tls.debug","description":"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose","default":"1","type":"integer"},{"name":"tls.ca_file","description":"Absolute path to CA certificate file","default":null,"type":"string"},{"name":"tls.ca_path","description":"Absolute path to scan for certificate files","default":null,"type":"string"},{"name":"tls.crt_file","description":"Absolute path to Certificate file","default":null,"type":"string"},{"name":"tls.key_file","description":"Absolute path to private Key file","default":null,"type":"string"},{"name":"tls.key_passwd","description":"Optional password for tls.key_file file","default":null,"type":"string"},{"name":"tls.vhost","description":"Hostname to be used for TLS SNI extension","default":null,"type":"string"}]}},{"type":"output","name":"plot","description":"Generate data file for GNU Plot","properties":{"options":[{"name":"key","description":"set a number of times to generate event.","default":null,"type":"string"},{"name":"file","description":"set a number of times to generate event.","default":null,"type":"string"}]}},{"type":"output","name":"pgsql","description":"PostgreSQL","properties":{}},{"type":"output","name":"skywalking","description":"Send logs into log collector on SkyWalking OAP","properties":{"options":[{"name":"auth_token","description":"Auth token for SkyWalking OAP","default":null,"type":"string"},{"name":"svc_name","description":"Service name","default":"sw-service","type":"string"},{"name":"svc_inst_name","description":"Instance name","default":"fluent-bit","type":"string"}],"networking":[{"name":"net.dns.mode","description":"Select the primary DNS connection type (TCP or UDP)","default":null,"type":"string"},{"name":"net.dns.resolver","description":"Select the primary DNS resolver type (LEGACY or ASYNC)","default":null,"type":"string"},{"name":"net.dns.prefer_ipv4","description":"Prioritize IPv4 DNS results when trying to establish a connection","default":"false","type":"boolean"},{"name":"net.keepalive","description":"Enable or disable Keepalive support","default":"true","type":"boolean"},{"name":"net.keepalive_idle_timeout","description":"Set maximum time allowed for an idle Keepalive connection","default":"30s","type":"time"},{"name":"net.connect_timeout","description":"Set maximum time allowed to establish a connection, this time includes the TLS handshake","default":"10s","type":"time"},{"name":"net.connect_timeout_log_error","description":"On connection timeout, specify if it should log an error. When disabled, the timeout is logged as a debug message","default":"true","type":"boolean"},{"name":"net.source_address","description":"Specify network address to bind for data traffic","default":null,"type":"string"},{"name":"net.keepalive_max_recycle","description":"Set maximum number of times a keepalive connection can be used before it is retired.","default":"2000","type":"integer"}],"network_tls":[{"name":"tls","description":"Enable or disable TLS/SSL support","default":"off","type":"boolean"},{"name":"tls.verify","description":"Force certificate validation","default":"on","type":"boolean"},{"name":"tls.debug","description":"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose","default":"1","type":"integer"},{"name":"tls.ca_file","description":"Absolute path to CA certificate file","default":null,"type":"string"},{"name":"tls.ca_path","description":"Absolute path to scan for certificate files","default":null,"type":"string"},{"name":"tls.crt_file","description":"Absolute path to Certificate file","default":null,"type":"string"},{"name":"tls.key_file","description":"Absolute path to private Key file","default":null,"type":"string"},{"name":"tls.key_passwd","description":"Optional password for tls.key_file file","default":null,"type":"string"},{"name":"tls.vhost","description":"Hostname to be used for TLS SNI extension","default":null,"type":"string"}]}},{"type":"output","name":"slack","description":"Send events to a Slack channel","properties":{"options":[{"name":"webhook","description":"","default":null,"type":"string"}],"networking":[{"name":"net.dns.mode","description":"Select the primary DNS connection type (TCP or UDP)","default":null,"type":"string"},{"name":"net.dns.resolver","description":"Select the primary DNS resolver type (LEGACY or ASYNC)","default":null,"type":"string"},{"name":"net.dns.prefer_ipv4","description":"Prioritize IPv4 DNS results when trying to establish a connection","default":"false","type":"boolean"},{"name":"net.keepalive","description":"Enable or disable Keepalive support","default":"true","type":"boolean"},{"name":"net.keepalive_idle_timeout","description":"Set maximum time allowed for an idle Keepalive connection","default":"30s","type":"time"},{"name":"net.connect_timeout","description":"Set maximum time allowed to establish a connection, this time includes the TLS handshake","default":"10s","type":"time"},{"name":"net.connect_timeout_log_error","description":"On connection timeout, specify if it should log an error. When disabled, the timeout is logged as a debug message","default":"true","type":"boolean"},{"name":"net.source_address","description":"Specify network address to bind for data traffic","default":null,"type":"string"},{"name":"net.keepalive_max_recycle","description":"Set maximum number of times a keepalive connection can be used before it is retired.","default":"2000","type":"integer"}],"network_tls":[{"name":"tls","description":"Enable or disable TLS/SSL support","default":"off","type":"boolean"},{"name":"tls.verify","description":"Force certificate validation","default":"on","type":"boolean"},{"name":"tls.debug","description":"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose","default":"1","type":"integer"},{"name":"tls.ca_file","description":"Absolute path to CA certificate file","default":null,"type":"string"},{"name":"tls.ca_path","description":"Absolute path to scan for certificate files","default":null,"type":"string"},{"name":"tls.crt_file","description":"Absolute path to Certificate file","default":null,"type":"string"},{"name":"tls.key_file","description":"Absolute path to private Key file","default":null,"type":"string"},{"name":"tls.key_passwd","description":"Optional password for tls.key_file file","default":null,"type":"string"},{"name":"tls.vhost","description":"Hostname to be used for TLS SNI extension","default":null,"type":"string"}]}},{"type":"output","name":"splunk","description":"Send events to Splunk HTTP Event Collector","properties":{"options":[{"name":"compress","description":"Set payload compression mechanism. Option available is 'gzip'","default":null,"type":"string"},{"name":"http_user","description":"Set HTTP auth user","default":null,"type":"string"},{"name":"http_passwd","description":"Set HTTP auth password","default":"","type":"string"},{"name":"http_buffer_size","description":"Specify the buffer size used to read the response from the Splunk HTTP service. This option is useful for debugging purposes where is required to read full responses, note that response size grows depending of the number of records inserted. To set an unlimited amount of memory set this value to 'false', otherwise the value must be according to the Unit Size specification","default":null,"type":"size"},{"name":"http_debug_bad_request","description":"If the HTTP server response code is 400 (bad request) and this flag is enabled, it will print the full HTTP request and response to the stdout interface. This feature is available for debugging purposes.","default":"false","type":"boolean"},{"name":"event_key","description":"Specify the key name that will be used to send a single value as part of the record.","default":null,"type":"string"},{"name":"event_host","description":"Set the host value to the event data. The value allows a record accessor pattern.","default":null,"type":"string"},{"name":"event_source","description":"Set the source value to assign to the event data.","default":null,"type":"string"},{"name":"event_sourcetype","description":"Set the sourcetype value to assign to the event data.","default":null,"type":"string"},{"name":"event_sourcetype_key","description":"Set a record key that will populate 'sourcetype'. If the key is found, it will have precedence over the value set in 'event_sourcetype'.","default":null,"type":"string"},{"name":"event_index","description":"The name of the index by which the event data is to be indexed.","default":null,"type":"string"},{"name":"event_index_key","description":"Set a record key that will populate the 'index' field. If the key is found, it will have precedence over the value set in 'event_index'.","default":null,"type":"string"},{"name":"event_field","description":"Set event fields for the record. This option can be set multiple times and the format is 'key_name record_accessor_pattern'.","default":null,"type":"space delimited strings (minimum 2)"},{"name":"splunk_token","description":"Specify the Authentication Token for the HTTP Event Collector interface.","default":null,"type":"string"},{"name":"splunk_send_raw","description":"When enabled, the record keys and values are set in the top level of the map instead of under the event key. Refer to the Sending Raw Events section from the docs for more details to make this option work properly.","default":"off","type":"boolean"},{"name":"channel","description":"Specify X-Splunk-Request-Channel Header for the HTTP Event Collector interface.","default":null,"type":"string"}],"networking":[{"name":"net.dns.mode","description":"Select the primary DNS connection type (TCP or UDP)","default":null,"type":"string"},{"name":"net.dns.resolver","description":"Select the primary DNS resolver type (LEGACY or ASYNC)","default":null,"type":"string"},{"name":"net.dns.prefer_ipv4","description":"Prioritize IPv4 DNS results when trying to establish a connection","default":"false","type":"boolean"},{"name":"net.keepalive","description":"Enable or disable Keepalive support","default":"true","type":"boolean"},{"name":"net.keepalive_idle_timeout","description":"Set maximum time allowed for an idle Keepalive connection","default":"30s","type":"time"},{"name":"net.connect_timeout","description":"Set maximum time allowed to establish a connection, this time includes the TLS handshake","default":"10s","type":"time"},{"name":"net.connect_timeout_log_error","description":"On connection timeout, specify if it should log an error. When disabled, the timeout is logged as a debug message","default":"true","type":"boolean"},{"name":"net.source_address","description":"Specify network address to bind for data traffic","default":null,"type":"string"},{"name":"net.keepalive_max_recycle","description":"Set maximum number of times a keepalive connection can be used before it is retired.","default":"2000","type":"integer"}],"network_tls":[{"name":"tls","description":"Enable or disable TLS/SSL support","default":"off","type":"boolean"},{"name":"tls.verify","description":"Force certificate validation","default":"on","type":"boolean"},{"name":"tls.debug","description":"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose","default":"1","type":"integer"},{"name":"tls.ca_file","description":"Absolute path to CA certificate file","default":null,"type":"string"},{"name":"tls.ca_path","description":"Absolute path to scan for certificate files","default":null,"type":"string"},{"name":"tls.crt_file","description":"Absolute path to Certificate file","default":null,"type":"string"},{"name":"tls.key_file","description":"Absolute path to private Key file","default":null,"type":"string"},{"name":"tls.key_passwd","description":"Optional password for tls.key_file file","default":null,"type":"string"},{"name":"tls.vhost","description":"Hostname to be used for TLS SNI extension","default":null,"type":"string"}]}},{"type":"output","name":"stackdriver","description":"Send events to Google Stackdriver Logging","properties":{"options":[{"name":"google_service_credentials","description":"Set the path for the google service credentials file","default":null,"type":"string"},{"name":"metadata_server","description":"Set the metadata server","default":null,"type":"string"},{"name":"service_account_email","description":"Set the service account email","default":null,"type":"string"},{"name":"service_account_secret","description":"Set the service account secret","default":null,"type":"string"},{"name":"export_to_project_id","description":"Export to project id","default":null,"type":"string"},{"name":"resource","description":"Set the resource","default":"global","type":"string"},{"name":"severity_key","description":"Set the severity key","default":"logging.googleapis.com/severity","type":"string"},{"name":"autoformat_stackdriver_trace","description":"Autoformat the stacrdriver trace","default":"false","type":"boolean"},{"name":"trace_key","description":"Set the trace key","default":"logging.googleapis.com/trace","type":"string"},{"name":"log_name_key","description":"Set the logname key","default":"logging.googleapis.com/logName","type":"string"},{"name":"http_request_key","description":"Set the http request key","default":"logging.googleapis.com/http_request","type":"string"},{"name":"k8s_cluster_name","description":"Set the kubernetes cluster name","default":null,"type":"string"},{"name":"k8s_cluster_location","description":"Set the kubernetes cluster location","default":null,"type":"string"},{"name":"location","description":"Set the resource location","default":null,"type":"string"},{"name":"namespace","description":"Set the resource namespace","default":null,"type":"string"},{"name":"node_id","description":"Set the resource node id","default":null,"type":"string"},{"name":"job","description":"Set the resource job","default":null,"type":"string"},{"name":"task_id","description":"Set the resource task id","default":null,"type":"string"},{"name":"labels","description":"Set the labels","default":null,"type":"multiple comma delimited strings"},{"name":"labels_key","description":"Set the labels key","default":"logging.googleapis.com/labels","type":"string"},{"name":"tag_prefix","description":"Set the tag prefix","default":null,"type":"string"},{"name":"stackdriver_agent","description":"Set the stackdriver agent","default":null,"type":"string"},{"name":"custom_k8s_regex","description":"Set a custom kubernetes regex filter","default":"(?<pod_name>[a-z0-9](?:[-a-z0-9]*[a-z0-9])?(?:\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*)_(?<namespace_name>[^_]+)_(?<container_name>.+)-(?<docker_id>[a-z0-9]{64})\\.log$","type":"string"}],"networking":[{"name":"net.dns.mode","description":"Select the primary DNS connection type (TCP or UDP)","default":null,"type":"string"},{"name":"net.dns.resolver","description":"Select the primary DNS resolver type (LEGACY or ASYNC)","default":null,"type":"string"},{"name":"net.dns.prefer_ipv4","description":"Prioritize IPv4 DNS results when trying to establish a connection","default":"false","type":"boolean"},{"name":"net.keepalive","description":"Enable or disable Keepalive support","default":"true","type":"boolean"},{"name":"net.keepalive_idle_timeout","description":"Set maximum time allowed for an idle Keepalive connection","default":"30s","type":"time"},{"name":"net.connect_timeout","description":"Set maximum time allowed to establish a connection, this time includes the TLS handshake","default":"10s","type":"time"},{"name":"net.connect_timeout_log_error","description":"On connection timeout, specify if it should log an error. When disabled, the timeout is logged as a debug message","default":"true","type":"boolean"},{"name":"net.source_address","description":"Specify network address to bind for data traffic","default":null,"type":"string"},{"name":"net.keepalive_max_recycle","description":"Set maximum number of times a keepalive connection can be used before it is retired.","default":"2000","type":"integer"}],"network_tls":[{"name":"tls","description":"Enable or disable TLS/SSL support","default":"off","type":"boolean"},{"name":"tls.verify","description":"Force certificate validation","default":"on","type":"boolean"},{"name":"tls.debug","description":"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose","default":"1","type":"integer"},{"name":"tls.ca_file","description":"Absolute path to CA certificate file","default":null,"type":"string"},{"name":"tls.ca_path","description":"Absolute path to scan for certificate files","default":null,"type":"string"},{"name":"tls.crt_file","description":"Absolute path to Certificate file","default":null,"type":"string"},{"name":"tls.key_file","description":"Absolute path to private Key file","default":null,"type":"string"},{"name":"tls.key_passwd","description":"Optional password for tls.key_file file","default":null,"type":"string"},{"name":"tls.vhost","description":"Hostname to be used for TLS SNI extension","default":null,"type":"string"}]}},{"type":"output","name":"stdout","description":"Prints events to STDOUT","properties":{"options":[{"name":"format","description":"Specifies the data format to be printed. Supported formats are msgpack json, json_lines and json_stream.","default":null,"type":"string"},{"name":"json_date_format","description":"Specify the format of the date, supported formats: double, iso8601 (e.g: 2018-05-30T09:39:52.000681Z), java_sql_timestamp (e.g: 2018-05-30 09:39:52.000681, useful for AWS Athena), and epoch.","default":null,"type":"string"},{"name":"json_date_key","description":"Specifies the name of the date field in output.","default":"date","type":"string"}]}},{"type":"output","name":"syslog","description":"Syslog","properties":{"options":[{"name":"mode","description":"Set the desired transport type, the available options are tcp and udp. If you need to use a TLS secure channel, choose 'tcp' mode here and enable the 'tls' option separately.","default":"udp","type":"string"},{"name":"syslog_format","description":"Specify the Syslog protocol format to use, the available options are rfc3164 and rfc5424.","default":"rfc5424","type":"string"},{"name":"syslog_maxsize","description":"Set the maximum size allowed per message. The value must be only integers representing the number of bytes allowed. If no value is provided, the default size is set depending of the protocol version specified by syslog_format , rfc3164 sets max size to 1024 bytes, while rfc5424 sets the size to 2048 bytes.","default":"0","type":"size"},{"name":"syslog_severity_key","description":"Specify the name of the key from the original record that contains the Syslog severity number. This configuration is optional.","default":null,"type":"string"},{"name":"syslog_facility_key","description":"Specify the name of the key from the original record that contains the Syslog facility number. This configuration is optional.","default":null,"type":"string"},{"name":"syslog_hostname_key","description":"Specify the key name from the original record that contains the hostname that generated the message. This configuration is optional.","default":null,"type":"string"},{"name":"syslog_appname_key","description":"Specify the key name from the original record that contains the application name that generated the message. This configuration is optional.","default":null,"type":"string"},{"name":"syslog_procid_key","description":"Specify the key name from the original record that contains the Process ID that generated the message. This configuration is optional.","default":null,"type":"string"},{"name":"syslog_msgid_key","description":"Specify the key name from the original record that contains the Message ID associated to the message. This configuration is optional.","default":null,"type":"string"},{"name":"syslog_sd_key","description":"Specify the key name from the original record that contains the Structured Data (SD) content. If set, the value of the key must be a map.This option can be set multiple times.","default":null,"type":"string"},{"name":"syslog_message_key","description":"Specify the key name that contains the message to deliver. Note that if this property is mandatory, otherwise the message will be empty.","default":null,"type":"string"}],"networking":[{"name":"net.dns.mode","description":"Select the primary DNS connection type (TCP or UDP)","default":null,"type":"string"},{"name":"net.dns.resolver","description":"Select the primary DNS resolver type (LEGACY or ASYNC)","default":null,"type":"string"},{"name":"net.dns.prefer_ipv4","description":"Prioritize IPv4 DNS results when trying to establish a connection","default":"false","type":"boolean"},{"name":"net.keepalive","description":"Enable or disable Keepalive support","default":"true","type":"boolean"},{"name":"net.keepalive_idle_timeout","description":"Set maximum time allowed for an idle Keepalive connection","default":"30s","type":"time"},{"name":"net.connect_timeout","description":"Set maximum time allowed to establish a connection, this time includes the TLS handshake","default":"10s","type":"time"},{"name":"net.connect_timeout_log_error","description":"On connection timeout, specify if it should log an error. When disabled, the timeout is logged as a debug message","default":"true","type":"boolean"},{"name":"net.source_address","description":"Specify network address to bind for data traffic","default":null,"type":"string"},{"name":"net.keepalive_max_recycle","description":"Set maximum number of times a keepalive connection can be used before it is retired.","default":"2000","type":"integer"}],"network_tls":[{"name":"tls","description":"Enable or disable TLS/SSL support","default":"off","type":"boolean"},{"name":"tls.verify","description":"Force certificate validation","default":"on","type":"boolean"},{"name":"tls.debug","description":"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose","default":"1","type":"integer"},{"name":"tls.ca_file","description":"Absolute path to CA certificate file","default":null,"type":"string"},{"name":"tls.ca_path","description":"Absolute path to scan for certificate files","default":null,"type":"string"},{"name":"tls.crt_file","description":"Absolute path to Certificate file","default":null,"type":"string"},{"name":"tls.key_file","description":"Absolute path to private Key file","default":null,"type":"string"},{"name":"tls.key_passwd","description":"Optional password for tls.key_file file","default":null,"type":"string"},{"name":"tls.vhost","description":"Hostname to be used for TLS SNI extension","default":null,"type":"string"}]}},{"type":"output","name":"tcp","description":"TCP Output","properties":{"options":[{"name":"format","description":"Specify the payload format, supported formats: msgpack, json, json_lines or json_stream.","default":"msgpack","type":"string"},{"name":"json_date_format","description":"Specify the format of the date, supported formats: double, iso8601 (e.g: 2018-05-30T09:39:52.000681Z), java_sql_timestamp (e.g: 2018-05-30 09:39:52.000681, useful for AWS Athena), and epoch.","default":"double","type":"string"},{"name":"json_date_key","description":"Specify the name of the date field in output.","default":"date","type":"string"}],"networking":[{"name":"net.dns.mode","description":"Select the primary DNS connection type (TCP or UDP)","default":null,"type":"string"},{"name":"net.dns.resolver","description":"Select the primary DNS resolver type (LEGACY or ASYNC)","default":null,"type":"string"},{"name":"net.dns.prefer_ipv4","description":"Prioritize IPv4 DNS results when trying to establish a connection","default":"false","type":"boolean"},{"name":"net.keepalive","description":"Enable or disable Keepalive support","default":"true","type":"boolean"},{"name":"net.keepalive_idle_timeout","description":"Set maximum time allowed for an idle Keepalive connection","default":"30s","type":"time"},{"name":"net.connect_timeout","description":"Set maximum time allowed to establish a connection, this time includes the TLS handshake","default":"10s","type":"time"},{"name":"net.connect_timeout_log_error","description":"On connection timeout, specify if it should log an error. When disabled, the timeout is logged as a debug message","default":"true","type":"boolean"},{"name":"net.source_address","description":"Specify network address to bind for data traffic","default":null,"type":"string"},{"name":"net.keepalive_max_recycle","description":"Set maximum number of times a keepalive connection can be used before it is retired.","default":"2000","type":"integer"}],"network_tls":[{"name":"tls","description":"Enable or disable TLS/SSL support","default":"off","type":"boolean"},{"name":"tls.verify","description":"Force certificate validation","default":"on","type":"boolean"},{"name":"tls.debug","description":"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose","default":"1","type":"integer"},{"name":"tls.ca_file","description":"Absolute path to CA certificate file","default":null,"type":"string"},{"name":"tls.ca_path","description":"Absolute path to scan for certificate files","default":null,"type":"string"},{"name":"tls.crt_file","description":"Absolute path to Certificate file","default":null,"type":"string"},{"name":"tls.key_file","description":"Absolute path to private Key file","default":null,"type":"string"},{"name":"tls.key_passwd","description":"Optional password for tls.key_file file","default":null,"type":"string"},{"name":"tls.vhost","description":"Hostname to be used for TLS SNI extension","default":null,"type":"string"}]}},{"type":"output","name":"td","description":"Treasure Data","properties":{"options":[{"name":"API","description":"Set the API key","default":null,"type":"string"},{"name":"Database","description":"Set the Database file","default":null,"type":"string"},{"name":"Table","description":"Set the Database Table","default":null,"type":"string"},{"name":"Region","description":"Set the Region: us or jp","default":null,"type":"string"}],"network_tls":[{"name":"tls","description":"Enable or disable TLS/SSL support","default":"off","type":"boolean"},{"name":"tls.verify","description":"Force certificate validation","default":"on","type":"boolean"},{"name":"tls.debug","description":"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose","default":"1","type":"integer"},{"name":"tls.ca_file","description":"Absolute path to CA certificate file","default":null,"type":"string"},{"name":"tls.ca_path","description":"Absolute path to scan for certificate files","default":null,"type":"string"},{"name":"tls.crt_file","description":"Absolute path to Certificate file","default":null,"type":"string"},{"name":"tls.key_file","description":"Absolute path to private Key file","default":null,"type":"string"},{"name":"tls.key_passwd","description":"Optional password for tls.key_file file","default":null,"type":"string"},{"name":"tls.vhost","description":"Hostname to be used for TLS SNI extension","default":null,"type":"string"}]}},{"type":"output","name":"lib","description":"Library mode Output","properties":{}},{"type":"output","name":"flowcounter","description":"FlowCounter","properties":{"options":[{"name":"unit","description":"","default":null,"type":"string"},{"name":"event_based","description":"","default":"false","type":"boolean"}]}},{"type":"output","name":"gelf","description":"GELF Output","properties":{"options":[{"name":"mode","description":"The protocol to use. 'tls', 'tcp' or 'udp'","default":"udp","type":"string"},{"name":"gelf_short_message_key","description":"A short descriptive message (MUST be set in GELF)","default":null,"type":"string"},{"name":"gelf_timestamp_key","description":"Timestamp key name (SHOULD be set in GELF)","default":null,"type":"string"},{"name":"gelf_host_key","description":"Key which its value is used as the name of the host,source or application that sent this message. (MUST be set in GELF) ","default":null,"type":"string"},{"name":"gelf_full_message_key","description":"Key to use as the long message that can i.e. contain a backtrace. (Optional in GELF)","default":null,"type":"string"},{"name":"gelf_level_key","description":"Key to be used as the log level. Its value must be in standard syslog levels (between 0 and 7). (Optional in GELF)","default":null,"type":"string"},{"name":"packet_size","description":"If transport protocol is udp, you can set the size of packets to be sent.","default":"1420","type":"integer"},{"name":"compress","description":"If transport protocol is udp, you can set this if you want your UDP packets to be compressed.","default":"true","type":"boolean"}],"networking":[{"name":"net.dns.mode","description":"Select the primary DNS connection type (TCP or UDP)","default":null,"type":"string"},{"name":"net.dns.resolver","description":"Select the primary DNS resolver type (LEGACY or ASYNC)","default":null,"type":"string"},{"name":"net.dns.prefer_ipv4","description":"Prioritize IPv4 DNS results when trying to establish a connection","default":"false","type":"boolean"},{"name":"net.keepalive","description":"Enable or disable Keepalive support","default":"true","type":"boolean"},{"name":"net.keepalive_idle_timeout","description":"Set maximum time allowed for an idle Keepalive connection","default":"30s","type":"time"},{"name":"net.connect_timeout","description":"Set maximum time allowed to establish a connection, this time includes the TLS handshake","default":"10s","type":"time"},{"name":"net.connect_timeout_log_error","description":"On connection timeout, specify if it should log an error. When disabled, the timeout is logged as a debug message","default":"true","type":"boolean"},{"name":"net.source_address","description":"Specify network address to bind for data traffic","default":null,"type":"string"},{"name":"net.keepalive_max_recycle","description":"Set maximum number of times a keepalive connection can be used before it is retired.","default":"2000","type":"integer"}],"network_tls":[{"name":"tls","description":"Enable or disable TLS/SSL support","default":"off","type":"boolean"},{"name":"tls.verify","description":"Force certificate validation","default":"on","type":"boolean"},{"name":"tls.debug","description":"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose","default":"1","type":"integer"},{"name":"tls.ca_file","description":"Absolute path to CA certificate file","default":null,"type":"string"},{"name":"tls.ca_path","description":"Absolute path to scan for certificate files","default":null,"type":"string"},{"name":"tls.crt_file","description":"Absolute path to Certificate file","default":null,"type":"string"},{"name":"tls.key_file","description":"Absolute path to private Key file","default":null,"type":"string"},{"name":"tls.key_passwd","description":"Optional password for tls.key_file file","default":null,"type":"string"},{"name":"tls.vhost","description":"Hostname to be used for TLS SNI extension","default":null,"type":"string"}]}},{"type":"output","name":"websocket","description":"Websocket","properties":{"options":[{"name":"uri","description":"Specify an optional URI for the target web socket server, e.g: /something","default":null,"type":"string"},{"name":"format","description":"Set desired payload format: json, json_stream, json_lines, gelf or msgpack","default":null,"type":"string"},{"name":"json_date_format","description":"Specify the format of the date","default":"double","type":"string"},{"name":"json_date_key","description":"Specify the name of the date field in output","default":"date","type":"string"}],"networking":[{"name":"net.dns.mode","description":"Select the primary DNS connection type (TCP or UDP)","default":null,"type":"string"},{"name":"net.dns.resolver","description":"Select the primary DNS resolver type (LEGACY or ASYNC)","default":null,"type":"string"},{"name":"net.dns.prefer_ipv4","description":"Prioritize IPv4 DNS results when trying to establish a connection","default":"false","type":"boolean"},{"name":"net.keepalive","description":"Enable or disable Keepalive support","default":"true","type":"boolean"},{"name":"net.keepalive_idle_timeout","description":"Set maximum time allowed for an idle Keepalive connection","default":"30s","type":"time"},{"name":"net.connect_timeout","description":"Set maximum time allowed to establish a connection, this time includes the TLS handshake","default":"10s","type":"time"},{"name":"net.connect_timeout_log_error","description":"On connection timeout, specify if it should log an error. When disabled, the timeout is logged as a debug message","default":"true","type":"boolean"},{"name":"net.source_address","description":"Specify network address to bind for data traffic","default":null,"type":"string"},{"name":"net.keepalive_max_recycle","description":"Set maximum number of times a keepalive connection can be used before it is retired.","default":"2000","type":"integer"}],"network_tls":[{"name":"tls","description":"Enable or disable TLS/SSL support","default":"off","type":"boolean"},{"name":"tls.verify","description":"Force certificate validation","default":"on","type":"boolean"},{"name":"tls.debug","description":"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose","default":"1","type":"integer"},{"name":"tls.ca_file","description":"Absolute path to CA certificate file","default":null,"type":"string"},{"name":"tls.ca_path","description":"Absolute path to scan for certificate files","default":null,"type":"string"},{"name":"tls.crt_file","description":"Absolute path to Certificate file","default":null,"type":"string"},{"name":"tls.key_file","description":"Absolute path to private Key file","default":null,"type":"string"},{"name":"tls.key_passwd","description":"Optional password for tls.key_file file","default":null,"type":"string"},{"name":"tls.vhost","description":"Hostname to be used for TLS SNI extension","default":null,"type":"string"}]}},{"type":"output","name":"cloudwatch_logs","description":"Send logs to Amazon CloudWatch","properties":{"options":[{"name":"region","description":"The AWS region to send logs to","default":null,"type":"string"},{"name":"log_group_name","description":"CloudWatch Log Group Name","default":null,"type":"string"},{"name":"log_stream_name","description":"CloudWatch Log Stream Name; not compatible with `log_stream_prefix`","default":null,"type":"string"},{"name":"log_stream_prefix","description":"Prefix for CloudWatch Log Stream Name; the tag is appended to the prefix to form the stream name","default":null,"type":"string"},{"name":"log_group_template","description":"Template for CW Log Group name using record accessor syntax. Plugin falls back to the log_group_name configured if needed.","default":null,"type":"string"},{"name":"log_stream_template","description":"Template for CW Log Stream name using record accessor syntax. Plugin falls back to the log_stream_name or log_stream_prefix configured if needed.","default":null,"type":"string"},{"name":"log_key","description":"By default, the whole log record will be sent to CloudWatch. If you specify a key name with this option, then only the value of that key will be sent to CloudWatch. For example, if you are using the Fluentd Docker log driver, you can specify log_key log and only the log message will be sent to CloudWatch.","default":null,"type":"string"},{"name":"extra_user_agent","description":"This option appends a string to the default user agent. AWS asks that you not manually set this field yourself, it is reserved for use in our vended configurations, for example, EKS Container Insights.","default":null,"type":"string"},{"name":"log_format","description":"An optional parameter that can be used to tell CloudWatch the format of the data. A value of json/emf enables CloudWatch to extract custom metrics embedded in a JSON payload.","default":null,"type":"string"},{"name":"role_arn","description":"ARN of an IAM role to assume (ex. for cross account access).","default":null,"type":"string"},{"name":"auto_create_group","description":"Automatically create the log group (log streams will always automatically be created)","default":"false","type":"boolean"},{"name":"auto_retry_requests","description":"Immediately retry failed requests to AWS services once. This option does not affect the normal Fluent Bit retry mechanism with backoff. Instead, it enables an immediate retry with no delay for networking errors, which may help improve throughput when there are transient/random networking issues.","default":"true","type":"boolean"},{"name":"log_retention_days","description":"If set to a number greater than zero, and newly create log group's retention policy is set to this many days. Valid values are: [1, 3, 5, 7, 14, 30, 60, 90, 120, 150, 180, 365, 400, 545, 731, 1827, 3653]","default":"0","type":"integer"},{"name":"endpoint","description":"Specify a custom endpoint for the CloudWatch Logs API","default":null,"type":"string"},{"name":"sts_endpoint","description":"Specify a custom endpoint for the STS API, can be used with the role_arn parameter","default":null,"type":"string"},{"name":"external_id","description":"Specify an external ID for the STS API, can be used with the role_arn parameter if your role requires an external ID.","default":null,"type":"string"},{"name":"metric_namespace","description":"Metric namespace for CloudWatch EMF logs","default":null,"type":"string"},{"name":"metric_dimensions","description":"Metric dimensions is a list of lists. If you have only one list of dimensions, put the values as a comma seperated string. If you want to put list of lists, use the list as semicolon seperated strings. If your value is 'd1,d2;d3', we will consider it as [[d1, d2],[d3]].","default":null,"type":"string"}]}},{"type":"output","name":"kinesis_firehose","description":"Send logs to Amazon Kinesis Firehose","properties":{"options":[{"name":"region","description":"The AWS region of your delivery stream","default":null,"type":"string"},{"name":"delivery_stream","description":"Firehose delivery stream name","default":null,"type":"string"},{"name":"time_key","description":"Add the timestamp to the record under this key. By default the timestamp from Fluent Bit will not be added to records sent to Kinesis.","default":null,"type":"string"},{"name":"time_key_format","description":"strftime compliant format string for the timestamp; for example, the default is '%Y-%m-%dT%H:%M:%S'. This option is used with time_key. ","default":null,"type":"string"},{"name":"role_arn","description":"ARN of an IAM role to assume (ex. for cross account access).","default":null,"type":"string"},{"name":"endpoint","description":"Specify a custom endpoint for the Firehose API","default":null,"type":"string"},{"name":"sts_endpoint","description":"Custom endpoint for the STS API.","default":null,"type":"string"},{"name":"external_id","description":"Specify an external ID for the STS API, can be used with the role_arn parameter if your role requires an external ID.","default":null,"type":"string"},{"name":"compression","description":"Compression type for Firehose records. Each log record is individually compressed and sent to Firehose. 'gzip' and 'arrow' are the supported values. 'arrow' is only an available if Apache Arrow was enabled at compile time. Defaults to no compression.","default":null,"type":"string"},{"name":"log_key","description":"By default, the whole log record will be sent to Firehose. If you specify a key name with this option, then only the value of that key will be sent to Firehose. For example, if you are using the Fluentd Docker log driver, you can specify `log_key log` and only the log message will be sent to Firehose.","default":null,"type":"string"},{"name":"auto_retry_requests","description":"Immediately retry failed requests to AWS services once. This option does not affect the normal Fluent Bit retry mechanism with backoff. Instead, it enables an immediate retry with no delay for networking errors, which may help improve throughput when there are transient/random networking issues.","default":"true","type":"boolean"}]}},{"type":"output","name":"kinesis_streams","description":"Send logs to Amazon Kinesis Streams","properties":{"options":[{"name":"region","description":"The AWS region of your kinesis stream","default":null,"type":"string"},{"name":"stream","description":"Kinesis stream name","default":null,"type":"string"},{"name":"time_key","description":"Add the timestamp to the record under this key. By default the timestamp from Fluent Bit will not be added to records sent to Kinesis.","default":null,"type":"string"},{"name":"time_key_format","description":"strftime compliant format string for the timestamp; for example, the default is '%Y-%m-%dT%H:%M:%S'. This option is used with time_key. ","default":null,"type":"string"},{"name":"role_arn","description":"ARN of an IAM role to assume (ex. for cross account access).","default":null,"type":"string"},{"name":"endpoint","description":"Specify a custom endpoint for the Kinesis API","default":null,"type":"string"},{"name":"sts_endpoint","description":"Custom endpoint for the STS API.","default":null,"type":"string"},{"name":"external_id","description":"Specify an external ID for the STS API, can be used with the role_arn parameter if your role requires an external ID.","default":null,"type":"string"},{"name":"log_key","description":"By default, the whole log record will be sent to Kinesis. If you specify a key name with this option, then only the value of that key will be sent to Kinesis. For example, if you are using the Fluentd Docker log driver, you can specify `log_key log` and only the log message will be sent to Kinesis.","default":null,"type":"string"},{"name":"auto_retry_requests","description":"Immediately retry failed requests to AWS services once. This option does not affect the normal Fluent Bit retry mechanism with backoff. Instead, it enables an immediate retry with no delay for networking errors, which may help improve throughput when there are transient/random networking issues.","default":"true","type":"boolean"}]}},{"type":"output","name":"opentelemetry","description":"OpenTelemetry","properties":{"options":[{"name":"add_label","description":"Adds a custom label to the metrics use format: 'add_label name value'","default":null,"type":"space delimited strings (minimum 1)"},{"name":"proxy","description":"Specify an HTTP Proxy. The expected format of this value is http://host:port. ","default":null,"type":"string"},{"name":"http_user","description":"Set HTTP auth user","default":null,"type":"string"},{"name":"http_passwd","description":"Set HTTP auth password","default":"","type":"string"},{"name":"header","description":"Add a HTTP header key/value pair. Multiple headers can be set","default":null,"type":"space delimited strings (minimum 1)"},{"name":"uri","description":"Specify an optional HTTP URI for the target OTel endpoint.","default":"/v1/metrics","type":"string"},{"name":"log_response_payload","description":"Specify if the response paylod should be logged or not","default":"true","type":"boolean"}],"networking":[{"name":"net.dns.mode","description":"Select the primary DNS connection type (TCP or UDP)","default":null,"type":"string"},{"name":"net.dns.resolver","description":"Select the primary DNS resolver type (LEGACY or ASYNC)","default":null,"type":"string"},{"name":"net.dns.prefer_ipv4","description":"Prioritize IPv4 DNS results when trying to establish a connection","default":"false","type":"boolean"},{"name":"net.keepalive","description":"Enable or disable Keepalive support","default":"true","type":"boolean"},{"name":"net.keepalive_idle_timeout","description":"Set maximum time allowed for an idle Keepalive connection","default":"30s","type":"time"},{"name":"net.connect_timeout","description":"Set maximum time allowed to establish a connection, this time includes the TLS handshake","default":"10s","type":"time"},{"name":"net.connect_timeout_log_error","description":"On connection timeout, specify if it should log an error. When disabled, the timeout is logged as a debug message","default":"true","type":"boolean"},{"name":"net.source_address","description":"Specify network address to bind for data traffic","default":null,"type":"string"},{"name":"net.keepalive_max_recycle","description":"Set maximum number of times a keepalive connection can be used before it is retired.","default":"2000","type":"integer"}],"network_tls":[{"name":"tls","description":"Enable or disable TLS/SSL support","default":"off","type":"boolean"},{"name":"tls.verify","description":"Force certificate validation","default":"on","type":"boolean"},{"name":"tls.debug","description":"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose","default":"1","type":"integer"},{"name":"tls.ca_file","description":"Absolute path to CA certificate file","default":null,"type":"string"},{"name":"tls.ca_path","description":"Absolute path to scan for certificate files","default":null,"type":"string"},{"name":"tls.crt_file","description":"Absolute path to Certificate file","default":null,"type":"string"},{"name":"tls.key_file","description":"Absolute path to private Key file","default":null,"type":"string"},{"name":"tls.key_passwd","description":"Optional password for tls.key_file file","default":null,"type":"string"},{"name":"tls.vhost","description":"Hostname to be used for TLS SNI extension","default":null,"type":"string"}]}},{"type":"output","name":"prometheus_exporter","description":"Prometheus Exporter","properties":{"options":[{"name":"add_timestamp","description":"Add timestamp to every metric honoring collection time.","default":"false","type":"boolean"},{"name":"add_label","description":"TCP port for listening for HTTP connections.","default":null,"type":"space delimited strings (minimum 1)"}],"networking":[{"name":"net.dns.mode","description":"Select the primary DNS connection type (TCP or UDP)","default":null,"type":"string"},{"name":"net.dns.resolver","description":"Select the primary DNS resolver type (LEGACY or ASYNC)","default":null,"type":"string"},{"name":"net.dns.prefer_ipv4","description":"Prioritize IPv4 DNS results when trying to establish a connection","default":"false","type":"boolean"},{"name":"net.keepalive","description":"Enable or disable Keepalive support","default":"true","type":"boolean"},{"name":"net.keepalive_idle_timeout","description":"Set maximum time allowed for an idle Keepalive connection","default":"30s","type":"time"},{"name":"net.connect_timeout","description":"Set maximum time allowed to establish a connection, this time includes the TLS handshake","default":"10s","type":"time"},{"name":"net.connect_timeout_log_error","description":"On connection timeout, specify if it should log an error. When disabled, the timeout is logged as a debug message","default":"true","type":"boolean"},{"name":"net.source_address","description":"Specify network address to bind for data traffic","default":null,"type":"string"},{"name":"net.keepalive_max_recycle","description":"Set maximum number of times a keepalive connection can be used before it is retired.","default":"2000","type":"integer"}]}},{"type":"output","name":"prometheus_remote_write","description":"Prometheus remote write","properties":{"options":[{"name":"add_label","description":"Adds a custom label to the metrics use format: 'add_label name value'","default":null,"type":"space delimited strings (minimum 1)"},{"name":"proxy","description":"Specify an HTTP Proxy. The expected format of this value is http://host:port. ","default":null,"type":"string"},{"name":"http_user","description":"Set HTTP auth user","default":null,"type":"string"},{"name":"http_passwd","description":"Set HTTP auth password","default":"","type":"string"},{"name":"header","description":"Add a HTTP header key/value pair. Multiple headers can be set","default":null,"type":"space delimited strings (minimum 1)"},{"name":"uri","description":"Specify an optional HTTP URI for the target web server, e.g: /something","default":null,"type":"string"},{"name":"log_response_payload","description":"Specify if the response paylod should be logged or not","default":"true","type":"boolean"}],"networking":[{"name":"net.dns.mode","description":"Select the primary DNS connection type (TCP or UDP)","default":null,"type":"string"},{"name":"net.dns.resolver","description":"Select the primary DNS resolver type (LEGACY or ASYNC)","default":null,"type":"string"},{"name":"net.dns.prefer_ipv4","description":"Prioritize IPv4 DNS results when trying to establish a connection","default":"false","type":"boolean"},{"name":"net.keepalive","description":"Enable or disable Keepalive support","default":"true","type":"boolean"},{"name":"net.keepalive_idle_timeout","description":"Set maximum time allowed for an idle Keepalive connection","default":"30s","type":"time"},{"name":"net.connect_timeout","description":"Set maximum time allowed to establish a connection, this time includes the TLS handshake","default":"10s","type":"time"},{"name":"net.connect_timeout_log_error","description":"On connection timeout, specify if it should log an error. When disabled, the timeout is logged as a debug message","default":"true","type":"boolean"},{"name":"net.source_address","description":"Specify network address to bind for data traffic","default":null,"type":"string"},{"name":"net.keepalive_max_recycle","description":"Set maximum number of times a keepalive connection can be used before it is retired.","default":"2000","type":"integer"}],"network_tls":[{"name":"tls","description":"Enable or disable TLS/SSL support","default":"off","type":"boolean"},{"name":"tls.verify","description":"Force certificate validation","default":"on","type":"boolean"},{"name":"tls.debug","description":"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose","default":"1","type":"integer"},{"name":"tls.ca_file","description":"Absolute path to CA certificate file","default":null,"type":"string"},{"name":"tls.ca_path","description":"Absolute path to scan for certificate files","default":null,"type":"string"},{"name":"tls.crt_file","description":"Absolute path to Certificate file","default":null,"type":"string"},{"name":"tls.key_file","description":"Absolute path to private Key file","default":null,"type":"string"},{"name":"tls.key_passwd","description":"Optional password for tls.key_file file","default":null,"type":"string"},{"name":"tls.vhost","description":"Hostname to be used for TLS SNI extension","default":null,"type":"string"}]}},{"type":"output","name":"s3","description":"Send to S3","properties":{"options":[{"name":"json_date_format","description":"Specify the format of the date, supported formats: double, iso8601 (e.g: 2018-05-30T09:39:52.000681Z), java_sql_timestamp (e.g: 2018-05-30 09:39:52.000681, useful for AWS Athena), and epoch.","default":null,"type":"string"},{"name":"json_date_key","description":"Specifies the name of the date field in output.","default":"date","type":"string"},{"name":"total_file_size","description":"Specifies the size of files in S3. Maximum size is 50GB, minimum is 1MB","default":"100000000","type":"size"},{"name":"upload_chunk_size","description":"This plugin uses the S3 Multipart Upload API to stream data to S3, ensuring your data gets-off-the-box as quickly as possible. This parameter configures the size of each “part” in the upload. The total_file_size option configures the size of the file you will see in S3; this option determines the size of chunks uploaded until that size is reached. These chunks are temporarily stored in chunk_buffer_path until their size reaches upload_chunk_size, which point the chunk is uploaded to S3. Default: 5M, Max: 50M, Min: 5M.","default":"5242880","type":"size"},{"name":"upload_timeout","description":"Optionally specify a timeout for uploads. Whenever this amount of time has elapsed, Fluent Bit will complete an upload and create a new file in S3. For example, set this value to 60m and you will get a new file in S3 every hour. Default is 10m.","default":"10m","type":"time"},{"name":"bucket","description":"S3 bucket name.","default":null,"type":"string"},{"name":"region","description":"AWS region.","default":"us-east-1","type":"string"},{"name":"role_arn","description":"ARN of an IAM role to assume (ex. for cross account access).","default":null,"type":"string"},{"name":"endpoint","description":"Custom endpoint for the S3 API.","default":null,"type":"string"},{"name":"sts_endpoint","description":"Custom endpoint for the STS API.","default":null,"type":"string"},{"name":"canned_acl","description":"Predefined Canned ACL policy for S3 objects.","default":null,"type":"string"},{"name":"compression","description":"Compression type for S3 objects. 'gzip' and 'arrow' are the supported values. 'arrow' is only an available if Apache Arrow was enabled at compile time. Defaults to no compression. If 'gzip' is selected, the Content-Encoding HTTP Header will be set to 'gzip'.","default":null,"type":"string"},{"name":"content_type","description":"A standard MIME type for the S3 object; this will be set as the Content-Type HTTP header.","default":null,"type":"string"},{"name":"store_dir","description":"Directory to locally buffer data before sending. Plugin uses the S3 Multipart upload API to send data in chunks of 5 MB at a time- only a small amount of data will be locally buffered at any given point in time.","default":"/tmp/fluent-bit/s3","type":"string"},{"name":"s3_key_format","description":"Format string for keys in S3. This option supports strftime time formatters and a syntax for selecting parts of the Fluent log tag using a syntax inspired by the rewrite_tag filter. Add $TAG in the format string to insert the full log tag; add $TAG[0] to insert the first part of the tag in the s3 key. The tag is split into “parts” using the characters specified with the s3_key_format_tag_delimiters option. Add $INDEX to enable sequential indexing for file names. Adding $INDEX will prevent random string being added to end of keywhen $UUID is not provided. See the in depth examples and tutorial in the documentation.","default":"/fluent-bit-logs/$TAG/%Y/%m/%d/%H/%M/%S","type":"string"},{"name":"s3_key_format_tag_delimiters","description":"A series of characters which will be used to split the tag into “parts” for use with the s3_key_format option. See the in depth examples and tutorial in the documentation.","default":".","type":"string"},{"name":"auto_retry_requests","description":"Immediately retry failed requests to AWS services once. This option does not affect the normal Fluent Bit retry mechanism with backoff. Instead, it enables an immediate retry with no delay for networking errors, which may help improve throughput when there are transient/random networking issues.","default":"true","type":"boolean"},{"name":"use_put_object","description":"Use the S3 PutObject API, instead of the multipart upload API","default":"false","type":"boolean"},{"name":"send_content_md5","description":"Send the Content-MD5 header with object uploads, as is required when Object Lock is enabled","default":"false","type":"boolean"},{"name":"preserve_data_ordering","description":"Normally, when an upload request fails, there is a high chance for the last received chunk to be swapped with a later chunk, resulting in data shuffling. This feature prevents this shuffling by using a queue logic for uploads.","default":"false","type":"boolean"},{"name":"log_key","description":"By default, the whole log record will be sent to S3. If you specify a key name with this option, then only the value of that key will be sent to S3.","default":null,"type":"string"},{"name":"external_id","description":"Specify an external ID for the STS API, can be used with the role_arn parameter if your role requires an external ID.","default":null,"type":"string"},{"name":"static_file_path","description":"Disables behavior where UUID string is automatically appended to end of S3 key name when $UUID is not provided in s3_key_format. $UUID, time formatters, $TAG, and other dynamic key formatters all work as expected while this feature is set to true.","default":"false","type":"boolean"},{"name":"storage_class","description":"Specify the storage class for S3 objects. If this option is not specified, objects will be stored with the default 'STANDARD' storage class.","default":null,"type":"string"}],"networking":[{"name":"net.dns.mode","description":"Select the primary DNS connection type (TCP or UDP)","default":null,"type":"string"},{"name":"net.dns.resolver","description":"Select the primary DNS resolver type (LEGACY or ASYNC)","default":null,"type":"string"},{"name":"net.dns.prefer_ipv4","description":"Prioritize IPv4 DNS results when trying to establish a connection","default":"false","type":"boolean"},{"name":"net.keepalive","description":"Enable or disable Keepalive support","default":"true","type":"boolean"},{"name":"net.keepalive_idle_timeout","description":"Set maximum time allowed for an idle Keepalive connection","default":"30s","type":"time"},{"name":"net.connect_timeout","description":"Set maximum time allowed to establish a connection, this time includes the TLS handshake","default":"10s","type":"time"},{"name":"net.connect_timeout_log_error","description":"On connection timeout, specify if it should log an error. When disabled, the timeout is logged as a debug message","default":"true","type":"boolean"},{"name":"net.source_address","description":"Specify network address to bind for data traffic","default":null,"type":"string"},{"name":"net.keepalive_max_recycle","description":"Set maximum number of times a keepalive connection can be used before it is retired.","default":"2000","type":"integer"}],"network_tls":[{"name":"tls","description":"Enable or disable TLS/SSL support","default":"off","type":"boolean"},{"name":"tls.verify","description":"Force certificate validation","default":"on","type":"boolean"},{"name":"tls.debug","description":"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose","default":"1","type":"integer"},{"name":"tls.ca_file","description":"Absolute path to CA certificate file","default":null,"type":"string"},{"name":"tls.ca_path","description":"Absolute path to scan for certificate files","default":null,"type":"string"},{"name":"tls.crt_file","description":"Absolute path to Certificate file","default":null,"type":"string"},{"name":"tls.key_file","description":"Absolute path to private Key file","default":null,"type":"string"},{"name":"tls.key_passwd","description":"Optional password for tls.key_file file","default":null,"type":"string"},{"name":"tls.vhost","description":"Hostname to be used for TLS SNI extension","default":null,"type":"string"}]}}]}

'''
'''--- conf/grafana/dashboard.json ---
{
    "annotations": {
      "list": [
        {
          "builtIn": 1,
          "datasource": {
            "type": "grafana",
            "uid": "-- Grafana --"
          },
          "enable": true,
          "hide": true,
          "iconColor": "rgba(0, 211, 255, 1)",
          "name": "Annotations & Alerts",
          "target": {
            "limit": 100,
            "matchAny": false,
            "tags": [],
            "type": "dashboard"
          },
          "type": "dashboard"
        }
      ]
    },
    "editable": true,
    "fiscalYearStartMonth": 0,
    "graphTooltip": 0,
    "id": 1,
    "links": [],
    "liveNow": false,
    "panels": [
      {
        "gridPos": {
          "h": 1,
          "w": 24,
          "x": 0,
          "y": 0
        },
        "id": 10,
        "title": "Service Monitoring",
        "type": "row"
      },
      {
        "datasource": {
          "type": "postgres",
          "uid": "HvAj3aV4z"
        },
        "fieldConfig": {
          "defaults": {
            "color": {
              "mode": "palette-classic"
            },
            "custom": {
              "axisCenteredZero": false,
              "axisColorMode": "text",
              "axisLabel": "",
              "axisPlacement": "auto",
              "axisSoftMin": 0,
              "barAlignment": 0,
              "drawStyle": "line",
              "fillOpacity": 0,
              "gradientMode": "none",
              "hideFrom": {
                "legend": false,
                "tooltip": false,
                "viz": false
              },
              "lineInterpolation": "linear",
              "lineWidth": 1,
              "pointSize": 5,
              "scaleDistribution": {
                "type": "linear"
              },
              "showPoints": "auto",
              "spanNulls": false,
              "stacking": {
                "group": "A",
                "mode": "none"
              },
              "thresholdsStyle": {
                "mode": "off"
              }
            },
            "mappings": [],
            "thresholds": {
              "mode": "absolute",
              "steps": [
                {
                  "color": "green",
                  "value": null
                },
                {
                  "color": "red",
                  "value": 80
                }
              ]
            }
          },
          "overrides": []
        },
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 0,
          "y": 1
        },
        "id": 14,
        "options": {
          "legend": {
            "calcs": [
              "lastNotNull",
              "sum"
            ],
            "displayMode": "list",
            "placement": "right",
            "showLegend": true
          },
          "tooltip": {
            "mode": "single",
            "sort": "none"
          }
        },
        "pluginVersion": "9.1.6",
        "targets": [
          {
            "datasource": {
              "type": "postgres",
              "uid": "HvAj3aV4z"
            },
            "format": "time_series",
            "group": [],
            "metricColumn": "none",
            "rawQuery": true,
            "rawSql": "SELECT \n  $__unixEpochGroup(block_timestamp,'5m') as time,\n  receiver_account_id,\n  count(receiver_account_id)\nFROM transactions \nWHERE\n  $__unixEpochNanoFilter(block_timestamp)\tAND\n  receiver_account_id IN ('vault.world-triathlon.testnet', 'nft.world-triathlon.testnet', 'asheltie.testnet')\nGROUP BY\n  time, receiver_account_id",
            "refId": "A",
            "select": [
              [
                {
                  "params": [
                    "value"
                  ],
                  "type": "column"
                }
              ]
            ],
            "timeColumn": "time",
            "where": [
              {
                "name": "$__timeFilter",
                "params": [],
                "type": "macro"
              }
            ]
          }
        ],
        "title": "Testnet Transaction Count",
        "transformations": [
          {
            "id": "convertFieldType",
            "options": {
              "conversions": [
                {
                  "destinationType": "number",
                  "targetField": "count"
                }
              ],
              "fields": {}
            }
          }
        ],
        "type": "timeseries"
      },
      {
        "datasource": {
          "type": "vertamedia-clickhouse-datasource",
          "uid": "${clickhouse_datasource}"
        },
        "fieldConfig": {
          "defaults": {
            "color": {
              "mode": "palette-classic"
            },
            "custom": {
              "axisCenteredZero": false,
              "axisColorMode": "text",
              "axisLabel": "",
              "axisPlacement": "auto",
              "barAlignment": 0,
              "drawStyle": "line",
              "fillOpacity": 0,
              "gradientMode": "none",
              "hideFrom": {
                "legend": false,
                "tooltip": false,
                "viz": false
              },
              "lineInterpolation": "linear",
              "lineWidth": 1,
              "pointSize": 5,
              "scaleDistribution": {
                "type": "linear"
              },
              "showPoints": "auto",
              "spanNulls": false,
              "stacking": {
                "group": "A",
                "mode": "none"
              },
              "thresholdsStyle": {
                "mode": "off"
              }
            },
            "mappings": [],
            "thresholds": {
              "mode": "absolute",
              "steps": [
                {
                  "color": "green",
                  "value": null
                },
                {
                  "color": "red",
                  "value": 80
                }
              ]
            }
          },
          "overrides": []
        },
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 12,
          "y": 1
        },
        "id": 8,
        "options": {
          "legend": {
            "calcs": [
              "last",
              "sum"
            ],
            "displayMode": "table",
            "placement": "right",
            "showLegend": true
          },
          "tooltip": {
            "mode": "multi",
            "sort": "none"
          }
        },
        "targets": [
          {
            "database": "grafana",
            "datasource": {
              "type": "vertamedia-clickhouse-datasource",
              "uid": "${clickhouse_datasource}"
            },
            "dateColDataType": "",
            "dateLoading": false,
            "dateTimeColDataType": "time",
            "dateTimeType": "DATETIME",
            "datetimeLoading": false,
            "extrapolate": true,
            "format": "time_series",
            "formattedQuery": "SELECT $timeSeries as t, count() FROM $table WHERE $timeFilter GROUP BY t ORDER BY t",
            "interval": "1m",
            "intervalFactor": 1,
            "query": "SELECT\n    $timeSeries as t,\n    concat(service_name, '-', log_type) as label,\n    count()\nFROM $table\n\nWHERE $timeFilter AND\nlog_type in ($log_type) AND\nservice_name in ($service_name) AND\nlevel in ($level)\n\nGROUP BY t, label\n\nORDER BY t, label\n",
            "rawQuery": "SELECT\n    (intDiv(toUInt32(time), 60) * 60) * 1000 as t,\n    concat(service_name, '-', log_type) as label,\n    count()\nFROM grafana.t_near_processor_logs\n\nWHERE time >= toDateTime(1664794301) AND time <= toDateTime(1664796101) AND\nlog_type in ('','default') AND\nservice_name in ('near_processor','arweave_processor') AND\nlevel in ('debug')\n\nGROUP BY t, label\n\nORDER BY t, label",
            "refId": "A",
            "round": "0s",
            "skip_comments": true,
            "table": "t_near_processor_logs",
            "tableLoading": false
          }
        ],
        "title": "Log Count Total",
        "type": "timeseries"
      },
      {
        "datasource": {
          "type": "postgres",
          "uid": "HvAj3aV4z"
        },
        "fieldConfig": {
          "defaults": {
            "color": {
              "mode": "thresholds"
            },
            "custom": {
              "align": "auto",
              "displayMode": "auto",
              "inspect": false
            },
            "mappings": [],
            "thresholds": {
              "mode": "absolute",
              "steps": [
                {
                  "color": "green",
                  "value": null
                },
                {
                  "color": "red",
                  "value": 80
                }
              ]
            }
          },
          "overrides": [
            {
              "matcher": {
                "id": "byName",
                "options": "block_timestamp"
              },
              "properties": [
                {
                  "id": "custom.width",
                  "value": 177
                }
              ]
            },
            {
              "matcher": {
                "id": "byName",
                "options": "signer_account_id"
              },
              "properties": [
                {
                  "id": "custom.width",
                  "value": 181
                }
              ]
            },
            {
              "matcher": {
                "id": "byName",
                "options": "receiver_account_id"
              },
              "properties": [
                {
                  "id": "custom.width",
                  "value": 207
                }
              ]
            },
            {
              "matcher": {
                "id": "byName",
                "options": "status"
              },
              "properties": [
                {
                  "id": "custom.width",
                  "value": 189
                }
              ]
            },
            {
              "matcher": {
                "id": "byName",
                "options": "action_kind"
              },
              "properties": [
                {
                  "id": "custom.width",
                  "value": 172
                }
              ]
            }
          ]
        },
        "gridPos": {
          "h": 9,
          "w": 24,
          "x": 0,
          "y": 9
        },
        "id": 15,
        "options": {
          "footer": {
            "fields": "",
            "reducer": [
              "sum"
            ],
            "show": false
          },
          "showHeader": true,
          "sortBy": []
        },
        "pluginVersion": "9.1.6",
        "targets": [
          {
            "datasource": {
              "type": "postgres",
              "uid": "HvAj3aV4z"
            },
            "format": "table",
            "group": [],
            "metricColumn": "none",
            "rawQuery": true,
            "rawSql": "SELECT \n  block_timestamp,\n  signer_account_id,\n  receiver_account_id,\n  status,\n  action_kind,\n  args\nFROM transactions \nLEFT JOIN transaction_actions ON transaction_actions.transaction_hash = transactions.transaction_hash\nWHERE\n  $__unixEpochNanoFilter(block_timestamp)\tAND\n  receiver_account_id IN ('vault.world-triathlon.testnet')\n",
            "refId": "A",
            "select": [
              [
                {
                  "params": [
                    "value"
                  ],
                  "type": "column"
                }
              ]
            ],
            "timeColumn": "time",
            "where": [
              {
                "name": "$__timeFilter",
                "params": [],
                "type": "macro"
              }
            ]
          }
        ],
        "title": "Testnet Transaction Logs",
        "transformations": [
          {
            "id": "convertFieldType",
            "options": {
              "conversions": [
                {
                  "destinationType": "number",
                  "targetField": "count"
                }
              ],
              "fields": {}
            }
          }
        ],
        "type": "table"
      },
      {
        "datasource": {
          "type": "vertamedia-clickhouse-datasource",
          "uid": "${clickhouse_datasource}"
        },
        "fieldConfig": {
          "defaults": {
            "color": {
              "mode": "thresholds"
            },
            "custom": {
              "align": "auto",
              "displayMode": "auto",
              "inspect": false
            },
            "mappings": [],
            "thresholds": {
              "mode": "absolute",
              "steps": [
                {
                  "color": "green",
                  "value": null
                },
                {
                  "color": "red",
                  "value": 80
                }
              ]
            }
          },
          "overrides": [
            {
              "matcher": {
                "id": "byName",
                "options": "time"
              },
              "properties": [
                {
                  "id": "custom.width",
                  "value": 153
                }
              ]
            },
            {
              "matcher": {
                "id": "byName",
                "options": "level"
              },
              "properties": [
                {
                  "id": "custom.width",
                  "value": 64
                }
              ]
            },
            {
              "matcher": {
                "id": "byName",
                "options": "log_type"
              },
              "properties": [
                {
                  "id": "custom.width",
                  "value": 67
                }
              ]
            },
            {
              "matcher": {
                "id": "byName",
                "options": "service_name"
              },
              "properties": [
                {
                  "id": "custom.width",
                  "value": 140
                }
              ]
            }
          ]
        },
        "gridPos": {
          "h": 11,
          "w": 24,
          "x": 0,
          "y": 18
        },
        "id": 12,
        "options": {
          "footer": {
            "fields": "",
            "reducer": [
              "sum"
            ],
            "show": false
          },
          "showHeader": true,
          "sortBy": [
            {
              "desc": true,
              "displayName": "time"
            }
          ]
        },
        "pluginVersion": "9.1.6",
        "targets": [
          {
            "database": "grafana",
            "datasource": {
              "type": "vertamedia-clickhouse-datasource",
              "uid": "${clickhouse_datasource}"
            },
            "dateColDataType": "",
            "dateLoading": false,
            "dateTimeColDataType": "time",
            "dateTimeType": "DATETIME",
            "datetimeLoading": false,
            "extrapolate": true,
            "format": "table",
            "formattedQuery": "SELECT $timeSeries as t, count() FROM $table WHERE $timeFilter GROUP BY t ORDER BY t",
            "intervalFactor": 1,
            "query": "SELECT\n    time,\n    level,\n    log_type,\n    service_name,\n    message\n    \nFROM $table\n\nWHERE $timeFilter AND\nlog_type in ($log_type) AND\nservice_name in ($service_name) AND\nlevel in ($level)\n\n",
            "rawQuery": "SELECT\n    time,\n    level,\n    log_type,\n    service_name,\n    message\n    \nFROM grafana.t_near_processor_logs\n\nWHERE time >= toDateTime(1664794317) AND time <= toDateTime(1664796117) AND\nlog_type in ('','default') AND\nservice_name in ('near_processor','arweave_processor') AND\nlevel in ('debug')",
            "refId": "A",
            "round": "0s",
            "skip_comments": true,
            "table": "t_near_processor_logs",
            "tableLoading": false
          }
        ],
        "title": "Service Logs",
        "type": "table"
      },
      {
        "gridPos": {
          "h": 1,
          "w": 24,
          "x": 0,
          "y": 29
        },
        "id": 6,
        "title": "Container Monitoring",
        "type": "row"
      },
      {
        "datasource": {
          "type": "vertamedia-clickhouse-datasource",
          "uid": "${clickhouse_datasource}"
        },
        "fieldConfig": {
          "defaults": {
            "color": {
              "mode": "thresholds"
            },
            "custom": {
              "align": "auto",
              "displayMode": "auto",
              "inspect": false
            },
            "mappings": [],
            "thresholds": {
              "mode": "absolute",
              "steps": [
                {
                  "color": "green",
                  "value": null
                },
                {
                  "color": "red",
                  "value": 80
                }
              ]
            }
          },
          "overrides": [
            {
              "matcher": {
                "id": "byName",
                "options": "time"
              },
              "properties": [
                {
                  "id": "custom.width",
                  "value": 162
                }
              ]
            },
            {
              "matcher": {
                "id": "byName",
                "options": "stream"
              },
              "properties": [
                {
                  "id": "custom.width",
                  "value": 67
                }
              ]
            }
          ]
        },
        "gridPos": {
          "h": 9,
          "w": 24,
          "x": 0,
          "y": 30
        },
        "id": 2,
        "options": {
          "footer": {
            "fields": "",
            "reducer": [
              "sum"
            ],
            "show": false
          },
          "showHeader": true,
          "sortBy": [
            {
              "desc": true,
              "displayName": "time"
            }
          ]
        },
        "pluginVersion": "9.1.6",
        "targets": [
          {
            "database": "grafana",
            "datasource": {
              "type": "vertamedia-clickhouse-datasource",
              "uid": "${clickhouse_datasource}"
            },
            "dateColDataType": "",
            "dateLoading": false,
            "dateTimeColDataType": "time",
            "dateTimeType": "DATETIME",
            "datetimeLoading": false,
            "extrapolate": true,
            "format": "table",
            "formattedQuery": "SELECT $timeSeries as t, count() FROM $table WHERE $timeFilter GROUP BY t ORDER BY t",
            "intervalFactor": 1,
            "query": "SELECT\n    time, stream, log, log_path\nFROM $table\n\nWHERE $timeFilter\n",
            "rawQuery": "SELECT\n    time, stream, log, log_path\nFROM grafana.t_docker_logs\n\nWHERE time >= toDateTime(1664793004) AND time <= toDateTime(1664794804)",
            "refId": "A",
            "round": "0s",
            "skip_comments": true,
            "table": "t_docker_logs",
            "tableLoading": false
          }
        ],
        "title": "Container Logs",
        "type": "table"
      }
    ],
    "refresh": false,
    "schemaVersion": 37,
    "style": "dark",
    "tags": [],
    "templating": {
      "list": [
        {
          "current": {
            "selected": false,
            "text": "Altinity plugin for ClickHouse",
            "value": "Altinity plugin for ClickHouse"
          },
          "hide": 0,
          "includeAll": false,
          "label": "Clickhouse Datasource",
          "multi": false,
          "name": "clickhouse_datasource",
          "options": [],
          "query": "vertamedia-clickhouse-datasource",
          "queryValue": "",
          "refresh": 1,
          "regex": "",
          "skipUrlSync": false,
          "type": "datasource"
        },
        {
          "current": {
            "selected": true,
            "text": [
              "near_processor"
            ],
            "value": [
              "near_processor"
            ]
          },
          "datasource": {
            "type": "vertamedia-clickhouse-datasource",
            "uid": "${clickhouse_datasource}"
          },
          "definition": "SELECT service_name from grafana.t_near_processor_logs",
          "hide": 0,
          "includeAll": false,
          "multi": true,
          "name": "service_name",
          "options": [],
          "query": "SELECT service_name from grafana.t_near_processor_logs",
          "refresh": 1,
          "regex": "",
          "skipUrlSync": false,
          "sort": 0,
          "type": "query"
        },
        {
          "current": {
            "selected": true,
            "text": [
              "default"
            ],
            "value": [
              "default"
            ]
          },
          "datasource": {
            "type": "vertamedia-clickhouse-datasource",
            "uid": "${clickhouse_datasource}"
          },
          "definition": "SELECT log_type from grafana.t_near_processor_logs",
          "hide": 0,
          "includeAll": false,
          "multi": true,
          "name": "log_type",
          "options": [],
          "query": "SELECT log_type from grafana.t_near_processor_logs",
          "refresh": 2,
          "regex": "",
          "skipUrlSync": false,
          "sort": 0,
          "type": "query"
        },
        {
          "current": {
            "selected": false,
            "text": "info",
            "value": "info"
          },
          "datasource": {
            "type": "vertamedia-clickhouse-datasource",
            "uid": "${clickhouse_datasource}"
          },
          "definition": "SELECT level from grafana.t_near_processor_logs",
          "hide": 0,
          "includeAll": false,
          "multi": true,
          "name": "level",
          "options": [],
          "query": "SELECT level from grafana.t_near_processor_logs",
          "refresh": 1,
          "regex": "",
          "skipUrlSync": false,
          "sort": 0,
          "type": "query"
        }
      ]
    },
    "time": {
      "from": "now-7d",
      "to": "now"
    },
    "timepicker": {},
    "timezone": "",
    "title": "New dashboard",
    "uid": "Cj2UsN4Vz",
    "version": 6,
    "weekStart": ""
  }
'''
'''--- conf/prometheus/prometheus.yml ---
# my global config
global:
  scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.
  # scrape_timeout is set to the global default (10s).

# Alertmanager configuration
alerting:
  alertmanagers:
    - static_configs:
        - targets:
          # - alertmanager:9093

# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.
rule_files:
  # - "first_rules.yml"
  # - "second_rules.yml"

# A scrape configuration containing exactly one endpoint to scrape:
# Here it's Prometheus itself.
scrape_configs:
  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.
  - job_name: "prometheus"

    # metrics_path defaults to '/metrics'
    # scheme defaults to 'http'.

    static_configs:
      - targets: ["localhost:9090", "node_exporter:9100", "host.docker.internal:9323"]

'''
'''--- contracts/nft/Cargo.toml ---
[package]
name = "nft"
version = "0.1.0"
edition = "2021"

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[lib]
crate-type = ["cdylib", "rlib"]

[dependencies]
near-contract-standards = "4.0.0"
near-sdk = "4.0.0"
uint = { version = "0.9.3", default-features = false }
'''
'''--- contracts/nft/README.md ---
# NFT Design Works Smart Contract
'''
'''--- contracts/nft/neardev/dev-account.env ---
CONTRACT_NAME=nft.world-triathlon.testnet
'''
'''--- contracts/nft/src/lib.rs ---
use std::{alloc::Layout, str::FromStr};

use near_contract_standards::{
    impl_non_fungible_token_approval, impl_non_fungible_token_core,
    impl_non_fungible_token_enumeration,
    non_fungible_token::{metadata::NFT_METADATA_SPEC, Token, TokenId},
    non_fungible_token::{
        metadata::{NFTContractMetadata, NonFungibleTokenMetadataProvider, TokenMetadata},
        NonFungibleToken,
    },
};

use near_sdk::{
    borsh::{self, BorshDeserialize, BorshSerialize},
    collections::LazyOption,
    env,
    json_types::Base64VecU8,
    near_bindgen, require, AccountId, BorshStorageKey, PanicOnDefault, Promise, PromiseOrValue,
};

#[near_bindgen]
#[derive(BorshDeserialize, BorshSerialize, BorshStorageKey)]
enum StorageKey {
    OwnerByID,
    TokenMetadata,
    Enumeration,
    Approval,
    ContractMetadata,
}

#[near_bindgen]
#[derive(BorshDeserialize, BorshSerialize, PanicOnDefault)]
struct Contract {
    nft: NonFungibleToken,
    metadata: LazyOption<NFTContractMetadata>,
}

#[near_bindgen]
impl Contract {
    #[init]
    pub fn default() -> Self {
        Self::new(
            "World Triathlon".to_string(),
            "WT".to_string(),
            Some("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAABmJLR0QA/wD/AP+gvaeTAAAAB3RJTUUH5QsIFRgqe3QJewAAAhZJREFUWMPt1b2LVVcUxuFnnXtmyEwUi4i9iKaKRot0CVhZRUTBQgULYQaFaCEIkWAIQv4ACxFLxYhFCsV0KRwbSREFCxGxSCAEhBj8ALnqnb1S3DPjHTk3MxqjhfcHm7PZ6z1rv3ut88GIESPed2Iqy3chaUZ/XgxfKwKhyHlNPxZLzvEiT41j77IC9cC8i4eYxPIW7RM8bmKTA/c8eI19lxHLyAUGruAQuQ9HFuoTLuB7fIs9TWAGB9FDDNms01xnm2uP/AqHX67A4zHdOz3jNxozK7B+IMHfyd3on/gJruM3rJlz2MJT4pdQKnyG8cbIR20t8NSH0fH8AbGffITz+KLlRLdCfJnKFvw4YHKQCn9ic7PxWaxCITqUedE8Y7qJDeShorpHHsdfLckL0W1KPz5k1BhrWhPNvG5inUGXbewNueOkyZ9DnhqiieaEl3CxGZdx3ytQD1mfCPnNAd1fgxNNGReYTdkpetcq9c7GT5KT+Amf/1cD8An5dYr90W/FxsHHPCiVehOm++ZyLt/aN1GBOXbhaig/pLgdenK+EFGwGvu0P4RLolokPoGjqbOO6vdcVP5mDLz8Tn+Mo/3+xhJSvho1zulnvjdrbE8om1p028k/iJvkOqxMZRfxKflvribIbSnqkB+0CWIqS4TZxG6cCVm91b/h6ahyOp/T/0Q+83/UeZEWzDGDrW/bwIgRI945/wC1LcF+TG5FFAAAACV0RVh0ZGF0ZTpjcmVhdGUAMjAyMS0xMS0wOFQyMToyNDo0MiswMDowMKSIPSMAAAAldEVYdGRhdGU6bW9kaWZ5ADIwMjEtMTEtMDhUMjE6MjQ6NDIrMDA6MDDV1YWfAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAFd6VFh0UmF3IHByb2ZpbGUgdHlwZSBpcHRjAAB4nOPyDAhxVigoyk/LzEnlUgADIwsuYwsTIxNLkxQDEyBEgDTDZAMjs1Qgy9jUyMTMxBzEB8uASKBKLgDqFxF08kI1lQAAAABJRU5ErkJggg==".to_string()), Some("https://arweave.net".to_string()),
            None,
            None
        )
    }

    #[init]
    pub fn new(
        name: String,
        symbol: String,
        icon: Option<String>,
        base_uri: Option<String>,
        reference: Option<String>,
        reference_hash: Option<Base64VecU8>,
    ) -> Self {
        let metadata = NFTContractMetadata {
            spec: NFT_METADATA_SPEC.to_string(),
            name,
            symbol,
            icon,
            base_uri,
            reference,
            reference_hash,
        };
        Self {
            nft: NonFungibleToken::new(
                StorageKey::OwnerByID,
                env::current_account_id(),
                Some(StorageKey::TokenMetadata),
                Some(StorageKey::Enumeration),
                Some(StorageKey::Approval),
            ),
            metadata: LazyOption::new(StorageKey::ContractMetadata, Some(&metadata)),
        }
    }

    #[private]
    pub fn update_meta(&mut self, name: Option<String>, symbol: Option<String>, icon: Option<String>, base_uri: Option<String>, reference: Option<String>, reference_hash: Option<Base64VecU8>) -> bool {
        let mut metadata = self.metadata.get().unwrap();

        if let Some(name) = name {
            metadata.name = name;
        }

        if let Some(symbol) = symbol {
            metadata.symbol = symbol;
        }

        if let Some(icon) = icon {
            metadata.icon = Some(icon);
        }

        if let Some(base_uri) = base_uri {
            metadata.base_uri = Some(base_uri);
        }

        if let Some(reference) = reference {
            metadata.reference = Some(reference);
        }

        if let Some(reference_hash) = reference_hash {
            metadata.reference_hash = Some(reference_hash);
        }
        
        self.metadata.set(&metadata)
    }

    #[private]
    #[payable]
    pub fn mint(
        &mut self,
        token_id: String,
        owner_address: Option<String>,
        media_id: String,
        metadata_id: String,
        title: Option<String>,
        description: Option<String>,
        media_hash: Option<Base64VecU8>,
        copies: Option<u64>,
        issued_at: Option<String>,
        expires_at: Option<String>,
        starts_at: Option<String>,
        updated_at: Option<String>,
        extra: Option<String>,
        reference_hash: Option<Base64VecU8>,
    ) -> Token {
        self.nft.internal_mint(
            TokenId::from_str(token_id.as_str()).unwrap(),
            match owner_address {
                Some(address) => address.parse().unwrap(),
                None => env::signer_account_id(),
            },
            Some(TokenMetadata {
                title,
                description,
                media: Some(media_id),
                media_hash,
                copies,
                issued_at,
                expires_at,
                starts_at,
                updated_at,
                extra,
                reference: Some(metadata_id),
                reference_hash,
            }),
        )
    }
}

impl_non_fungible_token_core!(Contract, nft);
impl_non_fungible_token_approval!(Contract, nft);
impl_non_fungible_token_enumeration!(Contract, nft);

#[near_bindgen]
impl NonFungibleTokenMetadataProvider for Contract {
    fn nft_metadata(&self) -> NFTContractMetadata {
        self.metadata.get().unwrap()
    }
}

'''
'''--- contracts/vault/Cargo.toml ---
[package]
name = "vault"
version = "0.1.0"
edition = "2021"

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[lib]
crate-type = ["cdylib", "rlib"]

[dependencies]
near-contract-standards = "4.0.0"
near-sdk = "4.0.0"
uint = { version = "0.9.3", default-features = false }
'''
'''--- contracts/vault/README.md ---
# NFT Design Works Smart Contract
'''
'''--- contracts/vault/neardev.bak2/dev-account.env ---
CONTRACT_NAME=dev-1667824170647-78369821344326
'''
'''--- contracts/vault/neardev/dev-account.env ---
CONTRACT_NAME=vault.world-triathlon.testnet
'''
'''--- contracts/vault/src/external/mod.rs ---
pub mod nft;
'''
'''--- contracts/vault/src/external/nft.rs ---
use near_contract_standards::non_fungible_token::{Token, TokenId};
use near_sdk::{ext_contract, AccountId};

#[ext_contract(external_nft)]
trait ExternalNFT {
    fn nft_token(&self, token_id: TokenId) -> Option<Token>;
    fn nft_transfer(
        &mut self,
        receiver_id: AccountId,
        token_id: TokenId,
        approval_id: Option<u64>,
        memo: Option<String>,
    );
}

'''
'''--- contracts/vault/src/internal/claimable.rs ---
use near_contract_standards::non_fungible_token::TokenId;
use near_sdk::{
    borsh::{self, BorshDeserialize, BorshSerialize},
    serde::{self, Deserialize, Serialize},
    AccountId, PublicKey,
};

/// A claimable nft
#[derive(BorshDeserialize, BorshSerialize, Serialize, Deserialize)]
#[serde(crate = "self::serde")]
pub struct Claimable {
    /// id of the nft
    pub token_id: TokenId,
    /// account id where the nft contract is deployed
    pub nft_account_id: AccountId,
    /// public key that is used to verify the authenticity of the claim
    ///
    /// near public key
    /// should be parsable into near_sdk::PublicKey
    pub public_key: String,
}

'''
'''--- contracts/vault/src/internal/mod.rs ---
pub mod claimable;
pub mod nft_token_callback_message;
pub mod on_transfer_message;

'''
'''--- contracts/vault/src/internal/nft_token_callback_message.rs ---
use near_sdk::{
    AccountId, serde::{self,Serialize, Deserialize}
};

#[derive(Serialize, Deserialize)]
#[serde(crate = "self::serde")]
pub struct NFTTokenCallbackMessage {
    pub public_key: String,
    pub nft_account_id: AccountId,
    pub message: String,
}
 
'''
'''--- contracts/vault/src/internal/on_transfer_message.rs ---
use near_sdk::{
    serde::{self, Deserialize, Serialize},
    serde_json,
};

#[derive(Serialize, Deserialize)]
#[serde(crate = "self::serde")]
pub struct OnTransferMessage {
    pub public_key: String,
    pub message: String,
}

impl OnTransferMessage {
    pub fn from_string(json: String) -> Result<OnTransferMessage, serde_json::Error> {
        serde_json::from_str(json.as_str())
    }
}

'''
'''--- contracts/vault/src/lib.rs ---
use external::nft::external_nft;
use internal::claimable::Claimable;
use near_contract_standards::{
    non_fungible_token::TokenId,
    non_fungible_token::{core::NonFungibleTokenReceiver, Token},
};
use near_sdk::{
    borsh::{self, BorshDeserialize, BorshSerialize},
    collections::{LookupMap, UnorderedSet},
    env, log, near_bindgen, require,
    utils::assert_one_yocto,
    AccountId, Balance, BorshStorageKey, Gas, PanicOnDefault, Promise, PromiseError,
    PromiseOrValue, PublicKey, ONE_YOCTO,
};

use crate::internal::{
    nft_token_callback_message::NFTTokenCallbackMessage, on_transfer_message::OnTransferMessage,
};

mod external;
mod internal;

const MILLIS_PER_MINUTE: u64 = 60_000;
const ONE_MILLINEAR: Balance = 1_000_000_000_000_000_000_000;

#[derive(BorshDeserialize, BorshSerialize, BorshStorageKey)]
enum StorageKey {
    ClaimablesKey,
    AllowedNFTsKey,
}

#[near_bindgen]
#[derive(BorshDeserialize, BorshSerialize, PanicOnDefault)]
struct Contract {
    pub claimables: LookupMap<String, Claimable>,
    pub allowed_nfts: UnorderedSet<AccountId>,
}

#[near_bindgen]
impl Contract {
    /// initial with default fields
    #[init]
    pub fn default() -> Self {
        Self {
            claimables: LookupMap::new(StorageKey::ClaimablesKey),
            allowed_nfts: UnorderedSet::new(StorageKey::AllowedNFTsKey),
        }
    }

    /// get allowed nft accounts to lock tokens on this vault
    pub fn get_allowed_nfts(&self) -> Vec<AccountId> {
        self.allowed_nfts.to_vec()
    }

    /// allow an nft contract to lock tokens on this vault
    #[private]
    pub fn allow_nft(&mut self, account_id: AccountId) -> bool {
        self.allowed_nfts.insert(&account_id)
    }

    /// remove an nft contract on the allowed list
    #[private]
    pub fn remove_allowed_nft(&mut self, account_id: AccountId) -> bool {
        self.allowed_nfts.remove(&account_id)
    }

    /// get a claimable
    pub fn get_claimable(&self, nft_account: String, token_id: String) -> Option<Claimable> {
        self.claimables
            .get(&format!("{}:{}", nft_account.as_str(), token_id.as_str()))
    }

    /// drop a claimable to specific user
    ///
    /// this method is called using an access key
    #[private]
    pub fn claim(&mut self, receiver_id: AccountId, claimable_id: String) -> Promise {
        let claimable = self.claimables.get(&claimable_id);
        if claimable.is_none() {
            env::panic_str(format!("claimable does not exist: {}", claimable_id).as_str());
        }
        let claimable = claimable.unwrap();

        if claimable.public_key.parse::<PublicKey>().unwrap() != env::signer_account_pk() {
            env::panic_str("claimable public key does not match signer_account_pk");
        }

        external_nft::ext(claimable.nft_account_id.clone())
            .with_attached_deposit(ONE_YOCTO)
            .nft_transfer(receiver_id, claimable.token_id.clone(), None, None)
            .then(Self::ext(env::current_account_id()).claim_callback(claimable))
    }

    /// claim callback
    #[private]
    pub fn claim_callback(
        &mut self,
        #[callback_result] call_result: Result<(), PromiseError>,
        claimable: Claimable,
    ) {
        if call_result.is_err() {
            return;
        }

        self.claimables.remove(&format!(
            "{}:{}",
            &claimable.nft_account_id, &claimable.token_id
        ));

        if env::current_account_id() == env::signer_account_id() {
            Promise::new(env::current_account_id()).delete_key(env::signer_account_pk());
        }
    }

    #[private]
    pub fn nft_token_callback(
        &mut self,
        #[callback_result] call_result: Result<Token, PromiseError>,
        msg: NFTTokenCallbackMessage,
    ) -> bool {
        if call_result.is_err() {
            return true;
        }

        let token = call_result.unwrap();

        let claimable = Claimable {
            token_id: token.token_id.clone(),
            nft_account_id: msg.nft_account_id.clone(),
            public_key: msg.public_key.clone(),
        };

        if token.owner_id.as_str() == env::current_account_id().as_str() {
            self.claimables.insert(
                &format!(
                    "{}:{}",
                    msg.nft_account_id.as_str(),
                    token.token_id.as_str()
                ),
                &claimable,
            );
        }

        Promise::new(env::current_account_id()).add_access_key(
            msg.public_key.parse().unwrap(),
            100 * ONE_MILLINEAR,
            env::current_account_id(),
            "claim".to_string(),
        );
        false
    }
}

#[near_bindgen]
impl NonFungibleTokenReceiver for Contract {
    #[payable]
    fn nft_on_transfer(
        &mut self,
        sender_id: AccountId,
        previous_owner_id: AccountId,
        token_id: TokenId,
        msg: String,
    ) -> PromiseOrValue<bool> {
        let payload: OnTransferMessage = OnTransferMessage::from_string(msg.to_string()).unwrap();
        log!(
            "message: {}, public_key: {}",
            payload.message,
            payload.public_key
        );

        let nft = env::predecessor_account_id();

        require!(
            self.allowed_nfts.contains(&nft),
            format!(
                "{} is not allowed to lock tokens on this vault",
                &nft.as_str()
            )
        );

        if let Some(err) = payload.public_key.parse::<PublicKey>().err() {
            panic!("public_key should be a valid near parsable PublicKey: {:?}", err)
        }

        PromiseOrValue::Promise(external_nft::ext(nft.clone()).nft_token(token_id).then(
            Self::ext(env::current_account_id()).nft_token_callback(NFTTokenCallbackMessage {
                public_key: payload.public_key,
                nft_account_id: nft,
                message: payload.message,
            }),
        ))
    }
}

'''
'''--- docs/assets/arweave_processor/DEMO.md ---
# Arweave Processor Local Demo

## Requirements
- [docker/docker-compose](https://docs.docker.com/get-docker/)

## Setting the Environment

create a .env file on jobs/arweave_processor directory

refer to [Environment Configuration](../../../jobs//arweave_processor/README.md#environment-configuration) for available keys, a .env.example file is also included in the directory for reference

## Build the Image
```bash
docker-compose build arweave_processor
```

## Install Dependencies
```bash
docker-compose run --rm arweave_processor install
```

## Run the Container

Once dependencies has been installed and .env file has been configured, processor service can be started easily using docker-compose.

```bash
# run the arweave processor
docker-compose up arweave_processor
```

Once the service is up and running, you can now interact with the queue using the [utility scripts](#utility-scripts) below

## Utility Scripts

Utility scripts are included in the repository.

```bash

# dispatch a test message on the configured storage queue
docker-compose run --rm arweave_processor dispatch:storage-queue

# peek first 32 messages from the storage queue
docker-compose run --rm arweave_processor peek:storage-queue

# clear storage queue
docker-compose run --rm arweave_processor clear:storage-queue
```

You can also import your wallet on [arweave.app](https://arweave.app) to see the created transactions
![](./arweave.app.png)
'''
'''--- frontend/vault/README.md ---
# Getting Started with Create React App

This project was bootstrapped with [Create React App](https://github.com/facebook/create-react-app).

## Available Scripts

In the project directory, you can run:

### `npm start`

Runs the app in the development mode.\
Open [http://localhost:3000](http://localhost:3000) to view it in the browser.

The page will reload if you make edits.\
You will also see any lint errors in the console.

### `npm test`

Launches the test runner in the interactive watch mode.\
See the section about [running tests](https://facebook.github.io/create-react-app/docs/running-tests) for more information.

### `npm run build`

Builds the app for production to the `build` folder.\
It correctly bundles React in production mode and optimizes the build for the best performance.

The build is minified and the filenames include the hashes.\
Your app is ready to be deployed!

See the section about [deployment](https://facebook.github.io/create-react-app/docs/deployment) for more information.

### `npm run eject`

**Note: this is a one-way operation. Once you `eject`, you can’t go back!**

If you aren’t satisfied with the build tool and configuration choices, you can `eject` at any time. This command will remove the single build dependency from your project.

Instead, it will copy all the configuration files and the transitive dependencies (webpack, Babel, ESLint, etc) right into your project so you have full control over them. All of the commands except `eject` will still work, but they will point to the copied scripts so you can tweak them. At this point you’re on your own.

You don’t have to ever use `eject`. The curated feature set is suitable for small and middle deployments, and you shouldn’t feel obligated to use this feature. However we understand that this tool wouldn’t be useful if you couldn’t customize it when you are ready for it.

## Learn More

You can learn more in the [Create React App documentation](https://facebook.github.io/create-react-app/docs/getting-started).

To learn React, check out the [React documentation](https://reactjs.org/).

'''
'''--- frontend/vault/docs/ClaimableNFT.md ---
# Claimable NFT

## About

This document describes essential details regarding claiming of nfts that are locked on the vault contract.
    
Smart contract code can be found [here](../../../contracts/)

## Architecture
![](./assets//architecture.png)

### NFT Processor
- a daemon service that mints nfts, and locks them on vault account if receiver is not present
- nft processor source code can be found [here](../../../jobs/near_processor/)

### NFT Smart Contract
- near nft standard smart contract [https://nomicon.io/Standards/Tokens/NonFungibleToken/](https://nomicon.io/Standards/Tokens/NonFungibleToken/)
- `nft_transfer_call` method calls `nft_on_transfer` on the receiver_id's account. [ref](https://nomicon.io/Standards/Tokens/NonFungibleToken/Core#nft-interface)
- nft smart contract source code can be found [here](../../../contracts/nft/)

### Vault Smart Contract
- near vault smart contract
- NFT receiver interface is implemented. [ref](https://nomicon.io/Standards/Tokens/NonFungibleToken/Core#receiver-interface)
- Receives nft together with a public key from the `nft_transfer_call` callback, stores the nft and the public key on the account state and adds the public key to the account's function call access keys, which allows anyone holding the private key of the corresponding public key to call the `claim` method in behalf of the vault account.
- The nft can be claimed from the vault, if the private key used to sign the transaction that calls the `claim` method, is matched with public key that is stored by the account from the `nft_on_tansfer` callback
- Upon successful claim, the nft and public key is deleted on the account's storage, also deletes the public key on the account's access keys
- vault contract source code can be found [here](../../../contracts/vault/)
- [sample claim txn](https://explorer.testnet.near.org/transactions/FneFtjU2zufjctZCnVnWznoUWiEaeYchcSkGJb2wxHS1)
    - this transaction is triggered by the claimer using an access key from the react frontend
    - notice that the transaction appears that it is signed by the vault contract, it is signed from the frontend using the associated public key that is added to the account's function call access keys, with this approach the vault contract will shoulder the gas fees for the user
    - the public key is also deleted from the account's function call access keys, which makes it a one time use key

## Security
- vault contract only accepts nfts from allowed accounts
    - to allow nfts to be locked on vaults [`allow_nft`](../../../contracts/vault/src/lib.rs) can be called on the vault contract
    - to view allowed nfts, call [`get_allowed_nfts`](../../../contracts/vault/src/lib.rs)
    - to delete an nft from the allowed list, call [`remove_allowed_nft`](../../../contracts/vault/src/lib.rs)
- claimable url contains a token query parameter, the token query parameter is a base64url string data that contains the private key that can be used to claim the nft
    - claimable url should be sent to the user securely, anyone that has access to the claimable url can claim the nft
- the private key from the claimable url token query parameter is added to the vault account's `claim` function call access keys, which allows the transactions fees to be charged on the vault account
- the `claim` method is marked as an internal method, which only allows it to be invoked by access keys added on the vault account's access keys
- `claim` function call checks if the claimable exists and if the private key used to sign the function call matches the public key stored on the storage
- upon successful claim, the access key is deleted on the account, making it a one time use access key

## Flow Chart
### NFT Locking Flow Chart
![](./assets/nft-locking-flow.png)

### NFT Claiming Flow Chart
![](./assets//nft-claim-flow.png)
'''
'''--- frontend/vault/package.json ---
{
  "name": "vault",
  "version": "0.1.0",
  "private": true,
  "dependencies": {
    "@testing-library/jest-dom": "^5.16.5",
    "@testing-library/react": "^13.3.0",
    "@testing-library/user-event": "^13.5.0",
    "@types/jest": "^27.5.2",
    "@types/node": "^16.11.56",
    "@types/react": "^18.0.18",
    "@types/react-dom": "^18.0.6",
    "@types/react-modal": "^3.13.1",
    "buffer": "^6.0.3",
    "detect-browser": "^5.3.0",
    "near-api-js": "^1.0.0",
    "react": "^18.2.0",
    "react-collapse": "^5.1.1",
    "react-dom": "^18.2.0",
    "react-icons": "^4.4.0",
    "react-json-pretty": "^2.2.0",
    "react-json-tree": "^0.17.0",
    "react-modal": "^3.16.1",
    "react-parallax-tilt": "^1.7.63",
    "react-router-dom": "^6.3.0",
    "react-scripts": "5.0.1",
    "react-spinners": "^0.13.4",
    "react-treeview": "^0.4.7",
    "typescript": "^4.8.2",
    "web-vitals": "^2.1.4"
  },
  "scripts": {
    "start": "react-scripts start",
    "build": "react-scripts build",
    "test": "react-scripts test",
    "eject": "react-scripts eject"
  },
  "eslintConfig": {
    "extends": [
      "react-app",
      "react-app/jest"
    ]
  },
  "browserslist": {
    "production": [
      ">0.2%",
      "not dead",
      "not op_mini all"
    ],
    "development": [
      "last 1 chrome version",
      "last 1 firefox version",
      "last 1 safari version"
    ]
  },
  "devDependencies": {
    "@types/react-collapse": "^5.0.1"
  }
}

'''
'''--- frontend/vault/public/index.html ---
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <link rel="icon" href="%PUBLIC_URL%/favicon.ico" />
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Share+Tech">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <meta name="theme-color" content="#000000" />
    <meta
      name="description"
      content="Web site created using create-react-app"
    />
    <link rel="apple-touch-icon" href="%PUBLIC_URL%/logo192.png" />
    <!--
      manifest.json provides metadata used when your web app is installed on a
      user's mobile device or desktop. See https://developers.google.com/web/fundamentals/web-app-manifest/
    -->
    <link rel="manifest" href="%PUBLIC_URL%/manifest.json" />
    <!--
      Notice the use of %PUBLIC_URL% in the tags above.
      It will be replaced with the URL of the `public` folder during the build.
      Only files inside the `public` folder can be referenced from the HTML.

      Unlike "/favicon.ico" or "favicon.ico", "%PUBLIC_URL%/favicon.ico" will
      work correctly both with client-side routing and a non-root public URL.
      Learn how to configure a non-root public URL by running `npm run build`.
    -->
    <script src="https://cdn.jsdelivr.net/npm/near-api-js@0.44.2/dist/near-api-js.min.js" integrity="sha256-W5o4c5DRZZXMKjuL41jsaoBpE/UHMkrGvIxN9HcjNSY=" crossorigin="anonymous"></script>
    <title>NFT Vault</title>
  </head>
  <body>
    <noscript>You need to enable JavaScript to run this app.</noscript>
    <div id="root"></div>
    <!--
      This HTML file is a template.
      If you open it directly in the browser, you will see an empty page.

      You can add webfonts, meta tags, or analytics to this file.
      The build step will place the bundled scripts into the <body> tag.

      To begin the development, run `npm start` or `yarn start`.
      To create a production bundle, use `npm run build` or `yarn build`.
    -->
  </body>
</html>

'''
'''--- frontend/vault/public/manifest.json ---
{
  "short_name": "React App",
  "name": "Create React App Sample",
  "icons": [
    {
      "src": "favicon.ico",
      "sizes": "64x64 32x32 24x24 16x16",
      "type": "image/x-icon"
    },
    {
      "src": "logo192.png",
      "type": "image/png",
      "sizes": "192x192"
    },
    {
      "src": "logo512.png",
      "type": "image/png",
      "sizes": "512x512"
    }
  ],
  "start_url": ".",
  "display": "standalone",
  "theme_color": "#000000",
  "background_color": "#ffffff"
}

'''
'''--- frontend/vault/public/robots.txt ---
# https://www.robotstxt.org/robotstxt.html
User-agent: *
Disallow:

'''
'''--- frontend/vault/src/App.css ---
body {
    background-color: rgb(243, 243, 243);
}

.App {
    overflow: hidden;
}
'''
'''--- frontend/vault/src/Components/Layout/Vault.module.css ---
.top_bar {
    width: fit-content;
    padding-top: 20px;
    margin: auto;
}
.top_bar > img {
    display: block;
}

.content {
    margin-top: 0px;
}

@media only screen and (min-width: 1250px) {
    .top_bar {
        margin-left: 30px;
        margin-bottom: 50px;
    }
}
'''
'''--- frontend/vault/src/Components/Media/Media.module.css ---
.media {
    max-width: 400px;
    min-height: 500px;
    max-height: 70vh;
    border-radius: 5px;
    box-shadow: 3px 3px 5px rgba(0, 0, 0, 0.3),
                -3px -3px 5px rgba(255, 255, 255, 0.05);
    border-radius: 20px;
}

.loader {
    margin: auto;
}

.hidden {
    height: 0;
    width: 0;
    padding: 0;
    margin: 0;
}

.media_container {
    text-align: center;
}
'''
'''--- frontend/vault/src/Libraries/Near/connection.ts ---
import * as nearAPI from 'near-api-js';

let _near: nearAPI.Near

function GetConfig(network: "mainnet" | "testnet"): nearAPI.ConnectConfig {
    switch(network) {
        case "mainnet":
            return {
                networkId: network,
                nodeUrl: 'https://rpc.mainnet.near.org',
                keyStore: new nearAPI.keyStores.BrowserLocalStorageKeyStore(),
                walletUrl: 'https://app.mynearwallet.com',
                headers: {}

            };
        case "testnet":
            return {
                networkId: network,
                nodeUrl: 'https://rpc.testnet.near.org',
                keyStore: new nearAPI.keyStores.BrowserLocalStorageKeyStore(),
                walletUrl: 'https://testnet.mynearwallet.com',
                headers: {}
            };
        default:
            throw new Error(`"Unsupported network ${network}`);
    }
}

function GetConfigInMemory(network: "mainnet" | "testnet"): nearAPI.ConnectConfig {
    switch(network) {
        case "mainnet":
            return {
                networkId: network,
                nodeUrl: 'https://rpc.mainnet.near.org',
                keyStore: new nearAPI.keyStores.InMemoryKeyStore(),
                walletUrl: 'https://app.mynearwallet.com',
                headers: {}

            };
        case "testnet":
            return {
                networkId: network,
                nodeUrl: 'https://rpc.testnet.near.org',
                keyStore: new nearAPI.keyStores.InMemoryKeyStore(),
                walletUrl: 'https://testnet.mynearwallet.com',
                headers: {}
            };
        default:
            throw new Error(`"Unsupported network ${network}`);
    }
}

async function GetConnection(config: nearAPI.ConnectConfig | undefined = undefined): Promise<nearAPI.Near> {
    if (_near) {
        return _near;
    }
    if (config === undefined) {
        throw Error('near config is undefined');
    }
    return _near = await nearAPI.connect(config);  
}

export {
    GetConfig,
    GetConnection,
    GetConfigInMemory
}
'''
'''--- frontend/vault/src/Libraries/Near/nft.ts ---
import * as nearAPI from "near-api-js";
import { GetConfig, GetConnection } from "./connection";

let _network = process.env.REACT_APP_NEAR_NETWORK ?? "testnet";
type NFTToken = {
    token_id: string,
    owner_id: string,
    metadata: {
        title: string | null,
        description: string | null,
        media: string | null,
        media_hash: string | null,
        copies: number | null,
        issued_at: number | null,
        expires_at: number | null,
        starts_at: number | null,
        updated_at: number | null,
        extra: string | null,
        reference: string | null,
        reference_hash: string | null,
    }
}

type NFTContractMetadata = {
    spec: string,
    name: string,
    symbol: string,
    icon: string | null,
    base_uri: string | null,
    reference: string | null,
    reference_hash: string | null
}

interface NFTContract extends nearAPI.Contract {
    nft_token: (args: {
        token_id: string
    }) => Promise<NFTToken | null>,
    nft_metadata: () => Promise<NFTContractMetadata>,
}

async function GetNFTContract(nft: string): Promise<NFTContract> {
    let near = await GetConnection(GetConfig(_network as any));
    let account = new nearAPI.Account(near.connection, nft);

    return new nearAPI.Contract(account, nft, {
        viewMethods: ['nft_token', 'nft_metadata'],
        changeMethods: []
    }) as NFTContract;
}

export {
    GetNFTContract
};

export type {
    NFTToken,
    NFTContractMetadata
};

'''
'''--- frontend/vault/src/Libraries/Near/vault.ts ---
import * as nearAPI from "near-api-js";
import { GetConfig, GetConnection } from "./connection";

type ClaimDetails = {
    VaultContract: string,
    NFTContract: string,
    PrivateKey: string,
    TokenId: string,
}

type Claimable = {
    token_id: string,
    nft_account_id: string,
    public_key: String,
}

type ClaimChallenge = {
    token_id: string,
    nft_account_id: string,
    timestamp_millis: number,
    owner_id: string,
}

const contractId = process.env.REACT_APP_VAULT_CONTRACT ?? "vault.world-triathlon.testnet";

const _networkId = process.env.REACT_APP_NEAR_NETWORK ?? "testnet";

interface VaultContract extends nearAPI.Contract {
    is_claimable: (args: {
        claim_token: string
    }) => Promise<Claimable | null>,
    get_claimable: (args: {
        nft_account: string,
        token_id: string
    }) => Promise<Claimable | null>,
    claim: (arg: {
        callbackUrl: string,
        args: {
            receiver_id: string,
            claimable_id: string
        }, gas: string
    }) => Promise<boolean> | Promise<void>
}

function GetVaultContract(account: nearAPI.Account): VaultContract {
    return new nearAPI.Contract(account, contractId, {
        viewMethods: ['is_claimable, get_claimable'],
        changeMethods: ['claim']
    }) as VaultContract;
}

async function GetVaultContractAnonAsync(): Promise<VaultContract> {
    let near = await GetConnection(GetConfig(_networkId as any));
    let account = new nearAPI.Account(near.connection, contractId);
    return new nearAPI.Contract(account, contractId, {
        viewMethods: ['is_claimable', 'get_claimable'],
        changeMethods: []
    }) as VaultContract;
}

export {
    GetVaultContract,
    GetVaultContractAnonAsync
}

export type {
    ClaimDetails,
    Claimable,
    ClaimChallenge
}

'''
'''--- frontend/vault/src/Providers/Wallet.ts ---
import { GetConfig, GetConnection } from "../Libraries/Near/connection";
import * as nearAPI from "near-api-js";
import { useState, Dispatch, SetStateAction, useEffect } from "react";

console.log(process.env.REACT_APP_NEAR_NETWORK);
let _network = process.env.REACT_APP_NEAR_NETWORK ?? 'testnet';
let _config = GetConfig(_network as any);
let _near: nearAPI.Near;
let _wallet: nearAPI.WalletConnection;
let _isLoggedInSetters: Map<number, Dispatch<SetStateAction<boolean>>> = new Map();
let _isLoggedInSettersId = 0;

async function Login(): Promise<void> {
    await initNear();
    if (_wallet.isSignedIn()) {
        notifyIsLoggedInSetters(true);
        return;
    }
    
    return _wallet.requestSignIn({});
}

async function Logout(): Promise<void> {
    await initNear();
    _wallet.signOut();
    notifyIsLoggedInSetters(false);
}

async function IsLoggedIn(): Promise<boolean> {
    await initNear();

    let isSignedIn = _wallet.isSignedIn();
    notifyIsLoggedInSetters(isSignedIn);
    return isSignedIn;
}

function GetWallet(): nearAPI.WalletConnection {
    return _wallet;
}

async function initNear(): Promise<void> {
    if (!_near) {
        _near = await GetConnection(_config);
        _wallet = new nearAPI.WalletConnection(_near, '');
    }
}

function useIsLoggedInHook(): boolean {
    const [isLoggedIn, setIsLoggedIn] = useState<boolean>(false);
    useEffect(() => {
        let id = _isLoggedInSettersId++;
        _isLoggedInSetters.set(id, setIsLoggedIn);
        IsLoggedIn();
        return () => {
            _isLoggedInSetters.delete(id);
        }
    }, []);
    return isLoggedIn;
}

function notifyIsLoggedInSetters(isLoggedIn: boolean) {
    _isLoggedInSetters.forEach((setter) => {
        setter(isLoggedIn);
    })
}

export {
    Login,
    Logout,
    IsLoggedIn,
    GetWallet,
    useIsLoggedInHook
}
'''
'''--- frontend/vault/src/Routes/ClaimNFT.module.css ---
.main_container {
    width: 100vw;
    height: auto;
    margin: auto;
    text-align: center;
    margin-bottom: 2em;
}

.greetings {
    color: #FFFFFF;
    max-width: 400px;
    margin: auto;
    margin-top: 20px;
    text-align: center;
    padding: 10px;
}

.greetings > p {
    font-size: 1.2em;
}

.greetings > p.full_name {
    margin: 0;
    font-size: 2em;
}

.hidden {
    display: none;
}

.card {
    width: fit-content;
    margin: auto;
}

.card:hover {
    cursor: pointer;
}

.card_header {
    padding: 10px;
    color: rgb(59, 59, 59);
    text-align: center
}

.nft_name {
    font-weight: bold;
    font-size: 18px;
}

.nft_title {
    padding: 5px;
    font-size: 16px;
}

.card_body {
    padding: 10px;
    text-align: left;
}

.card_footer {
    padding: 10px;
    margin: auto;
    text-align: center;
    color: rgb(116, 116, 116);
}

.card > .button_container {
    position: absolute;
    left: 50%;
    transform: translateX(-50%);
    bottom: 0;
    width: 100%;
}

.button {
    margin: 15px;
    border: none;
    padding: 5px 20px;
    font-weight: bold;
    box-shadow: 2px 2px 2px rgba(0, 0, 0, 0.151);
    background-color: rgb(155, 235, 183);
    color: rgb(42, 73, 220);
    border-radius: 25px;
    font-size: 18px;
}

.button:hover {
    cursor: pointer;
    box-shadow: none;
    background-color: rgb(96, 234, 144);
}

.button[disabled] {
    background-color: rgb(105, 105, 105);
}

.loader_container {
    text-align: center;
    margin-top: 300px;
}

.create_wallet_container {
    margin-top: 20px;
}

.create_wallet_container > div {
    font-size: 15px;
}

.create_wallet_button {
    margin: 15px;
    border: solid 4px rgb(0, 98, 190);
    padding: 10px 30px;
    width: 60%;
    box-shadow: 2px 2px 2px rgba(0, 0, 0, 0.151);
    background-color: rgb(240, 240, 240);
    color: rgb(49, 49, 49);
    border-radius: 3px;
    font-size: 18px;
}

.create_wallet_button:hover {
    cursor: pointer;
    box-shadow: none;
    border-color: rgb(2, 86, 165);
}

.confirm_modal {
    margin: 0;
    background: rgb(154,234,183);
    background: -moz-linear-gradient(315deg, rgba(154,234,183,1) 0%, rgba(46,79,219,1) 100%);
    background: -webkit-linear-gradient(315deg, rgba(154,234,183,1) 0%, rgba(46,79,219,1) 100%);
    background: linear-gradient(315deg, rgba(154,234,183,1) 0%, rgba(46,79,219,1) 100%);
    filter: progid:DXImageTransform.Microsoft.gradient(startColorstr="#9aeab7",endColorstr="#2e4fdb",GradientType=1);
    padding: 10px;
    padding-top: 2em;
    width: 340px;
}

.confirm_modal > .address_input_container {
    background-color: white;
    padding: 10px;
    border-radius: 5px;
    box-shadow: 3px 3px 3px rgba(0,0,0,0.1);
    margin-bottom: 5px;
    text-align: left;
    display: flex;
}

.confirm_modal > .address_input_container > span {
    font-weight: bold;
    margin-right: 5px;
}

.confirm_modal > .address_input_container > .address_input {
    width: auto;
    flex-grow: 1;
    outline: none;
    border-style: none;
    color: rgb(81, 81, 81);
}

.confirm_modal > .address_error_message {
    color: rgb(144, 0, 0);
    text-align: left;
    font-size: 14px;
    margin: 0;
}

.confirm_modal > .create_wallet_message {
    margin-bottom: 20px;
    color: white;
    text-align: left;
    font-size: 1.0em;
}

.confirm_modal > .create_wallet_message > span a {
    font-weight: bold;
}

.confirm_modal div {
    text-align: center;
    margin: auto;
    margin-top: 10px;
    margin-bottom: 10px;
}

.confirm_modal > div > button {
    padding: 5px 20px;
    border: none;
    box-shadow: 3px 3px 3px rgba(0,0,0,0.1);
    font-weight: bold;
    color: rgb(42, 73, 220);
    border-radius: 25px;
    font-weight: bold;
    font-size: 18px;
}

.confirm_modal > div > button:hover {
    box-shadow: none;
    cursor: pointer;
}

.confirm_modal > div > .proceed_btn {
    margin: 10px;
    border: none;
    box-shadow: 2px 2px 2px rgba(0, 0, 0, 0.151);
    background-color: rgb(155, 235, 183);
    
}

.confirm_modal > div > .proceed_btn:disabled {
    background-color: rgb(208, 208, 208);
    box-shadow: none;
}

.confirm_modal > div > .proceed_btn:hover {
    background-color: rgb(107, 255, 107);
}

.confirm_modal > div > .cancel_btn {
    
}

.confirm_modal > div > .cancel_btn:hover{
    background-color: rgb(210, 210, 210);
}

.success_message {
    padding: 1em;
    color: white;
    background: rgb(154,234,183);
    background: -moz-linear-gradient(315deg, rgba(154,234,183,1) 0%, rgba(46,79,219,1) 100%);
    background: -webkit-linear-gradient(315deg, rgba(154,234,183,1) 0%, rgba(46,79,219,1) 100%);
    background: linear-gradient(315deg, rgba(154,234,183,1) 0%, rgba(46,79,219,1) 100%);
    filter: progid:DXImageTransform.Microsoft.gradient(startColorstr="#9aeab7",endColorstr="#2e4fdb",GradientType=1);
    text-align: center;
}

.success_message > div {
    font-size: 20px;
    min-width: 340px;
}

.success_message > button {
    margin-top: 10px;
    padding: 5px 20px;
    border: none;
    box-shadow: 3px 3px 3px rgba(0,0,0,0.1);
    font-weight: bold;
    color: rgb(42, 73, 220);
    border-radius: 25px;
    font-weight: bold;
    font-size: 18px;
    box-shadow: 2px 2px 2px rgba(0, 0, 0, 0.151);
    background-color: rgb(155, 235, 183);
}

.success_message > button:hover {
    cursor: pointer;
    box-shadow: none;
}

@media only screen and (min-width: 1250px) {
    .greetings {
        float: left;
        margin-left: 20px;
        text-align: left;
    }

    .confirm_modal {
        padding: 50px;
        width: 400px;
    }

    .confirm_modal > .create_wallet_message {
        font-size: 1.1em;
    }
}
'''
'''--- frontend/vault/src/Routes/NotFound.module.css ---
.message {
    margin: auto;
    text-align: center;
    font-size: 100px;
    color: rgb(95, 95, 95);
    position: absolute;
    top: 50%;
    left: 50%;
    transform: translate(-50%, -50%);
}

'''
'''--- frontend/vault/src/Routes/ViewNFT.module.css ---
.main_container {
    width: fit-content;
    margin: auto;
    text-align: center;
    margin-top: 7em;
}

.card {
    width: 400px;
    overflow: hidden;
    margin: auto;
    margin-top: 1em;
    background-color: rgb(243, 243, 243);
    padding: 10px;
    border-radius: 20px;
    box-shadow: 3px 3px 10px rgba(27, 27, 27, 0.096);
}
.card:hover {
    cursor: pointer;
    background-color: rgb(238, 238, 238);
}

.card_header {
    padding: 10px;
    color: rgb(59, 59, 59);
    text-align: center
}

.card_title {
    font-weight: bold;
    font-size: 18px;
}

.card_subtitle {
    padding: 5px;
    font-size: 16px;
}

.card_body {
    padding: 10px;
    text-align: left;
}

.card_footer {
    padding: 10px;
    margin: auto;
    text-align: center;
    color: rgb(116, 116, 116);
}

.button {
    margin: 15px;
    border: none;
    padding: 10px 30px;
    width: 60%;
    font-weight: bold;
    box-shadow: 2px 2px 2px rgba(0, 0, 0, 0.151);
    background-color: rgb(0, 98, 190);
    color: rgb(245, 245, 245);
    border-radius: 3px;
    font-size: 18px;
}

.button:hover {
    cursor: pointer;
    box-shadow: none;
    background-color: rgb(2, 86, 165);
}

.button[disabled] {
    background-color: rgb(105, 105, 105);
}

.loader_container {
    text-align: center;
    margin-top: 300px;
}

.metadata_container > pre {
    overflow-x: scroll;
    padding: 15px;
    border-radius: 5px;
}

.metadata_container > pre::-webkit-scrollbar {
    height: 10px;
}

.metadata_container > pre::-webkit-scrollbar-track {
    background-color: rgb(160, 160, 160);
    
}

.metadata_container > pre::-webkit-scrollbar-thumb {
    background: rgb(85, 85, 85);
    border-radius: 10px;
}

.metadata_container > pre::-webkit-scrollbar-thumb:hover {
    background: rgb(58, 58, 58);
}
'''
'''--- frontend/vault/src/index.css ---
body {
  margin: 0;
  padding: 0;
  font-family: "Share Tech", "Share Tech Mono", sans-serif, -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Oxygen',
    'Ubuntu', 'Cantarell', 'Fira Sans', 'Droid Sans', 'Helvetica Neue';
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;

  background: rgb(154,234,183);
  background: -moz-linear-gradient(315deg, rgba(154,234,183,1) 0%, rgba(46,79,219,1) 100%);
  background: -webkit-linear-gradient(315deg, rgba(154,234,183,1) 0%, rgba(46,79,219,1) 100%);
  background: linear-gradient(315deg, rgba(154,234,183,1) 0%, rgba(46,79,219,1) 100%);
  filter: progid:DXImageTransform.Microsoft.gradient(startColorstr="#9aeab7",endColorstr="#2e4fdb",GradientType=1);
  min-height: 100vh;
}

code {
  font-family: source-code-pro, Menlo, Monaco, Consolas, 'Courier New',
    monospace;
}

.globe-background {
  position: fixed;
  bottom: -300px;
  right: -300px;
  z-index: -100;
  width: 600px;
}

'''
'''--- frontend/vault/src/logo.svg ---
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 841.9 595.3"><g fill="#61DAFB"><path d="M666.3 296.5c0-32.5-40.7-63.3-103.1-82.4 14.4-63.6 8-114.2-20.2-130.4-6.5-3.8-14.1-5.6-22.4-5.6v22.3c4.6 0 8.3.9 11.4 2.6 13.6 7.8 19.5 37.5 14.9 75.7-1.1 9.4-2.9 19.3-5.1 29.4-19.6-4.8-41-8.5-63.5-10.9-13.5-18.5-27.5-35.3-41.6-50 32.6-30.3 63.2-46.9 84-46.9V78c-27.5 0-63.5 19.6-99.9 53.6-36.4-33.8-72.4-53.2-99.9-53.2v22.3c20.7 0 51.4 16.5 84 46.6-14 14.7-28 31.4-41.3 49.9-22.6 2.4-44 6.1-63.6 11-2.3-10-4-19.7-5.2-29-4.7-38.2 1.1-67.9 14.6-75.8 3-1.8 6.9-2.6 11.5-2.6V78.5c-8.4 0-16 1.8-22.6 5.6-28.1 16.2-34.4 66.7-19.9 130.1-62.2 19.2-102.7 49.9-102.7 82.3 0 32.5 40.7 63.3 103.1 82.4-14.4 63.6-8 114.2 20.2 130.4 6.5 3.8 14.1 5.6 22.5 5.6 27.5 0 63.5-19.6 99.9-53.6 36.4 33.8 72.4 53.2 99.9 53.2 8.4 0 16-1.8 22.6-5.6 28.1-16.2 34.4-66.7 19.9-130.1 62-19.1 102.5-49.9 102.5-82.3zm-130.2-66.7c-3.7 12.9-8.3 26.2-13.5 39.5-4.1-8-8.4-16-13.1-24-4.6-8-9.5-15.8-14.4-23.4 14.2 2.1 27.9 4.7 41 7.9zm-45.8 106.5c-7.8 13.5-15.8 26.3-24.1 38.2-14.9 1.3-30 2-45.2 2-15.1 0-30.2-.7-45-1.9-8.3-11.9-16.4-24.6-24.2-38-7.6-13.1-14.5-26.4-20.8-39.8 6.2-13.4 13.2-26.8 20.7-39.9 7.8-13.5 15.8-26.3 24.1-38.2 14.9-1.3 30-2 45.2-2 15.1 0 30.2.7 45 1.9 8.3 11.9 16.4 24.6 24.2 38 7.6 13.1 14.5 26.4 20.8 39.8-6.3 13.4-13.2 26.8-20.7 39.9zm32.3-13c5.4 13.4 10 26.8 13.8 39.8-13.1 3.2-26.9 5.9-41.2 8 4.9-7.7 9.8-15.6 14.4-23.7 4.6-8 8.9-16.1 13-24.1zM421.2 430c-9.3-9.6-18.6-20.3-27.8-32 9 .4 18.2.7 27.5.7 9.4 0 18.7-.2 27.8-.7-9 11.7-18.3 22.4-27.5 32zm-74.4-58.9c-14.2-2.1-27.9-4.7-41-7.9 3.7-12.9 8.3-26.2 13.5-39.5 4.1 8 8.4 16 13.1 24 4.7 8 9.5 15.8 14.4 23.4zM420.7 163c9.3 9.6 18.6 20.3 27.8 32-9-.4-18.2-.7-27.5-.7-9.4 0-18.7.2-27.8.7 9-11.7 18.3-22.4 27.5-32zm-74 58.9c-4.9 7.7-9.8 15.6-14.4 23.7-4.6 8-8.9 16-13 24-5.4-13.4-10-26.8-13.8-39.8 13.1-3.1 26.9-5.8 41.2-7.9zm-90.5 125.2c-35.4-15.1-58.3-34.9-58.3-50.6 0-15.7 22.9-35.6 58.3-50.6 8.6-3.7 18-7 27.7-10.1 5.7 19.6 13.2 40 22.5 60.9-9.2 20.8-16.6 41.1-22.2 60.6-9.9-3.1-19.3-6.5-28-10.2zM310 490c-13.6-7.8-19.5-37.5-14.9-75.7 1.1-9.4 2.9-19.3 5.1-29.4 19.6 4.8 41 8.5 63.5 10.9 13.5 18.5 27.5 35.3 41.6 50-32.6 30.3-63.2 46.9-84 46.9-4.5-.1-8.3-1-11.3-2.7zm237.2-76.2c4.7 38.2-1.1 67.9-14.6 75.8-3 1.8-6.9 2.6-11.5 2.6-20.7 0-51.4-16.5-84-46.6 14-14.7 28-31.4 41.3-49.9 22.6-2.4 44-6.1 63.6-11 2.3 10.1 4.1 19.8 5.2 29.1zm38.5-66.7c-8.6 3.7-18 7-27.7 10.1-5.7-19.6-13.2-40-22.5-60.9 9.2-20.8 16.6-41.1 22.2-60.6 9.9 3.1 19.3 6.5 28.1 10.2 35.4 15.1 58.3 34.9 58.3 50.6-.1 15.7-23 35.6-58.4 50.6zM320.8 78.4z"/><circle cx="420.9" cy="296.5" r="45.7"/><path d="M520.5 78.1z"/></g></svg>
'''
'''--- frontend/vault/src/react-app-env.d.ts ---
/// <reference types="react-scripts" />

'''
'''--- frontend/vault/src/reportWebVitals.ts ---
import { ReportHandler } from 'web-vitals';

const reportWebVitals = (onPerfEntry?: ReportHandler) => {
  if (onPerfEntry && onPerfEntry instanceof Function) {
    import('web-vitals').then(({ getCLS, getFID, getFCP, getLCP, getTTFB }) => {
      getCLS(onPerfEntry);
      getFID(onPerfEntry);
      getFCP(onPerfEntry);
      getLCP(onPerfEntry);
      getTTFB(onPerfEntry);
    });
  }
};

export default reportWebVitals;

'''
'''--- frontend/vault/src/setupTests.ts ---
// jest-dom adds custom jest matchers for asserting on DOM nodes.
// allows you to do things like:
// expect(element).toHaveTextContent(/react/i)
// learn more: https://github.com/testing-library/jest-dom
import '@testing-library/jest-dom';

'''
'''--- frontend/vault/tsconfig.json ---
{
  "compilerOptions": {
    "target": "es5",
    "lib": [
      "dom",
      "dom.iterable",
      "esnext"
    ],
    "allowJs": true,
    "skipLibCheck": true,
    "esModuleInterop": true,
    "allowSyntheticDefaultImports": true,
    "strict": true,
    "forceConsistentCasingInFileNames": true,
    "noFallthroughCasesInSwitch": true,
    "module": "esnext",
    "moduleResolution": "node",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "noEmit": true,
    "jsx": "react-jsx"
  },
  "include": [
    "src"
  ]
}

'''
'''--- jobs/arweave_processor/.mocharc.json ---
{
    "extension": [
        "ts"
    ],
    "spec": "*/*.spec.ts",
    "require": "ts-node/esm",
    "node-option": [
        "loader=ts-node/esm",
        "experimental-specifier-resolution=node"
    ]
}
'''
'''--- jobs/arweave_processor/README.md ---
# Arweave Processor

Service responsible for uploading media to the permaweb.

## Environment Configuration
| Key | Description |
| :-- | :---------- |
| `AZURE_ACCOUNT_NAME` | **REQUIRED**. azure account name |
| `AZURE_ACCOUNT_KEY` | **REQUIRED**. azure secret key |
| `TOPIC` | **REQUIRED**. name of the topic |
| `WALLET_JSON` | **REQUIRED**. wallet to use, jwk in json string |
| `MIN_CONFIRMATIONS` | default number of confirmations before considering the arweave transaction to be successful |
| `DEFAULT_CALLBACK_URL` | **REQUIRED**. default callback url that will receive the emitted events |
| `MAX_JOBS` | maximum number of jobs to be processed at the same time, setting to 0 will have no limit |
| `LOG_LEVEL` | [winston log level](https://www.npmjs.com/package/winston#logging-levels)|

## Job Spec

The service will listen for messages on a pubsub queue, the message should be a valid json data that conforms with the format below.

Example Message from Queue
```json
{
    "JobId": "ff975cbd-32f4-4f09-9b9a-01964dd6eb90",
    "MediaURL":
    "https://nftdesignworks.blob.core.windows.net/mintedimages/429538da-ecfa-454f-9fe
d-b13b8b1de043.png",
    "Metadata": {
        "AthleteId": "128073",
        "FirstName": "Artūrs",
        "LastName": "Liepa",
        "Country": "LAT",
        ... REDACTED
    },
    "MinConfirmations": 15
}
```

| Field | Type | Description |
| :---- | :--- | :---------- |
| `JobId` | `string` | **Required**.  A unique string generated by the publisher of the message |
| `MediaURL` | `string` | **Required**. Url of the media to be uploaded to the permaweb |
| `Metadata` | `object` | **Required**. nft metadata, should be deserialiazable into a json string |
| `MinConfirmations` | `number` | **Optional**. the number of confirmations from the network to consider the transaction as successful, defaults to **15** |

## Events

the processor will emit events and send it to the configured `DEFAULT_CALLBACK_URL` if `CallbackURL` is not specified on jobs. 

### Started Event
Emmited to notify the callback url that the processor has received the job and is currently processing it.
```json
{
    "JobId": "ff975cbd-32f4-4f09-9b9a-01964dd6eb90",
    "Event": "started",
    "Time": 1659965535,
    "Message": "Job Started"
}
```
### Failure Event
Emmited to notify the callback url that an error has occured.
```json
{
    "JobId": "ff975cbd-32f4-4f09-9b9a-01964dd6eb90",
    "Event": "failure",
    "Time": 1659965535,
    "Message": "Example Error Message"
}

```
### Success Event
Emmited to notify the callback url that the job has successfully finished.
```json
{
    "JobId": "ff975cbd-32f4-4f09-9b9a-01964dd6eb90",
    "Event": "success",
    "Time": 1659965535,
    "Message": "Media has been successfully uploaded to the permaweb",
    "Details": {
        "TransactionID": "<ARWEAVE_TX_ID>",
        "Confirmations": 15
    }
}
```
## Process Flow
![](../../docs/assets/arweave_processor/arweave-flow.png)

'''
'''--- jobs/arweave_processor/bin/callback_server.ts ---
import { createServer, } from "http";
import { Logger } from "../src/lib/logger.js";

createServer((req, res) => {
    req.on('data', (data) => {
        Logger().info(data)
    });

    res.write("200")
    res.end();
}).listen(8080);
'''
'''--- jobs/arweave_processor/bin/storage_queue_clear.ts ---
import { GetConfig, LoadConfig } from "../src/config.js";
import {
    QueueServiceClient,
    StorageSharedKeyCredential
} from "@azure/storage-queue";
import { Logger } from "../src/lib/logger.js";

LoadConfig();

let config = GetConfig();

(async () => {
    let qsClient = new QueueServiceClient(`https://${config.AzureAccountName}.queue.core.windows.net`, new StorageSharedKeyCredential(config.AzureAccountName, config.AzureAccountKey));
    let qClient = qsClient.getQueueClient(config.Topic);

    await qClient.createIfNotExists();
    let response = await qClient.clearMessages();
    Logger().info(response);
    
    
})();

'''
'''--- jobs/arweave_processor/bin/storage_queue_dispatch.ts ---
import { GetConfig, LoadConfig } from "../src/config.js";
import {
    QueueServiceClient,
    StorageSharedKeyCredential
} from "@azure/storage-queue";
import {
    randomUUID
} from 'crypto';
import { Logger } from "../src/lib/logger.js";

LoadConfig();

let config = GetConfig();

(async () => {
    let qsClient = new QueueServiceClient(`https://${config.AzureAccountName}.queue.core.windows.net`, new StorageSharedKeyCredential(config.AzureAccountName, config.AzureAccountKey));
    let qClient = qsClient.getQueueClient(config.Topic);

    //let mediaURL = 'https://assets.entrepreneur.com/content/3x2/2000/1647397792-nft-art2.jpg';
    let mediaURL = 'https://2upyoaui2v5jgeozo2curva64y5yp7aplcq6zw3f5kvo7xw7uy3q.arweave.net/1R-HAojVepMR2XaFSNQe5juH_A9YoezbZeqq797fpjc';
    //let mediaURL = 'https://file-examples.com/storage/fe6a5406fa63112369b75a2/2017/04/file_example_MP4_480_1_5MG.mp4'
    await qClient.createIfNotExists();
    //let uuid = randomUUID();
    let uuid = '02bc722d-f790-4d1c-aab6-a3394921c638';
    let metadata = { "AthleteId": "106181", "FirstName": "Artjoms", "LastName": "Gajevskis", "Country": "LAT", "Status": "", "StartNumber": "5", "Position": 3, "TotalTime": "00:28:32", "Timings": [{ "Key": "Swim", "Value": "00:04:58" }, { "Key": "T1", "Value": "00:01:00" }, { "Key": "Bike", "Value": "00:14:29" }, { "Key": "T2", "Value": "00:00:29" }, { "Key": "Run", "Value": "00:07:36" }] };
    let response = await qClient.sendMessage(JSON.stringify(
        { "JobId": uuid, "MediaURL": mediaURL, "Metadata": metadata }
    ));

    Logger().info(response.messageId);
})();

'''
'''--- jobs/arweave_processor/bin/storage_queue_peek.ts ---
import { GetConfig, LoadConfig } from "../src/config.js";
import {
    QueueServiceClient,
    StorageSharedKeyCredential
} from "@azure/storage-queue";
import { Logger } from "../src/lib/logger.js";

LoadConfig();

let config = GetConfig();

(async () => {
    let qsClient = new QueueServiceClient(`https://${config.AzureAccountName}.queue.core.windows.net`, new StorageSharedKeyCredential(config.AzureAccountName, config.AzureAccountKey));
    let qClient = qsClient.getQueueClient(config.Topic);

    await qClient.createIfNotExists();
    let response = await qClient.peekMessages({
        numberOfMessages: 32
    });
    
    for (const item of response.peekedMessageItems) {
        Logger().info(JSON.stringify(item));
    }
})();

'''
'''--- jobs/arweave_processor/package.json ---
{
  "name": "arweave_processor",
  "version": "1.0.0",
  "main": "src/index.ts",
  "type": "module",
  "license": "MIT",
  "devDependencies": {
    "@types/mocha": "^10.0.0",
    "@types/node": "^18.6.5",
    "mocha": "^10.0.0",
    "nodemon": "^2.0.19",
    "ts-node": "^10.9.1",
    "typescript": "^4.8.4"
  },
  "scripts": {
    "dev": "ts-node --esm src/index.ts",
    "dev:watch": "nodemon -w src -e ts,js,json --exec 'yarn run dev'",
    "test": "yarn mocha",
    "test:watch": "nodemon -w src -w tests -e ts,js,json --exec 'yarn run test'",
    "build": "yarn clean && tsc -p tsconfig.build.json",
    "clean": "rm -rf ./dist",
    "dispatch:storage-queue": "ts-node --esm bin/storage_queue_dispatch.ts",
    "peek:storage-queue": "ts-node --esm bin/storage_queue_peek.ts",
    "clear:storage-queue": "ts-node --esm bin/storage_queue_clear.ts",
    "callback-server": "ts-node --esm bin/callback_server.ts"
  },
  "dependencies": {
    "@azure/storage-queue": "^12.10.0",
    "arbundles": "^0.6.21",
    "arweave": "^1.11.4",
    "axios": "^0.27.2",
    "dotenv": "^16.0.1",
    "file-type": "^17.1.6",
    "winston": "^3.8.1"
  }
}

'''
'''--- jobs/arweave_processor/src/config.ts ---
import { JWKInterface } from 'arweave/node/lib/wallet.js';
import env from 'dotenv';
import { Logger } from './lib/logger.js';

type Config = {
    AzureAccountName: string,
    AzureAccountKey: string,
    Topic: string,
    Wallet: JWKInterface,
    MinimumConfirmations: number,
    DefaultCallbackURL: string,
    MaxJobs: number,
}

let config: Config;
env.config();

function LoadConfig() {
    let wallet = {} as JWKInterface;
    if (!process.env.DEFAULT_CALLBACK_URL) {
        Logger().error("DEFAULT_CALLBACK_URL is a required environment variable")
        process.exit(1);
    }
    if (!process.env.TOPIC) {
        Logger().error("TOPIC is a required environment variable");
        process.exit(1);
    }
    if (!process.env.AZURE_ACCOUNT_NAME) {
        Logger().error("AZURE_ACCOUNT_NAME is a required environment variable");
        process.exit(1);
    }
    if (!process.env.AZURE_ACCOUNT_KEY) {
        Logger().error("AZURE_ACCOUNT_KEY is a required environment variable");
        process.exit(1);
    }
    try {
        wallet = JSON.parse(process.env.WALLET_JSON as string);
    } catch (e) {
        Logger().error(`Failed to load WALLET_JSON, make sure that it is a valid json string and a valid arweave wallet`);
        process.exit(1);
    }

    try {
        config = {
            AzureAccountName: process.env.AZURE_ACCOUNT_NAME,
            AzureAccountKey: process.env.AZURE_ACCOUNT_KEY,
            Topic: process.env.TOPIC,
            Wallet: wallet,
            MinimumConfirmations: process.env.MIN_CONFIRMATIONS ? parseInt(process.env.MIN_CONFIRMATIONS) : 15,
            DefaultCallbackURL: process.env.DEFAULT_CALLBACK_URL,
            MaxJobs: process.env.MAX_JOBS ? parseInt(process.env.MAX_JOBS) : 0
        }
    } catch (e) {
        let err = e as Error;
        Logger().error(`Failed to load configuration, check if .env setting is correct: ${err.message}\n${err.stack ?? ''}`);
        process.exit(1);
    }

    Logger().debug(`loaded configuration:\n${JSON.stringify(config)}`);
}

function GetConfig(): Config {
    if (!config) {
        throw new Error("Config is not yet loaded, call LoadConfig() first");
    }

    return config;
}

export {
    Config,
    LoadConfig,
    GetConfig
}
'''
'''--- jobs/arweave_processor/src/core/arweave.ts ---
import Arweave from "arweave";
import { JWKInterface } from "arweave/node/lib/wallet.js";
import { fileTypeFromBuffer } from "file-type";
import { Logger } from "../lib/logger.js";
import { Sleep } from "../lib/util.js";
import {
    createData,
    bundleAndSignData,
} from "arbundles";
import { ArweaveSigner } from "arbundles/src/signing/index.js";
import { logger } from "@azure/storage-queue";

type PathManifest = {
    manifest: 'arweave/paths',
    version: '0.1.0',
    index: {
        path: string
    },
    paths: Paths
}

type Paths = {
    [path: string]: {
        id: string
    }
}

type UploadResult = {
    BundleTxID: string,
    PathManifestTxID: string,
}

let _minConfirmations: number;

function SetMinConfirmations(minConfirmations: number) {
    _minConfirmations = minConfirmations;
}

let _client: Arweave = Arweave.init({
    host: 'arweave.net',
    port: 443,
    protocol: 'https'
});

let _wallet: JWKInterface;

function SetArweaveWallet(wallet: JWKInterface) {
    _wallet = wallet;
}

async function UploadMediaToPermaweb(media: Buffer, metadata: any, jobID: string): Promise<UploadResult> {
    let signer = new ArweaveSigner(_wallet);

    let mediaFileType = await fileTypeFromBuffer(media);
    if (!mediaFileType) {
        throw new Error(`Job ${jobID}, failed to get mime of media`);
    }

    let mediaData = createData(media, signer, {
        tags: [{
            name: 'Content-Type',
            value: mediaFileType.mime
        }]
    });
    await mediaData.sign(signer);

    let metadataData = createData(JSON.stringify(metadata), signer, {
        tags: [{
            name: 'Content-Type',
            value: 'application/json'
        }]
    });
    
    await metadataData.sign(signer);
    
    let mediaPath = `nft.${mediaFileType.ext}`

    let paths = {} as Paths;
    paths[mediaPath] = {
        id: mediaData.id
    };
    paths['metadata.json'] = {
        id: metadataData.id
    }
    let pathManifest = JSON.stringify({
        manifest: 'arweave/paths',
        version: '0.1.0',
        index: {
            path: mediaPath
        },
        paths: paths
    } as PathManifest);

    
    let pathManifestData = createData(pathManifest, signer, {
        tags: [{
            name: 'Content-Type',
            value: 'application/x.arweave-manifest+json'
        }]
    });
    await pathManifestData.sign(signer);
    
    Logger().debug(`path manifest ${pathManifestData.id}:\n${pathManifest}`)

    let bundle = await bundleAndSignData([mediaData, metadataData, pathManifestData], signer);

    let tx = await bundle.toTransaction({}, _client, _wallet);

    tx.addTag("App-Name", "NFTDesignWorks");
    if (jobID) tx.addTag("JobID", jobID);

    await _client.transactions.sign(tx, _wallet);

    let uploader = await _client.transactions.getUploader(tx);
    while (!uploader.isComplete) {
        await uploader.uploadChunk();
        Logger().debug(`Job: ${jobID}, Status; uploading ${uploader.pctComplete}%`)
    }
    
    return {
        BundleTxID: tx.id,
        PathManifestTxID: pathManifestData.id
    };
}

async function ConfirmUpload(txID: string, minConfirmations?: number): Promise<number> {
    if (minConfirmations === undefined || minConfirmations === null) {
        minConfirmations = _minConfirmations;
    }

    let currentConfirmations = 0;
    while (true) {
        let status = await _client.transactions.getStatus(txID);

        if (status.status === 429) {
            Logger().warn(`tx ${txID} Throttled by arweave api with status ${status.status}, sleeping for 6000ms`);
            await Sleep(6000);
            continue;
        }
        if (status.status < 200 || status.status > 299) {
            throw new Error(`Invalid transaction status code ${status.status} for tx ${txID}`);
        }

        if (status.confirmed) {
            if (currentConfirmations !== status.confirmed.number_of_confirmations) {
                currentConfirmations = status.confirmed.number_of_confirmations;
                Logger().info(`tx ${txID}: confirmed ${currentConfirmations} out of ${minConfirmations} confirmations`);
            }

            if (currentConfirmations >= minConfirmations) {
                Logger().info(`tx ${txID} has been confirmed with ${status.confirmed.number_of_confirmations} confirmations`);
                return currentConfirmations;
            }
        } else {
            Logger().debug(`TX: ${txID}, Status: ${status.status}`);
        }

        await Sleep(10000);
    }

}

export {
    SetArweaveWallet,
    UploadMediaToPermaweb,
    SetMinConfirmations,
    ConfirmUpload
}
'''
'''--- jobs/arweave_processor/src/core/event.ts ---
import axios from "axios";
import { Logger } from "../lib/logger.js";

let _callbackURL: string;
let _client = axios.default;

function SetDefaultCallBack(callback: string) {
    _callbackURL = callback;
}

interface Event {
    JobId: string,
    Event: "started" | "failure" | "success",
    Message: string,
    Details?: any
}

interface _Event extends Event {
    Time: number
}

type EmitResult = "ok" | "not_found" | "error";

async function Emit(event: Event): Promise<EmitResult> {
    let _event = event as _Event;
    _event.Time = new Date().getTime();
    let response = await _client.post(_callbackURL, _event);
    Logger().debug(`callback reponse: ${response.status}\n${JSON.stringify(response.data)}`);

    if ([400, "400"].includes(response.data)) {
        return "error";
    }

    if ([404, "404"].includes(response.data)) {
        return "not_found";
    }

    if (![true, "true", 200, "200"].includes(response.data)) {
        throw new Error(`Callback endpoint unexpected response ${response.data}, should be true or 200`);
    }

    return "ok";
}

export {
    Emit,
    SetDefaultCallBack,
    Event
}
'''
'''--- jobs/arweave_processor/src/core/processor.ts ---
import { Job, Payload, Queue } from '../queue/common.js';
import axios from 'axios';
import { Sleep } from '../lib/util.js';
import { ConfirmUpload, UploadMediaToPermaweb } from './arweave.js';
import { Logger } from '../lib/logger.js';
import { Emit } from './event.js';

let _queue: Queue;

/**
 * Number of jobs currently being processed
 */
let _processing: number = 0;

/**
 * Maximum number of jobs that will be processed
 */
let _maxProcessingJobs = 0;

function SetQueue(queue: Queue) {
    _queue = queue;
}

function SetMaxJobs(maxJobs: number) {
    _maxProcessingJobs = maxJobs;
}

async function Start() {
    // main processor  loop
    while (true) {
        await loop();
    }
}

async function loop() {
    if (_maxProcessingJobs > 0 && _processing >= _maxProcessingJobs) {
        Logger().debug(`throttling processing ${_processing} jobs`);
        await Sleep(5000);
        return;
    }
    let job!: Job;
    try {
        job = await _queue.getNextJob();
    } catch (e) {
        Logger().error(e);
        return;
    }

    _processing++;
    (async () => {
        await processJob(job);
    })().then(async () => {
        await job.complete();
    }).catch(async (e) => {
        await job.requeue();
        let err = e as Error;
        let err_message = `Requeing job ${job.payload.JobId}, failed due to error: ${err.message}\n${err.stack ?? ''}`;
        Logger().error(err_message, {
            log_type: 'job_failed',
            job_id: job.payload.JobId
        });
        Emit({
            JobId: job.payload.JobId,
            Event: "failure",
            Message: err_message,
            Details: {
                Error: err,
            }
        }).catch(e => {
            Logger().error(`Failed to send failure message to callback: ${e}`);
        });
    }).finally(() => {
        _processing--;
    });
}

async function processJob(job: Job) {
    let txID: string | undefined = undefined;
    let manifestTxID: string | undefined = undefined;
    let payload = job.payload;

    if (payload.State && payload.State.TxID && payload.State.PathManifestTxID) {
        txID = payload.State.TxID;
        manifestTxID = payload.State.PathManifestTxID;
    }

    if (!txID) {

        let emitResult = await Emit({
            JobId: job.payload.JobId,
            Event: "started",
            Message: `Job ${job.payload.JobId} has been started`
        });

        if (emitResult === "not_found" || emitResult === "error") {
            Logger().warn(`callback endpoint failed with emit result ${emitResult}, removing the job from queue`);
            return;
        }

        Logger().info(`Job ${job.payload.JobId} has been started`, {
            log_type: 'job_started',
            job_id: job.payload.JobId
        });

        let response = await axios.default.get<Buffer>(payload.MediaURL, {
            responseType: "arraybuffer"
        });

        if (response.status !== 200) {
            throw new Error(`Failure while trying to download media returned status: ${response.status}\n${response.data}`)
        }

        let result = await UploadMediaToPermaweb(response.data, payload.Metadata, payload.JobId);
        txID = result.BundleTxID;
        manifestTxID = result.PathManifestTxID;

        await job.setState({
            TxID: txID,
            PathManifestTxID: manifestTxID
        });
    } else {
        let emitResult = await Emit({
            JobId: job.payload.JobId,
            Event: "started",
            Message: `Job ${job.payload.JobId} has been restarted`
        });

        if (emitResult === "not_found" || emitResult === "error") {
            Logger().warn(`callback endpoint failed with emit result ${emitResult}, removing the job from queue`);
            return;
        }

        Logger().info(`Job ${job.payload.JobId} has been restarted`, {
            log_type: 'job_restarted',
            job_id: job.payload.JobId
        });
    }

    let confirmations = await ConfirmUpload(txID, payload.MinConfirmations);

    Logger().info(`Job ${payload.JobId} has been successfully processed: ${txID}`, {
        log_type: 'job_completed',
        job_id: job.payload.JobId
    });
    await Emit({
        JobId: payload.JobId,
        Event: "success",
        Message: `Job ${payload.JobId} has been successfully processed: ${txID}`,
        Details: {
            TransactionID: manifestTxID,
            Confirmations: confirmations
        }
    })
}

export {
    Start,
    SetQueue,
    SetMaxJobs
}
'''
'''--- jobs/arweave_processor/src/index.ts ---
import { SetMaxJobs, SetQueue, Start } from './core/processor.js';
import { GetConfig, LoadConfig } from './config.js';
import { CreateAzureStorageQueue } from './queue/azure_storage_queue.js';
import { SetArweaveWallet, SetMinConfirmations } from './core/arweave.js';
import { Logger } from './lib/logger.js';
import { SetDefaultCallBack } from './core/event.js';

LoadConfig();
let config = GetConfig();

// ---- CONFIGURATION ---- //
SetQueue(CreateAzureStorageQueue(config.AzureAccountName, config.AzureAccountKey, config.Topic));
SetArweaveWallet(config.Wallet);
SetMinConfirmations(config.MinimumConfirmations);
SetMaxJobs(config.MaxJobs);
SetDefaultCallBack(config.DefaultCallbackURL);
// ---- CONFIGURATION ---- //

Logger().debug('Starting Processor');
Start();
'''
'''--- jobs/arweave_processor/src/lib/logger.ts ---
import { Logger, createLogger, transports, format } from 'winston';
import { hostname } from 'os';

let _logger: Logger;
let host: string;

function GetLogger(): Logger {
    host = hostname();
    return _logger ?? InitLogger();
}

const CustomFormat = format((info, opts) => {
    if (!info.log_type) {
        info.log_type = 'default';
    }
    if (opts.service_name) {
        info.service_name = opts.service_name;
    }
    info.time = Math.round(new Date().getTime() / 1000);
    info.hostname = host;
    info.message = `${info.message}`

    return info;
});

function InitLogger(): Logger {
    _logger = createLogger({
        level: process.env.LOG_LEVEL ?? 'info',
        format: format.combine(
            CustomFormat({
                service_name: 'arweave_processor'
            }),
            format.json()
        ),
        transports: [
            new transports.Console(),
        ]
    });
    let consoleLog = console.log;
    console.log = (...data: any[]) => { _logger.warn("console.log called"); consoleLog(data) };
    return _logger;
}

export {
    GetLogger as Logger,
    InitLogger
}
'''
'''--- jobs/arweave_processor/src/lib/util.ts ---
function Sleep(ms: number): Promise<void> {
    return new Promise((resolve) => {
        setTimeout(() => {
            resolve()
        }, ms);
    });
}

export {
    Sleep
}
'''
'''--- jobs/arweave_processor/src/queue/azure_storage_queue.ts ---
import { DequeuedMessageItem, QueueServiceClient, StorageSharedKeyCredential } from "@azure/storage-queue";
import { Emit } from "../core/event.js";
import { Logger } from "../lib/logger.js";
import { Sleep } from "../lib/util.js";
import { Job, ParsePayloadFromJSONString, Payload, Queue } from "./common.js";

const STORAGE_QUEUE_POLL_INTERVAL = 3000;
const STORAGE_QUEUE_RENEW_LOCK_INTERVAL = 20000;

function CreateAzureStorageQueue(accountName: string, accountKey: string, queueName: string): Queue {
    let qsClient = new QueueServiceClient(
        `https://${accountName}.queue.core.windows.net`,
        new StorageSharedKeyCredential(accountName, accountKey)
    );

    let qClient = qsClient.getQueueClient(queueName);

    let queue: Queue = {
        async getNextJob(): Promise<Job> {

            return new Promise<Job>(async (resolve, reject) => {
                await qClient.createIfNotExists();
                let currentMessage: DequeuedMessageItem;

                do {
                    Logger().debug('fetching messages from storage queue');
                    let msgResponse = await qClient.receiveMessages({});

                    currentMessage = msgResponse.receivedMessageItems.pop() as DequeuedMessageItem;
                    if (currentMessage) {
                        Logger().info(`message received: ${currentMessage.messageId} : ${currentMessage.popReceipt}`)

                        let renewLockInterval = setInterval(async () => {
                            Logger().debug(`renewing lock for message: ${currentMessage.messageId}`)
                            let response = await qClient.updateMessage(currentMessage.messageId, currentMessage.popReceipt, undefined, 30);
                            currentMessage.popReceipt = response.popReceipt ?? currentMessage.popReceipt;
                        }, STORAGE_QUEUE_RENEW_LOCK_INTERVAL);

                        let parseResult = ParsePayloadFromJSONString(currentMessage.messageText);
                        if (parseResult.Error !== null && parseResult.Error !== undefined || parseResult.Payload === null) {
                            clearInterval(renewLockInterval);
                            Emit({
                                Event: 'failure',
                                JobId: parseResult.Payload?.JobId ?? "",
                                Message: `Failed to parse payload: ${parseResult.Error}`,
                            });
                            qClient.deleteMessage(currentMessage.messageId, currentMessage.popReceipt);
                            reject(parseResult.Error);
                            break;
                        }

                        let payload = parseResult.Payload;

                        resolve({
                            payload: payload,
                            async complete() {
                                clearInterval(renewLockInterval);
                                await qClient.deleteMessage(currentMessage.messageId, currentMessage.popReceipt);
                            },
                            async setState(newState) {
                                payload.State = newState;
                                let response = await qClient.updateMessage(currentMessage.messageId, currentMessage.popReceipt, JSON.stringify(payload), 30);
                                currentMessage.popReceipt = response.popReceipt ?? currentMessage.popReceipt;
                            },
                            async requeue() {
                                clearInterval(renewLockInterval);
                            }
                        });

                        break;
                    }
                    Logger().debug(`none received, sleeping for ${STORAGE_QUEUE_POLL_INTERVAL}ms`);
                    await Sleep(STORAGE_QUEUE_POLL_INTERVAL);
                } while (currentMessage === undefined);
            });
        },
    };

    return queue;
}

export {
    CreateAzureStorageQueue
}
'''
'''--- jobs/arweave_processor/src/queue/common.ts ---
interface Queue {
    getNextJob: () => Promise<Job>
}

interface Job {
    payload: Payload
    complete: () => Promise<void>,
    setState: (state: State) => Promise<void>,
    requeue: () => Promise<void>
}

type Payload = {
    JobId: string,
    MediaURL: string,
    Metadata: any,
    MinConfirmations?: number,
    State?: State
}

type State = {
    TxID?: string,
    PathManifestTxID?: string,
}

function ParsePayloadFromJSONString(payloadString: string): {
    Payload: Payload | null,
    Error: String | null
} {
    let payload: Payload
    try {
        payload = JSON.parse(payloadString);
    } catch (e) {
        return {
            Payload: null,
            Error: `failed to parse payload: ${(e as Error).message}`
        }
    }
    
    if (payload.JobId === null || payload.JobId === undefined) {
        return {
            Payload: payload,
            Error: "JobId is a required parameter"
        }
    }

    if (payload.Metadata === null || payload.Metadata === undefined) {
        return {
            Payload: payload,
            Error: "Metadata is a required parameter"
        };
    }

    if (payload.MediaURL === null || payload.MediaURL === undefined) {
        return {
            Payload: payload,
            Error: "MediaURL is a required parameter"
        };
    }

    try {
        new URL(payload.MediaURL);
    } catch (e) {
        return {
            Payload: payload,
            Error: `MediaURL should be a valid url: ${(e as Error).message}`
        };
    }

    return {
        Payload: payload,
        Error: null
    };
}

export {
    Queue,
    Job,
    Payload,
    ParsePayloadFromJSONString
}
'''
'''--- jobs/arweave_processor/tests/common.spec.ts ---
import { equal, notEqual } from "assert";
import { ParsePayloadFromJSONString, Payload } from "../src/queue/common.js";

describe("Payload Validation", () => {
    it("should be able to parse valid payload", () => {
        let payload = JSON.stringify({
            JobId: '1234567890',
            MediaURL: 'https://localhost/image',
            Metadata: {},
        } as Payload);

        let result = ParsePayloadFromJSONString(payload);
        equal(result.Error, null);
    });

    it("should fail on undefined JobId", () => {
        let payload = JSON.stringify({
            JobId: undefined as any,
            MediaURL: 'https://localhost/image',
            Metadata: {},
        } as Payload);
        let result = ParsePayloadFromJSONString(payload);
        equal(result.Error, "JobId is a required parameter");
    });

    it("should fail on null JobId", () => {
        let payload = JSON.stringify({
            JobId: undefined as any,
            MediaURL: 'https://localhost/image',
            Metadata: {},
        } as Payload);
        let result = ParsePayloadFromJSONString(payload);
        equal(result.Error, "JobId is a required parameter");
    });

    it("should fail on undefined MediaURL", () => {
        let payload = JSON.stringify({
            JobId: "1231232",
            MediaURL: undefined as any,
            Metadata: {},
        } as Payload);
        let result = ParsePayloadFromJSONString(payload);
        equal(result.Error, "MediaURL is a required parameter");

    });

    it("should fail on null MediaURL", () => {
        let payload = JSON.stringify({
            JobId: "1231232",
            MediaURL: null as any,
            Metadata: {},
        } as Payload);
        let result = ParsePayloadFromJSONString(payload);
        equal(result.Error, "MediaURL is a required parameter");
    });

    it("should fail on invalid MediaURL", () => {
        let payload = JSON.stringify({
            JobId: "1231232",
            MediaURL: "",
            Metadata: {},
        } as Payload);
        let result = ParsePayloadFromJSONString(payload);
        equal(result.Error, "MediaURL should be a valid url: Invalid URL");
    });

    it("should fail on undefined Metadata", () => {
        let payload = JSON.stringify({
            JobId: "1231232",
            MediaURL: "",
            Metadata: undefined,
        } as Payload);
        let result = ParsePayloadFromJSONString(payload);
        equal(result.Error, "Metadata is a required parameter");
    });

    it("should fail on null Metadata", () => {
        let payload = JSON.stringify({
            JobId: "1231232",
            MediaURL: "",
            Metadata: null,
        } as Payload);
        let result = ParsePayloadFromJSONString(payload);
        equal(result.Error, "Metadata is a required parameter");
    });

});
'''
'''--- jobs/arweave_processor/tsconfig.build.json ---
{
  "extends": "./tsconfig.json",
  "compilerOptions": {
    "rootDir": "./src"
  },
  "exclude": [
    "./bin/**/*",
    "./tests/**/*"
  ]
}
'''
'''--- jobs/arweave_processor/tsconfig.json ---
{
  "compilerOptions": {
    /* Visit https://aka.ms/tsconfig to read more about this file */
    /* Projects */
    // "incremental": true,                              /* Save .tsbuildinfo files to allow for incremental compilation of projects. */
    // "composite": true,                                /* Enable constraints that allow a TypeScript project to be used with project references. */
    // "tsBuildInfoFile": "./.tsbuildinfo",              /* Specify the path to .tsbuildinfo incremental compilation file. */
    // "disableSourceOfProjectReferenceRedirect": true,  /* Disable preferring source files instead of declaration files when referencing composite projects. */
    // "disableSolutionSearching": true,                 /* Opt a project out of multi-project reference checking when editing. */
    // "disableReferencedProjectLoad": true,             /* Reduce the number of projects loaded automatically by TypeScript. */
    /* Language and Environment */
    "target": "es2016", /* Set the JavaScript language version for emitted JavaScript and include compatible library declarations. */
    // "lib": [],                                        /* Specify a set of bundled library declaration files that describe the target runtime environment. */
    // "jsx": "preserve",                                /* Specify what JSX code is generated. */
    // "experimentalDecorators": true,                   /* Enable experimental support for TC39 stage 2 draft decorators. */
    // "emitDecoratorMetadata": true,                    /* Emit design-type metadata for decorated declarations in source files. */
    // "jsxFactory": "",                                 /* Specify the JSX factory function used when targeting React JSX emit, e.g. 'React.createElement' or 'h'. */
    // "jsxFragmentFactory": "",                         /* Specify the JSX Fragment reference used for fragments when targeting React JSX emit e.g. 'React.Fragment' or 'Fragment'. */
    // "jsxImportSource": "",                            /* Specify module specifier used to import the JSX factory functions when using 'jsx: react-jsx*'. */
    // "reactNamespace": "",                             /* Specify the object invoked for 'createElement'. This only applies when targeting 'react' JSX emit. */
    // "noLib": true,                                    /* Disable including any library files, including the default lib.d.ts. */
    // "useDefineForClassFields": true,                  /* Emit ECMAScript-standard-compliant class fields. */
    // "moduleDetection": "auto",                        /* Control what method is used to detect module-format JS files. */
    /* Modules */
    "module": "Node16", /* Specify what module code is generated. */
    "rootDir": ".",                                  /* Specify the root folder within your source files. */
    //"moduleResolution": "Node16", /* Specify how TypeScript looks up a file from a given module specifier. */
    // "baseUrl": "./",                                  /* Specify the base directory to resolve non-relative module names. */
    // "paths": {},                                      /* Specify a set of entries that re-map imports to additional lookup locations. */
    // "rootDirs": [],                                   /* Allow multiple folders to be treated as one when resolving modules. */
    // "typeRoots": [],                                  /* Specify multiple folders that act like './node_modules/@types'. */
    // "types": [],                                      /* Specify type package names to be included without being referenced in a source file. */
    // "allowUmdGlobalAccess": true,                     /* Allow accessing UMD globals from modules. */
    // "moduleSuffixes": [],                             /* List of file name suffixes to search when resolving a module. */
    // "resolveJsonModule": true,                        /* Enable importing .json files. */
    // "noResolve": true,                                /* Disallow 'import's, 'require's or '<reference>'s from expanding the number of files TypeScript should add to a project. */
    /* JavaScript Support */
    // "allowJs": true,                                  /* Allow JavaScript files to be a part of your program. Use the 'checkJS' option to get errors from these files. */
    // "checkJs": true,                                  /* Enable error reporting in type-checked JavaScript files. */
    // "maxNodeModuleJsDepth": 1,                        /* Specify the maximum folder depth used for checking JavaScript files from 'node_modules'. Only applicable with 'allowJs'. */
    /* Emit */
    // "declaration": true,                              /* Generate .d.ts files from TypeScript and JavaScript files in your project. */
    // "declarationMap": true,                           /* Create sourcemaps for d.ts files. */
    // "emitDeclarationOnly": true,                      /* Only output d.ts files and not JavaScript files. */
    // "sourceMap": true,                                /* Create source map files for emitted JavaScript files. */
    // "outFile": "./",                                  /* Specify a file that bundles all outputs into one JavaScript file. If 'declaration' is true, also designates a file that bundles all .d.ts output. */
    "outDir": "./dist", /* Specify an output folder for all emitted files. */
    // "removeComments": true,                           /* Disable emitting comments. */
    // "noEmit": true,                                   /* Disable emitting files from a compilation. */
    // "importHelpers": true,                            /* Allow importing helper functions from tslib once per project, instead of including them per-file. */
    // "importsNotUsedAsValues": "remove",               /* Specify emit/checking behavior for imports that are only used for types. */
    // "downlevelIteration": true,                       /* Emit more compliant, but verbose and less performant JavaScript for iteration. */
    // "sourceRoot": "",                                 /* Specify the root path for debuggers to find the reference source code. */
    // "mapRoot": "",                                    /* Specify the location where debugger should locate map files instead of generated locations. */
    // "inlineSourceMap": true,                          /* Include sourcemap files inside the emitted JavaScript. */
    // "inlineSources": true,                            /* Include source code in the sourcemaps inside the emitted JavaScript. */
    // "emitBOM": true,                                  /* Emit a UTF-8 Byte Order Mark (BOM) in the beginning of output files. */
    // "newLine": "crlf",                                /* Set the newline character for emitting files. */
    // "stripInternal": true,                            /* Disable emitting declarations that have '@internal' in their JSDoc comments. */
    // "noEmitHelpers": true,                            /* Disable generating custom helper functions like '__extends' in compiled output. */
    // "noEmitOnError": true,                            /* Disable emitting files if any type checking errors are reported. */
    // "preserveConstEnums": true,                       /* Disable erasing 'const enum' declarations in generated code. */
    // "declarationDir": "./",                           /* Specify the output directory for generated declaration files. */
    // "preserveValueImports": true,                     /* Preserve unused imported values in the JavaScript output that would otherwise be removed. */
    /* Interop Constraints */
    // "isolatedModules": true,                          /* Ensure that each file can be safely transpiled without relying on other imports. */
    // "allowSyntheticDefaultImports": true,             /* Allow 'import x from y' when a module doesn't have a default export. */
    "esModuleInterop": true, /* Emit additional JavaScript to ease support for importing CommonJS modules. This enables 'allowSyntheticDefaultImports' for type compatibility. */
    // "preserveSymlinks": true,                         /* Disable resolving symlinks to their realpath. This correlates to the same flag in node. */
    "forceConsistentCasingInFileNames": true, /* Ensure that casing is correct in imports. */
    /* Type Checking */
    "strict": true, /* Enable all strict type-checking options. */
    // "noImplicitAny": true,                            /* Enable error reporting for expressions and declarations with an implied 'any' type. */
    // "strictNullChecks": true,                         /* When type checking, take into account 'null' and 'undefined'. */
    // "strictFunctionTypes": true,                      /* When assigning functions, check to ensure parameters and the return values are subtype-compatible. */
    // "strictBindCallApply": true,                      /* Check that the arguments for 'bind', 'call', and 'apply' methods match the original function. */
    // "strictPropertyInitialization": true,             /* Check for class properties that are declared but not set in the constructor. */
    // "noImplicitThis": true,                           /* Enable error reporting when 'this' is given the type 'any'. */
    // "useUnknownInCatchVariables": true,               /* Default catch clause variables as 'unknown' instead of 'any'. */
    // "alwaysStrict": true,                             /* Ensure 'use strict' is always emitted. */
    // "noUnusedLocals": true,                           /* Enable error reporting when local variables aren't read. */
    // "noUnusedParameters": true,                       /* Raise an error when a function parameter isn't read. */
    // "exactOptionalPropertyTypes": true,               /* Interpret optional property types as written, rather than adding 'undefined'. */
    // "noImplicitReturns": true,                        /* Enable error reporting for codepaths that do not explicitly return in a function. */
    // "noFallthroughCasesInSwitch": true,               /* Enable error reporting for fallthrough cases in switch statements. */
    // "noUncheckedIndexedAccess": true,                 /* Add 'undefined' to a type when accessed using an index. */
    // "noImplicitOverride": true,                       /* Ensure overriding members in derived classes are marked with an override modifier. */
    // "noPropertyAccessFromIndexSignature": true,       /* Enforces using indexed accessors for keys declared using an indexed type. */
    // "allowUnusedLabels": true,                        /* Disable error reporting for unused labels. */
    // "allowUnreachableCode": true,                     /* Disable error reporting for unreachable code. */
    /* Completeness */
    // "skipDefaultLibCheck": true,                      /* Skip type checking .d.ts files that are included with TypeScript. */
    "skipLibCheck": true /* Skip type checking all .d.ts files. */
  },
}
'''
'''--- jobs/near_processor/.mocharc.json ---
{
    "extension": [
        "ts"
    ],
    "spec": "*/*.spec.ts",
    "require": "ts-node/esm",
    "node-option": [
        "loader=ts-node/esm",
        "experimental-specifier-resolution=node"
    ]
}
'''
'''--- jobs/near_processor/README.md ---
# Near Processor

Service responsible for minting the nft

## Environment Configuration
| Key | Description |
| :-- | :---------- |
| `AZURE_ACCOUNT_NAME` | **REQUIRED**. azure account name |
| `AZURE_ACCOUNT_KEY` | **REQUIRED**. azure secret key |
| `TOPIC` | **REQUIRED**. name of the topic |
| `NEAR_ACCOUNT_NAME` | **REQUIRED**. near account that will sign the transactions |
| `NEAR_ACCOUNT_PRIVATE_KEY` | **REQUIRED**. NEAR_ACCOUNT_NAME private key |
| `NEAR_ACCOUNT_CONTRACT_NAME` | name of the minter contract, defaults to `NEAR_ACCOUNT_NAME` if not provided |
| `NEAR_DEPOSIT` | **REQUIRED**. amount of near to deposit in each transaction call, excess tokens are refunded automatically |
| `VAULT_CONTRACT_ADDRESS` | **REQUIRED**. address of the vault contract |
| `VAULT_BASE_URL` | **REQUIRED**. base url of the vault frontend |
| `DEFAULT_CALLBACK_URL` | **REQUIRED**. default callback url that will receive the emitted events |
| `MAX_JOBS` | maximum number of jobs to be processed at the same time, setting to 0 will have no limit |
| `LOG_LEVEL` | [winston log level](https://www.npmjs.com/package/winston#logging-levels)|

## Job Spec
The service will listen for messages on a pubsub queue, the message should be a valid json data that conforms with the format below.

```json
{
    "JobId": "ff975cbd-32f4-4f09-9b9a-01964dd6eb90",
    "ArweaveTxnId": "ZkUxju5Y5Goy-OXw2O-mj8T_T4JSUHb7sDUhUMLNlgg",
    "OwnerAddress": "nft.nftdw-001.testnet",
    "Title": "Samuel Dickinson - 2022 World Triathlon Sprint & Relay Championships Montreal",
    "Description": "<TOKEN DESCRIPTION>",
    "Copies": 1,
    "IssuedAt": "1660818999614",
    "ExpiresAt": null,
    "StartsAt": "1660818999614",
    "UpdatedAt": "1660818999614"
}
```

| Field | Type | Description |
| :---- | :--- | :---------- |
| `JobId` | string | **Required**.  A unique string generated by the publisher of the message |
| `ArweaveTxnId` | string | **Required**. Arweave transaction id, the media should be accessible from https://arweave.net/<TX_ID> |
| `OwnerAddress` | string | **Optional**. The address of the owner of the nft to be minted, defaults to the minter if not provided |
| `Title` | string | **Optional**. NFT title, see [TokenMetadata.title](https://nomicon.io/Standards/Tokens/NonFungibleToken/Metadata#interface) |
| `Description` | string | **Optional**. NFT Description, see [TokenMetadata.description](https://nomicon.io/Standards/Tokens/NonFungibleToken/Metadata#interface) |
| `Copies` | number | **Optional**. NFT copies, see [TokenMetadata.copies](https://nomicon.io/Standards/Tokens/NonFungibleToken/Metadata#interface) |
| `IssuedAt` | string | **Optional**. NFT issuedAt, see [TokenMetadata.issued_at](https://nomicon.io/Standards/Tokens/NonFungibleToken/Metadata#interface) |
| `ExpiresAt` | string | **Optional**. NFT expiresAt, see [TokenMetadata.expires_at](https://nomicon.io/Standards/Tokens/NonFungibleToken/Metadata#interface) |
| `StartsAt` | string | **Optional**. NFT startsAt, see [TokenMetadata.starts_at](https://nomicon.io/Standards/Tokens/NonFungibleToken/Metadata#interface) |
| `UpdatedAt` | string | **Optional**. NFT updatedAt, see [TokenMetadata.updated_at](https://nomicon.io/Standards/Tokens/NonFungibleToken/Metadata#interface) |
## Events

the processor will emit events and send it to the configured `DEFAULT_CALLBACK_URL` if `CallbackURL` is not specified on jobs. 

### Started Event
Emmited to notify the callback url that the processor has received the job and is currently processing it.
```json
{
    "JobId": "ff975cbd-32f4-4f09-9b9a-01964dd6eb90",
    "Event": "started",
    "Time": 1659965535,
    "Message": "Job Started"
}
```
### Failure Event
Emmited to notify the callback url that an error has occured.
```json
{
    "JobId": "ff975cbd-32f4-4f09-9b9a-01964dd6eb90",
    "Event": "failure",
    "Time": 1659965535,
    "Message": "Example Error Message"
}

```
### Success Event
Emmited to notify the callback url that the job has successfully finished.
```json
{
    "JobId": "ff975cbd-32f4-4f09-9b9a-01964dd6eb90",
    "Event": "success",
    "Message": "Job ff975cbd-32f4-4f09-9b9a-01964dd6eb90 has been successfully processed",
    "Details": {
        "ClaimURL": "https://vault.nftdw-001.com/claim/nft.nftdw-001.testnet/2d7c23e9-134c-445e-8882-d5be78c0ce56?token=eyJORlRDb250cmFjdCI6Im5mdC5uZnRkdy0wMDEudGVzdG5ldCIsIlRva2VuSWQiOiIyZDdjMjNlOS0xMzRjLTQ0NWUtODg4Mi1kNWJlNzhjMGNlNTYiLCJQcml2YXRlS2V5IjoiZWQyNTUxOTozZHdpamhHcGNnRXYxN3VWZW14aEtEZkNHS3pLSDk4aERWNjVzVEg3ZVA1aXgza3htc3JLSzU3dE1NTXF2OWlybU1KTEoyM3JWcFYxZFROQ1c0aENkNzlmIiwiVmF1bHRDb250cmFjdCI6InZhdWx0Lm5mdGR3LTAwMS50ZXN0bmV0In0%3D",
        "ExplorerURL": "https://explorer.testnet.near.org/transactions/E9237ZHVQ8ANEtRfzEAkuVkSjxyCZYc9cxeTCfsREoQ9",
        "TransactionId": "E9237ZHVQ8ANEtRfzEAkuVkSjxyCZYc9cxeTCfsREoQ9"
    },
    "Time": 1661128110153
}
```
| Field | Description |
| :---- | :---------- |
| `Details.ClaimURL` | a url that claims the minted nft if OwnerAddress is not set on job payload, this should be sent securely to the supposed owner of the nft as anyone that has access to this data has the ability to claim the nft |
| `Details.ExplorerURL` | a url that redirects to the transaction |
| `TransactionId` | id of the transaction |

## Process Flow
'''
'''--- jobs/near_processor/bin/callback_server.ts ---
import { createServer, } from "http";
import { Logger } from "../src/lib/logger.js";

createServer((req, res) => {
    req.on('data', (data) => {
        Logger().info(data)
    });

    res.write("200")
    res.end();
}).listen(8080);
'''
'''--- jobs/near_processor/bin/claim.ts ---
import * as nearAPI from 'near-api-js';
import { GetConfig, LoadConfig } from '../src/config.js';

LoadConfig();
let config = GetConfig();

let key = 'ed25519:2U2myGfAK6W2u8wsYXQ71i3QUfWxzAbQKivHf7onygGs2borP7AWh67rTTdobUWRNkfdR6mDfsyoYt3T8dBqZPrw';
let keypair = nearAPI.KeyPair.fromString(key);
let accountId = config.VaultContractAddress;
let tokenId = '2988a185-bced-4efa-a7f0-5b184247c4ae';
let receiver_id = '74b5704603733ced2a0752021534b17e96dca7f312ecb0ab9c60bc4af3ccfc0f';

let keyStore = new nearAPI.keyStores.InMemoryKeyStore();
(async () => {
    let conn = await nearAPI.connect({
        networkId: 'testnet',
        nodeUrl: 'https://rpc.testnet.near.org',
        keyStore: keyStore,
        headers: {}
    });
    
    keyStore.setKey('testnet', accountId, keypair);

    let result = await (await conn.account(accountId)).functionCall({
        contractId: accountId,
        methodName: 'claim',
        args: {
            receiver_id: receiver_id,
            claimable_id: `${config.NearAccountName}:${tokenId}`
        }
    });
    console.log(JSON.stringify(result));
})();

'''
'''--- jobs/near_processor/bin/claim_challenge_generate.ts ---
import {
    KeyPair
} from 'near-api-js';
import { Logger } from '../src/lib/logger.js';
import { ClaimDetails } from '../src/core/near.js';

let claimURLString = 'http://localhost:8080//claim/nft.nftdw-001.testnet/438c0f6b-469e-4480-8ff0-51961e3b1f78?token=eyJORlRDb250cmFjdCI6Im5mdC5uZnRkdy0wMDEudGVzdG5ldCIsIlRva2VuSWQiOiI0MzhjMGY2Yi00NjllLTQ0ODAtOGZmMC01MTk2MWUzYjFmNzgiLCJQcml2YXRlS2V5IjoiZWQyNTUxOTpSRFJZOXRGTHc3ZmdUeU43ZUd4TlBGY1J4OVBVcHdRYTlEVTFNcExWWGU3TGNLRGlyTkp4ZFZuUUJIOGRHaGRUTHFqclVkZGpNZEhXWGtCQlhSYXlBbksiLCJWYXVsdENvbnRyYWN0IjoidmF1bHQubmZ0ZHctMDAxLnRlc3RuZXQifQ%3D%3D';
let claimURL = new URL(claimURLString);
let claimDetails = JSON.parse(Buffer.from(claimURL.searchParams.get('token') as string, 'base64').toString('utf-8')) as ClaimDetails;

let keypair = KeyPair.fromString(claimDetails.PrivateKey);

let payloadBuff = Buffer.from(JSON.stringify({
    token_id: claimDetails.TokenId,
    nft_account_id: claimDetails.NFTContract,
    timestamp_millis: (new Date()).getTime(),
}), 'utf-8');

let sign = keypair.sign(payloadBuff);

Logger().info(`${payloadBuff.toString('base64')}.${Buffer.from(sign.signature.buffer).toString('base64')}`);
'''
'''--- jobs/near_processor/bin/storage_queue_clear.ts ---
import { GetConfig, LoadConfig } from "../src/config.js";
import {
    QueueServiceClient,
    StorageSharedKeyCredential
} from "@azure/storage-queue";
import { Logger } from "../src/lib/logger.js";

LoadConfig();

let config = GetConfig();

(async () => {
    let qsClient = new QueueServiceClient(`https://${config.AzureAccountName}.queue.core.windows.net`, new StorageSharedKeyCredential(config.AzureAccountName, config.AzureAccountKey));
    let qClient = qsClient.getQueueClient(config.Topic);

    await qClient.createIfNotExists();
    let response = await qClient.clearMessages();
    Logger().info(response);
    
    
})();

'''
'''--- jobs/near_processor/bin/storage_queue_dispatch.ts ---
import { GetConfig, LoadConfig } from "../src/config.js";
import {
    QueueServiceClient,
    StorageSharedKeyCredential
} from "@azure/storage-queue";
import { Logger } from "../src/lib/logger.js";

LoadConfig();

let config = GetConfig();

(async () => {
    let qsClient = new QueueServiceClient(`https://${config.AzureAccountName}.queue.core.windows.net`, new StorageSharedKeyCredential(config.AzureAccountName, config.AzureAccountKey));
    let qClient = qsClient.getQueueClient(config.Topic);

    await qClient.createIfNotExists();
    //let uuid = randomUUID();
    //let uuid ='02bc722d-f790-4d1c-aab6-a3394921c638';
    let job = {
        JobId: "ff975cbd-32f4-4f09-9b9a-01964dd6eb90",
        //OwnerAddress: "test-claimer.testnet",
        ArweaveTxnId: "h1-yKxs4HbtxzZJpuqUW6jwsoVxVi97fcCja3Rf1FGU", // mp4
        //ArweaveTxnId: "NxWWRUM8twj9hipCFm6WMt-a-cucoZoJ-zoSuSc8zso", // image
        Title: "Tilda Månsson - 2022 World Triathlon Cup Bergen",
        Description: "Tilda Månsson - 2022 World Triathlon Cup Bergen",
        Copies: 1,
        IssuedAt: "1660818999614",
        ExpiresAt: null,
        StartsAt: "1660818999614",
        UpdatedAt: "1660818999614"
    }
    let response = await qClient.sendMessage(JSON.stringify(job));
    
    Logger().info(response.messageId);
})();

'''
'''--- jobs/near_processor/bin/storage_queue_list.ts ---
import { GetConfig, LoadConfig } from "../src/config.js";
import {
    QueueServiceClient,
    StorageSharedKeyCredential
} from "@azure/storage-queue";
import { Logger } from "../src/lib/logger.js";

LoadConfig();

let config = GetConfig();

(async () => {
    let qsClient = new QueueServiceClient(`https://${config.AzureAccountName}.queue.core.windows.net`, new StorageSharedKeyCredential(config.AzureAccountName, config.AzureAccountKey));
    for await (const queue of qsClient.listQueues()) {
        Logger().info(queue.name);
    }
})();

'''
'''--- jobs/near_processor/bin/storage_queue_peek.ts ---
import { GetConfig, LoadConfig } from "../src/config.js";
import {
    QueueServiceClient,
    StorageSharedKeyCredential
} from "@azure/storage-queue";
import { Logger } from "../src/lib/logger.js";

LoadConfig();

let config = GetConfig();

(async () => {
    let qsClient = new QueueServiceClient(`https://${config.AzureAccountName}.queue.core.windows.net`, new StorageSharedKeyCredential(config.AzureAccountName, config.AzureAccountKey));
    let qClient = qsClient.getQueueClient(config.Topic);

    await qClient.createIfNotExists();
    let response = await qClient.peekMessages({
        numberOfMessages: 32
    });
    
    for (const item of response.peekedMessageItems) {
        Logger().info(JSON.stringify(item));
    }
})();

'''
'''--- jobs/near_processor/package.json ---
{
  "name": "near_processor",
  "version": "1.0.0",
  "main": "src/index.ts",
  "type": "module",
  "license": "MIT",
  "devDependencies": {
    "@types/mocha": "^10.0.0",
    "@types/node": "^18.6.5",
    "@types/utf-8-validate": "^5.0.0",
    "mocha": "^10.1.0",
    "nodemon": "^2.0.19",
    "ts-node": "^10.9.1",
    "typescript": "^4.7.4"
  },
  "scripts": {
    "dev": "ts-node --esm src/index.ts",
    "dev:watch": "nodemon -w src -e ts,js,json --exec 'yarn run dev'",
    "test": "yarn mocha",
    "test:watch": "nodemon -w src -w tests -e ts,js,json --exec 'yarn run test'",
    "build": "yarn clean && tsc -p tsconfig.build.json",
    "clean": "rm -rf ./dist",
    "dispatch:storage-queue": "ts-node --esm bin/storage_queue_dispatch.ts",
    "peek:storage-queue": "ts-node --esm bin/storage_queue_peek.ts",
    "list:storage-queue": "ts-node --esm bin/storage_queue_list.ts",
    "clear:storage-queue": "ts-node --esm bin/storage_queue_clear.ts",
    "callback-server": "ts-node --esm bin/callback_server.ts"
  },
  "dependencies": {
    "@azure/storage-queue": "^12.10.0",
    "axios": "^0.27.2",
    "dotenv": "^16.0.1",
    "file-type": "^18.0.0",
    "near-api-js": "^0.45.1",
    "utf-8-validate": "^5.0.9",
    "winston": "^3.8.1"
  }
}

'''
'''--- jobs/near_processor/src/config.ts ---
import env from 'dotenv';
import { Logger } from './lib/logger.js';
import {
    
} from 'near-api-js'

type Config = {
    AzureAccountName: string,
    AzureAccountKey: string,
    Topic: string,
    NearAccountName: string,
    NearAccountPrivateKey: string,
    NearMinterContractName: string,
    NearEnv: "testnet" | "mainnet",
    NearDeposit: string,
    DefaultCallbackURL: string,
    VaultBaseURL: string,
    VaultContractAddress: string,
    MaxJobs: number
}

let config: Config;
env.config();

function LoadConfig() {
    
    if (!process.env.DEFAULT_CALLBACK_URL) {
        Logger().error("DEFAULT_CALLBACK_URL is a required environment variable")
        process.exit(1);
    }
    if (!process.env.TOPIC) {
        Logger().error("TOPIC is a required environment variable");
        process.exit(1);
    }
    if (!process.env.AZURE_ACCOUNT_NAME) {
        Logger().error("AZURE_ACCOUNT_NAME is a required environment variable");
        process.exit(1);
    }
    if (!process.env.AZURE_ACCOUNT_KEY) {
        Logger().error("AZURE_ACCOUNT_KEY is a required environment variable");
        process.exit(1);
    }
    if (!process.env.NEAR_ACCOUNT_NAME) {
        Logger().error("NEAR_ACCOUNT_NAME is a required environment variable");
        process.exit(1);
    }
    if (!process.env.NEAR_ACCOUNT_PRIVATE_KEY) {
        Logger().error("NEAR_ACCOUNT_PRIVATE_KEY is a required environment variable");
        process.exit(1);
    }
    if (process.env.NEAR_ENV !== "mainnet" && process.env.NEAR_ENV !== "testnet") {
        Logger().error("NEAR_ENV should be mainnet or testnet");
        process.exit(1);
    }
    if (!process.env.NEAR_DEPOSIT) {
        Logger().error("NEAR_DEPOSIT is a required environment variable");
        process.exit(1);
    }
    if (!process.env.VAULT_BASE_URL) {
        Logger().error("VAULT_BASE_URL is a required environment variable");
        process.exit(1);
    }
    if (!process.env.VAULT_CONTRACT_ADDRESS) {
        Logger().error("VAULT_CONTRACT_ADDRESS is a required environment variable");
        process.exit(1);
    }

    try {
        config = {
            AzureAccountName: process.env.AZURE_ACCOUNT_NAME,
            AzureAccountKey: process.env.AZURE_ACCOUNT_KEY,
            Topic: process.env.TOPIC,
            NearAccountName: process.env.NEAR_ACCOUNT_NAME,
            NearAccountPrivateKey: process.env.NEAR_ACCOUNT_PRIVATE_KEY,
            NearMinterContractName: process.env.NEAR_ACCOUNT_CONTRACT_NAME ?? process.env.NEAR_ACCOUNT_NAME,
            NearEnv: process.env.NEAR_ENV,
            DefaultCallbackURL: process.env.DEFAULT_CALLBACK_URL,
            MaxJobs: process.env.MAX_JOBS ? parseInt(process.env.MAX_JOBS) : 0,
            NearDeposit: process.env.NEAR_DEPOSIT,
            VaultBaseURL: new URL(process.env.VAULT_BASE_URL).toString(),
            VaultContractAddress: process.env.VAULT_CONTRACT_ADDRESS
        }
    } catch (e) {
        let err = e as Error;
        Logger().error(`Failed to load configuration, check if .env setting is correct: ${err.message}\n${err.stack ?? ''}`);
        process.exit(1);
    }

    Logger().debug(`loaded configuration:\n${JSON.stringify(config)}`);
}

function GetConfig(): Config {
    if (!config) {
        throw new Error("Config is not yet loaded, call LoadConfig() first");
    }

    return config;
}

export {
    Config,
    LoadConfig,
    GetConfig
}
'''
'''--- jobs/near_processor/src/core/event.ts ---
import axios from "axios";
import { Logger } from "../lib/logger.js";

let _callbackURL: string;
let _client = axios.default;

function SetDefaultCallBack(callback: string) {
    _callbackURL = callback;
}

interface Event {
    JobId: string,
    Event: "started" | "failure" | "success",
    Message: string,
    Details?: any
}

interface _Event extends Event {
    Time: number
}

type EmitResult = "ok" | "not_found" | "error";

async function Emit(event: Event): Promise<EmitResult> {
    let _event = event as _Event;
    _event.Time = new Date().getTime();
    let response = await _client.post(_callbackURL, _event);
    Logger().debug(`callback reponse: ${response.status}\n${JSON.stringify(response.data)}`);

    if ([400, "400"].includes(response.data)) {
        return "error";
    }

    if ([404, "404"].includes(response.data)) {
        return "not_found";
    }

    if (![true, "true", 200, "200"].includes(response.data)) {
        throw new Error(`Callback endpoint unexpected response ${response.data}, should be true or 200`);
    }

    return "ok";
}

export {
    Emit,
    SetDefaultCallBack,
    Event
}
'''
'''--- jobs/near_processor/src/core/near.ts ---
import {
    connect, ConnectConfig, Near, KeyPair, Account, DEFAULT_FUNCTION_CALL_GAS
} from 'near-api-js';
import {
    parseNearAmount,
} from 'near-api-js/lib/utils/format.js'
import {
    randomUUID,
    createHash
} from 'crypto';
import { Payload } from '../queue/common.js';
import { FinalExecutionStatus } from 'near-api-js/lib/providers'
import axios from 'axios';
import { Logger } from '../lib/logger.js';
import isValidUTF8 from 'utf-8-validate';
import { functionCall } from 'near-api-js/lib/transaction.js';
import { fileTypeFromBuffer } from 'file-type';
import { KeyPairEd25519 } from 'near-api-js/lib/utils/key_pair.js';

let _near: Near;
let _account: Account;
let _accountID: string;
let _accountKey: string;
let _contractID: string;
let _explorerBaseURL: string;
let _deposit: string;
let _minter: Minter;
let _vaultBaseUrl: string;
let _vaultContractAddress: string;

class Minter extends Account {

    public async MintNFT(payload: Payload): Promise<MintResult> {

        let media = (await axios.default.get<Buffer>(`https://arweave.net/${payload.ArweaveTxnId}`, {
            responseType: 'arraybuffer',
        })).data;
        let media_ext = (await fileTypeFromBuffer(media))?.ext ?? "jpeg";
        let metadata = (await axios.default.get<Buffer>(`https://arweave.net/${payload.ArweaveTxnId}/metadata.json`, {
            responseType: 'arraybuffer',
        })).data;

        let media_hash = createHash('sha256').update(media).digest().toString('base64');
        let ref_hash: string | undefined;
        if (isValidUTF8(metadata)) {
            ref_hash = createHash('sha256').update(metadata).digest().toString('base64');
        }

        let token_id = randomUUID();
        let actions = [
            functionCall(
                'mint',
                {
                    token_id: token_id,
                    owner_address: payload.OwnerAddress ?? this.accountId,
                    media_id: `${payload.ArweaveTxnId}/nft.${media_ext}`,
                    media_hash: media_hash,
                    metadata_id: `${payload.ArweaveTxnId}/metadata.json`,
                    reference_hash: ref_hash,
                    extra: ref_hash ? Buffer.from(metadata).toString('utf-8') : null,
                    copies: payload.Copies,
                    description: payload.Description,
                    expires_at: payload.ExpiresAt,
                    issued_at: payload.IssuedAt,
                    starts_at: payload.StartsAt,
                    title: payload.Title,
                    updated_at: payload.UpdatedAt
                },
                DEFAULT_FUNCTION_CALL_GAS,
                parseNearAmount(_deposit)
            ),
        ];

        let claimDetails: ClaimDetails | undefined;
        if (payload.OwnerAddress === undefined || payload.OwnerAddress === null) {
            let keypair = KeyPairEd25519.fromRandom();

            claimDetails = {
                NFTContract: this.accountId,
                TokenId: token_id,
                PrivateKey: keypair.toString(),
                VaultContract: _vaultContractAddress
            };

            actions.push(functionCall(
                'nft_transfer_call',
                {
                    receiver_id: _vaultContractAddress,
                    token_id: token_id,
                    msg: JSON.stringify({
                        public_key: keypair.publicKey.toString(),
                        message: `lock nft ${this.accountId}:${token_id} on vault`
                    })
                },
                40000000000000,
                "1"
            ));
        }

        let result = await this.signAndSendTransaction({
            receiverId: this.accountId,
            actions: actions
        });
        Logger().debug(`Mint transaction result:\n${JSON.stringify(result)}`);

        let status = result.status as FinalExecutionStatus;
        let token = {} as Token;
        if (status.SuccessValue) {
            token = JSON.parse(Buffer.from(status.SuccessValue, 'base64').toString('utf-8'));
            Logger().debug(JSON.stringify(token));
        } else {
            throw new Error(`Failed called to mint: ${status.Failure}`)
        }

        let mintResult = {
            ExplorerURL: `${_explorerBaseURL}/transactions/${result.transaction_outcome.id}`,
            TransactionId: result.transaction_outcome.id,
        } as MintResult;

        if (claimDetails) {
            let claimUrl = new URL(`${_vaultBaseUrl}claim/${_contractID}/${token_id}`);
            claimUrl.searchParams.append("token", Buffer.from(JSON.stringify(claimDetails), 'utf-8').toString('base64'));
            mintResult.ClaimURL = claimUrl.toString();
        }
        
        
        return mintResult;
    }
}

type Token = {
    token_id: string,
    owner_id: string,
    metadata: {
        title?: string,
        description?: string,
        media?: string,
        media_hash?: string,
        copies?: number,
        issued_at?: string,
        expires_at?: string,
        starts_at?: string,
        updated_at?: string,
        extra?: string,
        reference?: string,
        reference_hash?: string
    }
}

type MintResult = {
    ExplorerURL: string,
    TransactionId: string,
    ClaimURL?: string
}

type ClaimDetails = {
    VaultContract: string,
    NFTContract: string,
    PrivateKey: string,
    TokenId: string,
}

type InitConfig = {
    deposit: string,
    accountID: string,
    accountKey: string, 
    contractID: string,
    vaultBaseURL: string,
    vaultContractAddress: string
}

async function Init(connectConfig: ConnectConfig, initConfig: InitConfig) {
    _accountID = initConfig.accountID;
    _accountKey = initConfig.accountKey;
    _contractID = initConfig.contractID;
    _explorerBaseURL = `https://explorer.${connectConfig.networkId}.near.org`;
    _deposit = initConfig.deposit;
    _vaultBaseUrl = initConfig.vaultBaseURL;
    _vaultContractAddress = initConfig.vaultContractAddress;

    await connectConfig.keyStore?.setKey(connectConfig.networkId, _accountID, KeyPair.fromString(_accountKey))

    _near = await connect(connectConfig);
    _account = await _near.account(_accountID);
    _minter = new Minter(_near.connection, _accountID);
}

async function Mint(payload: Payload): Promise<MintResult> {
    return await _minter.MintNFT(payload);
}

export {
    Init,
    Mint,
    ClaimDetails
}

'''
'''--- jobs/near_processor/src/core/processor.ts ---
import { Job, Payload, Queue } from '../queue/common.js';
import axios from 'axios';
import { Sleep } from '../lib/util.js';
import { Logger } from '../lib/logger.js';
import { Emit } from './event.js';
import { Mint } from './near.js';

let _queue: Queue;

/**
 * Number of jobs currently being processed
 */
let _processing: number = 0;

/**
 * Maximum number of jobs that will be processed
 */
let _maxProcessingJobs = 0;

function SetQueue(queue: Queue) {
    _queue = queue;
}

function SetMaxJobs(maxJobs: number) {
    _maxProcessingJobs = maxJobs;
}

async function Start() {
    // main processor  loop
    while (true) {
        await loop();
    }
}

async function loop() {
    if (_maxProcessingJobs > 0 && _processing >= _maxProcessingJobs) {
        Logger().debug(`throttling processing ${_processing} jobs`);
        await Sleep(5000);
        return;
    }
    let job!: Job;
    try {
        job = await _queue.getNextJob();
    } catch (e) {
        Logger().error(e);
        return;
    }

    _processing++;
    (async () => {
        await processJob(job);
    })().then(async () => {
        await job.complete();
    }).catch(async (e) => {
        await job.requeue();
        let err = e as Error;
        let err_message = `Job ${job.payload.JobId} failed due to error: ${err.message}\n${err.stack ?? ''}\n${JSON.stringify(err)}`
        Logger().error(err_message, {
            log_type: 'job_failed',
            job_id: job.payload.JobId,
        });
        Emit({
            JobId: job.payload.JobId,
            Event: "failure",
            Message: err_message,
            Details: {
                Error: err,
            }
        }).catch(e => {
            Logger().error(`Failed to send failure message to callback: ${e}`);
        });
    }).finally(() => {
        _processing--;
    });
}

async function processJob(job: Job) {
    let payload = job.payload;
    Logger().info(`Job ${payload.JobId} received`, {
        log_type: 'job_started',
        job_id: job.payload.JobId
    });
    let emitResult = await Emit({
        JobId: payload.JobId,
        Event: 'started',
        Message: `Job ${payload.JobId} has been received and started`,
    });

    if (emitResult === "not_found" || emitResult === "error") {
        Logger().warn(`callback endpoint failed with emit result ${emitResult}, removing the job from queue`);
        return;
    }

    let result = await Mint(payload);
    Logger().info(`Job ${payload.JobId} has been successfully processed`, {
        log_type: 'job_completed',
        job_id: payload.JobId
    });
    Emit({
        JobId: payload.JobId,
        Event: "success",
        Message: `Job ${payload.JobId} has been successfully processed`,
        Details: result
    }).catch((e) => {
        Logger().warn(`JobId: ${payload.JobId}: Failed to send callback: ${JSON.stringify(e)}`);
    });
}

export {
    Start,
    SetQueue,
    SetMaxJobs
}
'''
'''--- jobs/near_processor/src/index.ts ---
import { SetMaxJobs, SetQueue, Start } from './core/processor.js';
import { GetConfig, LoadConfig } from './config.js';
import { CreateAzureStorageQueue } from './queue/azure_storage_queue.js';

import { Logger } from './lib/logger.js';
import { SetDefaultCallBack } from './core/event.js';
import {
    Init,
} from './core/near.js';
import { InMemoryKeyStore } from 'near-api-js/lib/key_stores/in_memory_key_store.js';

LoadConfig();
let config = GetConfig();

(async () => {
    // ---- CONFIGURATION ---- //
    SetQueue(CreateAzureStorageQueue(config.AzureAccountName, config.AzureAccountKey, config.Topic));
    SetMaxJobs(config.MaxJobs);
    SetDefaultCallBack(config.DefaultCallbackURL);
    await Init({
        networkId: config.NearEnv,
        nodeUrl: config.NearEnv === "mainnet" ? 'https://rpc.mainnet.near.org' : 'https://rpc.testnet.near.org',
        headers: {},
        keyStore: new InMemoryKeyStore(),
    }, {
        accountID: config.NearAccountName,
        accountKey: config.NearAccountPrivateKey,
        contractID: config.NearMinterContractName,
        deposit: config.NearDeposit,
        vaultBaseURL: config.VaultBaseURL,
        vaultContractAddress: config.VaultContractAddress
    });
    // ---- CONFIGURATION ---- //

    Logger().debug('Starting Processor');
    Start();
})();

'''
'''--- jobs/near_processor/src/lib/logger.ts ---
import { Logger, createLogger, transports, format } from 'winston';
import { hostname } from 'os';

let _logger: Logger;
let host: string;

function GetLogger(): Logger {
    host = hostname();
    return _logger ?? InitLogger();
}

const CustomFormat = format((info, opts) => {
    if (!info.log_type) {
        info.log_type = 'default';
    }
    if (opts.service_name) {
        info.service_name = opts.service_name;
    }
    info.time = Math.round(new Date().getTime() / 1000);
    info.hostname = host;
    info.message = `${info.message}`;

    return info;
});

function InitLogger(): Logger {
    _logger = createLogger({
        level: process.env.LOG_LEVEL ?? 'info',
        format: format.combine(
            CustomFormat({
                service_name: 'near_processor'
            }),
            format.json()
        ),
        transports: [
            new transports.Console(),
        ]
    });
    let consoleLog = console.log;
    console.log = (...data: any[]) => { _logger.warn("console.log called"); consoleLog(data) };
    return _logger;
}

export {
    GetLogger as Logger,
    InitLogger
}
'''
'''--- jobs/near_processor/src/lib/util.ts ---
function Sleep(ms: number): Promise<void> {
    return new Promise((resolve) => {
        setTimeout(() => {
            resolve()
        }, ms);
    });
}

export {
    Sleep
}
'''
'''--- jobs/near_processor/src/queue/azure_storage_queue.ts ---
import { DequeuedMessageItem, QueueServiceClient, StorageSharedKeyCredential } from "@azure/storage-queue";
import { Emit } from "../core/event.js";
import { Logger } from "../lib/logger.js";
import { Sleep } from "../lib/util.js";
import { Job, ParsePayloadFromJSONString, Payload, Queue } from "./common.js";

const STORAGE_QUEUE_POLL_INTERVAL = 3000;
const STORAGE_QUEUE_RENEW_LOCK_INTERVAL = 20000;

function CreateAzureStorageQueue(accountName: string, accountKey: string, queueName: string): Queue {
    let qsClient = new QueueServiceClient(
        `https://${accountName}.queue.core.windows.net`,
        new StorageSharedKeyCredential(accountName, accountKey)
    );

    let qClient = qsClient.getQueueClient(queueName);

    let queue: Queue = {
        async getNextJob(): Promise<Job> {

            return new Promise<Job>(async (resolve, reject) => {
                await qClient.createIfNotExists();
                let currentMessage: DequeuedMessageItem;
                do {
                    Logger().debug('fetching messages from storage queue');
                    let msgResponse = await qClient.receiveMessages({});

                    currentMessage = msgResponse.receivedMessageItems.pop() as DequeuedMessageItem;
                    if (currentMessage) {
                        Logger().info(`message received: ${currentMessage.messageId} : ${currentMessage.popReceipt}`)

                        let renewLockInterval = setInterval(async () => {
                            Logger().debug(`renewing lock for message: ${currentMessage.messageId}`)
                            let response = await qClient.updateMessage(currentMessage.messageId, currentMessage.popReceipt, undefined, 30);
                            currentMessage.popReceipt = response.popReceipt ?? currentMessage.popReceipt;
                        }, STORAGE_QUEUE_RENEW_LOCK_INTERVAL);

                        let parseResult = ParsePayloadFromJSONString(currentMessage.messageText);
                        if (parseResult.Error !== undefined && parseResult.Error !== null || parseResult.Payload === null) {
                            clearInterval(renewLockInterval);
                            Emit({
                                Event: 'failure',
                                JobId: parseResult.Payload?.JobId ?? '',
                                Message: `Failed to parse payload: ${parseResult.Error}`
                            });
                            reject(parseResult.Error);
                            break;
                        }
                        let payload = JSON.parse(currentMessage.messageText) as Payload;

                        resolve({
                            payload: payload,
                            async complete() {
                                clearInterval(renewLockInterval);
                                await qClient.deleteMessage(currentMessage.messageId, currentMessage.popReceipt);
                            },
                            async requeue() {
                                clearInterval(renewLockInterval);
                            }
                        });

                        break;
                    }
                    Logger().debug(`none received, sleeping for ${STORAGE_QUEUE_POLL_INTERVAL}ms`);
                    await Sleep(STORAGE_QUEUE_POLL_INTERVAL);
                } while (currentMessage === undefined);
            });
        },
    };

    return queue;
}

export {
    CreateAzureStorageQueue
}
'''
'''--- jobs/near_processor/src/queue/common.ts ---
interface Queue {
    getNextJob: () => Promise<Job>
}

interface Job {
    payload: Payload
    complete: () => Promise<void>,
    requeue: () => Promise<void>
}

type Payload = {
    JobId: string,
    ArweaveTxnId: string,
    OwnerAddress?: string,
    Title?: string,
    Description?: string,
    Copies?: number,
    IssuedAt?: string,
    ExpiresAt?: string,
    StartsAt?: string,
    UpdatedAt?: string,
}

function ParsePayloadFromJSONString(payloadString: string): {
    Payload: Payload | null,
    Error: string | null
} {
    let payload = JSON.parse(payloadString) as Payload;

    if (payload.JobId === undefined || payload.JobId === null) {
        return {
            Payload: payload,
            Error: "JobId is a required parameter"
        };
    }

    if (payload.ArweaveTxnId === undefined || payload.ArweaveTxnId === null) {
        return {
            Payload: payload,
            Error: "ArweaveTxnId is a required parameter"
        };
    }

    return {
        Payload: payload,
        Error: null
    };
}

export {
    Queue,
    Job,
    Payload,
    ParsePayloadFromJSONString
}
'''
'''--- jobs/near_processor/src/tsconfig.json ---
{
  "compilerOptions": {
    /* Visit https://aka.ms/tsconfig to read more about this file */
    /* Projects */
    // "incremental": true,                              /* Save .tsbuildinfo files to allow for incremental compilation of projects. */
    // "composite": true,                                /* Enable constraints that allow a TypeScript project to be used with project references. */
    // "tsBuildInfoFile": "./.tsbuildinfo",              /* Specify the path to .tsbuildinfo incremental compilation file. */
    // "disableSourceOfProjectReferenceRedirect": true,  /* Disable preferring source files instead of declaration files when referencing composite projects. */
    // "disableSolutionSearching": true,                 /* Opt a project out of multi-project reference checking when editing. */
    // "disableReferencedProjectLoad": true,             /* Reduce the number of projects loaded automatically by TypeScript. */
    /* Language and Environment */
    "target": "ES2020", /* Set the JavaScript language version for emitted JavaScript and include compatible library declarations. */
    // "lib": [],                                        /* Specify a set of bundled library declaration files that describe the target runtime environment. */
    // "jsx": "preserve",                                /* Specify what JSX code is generated. */
    // "experimentalDecorators": true,                   /* Enable experimental support for TC39 stage 2 draft decorators. */
    // "emitDecoratorMetadata": true,                    /* Emit design-type metadata for decorated declarations in source files. */
    // "jsxFactory": "",                                 /* Specify the JSX factory function used when targeting React JSX emit, e.g. 'React.createElement' or 'h'. */
    // "jsxFragmentFactory": "",                         /* Specify the JSX Fragment reference used for fragments when targeting React JSX emit e.g. 'React.Fragment' or 'Fragment'. */
    // "jsxImportSource": "",                            /* Specify module specifier used to import the JSX factory functions when using 'jsx: react-jsx*'. */
    // "reactNamespace": "",                             /* Specify the object invoked for 'createElement'. This only applies when targeting 'react' JSX emit. */
    // "noLib": true,                                    /* Disable including any library files, including the default lib.d.ts. */
    // "useDefineForClassFields": true,                  /* Emit ECMAScript-standard-compliant class fields. */
    // "moduleDetection": "auto",                        /* Control what method is used to detect module-format JS files. */
    /* Modules */
    "module": "Node16", /* Specify what module code is generated. */
    "rootDir": ".",                                  /* Specify the root folder within your source files. */
    //"moduleResolution": "Node16", /* Specify how TypeScript looks up a file from a given module specifier. */
    // "baseUrl": "./",                                  /* Specify the base directory to resolve non-relative module names. */
    // "paths": {},                                      /* Specify a set of entries that re-map imports to additional lookup locations. */
    // "rootDirs": [],                                   /* Allow multiple folders to be treated as one when resolving modules. */
    // "typeRoots": [],                                  /* Specify multiple folders that act like './node_modules/@types'. */
    // "types": [],                                      /* Specify type package names to be included without being referenced in a source file. */
    // "allowUmdGlobalAccess": true,                     /* Allow accessing UMD globals from modules. */
    // "moduleSuffixes": [],                             /* List of file name suffixes to search when resolving a module. */
    // "resolveJsonModule": true,                        /* Enable importing .json files. */
    // "noResolve": true,                                /* Disallow 'import's, 'require's or '<reference>'s from expanding the number of files TypeScript should add to a project. */
    /* JavaScript Support */
    // "allowJs": true,                                  /* Allow JavaScript files to be a part of your program. Use the 'checkJS' option to get errors from these files. */
    // "checkJs": true,                                  /* Enable error reporting in type-checked JavaScript files. */
    // "maxNodeModuleJsDepth": 1,                        /* Specify the maximum folder depth used for checking JavaScript files from 'node_modules'. Only applicable with 'allowJs'. */
    /* Emit */
    // "declaration": true,                              /* Generate .d.ts files from TypeScript and JavaScript files in your project. */
    // "declarationMap": true,                           /* Create sourcemaps for d.ts files. */
    // "emitDeclarationOnly": true,                      /* Only output d.ts files and not JavaScript files. */
    // "sourceMap": true,                                /* Create source map files for emitted JavaScript files. */
    // "outFile": "./",                                  /* Specify a file that bundles all outputs into one JavaScript file. If 'declaration' is true, also designates a file that bundles all .d.ts output. */
    "outDir": "../dist", /* Specify an output folder for all emitted files. */
    // "removeComments": true,                           /* Disable emitting comments. */
    // "noEmit": true,                                   /* Disable emitting files from a compilation. */
    // "importHelpers": true,                            /* Allow importing helper functions from tslib once per project, instead of including them per-file. */
    // "importsNotUsedAsValues": "remove",               /* Specify emit/checking behavior for imports that are only used for types. */
    // "downlevelIteration": true,                       /* Emit more compliant, but verbose and less performant JavaScript for iteration. */
    // "sourceRoot": "",                                 /* Specify the root path for debuggers to find the reference source code. */
    // "mapRoot": "",                                    /* Specify the location where debugger should locate map files instead of generated locations. */
    // "inlineSourceMap": true,                          /* Include sourcemap files inside the emitted JavaScript. */
    // "inlineSources": true,                            /* Include source code in the sourcemaps inside the emitted JavaScript. */
    // "emitBOM": true,                                  /* Emit a UTF-8 Byte Order Mark (BOM) in the beginning of output files. */
    // "newLine": "crlf",                                /* Set the newline character for emitting files. */
    // "stripInternal": true,                            /* Disable emitting declarations that have '@internal' in their JSDoc comments. */
    // "noEmitHelpers": true,                            /* Disable generating custom helper functions like '__extends' in compiled output. */
    // "noEmitOnError": true,                            /* Disable emitting files if any type checking errors are reported. */
    // "preserveConstEnums": true,                       /* Disable erasing 'const enum' declarations in generated code. */
    // "declarationDir": "./",                           /* Specify the output directory for generated declaration files. */
    // "preserveValueImports": true,                     /* Preserve unused imported values in the JavaScript output that would otherwise be removed. */
    /* Interop Constraints */
    // "isolatedModules": true,                          /* Ensure that each file can be safely transpiled without relying on other imports. */
    // "allowSyntheticDefaultImports": true,             /* Allow 'import x from y' when a module doesn't have a default export. */
    "esModuleInterop": true, /* Emit additional JavaScript to ease support for importing CommonJS modules. This enables 'allowSyntheticDefaultImports' for type compatibility. */
    // "preserveSymlinks": true,                         /* Disable resolving symlinks to their realpath. This correlates to the same flag in node. */
    "forceConsistentCasingInFileNames": true, /* Ensure that casing is correct in imports. */
    /* Type Checking */
    "strict": true, /* Enable all strict type-checking options. */
    // "noImplicitAny": true,                            /* Enable error reporting for expressions and declarations with an implied 'any' type. */
    // "strictNullChecks": true,                         /* When type checking, take into account 'null' and 'undefined'. */
    // "strictFunctionTypes": true,                      /* When assigning functions, check to ensure parameters and the return values are subtype-compatible. */
    // "strictBindCallApply": true,                      /* Check that the arguments for 'bind', 'call', and 'apply' methods match the original function. */
    // "strictPropertyInitialization": true,             /* Check for class properties that are declared but not set in the constructor. */
    // "noImplicitThis": true,                           /* Enable error reporting when 'this' is given the type 'any'. */
    // "useUnknownInCatchVariables": true,               /* Default catch clause variables as 'unknown' instead of 'any'. */
    // "alwaysStrict": true,                             /* Ensure 'use strict' is always emitted. */
    // "noUnusedLocals": true,                           /* Enable error reporting when local variables aren't read. */
    // "noUnusedParameters": true,                       /* Raise an error when a function parameter isn't read. */
    // "exactOptionalPropertyTypes": true,               /* Interpret optional property types as written, rather than adding 'undefined'. */
    // "noImplicitReturns": true,                        /* Enable error reporting for codepaths that do not explicitly return in a function. */
    // "noFallthroughCasesInSwitch": true,               /* Enable error reporting for fallthrough cases in switch statements. */
    // "noUncheckedIndexedAccess": true,                 /* Add 'undefined' to a type when accessed using an index. */
    // "noImplicitOverride": true,                       /* Ensure overriding members in derived classes are marked with an override modifier. */
    // "noPropertyAccessFromIndexSignature": true,       /* Enforces using indexed accessors for keys declared using an indexed type. */
    // "allowUnusedLabels": true,                        /* Disable error reporting for unused labels. */
    // "allowUnreachableCode": true,                     /* Disable error reporting for unreachable code. */
    /* Completeness */
    // "skipDefaultLibCheck": true,                      /* Skip type checking .d.ts files that are included with TypeScript. */
    "skipLibCheck": true /* Skip type checking all .d.ts files. */
  },
}
'''
'''--- jobs/near_processor/tests/common.spec.ts ---
import { equal } from "assert";
import { ParsePayloadFromJSONString, Payload } from "../src/queue/common.js";

describe("test", () => {
    it("should pass", () => {
        equal(true, true);
    });
});

describe("Payload Validation", () => {
    it("should be able parse valid payload", () => {
        let payloadString = JSON.stringify({
            ArweaveTxnId: "",
            JobId: "",
        } as Payload);
        let result = ParsePayloadFromJSONString(payloadString);
        equal(result.Error, null);
    });

    it("should fail on undefined JobId", () => {
        let payloadString = JSON.stringify({
            ArweaveTxnId: "",
            JobId: undefined as any,
        } as Payload);
        let result = ParsePayloadFromJSONString(payloadString);
        equal(result.Error, "JobId is a required parameter");
    });

    it("should fail on null JobId", () => {
        let payloadString = JSON.stringify({
            ArweaveTxnId: "",
            JobId: null as any,
        } as Payload);
        let result = ParsePayloadFromJSONString(payloadString);
        equal(result.Error, "JobId is a required parameter");
    });

    it("should fail on undefined ArweaveTxnId", () => {
        let payloadString = JSON.stringify({
            ArweaveTxnId: undefined as any,
            JobId: "",
        } as Payload);
        let result = ParsePayloadFromJSONString(payloadString);
        equal(result.Error, "ArweaveTxnId is a required parameter");
    });

    it("should fail on null ArweaveTxnId", () => {
        let payloadString = JSON.stringify({
            ArweaveTxnId: null as any,
            JobId: "",
        } as Payload);
        let result = ParsePayloadFromJSONString(payloadString);
        equal(result.Error, "ArweaveTxnId is a required parameter");
    });
});
'''
'''--- jobs/near_processor/tsconfig.build.json ---
{
    "extends": "./tsconfig.json",
    "compilerOptions": {
        "rootDir": "src"
    },
    "exclude": [
        "bin/**/*",
        "tests/**/*"
    ]
}
'''
'''--- jobs/near_processor/tsconfig.json ---
{
  "compilerOptions": {
    /* Visit https://aka.ms/tsconfig to read more about this file */
    /* Projects */
    // "incremental": true,                              /* Save .tsbuildinfo files to allow for incremental compilation of projects. */
    // "composite": true,                                /* Enable constraints that allow a TypeScript project to be used with project references. */
    // "tsBuildInfoFile": "./.tsbuildinfo",              /* Specify the path to .tsbuildinfo incremental compilation file. */
    // "disableSourceOfProjectReferenceRedirect": true,  /* Disable preferring source files instead of declaration files when referencing composite projects. */
    // "disableSolutionSearching": true,                 /* Opt a project out of multi-project reference checking when editing. */
    // "disableReferencedProjectLoad": true,             /* Reduce the number of projects loaded automatically by TypeScript. */
    /* Language and Environment */
    "target": "ES2020", /* Set the JavaScript language version for emitted JavaScript and include compatible library declarations. */
    // "lib": [],                                        /* Specify a set of bundled library declaration files that describe the target runtime environment. */
    // "jsx": "preserve",                                /* Specify what JSX code is generated. */
    // "experimentalDecorators": true,                   /* Enable experimental support for TC39 stage 2 draft decorators. */
    // "emitDecoratorMetadata": true,                    /* Emit design-type metadata for decorated declarations in source files. */
    // "jsxFactory": "",                                 /* Specify the JSX factory function used when targeting React JSX emit, e.g. 'React.createElement' or 'h'. */
    // "jsxFragmentFactory": "",                         /* Specify the JSX Fragment reference used for fragments when targeting React JSX emit e.g. 'React.Fragment' or 'Fragment'. */
    // "jsxImportSource": "",                            /* Specify module specifier used to import the JSX factory functions when using 'jsx: react-jsx*'. */
    // "reactNamespace": "",                             /* Specify the object invoked for 'createElement'. This only applies when targeting 'react' JSX emit. */
    // "noLib": true,                                    /* Disable including any library files, including the default lib.d.ts. */
    // "useDefineForClassFields": true,                  /* Emit ECMAScript-standard-compliant class fields. */
    // "moduleDetection": "auto",                        /* Control what method is used to detect module-format JS files. */
    /* Modules */
    "module": "Node16", /* Specify what module code is generated. */
    "rootDir": ".",                                  /* Specify the root folder within your source files. */
    //"moduleResolution": "Node16", /* Specify how TypeScript looks up a file from a given module specifier. */
    // "baseUrl": "./",                                  /* Specify the base directory to resolve non-relative module names. */
    // "paths": {},                                      /* Specify a set of entries that re-map imports to additional lookup locations. */
    // "rootDirs": [],                                   /* Allow multiple folders to be treated as one when resolving modules. */
    // "typeRoots": [],                                  /* Specify multiple folders that act like './node_modules/@types'. */
    // "types": [],                                      /* Specify type package names to be included without being referenced in a source file. */
    // "allowUmdGlobalAccess": true,                     /* Allow accessing UMD globals from modules. */
    // "moduleSuffixes": [],                             /* List of file name suffixes to search when resolving a module. */
    // "resolveJsonModule": true,                        /* Enable importing .json files. */
    // "noResolve": true,                                /* Disallow 'import's, 'require's or '<reference>'s from expanding the number of files TypeScript should add to a project. */
    /* JavaScript Support */
    // "allowJs": true,                                  /* Allow JavaScript files to be a part of your program. Use the 'checkJS' option to get errors from these files. */
    // "checkJs": true,                                  /* Enable error reporting in type-checked JavaScript files. */
    // "maxNodeModuleJsDepth": 1,                        /* Specify the maximum folder depth used for checking JavaScript files from 'node_modules'. Only applicable with 'allowJs'. */
    /* Emit */
    // "declaration": true,                              /* Generate .d.ts files from TypeScript and JavaScript files in your project. */
    // "declarationMap": true,                           /* Create sourcemaps for d.ts files. */
    // "emitDeclarationOnly": true,                      /* Only output d.ts files and not JavaScript files. */
    // "sourceMap": true,                                /* Create source map files for emitted JavaScript files. */
    // "outFile": "./",                                  /* Specify a file that bundles all outputs into one JavaScript file. If 'declaration' is true, also designates a file that bundles all .d.ts output. */
    "outDir": "./dist", /* Specify an output folder for all emitted files. */
    // "removeComments": true,                           /* Disable emitting comments. */
    // "noEmit": true,                                   /* Disable emitting files from a compilation. */
    // "importHelpers": true,                            /* Allow importing helper functions from tslib once per project, instead of including them per-file. */
    // "importsNotUsedAsValues": "remove",               /* Specify emit/checking behavior for imports that are only used for types. */
    // "downlevelIteration": true,                       /* Emit more compliant, but verbose and less performant JavaScript for iteration. */
    // "sourceRoot": "",                                 /* Specify the root path for debuggers to find the reference source code. */
    // "mapRoot": "",                                    /* Specify the location where debugger should locate map files instead of generated locations. */
    // "inlineSourceMap": true,                          /* Include sourcemap files inside the emitted JavaScript. */
    // "inlineSources": true,                            /* Include source code in the sourcemaps inside the emitted JavaScript. */
    // "emitBOM": true,                                  /* Emit a UTF-8 Byte Order Mark (BOM) in the beginning of output files. */
    // "newLine": "crlf",                                /* Set the newline character for emitting files. */
    // "stripInternal": true,                            /* Disable emitting declarations that have '@internal' in their JSDoc comments. */
    // "noEmitHelpers": true,                            /* Disable generating custom helper functions like '__extends' in compiled output. */
    // "noEmitOnError": true,                            /* Disable emitting files if any type checking errors are reported. */
    // "preserveConstEnums": true,                       /* Disable erasing 'const enum' declarations in generated code. */
    // "declarationDir": "./",                           /* Specify the output directory for generated declaration files. */
    // "preserveValueImports": true,                     /* Preserve unused imported values in the JavaScript output that would otherwise be removed. */
    /* Interop Constraints */
    // "isolatedModules": true,                          /* Ensure that each file can be safely transpiled without relying on other imports. */
    // "allowSyntheticDefaultImports": true,             /* Allow 'import x from y' when a module doesn't have a default export. */
    "esModuleInterop": true, /* Emit additional JavaScript to ease support for importing CommonJS modules. This enables 'allowSyntheticDefaultImports' for type compatibility. */
    // "preserveSymlinks": true,                         /* Disable resolving symlinks to their realpath. This correlates to the same flag in node. */
    "forceConsistentCasingInFileNames": true, /* Ensure that casing is correct in imports. */
    /* Type Checking */
    "strict": true, /* Enable all strict type-checking options. */
    // "noImplicitAny": true,                            /* Enable error reporting for expressions and declarations with an implied 'any' type. */
    // "strictNullChecks": true,                         /* When type checking, take into account 'null' and 'undefined'. */
    // "strictFunctionTypes": true,                      /* When assigning functions, check to ensure parameters and the return values are subtype-compatible. */
    // "strictBindCallApply": true,                      /* Check that the arguments for 'bind', 'call', and 'apply' methods match the original function. */
    // "strictPropertyInitialization": true,             /* Check for class properties that are declared but not set in the constructor. */
    // "noImplicitThis": true,                           /* Enable error reporting when 'this' is given the type 'any'. */
    // "useUnknownInCatchVariables": true,               /* Default catch clause variables as 'unknown' instead of 'any'. */
    // "alwaysStrict": true,                             /* Ensure 'use strict' is always emitted. */
    // "noUnusedLocals": true,                           /* Enable error reporting when local variables aren't read. */
    // "noUnusedParameters": true,                       /* Raise an error when a function parameter isn't read. */
    // "exactOptionalPropertyTypes": true,               /* Interpret optional property types as written, rather than adding 'undefined'. */
    // "noImplicitReturns": true,                        /* Enable error reporting for codepaths that do not explicitly return in a function. */
    // "noFallthroughCasesInSwitch": true,               /* Enable error reporting for fallthrough cases in switch statements. */
    // "noUncheckedIndexedAccess": true,                 /* Add 'undefined' to a type when accessed using an index. */
    // "noImplicitOverride": true,                       /* Ensure overriding members in derived classes are marked with an override modifier. */
    // "noPropertyAccessFromIndexSignature": true,       /* Enforces using indexed accessors for keys declared using an indexed type. */
    // "allowUnusedLabels": true,                        /* Disable error reporting for unused labels. */
    // "allowUnreachableCode": true,                     /* Disable error reporting for unreachable code. */
    /* Completeness */
    // "skipDefaultLibCheck": true,                      /* Skip type checking .d.ts files that are included with TypeScript. */
    "skipLibCheck": true /* Skip type checking all .d.ts files. */
  },
}
'''