*GitHub Repository "NEAR-Analytics/SOCIAL_DB_CODE_TOOLKIT"*

'''--- dev_commit.py ---

import os
import pandas as pd
import subprocess
from collections import defaultdict

from query_engine.client import *
import time
# Read the CSV file
import json

# create a delay within a loop:
import uuid
from helpers import *

def get_df_from_widget_name(widget_name, existing_widgets):
    try:
        if widget_name in existing_widgets:
            print(f"Updating Index {widget_name}")
            df_arr = get_widget_updates(widget_name, existing_widgets[widget_name]['block_timestamp'])
        else:
            print(f"Creating Index {widget_name}")
            df_arr = get_widget_updates(widget_name)
        return pd.DataFrame(df_arr)
    except Exception as e:
        print(f"An error occurred: {e}")
        failed_widgets.append(widget_name)
        return None

def create_or_update_widget(widget_entry, existing_widgets, widget_path):
    signer_dir = widget_entry['signer_id']
    widget_name = widget_entry['widget_name']
    timestamp = widget_entry['block_timestamp']

    env = {
        'GIT_COMMITTER_NAME': signer_dir,
        'GIT_COMMITTER_EMAIL': signer_dir
    }

    # put this following if else inside a try except block
    # try:
    if widget_name in existing_widgets:
        print(f"Updating Index {widget_name}")

        df_arr = get_widget_updates(widget_name, existing_widgets[widget_name]['block_timestamp'])
        df = pd.DataFrame(df_arr)

    else:
        print(f"Creating Index {widget_name}")
        df_arr = get_widget_updates(widget_name)
        df = pd.DataFrame(df_arr)
    # except:
    #     # if there is an error, skip this widget and add it failed widgets list
    #     failed_widgets.append(widget_name)

    run_git_command(['git', 'add', '.'], widget_path, env=env)
    run_git_command(['git', 'commit', '-m', f'Update {widget_name} by {signer_dir} at {timestamp}', '--date', timestamp], widget_path, env=env)

def process_widgets(widget_names_list, existing_widgets, base_path):
    os.chdir(base_path)

    for widget_name in widget_names_list:
        df = get_df_from_widget_name(widget_name, existing_widgets)
        if df is None:
            continue

        df = df.sort_values(by=['block_timestamp'])
        data = df.to_dict('records')
        dev_widgets = defaultdict(list)

        for entry in data:
            dev_widgets[entry['signer_id']].append(entry)

        for near_dev, widget_entries in dev_widgets.items():
            widget_path = os.path.join(base_path, near_dev)

            if not os.path.exists(widget_path):
                os.makedirs(widget_path)

            os.chdir(widget_path)

            for widget_entry in widget_entries:
                create_or_update_widget(widget_entry, existing_widgets, widget_path)

        os.chdir(base_path)

base_path = os.environ['WIDGET_ROOT_DIR']

# Sort data by block_timestamp
snowflake_data = get_widget_names()
widget_names_list = [row['widget_name'] for row in snowflake_data]
widget_names_list = set(widget_names_list)
# widget_names_list = [name for name in widget_names_list if name]
# ad_hot, skip widgets in this list already:
existing_widgets = get_checkpoints(base_path)
failed_widgets = []

# Call the main function to process widgets
process_widgets(widget_names_list, existing_widgets, base_path)
'''
'''--- get_dev_list.py ---
import os
import pandas as pd
import subprocess
from collections import defaultdict
import json

from query_engine.client import *
import time
# Read the CSV file

# create a delay within a loop:

dev_list = get_list_of_all_devs()
dev_profiles = {}

for dev in dev_list:
    time.sleep(1)
    try:
        dev_profile = get_dev_info(dev)
        dev_profiles[dev] = dev_profile.to_dict('records')
    except Exception as e:
        print(f"Error getting profile for {dev}: {e}")
# save json to text json file:

# open a file in write mode
with open("dev_profiles.json", "w") as f:
    # write the dictionary to the file in JSON format
    json.dump(dev_profiles, f)
# df = get_all_widget()
'''
'''--- helpers.py ---

import os
import pandas as pd
import subprocess
from collections import defaultdict

from query_engine.client import *
import time
# Read the CSV file
import json

# create a delay within a loop:
import uuid

def get_github_id(signed_id):
    if signed_id in dev_profiles:
        profile_data_raw = dev_profiles[signed_id]
        if len(profile_data_raw):
            profile_data = dev_profiles[signed_id]['profile_data']
            if 'github' in profile_data:
                return profile_data['github']

    return signed_id

# df = get_all_widget()

# Function to run git commands
def run_git_command(command, path='.', env=None):
    process = subprocess.Popen(command, cwd=path, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)
    stdout, stderr = process.communicate()
    if process.returncode != 0:
        print(stderr.decode())
    else:
        print(stdout.decode())

def commit_parse_date(date_string):
    formats = ['%Y-%m-%d %H:%M:%S.%f', '%Y-%m-%dT%H:%M:%S.%fZ']
    for fmt in formats:
        try:
            return datetime.strptime(date_string, fmt)
        except ValueError:
            pass
    raise ValueError(f'time data {date_string} does not match any of the formats')

def find_files(root_dir, file_name):
    file_paths = []

    for dirpath, dirnames, filenames in os.walk(root_dir):
        for file in filenames:
            if file == file_name:
                file_path = os.path.join(dirpath, file)
                file_paths.append(file_path)

    return file_paths

def get_checkpoints(root_directory):
    target_file_name = 'commit_raw.json'

    checkpoints = {}

    files_found = find_files(root_directory, target_file_name)

    if files_found:
        print(f"Found {len(files_found)} {target_file_name} files:")
        for file_path in files_found:
            # open json file
            with open(file_path) as json_file:
                data = json.load(json_file)
                # print(data)
                # print(data['widget_name'])
                # print(data['source_code'])
                # print(data['block_timestamp'])
                # print(data['signer_id'])
                # print(data['block_height'])
                # print(data['block_hash'])
                # print(
            checkpoints[data['widget_name']] = data
            print(file_path)
    else:
        print(f"No {target_file_name} files found in {root_directory}")
    return checkpoints

'''
'''--- query_engine/__init__.py ---

'''
'''--- query_engine/client.py ---

import os
import json
import pandas as pd
from datetime import datetime
from itertools import chain
from pprint import pprint
from fuzzywuzzy import process, fuzz
from decimal import *
# from shroomdk import ShroomDK

from flipside import Flipside

SHROOM_SDK_API = os.environ['FLIPSIDE_API_KEY']

flipside = Flipside(SHROOM_SDK_API)

def querying_pagination(query_string, API_KEY=SHROOM_SDK_API):
    """
    This function queries the Flipside database using the Shroom SDK and returns a pandas dataframe

    :param query_string: SQL query string
    :param API_KEY: API key for Shroom SDK
    :return: pandas dataframe

    """

    print("query_string", query_string)
    sdk = ShroomDK(API_KEY)

    # Query results page by page and saves the results in a list
    # If nothing is returned then just stop the loop and start adding the data to the dataframe
    result_list = []
    for i in range(1,11): # max is a million rows @ 100k per page
        data=sdk.query(query_string,page_size=100000,page_number=i)
        if data.run_stats.record_count == 0:
            break
        else:
            result_list.append(data.records)

    # Loops through the returned results and adds into a pandas dataframe
    result_df=pd.DataFrame()
    for idx, each_list in enumerate(result_list):
        if idx == 0:
            result_df=pd.json_normalize(each_list)
        else:
            result_df=pd.concat([result_df, pd.json_normalize(each_list)])

    return result_df

def get_widget_names():

    sql_statement = """
    SELECT WIDGET_NAME, COUNT(*) as COUNT
    FROM near.social.fact_widget_deployments
    GROUP BY WIDGET_NAME;
    """
    snowflake_data = flipside.query(sql_statement)
    return snowflake_data.records

def get_all_widget():
    """
    This function queries transactions received by address and returns the top n addresses

    :param top_n: number of top addresses to return
    :param time_period: time period to query
    :return: pandas dataframe

    """

    sql_statement =f"""
    select * from
    near.social.fact_widget_deployments
    where WIDGET_NAME = 'app__frame'
    """

    snowflake_data = flipside.query(sql_statement)
    return snowflake_data.records

def get_dev_info(dev_name):

    sql_statement = f"""

    select * from
    near.social.fact_profile_changes
    where signer_id = '{dev_name}'
    and profile_section = 'linktree'
    order by BLOCK_TIMESTAMP asc
    limit 1
    """

    snowflake_data = flipside.query(sql_statement)
    return snowflake_data.records

def get_list_of_all_devs():

    sql_statement = f"""
    SELECT signer_id, COUNT(*) as COUNT
    FROM near.social.fact_widget_deployments
    GROUP BY signer_id;

    """

    snowflake_data = flipside.query(sql_statement)
    snowflake_data = snowflake_data.records
    signer_ids = [row['signer_id'] for row in snowflake_data]

    data = set(signer_ids)
    return data

def get_widget_updates(widget_name, timestamp=None):

    widget_name = widget_name.replace("'", "\\'")

    if timestamp:
        sql_statement = f"""
        SELECT *
        FROM near.social.fact_widget_deployments
        WHERE WIDGET_NAME = '{widget_name}'
        AND BLOCK_TIMESTAMP > '{timestamp}';
        """
    else:

        sql_statement =f"""
        select * from
        near.social.fact_widget_deployments
        where WIDGET_NAME = '{widget_name}'
        """

    snowflake_data = flipside.query(sql_statement)
    return snowflake_data.records

    """
    This function queries transactions received by address and returns the top n addresses

    :param top_n: number of top addresses to return
    :param time_period: time period to query
    :return: pandas dataframe

    """

    # sql_statement =f"""
    # SELECT
    #     tx_signer,
    #     COUNT(*)
    # FROM
    #     near.CORE.fact_transactions
    # WHERE
    #     block_timestamp > (CURRENT_DATE() - INTERVAL '{time_period}')
    #     AND tx_status = 'Success'
    # GROUP BY
    #     tx_signer
    # ORDER BY
    #     COUNT(*) DESC
    # LIMIT {top_n}
    # """
'''
'''--- readme.md ---

This toolkit helps BOS devs to access and mirror their work on the Blockchain Operating System to Github.

It uses Flipside Crypto API to access the BOS and turn it into a github repo.

- set  the following variables before running the script:

```

export WIDGET_ROOT_DIR='~/dev/_widgets/'
export FLIPSIDE_API_KEY=''

python get_dev_list.py

```
'''