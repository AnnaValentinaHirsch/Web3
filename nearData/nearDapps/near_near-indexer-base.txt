*GitHub Repository "near/near-indexer-base"*

'''--- Cargo.toml ---
[package]
name = "indexer-base"
version = "0.1.0"
edition = "2021"
rust-version = "1.61.0"

[lib]
proc-macro = true

[dependencies]
anyhow = "1.0.51"
base64 = "0.11"
bigdecimal = { version = "0.2", features = ["serde"] }
cached = "0.23.0"
clap = { version = "3.1.4", features = ["color", "derive", "env"] }
dotenv = "0.15.0"
futures = "0.3.5"
hex = "0.4"
itertools = "0.9.0"
num-traits = "0.2.11"
serde = { version = "1", features = ["derive"] }
serde_json = "1.0.55"
sqlx = { version = "0.5.13", features = ["runtime-tokio-native-tls", "postgres", "bigdecimal", "json"] }
syn = "1.0.90"
tokio = { version = "1", features = ["full"] }
tokio-stream = { version = "0.1" }
tracing = "0.1.13"
tracing-subscriber = "0.2.4"
quote = "1.0.17"

near-crypto = { git = "https://github.com/near/nearcore", rev = "5f09a3bf042b32d1ff26554433ad6449199ea02a" }
near-indexer-primitives = "0.12.0"
near-lake-framework = "0.3.0"

'''
'''--- README.md ---
# Indexer Base [DEPRECATED]

Consider using [Microindexers](https://github.com/near/near-microindexers) instead.

Async Postgres-compatible solution to load the data from NEAR blockchain.
Based on [NEAR Lake Framework](https://github.com/near/near-lake-framework-rs).

[Indexer For Explorer](https://github.com/near/near-indexer-for-explorer) has some disadvantages that we wanted to fix.
That's why we've created smaller projects, almost independent mini-indexers:
- [Indexer Base](https://github.com/near/near-indexer-base) works with basic information about transactions, receipts;
- [Indexer Accounts](https://github.com/near/near-indexer-accounts) works with accounts and access_keys;
- [Indexer Balances](https://github.com/near/near-indexer-balances) collects the info about native NEAR token balance changes;
- [Indexer Events](https://github.com/near/near-indexer-events) works with events produced by NEPs (FT, NFT, etc).

### What are the differences with Indexer For Explorer?

- The data model changed a bit, naming changed;
- We moved from `diesel` to `sqlx`, we prefer having lightweight ORM and write raw SQL queries;
- Separate projects are easier to maintain;
- The main difference is in the future: we are thinking where to go next if we decide to get rid of Postgres.

### Why do the projects _almost_ independent?

We still hope to leave foreign keys in the tables.
The data provided by all the indexers depend on Indexer Base.  
All the indexers may have the dependency to Indexer Accounts, but it will give us circular dependency, that's why we don't use these constraints.

### Can I create my own indexer?

Sure!
Feel free to use this project as the example.

## Linux installation guide

```bash
sudo apt install git build-essential pkg-config libssl-dev tmux postgresql-client libpq-dev -y
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
source $HOME/.cargo/env
cargo install --version=0.5.13 sqlx-cli --features postgres
ulimit -n 30000
cargo build --release
#!!! here you need to create .env in the root of the project, and .aws in ~
cargo run --release -- --s3-bucket-name near-lake-data-mainnet --s3-region-name eu-central-1 --start-block-height 9820210
```

## Migrations

Unfortunately, migrations do not work if you have several projects writing to the same DB.
We still use the migrations folder in each project, but we have to apply the changes manually.

## Creating read-only PostgreSQL user

We highly recommend using a separate read-only user to access the data.
It helps you to avoid unexpected corruption of the indexed data.

We use `public` schema for all tables.
By default, new users have the possibility to create new tables/views/etc there.
If you want to restrict that, you have to revoke these rights:

```sql
REVOKE CREATE ON SCHEMA PUBLIC FROM PUBLIC;
REVOKE ALL PRIVILEGES ON ALL TABLES IN SCHEMA PUBLIC FROM PUBLIC;
ALTER DEFAULT PRIVILEGES IN SCHEMA PUBLIC GRANT SELECT ON TABLES TO PUBLIC;
```

After that, you could create read-only user in PostgreSQL:

```sql
CREATE ROLE readonly;
GRANT USAGE ON SCHEMA public TO readonly;
GRANT SELECT ON ALL TABLES IN SCHEMA public to readonly;
-- Put here your limit or just ignore this command
ALTER ROLE readonly SET statement_timeout = '30s';

CREATE USER explorer with login password 'password';
GRANT readonly TO explorer;
```

```bash
$ PGPASSWORD="password" psql -h 127.0.0.1 -U explorer databasename
```

## Redshift

We keep in mind the possibility to move the data to AWS Redshift.
Some notes are [here](redshift/REDSHIFT_NOTES.md).
'''
'''--- benchmarks.sql ---
use indexer_16_partitions;

-- We have only one writer when I do these tests. It takes ~30% of CPU on each leaf node
-- No other processes are running during this time
-- DB size is 600 Gb
-- We use S1 tier (8 vCPUs | 64 GB Memory, 1 Tb SSD storage)

-- 200 ms
select count(*) from blocks;

-- 250 ms
select count(*) from blocks
where block_height > 3000000;

-- !!! 1 minute 10 seconds first time, 21s second time. Postgres: 150 ms
select account_id from account_changes
order by block_timestamp desc
limit 100;

-- !!! 25s. Postgres: 150 ms
select * from blocks
order by block_timestamp desc
limit 100;

-- 400 ms
select * from blocks
where block_timestamp > 50000000
order by block_timestamp
limit 100;

------------------
-- Random queries from Explorer, taken from
-- https://github.com/near/near-explorer/blob/master/backend/src/db-utils.ts

-- !!! ERROR 1858 ER_TOO_MANY_SORTED_RUNS: Leaf Error (node-49a0b120-c01e-4247-acc8-88be43f66613-leaf-ag2-0.svc-49a0b120-c01e-4247-acc8-88be43f66613:3306): There are too many distinct sorted row segment groups (223) for sorted iteration on ''table account_changes''. Run ''OPTIMIZE TABLE account_changes'' and then try again.
-- after I run optimize, it took 5 sec. But it's anyway not OK that optimize was required. We can say it took 40 minutes and 5 seconds.
select blocks.block_height, account_changes.account_id, account_changes.block_timestamp, account_changes.caused_by_transaction_hash,
       account_changes.caused_by_receipt_id, account_changes.update_reason
from account_changes join blocks on account_changes.block_hash = blocks.block_hash
order by account_changes.block_timestamp
limit 100;

-- 40 mins
OPTIMIZE TABLE account_changes;

-- 35s. Postgres: 13s
-- aurora: 2s
SELECT
    receiver_account_id,
    COUNT(*) AS transactions_count
FROM transactions
WHERE receiver_account_id IN ('cheese.zest.near',
                              'miguel.zest.near',
                              'zest.near',
                              'paras.near',
                              'diagnostics.tessab.near',
                              'contract.paras.near',
                              'plutus.paras.near',
                              'berryclub.ek.near',
                              'farm.berryclub.ek.near',
                              'berryclub.near',
                              'cards.berryclub.ek.near',
                              'giveaway.paras.near',
                              'bananaswap.near',
                              'jerry.near.zest',
                              'tessab.near',
                              'amm.counselor.near')
GROUP BY receiver_account_id
ORDER BY transactions_count DESC;

-- 2s
SELECT
    COUNT(transaction_hash) AS total
FROM transactions
WHERE
        block_timestamp > UNIX_TIMESTAMP(DATE_SUB(NOW(), INTERVAL 30 day)) * 1000 * 1000 * 1000;

use indexer_16_partitions;

-- 700ms. Postgres: 6s
-- aurora: 1.5 minutes :(
SELECT COUNT(DISTINCT transactions.transaction_hash) AS in_transactions_count
FROM transactions
         LEFT JOIN action_receipts ON action_receipts.originated_from_transaction_hash = transactions.transaction_hash
    AND transactions.block_timestamp >= 1650067200000000000 AND transactions.block_timestamp < 1650153600000000000
WHERE action_receipts.block_timestamp >= 1650067200000000000 AND action_receipts.block_timestamp < 1650153600000000000
  AND transactions.signer_account_id != 'aurora'
  AND action_receipts.receiver_account_id = 'aurora';

-- !!! killed it after 9 minutes of waiting.  Postgres: 500 ms
-- aurora can't give the answer :( killed after 4 minutes

SELECT round(account_changes.block_timestamp / (1000 * 1000 * 1000)) AS timestamp,
       account_changes.update_reason,
       account_changes.nonstaked_balance AS nonstaked_balance,
       account_changes.staked_balance AS staked_balance,
       account_changes.storage_usage AS storage_usage,
       action_receipts.receipt_id,
       action_receipts.predecessor_account_id AS receipt_signer_id,
       action_receipts.receiver_account_id AS receipt_receiver_id,
       transactions.signer_account_id AS transaction_signer_id,
       transactions.receiver_account_id AS transaction_receiver_id,
       action_receipts__actions.action_kind AS receipt_kind,
       action_receipts__actions.args AS receipt_args
FROM account_changes
         LEFT JOIN transactions ON transactions.transaction_hash = account_changes.caused_by_transaction_hash
         LEFT JOIN action_receipts ON action_receipts.receipt_id = account_changes.caused_by_receipt_id
         LEFT JOIN action_receipts__actions ON action_receipts__actions.receipt_id = action_receipts.receipt_id
WHERE account_changes.account_id = 'aurora' and account_changes.block_timestamp < 1647578052869592081
ORDER BY account_changes.block_timestamp DESC
LIMIT 100;

SELECT round(account_changes.block_timestamp / (1000 * 1000 * 1000)) AS timestamp,
       account_changes.update_reason,
       account_changes.nonstaked_balance,
       account_changes.staked_balance,
       account_changes.storage_usage,
       transactions.signer_account_id AS transaction_signer_id,
       transactions.receiver_account_id AS transaction_receiver_id
FROM account_changes
         LEFT JOIN transactions ON transactions.transaction_hash = account_changes.caused_by_transaction_hash
WHERE account_changes.account_id = 'aurora' and account_changes.block_timestamp < 1647578052869592081
ORDER BY account_changes.block_timestamp DESC
LIMIT 100;

------------------
-- Analytical queries. Postgres is able to perform each of them in 30 s .. 7 minutes

-- https://github.com/near/near-analytics/blob/main/aggregations/db_tables/deployed_contracts.py
-- !!! I've found another problem here: we slow down `INSERT`s when running such heavy queries.
-- !!! after 12 minutes of executing, I got
-- ERROR 2470 UNKNOWN_ERR_CODE: Leaf Error (node-49a0b120-c01e-4247-acc8-88be43f66613-leaf-ag1-0.svc-49a0b120-c01e-4247-acc8-88be43f66613:3306): Leaf Error (node-49a0b120-c01e-4247-acc8-88be43f66613-leaf-ag1-0.svc-49a0b120-c01e-4247-acc8-88be43f66613:3306): Leaf Error (node-49a0b120-c01e-4247-acc8-88be43f66613-leaf-ag2-0.svc-49a0b120-c01e-4247-acc8-88be43f66613:3306): 'indexer_16_partitions_14': Failed to GetObject `prd/49a0b120-c01e-4247-acc8-88be43f66613/dbaaddab/9285098404147030757_4000/partition_14/blobs/00000000000/018/0x60ac_928509
SELECT
    action_receipts__actions.args::%code_sha256 as contract_code_sha256,
                action_receipts.receiver_account_id as deployed_to_account_id,
                action_receipts.receipt_id as deployed_by_receipt_id,
                action_receipts.block_timestamp as deployed_at_block_timestamp
FROM action_receipts__actions
    JOIN action_receipts ON action_receipts.receipt_id = action_receipts__actions.receipt_id
WHERE action_kind = 'DEPLOY_CONTRACT'
    -- random day from the last few weeks, that is fully written to the DB
    -- 16 april 2022 1650067200000000000
  AND action_receipts.block_timestamp >= 1650067200000000000
    -- 17 april 2022 1650153600000000000
  AND action_receipts.block_timestamp < 1650153600000000000
ORDER BY action_receipts.block_timestamp;

-- https://github.com/near/near-analytics/blob/main/aggregations/db_tables/daily_receipts_per_contract_count.py
-- Aurora: 10s
SELECT
    action_receipts__actions.receiver_account_id,
    COUNT(action_receipts__actions.receipt_id) AS receipts_count
FROM action_receipts__actions
WHERE action_receipts__actions.action_kind = 'FUNCTION_CALL'
  AND action_receipts__actions.block_timestamp >= 1650067200000000000
  AND action_receipts__actions.block_timestamp < 1650153600000000000
GROUP BY action_receipts__actions.receiver_account_id;

-- https://github.com/near/near-analytics/blob/main/aggregations/db_tables/daily_ingoing_transactions_per_account_count.py
-- 7s in aurora
-- It has a cheat in JOIN clause, it is also helpful there. Wihout it, it runs 40s
SELECT
    action_receipts.receiver_account_id,
    COUNT(DISTINCT transactions.transaction_hash) AS ingoing_transactions_count
FROM transactions
         LEFT JOIN action_receipts ON action_receipts.originated_from_transaction_hash = transactions.transaction_hash
    AND transactions.block_timestamp >= 1650067200000000000
    AND transactions.block_timestamp < 1650153600000000000
WHERE action_receipts.block_timestamp >= 1650067200000000000
  AND action_receipts.block_timestamp < (1650153600000000000 + 600000000000)
  AND transactions.signer_account_id != action_receipts.receiver_account_id
GROUP BY action_receipts.receiver_account_id;

-- Others we can try:
-- https://github.com/near/near-analytics/blob/main/aggregations/db_tables/daily_new_contracts_count.py
-- https://github.com/near/near-analytics/blob/main/aggregations/db_tables/daily_deposit_amount.py
-- https://github.com/near/near-analytics/blob/main/aggregations/db_tables/daily_active_contracts_count.py

'''
'''--- migrations/20220221161526_initial.sql ---
-- update_reason options:
--     {
--         'TRANSACTION_PROCESSING',
--         'ACTION_RECEIPT_PROCESSING_STARTED',
--         'ACTION_RECEIPT_GAS_REWARD',
--         'RECEIPT_PROCESSING',
--         'POSTPONED_RECEIPT',
--         'UPDATED_DELAYED_RECEIPTS',
--         'VALIDATOR_ACCOUNTS_UPDATE',
--         'MIGRATION',
--         'RESHARDING'
--     }
CREATE TABLE account_changes
(
    account_id                 text           NOT NULL,
    block_timestamp            numeric(20, 0) NOT NULL,
    block_hash                 text           NOT NULL,
    caused_by_transaction_hash text,
    caused_by_receipt_id       text,
    update_reason              text           NOT NULL,
    nonstaked_balance          numeric(38, 0) NOT NULL,
    staked_balance             numeric(38, 0) NOT NULL,
    storage_usage              numeric(20, 0) NOT NULL,
    chunk_index_in_block       integer        NOT NULL,
    index_in_chunk             integer        NOT NULL,
    PRIMARY KEY (block_timestamp, chunk_index_in_block, index_in_chunk)
);
ALTER TABLE account_changes
    ADD CONSTRAINT account_changes_block_hash_fk FOREIGN KEY (block_hash) REFERENCES blocks (block_hash);
ALTER TABLE account_changes
    ADD CONSTRAINT account_changes_receipt_id_fk FOREIGN KEY (caused_by_receipt_id) REFERENCES action_receipts (receipt_id);
ALTER TABLE account_changes
    ADD CONSTRAINT account_changes_tx_hash_fk FOREIGN KEY (caused_by_transaction_hash) REFERENCES transactions (transaction_hash);
CREATE INDEX CONCURRENTLY account_changes_account_idx ON account_changes (account_id);
CREATE INDEX CONCURRENTLY account_changes_block_hash_idx ON account_changes (block_hash);
CREATE INDEX CONCURRENTLY account_changes_block_timestamp_idx ON account_changes (block_timestamp);
CREATE INDEX CONCURRENTLY account_changes_receipt_id_idx ON account_changes (caused_by_receipt_id);
CREATE INDEX CONCURRENTLY account_changes_tx_hash_idx ON account_changes (caused_by_transaction_hash);
CREATE INDEX CONCURRENTLY account_changes_update_reason_idx ON account_changes (update_reason);

-- action_kind options:
--      {
--         'CREATE_ACCOUNT',
--         'DEPLOY_CONTRACT',
--         'FUNCTION_CALL',
--         'TRANSFER',
--         'STAKE',
--         'ADD_KEY',
--         'DELETE_KEY',
--         'DELETE_ACCOUNT'
--      }
CREATE TABLE action_receipts__actions
(
    block_hash             text           NOT NULL,
    block_timestamp        numeric(20, 0) NOT NULL,
    receipt_id             text           NOT NULL,
    action_kind            text           NOT NULL,
    -- https://docs.aws.amazon.com/redshift/latest/dg/json-functions.html
    -- https://docs.aws.amazon.com/redshift/latest/dg/super-overview.html
    args                   jsonb          NOT NULL,
    predecessor_account_id text           NOT NULL,
    receiver_account_id    text           NOT NULL,
    chunk_index_in_block   integer        NOT NULL,
    index_in_chunk         integer        NOT NULL,
    PRIMARY KEY (block_timestamp, chunk_index_in_block, index_in_chunk)
);
ALTER TABLE action_receipts__actions
    ADD CONSTRAINT action_receipt_actions_receipt_fk FOREIGN KEY (receipt_id) REFERENCES action_receipts (receipt_id);
CREATE INDEX CONCURRENTLY actions_action_kind_idx ON action_receipts__actions (action_kind);
CREATE INDEX CONCURRENTLY actions_predecessor_idx ON action_receipts__actions (predecessor_account_id);
CREATE INDEX CONCURRENTLY actions_receiver_idx ON action_receipts__actions (receiver_account_id);
CREATE INDEX CONCURRENTLY actions_block_timestamp_idx ON action_receipts__actions (block_timestamp);
CREATE INDEX CONCURRENTLY actions_args_function_call_idx ON action_receipts__actions ((args - >> 'method_name')) WHERE action_kind = 'FUNCTION_CALL';
-- CREATE INDEX CONCURRENTLY actions_args_receiver_id_idx ON action_receipts__actions ((args -> 'args_json' ->> 'receiver_id')) WHERE action_kind = 'FUNCTION_CALL' AND (args ->> 'args_json') IS NOT NULL;
-- CREATE INDEX CONCURRENTLY actions_receiver_and_timestamp_idx ON action_receipts__actions (receiver_account_id, block_timestamp);

CREATE TABLE action_receipts__outputs
(
    block_hash           text           NOT NULL,
    block_timestamp      numeric(20, 0) NOT NULL,
    receipt_id           text           NOT NULL,
    output_data_id       text           NOT NULL,
    receiver_account_id  text           NOT NULL,
    chunk_index_in_block integer        NOT NULL,
    index_in_chunk       integer        NOT NULL,
    PRIMARY KEY (block_timestamp, chunk_index_in_block, index_in_chunk)
);
ALTER TABLE action_receipts__outputs
    ADD CONSTRAINT outputs_receipt_fk FOREIGN KEY (receipt_id) REFERENCES action_receipts (receipt_id);
CREATE INDEX CONCURRENTLY outputs_block_timestamp_idx ON action_receipts__outputs (block_timestamp);
CREATE INDEX CONCURRENTLY outputs_output_data_id_idx ON action_receipts__outputs (output_data_id);
CREATE INDEX CONCURRENTLY outputs_receipt_id_idx ON action_receipts__outputs (receipt_id);
CREATE INDEX CONCURRENTLY outputs_receiver_account_id_idx ON action_receipts__outputs (receiver_account_id);

CREATE TABLE action_receipts
(
    receipt_id                       text           NOT NULL,
    block_hash                       text           NOT NULL,
    chunk_hash                       text           NOT NULL,
    block_timestamp                  numeric(20, 0) NOT NULL,
    chunk_index_in_block             integer        NOT NULL,
    receipt_index_in_chunk           integer        NOT NULL, -- goes both through action and data receipts
    predecessor_account_id           text           NOT NULL,
    receiver_account_id              text           NOT NULL,
    originated_from_transaction_hash text           NOT NULL,
    signer_account_id                text           NOT NULL,
    signer_public_key                text           NOT NULL,
--     todo change logic with gas_price + gas_used
-- https://github.com/near/near-analytics/issues/19
    gas_price                        numeric(38, 0) NOT NULL,
    PRIMARY KEY (receipt_id)
);
ALTER TABLE action_receipts
    ADD CONSTRAINT action_receipts_block_hash_fk FOREIGN KEY (block_hash) REFERENCES blocks (block_hash);
ALTER TABLE action_receipts
    ADD CONSTRAINT action_receipts_chunk_hash_fk FOREIGN KEY (chunk_hash) REFERENCES chunks (chunk_hash);
ALTER TABLE action_receipts
    ADD CONSTRAINT action_receipts_transaction_hash_fk FOREIGN KEY (originated_from_transaction_hash) REFERENCES transactions (transaction_hash);
CREATE INDEX CONCURRENTLY action_receipts_block_hash_idx ON action_receipts (block_hash);
-- CREATE INDEX CONCURRENTLY action_receipts_chunk_hash_idx ON action_receipts (chunk_hash);
CREATE INDEX CONCURRENTLY action_receipts_block_timestamp_idx ON action_receipts (block_timestamp);
CREATE INDEX CONCURRENTLY action_receipts_predecessor_idx ON action_receipts (predecessor_account_id);
CREATE INDEX CONCURRENTLY action_receipts_receiver_idx ON action_receipts (receiver_account_id);
CREATE INDEX CONCURRENTLY action_receipts_transaction_hash_idx ON action_receipts (originated_from_transaction_hash);
CREATE INDEX CONCURRENTLY action_receipts_signer_idx ON action_receipts (signer_account_id);

CREATE TABLE blocks
(
    block_height      numeric(20, 0) NOT NULL,
    block_hash        text           NOT NULL,
    prev_block_hash   text           NOT NULL,
    block_timestamp   numeric(20, 0) NOT NULL,
    total_supply      numeric(38, 0) NOT NULL,
--     todo next_block_gas_price? https://github.com/near/near-analytics/issues/19
    gas_price         numeric(38, 0) NOT NULL,
    author_account_id text           NOT NULL,
    PRIMARY KEY (block_hash)
);
CREATE INDEX CONCURRENTLY blocks_height_idx ON blocks (block_height);
-- CREATE INDEX CONCURRENTLY blocks_prev_hash_idx ON blocks (prev_block_hash);
CREATE INDEX CONCURRENTLY blocks_timestamp_idx ON blocks (block_timestamp);

CREATE TABLE chunks
(
    block_timestamp   numeric(20, 0) NOT NULL,
    block_hash        text           NOT NULL,
    chunk_hash        text           NOT NULL,
    index_in_block    integer        NOT NULL,
    signature         text           NOT NULL,
    gas_limit         numeric(20, 0) NOT NULL,
    gas_used          numeric(20, 0) NOT NULL,
    author_account_id text           NOT NULL,
    PRIMARY KEY (chunk_hash)
);
ALTER TABLE chunks
    ADD CONSTRAINT chunks_block_hash_fk FOREIGN KEY (block_hash) REFERENCES blocks (block_hash);
CREATE INDEX CONCURRENTLY chunks_block_timestamp_idx ON chunks (block_timestamp);
CREATE INDEX CONCURRENTLY chunks_block_hash_idx ON chunks (block_hash);

CREATE TABLE data_receipts
(
    receipt_id                       text           NOT NULL,
    block_hash                       text           NOT NULL,
    chunk_hash                       text           NOT NULL,
    block_timestamp                  numeric(20, 0) NOT NULL,
    chunk_index_in_block             integer        NOT NULL,
    receipt_index_in_chunk           integer        NOT NULL, -- goes both through action and data receipts
    predecessor_account_id           text           NOT NULL,
    receiver_account_id              text           NOT NULL,
    originated_from_transaction_hash text           NOT NULL,
    data_id                          text           NOT NULL,
    data                             bytea,
    PRIMARY KEY (receipt_id)
);
ALTER TABLE data_receipts
    ADD CONSTRAINT data_receipts_block_hash_fk FOREIGN KEY (block_hash) REFERENCES blocks (block_hash);
ALTER TABLE data_receipts
    ADD CONSTRAINT data_receipts_chunk_hash_fk FOREIGN KEY (chunk_hash) REFERENCES chunks (chunk_hash);
ALTER TABLE data_receipts
    ADD CONSTRAINT data_receipts_tx_hash_fk FOREIGN KEY (originated_from_transaction_hash) REFERENCES transactions (transaction_hash);
CREATE INDEX CONCURRENTLY data_receipts_block_hash_idx ON data_receipts (block_hash);
-- CREATE INDEX CONCURRENTLY data_receipts_chunk_hash_idx ON data_receipts (chunk_hash);
CREATE INDEX CONCURRENTLY data_receipts_block_timestamp_idx ON data_receipts (block_timestamp);
CREATE INDEX CONCURRENTLY data_receipts_predecessor_idx ON data_receipts (predecessor_account_id);
CREATE INDEX CONCURRENTLY data_receipts_receiver_idx ON data_receipts (receiver_account_id);
CREATE INDEX CONCURRENTLY data_receipts_transaction_hash_idx ON data_receipts (originated_from_transaction_hash);

CREATE TABLE execution_outcomes__receipts
(
    block_hash           text           NOT NULL,
    block_timestamp      numeric(20, 0) NOT NULL,
    executed_receipt_id  text           NOT NULL,
    produced_receipt_id  text           NOT NULL,
    chunk_index_in_block integer        NOT NULL,
    index_in_chunk       integer        NOT NULL,
    PRIMARY KEY (block_timestamp, chunk_index_in_block, index_in_chunk)
);
ALTER TABLE execution_outcomes__receipts
    ADD CONSTRAINT eo_receipts_block_hash_fk FOREIGN KEY (block_hash) REFERENCES blocks (block_hash);
ALTER TABLE execution_outcomes__receipts
    ADD CONSTRAINT eo_receipts_receipt_id_fk FOREIGN KEY (executed_receipt_id) REFERENCES execution_outcomes (receipt_id);
CREATE INDEX CONCURRENTLY execution_receipts_timestamp_idx ON execution_outcomes__receipts (block_timestamp);
CREATE INDEX CONCURRENTLY execution_receipts_produced_receipt_idx ON execution_outcomes__receipts (produced_receipt_id);

-- status options:
--      {
--         'UNKNOWN',
--         'FAILURE',
--         'SUCCESS_VALUE',
--         'SUCCESS_RECEIPT_ID'
--      }
-- todo we want to store more data for this table and maybe for the others
CREATE TABLE execution_outcomes
(
    receipt_id           text           NOT NULL,
    block_hash           text           NOT NULL,
    block_timestamp      numeric(20, 0) NOT NULL,
    chunk_index_in_block integer        NOT NULL,
    index_in_chunk       integer        NOT NULL,
    gas_burnt            numeric(20, 0) NOT NULL,
    tokens_burnt         numeric(38, 0) NOT NULL,
    executor_account_id  text           NOT NULL,
    status               text           NOT NULL,
    PRIMARY KEY (receipt_id)
);
ALTER TABLE execution_outcomes
    ADD CONSTRAINT execution_outcomes_block_hash_fk FOREIGN KEY (block_hash) REFERENCES blocks (block_hash);
ALTER TABLE execution_outcomes
    ADD CONSTRAINT execution_outcomes_receipt_id_fk FOREIGN KEY (receipt_id) REFERENCES action_receipts (receipt_id);
CREATE INDEX CONCURRENTLY execution_outcomes_block_timestamp_idx ON execution_outcomes (block_timestamp);
CREATE INDEX CONCURRENTLY execution_outcomes_block_hash_idx ON execution_outcomes (block_hash);
CREATE INDEX CONCURRENTLY execution_outcomes_status_idx ON execution_outcomes (status);

-- status options:
--      {
--         'UNKNOWN',
--         'FAILURE',
--         'SUCCESS_VALUE',
--         'SUCCESS_RECEIPT_ID'
--      }
CREATE TABLE transactions
(
    transaction_hash                text           NOT NULL,
    block_hash                      text           NOT NULL,
    chunk_hash                      text           NOT NULL,
    block_timestamp                 numeric(20, 0) NOT NULL,
    chunk_index_in_block            integer        NOT NULL,
    index_in_chunk                  integer        NOT NULL,
    signer_account_id               text           NOT NULL,
    signer_public_key               text           NOT NULL,
    nonce                           numeric(20, 0) NOT NULL,
    receiver_account_id             text           NOT NULL,
    signature                       text           NOT NULL,
    status                          text           NOT NULL,
    converted_into_receipt_id       text           NOT NULL,
    receipt_conversion_gas_burnt    numeric(20, 0),
    receipt_conversion_tokens_burnt numeric(38, 0),
    PRIMARY KEY (transaction_hash)
);
ALTER TABLE transactions
    ADD CONSTRAINT transactions_block_hash_fk FOREIGN KEY (block_hash) REFERENCES blocks (block_hash);
ALTER TABLE transactions
    ADD CONSTRAINT transactions_chunk_hash_fk FOREIGN KEY (chunk_hash) REFERENCES chunks (chunk_hash);
CREATE INDEX CONCURRENTLY transactions_receipt_id_idx ON transactions (converted_into_receipt_id);
CREATE INDEX CONCURRENTLY transactions_block_hash_idx ON transactions (block_hash);
CREATE INDEX CONCURRENTLY transactions_block_timestamp_idx ON transactions (block_timestamp);
-- CREATE INDEX CONCURRENTLY transactions_chunk_hash_idx ON transactions (chunk_hash);
CREATE INDEX CONCURRENTLY transactions_signer_idx ON transactions (signer_account_id);
-- CREATE INDEX CONCURRENTLY transactions_signer_public_key_idx ON transactions (signer_public_key);
CREATE INDEX CONCURRENTLY transactions_receiver_idx ON transactions (receiver_account_id);
-- CREATE INDEX CONCURRENTLY transactions_sorting_idx ON transactions (block_timestamp, chunk_index_in_block, index_in_chunk);

CREATE TABLE _blocks_to_rerun
(
    block_height numeric(20, 0) NOT NULL,
    PRIMARY KEY (block_height)
);

'''
'''--- redshift/REDSHIFT_NOTES.md ---
### How to create readonly user

```sql
create group readonly;
revoke create on schema public from group readonly;
grant usage on schema public to group readonly;
grant select on all tables in schema public to group readonly;
alter default privileges in schema public grant select on tables to group readonly;
create user public_readonly with password 'Password1';
alter group readonly add user public_readonly;
```

### How to connect Redshift to Aurora DB

1. There's no ready-to-go automated process to copy data from Aurora to Redshift
2. The instruction is close to https://aws.amazon.com/blogs/big-data/announcing-amazon-redshift-federated-querying-to-amazon-aurora-mysql-and-amazon-rds-for-mysql/
3. But it has some additional details. Notes for the future me:
    1. Our instances should be in the same VPC and subnet group (it's true for the current configuration)
    2. I need to create new secret (1), new policy (by raw json), new iam role for Redshift (2), add it to redshift cluster.
    3. The external schema could be created by the command

```sql
CREATE EXTERNAL SCHEMA apg
FROM POSTGRES
DATABASE 'indexer_mainnet'
URI 'take-me-from-redshift-reader-instance-page'
IAM_ROLE 'copy-me-from-(2)-arm-line'
SECRET_ARN 'copy-me-from-(1)';
```

There is some issues with data types.
https://docs.aws.amazon.com/redshift/latest/dg/federated-data-types.html

Issues:
- All the jsons and bytearrays will be cast to varchar(64000). TODO: check that we don't break binary data here.
- It's impossible to access any data from aurora table, if you have there `numeric(>38)`. Even if you don't access this bad column and only ask for the others. So, we have to alter table in aurora and change the types.

Then, the data should be transferred this way

```sql
-- Find the right timestamp in Aurora DB
select block_timestamp from blocks where block_height = 40000000; -- 1623683746564879190

-- Run these and similar insert statements in Redshift. Don't forget to add second border if you run it second time, Redshift doesn't have unique indexes
insert into chunks select * from apg.chunks where block_timestamp <= 1623683746564879190;
insert into data_receipts select * from apg.data_receipts where block_timestamp <= 1623683746564879190;
-- ...
```

We also may want to drop some data from Aurora.
I didn't dig into it too much, but in Postgres I'd prefer to create the other partition and drop the whole previous one instead of running `delete from table where timestamp < X`.
'''
'''--- redshift/migration.sql ---
-- All `text` fields now have type `varchar(64000)`
-- We spend 4 bytes on it + the length of the string, so I just took bigger limit
-- https://docs.aws.amazon.com/redshift/latest/dg/r_Character_types.html#r_Character_types-varchar-or-character-varying

-- update_reason options:
--     {
--         'TRANSACTION_PROCESSING',
--         'ACTION_RECEIPT_PROCESSING_STARTED',
--         'ACTION_RECEIPT_GAS_REWARD',
--         'RECEIPT_PROCESSING',
--         'POSTPONED_RECEIPT',
--         'UPDATED_DELAYED_RECEIPTS',
--         'VALIDATOR_ACCOUNTS_UPDATE',
--         'MIGRATION',
--         'RESHARDING'
--     }
CREATE TABLE account_changes
(
    account_id                 varchar(64000) NOT NULL,
    block_timestamp            numeric(20, 0) NOT NULL,
    block_hash                 varchar(64000) NOT NULL,
    caused_by_transaction_hash varchar(64000),
    caused_by_receipt_id       varchar(64000),
    update_reason              varchar(64000) NOT NULL,
    nonstaked_balance          numeric(38, 0) NOT NULL,
    staked_balance             numeric(38, 0) NOT NULL,
    storage_usage              numeric(20, 0) NOT NULL,
    chunk_index_in_block       integer        NOT NULL,
    index_in_chunk             integer        NOT NULL
);

-- action_kind options:
--      {
--         'CREATE_ACCOUNT',
--         'DEPLOY_CONTRACT',
--         'FUNCTION_CALL',
--         'TRANSFER',
--         'STAKE',
--         'ADD_KEY',
--         'DELETE_KEY',
--         'DELETE_ACCOUNT'
--      }
CREATE TABLE action_receipts__actions
(
    block_hash             varchar(64000) NOT NULL,
    block_timestamp        numeric(20, 0) NOT NULL,
    receipt_id             varchar(64000) NOT NULL,
    action_kind            varchar(64000) NOT NULL,
    -- https://docs.aws.amazon.com/redshift/latest/dg/json-functions.html
    -- https://docs.aws.amazon.com/redshift/latest/dg/super-overview.html
    -- todo check that redshift can load to super (docs said that the default is varchar(64000))
    args                   super          NOT NULL,
    predecessor_account_id varchar(64000) NOT NULL,
    receiver_account_id    varchar(64000) NOT NULL,
    chunk_index_in_block   integer        NOT NULL,
    index_in_chunk         integer        NOT NULL
);

CREATE TABLE action_receipts__outputs
(
    block_hash           varchar(64000) NOT NULL,
    block_timestamp      numeric(20, 0) NOT NULL,
    receipt_id           varchar(64000) NOT NULL,
    output_data_id       varchar(64000) NOT NULL,
    receiver_account_id  varchar(64000) NOT NULL,
    chunk_index_in_block integer        NOT NULL,
    index_in_chunk       integer        NOT NULL
);

CREATE TABLE action_receipts
(
    receipt_id                       varchar(64000) NOT NULL,
    block_hash                       varchar(64000) NOT NULL,
    chunk_hash                       varchar(64000) NOT NULL,
    block_timestamp                  numeric(20, 0) NOT NULL,
    chunk_index_in_block             integer        NOT NULL,
    receipt_index_in_chunk           integer        NOT NULL, -- goes both through action and data receipts
    predecessor_account_id           varchar(64000) NOT NULL,
    receiver_account_id              varchar(64000) NOT NULL,
    originated_from_transaction_hash varchar(64000) NOT NULL,
    signer_account_id                varchar(64000) NOT NULL,
    signer_public_key                varchar(64000) NOT NULL,
    gas_price                        numeric(38, 0) NOT NULL
);

CREATE TABLE blocks
(
    block_height      numeric(20, 0) NOT NULL,
    block_hash        varchar(64000) NOT NULL,
    prev_block_hash   varchar(64000) NOT NULL,
    block_timestamp   numeric(20, 0) NOT NULL,
    total_supply      numeric(38, 0) NOT NULL,
    gas_price         numeric(38, 0) NOT NULL,
    author_account_id varchar(64000) NOT NULL
);

CREATE TABLE chunks
(
    block_timestamp   numeric(20, 0) NOT NULL,
    block_hash        varchar(64000) NOT NULL,
    chunk_hash        varchar(64000) NOT NULL,
    index_in_block    integer        NOT NULL,
    signature         varchar(64000) NOT NULL,
    gas_limit         numeric(20, 0) NOT NULL,
    gas_used          numeric(20, 0) NOT NULL,
    author_account_id varchar(64000) NOT NULL
);

CREATE TABLE data_receipts
(
    receipt_id                       varchar(64000) NOT NULL,
    block_hash                       varchar(64000) NOT NULL,
    chunk_hash                       varchar(64000) NOT NULL,
    block_timestamp                  numeric(20, 0) NOT NULL,
    chunk_index_in_block             integer        NOT NULL,
    receipt_index_in_chunk           integer        NOT NULL, -- goes both through action and data receipts
    predecessor_account_id           varchar(64000) NOT NULL,
    receiver_account_id              varchar(64000) NOT NULL,
    originated_from_transaction_hash varchar(64000) NOT NULL,
    data_id                          varchar(64000) NOT NULL,
    -- todo it's the only working way to copy data from aurora i've found. need to check that we do not corrupt binary data in varchar type
    data                             varbyte(64000)--varchar(64000)
);

CREATE TABLE execution_outcomes__receipts
(
    block_hash           varchar(64000) NOT NULL,
    block_timestamp      numeric(20, 0) NOT NULL,
    executed_receipt_id  varchar(64000) NOT NULL,
    produced_receipt_id  varchar(64000) NOT NULL,
    chunk_index_in_block integer        NOT NULL,
    index_in_chunk       integer        NOT NULL
);

-- status options:
--      {
--         'UNKNOWN',
--         'FAILURE',
--         'SUCCESS_VALUE',
--         'SUCCESS_RECEIPT_ID'
--      }
CREATE TABLE execution_outcomes
(
    receipt_id           varchar(64000) NOT NULL,
    block_hash           varchar(64000) NOT NULL,
    block_timestamp      numeric(20, 0) NOT NULL,
    chunk_index_in_block integer        NOT NULL,
    index_in_chunk       integer        NOT NULL,
    gas_burnt            numeric(20, 0) NOT NULL,
    tokens_burnt         numeric(38, 0) NOT NULL,
    executor_account_id  varchar(64000) NOT NULL,
    status               varchar(64000) NOT NULL
);

-- status options:
--      {
--         'UNKNOWN',
--         'FAILURE',
--         'SUCCESS_VALUE',
--         'SUCCESS_RECEIPT_ID'
--      }
CREATE TABLE transactions
(
    transaction_hash                varchar(64000) NOT NULL,
    block_hash                      varchar(64000) NOT NULL,
    chunk_hash                      varchar(64000) NOT NULL,
    block_timestamp                 numeric(20, 0) NOT NULL,
    chunk_index_in_block            integer        NOT NULL,
    index_in_chunk                  integer        NOT NULL,
    signer_account_id               varchar(64000) NOT NULL,
    signer_public_key               varchar(64000) NOT NULL,
    nonce                           numeric(20, 0) NOT NULL,
    receiver_account_id             varchar(64000) NOT NULL,
    signature                       varchar(64000) NOT NULL,
    status                          varchar(64000) NOT NULL,
    converted_into_receipt_id       varchar(64000) NOT NULL,
    receipt_conversion_gas_burnt    numeric(20, 0),
    receipt_conversion_tokens_burnt numeric(38, 0)
);

CREATE TABLE _last_successful_load
(
    block_height    numeric(20, 0) NOT NULL,
    block_timestamp numeric(20, 0) NOT NULL
);
'''
'''--- src/configs.rs ---
use clap::Parser;

/// NEAR Indexer for Explorer
/// Watches for stream of blocks from the chain
#[derive(Parser, Debug)]
#[clap(
    version,
    author,
    about,
    disable_help_subcommand(true),
    propagate_version(true),
    next_line_help(true)
)]
pub(crate) struct Opts {
    /// Enabled Indexer for Explorer debug level of logs
    #[clap(long)]
    pub debug: bool,
    // todo fix wording
    /// Switches indexer to non-strict mode (skips Receipts without parent Transaction hash, puts such block_height into special table)
    #[clap(long)]
    pub non_strict_mode: bool,
    // todo
    // /// Store initial data from genesis like Accounts, AccessKeys
    // #[clap(long)]
    // pub store_genesis: bool,
    /// AWS S3 bucket name to get the stream from
    #[clap(long)]
    pub s3_bucket_name: String,
    /// AWS S3 bucket region
    #[clap(long)]
    pub s3_region_name: String,
    /// Block height to start the stream from. If None, start from interruption
    #[clap(long, short)]
    pub start_block_height: Option<u64>,
}

'''
'''--- src/db_adapters/account_changes.rs ---
use futures::future::try_join_all;

use crate::models;

pub(crate) async fn store_account_changes(
    pool: &sqlx::Pool<sqlx::Postgres>,
    shards: &[near_indexer_primitives::IndexerShard],
    block_hash: &near_indexer_primitives::CryptoHash,
    block_timestamp: u64,
) -> anyhow::Result<()> {
    let futures = shards.iter().map(|shard| {
        store_account_changes_for_chunk(
            pool,
            &shard.state_changes,
            block_hash,
            block_timestamp,
            shard.shard_id,
        )
    });

    try_join_all(futures).await.map(|_| ())
}

async fn store_account_changes_for_chunk(
    pool: &sqlx::Pool<sqlx::Postgres>,
    state_changes: &near_indexer_primitives::views::StateChangesView,
    block_hash: &near_indexer_primitives::CryptoHash,
    block_timestamp: u64,
    shard_id: near_indexer_primitives::types::ShardId,
) -> anyhow::Result<()> {
    models::chunked_insert(
        pool,
        &state_changes
            .iter()
            .filter_map(|state_change| {
                models::AccountChange::from_state_change_with_cause(
                    state_change,
                    block_hash,
                    block_timestamp,
                    shard_id as i32,
                    // we fill it later because we can't enumerate before filtering finishes
                    0,
                )
            })
            .enumerate()
            .map(|(i, mut account_change)| {
                account_change.index_in_chunk = i as i32;
                account_change
            })
            .collect::<Vec<models::AccountChange>>(),
    )
    .await?;

    Ok(())
}

'''
'''--- src/db_adapters/blocks.rs ---
use crate::models;

pub(crate) async fn store_block(
    pool: &sqlx::Pool<sqlx::Postgres>,
    block: &near_indexer_primitives::views::BlockView,
) -> anyhow::Result<()> {
    models::chunked_insert(pool, &vec![models::Block::from_block_view(block)]).await?;
    Ok(())
}

'''
'''--- src/db_adapters/chunks.rs ---
use crate::models;

pub(crate) async fn store_chunks(
    pool: &sqlx::Pool<sqlx::Postgres>,
    shards: &[near_indexer_primitives::IndexerShard],
    block_hash: &near_indexer_primitives::CryptoHash,
    block_timestamp: u64,
) -> anyhow::Result<()> {
    models::chunked_insert(
        pool,
        &shards
            .iter()
            .filter_map(|shard| {
                shard
                    .chunk
                    .as_ref()
                    .map(|chunk| models::Chunk::from_chunk_view(chunk, block_hash, block_timestamp))
            })
            .collect::<Vec<models::Chunk>>(),
    )
    .await?;

    Ok(())
}

'''
'''--- src/db_adapters/execution_outcomes.rs ---
use cached::Cached;
use futures::future::try_join_all;

use crate::models;

pub(crate) async fn store_execution_outcomes(
    pool: &sqlx::Pool<sqlx::Postgres>,
    shards: &[near_indexer_primitives::IndexerShard],
    block_hash: &near_indexer_primitives::CryptoHash,
    block_timestamp: u64,
    receipts_cache: crate::ReceiptsCache,
) -> anyhow::Result<()> {
    let futures = shards.iter().map(|shard| {
        store_execution_outcomes_for_chunk(
            pool,
            &shard.receipt_execution_outcomes,
            shard.shard_id,
            block_hash,
            block_timestamp,
            receipts_cache.clone(),
        )
    });

    try_join_all(futures).await.map(|_| ())
}

/// Saves ExecutionOutcome to database and then saves ExecutionOutcomesReceipts
pub async fn store_execution_outcomes_for_chunk(
    pool: &sqlx::Pool<sqlx::Postgres>,
    execution_outcomes: &[near_indexer_primitives::IndexerExecutionOutcomeWithReceipt],
    shard_id: near_indexer_primitives::types::ShardId,
    block_hash: &near_indexer_primitives::CryptoHash,
    block_timestamp: u64,
    receipts_cache: crate::ReceiptsCache,
) -> anyhow::Result<()> {
    models::chunked_insert(
        pool,
        &execution_outcomes
            .iter()
            .enumerate()
            .map(|(index_in_chunk, outcome)| {
                models::ExecutionOutcome::from_execution_outcome(
                    &outcome.execution_outcome,
                    index_in_chunk as i32,
                    block_timestamp,
                    shard_id,
                )
            })
            .collect::<Vec<models::ExecutionOutcome>>(),
    )
    .await?;

    let mut outcome_receipt_models: Vec<models::ExecutionOutcomeReceipt> = vec![];
    let mut receipts_cache_lock = receipts_cache.lock().await;
    for outcome in execution_outcomes {
        // Trying to take the parent Transaction hash for the Receipt from ReceiptsCache
        // remove it from cache once found as it is not expected to observe the Receipt for
        // second time
        let parent_transaction_hash = receipts_cache_lock.cache_remove(
            &crate::ReceiptOrDataId::ReceiptId(outcome.execution_outcome.id),
        );

        outcome_receipt_models.extend(outcome.execution_outcome.outcome.receipt_ids.iter().map(
            |receipt_id| {
                // if we have `parent_transaction_hash` from cache, then we put all "produced" Receipt IDs
                // as key and `parent_transaction_hash` as value, so the Receipts from one of the next blocks
                // could find their parents in cache
                if let Some(transaction_hash) = &parent_transaction_hash {
                    receipts_cache_lock.cache_set(
                        crate::ReceiptOrDataId::ReceiptId(*receipt_id),
                        transaction_hash.clone(),
                    );
                }

                models::ExecutionOutcomeReceipt {
                    block_hash: block_hash.to_string(),
                    block_timestamp: block_timestamp.into(),
                    executed_receipt_id: outcome.execution_outcome.id.to_string(),
                    produced_receipt_id: receipt_id.to_string(),
                    chunk_index_in_block: shard_id as i32,
                    // we fill it later because we need flatmap result
                    index_in_chunk: 0,
                }
            },
        ));
    }
    drop(receipts_cache_lock);

    outcome_receipt_models
        .iter_mut()
        .enumerate()
        .for_each(|(i, execution_outcomes_receipt)| {
            execution_outcomes_receipt.index_in_chunk = i as i32;
        });

    models::chunked_insert(pool, &outcome_receipt_models).await?;

    Ok(())
}

'''
'''--- src/db_adapters/mod.rs ---
pub(crate) mod account_changes;
pub(crate) mod blocks;
pub(crate) mod chunks;
pub(crate) mod execution_outcomes;
pub(crate) mod receipts;
pub(crate) mod transactions;

pub(crate) const CHUNK_SIZE_FOR_BATCH_INSERT: usize = 100;
pub(crate) const RETRY_COUNT: usize = 10;

'''
'''--- src/db_adapters/receipts.rs ---
use std::collections::HashMap;
use std::str::FromStr;

use bigdecimal::BigDecimal;
use cached::Cached;
use futures::future::try_join_all;
use futures::try_join;
use itertools::{Either, Itertools};
use sqlx::Arguments;
use sqlx::Row;

use crate::models;

/// Saves receipts to database
pub(crate) async fn store_receipts(
    pool: &sqlx::Pool<sqlx::Postgres>,
    strict_mode: bool,
    shards: &[near_indexer_primitives::IndexerShard],
    block_header: &near_indexer_primitives::views::BlockHeaderView,
    receipts_cache: crate::ReceiptsCache,
) -> anyhow::Result<()> {
    let futures = shards
        .iter()
        .filter_map(|shard| shard.chunk.as_ref())
        .filter(|chunk| !chunk.receipts.is_empty())
        .map(|chunk| {
            store_chunk_receipts(
                pool,
                strict_mode,
                &chunk.receipts,
                block_header,
                &chunk.header,
                std::sync::Arc::clone(&receipts_cache),
            )
        });

    try_join_all(futures).await.map(|_| ())
}

async fn store_chunk_receipts(
    pool: &sqlx::Pool<sqlx::Postgres>,
    strict_mode: bool,
    receipts: &[near_indexer_primitives::views::ReceiptView],
    block_header: &near_indexer_primitives::views::BlockHeaderView,
    chunk_header: &near_indexer_primitives::views::ChunkHeaderView,
    receipts_cache: crate::ReceiptsCache,
) -> anyhow::Result<()> {
    let tx_hashes_for_receipts: HashMap<
        crate::ReceiptOrDataId,
        crate::ParentTransactionHashString,
    > = find_tx_hashes_for_receipts(
        pool,
        strict_mode,
        receipts.to_vec(),
        block_header.height,
        receipts_cache.clone(),
    )
    .await?;

    // At the moment we can observe output data in the Receipt it's impossible to know
    // the Receipt Id of that Data Receipt. That's why we insert the pair DataId<>ParentTransactionHash
    // to ReceiptsCache
    let mut receipts_cache_lock = receipts_cache.lock().await;
    for receipt in receipts {
        if let near_indexer_primitives::views::ReceiptEnumView::Action {
            output_data_receivers,
            ..
        } = &receipt.receipt
        {
            if !output_data_receivers.is_empty() {
                if let Some(transaction_hash) = tx_hashes_for_receipts
                    .get(&crate::ReceiptOrDataId::ReceiptId(receipt.receipt_id))
                {
                    for data_receiver in output_data_receivers {
                        receipts_cache_lock.cache_set(
                            crate::ReceiptOrDataId::DataId(data_receiver.data_id),
                            transaction_hash.clone(),
                        );
                    }
                }
            }
        }
    }
    // releasing the lock
    drop(receipts_cache_lock);

    // enumeration goes through all the receipts
    let enumerated_receipts_with_parent_tx: Vec<(
        usize,
        &String,
        &near_indexer_primitives::views::ReceiptView,
    )> = receipts
        .iter()
        .enumerate()
        .filter_map(|(index, receipt)| match receipt.receipt {
            near_indexer_primitives::views::ReceiptEnumView::Action { .. } => {
                tx_hashes_for_receipts
                    .get(&crate::ReceiptOrDataId::ReceiptId(receipt.receipt_id))
                    .map(|tx| (index, tx, receipt))
            }
            near_indexer_primitives::views::ReceiptEnumView::Data { data_id, .. } => {
                tx_hashes_for_receipts
                    .get(&crate::ReceiptOrDataId::DataId(data_id))
                    .map(|tx| (index, tx, receipt))
            }
        })
        .collect();
    if strict_mode && receipts.len() != enumerated_receipts_with_parent_tx.len() {
        // todo maybe it's better to collect blocks for rerun here
        return Err(anyhow::anyhow!(
            "Some tx hashes were not found at block {}",
            block_header.height
        ));
    }

    let (action_receipts, data_receipts) = enumerated_receipts_with_parent_tx.iter().partition_map(
        |(index, tx, receipt)| match receipt.receipt {
            near_indexer_primitives::views::ReceiptEnumView::Action { .. } => {
                Either::Left((*index, *tx, *receipt))
            }
            near_indexer_primitives::views::ReceiptEnumView::Data { .. } => {
                Either::Right((*index, *tx, *receipt))
            }
        },
    );

    let process_receipt_actions_future =
        store_receipt_actions(pool, action_receipts, block_header, chunk_header);

    let process_receipt_data_future =
        store_data_receipts(pool, data_receipts, block_header, chunk_header);

    try_join!(process_receipt_actions_future, process_receipt_data_future)?;
    Ok(())
}

/// Looks for already created parent transaction hash for given receipts
async fn find_tx_hashes_for_receipts(
    pool: &sqlx::Pool<sqlx::Postgres>,
    strict_mode: bool,
    mut receipts: Vec<near_indexer_primitives::views::ReceiptView>,
    block_height: u64,
    receipts_cache: crate::ReceiptsCache,
) -> anyhow::Result<HashMap<crate::ReceiptOrDataId, crate::ParentTransactionHashString>> {
    let mut tx_hashes_for_receipts: HashMap<
        crate::ReceiptOrDataId,
        crate::ParentTransactionHashString,
    > = HashMap::new();

    let mut receipts_cache_lock = receipts_cache.lock().await;
    // add receipt-transaction pairs from the cache to the response
    tx_hashes_for_receipts.extend(receipts.iter().filter_map(|receipt| {
        match receipt.receipt {
            near_indexer_primitives::views::ReceiptEnumView::Action { .. } => receipts_cache_lock
                .cache_get(&crate::ReceiptOrDataId::ReceiptId(receipt.receipt_id))
                .map(|parent_transaction_hash| {
                    (
                        crate::ReceiptOrDataId::ReceiptId(receipt.receipt_id),
                        parent_transaction_hash.clone(),
                    )
                }),
            near_indexer_primitives::views::ReceiptEnumView::Data { data_id, .. } => {
                // Pair DataId:ParentTransactionHash won't be used after this moment
                // We want to clean it up to prevent our cache from growing
                receipts_cache_lock
                    .cache_remove(&crate::ReceiptOrDataId::DataId(data_id))
                    .map(|parent_transaction_hash| {
                        (
                            crate::ReceiptOrDataId::DataId(data_id),
                            parent_transaction_hash,
                        )
                    })
            }
        }
    }));
    // releasing the lock
    drop(receipts_cache_lock);

    // discard the Receipts already in cache from the attempts to search
    receipts.retain(|r| match r.receipt {
        near_indexer_primitives::views::ReceiptEnumView::Data { data_id, .. } => {
            !tx_hashes_for_receipts.contains_key(&crate::ReceiptOrDataId::DataId(data_id))
        }
        near_indexer_primitives::views::ReceiptEnumView::Action { .. } => {
            !tx_hashes_for_receipts.contains_key(&crate::ReceiptOrDataId::ReceiptId(r.receipt_id))
        }
    });

    if receipts.is_empty() {
        return Ok(tx_hashes_for_receipts);
    }

    eprintln!(
        "Looking for parent transaction hash in database for {} receipts", // {:#?}",
        &receipts.len(),
        //  &receipts,
    );

    let (action_receipt_ids, data_ids): (Vec<String>, Vec<String>) =
        receipts.iter().partition_map(|r| match r.receipt {
            near_indexer_primitives::views::ReceiptEnumView::Action { .. } => {
                Either::Left(r.receipt_id.to_string())
            }
            near_indexer_primitives::views::ReceiptEnumView::Data { data_id, .. } => {
                Either::Right(data_id.to_string())
            }
        });

    if !data_ids.is_empty() {
        let tx_hashes_for_data_receipts =
            find_transaction_hashes_for_data_receipts(pool, &data_ids).await?;
        tx_hashes_for_receipts.extend(tx_hashes_for_data_receipts.clone());

        receipts.retain(|r| match r.receipt {
            near_indexer_primitives::views::ReceiptEnumView::Action { .. } => true,
            near_indexer_primitives::views::ReceiptEnumView::Data { data_id, .. } => {
                !tx_hashes_for_data_receipts.contains_key(&crate::ReceiptOrDataId::DataId(data_id))
            }
        });
        if receipts.is_empty() {
            return Ok(tx_hashes_for_receipts);
        }
    }

    if !action_receipt_ids.is_empty() {
        let tx_hashes_for_receipts_via_outcomes =
            find_transaction_hashes_for_receipts_via_outcomes(pool, &action_receipt_ids).await?;
        tx_hashes_for_receipts.extend(tx_hashes_for_receipts_via_outcomes.clone());

        receipts.retain(|r| {
            !tx_hashes_for_receipts_via_outcomes
                .contains_key(&crate::ReceiptOrDataId::ReceiptId(r.receipt_id))
        });
        if receipts.is_empty() {
            return Ok(tx_hashes_for_receipts);
        }

        let tx_hashes_for_receipt_via_transactions =
            find_transaction_hashes_for_receipt_via_transactions(pool, &action_receipt_ids).await?;
        tx_hashes_for_receipts.extend(tx_hashes_for_receipt_via_transactions.clone());

        receipts.retain(|r| {
            !tx_hashes_for_receipt_via_transactions
                .contains_key(&crate::ReceiptOrDataId::ReceiptId(r.receipt_id))
        });
    }

    if !receipts.is_empty() {
        eprintln!(
            "The block {} has {} receipt(s) we still need to put to the DB later: {:?}",
            block_height,
            receipts.len(),
            receipts
                .iter()
                .map(|r| r.receipt_id.to_string())
                .collect::<Vec<String>>()
        );
        if strict_mode {
            panic!("all the transactions should be found by this place");
        }

        let mut args = sqlx::postgres::PgArguments::default();
        args.add(BigDecimal::from(block_height));
        let query = "INSERT INTO _blocks_to_rerun VALUES ($1) ON CONFLICT DO NOTHING";
        sqlx::query_with(query, args).execute(pool).await?;
    }

    Ok(tx_hashes_for_receipts)
}

async fn find_transaction_hashes_for_data_receipts(
    pool: &sqlx::Pool<sqlx::Postgres>,
    data_ids: &[String],
) -> anyhow::Result<HashMap<crate::ReceiptOrDataId, crate::ParentTransactionHashString>> {
    let query = "SELECT action_receipts__outputs.output_data_id, action_receipts.originated_from_transaction_hash
                        FROM action_receipts__outputs JOIN action_receipts ON action_receipts__outputs.receipt_id = action_receipts.receipt_id
                        WHERE action_receipts__outputs.output_data_id IN ".to_owned() + &models::create_placeholder(&mut 1,data_ids.len())?;

    let res = models::select_retry_or_panic(pool, &query, data_ids).await?;
    Ok(res
        .iter()
        .map(|q| (q.get(0), q.get(1)))
        .map(
            |(data_id_string, transaction_hash_string): (String, String)| {
                (
                    crate::ReceiptOrDataId::DataId(
                        near_indexer_primitives::CryptoHash::from_str(&data_id_string)
                            .expect("Failed to convert String to CryptoHash"),
                    ),
                    transaction_hash_string,
                )
            },
        )
        .collect())
}

async fn find_transaction_hashes_for_receipts_via_outcomes(
    pool: &sqlx::Pool<sqlx::Postgres>,
    action_receipt_ids: &[String],
) -> anyhow::Result<HashMap<crate::ReceiptOrDataId, crate::ParentTransactionHashString>> {
    let query = "SELECT execution_outcomes__receipts.produced_receipt_id, action_receipts.originated_from_transaction_hash
                        FROM execution_outcomes__receipts JOIN action_receipts ON execution_outcomes__receipts.executed_receipt_id = action_receipts.receipt_id
                        WHERE execution_outcomes__receipts.produced_receipt_id IN ".to_owned() + &models::create_placeholder(&mut 1,action_receipt_ids.len())?;

    let res = models::select_retry_or_panic(pool, &query, action_receipt_ids).await?;
    Ok(res
        .iter()
        .map(|q| (q.get(0), q.get(1)))
        .map(
            |(receipt_id_string, transaction_hash_string): (String, String)| {
                (
                    crate::ReceiptOrDataId::ReceiptId(
                        near_indexer_primitives::CryptoHash::from_str(&receipt_id_string)
                            .expect("Failed to convert String to CryptoHash"),
                    ),
                    transaction_hash_string,
                )
            },
        )
        .collect())
}

async fn find_transaction_hashes_for_receipt_via_transactions(
    pool: &sqlx::Pool<sqlx::Postgres>,
    action_receipt_ids: &[String],
) -> anyhow::Result<HashMap<crate::ReceiptOrDataId, crate::ParentTransactionHashString>> {
    let query = "SELECT converted_into_receipt_id, transaction_hash
                        FROM transactions
                        WHERE converted_into_receipt_id IN "
        .to_owned()
        + &models::create_placeholder(&mut 1, action_receipt_ids.len())?;

    let res = models::select_retry_or_panic(pool, &query, action_receipt_ids).await?;
    Ok(res
        .iter()
        .map(|q| (q.get(0), q.get(1)))
        .map(
            |(receipt_id_string, transaction_hash_string): (String, String)| {
                (
                    crate::ReceiptOrDataId::ReceiptId(
                        near_indexer_primitives::CryptoHash::from_str(&receipt_id_string)
                            .expect("Failed to convert String to CryptoHash"),
                    ),
                    transaction_hash_string,
                )
            },
        )
        .collect())
}

async fn store_receipt_actions(
    pool: &sqlx::Pool<sqlx::Postgres>,
    receipts: Vec<(usize, &String, &near_indexer_primitives::views::ReceiptView)>,
    block_header: &near_indexer_primitives::views::BlockHeaderView,
    chunk_header: &near_indexer_primitives::views::ChunkHeaderView,
) -> anyhow::Result<()> {
    let receipt_actions: Vec<models::ActionReceipt> = receipts
        .iter()
        .filter_map(|(index, tx, receipt)| {
            models::ActionReceipt::try_from_action_receipt_view(
                *receipt,
                &block_header.hash,
                *tx,
                chunk_header,
                *index as i32,
                block_header.timestamp,
            )
            .ok()
        })
        .collect();

    let receipt_action_actions: Vec<models::ActionReceiptAction> = receipts
        .iter()
        .filter_map(|(_, _, receipt)| {
            if let near_indexer_primitives::views::ReceiptEnumView::Action { actions, .. } =
                &receipt.receipt
            {
                Some(actions.iter().map(move |action| {
                    models::ActionReceiptAction::from_action_view(
                        receipt.receipt_id.to_string(),
                        action,
                        receipt.predecessor_id.to_string(),
                        receipt.receiver_id.to_string(),
                        block_header,
                        chunk_header.shard_id as i32,
                        // we fill it later because we can't enumerate before filtering finishes
                        0,
                    )
                }))
            } else {
                None
            }
        })
        .flatten()
        .enumerate()
        .map(|(i, mut action)| {
            action.index_in_chunk = i as i32;
            action
        })
        .collect();

    let receipt_action_output_data: Vec<models::ActionReceiptsOutput> = receipts
        .iter()
        .filter_map(|(_, _, receipt)| {
            if let near_indexer_primitives::views::ReceiptEnumView::Action {
                output_data_receivers,
                ..
            } = &receipt.receipt
            {
                Some(output_data_receivers.iter().map(move |receiver| {
                    models::ActionReceiptsOutput::from_data_receiver(
                        receipt.receipt_id.to_string(),
                        receiver,
                        &block_header.hash,
                        block_header.timestamp,
                        chunk_header.shard_id as i32,
                        // we fill it later because we can't enumerate before filtering finishes
                        0,
                    )
                }))
            } else {
                None
            }
        })
        .flatten()
        .enumerate()
        .map(|(i, mut output)| {
            output.index_in_chunk = i as i32;
            output
        })
        .collect();

    // Next 2 tables depend on action_receipts, so we have to wait for it at first
    models::chunked_insert(pool, &receipt_actions).await?;
    try_join!(
        models::chunked_insert(pool, &receipt_action_actions),
        models::chunked_insert(pool, &receipt_action_output_data),
    )?;

    Ok(())
}

async fn store_data_receipts(
    pool: &sqlx::Pool<sqlx::Postgres>,
    receipts: Vec<(usize, &String, &near_indexer_primitives::views::ReceiptView)>,
    block_header: &near_indexer_primitives::views::BlockHeaderView,
    chunk_header: &near_indexer_primitives::views::ChunkHeaderView,
) -> anyhow::Result<()> {
    models::chunked_insert(
        pool,
        &receipts
            .iter()
            .filter_map(|(index, tx, receipt)| {
                models::DataReceipt::try_from_data_receipt_view(
                    receipt,
                    &block_header.hash,
                    tx,
                    chunk_header,
                    *index as i32,
                    block_header.timestamp,
                )
                .ok()
            })
            .collect::<Vec<models::DataReceipt>>(),
    )
    .await
}

'''
'''--- src/db_adapters/transactions.rs ---
use cached::Cached;
use futures::future::try_join_all;

use crate::models;

pub(crate) async fn store_transactions(
    pool: &sqlx::Pool<sqlx::Postgres>,
    shards: &[near_indexer_primitives::IndexerShard],
    block_hash: &near_indexer_primitives::CryptoHash,
    block_timestamp: u64,
    receipts_cache: crate::ReceiptsCache,
) -> anyhow::Result<()> {
    let tx_futures = shards
        .iter()
        .filter_map(|shard| shard.chunk.as_ref())
        .filter(|chunk| !chunk.transactions.is_empty())
        .map(|chunk| {
            store_chunk_transactions(
                pool,
                &chunk.transactions,
                block_hash,
                block_timestamp,
                &chunk.header,
                std::sync::Arc::clone(&receipts_cache),
            )
        });

    try_join_all(tx_futures).await?;
    Ok(())
}

async fn store_chunk_transactions(
    pool: &sqlx::Pool<sqlx::Postgres>,
    transactions: &[near_indexer_primitives::IndexerTransactionWithOutcome],
    block_hash: &near_indexer_primitives::CryptoHash,
    block_timestamp: u64,
    chunk_view: &near_indexer_primitives::views::ChunkHeaderView,
    receipts_cache: crate::ReceiptsCache,
) -> anyhow::Result<()> {
    let mut receipts_cache_lock = receipts_cache.lock().await;
    let transaction_models = transactions
        .iter()
        .enumerate()
        .map(|(i, transaction)| {
            let converted_into_receipt_id = transaction
                .outcome
                .execution_outcome
                .outcome
                .receipt_ids
                .first()
                .expect("`receipt_ids` must contain one Receipt Id");

            // Save this Transaction hash to ReceiptsCache
            // we use the Receipt ID to which this transaction was converted
            // and the Transaction hash as a value.
            // Later, while Receipt will be looking for a parent Transaction hash
            // it will be able to find it in the ReceiptsCache
            receipts_cache_lock.cache_set(
                crate::ReceiptOrDataId::ReceiptId(*converted_into_receipt_id),
                transaction.transaction.hash.to_string(),
            );

            models::Transaction::from_indexer_transaction(
                transaction,
                &transaction.transaction.hash.to_string(),
                &converted_into_receipt_id.to_string(),
                block_hash,
                block_timestamp,
                chunk_view,
                i as i32,
            )
        })
        .collect::<Vec<models::Transaction>>();
    drop(receipts_cache_lock);

    models::chunked_insert(pool, &transaction_models).await?;

    Ok(())
}

'''
'''--- src/lib.rs ---
use proc_macro::TokenStream;
use quote::quote;
use syn::{parse_macro_input, ItemStruct};

#[proc_macro_derive(FieldCount)]
pub fn derive_field_count(input: TokenStream) -> TokenStream {
    let input = parse_macro_input!(input as ItemStruct);

    let name = &input.ident;
    let (impl_generics, ty_generics, where_clause) = input.generics.split_for_impl();
    let field_count = input.fields.iter().count();

    let output = quote! {
        impl #impl_generics FieldCount for #name #ty_generics #where_clause {
            fn field_count() -> usize {
                #field_count
            }
        }
    };

    TokenStream::from(output)
}

'''
'''--- src/main.rs ---
use cached::SizedCache;
use clap::Parser;
use dotenv::dotenv;
use futures::{try_join, StreamExt};
use std::env;
use tokio::sync::Mutex;
use tracing_subscriber::EnvFilter;

use crate::configs::Opts;

mod configs;
mod db_adapters;
mod models;

// Categories for logging
// TODO naming
pub(crate) const INDEXER: &str = "indexer";

const INTERVAL: std::time::Duration = std::time::Duration::from_millis(100);
const MAX_DELAY_TIME: std::time::Duration = std::time::Duration::from_secs(120);

#[derive(Clone, Hash, PartialEq, Eq, Debug)]
pub enum ReceiptOrDataId {
    ReceiptId(near_indexer_primitives::CryptoHash),
    DataId(near_indexer_primitives::CryptoHash),
}
// Creating type aliases to make HashMap types for cache more explicit
pub type ParentTransactionHashString = String;
// Introducing a simple cache for Receipts to find their parent Transactions without
// touching the database
// The key is ReceiptID
// The value is TransactionHash (the very parent of the Receipt)
pub type ReceiptsCache =
    std::sync::Arc<Mutex<SizedCache<ReceiptOrDataId, ParentTransactionHashString>>>;

#[tokio::main]
async fn main() -> anyhow::Result<()> {
    dotenv().ok();

    let opts: Opts = Opts::parse();
    let pool = sqlx::PgPool::connect(&env::var("DATABASE_URL")?).await?;
    let config = near_lake_framework::LakeConfig {
        s3_config: None,
        s3_bucket_name: opts.s3_bucket_name.clone(),
        s3_region_name: opts.s3_region_name.clone(),
        start_block_height: opts.start_block_height.unwrap(),
    };
    init_tracing();

    let stream = near_lake_framework::streamer(config);

    // We want to prevent unnecessary SELECT queries to the database to find
    // the Transaction hash for the Receipt.
    // Later we need to find the Receipt which is a parent to underlying Receipts.
    // Receipt ID will of the child will be stored as key and parent Transaction hash/Receipt ID
    // will be stored as a value
    let receipts_cache: ReceiptsCache =
        std::sync::Arc::new(Mutex::new(SizedCache::with_size(100_000)));

    let mut handlers = tokio_stream::wrappers::ReceiverStream::new(stream)
        .map(|streamer_message| {
            handle_streamer_message(
                streamer_message,
                &pool,
                receipts_cache.clone(),
                !opts.non_strict_mode,
            )
        })
        .buffer_unordered(1usize);

    // let mut time_now = std::time::Instant::now();
    while let Some(handle_message) = handlers.next().await {
        match handle_message {
            Ok(_block_height) => {
                // let elapsed = time_now.elapsed();
                // println!(
                //     "Elapsed time spent on block {}: {:.3?}",
                //     _block_height, elapsed
                // );
                // time_now = std::time::Instant::now();
            }
            Err(e) => {
                return Err(anyhow::anyhow!(e));
            }
        }
    }

    Ok(())
}

async fn handle_streamer_message(
    streamer_message: near_indexer_primitives::StreamerMessage,
    pool: &sqlx::Pool<sqlx::Postgres>,
    receipts_cache: ReceiptsCache,
    strict_mode: bool,
) -> anyhow::Result<u64> {
    if streamer_message.block.header.height % 100 == 0 {
        eprintln!(
            "{} / shards {}",
            streamer_message.block.header.height,
            streamer_message.shards.len()
        );
    }

    let blocks_future = db_adapters::blocks::store_block(pool, &streamer_message.block);

    let chunks_future = db_adapters::chunks::store_chunks(
        pool,
        &streamer_message.shards,
        &streamer_message.block.header.hash,
        streamer_message.block.header.timestamp,
    );

    let transactions_future = db_adapters::transactions::store_transactions(
        pool,
        &streamer_message.shards,
        &streamer_message.block.header.hash,
        streamer_message.block.header.timestamp,
        receipts_cache.clone(),
    );

    let receipts_future = db_adapters::receipts::store_receipts(
        pool,
        strict_mode,
        &streamer_message.shards,
        &streamer_message.block.header,
        receipts_cache.clone(),
    );

    let execution_outcomes_future = db_adapters::execution_outcomes::store_execution_outcomes(
        pool,
        &streamer_message.shards,
        &streamer_message.block.header.hash,
        streamer_message.block.header.timestamp,
        receipts_cache.clone(),
    );

    let account_changes_future = db_adapters::account_changes::store_account_changes(
        pool,
        &streamer_message.shards,
        &streamer_message.block.header.hash,
        streamer_message.block.header.timestamp,
    );

    blocks_future.await?;
    // FK to block_hash
    chunks_future.await?;
    // we have FK both to blocks and chunks
    transactions_future.await?;
    // this guy can contain local receipts, so we have to do that after transactions_future finished the work
    receipts_future.await?;
    try_join!(
        // this guy depends on transactions and receipts with its FKs
        account_changes_future,
        // this guy thinks that receipts_future finished, and clears the cache
        execution_outcomes_future
    )?;
    Ok(streamer_message.block.header.height)
}

fn init_tracing() {
    let mut env_filter = EnvFilter::new("near_lake_framework=info");

    if let Ok(rust_log) = std::env::var("RUST_LOG") {
        if !rust_log.is_empty() {
            for directive in rust_log.split(',').filter_map(|s| match s.parse() {
                Ok(directive) => Some(directive),
                Err(err) => {
                    eprintln!("Ignoring directive `{}`: {}", s, err);
                    None
                }
            }) {
                env_filter = env_filter.add_directive(directive);
            }
        }
    }

    tracing_subscriber::fmt::Subscriber::builder()
        .with_env_filter(env_filter)
        .with_writer(std::io::stderr)
        .init();
}

'''
'''--- src/models/account_changes.rs ---
use std::str::FromStr;

use bigdecimal::BigDecimal;
use sqlx::Arguments;

use crate::models::{FieldCount, PrintEnum};

#[derive(Debug, sqlx::FromRow, FieldCount)]
pub struct AccountChange {
    pub account_id: String,
    pub block_timestamp: BigDecimal,
    pub block_hash: String,
    pub caused_by_transaction_hash: Option<String>,
    pub caused_by_receipt_id: Option<String>,
    pub update_reason: String,
    pub nonstaked_balance: BigDecimal,
    pub staked_balance: BigDecimal,
    pub storage_usage: BigDecimal,
    pub chunk_index_in_block: i32,
    pub index_in_chunk: i32,
}

impl AccountChange {
    pub fn from_state_change_with_cause(
        state_change_with_cause: &near_indexer_primitives::views::StateChangeWithCauseView,
        changed_in_block_hash: &near_indexer_primitives::CryptoHash,
        changed_in_block_timestamp: u64,
        chunk_index_in_block: i32,
        index_in_chunk: i32,
    ) -> Option<Self> {
        let near_indexer_primitives::views::StateChangeWithCauseView { cause, value } =
            state_change_with_cause;

        let (account_id, account): (String, Option<&near_indexer_primitives::views::AccountView>) =
            match value {
                near_indexer_primitives::views::StateChangeValueView::AccountUpdate {
                    account_id,
                    account,
                } => (account_id.to_string(), Some(account)),
                near_indexer_primitives::views::StateChangeValueView::AccountDeletion {
                    account_id,
                } => (account_id.to_string(), None),
                _ => return None,
            };

        Some(Self {
            account_id,
            block_timestamp: changed_in_block_timestamp.into(),
            block_hash: changed_in_block_hash.to_string(),
            caused_by_transaction_hash: if let near_indexer_primitives::views::StateChangeCauseView::TransactionProcessing { tx_hash } = cause {
                Some(tx_hash.to_string())
            } else {
                None
            },
            caused_by_receipt_id: match cause {
                near_indexer_primitives::views::StateChangeCauseView::ActionReceiptProcessingStarted { receipt_hash } => Some(receipt_hash.to_string()),
                near_indexer_primitives::views::StateChangeCauseView::ActionReceiptGasReward { receipt_hash } => Some(receipt_hash.to_string()),
                near_indexer_primitives::views::StateChangeCauseView::ReceiptProcessing { receipt_hash } => Some(receipt_hash.to_string()),
                near_indexer_primitives::views::StateChangeCauseView::PostponedReceipt { receipt_hash } => Some(receipt_hash.to_string()),
                _ => None,
            },
            update_reason: cause.print().to_string(),
            nonstaked_balance: if let Some(acc) = account {
                BigDecimal::from_str(acc.amount.to_string().as_str())
                    .expect("`amount` expected to be u128")
            } else {
                BigDecimal::from(0)
            },
            staked_balance: if let Some(acc) = account {
                BigDecimal::from_str(acc.locked.to_string().as_str())
                    .expect("`locked` expected to be u128")
            } else {
                BigDecimal::from(0)
            },
            storage_usage: if let Some(acc) = account {
                acc.storage_usage.into()
            } else {
                BigDecimal::from(0)
            },
            chunk_index_in_block,
            index_in_chunk
        })
    }
}

impl crate::models::SqlMethods for AccountChange {
    fn add_to_args(&self, args: &mut sqlx::postgres::PgArguments) {
        args.add(&self.account_id);
        args.add(&self.block_timestamp);
        args.add(&self.block_hash);
        args.add(&self.caused_by_transaction_hash);
        args.add(&self.caused_by_receipt_id);
        args.add(&self.update_reason);
        args.add(&self.nonstaked_balance);
        args.add(&self.staked_balance);
        args.add(&self.storage_usage);
        args.add(&self.chunk_index_in_block);
        args.add(&self.index_in_chunk);
    }

    fn insert_query(account_changes_count: usize) -> anyhow::Result<String> {
        Ok("INSERT INTO account_changes VALUES ".to_owned()
            + &crate::models::create_placeholders(
                account_changes_count,
                AccountChange::field_count(),
            )?
            + " ON CONFLICT DO NOTHING")
    }

    fn delete_query() -> String {
        "DELETE FROM account_changes WHERE block_timestamp >= $1".to_string()
    }

    fn name() -> String {
        "account_changes".to_string()
    }
}

'''
'''--- src/models/blocks.rs ---
use std::str::FromStr;

use bigdecimal::BigDecimal;
use sqlx::Arguments;

use crate::models::FieldCount;

#[derive(Debug, sqlx::FromRow, FieldCount)]
pub struct Block {
    pub block_height: BigDecimal,
    pub block_hash: String,
    pub prev_block_hash: String,
    pub block_timestamp: BigDecimal,
    pub total_supply: BigDecimal,
    pub gas_price: BigDecimal,
    pub author_account_id: String,
}

impl Block {
    pub fn from_block_view(block_view: &near_indexer_primitives::views::BlockView) -> Self {
        Self {
            block_height: block_view.header.height.into(),
            block_hash: block_view.header.hash.to_string(),
            prev_block_hash: block_view.header.prev_hash.to_string(),
            block_timestamp: block_view.header.timestamp.into(),
            total_supply: BigDecimal::from_str(block_view.header.total_supply.to_string().as_str())
                .expect("`total_supply` expected to be u128"),
            gas_price: BigDecimal::from_str(block_view.header.gas_price.to_string().as_str())
                .expect("`gas_price` expected to be u128"),
            author_account_id: block_view.author.to_string(),
        }
    }
}

impl crate::models::SqlMethods for Block {
    fn add_to_args(&self, args: &mut sqlx::postgres::PgArguments) {
        args.add(&self.block_height);
        args.add(&self.block_hash);
        args.add(&self.prev_block_hash);
        args.add(&self.block_timestamp);
        args.add(&self.total_supply);
        args.add(&self.gas_price);
        args.add(&self.author_account_id);
    }

    fn insert_query(blocks_count: usize) -> anyhow::Result<String> {
        Ok("INSERT INTO blocks VALUES ".to_owned()
            + &crate::models::create_placeholders(blocks_count, Block::field_count())?
            + " ON CONFLICT DO NOTHING")
    }

    fn delete_query() -> String {
        "DELETE FROM blocks WHERE block_timestamp >= $1".to_string()
    }

    fn name() -> String {
        "blocks".to_string()
    }
}

'''
'''--- src/models/chunks.rs ---
use bigdecimal::BigDecimal;
use sqlx::Arguments;

use crate::models::FieldCount;

#[derive(Debug, sqlx::FromRow, FieldCount)]
pub struct Chunk {
    pub block_timestamp: BigDecimal,
    pub block_hash: String,
    pub chunk_hash: String,
    pub index_in_block: BigDecimal,
    pub signature: String,
    pub gas_limit: BigDecimal,
    pub gas_used: BigDecimal,
    pub author_account_id: String,
}

impl Chunk {
    pub fn from_chunk_view(
        chunk_view: &near_indexer_primitives::IndexerChunkView,
        block_hash: &near_indexer_primitives::CryptoHash,
        block_timestamp: u64,
    ) -> Self {
        Self {
            block_timestamp: block_timestamp.into(),
            block_hash: block_hash.to_string(),
            chunk_hash: chunk_view.header.chunk_hash.to_string(),
            index_in_block: chunk_view.header.shard_id.into(),
            signature: chunk_view.header.signature.to_string(),
            gas_limit: chunk_view.header.gas_limit.into(),
            gas_used: chunk_view.header.gas_used.into(),
            author_account_id: chunk_view.author.to_string(),
        }
    }
}

impl crate::models::SqlMethods for Chunk {
    fn add_to_args(&self, args: &mut sqlx::postgres::PgArguments) {
        args.add(&self.block_timestamp);
        args.add(&self.block_hash);
        args.add(&self.chunk_hash);
        args.add(&self.index_in_block);
        args.add(&self.signature);
        args.add(&self.gas_limit);
        args.add(&self.gas_used);
        args.add(&self.author_account_id);
    }

    fn insert_query(chunks_count: usize) -> anyhow::Result<String> {
        Ok("INSERT INTO chunks VALUES ".to_owned()
            + &crate::models::create_placeholders(chunks_count, Chunk::field_count())?
            + " ON CONFLICT DO NOTHING")
    }

    fn delete_query() -> String {
        "DELETE FROM chunks WHERE block_timestamp >= $1".to_string()
    }

    fn name() -> String {
        "chunks".to_string()
    }
}

'''
'''--- src/models/execution_outcomes.rs ---
use std::str::FromStr;

use bigdecimal::BigDecimal;
use sqlx::Arguments;

use crate::models::{FieldCount, PrintEnum};

#[derive(Debug, sqlx::FromRow, FieldCount)]
pub struct ExecutionOutcome {
    pub receipt_id: String,
    pub block_hash: String,
    pub block_timestamp: BigDecimal,
    pub chunk_index_in_block: i32,
    pub index_in_chunk: i32,
    pub gas_burnt: BigDecimal,
    pub tokens_burnt: BigDecimal,
    pub executor_account_id: String,
    pub status: String,
}

impl ExecutionOutcome {
    pub fn from_execution_outcome(
        execution_outcome: &near_indexer_primitives::views::ExecutionOutcomeWithIdView,
        index_in_chunk: i32,
        executed_in_block_timestamp: u64,
        shard_id: u64,
    ) -> Self {
        Self {
            receipt_id: execution_outcome.id.to_string(),
            block_hash: execution_outcome.block_hash.to_string(),
            block_timestamp: executed_in_block_timestamp.into(),
            chunk_index_in_block: shard_id as i32,
            index_in_chunk,
            gas_burnt: execution_outcome.outcome.gas_burnt.into(),
            tokens_burnt: BigDecimal::from_str(
                execution_outcome.outcome.tokens_burnt.to_string().as_str(),
            )
            .expect("`tokens_burnt` expected to be u128"),
            executor_account_id: execution_outcome.outcome.executor_id.to_string(),
            status: execution_outcome.outcome.status.print().to_string(),
        }
    }
}

impl crate::models::SqlMethods for ExecutionOutcome {
    fn add_to_args(&self, args: &mut sqlx::postgres::PgArguments) {
        args.add(&self.receipt_id);
        args.add(&self.block_hash);
        args.add(&self.block_timestamp);
        args.add(&self.chunk_index_in_block);
        args.add(&self.index_in_chunk);
        args.add(&self.gas_burnt);
        args.add(&self.tokens_burnt);
        args.add(&self.executor_account_id);
        args.add(&self.status);
    }

    fn insert_query(execution_outcome_count: usize) -> anyhow::Result<String> {
        Ok("INSERT INTO execution_outcomes VALUES ".to_owned()
            + &crate::models::create_placeholders(
                execution_outcome_count,
                ExecutionOutcome::field_count(),
            )?
            + " ON CONFLICT DO NOTHING")
    }

    fn delete_query() -> String {
        "DELETE FROM execution_outcomes WHERE block_timestamp >= $1".to_string()
    }

    fn name() -> String {
        "execution_outcomes".to_string()
    }
}

#[derive(Debug, sqlx::FromRow, FieldCount)]
pub struct ExecutionOutcomeReceipt {
    pub block_hash: String,
    pub block_timestamp: BigDecimal,
    pub executed_receipt_id: String,
    pub produced_receipt_id: String,
    pub chunk_index_in_block: i32,
    pub index_in_chunk: i32,
}

impl crate::models::SqlMethods for ExecutionOutcomeReceipt {
    fn add_to_args(&self, args: &mut sqlx::postgres::PgArguments) {
        args.add(&self.block_hash);
        args.add(&self.block_timestamp);
        args.add(&self.executed_receipt_id);
        args.add(&self.produced_receipt_id);
        args.add(&self.chunk_index_in_block);
        args.add(&self.index_in_chunk);
    }

    fn insert_query(execution_outcome_receipt_count: usize) -> anyhow::Result<String> {
        Ok(
            "INSERT INTO execution_outcomes__receipts VALUES ".to_owned()
                + &crate::models::create_placeholders(
                    execution_outcome_receipt_count,
                    ExecutionOutcomeReceipt::field_count(),
                )?
                + " ON CONFLICT DO NOTHING",
        )
    }

    fn delete_query() -> String {
        "DELETE FROM execution_outcomes__receipts WHERE block_timestamp >= $1".to_string()
    }

    fn name() -> String {
        "execution_outcomes__receipts".to_string()
    }
}

'''
'''--- src/models/mod.rs ---
use futures::future::try_join_all;
use sqlx::Arguments;
use std::fmt::Write;

use near_indexer_primitives::views::{
    AccessKeyPermissionView, ExecutionStatusView, StateChangeCauseView,
};

pub(crate) use account_changes::AccountChange;
pub(crate) use blocks::Block;
pub(crate) use chunks::Chunk;
pub(crate) use execution_outcomes::{ExecutionOutcome, ExecutionOutcomeReceipt};
pub(crate) use indexer_base::FieldCount;
pub(crate) use receipts::{ActionReceipt, ActionReceiptAction, ActionReceiptsOutput, DataReceipt};
pub(crate) use transactions::Transaction;

pub(crate) mod account_changes;
pub(crate) mod blocks;
pub(crate) mod chunks;
pub(crate) mod execution_outcomes;
pub(crate) mod receipts;
pub(crate) mod serializers;
pub(crate) mod transactions;

pub trait FieldCount {
    /// Get the number of fields on a struct.
    fn field_count() -> usize;
}

pub trait SqlMethods {
    fn add_to_args(&self, args: &mut sqlx::postgres::PgArguments);

    fn insert_query(count: usize) -> anyhow::Result<String>;

    fn delete_query() -> String;

    fn name() -> String;
}

pub async fn chunked_insert<T: SqlMethods + std::fmt::Debug>(
    pool: &sqlx::Pool<sqlx::Postgres>,
    items: &[T],
) -> anyhow::Result<()> {
    let futures = items
        .chunks(crate::db_adapters::CHUNK_SIZE_FOR_BATCH_INSERT)
        .map(|items_part| insert_retry_or_panic(pool, items_part, crate::db_adapters::RETRY_COUNT));
    try_join_all(futures).await.map(|_| ())
}

async fn insert_retry_or_panic<T: SqlMethods + std::fmt::Debug>(
    pool: &sqlx::Pool<sqlx::Postgres>,
    items: &[T],
    retry_count: usize,
) -> anyhow::Result<()> {
    let mut interval = crate::INTERVAL;
    let mut retry_attempt = 0usize;
    let query = T::insert_query(items.len())?;

    loop {
        if retry_attempt == retry_count {
            return Err(anyhow::anyhow!(
                "Failed to perform query to database after {} attempts. Stop trying.",
                retry_count
            ));
        }
        retry_attempt += 1;

        let mut args = sqlx::postgres::PgArguments::default();
        for item in items {
            item.add_to_args(&mut args);
        }

        match sqlx::query_with(&query, args).execute(pool).await {
            Ok(_) => break,
            Err(async_error) => {
                eprintln!(
                         "Error occurred during {}:\n{} were not stored. \n{:#?} \n Retrying in {} milliseconds...",
                         async_error,
                         &T::name(),
                         &items,
                         interval.as_millis(),
                     );
                tokio::time::sleep(interval).await;
                if interval < crate::MAX_DELAY_TIME {
                    interval *= 2;
                }
            }
        }
    }
    Ok(())
}

pub async fn select_retry_or_panic(
    pool: &sqlx::Pool<sqlx::Postgres>,
    query: &str,
    substitution_items: &[String],
) -> anyhow::Result<Vec<sqlx::postgres::PgRow>> {
    let mut interval = crate::INTERVAL;
    let mut retry_attempt = 0usize;

    loop {
        if retry_attempt == crate::db_adapters::RETRY_COUNT {
            return Err(anyhow::anyhow!(
                "Failed to perform query to database after {} attempts. Stop trying.",
                crate::db_adapters::RETRY_COUNT
            ));
        }
        retry_attempt += 1;

        let mut args = sqlx::postgres::PgArguments::default();
        for item in substitution_items {
            args.add(item);
        }

        match sqlx::query_with(query, args).fetch_all(pool).await {
            Ok(res) => return Ok(res),
            Err(async_error) => {
                // todo we print here select with non-filled placeholders. It would be better to get the final select statement here
                tracing::error!(
                         target: crate::INDEXER,
                         "Error occurred during {}:\nFailed SELECT:\n{}\n Retrying in {} milliseconds...",
                         async_error,
                    query,
                         interval.as_millis(),
                     );
                tokio::time::sleep(interval).await;
                if interval < crate::MAX_DELAY_TIME {
                    interval *= 2;
                }
            }
        }
    }
}

// Generates `($1, $2), ($3, $4)`
pub(crate) fn create_placeholders(
    mut items_count: usize,
    fields_count: usize,
) -> anyhow::Result<String> {
    if items_count < 1 {
        return Err(anyhow::anyhow!("At least 1 item expected"));
    }

    let mut start_num: usize = 1;
    let mut res = create_placeholder(&mut start_num, fields_count)?;
    items_count -= 1;
    while items_count > 0 {
        write!(
            res,
            ", {}",
            create_placeholder(&mut start_num, fields_count)?
        )?;
        items_count -= 1;
    }

    Ok(res)
}

// Generates `($1, $2, $3)`
pub(crate) fn create_placeholder(
    start_num: &mut usize,
    mut fields_count: usize,
) -> anyhow::Result<String> {
    if fields_count < 1 {
        return Err(anyhow::anyhow!("At least 1 field expected"));
    }
    let mut item = format!("(${}", start_num);
    *start_num += 1;
    fields_count -= 1;
    while fields_count > 0 {
        write!(item, ", ${}", start_num)?;
        *start_num += 1;
        fields_count -= 1;
    }
    item += ")";
    Ok(item)
}

pub(crate) trait PrintEnum {
    fn print(&self) -> &str;
}

impl PrintEnum for ExecutionStatusView {
    fn print(&self) -> &str {
        match self {
            ExecutionStatusView::Unknown => "UNKNOWN",
            ExecutionStatusView::Failure(_) => "FAILURE",
            ExecutionStatusView::SuccessValue(_) => "SUCCESS_VALUE",
            ExecutionStatusView::SuccessReceiptId(_) => "SUCCESS_RECEIPT_ID",
        }
    }
}

impl PrintEnum for AccessKeyPermissionView {
    fn print(&self) -> &str {
        match self {
            AccessKeyPermissionView::FunctionCall { .. } => "FUNCTION_CALL",
            AccessKeyPermissionView::FullAccess => "FULL_ACCESS",
        }
    }
}

impl PrintEnum for StateChangeCauseView {
    fn print(&self) -> &str {
        match self {
            StateChangeCauseView::NotWritableToDisk => {
                panic!("Unexpected variant {:?} received", self)
            }
            StateChangeCauseView::InitialState => panic!("Unexpected variant {:?} received", self),
            StateChangeCauseView::TransactionProcessing { .. } => "TRANSACTION_PROCESSING",
            StateChangeCauseView::ActionReceiptProcessingStarted { .. } => {
                "ACTION_RECEIPT_PROCESSING_STARTED"
            }
            StateChangeCauseView::ActionReceiptGasReward { .. } => "ACTION_RECEIPT_GAS_REWARD",
            StateChangeCauseView::ReceiptProcessing { .. } => "RECEIPT_PROCESSING",
            StateChangeCauseView::PostponedReceipt { .. } => "POSTPONED_RECEIPT",
            StateChangeCauseView::UpdatedDelayedReceipts => "UPDATED_DELAYED_RECEIPTS",
            StateChangeCauseView::ValidatorAccountsUpdate => "VALIDATOR_ACCOUNTS_UPDATE",
            StateChangeCauseView::Migration => "MIGRATION",
            StateChangeCauseView::Resharding => "RESHARDING",
        }
    }
}

'''
'''--- src/models/receipts.rs ---
use std::str::FromStr;

use bigdecimal::BigDecimal;
use sqlx::Arguments;

use crate::models::{self, FieldCount};

#[derive(Debug, sqlx::FromRow, FieldCount)]
pub struct DataReceipt {
    pub receipt_id: String,
    pub block_hash: String,
    pub chunk_hash: String,
    pub block_timestamp: BigDecimal,
    pub chunk_index_in_block: i32,
    pub receipt_index_in_chunk: i32,
    pub predecessor_account_id: String,
    pub receiver_account_id: String,
    pub originated_from_transaction_hash: String,
    pub data_id: String,
    pub data: Option<Vec<u8>>,
}

impl DataReceipt {
    pub fn try_from_data_receipt_view(
        receipt: &near_indexer_primitives::views::ReceiptView,
        block_hash: &near_indexer_primitives::CryptoHash,
        transaction_hash: &str,
        chunk_header: &near_indexer_primitives::views::ChunkHeaderView,
        index_in_chunk: i32,
        block_timestamp: u64,
    ) -> anyhow::Result<Self> {
        if let near_indexer_primitives::views::ReceiptEnumView::Data { data_id, data } =
            &receipt.receipt
        {
            Ok(Self {
                receipt_id: receipt.receipt_id.to_string(),
                block_hash: block_hash.to_string(),
                chunk_hash: chunk_header.chunk_hash.to_string(),
                block_timestamp: block_timestamp.into(),
                chunk_index_in_block: chunk_header.shard_id as i32,
                receipt_index_in_chunk: index_in_chunk,
                predecessor_account_id: receipt.predecessor_id.to_string(),
                receiver_account_id: receipt.receiver_id.to_string(),
                originated_from_transaction_hash: transaction_hash.to_string(),
                data_id: data_id.to_string(),
                data: data.clone(),
            })
        } else {
            Err(anyhow::anyhow!("Given ReceiptView is not of Data variant"))
        }
    }
}

impl crate::models::SqlMethods for DataReceipt {
    fn add_to_args(&self, args: &mut sqlx::postgres::PgArguments) {
        args.add(&self.receipt_id);
        args.add(&self.block_hash);
        args.add(&self.chunk_hash);
        args.add(&self.block_timestamp);
        args.add(&self.chunk_index_in_block);
        args.add(&self.receipt_index_in_chunk);
        args.add(&self.predecessor_account_id);
        args.add(&self.receiver_account_id);
        args.add(&self.originated_from_transaction_hash);
        args.add(&self.data_id);
        args.add(&self.data);
    }

    fn insert_query(items_count: usize) -> anyhow::Result<String> {
        Ok("INSERT INTO data_receipts VALUES ".to_owned()
            + &crate::models::create_placeholders(items_count, DataReceipt::field_count())?
            + " ON CONFLICT DO NOTHING")
    }

    fn delete_query() -> String {
        "DELETE FROM data_receipts WHERE block_timestamp >= $1".to_string()
    }

    fn name() -> String {
        "data_receipts".to_string()
    }
}

#[derive(Debug, sqlx::FromRow, FieldCount)]
pub struct ActionReceipt {
    pub receipt_id: String,
    pub block_hash: String,
    pub chunk_hash: String,
    pub block_timestamp: BigDecimal,
    pub chunk_index_in_block: i32,
    pub receipt_index_in_chunk: i32,
    pub predecessor_account_id: String,
    pub receiver_account_id: String,
    pub originated_from_transaction_hash: String,
    pub signer_account_id: String,
    pub signer_public_key: String,
    pub gas_price: BigDecimal,
}

impl ActionReceipt {
    pub fn try_from_action_receipt_view(
        receipt: &near_indexer_primitives::views::ReceiptView,
        block_hash: &near_indexer_primitives::CryptoHash,
        transaction_hash: &str,
        chunk_header: &near_indexer_primitives::views::ChunkHeaderView,
        index_in_chunk: i32,
        block_timestamp: u64,
    ) -> anyhow::Result<Self> {
        if let near_indexer_primitives::views::ReceiptEnumView::Action {
            signer_id,
            signer_public_key,
            gas_price,
            ..
        } = &receipt.receipt
        {
            Ok(Self {
                receipt_id: receipt.receipt_id.to_string(),
                block_hash: block_hash.to_string(),
                chunk_hash: chunk_header.chunk_hash.to_string(),
                block_timestamp: block_timestamp.into(),
                chunk_index_in_block: chunk_header.shard_id as i32,
                receipt_index_in_chunk: index_in_chunk,
                predecessor_account_id: receipt.predecessor_id.to_string(),
                receiver_account_id: receipt.receiver_id.to_string(),
                originated_from_transaction_hash: transaction_hash.to_string(),
                signer_account_id: signer_id.to_string(),
                signer_public_key: signer_public_key.to_string(),
                gas_price: BigDecimal::from_str(gas_price.to_string().as_str())
                    .expect("gas_price expected to be u128"),
            })
        } else {
            Err(anyhow::anyhow!(
                "Given ReceiptView is not of Action variant"
            ))
        }
    }
}

impl models::SqlMethods for ActionReceipt {
    fn add_to_args(&self, args: &mut sqlx::postgres::PgArguments) {
        args.add(&self.receipt_id);
        args.add(&self.block_hash);
        args.add(&self.chunk_hash);
        args.add(&self.block_timestamp);
        args.add(&self.chunk_index_in_block);
        args.add(&self.receipt_index_in_chunk);
        args.add(&self.predecessor_account_id);
        args.add(&self.receiver_account_id);
        args.add(&self.originated_from_transaction_hash);
        args.add(&self.signer_account_id);
        args.add(&self.signer_public_key);
        args.add(&self.gas_price);
    }

    fn insert_query(items_count: usize) -> anyhow::Result<String> {
        Ok("INSERT INTO action_receipts VALUES ".to_owned()
            + &crate::models::create_placeholders(items_count, ActionReceipt::field_count())?
            + " ON CONFLICT DO NOTHING")
    }

    fn delete_query() -> String {
        "DELETE FROM action_receipts WHERE block_timestamp >= $1".to_string()
    }

    fn name() -> String {
        "action_receipts".to_string()
    }
}

#[derive(Debug, sqlx::FromRow, FieldCount)]
pub struct ActionReceiptAction {
    pub block_hash: String,
    pub block_timestamp: BigDecimal,
    pub receipt_id: String,
    pub action_kind: String,
    pub args: serde_json::Value,
    pub predecessor_account_id: String,
    pub receiver_account_id: String,
    pub chunk_index_in_block: i32,
    pub index_in_chunk: i32,
}

impl ActionReceiptAction {
    pub fn from_action_view(
        receipt_id: String,
        action_view: &near_indexer_primitives::views::ActionView,
        predecessor_account_id: String,
        receiver_account_id: String,
        block_header: &near_indexer_primitives::views::BlockHeaderView,
        chunk_index_in_block: i32,
        index_in_chunk: i32,
    ) -> Self {
        let (action_kind, args) =
            models::serializers::extract_action_type_and_value_from_action_view(action_view);

        Self {
            block_hash: block_header.hash.to_string(),
            block_timestamp: block_header.timestamp.into(),
            receipt_id,
            action_kind,
            args,
            predecessor_account_id,
            receiver_account_id,
            chunk_index_in_block,
            index_in_chunk,
        }
    }
}

impl crate::models::SqlMethods for ActionReceiptAction {
    fn add_to_args(&self, args: &mut sqlx::postgres::PgArguments) {
        args.add(&self.block_hash);
        args.add(&self.block_timestamp);
        args.add(&self.receipt_id);
        args.add(&self.action_kind);
        args.add(&self.args);
        args.add(&self.predecessor_account_id);
        args.add(&self.receiver_account_id);
        args.add(&self.chunk_index_in_block);
        args.add(&self.index_in_chunk);
    }

    fn insert_query(items_count: usize) -> anyhow::Result<String> {
        Ok("INSERT INTO action_receipts__actions VALUES ".to_owned()
            + &crate::models::create_placeholders(items_count, ActionReceiptAction::field_count())?
            + " ON CONFLICT DO NOTHING")
    }

    fn delete_query() -> String {
        "DELETE FROM action_receipts__actions WHERE block_timestamp >= $1".to_string()
    }

    fn name() -> String {
        "action_receipts__actions".to_string()
    }
}

#[derive(Debug, sqlx::FromRow, FieldCount)]
pub struct ActionReceiptsOutput {
    pub block_hash: String,
    pub block_timestamp: BigDecimal,
    pub receipt_id: String,
    pub output_data_id: String,
    pub receiver_account_id: String,
    pub chunk_index_in_block: i32,
    pub index_in_chunk: i32,
}

impl ActionReceiptsOutput {
    pub fn from_data_receiver(
        receipt_id: String,
        data_receiver: &near_indexer_primitives::views::DataReceiverView,
        block_hash: &near_indexer_primitives::CryptoHash,
        block_timestamp: u64,
        chunk_index_in_block: i32,
        index_in_chunk: i32,
    ) -> Self {
        Self {
            block_hash: block_hash.to_string(),
            block_timestamp: block_timestamp.into(),
            receipt_id,
            output_data_id: data_receiver.data_id.to_string(),
            receiver_account_id: data_receiver.receiver_id.to_string(),
            chunk_index_in_block,
            index_in_chunk,
        }
    }
}

impl crate::models::SqlMethods for ActionReceiptsOutput {
    fn add_to_args(&self, args: &mut sqlx::postgres::PgArguments) {
        args.add(&self.block_hash);
        args.add(&self.block_timestamp);
        args.add(&self.receipt_id);
        args.add(&self.output_data_id);
        args.add(&self.receiver_account_id);
        args.add(&self.chunk_index_in_block);
        args.add(&self.index_in_chunk);
    }

    fn insert_query(items_count: usize) -> anyhow::Result<String> {
        Ok("INSERT INTO action_receipts__outputs VALUES ".to_owned()
            + &crate::models::create_placeholders(
                items_count,
                ActionReceiptsOutput::field_count(),
            )?
            + " ON CONFLICT DO NOTHING")
    }

    fn delete_query() -> String {
        "DELETE FROM action_receipts__outputs WHERE block_timestamp >= $1".to_string()
    }

    fn name() -> String {
        "action_receipts__outputs".to_string()
    }
}

'''
'''--- src/models/serializers.rs ---
use serde::{Deserialize, Serialize};
use serde_json::json;

/// We want to store permission field more explicitly so we are making copy of nearcore struct
/// to change serde parameters of serialization.
#[derive(Serialize, Deserialize, Debug, sqlx::FromRow)]
pub(crate) struct AccessKeyView {
    pub nonce: near_indexer_primitives::types::Nonce,
    pub permission: AccessKeyPermissionView,
}

impl From<&near_indexer_primitives::views::AccessKeyView> for AccessKeyView {
    fn from(access_key_view: &near_indexer_primitives::views::AccessKeyView) -> Self {
        Self {
            nonce: access_key_view.nonce,
            permission: access_key_view.permission.clone().into(),
        }
    }
}

/// This is a enum we want to store more explicitly, so we copy it from nearcore and provide
/// different serde representation settings
#[derive(Serialize, Deserialize, Debug, Clone)]
pub(crate) enum AccessKeyPermissionView {
    FunctionCall {
        allowance: Option<String>,
        receiver_id: String,
        method_names: Vec<String>,
    },
    FullAccess,
}

impl From<near_indexer_primitives::views::AccessKeyPermissionView> for AccessKeyPermissionView {
    fn from(permission: near_indexer_primitives::views::AccessKeyPermissionView) -> Self {
        match permission {
            near_indexer_primitives::views::AccessKeyPermissionView::FullAccess => Self::FullAccess,
            near_indexer_primitives::views::AccessKeyPermissionView::FunctionCall {
                allowance,
                receiver_id,
                method_names,
            } => Self::FunctionCall {
                allowance: allowance.map(|v| v.to_string()),
                receiver_id: receiver_id.escape_default().to_string(),
                method_names: method_names
                    .into_iter()
                    .map(|method_name| method_name.escape_default().to_string())
                    .collect(),
            },
        }
    }
}

pub(crate) fn extract_action_type_and_value_from_action_view(
    action_view: &near_indexer_primitives::views::ActionView,
) -> (String, serde_json::Value) {
    match action_view {
        near_indexer_primitives::views::ActionView::CreateAccount => {
            ("CREATE_ACCOUNT".to_string(), json!({}))
        }
        near_indexer_primitives::views::ActionView::DeployContract { code } => (
            "DEPLOY_CONTRACT".to_string(),
            json!({
                "code_sha256":  hex::encode(
                    base64::decode(code).expect("code expected to be encoded to base64")
                )
            }),
        ),
        near_indexer_primitives::views::ActionView::FunctionCall {
            method_name,
            args,
            gas,
            deposit,
        } => {
            let mut arguments = json!({
                "method_name": method_name.escape_default().to_string(),
                "args_base64": args,
                "gas": gas,
                "deposit": deposit.to_string(),
            });

            // During denormalization of action_receipt_actions table we wanted to try to decode
            // args which is base64 encoded in case if it is a JSON object and put them near initial
            // args_base64
            // See for reference https://github.com/near/near-indexer-for-explorer/issues/87
            if let Ok(decoded_args) = base64::decode(args) {
                if let Ok(mut args_json) = serde_json::from_slice(&decoded_args) {
                    escape_json(&mut args_json);
                    arguments["args_json"] = args_json;
                }
            }

            ("FUNCTION_CALL".to_string(), arguments)
        }
        near_indexer_primitives::views::ActionView::Transfer { deposit } => (
            "TRANSFER".to_string(),
            json!({ "deposit": deposit.to_string() }),
        ),
        near_indexer_primitives::views::ActionView::Stake { stake, public_key } => (
            "STAKE".to_string(),
            json!({
                "stake": stake.to_string(),
                "public_key": public_key,
            }),
        ),
        near_indexer_primitives::views::ActionView::AddKey {
            public_key,
            access_key,
        } => (
            "ADD_KEY".to_string(),
            json!({
                "public_key": public_key,
                "access_key": crate::models::serializers::AccessKeyView::from(access_key),
            }),
        ),
        near_indexer_primitives::views::ActionView::DeleteKey { public_key } => (
            "DELETE_KEY".to_string(),
            json!({
                "public_key": public_key,
            }),
        ),
        near_indexer_primitives::views::ActionView::DeleteAccount { beneficiary_id } => (
            "DELETE_ACCOUNT".to_string(),
            json!({
                "beneficiary_id": beneficiary_id,
            }),
        ),
    }
}

/// This function will modify the JSON escaping the values
/// We can not store data with null-bytes in TEXT or JSONB fields
/// of PostgreSQL
/// ref: https://www.commandprompt.com/blog/null-characters-workarounds-arent-good-enough/
fn escape_json(object: &mut serde_json::Value) {
    match object {
        serde_json::Value::Object(ref mut value) => {
            for (_key, val) in value {
                escape_json(val);
            }
        }
        serde_json::Value::Array(ref mut values) => {
            for element in values.iter_mut() {
                escape_json(element)
            }
        }
        serde_json::Value::String(ref mut value) => *value = value.escape_default().to_string(),
        _ => {}
    }
}

'''
'''--- src/models/transactions.rs ---
use std::str::FromStr;

use bigdecimal::BigDecimal;
use sqlx::Arguments;

use crate::models::{FieldCount, PrintEnum};

#[derive(Debug, sqlx::FromRow, FieldCount)]
pub struct Transaction {
    pub transaction_hash: String,
    pub block_hash: String,
    pub chunk_hash: String,
    pub block_timestamp: BigDecimal,
    pub chunk_index_in_block: i32,
    pub index_in_chunk: i32,
    pub signer_account_id: String,
    pub signer_public_key: String,
    pub nonce: BigDecimal,
    pub receiver_account_id: String,
    pub signature: String,
    pub status: String,
    pub converted_into_receipt_id: String,
    pub receipt_conversion_gas_burnt: BigDecimal,
    pub receipt_conversion_tokens_burnt: BigDecimal,
}

impl Transaction {
    pub fn from_indexer_transaction(
        tx: &near_indexer_primitives::IndexerTransactionWithOutcome,
        // hack for supporting duplicated transaction hashes
        transaction_hash: &str,
        converted_into_receipt_id: &str,
        block_hash: &near_indexer_primitives::CryptoHash,
        block_timestamp: u64,
        chunk_header: &near_indexer_primitives::views::ChunkHeaderView,
        index_in_chunk: i32,
    ) -> Self {
        Self {
            transaction_hash: transaction_hash.to_string(),
            block_hash: block_hash.to_string(),
            chunk_hash: chunk_header.chunk_hash.to_string(),
            block_timestamp: block_timestamp.into(),
            chunk_index_in_block: chunk_header.shard_id as i32,
            index_in_chunk,
            nonce: tx.transaction.nonce.into(),
            signer_account_id: tx.transaction.signer_id.to_string(),
            signer_public_key: tx.transaction.public_key.to_string(),
            signature: tx.transaction.signature.to_string(),
            receiver_account_id: tx.transaction.receiver_id.to_string(),
            converted_into_receipt_id: converted_into_receipt_id.to_string(),
            status: tx
                .outcome
                .execution_outcome
                .outcome
                .status
                .print()
                .to_string(),
            receipt_conversion_gas_burnt: tx.outcome.execution_outcome.outcome.gas_burnt.into(),
            receipt_conversion_tokens_burnt: BigDecimal::from_str(
                tx.outcome
                    .execution_outcome
                    .outcome
                    .tokens_burnt
                    .to_string()
                    .as_str(),
            )
            .expect("`token_burnt` must be u128"),
        }
    }
}

impl crate::models::SqlMethods for Transaction {
    fn add_to_args(&self, args: &mut sqlx::postgres::PgArguments) {
        args.add(&self.transaction_hash);
        args.add(&self.block_hash);
        args.add(&self.chunk_hash);
        args.add(&self.block_timestamp);
        args.add(&self.chunk_index_in_block);
        args.add(&self.index_in_chunk);
        args.add(&self.signer_account_id);
        args.add(&self.signer_public_key);
        args.add(&self.nonce);
        args.add(&self.receiver_account_id);
        args.add(&self.signature);
        args.add(&self.status);
        args.add(&self.converted_into_receipt_id);
        args.add(&self.receipt_conversion_gas_burnt);
        args.add(&self.receipt_conversion_tokens_burnt);
    }

    fn insert_query(transactions_count: usize) -> anyhow::Result<String> {
        Ok("INSERT INTO transactions VALUES ".to_owned()
            + &crate::models::create_placeholders(transactions_count, Transaction::field_count())?
            + " ON CONFLICT DO NOTHING")
    }

    fn delete_query() -> String {
        "DELETE FROM transactions WHERE block_timestamp >= $1".to_string()
    }

    fn name() -> String {
        "transactions".to_string()
    }
}

'''