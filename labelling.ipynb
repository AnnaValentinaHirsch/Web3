{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook is used to label the training data for finetuning the Starcoder2 model for the Near dApps domain. Openai API is used to generate the labels (user prompts) corresponding to the github repos, tree structures, and readme contents.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install pandas numpy openai python-dotenv datasets transformers torch matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import Dataset, load_dataset # huggingface\n",
    "from transformers import pipeline # summarizer\n",
    "from openai import OpenAI # new\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = 'secretkey'\n",
    "client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Load Dataset\n",
    "dataset = load_dataset('jcarbonnell/structTuningNEAR')\n",
    "\n",
    "# Convert the train split of the dataset to a pandas DataFrame\n",
    "train = dataset['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order by size of readme file\n",
    "train = train.sort_values(by='readme', key=lambda x: x.str.len(), ascending=False)\n",
    "\n",
    "# Remove rows with empty readme files\n",
    "train = train[train['readme'].str.len() > 200]\n",
    "train = train[train['readme'].str.len() < 2000000] # remove problematic files that cause crash\n",
    "\n",
    "# Reset index and drop the old index\n",
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the wordcount of readme files\n",
    "train['readme_word_count'] = train['readme'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Plot the histogram of README word counts\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(train['readme_word_count'], bins=30, edgecolor='black')\n",
    "plt.title('Histogram of README File Word Counts')\n",
    "plt.xlabel('Number of Words in README')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Display the top 10 longest README files\n",
    "print(train[['repoName', 'readme_word_count']].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Def params\n",
    "engine = \"gpt-4o\"\n",
    "max_output_tokens = 500\n",
    "example = \"\"\"Create a project structure for a NEAR DApp based on the following requirements:\n",
    "\n",
    "1. The project should be related to the Nearuko NFT, which can be converted into a character in the Etheruko game.\n",
    "2. Use necessary files typically required for a NEAR Protocol mainnet DApp.\n",
    "3. Include all relevant dependencies and packages for an NFT project on the NEAR Protocol.\n",
    "4. The main coding language should be TypeScript.\n",
    "5. Ensure the project includes configurations, tests, and NEAR-compatible contract files.\n",
    "6. Capture any common NEAR DApp structure conventions while setting up the project.\n",
    "\n",
    "Provide a well-organized directory structure and file list based on these requirements.\"\"\"\n",
    "\n",
    "# Function to generate labels (prompts)\n",
    "def generate_prompt(repoName, tree, readme, example):\n",
    "    # Create a user prompt for a coding assistant\n",
    "    prompt = (\n",
    "        f\"You are provided with a GitHub repository called \\n{repoName}\\n\\n. This repository has the following directory structure:\\n\"\n",
    "        f\"{tree}\\n\\n\"\n",
    "        f\"The README file contains the following information:\\n{readme}\\n\\n\"\n",
    "        f\"Step 1: Extract all the relevant information from the README file needed to predict the corresponding tree for a NEAR DApp, such as necessary files, dependencies, packages, and any particular coding languages or frameworks that should be used. \"\n",
    "        f\"Step 2: Write a perfect user prompt asking a coding assistant to create a project stucture based only on the extracted information from the README file. Only return the user prompt from Step 2. Do not return any information about the tree or file names. Here is an example: \\n{example}\\n\\n\"\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=engine,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=max_output_tokens\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "# Print example prompt\n",
    "#print(generate_prompt(train['repoName'][1500], train['tree'][1500], train['readme'][1500], example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define checkpoint file\n",
    "checkpoint_file = 'checkpoint.csv'\n",
    "\n",
    "# Define the summarizer\n",
    "summarizer = pipeline(\"summarization\", model=\"t5-base\", tokenizer=\"t5-base\")\n",
    "max_input_tokens = 2000\n",
    "delay_between_requests = 1 # in seconds\n",
    "\n",
    "# Function to summarize long readme files\n",
    "def summarize(text, max_length):\n",
    "    if len(text.split()) > max_length:\n",
    "        summary = summarizer(text, max_length=max_length, do_sample=False)\n",
    "        return summary[0]['summary_text']\n",
    "    return text\n",
    "\n",
    "# Function to count tokens\n",
    "def count_tokens(text, tokenizer):\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "# Initialize user_prompts list with None values\n",
    "user_prompts = [None] * len(train)\n",
    "\n",
    "# Save function\n",
    "def save_checkpoint(dataframe, prompts, filename):\n",
    "    dataframe['user_prompt'] = prompts\n",
    "    dataframe.to_csv(filename, index=False, mode='a', header=False, escapechar='\\\\')\n",
    "\n",
    "\n",
    "# Load function\n",
    "def load_checkpoint(filename):\n",
    "    return pd.read_csv(filename)\n",
    "\n",
    "checkpoint_file = 'checkpoint.csv'\n",
    "\n",
    "# Load existing checkpoint if available\n",
    "try:\n",
    "    train = load_checkpoint(checkpoint_file)\n",
    "    user_prompts = train['user_prompt'].tolist()\n",
    "except FileNotFoundError:\n",
    "    user_prompts = [None] * len(train)\n",
    "\n",
    "# Processing loop\n",
    "for index, row in train.iterrows():\n",
    "    if user_prompts[index] is not None:\n",
    "        continue  # Skip already processed rows\n",
    "\n",
    "    repoName = row['repoName']\n",
    "    tree = row['tree']\n",
    "    readme = row['readme']\n",
    "    try:\n",
    "        # Check if readme is too long and summarize if needed\n",
    "        readme = summarize(readme, max_input_tokens)\n",
    "\n",
    "        # Generate the user prompt\n",
    "        user_prompt = generate_prompt(repoName, tree, readme, example)\n",
    "\n",
    "        # Count tokens in the user prompt\n",
    "        input_tokens = count_tokens(user_prompt, summarizer.tokenizer)\n",
    "\n",
    "        if input_tokens > max_input_tokens + max_output_tokens:\n",
    "            raise ValueError(f\"Prompt too long: {input_tokens} tokens (max allowed is {max_input_tokens + max_output_tokens})\")\n",
    "\n",
    "        user_prompts[index] = user_prompt  # None for errors\n",
    "\n",
    "        # Log the length of the input and prompt for debugging\n",
    "        print(f\"Row {index}: Input length {len(summarizer.tokenizer.encode(readme))} tokens, Prompt length {input_tokens} tokens\")\n",
    "\n",
    "        # Free up GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Implement delay between requests\n",
    "        time.sleep(delay_between_requests)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {index}: {e}\")\n",
    "\n",
    "    # Save checkpoint after each row\n",
    "    save_checkpoint(train, user_prompts, checkpoint_file)\n",
    "\n",
    "# Append column to dataframe\n",
    "train['user_prompt'] = user_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web3venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
